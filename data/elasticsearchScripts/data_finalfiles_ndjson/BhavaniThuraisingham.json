{"title": "A new intrusion detection system using support vector machines and hierarchical clustering\n", "abstract": " Whenever an intrusion occurs, the security and value of a computer system is compromised. Network-based attacks make it difficult for legitimate users to access various network services by purposely occupying or sabotaging network resources and services. This can be done by sending large amounts of network traffic, exploiting well-known faults in networking services, and by overloading network hosts. Intrusion Detection attempts to detect computer attacks by examining various data records observed in processes on the network and it is split into two groups, anomaly detection systems and misuse detection systems. Anomaly detection is an attempt to search for malicious behavior that deviates from established normal patterns. Misuse detection is used to identify intrusions that match known attack scenarios. Our interest here is in anomaly detection and our proposed method is a scalable solution for\u00a0\u2026", "num_citations": "495\n", "authors": ["1190"]}
{"title": "Classification and novel class detection in concept-drifting data streams under time constraints\n", "abstract": " Most existing data stream classification techniques ignore one important aspect of stream data: arrival of a novel class. We address this issue and propose a data stream classification technique that integrates a novel class detection mechanism into traditional classifiers, enabling automatic detection of novel classes before the true labels of the novel class instances arrive. Novel class detection problem becomes more challenging in the presence of concept-drift, when the underlying data distributions evolve in streams. In order to determine whether an instance belongs to a novel class, the classification model sometimes needs to wait for more test instances to discover similarities among those instances. A maximum allowable wait time T c  is imposed as a time constraint to classify a test instance. Furthermore, most existing stream classification approaches assume that the true label of a data point can be accessed\u00a0\u2026", "num_citations": "413\n", "authors": ["1190"]}
{"title": "Privacy preservation in wireless sensor networks: A state-of-the-art survey\n", "abstract": " Much of the existing work on wireless sensor networks (WSNs) has focused on addressing the power and computational resource constraints of WSNs by the design of specific routing, MAC, and cross-layer protocols. Recently, there have been heightened privacy concerns over the data collected by and transmitted through WSNs. The wireless transmission required by a WSN, and the self-organizing nature of its architecture, makes privacy protection for WSNs an especially challenging problem. This paper provides a state-of-the-art survey of privacy-preserving techniques for WSNs. In particular, we review two main categories of privacy-preserving techniques for protecting two types of private information, data-oriented and context-oriented privacy, respectively. We also discuss a number of important open challenges for future research. Our hope is that this paper sheds some light on a fruitful direction of future\u00a0\u2026", "num_citations": "405\n", "authors": ["1190"]}
{"title": "Security issues for cloud computing\n", "abstract": " In this paper, the authors discuss security issues for cloud computing and present a layered framework for secure clouds and then focus on two of the layers, ie, the storage layer and the data layer. In particular, the authors discuss a scheme for secure third party publications of documents in a cloud. Next, the paper will converse secure federated query processing with map Reduce and Hadoop, and discuss the use of secure co-processors for cloud computing. Finally, the authors discuss XACML implementation for Hadoop and discuss their beliefs that building trusted applications from untrusted components will be a major aspect of secure cloud computing.", "num_citations": "315\n", "authors": ["1190"]}
{"title": "ROWLBAC: representing role based access control in OWL\n", "abstract": " There have been two parallel themes in access control research in recent years. On the one hand there are efforts to develop new access control models to meet the policy needs of real world application domains. In parallel, and almost separately, researchers have developed policy languages for access control. This paper is motivated by the consideration that these two parallel efforts need to develop synergy. A policy language in the abstract without ties to a model gives the designer little guidance. Conversely a model may not have the machinery to express all the policy details of a given system or may deliberately leave important aspects unspecified. Our vision for the future is a world where advanced access control concepts are embodied in models that are supported by policy languages in a natural intuitive manner, while allowing for details beyond the models to be further specified in the policy language.", "num_citations": "305\n", "authors": ["1190"]}
{"title": "Inferring private information using social network data\n", "abstract": " On-line social networks, such as Facebook, are increasingly utilized by many users. These networks allow people to publish details about themselves and connect to their friends. Some of the information revealed inside these networks is private and it is possible that corporations could use learning algorithms on the released data to predict undisclosed private information. In this paper, we explore how to launch inference attacks using released social networking data to predict undisclosed private information about individuals. We then explore the effectiveness of possible sanitization techniques that can be used to combat such inference attacks under different scenarios.", "num_citations": "303\n", "authors": ["1190"]}
{"title": "Heuristics-based query processing for large RDF graphs using cloud computing\n", "abstract": " Semantic web is an emerging area to augment human reasoning. Various technologies are being developed in this arena which have been standardized by the World Wide Web Consortium (W3C). One such standard is the Resource Description Framework (RDF). Semantic web technologies can be utilized to build efficient and scalable systems for Cloud Computing. With the explosion of semantic web technologies, large RDF graphs are common place. This poses significant challenges for the storage and retrieval of RDF graphs. Current frameworks do not scale for large RDF graphs and as a result do not address these challenges. In this paper, we describe a framework that we built using Hadoop to store and retrieve large numbers of RDF triples by exploiting the cloud computing paradigm. We describe a scheme to store RDF data in Hadoop Distributed File System. More than one Hadoop job (the smallest unit\u00a0\u2026", "num_citations": "264\n", "authors": ["1190"]}
{"title": "technologies, techniques, tools, and Trends\n", "abstract": " Recent developments in information systems technologies have resulted in computerizing many applications in various business areas. Data has become a critical resource in many organizations, and therefore, efficient access to data, sharing the data, extracting information from the data, and making use of the information has become an urgent need. As a result, there have been many efforts on not only integrating the various data sources scattered across several sites, but extracting information from these databases in the form of patterns and trends has also become important. These data sources may be databases managed by database management systems, or they could be data warehoused in a repository from multiple data sources. To provide the interoperability as well as warehousing between the multiple data sources and systems, and to extract information from the databases and warehouses various\u00a0\u2026", "num_citations": "259\n", "authors": ["1190"]}
{"title": "A semantic web based framework for social network access control\n", "abstract": " The existence of on-line social networks that include person specific information creates interesting opportunities for various applications ranging from marketing to community organization. On the other hand, security and privacy concerns need to be addressed for creating such applications. Improving social network access control systems appears as the first step toward addressing the existing security and privacy concerns related to on-line social networks. To address some of the current limitations, we propose an extensible fine grained access control model based on semantic web tools. In addition, we propose authorization, admin and filtering policies that depend on trust relationships among various users, and are modeled using OWL and SWRL. Besides describing the model, we present the architecture of the framework in its support.", "num_citations": "219\n", "authors": ["1190"]}
{"title": "Preventing private information inference attacks on social networks\n", "abstract": " Online social networks, such as Facebook, are increasingly utilized by many people. These networks allow users to publish details about themselves and to connect to their friends. Some of the information revealed inside these networks is meant to be private. Yet it is possible to use learning algorithms on released data to predict private information. In this paper, we explore how to launch inference attacks using released social networking data to predict private information. We then devise three possible sanitization techniques that could be used in various situations. Then, we explore the effectiveness of these techniques and attempt to use methods of collective inference to discover sensitive attributes of the data set. We show that we can decrease the effectiveness of both local and relational classification algorithms by using the sanitization methods we described.", "num_citations": "212\n", "authors": ["1190"]}
{"title": "Knowledge management: Classic and contemporary works\n", "abstract": " This book provides an introduction to the field of knowledge management. Taking a learning-centric rather than information-centric approach, it emphasizes the continuous acquisition and application of knowledge. The book is organized into three sections, each opening with a classic work from a leader in the field. The first section, Strategy, discusses the motivation for knowledge management and how to structure a knowledge management program. The second section, Process, discusses the use of knowledge management to make existing practices more effective, the speeding up of organizational learning, and effective methods for implementing knowledge management. The third section, Metrics, discusses how to measure the impact of knowledge management on an organization. In addition to the classic essays, each section contains unpublished works that further develop the foundational concepts and strategies.", "num_citations": "203\n", "authors": ["1190"]}
{"title": "Eksploracja danych\n", "abstract": " OPIS: Przedmiot ma na celu zapoznanie student\u00f3w z podstawowymi metodami eksploracji danych. Podczas laboratori\u00f3w studenci b\u0119d\u0105 implementowa\u0107 r\u00f3\u017cne algorytmy eksploracji (okre\u015blanie wa\u017cno\u015bci atrybut\u00f3w, odkrywanie cech, odkrywanie asocjacji, klasyfikacja, analiza skupie\u0144, regresja) oraz b\u0119d\u0105 \u0107wiczy\u0107 proces odkrywania wiedzy w bazach danych. G\u0142\u00f3wny nacisk po\u0142o\u017cony jest na \u0107wiczenia praktyczne, studenci poznaj\u0105 istniej\u0105ce narz\u0119dzia s\u0142u\u017c\u0105ce do eksploracji danych (Oracle Data Mining, Weka, Rapid Miner, Project R), jak i implementuj\u0105 w\u0142asne rozwi\u0105zania przy u\u017cyciu j\u0119zyk\u00f3w SQL, PL/SQL i Java.", "num_citations": "188\n", "authors": ["1190"]}
{"title": "A practical approach to classify evolving data streams: Training with limited amount of labeled data\n", "abstract": " Recent approaches in classifying evolving data streams are based on supervised learning algorithms, which can be trained with labeled data only. Manual labeling of data is both costly and time consuming. Therefore, in a real streaming environment, where huge volumes of data appear at a high speed, labeled data may be very scarce. Thus, only a limited amount of training data may be available for building the classification models, leading to poorly trained classifiers. We apply a novel technique to overcome this problem by building a classification model from a training set having both unlabeled and a small amount of labeled instances. This model is built as micro-clusters using semi-supervised clustering technique and classification is performed with kappa-nearest neighbor algorithm. An ensemble of these models is used to classify the unlabeled data. Empirical evaluation on both synthetic data and real\u00a0\u2026", "num_citations": "184\n", "authors": ["1190"]}
{"title": "Selective and authentic third-party distribution of XML documents\n", "abstract": " Third-party architectures for data publishing over the Internet today are receiving growing attention, due to their scalability properties and to the ability of efficiently managing large number of subjects and great amount of data. In a third-party architecture, there is a distinction between the Owner and the Publisher of information. The Owner is the producer of information, whereas Publishers are responsible for managing (a portion of) the Owner information and for answering subject queries. A relevant issue in this architecture is how the Owner can ensure a secure and selective publishing of its data, even if the data are managed by a third-party, which can prune some of the nodes of the original document on the basis of subject queries and access control policies. An approach can be that of requiring the Publisher to be trusted with regard to the considered security properties. However, the serious drawback of this\u00a0\u2026", "num_citations": "176\n", "authors": ["1190"]}
{"title": "Proactive user-centric secure data scheme using attribute-based semantic access controls for mobile clouds in financial industry\n", "abstract": " As one of the most significant issues in the financial industry, customers\u2019 privacy information protection has been considered a challenging research over years. The constant emergence of the novel technologies often leads to dynamic threats from both internal and external service providers. We consider the implementations of mobile cloud-based financial services an important approach of service provisions, which also causes risks to privacy protections due to the data sharing with the unknown third parties. The data generated by mobility are usually associated with mobile users\u2019 personal privacy information. This paper addresses this issue and proposes an approach proactively protect financial customers\u2019 privacy information using Attributed-Based Access Control (ABAC) as well as data self-deterministic scheme. The proposed approach is called Proactive Dynamic Secure Data Scheme (P2DS), which aims to\u00a0\u2026", "num_citations": "174\n", "authors": ["1190"]}
{"title": "Storage and retrieval of large rdf graph using hadoop and mapreduce\n", "abstract": " Handling huge amount of data scalably is a matter of concern for a long time. Same is true for semantic web data. Current semantic web frameworks lack this ability. In this paper, we describe a framework that we built using Hadoop to store and retrieve large number of RDF triples. We describe our schema to store RDF data in Hadoop Distribute File System. We also present our algorithms to answer a SPARQL query. We make use of Hadoop\u2019s MapReduce framework to actually answer the queries. Our results reveal that we can store huge amount of semantic web data in Hadoop clusters built mostly by cheap commodity class hardware and still can answer queries fast enough. We conclude that ours is a scalable framework, able to handle large amount of RDF data efficiently.", "num_citations": "171\n", "authors": ["1190"]}
{"title": "Design of LDV: a multilevel secure relational database management\n", "abstract": " The authors describe the design of a secure database system, LDV (Lock Data Views), that builds upon the classical security policies for operating systems. LDV is hosted on the LOgical Coprocessing Kernel (LOCK) Trusted Computing Base (TCB). LDVs security policy builds on the security policy of LOCK. Its design is based on three assured pipelines for the query, update, and metadata management operations. The authors describe the security policy of LDV, its system architecture, the designs of the query processor, the update processor, the metadata manager, and the operating system issues. LDVs solutions to the inference and aggregation problems are also described.", "num_citations": "169\n", "authors": ["1190"]}
{"title": "Secure knowledge management: confidentiality, trust, and privacy\n", "abstract": " Knowledge management enhances the value of a corporation by identifying the assets and expertise as well as efficiently managing the resources. Security for knowledge management is critical as organizations have to protect their intellectual assets. Therefore, only authorized individuals must be permitted to execute various operations and functions in an organization. In this paper, secure knowledge management will be discussed, focusing on confidentiality, trust, and privacy. In particular, certain access-control techniques will be investigated, and trust management as well as privacy control for knowledge management will be explored", "num_citations": "166\n", "authors": ["1190"]}
{"title": "Web data mining and applications in business intelligence and counter-terrorism\n", "abstract": " The explosion of Web-based data has created a demand among executives and technologists for methods to identify, gather, analyze, and utilize data that may be of value to corporations and organizations. The emergence of data mining, and the larger field of Web mining, has businesses lost within a confusing maze of mechanisms and strategies for obta", "num_citations": "157\n", "authors": ["1190"]}
{"title": "Apparatus for design of a multilevel secure database management system based on a multilevel logic programming system\n", "abstract": " Apparatus for designing a multilevel secure database management system based on a multilevel logic programming system. The apparatus includes a multilevel knowledge base which has a multilevel database in which data are classified at different security levels. The multilevel knowledge base also includes schema, which describe the data in the database, and rules, which are used to deduce new data. Also included are integrity constraints, which are constraints enforced on the data, and security constraints, which are rules that assign security levels to the data. The system further includes users cleared to the different security levels for querying the multilevel database, and a multilevel logic programming system is provided for accessing the multilevel knowledge base for processing queries and for processing the integrity and security constraints. The multilevel database management system makes deductions\u00a0\u2026", "num_citations": "157\n", "authors": ["1190"]}
{"title": "Semantic web-based social network access control\n", "abstract": " The existence of online social networks that include person specific information creates interesting opportunities for various applications ranging from marketing to community organization. On the other hand, security and privacy concerns need to be addressed for creating such applications. Improving social network access control systems appears as the first step toward addressing the existing security and privacy concerns related to online social networks. To address some of the current limitations, we have created an experimental social network using synthetic data which we then use to test the efficacy of the semantic reasoning based approaches we have previously suggested.", "num_citations": "149\n", "authors": ["1190"]}
{"title": "Flow-based identification of botnet traffic by mining multiple log files\n", "abstract": " Botnet detection and disruption has been a major research topic in recent years. One effective technique for botnet detection is to identify Command and Control (C&C) traffic, which is sent from a C&C center to infected, hosts (bots) to control the bots. If this traffic can be detected, both the C&C center and the bots it controls can be detected, and the botnet can be disrupted. We propose a multiple log-file based temporal correlation technique for detecting C&C traffic. Our main assumption is that bots respond much faster than humans. By temporally correlating two host-based log files, we are able to detect this property and thereby detect bot activity in a host machine. In our experiments we apply this technique to log files produced by tcpdump and exedump, which record all incoming and outgoing network packets, and the start times of application executions at the host machine, respectively. We apply data mining to\u00a0\u2026", "num_citations": "145\n", "authors": ["1190"]}
{"title": "Effective software fault localization using an RBF neural network\n", "abstract": " We propose the application of a modified radial basis function neural network in the context of software fault localization, to assist programmers in locating bugs effectively. This neural network is trained to learn the relationship between the statement coverage information of a test case and its corresponding execution result, success or failure. The trained network is then given as input a set of virtual test cases, each covering a single statement. The output of the network, for each virtual test case, is considered to be the suspiciousness of the corresponding covered statement. A statement with a higher suspiciousness has a higher likelihood of containing a bug, and thus statements can be ranked in descending order of their suspiciousness. The ranking can then be examined one by one, starting from the top, until a bug is located. Case studies on 15 different programs were conducted, and the results clearly show that\u00a0\u2026", "num_citations": "144\n", "authors": ["1190"]}
{"title": "Least cost rumor blocking in social networks\n", "abstract": " In many real-world scenarios, social network serves as a platform for information diffusion, alongside with positive information (truth) dissemination, negative information (rumor) also spread among the public. To make the social network as a reliable medium, it is necessary to have strategies to control rumor diffusion. In this article, we address the Least Cost Rumor Blocking (LCRB) problem where rumors originate from a community Cr in the network and a notion of protectors are used to limit the bad influence of rumors. The problem can be summarized as identifying a minimal subset of individuals as initial protectors to minimize the number of people infected in neighbor communities of Cr at the end of both diffusion processes. Observing the community structure property, we pay attention to a kind of vertex set, called bridge end set, in which each node has at least one direct in-neighbor in Cr and is reachable from\u00a0\u2026", "num_citations": "138\n", "authors": ["1190"]}
{"title": "Addressing concept-evolution in concept-drifting data streams\n", "abstract": " The problem of data stream classification is challenging because of many practical aspects associated with efficient processing and temporal behavior of the stream. Two such well studied aspects are infinite length and concept-drift. Since a data stream may be considered a continuous process, which is theoretically infinite in length, it is impractical to store and use all the historical data for training. Data streams also frequently experience concept-drift as a result of changes in the underlying concepts. However, another important characteristic of data streams, namely, concept-evolution is rarely addressed in the literature. Concept-evolution occurs as a result of new classes evolving in the stream. This paper addresses concept-evolution in addition to the existing challenges of infinite-length and concept-drift. In this paper, the concept-evolution phenomenon is studied, and the insights are used to construct superior\u00a0\u2026", "num_citations": "137\n", "authors": ["1190"]}
{"title": "A primer for understanding and applying data mining\n", "abstract": " Data mining is such a hot topic that it has become an obscured buzzword. Data mining can be a powerful tool for extracting useful information from tons of data. But it can just as easily extract erroneous and useless information if it's not used correctly. Key to avoiding the pitfalls is a basic understanding of what data mining is and what things to consider in planning a data mining project. The steps in a data mining project include: integrating and cleaning or modifying the data sources, mining the data, examining and pruning the mining results, and reporting the final results.", "num_citations": "136\n", "authors": ["1190"]}
{"title": "Systems and methods for determining user attribute values by mining user network data and information\n", "abstract": " Systems and methods for determining one or more attributes and their associated values for a user, by mining the user's social network profiles. The systems and methods determine if the value of an attribute of interest for a user is specified on one or more social networking websites. If so, the systems and methods set the attribute value for the user to the specified attribute value. If not, the systems and methods retrieve, from the social networking websites, attribute values for the attribute of interest for friends of the user to form a group of possible attribute values for the user. The method, selects a value from the group of possible attribute values, and then sets the selected value as the attribute value for the user.", "num_citations": "135\n", "authors": ["1190"]}
{"title": "Jena-HBase: A distributed, scalable and efficient RDF triple store\n", "abstract": " Lack of scalability is one of the most significant problems faced by single machine RDF data stores. The advent of Cloud Computing has paved a way for a distributed ecosystem of RDF triple stores that can potentially allow up to a planet scale storage along with distributed query processing capabilities. Towards this end, we present Jena-HBase, a HBase backed triple store that can be used with the Jena framework. Jena-HBase provides end-users with a scalable storage and querying solution that supports all features from the RDF specification.", "num_citations": "130\n", "authors": ["1190"]}
{"title": "The applicability of the perturbation based privacy preserving data mining for real-world data\n", "abstract": " The perturbation method has been extensively studied for privacy preserving data mining. In this method, random noise from a known distribution is added to the privacy sensitive data before the data is sent to the data miner. Subsequently, the data miner reconstructs an approximation to the original data distribution from the perturbed data and uses the reconstructed distribution for data mining purposes. Due to the addition of noise, loss of information versus preservation of privacy is always a trade off in the perturbation based approaches. The question is, to what extent are the users willing to compromise their privacy? This is a choice that changes from individual to individual. Different individuals may have different attitudes towards privacy based on customs and cultures. Unfortunately, current perturbation based privacy preserving data mining techniques do not allow the individuals to choose their desired\u00a0\u2026", "num_citations": "128\n", "authors": ["1190"]}
{"title": "Adversarial support vector machine learning\n", "abstract": " Many learning tasks such as spam filtering and credit card fraud detection face an active adversary that tries to avoid detection. For learning problems that deal with an active adversary, it is important to model the adversary's attack strategy and develop robust learning models to mitigate the attack. These are the two objectives of this paper. We consider two attack models: a free-range attack model that permits arbitrary data corruption and a restrained attack model that anticipates more realistic attacks that a reasonable adversary would devise under penalties. We then develop optimal SVM learning strategies against the two attack models. The learning algorithms minimize the hinge loss while assuming the adversary is modifying data to maximize the loss. Experiments are performed on both artificial and real data sets. We demonstrate that optimal solutions may be overly pessimistic when the actual attacks are\u00a0\u2026", "num_citations": "123\n", "authors": ["1190"]}
{"title": "Integrating novel class detection with classification for concept-drifting data streams\n", "abstract": " In a typical data stream classification task, it is assumed that the total number of classes are fixed. This assumption may not be valid in a real streaming environment, where new classes may evolve. Traditional data stream classification techniques are not capable of recognizing novel class instances until the appearance of the novel class is manually identified, and labeled instances of that class are presented to the learning algorithm for training. The problem becomes more challenging in the presence of concept-drift, when the underlying data distribution changes over time. We propose a novel and efficient technique that can automatically detect the emergence of a novel class in the presence of concept-drift by quantifying cohesion among unlabeled test instances, and separation of the test instances from training instances. Our approach is non-parametric, meaning, it does not assume any underlying\u00a0\u2026", "num_citations": "110\n", "authors": ["1190"]}
{"title": "Data intensive query processing for large RDF graphs using cloud computing tools\n", "abstract": " Cloud computing is the newest paradigm in the IT world and hence the focus of new research. Companies hosting cloud computing services face the challenge of handling data intensive applications. Semantic web technologies can be an ideal candidate to be used together with cloud computing tools to provide a solution. These technologies have been standardized by the World Wide Web Consortium (W3C). One such standard is the Resource Description Framework (RDF). With the explosion of semantic web technologies, large RDF graphs are common place. Current frameworks do not scale for large RDF graphs. In this paper, we describe a framework that we built using Hadoop, a popular open source framework for Cloud Computing, to store and retrieve large numbers of RDF triples. We describe a scheme to store RDF data in Hadoop Distributed File System. We present an algorithm to generate the best\u00a0\u2026", "num_citations": "109\n", "authors": ["1190"]}
{"title": "Apparatus and method for the detection of security violations in multilevel secure databases\n", "abstract": " A tool for assisting an operator to detect security violations in a multilevel secure database. The database stores data classified at a plurality of security levels, where the different users of the database are cleared to different security levels and access the database through a multilevel secure database management system. Security constraints are represented in conceptual structures, rules, or frames, and operator communication with the constraint representation is provided by a user interface. An inference engine uses reasoning strategies on the security-constraint representations base to detect security violations in the database by inference.", "num_citations": "107\n", "authors": ["1190"]}
{"title": "Emerging standards for data mining\n", "abstract": " This paper presents an overview of data mining, then discusses standards (both existing and proposed) that are relevant to data mining. This includes standards that affect several stages of a data mining project. Summaries of several emerging standards are given, as well as proposals that have the potential to change the way data mining tools are built.", "num_citations": "105\n", "authors": ["1190"]}
{"title": "Security constraint processing in a multilevel secure distributed database management system\n", "abstract": " In a multilevel secure distributed database management system, users cleared at different security levels access and share a distributed database consisting of data at different sensitivity levels. An approach to assigning sensitivity levels, also called security levels, to data is one which utilizes constraints or classification rules. Security constraints provide an effective classification policy. They can be used to assign security levels to the data based on content, context, and time. We extend our previous work on security constraint processing in a centralized multilevel secure database management system by describing techniques for processing security constraints in a distributed environment during query, update, and database design operations.< >", "num_citations": "105\n", "authors": ["1190"]}
{"title": "Face recognition using multiple classifiers\n", "abstract": " In this paper, we propose a near real-time effective face recognition system for consumer applications. Since the nature of application domain requires real time result and better accuracy, it poses a serious challenge. To address this challenge, we study various classification techniques, namely, support vector machine (SVM), linear discriminant analysis (LDA) and K nearest neighbor (KNN). We observe that although KNN is as effective as SVM but KNN prohibits its usage due to high response time when data is high dimensional. To speed up KNN retrieval, we propose a feature reduction technique using principle component analysis (PCA) to facilitate near real time face recognition along with better accuracy. We apply KNN after we reduce the number of features by PCA. Hence, we test various classification approaches, namely, SVM, KNN, KNN with PCA, LDA, and LDA with PCA on a benchmark dataset and\u00a0\u2026", "num_citations": "100\n", "authors": ["1190"]}
{"title": "Differentiating code from data in x86 binaries\n", "abstract": " Robust, static disassembly is an important part of achieving high coverage for many binary code analyses, such as reverse engineering, malware analysis, reference monitor in-lining, and software fault isolation. However, one of the major difficulties current disassemblers face is differentiating code from data when they are interleaved. This paper presents a machine learning-based disassembly algorithm that segments an x86 binary into subsequences of bytes and then classifies each subsequence as code or data. The algorithm builds a language model from a set of pre-tagged binaries using a statistical data compression technique. It sequentially scans a new binary executable and sets a breaking point at each potential code-to-code and code-to-data/data-to-code transition. The classification of each segment as code or data is based on the minimum cross-entropy. Experimental results are presented to\u00a0\u2026", "num_citations": "98\n", "authors": ["1190"]}
{"title": "Efficient handling of concept drift and concept evolution over stream data\n", "abstract": " To decide if an update to a data stream classifier is necessary, existing sliding window based techniques monitor classifier performance on recent instances. If there is a significant change in classifier performance, these approaches determine a chunk boundary, and update the classifier. However, monitoring classifier performance is costly due to scarcity of labeled data. In our previous work, we presented a semi-supervised framework SAND, which uses change detection on classifier confidence to detect a concept drift. Unlike most approaches, it requires only a limited amount of labeled data to detect chunk boundaries and to update the classifier. However, SAND is expensive in terms of execution time due to exhaustive invocation of the change detection module. In this paper, we present an efficient framework, which is based on the same principle as SAND, but exploits dynamic programming and executes the\u00a0\u2026", "num_citations": "95\n", "authors": ["1190"]}
{"title": "Proactive attribute-based secure data schema for mobile cloud in financial industry\n", "abstract": " Cyber-security has become one of the most significant issues in the financial industry. New threats continuously emerge when new technologies or approaches are introduced. One security challenge is that financial customers are unaware of the hazards due to the hidden third party. We address this issue and propose a novel approach that is proactive defense means using attributed-based encryption and data self-deterministic schema to protect financial clients' privacy information. The proposed novel approach is named as Proactive Dynamic Secure Data Schema (P2DS), which is designed to ensure that the sensitive data cannot be directly used by the unanticipated party. Three algorithms have been proposed for P2DS, including Static Decryption Attribute Algorithm (SDAA), Corresponded Decryption Attribute Algorithm (CDAA), and Proactive Determinative Encryption Algorithm (PDEA). The main contributions\u00a0\u2026", "num_citations": "94\n", "authors": ["1190"]}
{"title": "Classification and novel class detection of data streams in a dynamic feature space\n", "abstract": " Data stream classification poses many challenges, most of which are not addressed by the state-of-the-art. We present DXMiner, which addresses four major challenges to data stream classification, namely, infinite length, concept-drift, concept-evolution, and feature-evolution. Data streams are assumed to be infinite in length, which necessitates single-pass incremental learning techniques. Concept-drift occurs in a data stream when the underlying concept changes over time. Most existing data stream classification techniques address only the infinite length and concept-drift problems. However, concept-evolution and feature- evolution are also major challenges, and these are ignored by most of the existing approaches. Concept-evolution occurs in the stream when novel classes arrive, and feature-evolution occurs when new features emerge in the stream. Our previous work addresses the concept\u00a0\u2026", "num_citations": "94\n", "authors": ["1190"]}
{"title": "Data mining, national security, privacy and civil liberties\n", "abstract": " In this paper, we describe the threats to privacy that can occur through data mining and then view the privacy problem as a variation of the inference problem in databases.", "num_citations": "91\n", "authors": ["1190"]}
{"title": "A scalable multi-level feature extraction technique to detect malicious executables\n", "abstract": " We present a scalable and multi-level feature extraction technique to detect malicious executables. We propose a novel combination of three different kinds of features at different levels of abstraction. These are binary n-grams, assembly instruction sequences, and Dynamic Link Library (DLL) function calls; extracted from binary executables, disassembled executables, and executable headers, respectively. We also propose an efficient and scalable feature extraction technique, and apply this technique on a large corpus of real benign and malicious executables. The above mentioned features are extracted from the corpus data and a classifier is trained, which achieves high accuracy and low false positive rate in detecting malicious executables. Our approach is knowledge-based because of several reasons. First, we apply the knowledge obtained from the binary n-gram features to extract assembly\u00a0\u2026", "num_citations": "89\n", "authors": ["1190"]}
{"title": "PP-trust-X: A system for privacy preserving trust negotiations\n", "abstract": " Trust negotiation is a promising approach for establishing trust in open systems, in which sensitive interactions may often occur between entities with no prior knowledge of each other. Although, to date several trust negotiation systems have been proposed, none of them fully address the problem of privacy preservation. Today, privacy is one of the major concerns of users when exchanging information through the Web and thus we believe that trust negotiation systems must effectively address privacy issues in order to be widely applicable. For these reasons, in this paper, we investigate privacy in the context of trust negotiations. We propose a set of privacy-preserving features for inclusion in any trust negotiation system, such as the support for the P3P standard, as well as a number of innovative features, such as a novel format for encoding digital credentials specifically designed for preserving privacy. Further, we\u00a0\u2026", "num_citations": "89\n", "authors": ["1190"]}
{"title": "XML databases and the semantic web\n", "abstract": " Efficient access to data, sharing data, extracting information from data, and making use of the information have become urgent needs for today's corporations. With so much data on the Web, managing it with conventional tools is becoming almost impossible. New tools and techniques are necessary to provide interoperability as well as warehousing betw", "num_citations": "89\n", "authors": ["1190"]}
{"title": "Security standards for the semantic web\n", "abstract": " This paper first describes the developments in standards for the semantic web and then describes standards for secure semantic web. In particular XML security, RDF security, and secure information integration and trust on the semantic web are discussed. Some details of our research on access control and dissemination of XML documents are also given. Next privacy issues for the semantic web are discussed. Finally some aspects of secure web services as well as directions for research and standards efforts for secure semantic web are provided.", "num_citations": "84\n", "authors": ["1190"]}
{"title": "Security for enterprise resource planning systems\n", "abstract": " Enterprise Resource Planning (ERP) is the technology that provides the unified business function to the organization by integrating the core processes. ERP now is experiencing the transformation that will make it highly integrated, more intelligent, more collaborative, web-enabled, and even wireless. The ERP system is becoming the system with high vulnerability and high confidentiality in which the security is critical for it to operate. Many ERP vendors have already integrated their security solution, which may work well internally; while in an open environment, we need new technical approaches to secure an ERP system. This paper introduces ERP technology from its evolution through architecture to its products. The security solution in ERP as well as directions for secure ERP systems is presented.", "num_citations": "82\n", "authors": ["1190"]}
{"title": "Detecting recurring and novel classes in concept-drifting data streams\n", "abstract": " Concept-evolution is one of the major challenges in data stream classification, which occurs when a new class evolves in the stream. This problem remains unaddressed by most state-of-the-art techniques. A recurring class is a special case of concept-evolution. This special case takes place when a class appears in the stream, then disappears for a long time, and again appears. Existing data stream classification techniques that address the concept-evolution problem, wrongly detect the recurring classes as novel class. This creates two main problems. First, much resource is wasted in detecting a recurring class as novel class, because novel class detection is much more computationally- and memory-intensive, as compared to simply recognizing an existing class. Second, when a novel class is identified, human experts are involved in collecting and labeling the instances of that class for future modeling. If a\u00a0\u2026", "num_citations": "77\n", "authors": ["1190"]}
{"title": "A language for provenance access control\n", "abstract": " Provenance is a directed acyclic graph that explains how a resource came to be in its current form. Traditional access control does not support provenance graphs. We cannot achieve all the benefits of access control if the relationships between the data and their sources are not protected. In this paper, we propose a language that complements and extends existing access control languages to support provenance. This language also provides access to data based on integrity criteria. We have also built a prototype to show that this language can be implemented effectively using Semantic Web technologies.", "num_citations": "77\n", "authors": ["1190"]}
{"title": "Stream classification with recurring and novel class detection using class-based ensemble\n", "abstract": " Concept-evolution has recently received a lot of attention in the context of mining data streams. Concept-evolution occurs when a new class evolves in the stream. Although many recent studies address this issue, most of them do not consider the scenario of recurring classes in the stream. A class is called recurring if it appears in the stream, disappears for a while, and then reappears again. Existing data stream classification techniques either misclassify the recurring class instances as another class, or falsely identify the recurring classes as novel. This increases the prediction error of the classifiers, and in some cases causes unnecessary waste in memory and computational resources. In this paper we address the recurring class issue by proposing a novel \"class-based\" ensemble technique, which substitutes the traditional \"chunk-based\" ensemble approaches and correctly distinguishes between a recurring\u00a0\u2026", "num_citations": "75\n", "authors": ["1190"]}
{"title": "Insider threat detection using stream mining and graph mining\n", "abstract": " Evidence of malicious insider activity is often buried within large data streams, such as system logs accumulated over months or years. Ensemble-based stream mining leverages multiple classification models to achieve highly accurate anomaly detection in such streams even when the stream is unbounded, evolving, and unlabeled. This makes the approach effective for identifying insider threats who attempt to conceal their activities by varying their behaviors over time. This paper applies ensemble-based stream mining, unsupervised learning, and graph-based anomaly detection to the problem of insider threat detection, demonstrating that the ensemble-based approach is significantly more effective than traditional single-model methods.", "num_citations": "75\n", "authors": ["1190"]}
{"title": "Cloud-based malware detection for evolving data streams\n", "abstract": " Data stream classification for intrusion detection poses at least three major challenges. First, these data streams are typically infinite-length, making traditional multipass learning algorithms inapplicable. Second, they exhibit significant concept-drift as attackers react and adapt to defenses. Third, for data streams that do not have any fixed feature set, such as text streams, an additional feature extraction and selection task must be performed. If the number of candidate features is too large, then traditional feature extraction techniques fail. In order to address the first two challenges, this article proposes a multipartition, multichunk ensemble classifier in which a collection of v classifiers is trained from r consecutive data chunks using v-fold partitioning of the data, yielding an ensemble of such classifiers. This multipartition, multichunk ensemble technique significantly reduces classification error compared to existing single\u00a0\u2026", "num_citations": "73\n", "authors": ["1190"]}
{"title": "Database and applications security: Integrating information security and data management\n", "abstract": " This is the first book to provide an in-depth coverage of all the developments, issues and challenges in secure databases and applications. It provides directions for data and application security, including securing emerging applications such as bioinformatics, stream information processing and peer-to-peer computing. Divided into eight sections,", "num_citations": "72\n", "authors": ["1190"]}
{"title": "Data mining tools for malware detection\n", "abstract": " Although the use of data mining for security and malware detection is quickly on the rise, most books on the subject provide high-level theoretical discussions to the near exclusion of the practical aspects. Breaking the mold, Data Mining Tools for Malware Detection provides a step-by-step breakdown of how to develop data mining tools for malware detection. Integrating theory with practical techniques and experimental results, it focuses on malware detection applications for email worms, malicious code, remote exploits, and botnets. The authors describe the systems they have designed and developed: email worm detection using data mining, a scalable multi-level feature extraction technique to detect malicious executables, detecting remote exploits using data mining, and flow-based identification of botnet traffic by mining multiple log files. For each of these tools, they detail the system architecture, algorithms, performance results, and limitations. Discusses data mining for emerging applications, including adaptable malware detection, insider threat detection, firewall policy analysis, and real-time data mining Includes four appendices that provide a firm foundation in data management, secure systems, and the semantic web Describes the authors\u2019 tools for stream data mining From algorithms to experimental results, this is one of the few books that will be equally valuable to those in industry, government, and academia. It will help technologists decide which tools to select for specific applications, managers will learn how to determine whether or not to proceed with a data mining project, and developers will find innovative alternative designs for a\u00a0\u2026", "num_citations": "71\n", "authors": ["1190"]}
{"title": "Classification and novel class detection in data streams with active mining\n", "abstract": " We present ActMiner, which addresses four major challenges to data stream classification, namely, infinite length, concept-drift, concept-evolution, and limited labeled data. Most of the existing data stream classification techniques address only the infinite length and concept-drift problems. Our previous work, MineClass, addresses the concept-evolution problem in addition to addressing the infinite length and concept-drift problems. Concept-evolution occurs in the stream when novel classes arrive. However, most of the existing data stream classification techniques, including MineClass, require that all the instances in a data stream be labeled by human experts and become available for training. This assumption is impractical, since data labeling is both time consuming and costly. Therefore, it is impossible to label a majority of the data points in a high-speed data stream. This scarcity of labeled data\u00a0\u2026", "num_citations": "71\n", "authors": ["1190"]}
{"title": "Detection and resolution of anomalies in firewall policy rules\n", "abstract": " A firewall is a system acting as an interface of a network to one or more external networks. It implements the security policy of the network by deciding which packets to let through based on rules defined by the network administrator. Any error in defining the rules may compromise the system security by letting unwanted traffic pass or blocking desired traffic. Manual definition of rules often results in a set that contains conflicting, redundant or overshadowed rules, resulting in anomalies in the policy. Manually detecting and resolving these anomalies is a critical but tedious and error prone task. Existing research on this problem have been focused on the analysis and detection of the anomalies in firewall policy. Previous works define the possible relations between rules and also define anomalies in terms of the relations and present algorithms to detect the anomalies by analyzing the rules. In this paper, we\u00a0\u2026", "num_citations": "71\n", "authors": ["1190"]}
{"title": "Managing and mining multimedia databases\n", "abstract": " There is now so much data on the Web that managing it with conventional tools is becoming almost impossible. To manage this data, provide interoperability and warehousing between multiple data sources and systems, and extract information from the databases and warehouses, various tools are being developed. In fact, developments in multimedia databa", "num_citations": "71\n", "authors": ["1190"]}
{"title": "A multi-partition multi-chunk ensemble technique to classify concept-drifting data streams\n", "abstract": " We propose a multi-partition, multi-chunk ensemble classifier based data mining technique to classify concept-drifting data streams. Existing ensemble techniques in classifying concept-drifting data streams follow a single-partition, single-chunk approach, in which a single data chunk is used to train one classifier. In our approach, we train a collection of v classifiers from r consecutive data chunks using v-fold partitioning of the data, and build an ensemble of such classifiers. By introducing this multi-partition, multi-chunk ensemble technique, we significantly reduce classification error compared to the single-partition, single-chunk ensemble approaches. We have theoretically justified the usefulness of our algorithm, and empirically proved its effectiveness over other state-of-the-art stream classification techniques on synthetic data and real botnet traffic.", "num_citations": "70\n", "authors": ["1190"]}
{"title": "Collaborative commerce and knowledge management\n", "abstract": " This paper describes collaborative commerce (c\u2010commerce); it essentially combines e\u2010commerce, knowledge management and collaboration to carry out transactions and other activities within and across organizations. We first discuss the building blocks for c\u2010commerce. Then we describe models and federated architectures. Next we analyze the strategic role of knowledge management for c\u2010commerce as well as discussing managerial and business implications. Finally, we provide directions for c\u2010commerce. Copyright \u00a9 2002 John Wiley & Sons, Ltd.", "num_citations": "70\n", "authors": ["1190"]}
{"title": "The SCIFC model for information flow control in web service composition\n", "abstract": " Existing Web service access control models focus on individual Web services, and do not consider service composition. In composite services, a major issue is information flow control. Critical information may flow from one service to another in a service chain through requests and responses and there is no mechanism for verifying that the flow complies with the access control policies. In this paper, we propose an innovative access control model to empower the services in a service chain to control the flow of their sensitive information. Our model supports information flow control through a back-check procedure and pass-on certificates. We also introduce additional factors such as the carry-along policy, security class, and transformation factor, to improve the protocol efficiency. A formal analysis is also presented to show the power and complexity of our protocol.", "num_citations": "69\n", "authors": ["1190"]}
{"title": "A framework for a video analysis tool for suspicious event detection\n", "abstract": " This paper proposes a framework to aid video analysts in detecting suspicious activity within the tremendous amounts of video data that exists in today\u2019s world of omnipresent surveillance video. Ideas and techniques for closing the semantic gap between low-level machine readable features of video data and high-level events seen by a human observer are discussed. An evaluation of the event classification and detection technique is presented and a future experiment to refine this technique is proposed. These experiments are used as a lead to a discussion on the most optimal machine learning algorithm to learn the event representation scheme proposed in this paper.", "num_citations": "69\n", "authors": ["1190"]}
{"title": "Real-time CORBA\n", "abstract": " This paper presents a survey of results in developing Real-Time CORBA, a standard for real-time management of distributed objects. This paper includes background on two areas that have been combined to realize Real-Time CORBA: the CORBA standards that have been produced by the international Object Management Group; and techniques for distributed real-time computing that have been produced in the research community. The survey describes major RT CORBA research efforts, commercial development efforts, and standardization efforts by the Object Management Group.", "num_citations": "69\n", "authors": ["1190"]}
{"title": "Predicting WWW surfing using multiple evidence combination\n", "abstract": " The improvement of many applications such as web search, latency reduction, and personalization/ recommendation systems depends on surfing prediction. Predicting user surfing paths involves tradeoffs between model complexity and predictive accuracy. In this paper, we combine two classification techniques, namely, the Markov model and Support Vector Machines (SVM), to resolve prediction using Dempster\u2019s rule. Such fusion overcomes the inability of the Markov model in predicting the unseen data as well as overcoming the problem of multiclassification in the case of SVM, especially when dealing with large number of classes. We apply feature extraction to increase the power of discrimination of SVM. In addition, during prediction we employ domain knowledge to reduce the number of classifiers for the improvement of accuracy and the reduction of prediction time. We demonstrate the effectiveness\u00a0\u2026", "num_citations": "67\n", "authors": ["1190"]}
{"title": "Towards privacy preserving access control in the cloud\n", "abstract": " It is very costly and cumbersome to manage database systems in-house especially for small or medium organizations. Data-as-a-Service (DaaS) hosted in the cloud provides an attractive solution, which is flexible, reliable, easy and economical to operate, for such organizations. However security and privacy issues concerning the storage of the data in the cloud and access via the Internet have been major concerns for many organizations. The data and the human resources are the life blood of any organization. Hence, they should be strongly protected. In this paper, we identify the challenges in securing DaaS model and propose a system called CloudMask that lays the foundation for organizations to enjoy all the benefits of hosting their data in the cloud while at the same time supporting fine-grained and flexible access control for shared data hosted in the cloud.", "num_citations": "62\n", "authors": ["1190"]}
{"title": "Privacy constraint processing in a privacy-enhanced database management system\n", "abstract": " This paper views the privacy problem as a form of inference problem. It first provides an overview of the privacy problem and then introduces the notion of privacy constraints. Next it describes architecture for a privacy-enhanced database management system and discusses algorithms for privacy constraint processing. A note on privacy constraints processing and release control are given next. Finally some directions for future research on privacy are stated.", "num_citations": "62\n", "authors": ["1190"]}
{"title": "Systems and methods for automated detection of application vulnerabilities\n", "abstract": " Disclosed are systems and methods for performing automatic, large-scale analysis mobile applications to determine and analyze application vulnerability. The disclosed systems and methods include identifying potentially vulnerable applications, identifying the application entry points that lead to vulnerable behavior, and generating smart input for text fields. Thus, a fully automated framework is implemented to run in parallel on multiple emulators, while collecting vital information.", "num_citations": "61\n", "authors": ["1190"]}
{"title": "Dynamic service and data migration in the clouds\n", "abstract": " Cloud computing is an emerging computation paradigm. To support successful cloud computing, service oriented architecture (SOA) should play a major role. Due to the nature of widely distributed service providers in clouds, the service performance could be impacted when the network traffic is congested. This can be a major barrier for tasks with real-time requirements. In clouds, this problem can be solved by migrating services to different platforms such that the communication cost can be minimized. In this paper, we consider the problem of service selection and migration in clouds. We develop a framework to facilitate service migration and design a cost model and the decision algorithm to determine the tradeoffs on service selection and migration.", "num_citations": "61\n", "authors": ["1190"]}
{"title": "Secure data objects replication in data grid\n", "abstract": " Secret sharing and erasure coding-based approaches have been used in distributed storage systems to ensure the confidentiality, integrity, and availability of critical information. To achieve performance goals in data accesses, these data fragmentation approaches can be combined with dynamic replication. In this paper, we consider data partitioning (both secret sharing and erasure coding) and dynamic replication in data grids, in which security and data access performance are critical issues. More specifically, we investigate the problem of optimal allocation of sensitive data objects that are partitioned by using secret sharing scheme or erasure coding scheme and/or replicated. The grid topology we consider consists of two layers. In the upper layer, multiple clusters form a network topology that can be represented by a general graph. The topology within each cluster is represented by a tree graph. We decompose\u00a0\u2026", "num_citations": "61\n", "authors": ["1190"]}
{"title": "Data management systems: Evolution and interoperation\n", "abstract": " As the information contained in databases has become a critical resource in organizations, efficient access to that information and the ability to share it among different users and across different systems has become an urgent need. The interoperability of heterogeneous database systems-literally, the ability to access information between or among differing types of databases, is the topic of this timely book. In the last two decades, tremendous improvements in tools and technologies have resulted in new products that provide distributed data processing capabilities. This book describes these tools and emerging technologies, explaining the essential concepts behind the topics but focusing on practical applications. Selected products are discussed to illustrate the characteristics of the different technologies. This is an ideal source for anyone who needs a broad perspective on heterogeneous database integration and related technologies.", "num_citations": "60\n", "authors": ["1190"]}
{"title": "Privacy preserving synthetic data release using deep learning\n", "abstract": " For many critical applications ranging from health care to social sciences, releasing personal data while protecting individual privacy is paramount. Over the years, data anonymization and synthetic data generation techniques have been proposed to address this challenge. Unfortunately, data anonymization approaches do not provide rigorous privacy guarantees. Although, there are existing synthetic data generation techniques that use rigorous definitions of differential privacy, to our knowledge, these techniques have not been compared extensively using different utility metrics.                 In this work, we provide two novel contributions. First, we compare existing techniques on different datasets using different utility metrics. Second, we present a novel approach that utilizes deep learning techniques coupled with an efficient analysis of privacy costs to generate differentially private synthetic datasets with\u00a0\u2026", "num_citations": "58\n", "authors": ["1190"]}
{"title": "Real-time method invocations in distributed environments\n", "abstract": " Current distributed computing environments, such as the Object Management Group's Common Object Request Broker Architecture CORBA, do not support real-time requirements. This paper presents the syntax, semantics and support for one necessary feature in a real-time distributed computing environment: timed distributed method invocations. It presents the general concept and illustrates its application as an extension to CORBA.", "num_citations": "58\n", "authors": ["1190"]}
{"title": "Security issues for federated database systems\n", "abstract": " This paper describes security issues for federated database management systems set up for managing distributed, heterogeneous and autonomous multilevel databases. It builds on our previous work in multilevel secure distributed database management systems and on the results of others' work in federated database systems. In particular, we define a multilevel secure federated database system and discuss issues on heterogeneity, autonomy, security policy and architecture.", "num_citations": "58\n", "authors": ["1190"]}
{"title": "Multi-level security in database management systems\n", "abstract": " Multi-level secure database management system (MLS-DBMS) security requirements are defined in terms of the view of the database presented to users with different authorizations. These security requirements are intended to be consistent with DoD secure computing system requirements. An informal security policy for a multi-level secure database management system is outlined, and mechanisms are introduced that support the policy. Security constraints are the mechanism for defining classification rules, and query modification is the mechanism for implementing the classification policy. These mechanisms ensure that responses to users' queries can be assigned classifications which will make them observable to the querying users.", "num_citations": "57\n", "authors": ["1190"]}
{"title": "A hybrid model to detect malicious executables\n", "abstract": " We present a hybrid data mining approach to detect malicious executables. In this approach we identify important features of the malicious and benign executables. These features are used by a classifier to learn a classification model that can distinguish between malicious and benign executables. We construct a novel combination of three different kinds of features: binary n-grams, assembly n-grams, and library function calls. Binary features are extracted from the binary executables, whereas assembly features are extracted from the disassembled executables. The function call features are extracted from the program headers. We also propose an efficient and scalable feature extraction technique. We apply our model on a large corpus of real benign and malicious executables. We extract the above mentioned features from the data and train a classifier using support vector machine. This classifier achieves a very\u00a0\u2026", "num_citations": "56\n", "authors": ["1190"]}
{"title": "Cyberphysical systems security applied to telesurgical robotics\n", "abstract": " Researchers in telesurgical robotics and security collaborated to develop the Secure ITP, a security enhancement to the Interoperable Telesurgery Protocol (ITP). The ITP defines the structure for communication between telesurgery robots and controllers and has been adopted and tested by fourteen research groups in telesurgical robotics. The Secure ITP uses open source software tools and follows guidelines in Federal Information Processing Standards (FIPS) documents published by the National Institute of Standards and Technology (NIST) to create asecurity enhancement prototype for demonstration purposes and tofacilitate the development of new security technologies which addressthe stringent requirements of telesurgery.", "num_citations": "54\n", "authors": ["1190"]}
{"title": "Data mining for security applications\n", "abstract": " In this paper we discuss various data mining techniques that we have successfully applied for cyber security. These applications include but are not limited to malicious code detection by mining binary executables, network intrusion detection by mining network traffic, anomaly detection, and data stream mining. We summarize our achievements and current works at the University of Texas at Dallas on intrusion detection, and cyber-security research.", "num_citations": "54\n", "authors": ["1190"]}
{"title": "Transforming provenance using redaction\n", "abstract": " Ongoing mutual relationships among entities rely on sharing quality information while preventing release of sensitive content. Provenance records the history of a document for ensuring both, the quality and trustworthiness; while redaction identifies and removes sensitive information from a document. Traditional redaction techniques do not extend to the directed graph representation of provenance. In this paper, we propose a graph grammar approach for rewriting redaction policies over provenance. Our rewriting procedure converts a high level specification of a redaction policy into a graph grammar rule that transforms a provenance graph into a redacted provenance graph. Our prototype shows that this approach can be effectively implemented using Semantic Web technologies.", "num_citations": "53\n", "authors": ["1190"]}
{"title": "Unsupervised incremental sequence learning for insider threat detection\n", "abstract": " Insider threat detection requires the identification of rare anomalies in contexts where evolving behaviors tend to mask such anomalies. This paper proposes and tests an incremental learning algorithm based on unsupervised learning that addresses this challenge by maintaining repetitive sequences in a compressed dictionary to identify anomaly over dynamic data streams of unbounded length. For unsupervised learning, compression-based techniques are used to model normal behavior sequences. The result is a classifier that exhibits substantially increased classification accuracy for insider threat streams relative to traditional static learning approaches and effectiveness over supervised learning approaches.", "num_citations": "52\n", "authors": ["1190"]}
{"title": "Rule-based run-time information flow control in service cloud\n", "abstract": " Service cloud provides added value to customers by allowing them to compose services from multiple providers. Most existing web service security models focus on the protection of individual web services. When multiple services from different domains are composed together, it is critical to ensure the proper information flow on the chain of services. In a service chain, each service needs to determine whether the sensitive information can be directly or indirectly disseminated to the subsequent services. Also, each service in the chain needs to decide whether to accept the data passed to it directly or indirectly from prior services. Moreover, the input data that service si receives from si-1, si. InF, may cause certain side effects inside si, such as updating si's backend database using data computed from si. InF. Service si may wish to allow such side effects in one situation while reject some side effects in another situation\u00a0\u2026", "num_citations": "52\n", "authors": ["1190"]}
{"title": "Privacy preserving decision tree mining from perturbed data\n", "abstract": " Privacy preserving data mining has been investigated extensively. The previous works mainly fall into two categories, perturbation and randomization based approaches and secure multi-party computation based approaches. The earlier perturbation and randomization approaches have a step to reconstruct the original data distribution. The new research in this area adopts different data distortion methods or modifies the data mining techniques to make it more suitable to the perturbation scenario. Secure multi-party computation approaches which employ cryptographic tools to build data mining models face high communication and computation costs, especially when the number of parties participating in the computation is large. In this paper, we propose a new perturbation based technique. In our solution, we modify the data mining algorithms so that they can be directly used on the perturbed data. In other words\u00a0\u2026", "num_citations": "52\n", "authors": ["1190"]}
{"title": "A risk management approach to RBAC\n", "abstract": " Even if Role Based Access Control (RBAC) is employed properly, distributed database environments are still prone to illegitimate access attempts: in RBAC, users potentially carry the risk of illegal access attempts via credentials violation, or unintentional/intentional incorrect use of already granted permissions via role misuse/abuse. We introduce a probabilistic risk management model for enhanced access control in such databases. We incorporate failure modes and effects analysis (FMEA) scheme for measuring user risks in our design. We combine components as credentials, queries, role history logs and expected utility for a probabilistic formulation of risk. We present experimental results that we obtained on real world database. The results emphasize the need for a database where roles are well defined and queries under different roles do not overlap. We suggest using query templates with minimized role\u00a0\u2026", "num_citations": "52\n", "authors": ["1190"]}
{"title": "Design and implementation of a database inference controller\n", "abstract": " The Inference Problem compromises database systems which are usually considered to be secure. here, users pose sets of queries and infer unauthorized information from the responses that they obtain. An Inference Controller is a device that prevents and/or detects security violations via inference. We are particularly interested in the inference problem which occurs in a multilevel operating environment. In such an environment, the users are cleared at different security levels and they access a multilevel database where the data is classified at different sensitivity levels. A multilevel secure database management system (MLS/DBMS) manages a multilevel database where its users cannot access data to which they are not authorized. However, providing a solution to the inference problem, where users issue multiple requests and consequently infer unauthorized knowledge is beyond the capability of currently\u00a0\u2026", "num_citations": "52\n", "authors": ["1190"]}
{"title": "Systems and methods for detecting a novel data class\n", "abstract": " Systems and methods for data classification and novel data class detection are provided. In one illustrative embodiment, a system or method for detecting a novel class includes receiving a data stream comprising a plurality of data points, and identifying a set of filtered outliers, in the plurality of data points, that are outside of a decision boundary. A cohesion and a separation for the set of filtered outliers may be determined. A novel class may be detected using the cohesion and the separation of the set of filtered outliers, and the novel class may include the set of filtered outliers.", "num_citations": "51\n", "authors": ["1190"]}
{"title": "Supervised learning for insider threat detection using stream mining\n", "abstract": " Insider threat detection requires the identification of rare anomalies in contexts where evolving behaviors tend to mask such anomalies. This paper proposes and tests an ensemble-based stream mining algorithm based on supervised learning that addresses this challenge by maintaining an evolving collection of multiple models to classify dynamic data streams of unbounded length. The result is a classifier that exhibits substantially increased classification accuracy for real insider threat streams relative to traditional supervised learning (traditional SVM and one-class SVM) and other single-model approaches.", "num_citations": "51\n", "authors": ["1190"]}
{"title": "Multimedia Database Systems: design and implementation strategies\n", "abstract": " Multimedia Database Systems: Design and Implementation Strategies is a compendium of the state-of-the-art research and development work pertaining to the problems and issues in the design and development of multimedia database systems. The chapters in the book are developed from presentations given at previous meetings of the International Workshop on Multi-Media Data Base Management Systems (IW-MMDBMS), and address the following issues: development of adequate multimedia database models, design of multimedia database query and retrieval languages, design of indexing and organization techniques, development of efficient and reliable storage models, development of efficient and dependable retrieval and delivery strategies, and development of flexible, adaptive, and reliable presentation techniques.", "num_citations": "50\n", "authors": ["1190"]}
{"title": "Building trustworthy semantic webs\n", "abstract": " Semantic Webs promise to revolutionize the way computers find and integrate data over the internet. They will allow Web agents to share and reuse data across applications, enterprises, and community boundaries. However, this improved accessibility poses a greater threat of unauthorized access, which could lead to the malicious corruption of informa", "num_citations": "49\n", "authors": ["1190"]}
{"title": "Spark-based anomaly detection over multi-source VMware performance data in real-time\n", "abstract": " Anomaly detection refers to identifying the patterns in data that deviate from expected behavior. These non-conforming patterns are often termed as outliers, malwares, anomalies or exceptions in different application domains. This paper presents a novel, generic real-time distributed anomaly detection framework for multi-source stream data. As a case study, we have decided to detect anomaly for multi-source VMware-based cloud data center. The framework monitors VMware performance stream data (e.g., CPU load, memory usage, etc.) continuously. It collects these data simultaneously from all the VMwares connected to the network. It notifies the resource manager to reschedule its resources dynamically when it identifies any abnormal behavior of its collected data. We have used Apache Spark, a distributed framework for processing performance stream data and making prediction without any delay. Spark is\u00a0\u2026", "num_citations": "48\n", "authors": ["1190"]}
{"title": "Big data security and privacy\n", "abstract": " This paper describes the issues surrounding big data security and privacy and provides a summary of the National Science Foundation sponsored workshop on this topic held in Dallas, Texas on September 16-17, 2014. Our goal is to build a community in big data security and privacy to explore the challenging research problems.", "num_citations": "47\n", "authors": ["1190"]}
{"title": "Role-based integrated access control and data provenance for SOA based net-centric systems\n", "abstract": " In multi-domain service-based systems, services from different domains are composed together to accomplish critical tasks. In these systems, data flow from one domain to another through the composed services. Thus, security and trustworthiness are the major concerns. Many access control models have been developed for service-based systems. Also, many data provenance schemes have been proposed in recent years to support data quality assessment and enhancement, data reproduction, etc. However, none of the existing mechanisms consider both access control and data provenance in an integrated model. In this paper, we propose an integrated role-based access control and data provenance model to secure the cross-domain interactions. We develop a role-based data provenance scheme which tracks the roles of originators/contributors of a data object and uses this information to help evaluate data\u00a0\u2026", "num_citations": "45\n", "authors": ["1190"]}
{"title": "Security and privacy for geospatial data: concepts and research directions\n", "abstract": " Geospatial data play a key role in a wide spectrum of critical data management applications, such as disaster and emergency management, environmental monitoring, land and city planning, and military operations, often requiring the coordination among diverse organizations, their data repositories, and users with different responsibilities. Although a variety of models and techniques are available to manage, access and share geospatial data, very little attention has been paid to addressing security concerns, such as access control, security and privacy policies, and the development of secure and in particular interoperable GIS applications. The objective of this paper is to discuss the technical challenges raised by the unique requirements of secure geospatial data management and to suggest a comprehensive framework for security and privacy for geospatial data and GIS. Such a framework is the first coherent\u00a0\u2026", "num_citations": "45\n", "authors": ["1190"]}
{"title": "Securing data analytics on sgx with randomization\n", "abstract": " Protection of data privacy and prevention of unwarranted information disclosure is an enduring challenge in cloud computing when data analytics is performed on an untrusted third-party resource. Recent advances in trusted processor technology, such as Intel SGX, have rejuvenated the efforts of performing data analytics on a shared platform where data security and trustworthiness of computations are ensured by the hardware. However, a powerful adversary may still be able to infer private information in this setting from side channels such as cache access, CPU usage and other timing channels, thereby threatening data and user privacy. Though studies have proposed techniques to hide such information leaks through carefully designed data-independent access paths, such techniques can be prohibitively slow on models with large number of parameters, especially when employed in a real-time\u00a0\u2026", "num_citations": "44\n", "authors": ["1190"]}
{"title": "Statistical technique for online anomaly detection using spark over heterogeneous data from multi-source vmware performance data\n", "abstract": " Anomaly detection refers to the identification of patterns in a dataset that do not conform to expected patterns. Depending on the domain, the non-conformant patterns are assigned various tags, e.g. anomalies, outliers, exceptions, malwares and so forth. Online anomaly detection aims to detect anomalies in data flowing in a streaming fashion. Such stream data is commonplace in today's cloud data centers that house a large array of virtual machines(VM) producing vast amounts of performance data in real-time. Sophisticated detection mechanism will likely entail collation of data from heterogeneous sources with diversified data format and semantics. Therefore, detection of performance anomaly in this context requires a distributed framework with high throughput and low latency. Apache Spark is one such framework that represents the bleeding-edge amongst its contemporaries. In this paper, we have taken up the\u00a0\u2026", "num_citations": "43\n", "authors": ["1190"]}
{"title": "Standards for secure data sharing across organizations\n", "abstract": " This paper discusses standards-based approaches for secure data sharing across organizations. In particular, current standards as well as standardization trends for data integration, multimedia data management, active real-time data management, data warehousing and mining, expert data management, semantic web data management, knowledge management, visualization, metadata extraction and management, and security management for data sharing are discussed. We will illustrate the ideas with an example from emergency response and public health awareness application domain.", "num_citations": "43\n", "authors": ["1190"]}
{"title": "Secure sensor information management and mining\n", "abstract": " This article describes issues and challenges for secure sensor information management. In particular, we discuss data management for sensor information systems including stream data management, distributed data management for sensor data, sensor information management including mining sensor data, security for sensor databases, and dependable sensor information management such as tradeoffs between security, real-time processing, and fault tolerance. Finally we discuss object-based infrastructures for sensor systems as well as directions for sensor information management.", "num_citations": "43\n", "authors": ["1190"]}
{"title": "Security issues for the semantic web\n", "abstract": " This paper first describes the developments in semantic Web and then provides an overview of secure semantic Web. In particular XML security, RDF security, and secure information integration and trust on the semantic Seb are discussed. Finally directions for research on secure semantic Web are provided.", "num_citations": "43\n", "authors": ["1190"]}
{"title": "A roadmap for privacy-enhanced secure data provenance\n", "abstract": " The notion of data provenance was formally introduced a decade ago and has since been investigated, but mainly from a functional perspective, which follows the historical pattern of introducing new technologies with the expectation that security and privacy can be added later. Despite very recent interests from the cyber security community on some specific aspects of data provenance, there is no long-haul, overarching, systematic framework for the security and privacy of provenance. The importance of secure provenance R&D has been emphasized in the recent report on Federal game-changing R&D for cyber security especially with respect to the theme of Tailored Trustworthy Spaces. Secure data provenance can significantly enhance data trustworthiness, which is crucial to various decision-making processes. Moreover, data provenance can facilitate accountability and compliance (including\u00a0\u2026", "num_citations": "42\n", "authors": ["1190"]}
{"title": "An analysis of user influence ranking algorithms on dark web forums\n", "abstract": " Social media is actively utilized by extremists to spread out their ideologies. While the Internet provides a platform for any users around the world to share their opinions, some opinions in social media can be related to the national security and threatening to others. Given the large volume and exponential growing rate of messages on the social media platforms, it is impossible to analyze the messages by manual effort. An effective way to identify the threat through social media is detecting the influential users automatically. Bu identifying the influential users, we can determine the impact and the neighborhood of these users. In this work, we develop weights to incorporate message content similarity and response immediacy to measure the degree of influence between any two users on a social networking site and integrate the weights with the typical link analysis techniques. In our experiment, we investigate the\u00a0\u2026", "num_citations": "42\n", "authors": ["1190"]}
{"title": "Exploiting an antivirus interface\n", "abstract": " We propose a technique for defeating signature-based malware detectors by exploiting information disclosed by antivirus interfaces. This information is leveraged to reverse engineer relevant details of the detector's underlying signature database, revealing binary obfuscations that suffice to conceal malware from the detector. Experiments with real malware and antivirus interfaces on Windows operating systems justify the effectiveness of our approach.", "num_citations": "42\n", "authors": ["1190"]}
{"title": "Emergency response applications: Dynamic plume modeling and real-time routing\n", "abstract": " In a crisis situation, toxic gas can be released into the air, blocking routes for emergency responders. Rescue workers must be able to compute the shortest and safest paths in the presence of toxic gas dispersions that move dynamically with changing wind speed and direction. To model this, the authors developed weather retriever software, which fetches weather details about a particular location from the Internet and acts as the station for atmospheric measurements for the Aloha plume-modeling software. The authors also explored options for displaying this dynamic plume on a geographic map.", "num_citations": "42\n", "authors": ["1190"]}
{"title": "Policy-driven service composition with information flow control\n", "abstract": " Ensuring secure information flow is a critical task for service composition in multi-domain systems. Research in security-aware service composition provides some preliminary solutions to this problem, but there are still issues to be addressed. In this paper, we develop a service composition mechanism specifically focusing on the secure information flow control issues. We first introduce a general model for information flow control in service chains, considering the transformation factors of services and security classes of data resources in a service chain. Then, we develop general rules to guide service composition satisfying secure information flow requirements. Finally, to achieve efficient service composition, we develop a three-phase protocol to allow rapid filtering of candidate compositions that are unlikely to satisfy the information flow constraints and thorough evaluation of highly promising candidates. Our\u00a0\u2026", "num_citations": "41\n", "authors": ["1190"]}
{"title": "Enhancing security modeling for web services using delegation and pass-on\n", "abstract": " In recent years, security issues in web service environments have been widely studied and various security standards and models have been proposed. However, most of these standards and models focus on individual web services and do not consider the security issues in composite services. In this article, the authors propose an enhanced security model to control the information flow in service chains. It extends the basic web service security models by introducing the concepts of delegation and pass-on. Based on these concepts, new certificates, certificate chains, delegation and pass-on policies, and how they are used to control the information flow are discussed. The authors also introduce a case study from a healthcare information system to illustrate the protocols.", "num_citations": "40\n", "authors": ["1190"]}
{"title": "Information survivability for evolvable and adaptable real-time command and control systems\n", "abstract": " MITRE's Evolvable Real-Time C3 (command, control and communications) project has developed an approach that would enable current real-time systems to evolve into the systems of the future. This paper first summarizes the design and implementation of an infrastructure for an evolvable real-time C3 system. Then, a detailed discussion of the infrastructure requirements for a survivable real-time C3 system is presented. Finally, security issues for survivability, as well as open implementation of the infrastructure are described. In particular, adaptable middleware for survivable systems is discussed.", "num_citations": "40\n", "authors": ["1190"]}
{"title": "Adaptive encrypted traffic fingerprinting with bi-directional dependence\n", "abstract": " Recently, network traffic analysis has been increasingly used in various applications including security, targeted advertisements, and network management. However, data encryption performed on network traffic poses a challenge to these analysis techniques. In this paper, we present a novel method to extract characteristics from encrypted traffic by utilizing data dependencies that occur over sequential transmissions of network packets. Furthermore, we explore the temporal nature of encrypted traffic and introduce an adaptive model that considers changes in data content over time. We evaluate our analysis on two packet encrypted applications: website fingerprinting and mobile application (app) fingerprinting. Our evaluation shows how the proposed approach outperforms previous works especially in the open-world scenario and when defense mechanisms are considered.", "num_citations": "39\n", "authors": ["1190"]}
{"title": "Secure data storage and retrieval in the cloud\n", "abstract": " With the advent of the World Wide Web and the emergence of e-commerce applications and social networks, organizations across the world generate a large amount of data daily. This data would be more useful to cooperating organizations if they were able to share their data. Two major obstacles to this process of data sharing are providing a common storage space and secure access to the shared data. In this paper we address these issues by combining cloud computing technologies such as Hive and Hadoop with XACML policy based security mechanisms that provide fine-grained access to resources. We further present a web-based application that uses this combination and allows collaborating organizations to securely store and retrieve large amounts of data.", "num_citations": "39\n", "authors": ["1190"]}
{"title": "Security-aware service composition with fine-grained information flow control\n", "abstract": " Enforcing access control in composite services is essential in distributed multidomain environment. Many advanced access control models have been developed to secure web services at execution time. However, they do not consider access control validation at composition time, resulting in high execution-time failure rate of composite services due to access control violations. Performing composition-time access control validation is not straightforward. First, many candidate compositions need to be considered and validating them can be costly. Second, some service composers may not be trusted to access protected policies and validation has to be done remotely. Another major issue with existing models is that they do not consider information flow control in composite services, which may result in undesirable information leakage. To resolve all these problems, we develop a novel three-phase composition\u00a0\u2026", "num_citations": "38\n", "authors": ["1190"]}
{"title": "Extended RBAC-based design and implementation for a secure data warehouse\n", "abstract": " This paper first discusses security issues for data warehousing. In particular, issues on building a secure data warehouse, secure data warehousing technologies as well as design issues are discussed. Our design of a secure data warehouse that enforces an Extended RBAC Policy is described next. This will be followed by a discussion of privacy issues for data warehousing. Finally directions for secure data warehouses are discussed.", "num_citations": "38\n", "authors": ["1190"]}
{"title": "Web data management and electronic commerce\n", "abstract": " Effective electronic commerce requires integrating resources and extracting the critical information from across Web sites. From the recent efforts to develop tools for interoperability and warehousing between scattered information on the web emerged the new discipline of web data management, and this book, Web Data Management and Electronic Commerce. The first of its kind, it combines data management and mining, object technology, electronic commerce, Java, and the Internet into a complete overview of the concepts and developments in this new field. It details technologies in security, multimedia data management techniques, and real-time processing and discusses the emerging standards of Java Database Connectivity, XML, metadata, and middleware. A simple Web site isn't good enough anymore To remain competitive, you need Internet capabilities that allow you and your customers to buy, sell, and advertise. Even if you are unfamiliar with e-commerce, this self-contained volume provides the background you need to understand it through appendices that explain data management, Internet, security, and object technology. Approachable enough for the beginner and complete enough for the expert, Web Data Management and Electronic Commerce helps you to manage information effectively and efficiently.", "num_citations": "38\n", "authors": ["1190"]}
{"title": "An effective evidence theory based k-nearest neighbor (knn) classification\n", "abstract": " In this paper, we study various K nearest neighbor (KNN) algorithms and present a new KNN algorithm based on evidence theory. We introduce global frequency estimation of prior probability (GE) and local frequency estimation of prior probability (LE). A GE for a class is the prior probability of the class across the whole training data space based on frequency estimation; on the other hand, a LE for a class in a particular neighborhood is the prior probability of the class in this neighborhood space based on frequency estimation. By considering the difference between the GE and the LE of each class, we present a solution to the imbalanced data problem in some degree without doing re-sampling. We compare our algorithm with other KNN algorithms using two benchmark datasets. Results show that our KNN algorithm outperforms other KNN algorithms, including basic evidence based KNN.", "num_citations": "37\n", "authors": ["1190"]}
{"title": "Towards a multilevel secure database management system for real-time applications\n", "abstract": " Database systems for real-time applications must satisfy timing constraints associated with transactions, in addition to maintaining data consistency. In addition to real-time requirements, security is usually required in many applications, because sensitive information must be safeguarded. Multilevel security requirements introduce a new dimension to transaction processing in real-time database systems. The paper addresses issues that must be investigated in order to design and develop a multilevel secure database management system for real-time applications.< >", "num_citations": "37\n", "authors": ["1190"]}
{"title": "The use of conceptual structures for handling the inference problem\n", "abstract": " The Use of Conceptual Structures for Handling the Inference Problem | Results of the IFIP WG 11.3 Workshop on Database Security V: Status and Prospects ACM Digital Library home ACM home Google, Inc. (search) Advanced Search Browse About Sign in Register Advanced Search Journals Magazines Proceedings Books SIGs Conferences People More Search ACM Digital Library SearchSearch Advanced Search Browse Browse Digital Library Collections More HomeBrowse by TitleProceedingsResults of the IFIP WG 11.3 Workshop on Database Security V: Status and ProspectsThe Use of Conceptual Structures for Handling the Inference Problem ARTICLE The Use of Conceptual Structures for Handling the Inference Problem Share on Author: Bhavani M Thuraisingham profile image Bhavani M. Thuraisingham View Profile Authors Info & Affiliations Publication: Results of the IFIP WG 11.3 Workshop on \u2026", "num_citations": "37\n", "authors": ["1190"]}
{"title": "Scrub-tcpdump: A multi-level packet anonymizer demonstrating privacy/analysis tradeoffs\n", "abstract": " To promote sharing of packet traces across security domains we introduce SCRUB-tcpdump, a tool that adds multi-field multi-option anonymization to tcpdump functionality. Experimental results show how SCRUB-tcpdump provides flexibility to balance the often conflicting requirements for privacy protection versus security analysis. Specifically, we demonstrate with empirical experimentation how different SCRUB-tcpdump anonymization options applied to the same data set can result in different levels of privacy protection and security analysis. Based on these results we propose that optimal network data sharing needs to have different levels of anonymization tailored to the participating organizations in order to tradeoff the risks of potential loss or disclosure of sensitive information.", "num_citations": "36\n", "authors": ["1190"]}
{"title": "Security and privacy for multimedia database management systems\n", "abstract": " This paper describes security and privacy issues for multimedia database management systems. Multimedia data includes text, images, audio and video. It describes access control for multimedia database management systems and describes security policies and security architectures for such systems. Privacy problems that result from multimedia data mining are also discussed.", "num_citations": "35\n", "authors": ["1190"]}
{"title": "Privacy-preserving data mining: Development and directions\n", "abstract": " This article first describes the privacy concerns that arise due to data mining, especially for national security applications. Then we discuss privacy-preserving data mining. In particular, we view the privacy problem as a form of inference problem and introduce the notion of privacy constraints. We also describe an approach for privacy constraint processing and discuss its relationship to privacy-preserving data mining. Then we give an overview of the developments on privacy-preserving data mining that attempt to maintain privacy and at the same time extract useful information from data mining. Finally, some directions for future research on privacy as related to data mining are given.", "num_citations": "35\n", "authors": ["1190"]}
{"title": "Data mining for counter-terrorism\n", "abstract": " Data mining is becoming a useful tool for detecting and preventing terrorism. This paper first discusses some technical challenges for data mining as applied for counterterrorism applications. Next it provides an overview of the various types of terrorist threats and describes how data mining techniques could provide solutions to counterterrorism. Finally some privacy concerns and potential solutions that could detect terrorist activities and yet attempt to maintain privacy will be discussed.", "num_citations": "35\n", "authors": ["1190"]}
{"title": "An adaptive policy for improved timeliness in secure database systems\n", "abstract": " Database systems for real-time applications must satisfy timing constraints associated with transactions, in addition to maintaining data consistency. In addition to real-time requirements, security is usually required in many applications. Multilevel security requirements introduce a new dimension to transaction processing in real-time database systems. In this paper, we argue that because of the complexities involved, tradeoffs need to be made between security and timeliness. We first describe a secure two-phase locking protocol. The protocol is then modified to support an adaptive method of trading off security for timeliness, depending on the current state of the system. The performance of the Adaptive 2PL protocol is evaluated for a spectrum of security-factor values ranging from fully secure (1.0) right upto fully real-time (0.0).", "num_citations": "35\n", "authors": ["1190"]}
{"title": "Efficient influence spread estimation for influence maximization under the linear threshold model\n", "abstract": " This paper investigates the influence maximization (IM) problem in social networks under the linear threshold (LT) model. Kempe et al. (ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pp. 137\u2013146, 2003) showed that the standard greedy algorithm, which selects the node with the maximum marginal gain repeatedly, brings a                                                                                                                                                      -factor approximation solution to this problem. However, Chen et al. (International Conference on Data Mining, pp. 88\u201397, 2010) proved that the problem of computing the expected influence spread (EIS) of a node is #P-hard. Therefore, to compute the marginal gain exactly is computational intractable. We step-up on investigating efficient algorithm to compute EIS. We show that the EIS of a node can be computed by finding cycles through it, and we further develop an exact algorithm to compute EIS within a small number of hops and an approximation algorithm to estimate EIS without the hop constraint. Based on the proposed EIS algorithms, we finally develop an efficient greedy based algorithm for IM. We compare our algorithm with some well-known IM algorithms on four real-world social networks. The experimental results show that our algorithm is more accurate than others in finding the most influential nodes, and it is also better than or competitive with them in terms of running time. IM is a big topic in social network analysis. In this paper, we investigate efficient influence spread estimation for IM under the LT model. We develop two influence spread estimation algorithms and a new greedy based algorithm for IM\u00a0\u2026", "num_citations": "34\n", "authors": ["1190"]}
{"title": "Cyber Operations\u2013Bridging from Concept to Cyber Superiority\n", "abstract": " Key to success in cyber warfare is the ability to transpose innovative strategic theories to methodologies, tools, and implementation. Drawing on the analogy with the dawn of armored warfare, with reflection over the French and English military disaster of France 1940, where the Allied had the best ideas but Germans utilized the ideas, a comparison is made with the contemporary DOD pursuit of cyber superiority. The United States has world-leading creativity for cutting-edge cyber concepts and new strategies, but those who should transfer these ideas to cyber weapons, cyber tactics, and workforce training are over-emphasizing passive information assurance that is similar to positional warfare. A survey presents a misalignment between and what is researched and educated in the nation\u2019s cyber security research centers and the DOD overarching goals and doctrine. Unless corrected, the misalignment creates a national security risk when these innovative ideas instead can be put to use by our adversaries.", "num_citations": "34\n", "authors": ["1190"]}
{"title": "Risk-aware workload distribution in hybrid clouds\n", "abstract": " This paper explores an efficient and secure mechanism to partition computations across public and private machines in a hybrid cloud setting. We propose a principled framework for distributing data and processing in a hybrid cloud that meets the conflicting goals of performance, sensitive data disclosure risk and resource allocation costs. The proposed solution is implemented as an add-on tool for a Hadoop and Hive based cloud computing infrastructure. Our experiments demonstrate that the developed mechanism can lead to a major performance gain by exploiting both the hybrid cloud components without violating any pre-determined public cloud usage constraints.", "num_citations": "34\n", "authors": ["1190"]}
{"title": "Data security services, solutions and standards for outsourcing\n", "abstract": " Globalization has resulted in outsourcing data, software, hardware and various services. However, outsourcing introduces new security vulnerabilities due to the corporation's limited knowledge and control of external providers operating in foreign countries. Security of operation is therefore critical for effectively introducing and maintaining these business relationships without sacrificing product quality. This paper discusses some of these security concerns for outsourcing. In particular, it discusses security issues pertaining to data-as-a-service and software-as-a-service models as well as supply chain security issues. Relevant standards for data outsourcing are also presented. The goal is for the composite system to be secure even if the individual components that are developed by multiple organizations might be compromised.", "num_citations": "33\n", "authors": ["1190"]}
{"title": "Data mining for malicious code detection and security applications\n", "abstract": " Data mining is the process of posing queries and extracting patterns, often previously unknown from large quantities of data using pattern matching or other reasoning techniques. Data mining has many applications in security including for national security as well as for cyber security. The threats to national security include attacking buildings, destroying critical infrastructures such as power grids and telecommunication systems. Data mining techniques are being investigated to find out who the suspicious people are and who is capable of carrying out terrorist activities. Cyber security is involved with protecting the computer and network systems against corruption due to Trojan horses, worms and viruses. Data mining is also being applied to provide solutions such as intrusion detection and auditing. The first part of the presentation will discuss my joint research with Prof. Latifur Khan and our students at the\u00a0\u2026", "num_citations": "33\n", "authors": ["1190"]}
{"title": "Scheduling and priority mapping for static real-time middleware\n", "abstract": " This paper presents a middleware real-time scheduling technique for static, distributed, real-time applications. The technique uses global deadline monotonic priority assignment to clients and the Distributed Priority Ceiling protocol to provide concurrency control and priorities for server execution. The paper presents a new algorithm for mapping the potentially large number of unique global priorities required by this scheduling technique to the restricted set of priorities provided by commercial real-time operating systems. This algorithm is called Lowest Overlap First Priority Mapping; we prove that it is optimal among direct priority mapping algorithms. This paper also presents the implementation of these real-time middleware scheduling techniques in a Scheduling Service that meets the interface proposed for such a service in the Real-Time CORBA 1.0 standard. Our prototype Scheduling Service is integrated\u00a0\u2026", "num_citations": "33\n", "authors": ["1190"]}
{"title": "Social network classification incorporating link type values\n", "abstract": " Classification of nodes in a social network and its applications to security informatics have been extensively studied in the past. However, previous work generally does not consider the types of links (e.g., whether a person is friend or a close friend) that connect social networks members for classification purposes. Here, we propose modified Naive Bayes Classification schemes to make use of the link type information in classification tasks. Basically, we suggest two new Bayesian classification methods that extend a traditional relational Naive Bayes Classifier, namely, the Link Type relational Bayes Classifier and the Weighted Link Type Bayes Classifier. We then show the efficacy of our proposed techniques by conducting experiments on data obtained from the Internet Movie Database.", "num_citations": "32\n", "authors": ["1190"]}
{"title": "Directions for Web and e-commerce applications security\n", "abstract": " This paper provides directions for Web and e-commerce application security. In particular, access control policies, workflow security, XML security and federated database security issues pertaining to the Web and e-commerce applications are discussed.", "num_citations": "32\n", "authors": ["1190"]}
{"title": "Parallel processing and trusted database management systems\n", "abstract": " This paper applies parallel processing technology to database security technology and vice versa. We first describe the issues involved in incorporating multilevel security into parallel database management systems. In particular, we describe how multilevel security could be incorporated into the GAMMA architecture. Then we describe the use of parallel architectures to perform trusted database management system functions. In particular, we describe security constraint processing in trusted database management systems and show how parallel processing could enhance the performance of this function.", "num_citations": "32\n", "authors": ["1190"]}
{"title": "Towards cyber operations-The new role of academic cyber security research and education\n", "abstract": " The shift towards cyber operations represents a shift not only for the defense establishments worldwide but also cyber security research and education. Traditionally cyber security research and education has been founded on information assurance, expressed in underlying subfields such as forensics, network security, and penetration testing. Cyber security research and education is connected to the homeland security agencies and defense through funding, mutual interest in the outcome of the research, and the potential job market for graduates. The future of cyber security is both defensive information assurance measures and active defense driven information operations that jointly and coordinately are launched, in the pursuit of a cohesive and decisive execution of the national cyber defense strategy. The cohesive cyber defense requires universities to optimize their campus wide resources to fuse knowledge\u00a0\u2026", "num_citations": "31\n", "authors": ["1190"]}
{"title": "R2D: A bridge between the semantic web and relational visualization tools\n", "abstract": " The widespread deployment of Resource Description Framework has resulted in the emergence of a new data storage paradigm, the RDF Graph Model, which, in turn, requires a rich suite of modeling and visualization tools to aid with data management. This paper presents R2D (RDF-to-Database), an effort whose goal is to enable reusability of relational tools on RDF data. R2D aims to transform RDF data, at run-time, into an equivalent normalized relational schema, thereby bridging the gap between RDF and RDBMS concepts and making the abundance of existing relational tools available to RDF Stores. The work in this paper extends our earlier work by including the ability to map blank nodes, which are used to represent complex relationships between entities, and to perform pattern matching and aggregation functions on data. The R2D system architecture and mapping constructs, with particular emphasis on\u00a0\u2026", "num_citations": "31\n", "authors": ["1190"]}
{"title": "Design and implementation of data mining tools\n", "abstract": " Focusing on three applications of data mining, Design and Implementation of Data Mining Tools explains how to create and employ systems and tools for intrusion detection, Web page surfing prediction, and image classification. Mainly based on the authors' own research work, the book takes a practical approach to the subject. The first part of the boo", "num_citations": "31\n", "authors": ["1190"]}
{"title": "Peer to peer botnet detection for cyber-security: A data mining approach\n", "abstract": " Botnet is a network of compromised hosts or bots, under the control of a human attacker known as the botmaster [7, 8]. Botnets are used to perform malicious actions, such as launching DDoS attacks, sending spam or phishing emails and so on. Thus, botnets have emerged as a threat to internet community. Peer to Peer (P2P) is a relatively new architecture of botnets [4]. These botnets are distributed, and small. So, they are difficult to locate and destroy. Most of the recent works in P2P botnet are in the analysis phase [4, 5, 6]. On the contrary, our work is aimed at detecting P2P botnets using network traffic mining.", "num_citations": "30\n", "authors": ["1190"]}
{"title": "MOMT: A multilevel object modeling technique for designing secure database applications\n", "abstract": " This article describes a methodology called the Multilevel Object Modeling Technique (MOMT) for designing multilevel secure database applications. MOMT adapts the Object Modeling Technique methodology for this purpose. In particular, a multilevel object model, multilevel dynamic model, and multilevel functional model are described.", "num_citations": "30\n", "authors": ["1190"]}
{"title": "Towards the design of a secure data/knowledge 11 base management system\n", "abstract": " This paper describes the notion of Multilevel Secure Database Management System (MLS/DBMS) and the difficulties encountered in designing such a system. It then states a security policy for an MLS/DBMS and describes a design for query and update operation. Finally, the notion of a Multilevel Secure Knowledge Base Management System (MLS/KBMS) and a design for query operation in an MLS/KBMS is discussed.", "num_citations": "30\n", "authors": ["1190"]}
{"title": "Cloud guided stream classification using class-based ensemble\n", "abstract": " We propose a novel class-based micro-classifier ensemble classification technique (MCE) for classifying data streams. Traditional ensemble-based data stream classification techniques build a classification model from each data chunk and keep an ensemble of such models. Due to the fixed length of the ensemble, when a new model is trained, one existing model is discarded. This creates several problems. First, if a class disappears from the stream and reappears after a long time, it would be misclassified if a majority of the classifiers in the ensemble does not contain any model of that class. Second, discarding a model means discarding the corresponding data chunk completely. However, knowledge obtained from some classes might be still useful and if they are discarded, the overall error rate would increase. To address these problems, we propose an ensemble model where each class information is stored\u00a0\u2026", "num_citations": "29\n", "authors": ["1190"]}
{"title": "Ontology-driven query expansion using map/reduce framework to facilitate federated queries\n", "abstract": " In view of the need for a highly distributed and federated architecture, a robust query expansion has great impact on the performance of information retrieval. We aim to determine ontology-driven query expansion terms using different weighting techniques. For this, we consider each individual ontology and user query keywords to determine the Basic Expansion Terms (BET) using a number of semantic measures including Betweenness Measure (BM) and Semantic Similarity Measure (SSM). We propose a Map/Reduce distributed algorithm for calculating all the shortest paths in ontology graph. Map/Reduce algorithm will improve considerably the efficiency of BET calculation for large ontologies.", "num_citations": "29\n", "authors": ["1190"]}
{"title": "A data driven approach for the science of cyber security: Challenges and directions\n", "abstract": " This paper describes a data driven approach to studying the science of cyber security (SoS). It argues that science is driven by data. It then describes issues and approaches towards the following three aspects: (i) Data Driven Science for Attack Detection and Mitigation, (ii) Foundations for Data Trustworthiness and Policy-based Sharing, and (iii) A Risk-based Approach to Security Metrics. We believe that the three aspects addressed in this paper will form the basis for studying the Science of Cyber Security.", "num_citations": "28\n", "authors": ["1190"]}
{"title": "Feature based techniques for auto-detection of novel email worms\n", "abstract": " This work focuses on applying data mining techniques to detect email worms. We apply a feature-based detection technique. These features are extracted using different statistical and behavioral analysis of emails sent over a certain period of time. The number of features thus extracted is too large. So, our goal is to select the best set of features that can efficiently distinguish between normal and viral emails using classification techniques. First, we apply Principal Component Analysis (PCA) to reduce the high dimensionality of data and to find a projected, optimal set of attributes. We observe that the application of PCA on a benchmark dataset improves the accuracy of detecting novel worms. Second, we apply J48 decision tree algorithm to determine the relative importance of features based on information gain. We are able to identify a subset of features, along with a set of classification rules that have a\u00a0\u2026", "num_citations": "28\n", "authors": ["1190"]}
{"title": "The applicability of the perturbation model-based privacy preserving data mining for real-world data\n", "abstract": " Perturbation method is a very important technique in privacy preserving data mining. In this technique, loss of information versus preservation of privacy is always a trade off. The question is, how much are the users willing to compromise their privacy? This is a choice that changes from individual to individual. In this paper, we propose an individually adaptable perturbation model, which enables the individuals to choose their own privacy level. Hence our model provides different privacy guarantees for different privacy preferences. We test our new perturbation model by applying different reconstruction methods to the perturbed data sets. Furthermore, we build decision tree and Naive Bayes classifier models on the reconstructed data sets both for synthetic and real world data sets. For the synthetic data set, our experimental results indicate that our model enables the users to choose their own privacy level without\u00a0\u2026", "num_citations": "28\n", "authors": ["1190"]}
{"title": "Advances in Knowledge Discovery and Data Mining: 8th Pacific-Asia Conference, PAKDD 2004, Sydney, Australia, May 26-28, 2004, Proceedings\n", "abstract": " ThePaci? c-AsiaConferenceonKnowledgeDiscoveryandDataMining (PAKDD) has been held every year since 1997. This year, the eighth in the series (PAKDD 2004) was held at Carlton Crest Hotel, Sydney, Australia, 26\u201328 May 2004. PAKDD is a leading international conference in the area of data mining. It p-vides an international forum for researchers and industry practitioners to share their new ideas, original research results and practical development experiences from all KDD-related areas including data mining, data warehousing, machine learning, databases, statistics, knowledge acquisition and automatic scienti? c discovery, data visualization, causal induction, and knowledge-based systems. The selection process this year was extremely competitive. We received 238 researchpapersfrom23countries, whichisthehighestinthehistoryofPAKDD, and re? ects the recognition of and interest in this conference. Each submitted research paper was reviewed by three members of the program committee. F-lowing this independent review, there were discussions among the reviewers, and when necessary, additional reviews from other experts were requested. A total of 50 papers were selected as full papers (21%), and another 31 were selected as short papers (13%), yielding a combined acceptance rate of approximately 34%. The conference accommodated both research papers presenting original-vestigation results and industrial papers reporting real data mining applications andsystemdevelopmentexperience. Theconferencealsoincludedthreetutorials on key technologies of knowledge discovery and data mining, and one workshop focusing\u00a0\u2026", "num_citations": "28\n", "authors": ["1190"]}
{"title": "Security and privacy for web databases and services\n", "abstract": " A semantic web can be thought of as a web that is highly intelligent and sophisticated and one needs little or no human intervention to carry out tasks such as scheduling appointments, coordinating activities, searching for complex documents as well as integrating disparate databases and information systems. While much progress has been made toward developing such an intelligent web, there is still a lot to be done. For example, there is little work on security and privacy for the semantic web. However, before we examine security for the semantic web we need to ensure that its key components, such as web databases and services, are secure. This paper will mainly focus on security and privacy issues for web databases and services. Finally, some directions toward developing a secure semantic web will be provided.", "num_citations": "28\n", "authors": ["1190"]}
{"title": "R2D: Extracting relational structure from RDF stores\n", "abstract": " The enthusiastic acceptance of Resource Description Framework (RDF) as a data model has given birth to a new data storage paradigm, namely, the RDF Graph model. The pool of modeling and visualization tools available for RDF stores is limited due to the technology being in its fledgling stage. The work presented in this paper, called R2D (RDF-to-Database) is an effort to make available, to RDF data stores, the abundance of relational tools that are currently in the market. This is done in the form of a JDBC wrapper around RDF Stores that presents a relational view of the stores and their data to the modeling and visualization tools. This paper presents key R2D functionalities and mapping constructs, procedures for every stage of R2D deployment, and sample results in the form of screenshots and performance graphs.", "num_citations": "27\n", "authors": ["1190"]}
{"title": "Security issues in federated database systems: panel contributions\n", "abstract": " Security Issues in Federated Database Systems | Results of the IFIP WG 11.3 Workshop on Database Security V: Status and Prospects ACM Digital Library home ACM home Google, Inc. (search) Advanced Search Browse About Sign in Register Advanced Search Journals Magazines Proceedings Books SIGs Conferences People More Search ACM Digital Library SearchSearch Advanced Search Browse Browse Digital Library Collections More HomeBrowse by TitleProceedingsResults of the IFIP WG 11.3 Workshop on Database Security V: Status and ProspectsSecurity Issues in Federated Database Systems: Panel Contributions Article Security Issues in Federated Database Systems: Panel Contributions Share on Authors: Matthew Morgenstern View Profile , Teresa F. Lunt View Profile , Bhavani M. Thuraisingham View Profile , David L. Spooner View Profile Authors Info & Affiliations Publication: Results of the IFIP \u2026", "num_citations": "27\n", "authors": ["1190"]}
{"title": "Evolving insider threat detection stream mining perspective\n", "abstract": " Evidence of malicious insider activity is often buried within large data streams, such as system logs accumulated over months or years. Ensemble-based stream mining leverages multiple classification models to achieve highly accurate anomaly detection in such streams, even when the stream is unbounded, evolving, and unlabeled. This makes the approach effective for identifying insider threats who attempt to conceal their activities by varying their behaviors over time. This paper applies ensemble-based stream mining, supervised and unsupervised learning, and graph-based anomaly detection to the problem of insider threat detection. It demonstrates that the ensemble-based approach is significantly more effective than traditional single-model methods, supervised learning outperforms unsupervised learning, and increasing the cost of false negatives correlates to higher accuracy. Future work will consider a\u00a0\u2026", "num_citations": "26\n", "authors": ["1190"]}
{"title": "On secure and resilient telesurgery communications over unreliable networks\n", "abstract": " Telesurgical Robot Systems (TRSs) address mission critical operations emerging in extreme fields such as battlefields, underwater, and disaster territories. The lack of wirelined communication infrastructure in such fields makes the use of wireless technologies including satellite and ad-hoc networks inevitable. TRSs over wireless environments pose unique challenges such as preserving a certain reliability threshold, adhering some maximum tolerable delay, and providing various security measures depending on the nature of the operation and communication environment. In this study we present a novel approach that uses information coding to integrate both light-weight privacy and adaptive reliability in a single protocol called Secure and Statistically Reliable UDP (SSR-UDP). We prove that the offered security is equivalent to the existing AES-based long key crypto systems, yet, with significantly less\u00a0\u2026", "num_citations": "26\n", "authors": ["1190"]}
{"title": "Mining concept-drifting data stream to detect peer to peer botnet traffic\n", "abstract": " We propose a novel stream data classification technique to detect Peer to Peer botnet. Botnet traffic can be considered as stream data having two important properties: infinite length and drifting concept. Thus, stream data classification technique is more appealing to botnet detection than simple classification technique. However, no other botnet detection approaches so far have applied stream data classification technique. We propose a multi-chunk, multi-level ensemble classifier based data mining technique to classify concept-drifting stream data. Previous ensemble techniques in classifying concept-drifting stream data use a single data chunk to train a classifier. In our approach, we train an ensemble of v classifiers from r consecutive data chunks. K of these v-classifier ensembles are used to build another level of ensemble. By introducing this multi-chunk, multi-level ensemble, we significantly reduce error compared to the singlechunk, single level ensemble. We have established the justification of using our algorithm theoretically. We have also tested our technique on both botnet traffic and simulated data, and obtained better detection accuracies compared to other published works.", "num_citations": "26\n", "authors": ["1190"]}
{"title": "Access control, confidentiality and privacy for video surveillance databases\n", "abstract": " In this paper we have addressed confidentiality and privacy for video surveillance databases. First we discussed our overall approach for suspicious event detection. Next we discussed an access control model and accedes control algorithms for confidentiality. Finally we discuss privacy preserving video surveillance. Our goal is build a comprehensive system that can detect suspicious events, ensure confidentiality as well as privacy.", "num_citations": "26\n", "authors": ["1190"]}
{"title": "Dependable real-time data mining\n", "abstract": " In this paper we discuss the need for real-time data mining for many applications in government and industry and describe resulting research issues. We also discuss dependability issues including incorporating security, integrity, timeliness and fault tolerance into data mining. Several different data mining outcomes are described with regard to their implementation in a real-time environment. These outcomes include clustering, association-rule mining, link analysis and anomaly detection. The paper describes how they would be used together in various parallel-processing architectures. Stream mining is discussed with respect to the challenges of performing data mining on stream data from sensors. The paper concludes with a summary and discussion of directions in this emerging area.", "num_citations": "26\n", "authors": ["1190"]}
{"title": "Multimedia database systems-a new frontier\n", "abstract": " While the data model describes the conceptual representation of the multimedia database, the storage structures describe the physical representation\u2014they are closer to the database\u2019s physical implementation. Researchers have proposed storage management for multimedia databases, such as using clustering techniques. Mappings between the storage structures and the conceptual data model are needed, along with special indexing techniques for multimedia databases.", "num_citations": "26\n", "authors": ["1190"]}
{"title": "Multilevel security for information retrieval systems\u2014II\n", "abstract": " In this paper we describe multilevel security issues for hypermedia-based information retrieval database management systems. We first summarize the discussion in our earlier paper on the use of object-oriented approach for developing secure information retrieval systems and then discuss in detail how such a system can be augmented by a Linker to provide hypermedia database management system capabilities. In particular, we discuss various types of nodes and links, an architecture for a hypermedia-based information retrieval system, the various functions of such a system including information retrieval, browsing and update operations, and the interface to the underlying multilevel secure information retrieval system. Finally, the concepts are illustrated using an example.", "num_citations": "26\n", "authors": ["1190"]}
{"title": "Spark-based political event coding\n", "abstract": " Political event data have been widely used to study international politics. Previously, natural text processing and event generation required a lot of human efforts. Today we have high computing infrastructure with advance NLP metadata to leverage those tiresome efforts. TABARI -- an open source non distributed event-coding software -- was an early effort to generate events from a large corpus. It uses a shallow parser to identify the political actors, but ignores semantics and relation among the sentences. PETRARCH, the successor of TABARI, encodes event data into \"who-did-what-to-whom\" format. It uses Stanford CoreNLP to parse sentences and a static CAMEO dictionary to encode the data. To build dynamic dictionaries, we need to analyze more metadata such as the token, Named Entity Recognition (NER), co-reference, and many more from parsed sentences. Although these tools can code modest amounts\u00a0\u2026", "num_citations": "25\n", "authors": ["1190"]}
{"title": "RETRO: a framework for semantics preserving SQL-to-SPARQL translation\n", "abstract": " There have been several attempts to make RDBMS and RDF stores inter-operate. The most popular one, D2RQ, has explored one direction, ie, to look at RDBMS through RDF lenses. In this paper we present RETRO, which investigates the reverse direction, ie, to look at RDF through Relational lenses. RETRO generates a relational schema from an RDF store, enabling a user to query RDF data using SQL. A significant advantage of this direction in-addition to interoperability is that it makes numerous relational tools developed over the past several decades, available to the RDF stores. In order to provide interoperability between these two systems one needs to resolve the heterogeneity between their respective data models and include schema mapping, data mapping and query mapping in the transformation process [5]. However, like D2RQ, RETRO chooses not to physically transform the data and deals only with schema mapping and query mapping. RETRO\u2019s schema mapping derives a domain specific relational schema from RDF data and its query mapping transforms an SQL query over the schema into a provably equivalent SPARQL query, which in-turn is executed upon the RDF store. Since RETRO is a read-only framework, its query mapping uses only a relevant and relationally complete subset of SQL. A proof of correctness of this transformation is given based on compositional semantics of the two query languages.", "num_citations": "25\n", "authors": ["1190"]}
{"title": "Self-training with selection-by-rejection\n", "abstract": " Practical machine learning and data mining problems often face shortage of labeled training data. Self-training algorithms are among the earliest attempts of using unlabeled data to enhance learning. Traditional self-training algorithms label unlabeled data on which classifiers trained on limited training data have the highest confidence. In this paper, a self-training algorithm that decreases the disagreement region of hypotheses is presented. The algorithm supplements the training set with self-labeled instances. Only instances that greatly reduce the disagreement region of hypotheses are labeled and added to the training set. Empirical results demonstrate that the proposed self-training algorithm can effectively improve classification performance.", "num_citations": "24\n", "authors": ["1190"]}
{"title": "Unsupervised ensemble based learning for insider threat detection\n", "abstract": " Insider threats are veritable needles within the haystack. Their occurrence is rare and when they do occur, are usually masked well within normal operation. The detection of these threats requires identifying these rare anomalous needles in a contextualized setting where behaviors are constantly evolving over time. To this refined search, this paper proposes and tests an unsupervised, ensemble based learning algorithm that maintains a compressed dictionary of repetitive sequences found throughout dynamic data streams of unbounded length to identify anomalies. In unsupervised learning, compression-based techniques are used to model common behavior sequences. This results in a classifier exhibiting a substantial increase in classification accuracy for data streams containing insider threat anomalies. This ensemble of classifiers allows the unsupervised approach to outperform traditional static learning\u00a0\u2026", "num_citations": "24\n", "authors": ["1190"]}
{"title": "A token-based access control system for RDF data in the clouds\n", "abstract": " The Semantic Web is gaining immense popularity-and with it, the Resource Description Framework (RDF)broadly used to model Semantic Web content. However, access control on RDF stores used for single machines has been seldom discussed in the literature. One significant obstacle to using RDF stores defined for single machines is their scalability. Cloud computers, on the other hand, have proven useful for storing large RDF stores, but these system slack access control on RDF data to our knowledge. This work proposes a token-based access control system that is being implemented in Hadoop (an open source cloud computing framework). It defines six types of access levels and an enforcement strategy for the resulting access control policies. The enforcement strategy is implemented at three levels: Query Rewriting, Embedded Enforcement, and Post processing Enforcement. In Embedded Enforcement\u00a0\u2026", "num_citations": "24\n", "authors": ["1190"]}
{"title": "Incentive and trust issues in assured information sharing\n", "abstract": " Assured information sharing among different organizations in a coalitional environment is an important first step in accomplishing many critical tasks. For example, different security agencies may need to share intelligence information for detecting terrorist plots. At the same, each organization participating in the assured information sharing process may have different incentives. In this paper, we explore the effects of different incentives and potential trust issues among organizations on the assured information sharing process by developing an evolutionary game theoretic framework. In addition, we provide extensive simulation analysis that illustrates the impact of various different information sharing strategies.", "num_citations": "24\n", "authors": ["1190"]}
{"title": "Role based access control and OWL\n", "abstract": " Current access control research follows two parallel themes: many efforts focus on developing novel access control models meeting the policy needs of real world application domains while others are exploring new policy languages. This paper is motivated by the desire to develop a synergy between these themes facilitated by OWL. Our vision for the future is a world where advanced access control concepts are embodied in models that are supported by policy languages in a natural intuitive manner, while allowing for details beyond the models to be further specified in the policy language. In this paper we specifically study the relationship between the Web Ontology Language (OWL) and the Role Based Access Control (RBAC) model. Although OWL is a web ontology language and not specifically designed for expressing authorization policies, it has been used successfully for this purpose in previous work such as KAoS and Rei. We show two different ways to support the NIST Standard RBAC model in OWL and then discuss how the OWL constructions can be extended to model attribute-based RBAC or more generally attribute-based access control.", "num_citations": "24\n", "authors": ["1190"]}
{"title": "Towards a real-time agent architecture-a whitepaper\n", "abstract": " Applications such as military training simulations and electronic commerce can benefit from the flexible and responsive nature of multi-agent systems. These applications have inherent timing constraints on the operations and interactions that the agents might perform. This paper presents a real-time agent architecture in which agents communicate, cooperate, coordinate and negotiate to meet the goals of a particular application under specified timing constraints. The architecture provides a real-time CORBA layer to handle the underlying real-time communication. It also has a real-time agent communication layer in which agents interact via a real-time extension of a well-known agent communication language.", "num_citations": "24\n", "authors": ["1190"]}
{"title": "A nonmonotonic typed multilevel logic for multilevel secure data/knowledge base management systems\n", "abstract": " The paper describes nonmonotonic typed multilevel logic (NTML) for multilevel database applications. It also describes various approaches to viewing multilevel databases through NTML and discusses techniques for query evaluation and integrity checking.<>", "num_citations": "24\n", "authors": ["1190"]}
{"title": "Deep Residual Learning-Based Enhanced JPEG Compression in the Internet of Things\n", "abstract": " With the development of big data and network technology, there are more use cases, such as edge computing, that require more secure and efficient multimedia big data transmission. Data compression methods can help achieving many tasks like providing data integrity, protection, as well as efficient transmission. Classical multimedia big data compression relies on methods like the spatial-frequency transformation for compressing with loss. Recent approaches use deep learning to further explore the limit of the data compression methods in communication constrained use cases like the Internet of Things (IoT). In this article, we propose a novel method to significantly enhance the transformation-based compression standards like JPEG by transmitting much fewer data of one image at the sender's end. At the receiver's end, we propose a two-step method by combining the state-of-the-art signal processing based\u00a0\u2026", "num_citations": "23\n", "authors": ["1190"]}
{"title": "Web and information security\n", "abstract": " Web and Information Security consists of a collection of papers written by leading experts in the field that describe state-of-the-art topics pertaining to Web and information systems security. In particular, security for the semantic Web, privacy, security policy management and emerging topics such as secure semantic grids and secure multimedia systems are also discussed. As well as covering basic concepts of Web and information system security, this book provides new insights into the semantic Web field and its related security challenges. Web and Information Security is valuable as a reference book for senior undergraduate or graduate courses in information security which have special focuses on Web security. It is also useful for technologists, researchers, managers and developers who want to know more about emerging security technologies.", "num_citations": "23\n", "authors": ["1190"]}
{"title": "Improving timeliness in real-time secure database systems\n", "abstract": " Database systems for real-time applications must satisfy timing constraints associated with transactions, while maintaining data consistency. In addition to real-time requirements, security is usually required in many applications. Multilevel security requirements introduce a new dimension to transaction processing in real-time database systems. In this paper, we argue that because of the complexities involved, trade-offs need to be made between security and timeliness. We briefly present the secure two-phase locking protocol and discuss an adaptive method to support trading off security for timeliness, depending on the current state of the system. The performance of the adaptive secure two-phase locking protocol shows improved timeliness. We also discuss future research direction to improve timeliness of secure database systems.", "num_citations": "23\n", "authors": ["1190"]}
{"title": "Secure semantic service-oriented systems\n", "abstract": " As the demand for data and information management continues to grow, so does the need to maintain and improve the security of databases, applications, and information systems. In order to effectively protect this data against evolving threats, an up-to-date understanding of the mechanisms for securing semantic Web technologies is essential. Reviewi", "num_citations": "21\n", "authors": ["1190"]}
{"title": "Content-based ontology matching for GIS datasets\n", "abstract": " The alignment of separate ontologies by matching related concepts continues to attract great attention within the database and artificial intelligence communities, especially since semantic heterogeneity across data sources remains a widespread and relevant problem. In particular, the Geographic Information System (GIS) domain presents unique forms of semantic heterogeneity that require a variety of matching approaches.", "num_citations": "21\n", "authors": ["1190"]}
{"title": "Directions for security and privacy for semantic e-business applications\n", "abstract": " Developing secure semantic e-business applications requires focusing first on securing the Semantic Web, knowledge management, and e-business processes.", "num_citations": "21\n", "authors": ["1190"]}
{"title": "Using RDF for policy specification and enforcement\n", "abstract": " Security issues for the resource description framework (RDF) [RDF] are today crucial due to the key role played by RDF for the semantic Web [Berners-Lee, T. et al. (2001)]. However, despite its importance, RDF security is still a novel area and very little work has been done so far. We make a first step in this direction by proposing an approach for using RDF for policy specification and enforcement.", "num_citations": "21\n", "authors": ["1190"]}
{"title": "Multilevel security issues in distributed database management systems II\n", "abstract": " The rapid growth of the networking and information-processing industries has led to the development of distributed database management system prototypes and commercial distributed database management systems. In such a system, the database is stored in several computers which are interconnected by some communication media. The aim of a distributed database management system (DDBMS) is to process and communicate data in an efficient and cost-effective manner. It has been recognized that such distributed systems are vital for the efficient processing required in military as well as commercial applications. For many of these applications it is especially important that the DDBMS should provide multilevel security. For example, the DDBMS should allow users who are cleared at different security levels access to the database consisting of data at a variety of sensitivity levels without compromising\u00a0\u2026", "num_citations": "21\n", "authors": ["1190"]}
{"title": "Chainnet: Learning on blockchain graphs with topological features\n", "abstract": " With emergence of blockchain technologies and the associated cryptocurrencies, such as Bitcoin, understanding network dynamics behind Blockchain graphs has become a rapidly evolving research direction. Unlike other financial networks, such as stock and currency trading, blockchain based cryptocurrencies have the entire transaction graph accessible to the public (i.e., all transactions can be downloaded and analyzed). A natural question is then to ask whether the dynamics of the transaction graph impacts the price of the underlying cryptocurrency. We show that standard graph features such as degree distribution of the transaction graph may not be sufficient to capture network dynamics and its potential impact on fluctuations of Bitcoin price. In contrast, the new graph associated topological features computed using the tools of persistent homology, are found to exhibit a high utility for predicting Bitcoin price dynamics. %explain higher order interactions among the nodes in Blockchain graphs and can be used to build much more accurate price prediction models. Using the proposed persistent homology-based techniques, we offer a new elegant, easily extendable and computationally light approach for graph representation learning on Blockchain.", "num_citations": "20\n", "authors": ["1190"]}
{"title": "Enhanced geographically typed semantic schema matching\n", "abstract": " Resolving semantic heterogeneity across distinct data sources remains a highly relevant problem in the GIS domain requiring innovative solutions. Our approach, called GSim, semantically aligns tables from respective GIS databases by first choosing attributes for comparison. We then examine their instances and calculate a similarity value between them called entropy-based distribution (EBD)1 by combining two separate methods. Our primary method discerns the geographic types from instances of compared attributes. If successful, EBD is calculated using only this method. GSim further facilitates geographic type matching by using latlong values to further disambiguate between multiple types of a given instance and applying attribute weighting to quantify the uniqueness of mapped attributes. If geographic type matching is not possible, we then apply a generic schema matching method, independent of the\u00a0\u2026", "num_citations": "20\n", "authors": ["1190"]}
{"title": "Real-time anomaly detection over VMware performance data using storm\n", "abstract": " Anomaly detection is the identification of items or observations which deviate from an expected pattern in a dataset. This paper proposes a novel real time anomaly detection framework for dynamic resource scheduling of a VMware-based cloud data center. The framework monitors VMware performance stream data (e.g. CPU load, memory usage, etc.). Hence, the framework continuously needs to collect data and make decision without any delay. We have used Apache Storm, distributed framework for handling performance stream data and making prediction without any delay. Storm is chosen over a traditional distributed framework (e.g., Hadoop and MapReduce, Mahout) that is good for batch processing. An incremental clustering algorithm to model benign characteristics is incorporated in our storm-based framework. During continuous incoming test stream, if the model finds data deviated from its benign\u00a0\u2026", "num_citations": "19\n", "authors": ["1190"]}
{"title": "A relational wrapper for RDF reification\n", "abstract": " The importance of provenance information as a means to trust and validate the authenticity of available data cannot be stressed enough in today\u2019s web-enabled world. The abundance of data now accessible due to the Internet explosion brings with it the related issue of determining how much of it is trustworthy. Provenance information, such as who is responsible for the data or how the data came to be, assists in the process of verifying the authenticity of the data. Semantic web technologies such as Resource Description Framework (RDF) include the ability to record such provenance information through the process of reification. RDF\u2019s popularity has resulted in a demand for modeling and visualization tools. The work presented in this paper, called R2D, attempts to address this demand by innovatively integrating existing, stable technologies such as relational systems with the newer web technologies\u00a0\u2026", "num_citations": "19\n", "authors": ["1190"]}
{"title": "Delegation-based security model for web services\n", "abstract": " Web service is the emerging standard that supports the seamless interoperation between different applications. While the interoperability, flexibility and automated composition are continuously enhanced, security is still the major hurdle. In recent years, lots of studies have been conducted in web service security and various security standards have been proposed. But most of these studies and standards focus on the access control policies for individual web services and do not consider the access issues in composed services. Consider a simplest service chain wherein a user x accesses service s 1 , and s 2 , in turn, accesses service s 2 - The current web service security framework assumes .s 1  accesses s 2  based on its own privilege; thus sensitive information may be incorrectly revealed to x. A better solution is that x delegates its privilege to service s 1  for this access. However, problems such as how much\u00a0\u2026", "num_citations": "19\n", "authors": ["1190"]}
{"title": "Understanding data mining and applying it to command, control, communications and intelligence environments\n", "abstract": " The paper describes data mining and the various ways in which it can be applied to command, control, communications and intelligence. It begins with an overview of data mining and the various steps in the data mining process. Examples of data mining outcomes are cited and an example of a text mining scenario for intelligence applications is described. Problems associated with data mining are described, including the conflict between security and privacy and the challenges of distributed mining. The paper concludes with a discussion of future trends in data mining.", "num_citations": "19\n", "authors": ["1190"]}
{"title": "Security and data mining\n", "abstract": " Database mining can be defined as the process of mining for implicit, previously unknown, and potentially useful information from very large databases by efficient knowledge discovery techniques. Naturally such a process may open up new inference channels, detect new intrusion patterns, and raises new security problems. New security concern and research problems are addressed and identified. Finally a particularly well developed theory, rough set theory, is discussed and some potential applications to security problems are illustrated.", "num_citations": "19\n", "authors": ["1190"]}
{"title": "Multilevel security issues in distributed database management systems\n", "abstract": " This paper describes the security needs in a distributed processing environment common to many enterprises and discusses the applicability of multilevel secure database management systems in such an environment.", "num_citations": "19\n", "authors": ["1190"]}
{"title": "Focus location extraction from political news reports with bias correction\n", "abstract": " Automatic identification of geolocation mentioned in online news articles provide vital information for understanding associated events. While numerous open-source and commercial tools exist for geolocation extraction, they lack in reliable identification of fine-grained location, i.e., they identify location at country-level rather than a fine-grained city or locality level. The problem of location identification has been widely studied. Yet, most techniques depend on external knowledge-base or view the problem only in terms of Named Entity Recognition (NER), only to identify country-level location information. In this paper, we focus on news articles describing an event. A set of locations directly associated with the event are called focus locations. However, an event can occur only at a single location. Therefore, we aim to extract this location among focus locations, and call this as primary focus location. We propose a\u00a0\u2026", "num_citations": "18\n", "authors": ["1190"]}
{"title": "Towards fine grained RDF access control\n", "abstract": " The Semantic Web is envisioned as the future of the current web, where the information is enriched with machine understandable semantics. According to the World Wide Web Consortium (W3C),\" The Semantic Web provides a common framework that allows data to be shared and reused across application, enterprise, and community boundaries\". Among the various technologies that empower Semantic Web, the most significant ones are Resource Description Framework (RDF) and SPARQL, which facilitate data integration and a means to query respectively. Although Semantic Web is elegantly and effectively equipped for data sharing and integration via RDF, lack of efficient means to securely share data pose limitations in practice. In order to make data sharing and integration pragmatic for Semantic Web, we present a query language based secure data sharing mechanism. We extend SPARQL with a new\u00a0\u2026", "num_citations": "18\n", "authors": ["1190"]}
{"title": "Bin-Carver: Automatic recovery of binary executable files\n", "abstract": " File carving is the process of reassembling files from disk fragments based on the file content in the absence of file system metadata. By leveraging both file header and footer pairs, traditional file carving mainly focuses on document and image files such as PDF and JPEG. With the vast amount of malware code appearing in the wild daily, recovery of binary executable files becomes an important problem, especially for the case in which malware deletes itself after compromising a computer. However, unlike image files that usually have both a header and footer pair, executable files only have header information, which makes the carving much harder. In this paper, we present Bin-Carver, a first-of-its-kind system to automatically recover executable files with deleted or corrupted metadata. The key idea is to explore the road map information defined in executable file headers and the explicit control flow paths present\u00a0\u2026", "num_citations": "18\n", "authors": ["1190"]}
{"title": "Geospatial data qualities as web services performance metrics\n", "abstract": " Service discovery is the crucial phase in the emerging Geospatial Semantic Web to select functionally similar services for the user query. Quality of Service (QoS) based service discovery, popularly studied in traditional Web Services, applies also to Geospatial Web Services. QoS allows service clients to fine-tune their search according to their specific needs and criteria. In high-performance service-based geospatial applications, it becomes an interesting research challenge to identify geospatial parameters to further improve the search process. In this paper we have proposed a set of geospatial criteria that can be used alongside the regular QoS parameters in service discovery and invocation. We show that using this novel approach of incorporating domain-specific drill-down information in addition to the commonly used QoS parameters yield more accurate and trustable Web services platform. We use the\u00a0\u2026", "num_citations": "18\n", "authors": ["1190"]}
{"title": "Multilevel security issues in distributed database management systems\u2014III\n", "abstract": " In this paper we describe multilevel security issues for a distributed database management system which operates in a limited heterogeneous environment. We first describe an architecture for such a system and then discuss techniques for query processing and transaction management.", "num_citations": "18\n", "authors": ["1190"]}
{"title": "Towards self-adaptive metric learning on the fly\n", "abstract": " Good quality similarity metrics can significantly facilitate the performance of many large-scale, real-world applications. Existing studies have proposed various solutions to learn a Mahalanobis or bilinear metric in an online fashion by either restricting distances between similar (dissimilar) pairs to be smaller (larger) than a given lower (upper) bound or requiring similar instances to be separated from dissimilar instances with a given margin. However, these linear metrics learned by leveraging fixed bounds or margins may not perform well in real-world applications, especially when data distributions are complex. We aim to address the open challenge of \u201cOnline Adaptive Metric Learning\u201d(OAML) for learning adaptive metric functions on-the-fly. Unlike traditional online metric learning methods, OAML is significantly more challenging since the learned metric could be non-linear and the model has to be self-adaptive as\u00a0\u2026", "num_citations": "17\n", "authors": ["1190"]}
{"title": "Tweecalization: Efficient and intelligent location mining in twitter using semi-supervised learning\n", "abstract": " Geosocial Networking is the new hotness, with social networks providing services and capabilities to the users to associate location to their profiles. But, because of privacy and security reasons, most of the people on social networking sites like Twitter are unwilling to provide locations in their profiles. This creates a need for an algorithm that predicts the location of the user based on the implicit attributes associated with him. In this paper, we develop a tool, Tweecalization that predicts the location of the user purely on the basis of his social network, using the strong theoretical framework of semi-supervised learning. In particular we employ the label propagation algorithm. On the city locations returned by the algorithm, the system performs agglomerative clustering based on geospatial proximity and their individual scores to return cluster of locations with higher confidence. We perform extensive experiments to show\u00a0\u2026", "num_citations": "17\n", "authors": ["1190"]}
{"title": "A Framework for Policies over Provenance.\n", "abstract": " Provenance captures the history of a data item. This ensures the quality, the trustworthiness and the correctness of shared information, but the provenance may contain sensitive information so we may need to hide it. Sometimes we need access control policies to protect sensitive components and allow access based on certain properties. In other cases, we may need to share provenance but use redaction policies to circumvent the release of sensitive information. In this paper, we formulate an automatic procedure over provenance by combining these policies in an unified framework.", "num_citations": "17\n", "authors": ["1190"]}
{"title": "Semantic schema matching without shared instances\n", "abstract": " Semantic heterogeneity across data sources remains a widespread and relevant problem requiring innovative solutions. Our approach towards resolving semantic disparities among distinct data sources aligns their constituent tables by first choosing attributes for comparison. We then examine their instances and calculate a similarity value between them known as entropy-based distribution (EBD). One method of calculating EBD applies a state-of-the-art instance matching strategy based on N-grams in the data. However, this method often fails because it relies on shared instance data to determine similarity. This results in an overestimation of semantic similarity between unrelated attributes and an underestimation of semantic similarity between related attributes. Our method resolves this using clustering and a measure known as Normalized Google Distance. The EBD is then calculated among all clusters by\u00a0\u2026", "num_citations": "17\n", "authors": ["1190"]}
{"title": "Fingerprint matching algorithm based on tree comparison using ratios of relational distances\n", "abstract": " We present a fingerprint matching algorithm that initially identifies the candidate common unique (minutiae) points in both the base and the input images using ratios of relative distances as the comparing function. A tree like structure is then drawn connecting the common minutiae points from bottom up in both the base and the input images. Matching score is obtained by comparing the similarity of the two tree structures based on a threshold value. We define a new term called the 'M(i)-tuple' for each minutiae point which uniquely encodes details about the local surrounding region, where i = 1 to N, and N is the number of minutiae. The proposed algorithm requires no explicit alignment of the two to-be compared fingerprint images and also tolerates distortions caused by spurious minutiae points. The algorithm is also capable of comparing and producing matching scores between two images obtained from two\u00a0\u2026", "num_citations": "17\n", "authors": ["1190"]}
{"title": "Managing and mining multimedia databases\n", "abstract": " Several advances have been made on managing multimedia databases as well as on data mining. Recently there is active research on mining multimedia databases. This paper provides an overview of managing multimedia databases and then describes issues on mining multimedia databases. In particular mining text, image, audio and video data are discussed.", "num_citations": "17\n", "authors": ["1190"]}
{"title": "Data mining and data visualization: Position paper for the second IEEE workshop on database issues for data visualization\n", "abstract": " The government, corporate, and industrial communities are faced with an ever increasing number of databases. These databases need not only to be managed, but also explored. The first requires secure access to distributed heterogeneous multimedia databases with rich metadata and having to meet timing constraints. The second requires exploratory tools supporting the identification of domain and mission critical elements such as patterns in data access (eg, security breach determinations), patterns in data (eg, marketing and clustering), or for patterns in transactions (eg, data compression), to site a few. Knowledge Discovery in Databases is a relatively new research area that employs a variety of tools to explore and identify structure and patterns in these large databases. Often the data is preprocessed to facilitate such computations (data warehousing). The data is then mined for specific rules that are built\u00a0\u2026", "num_citations": "17\n", "authors": ["1190"]}
{"title": "INSuRE: collaborating centers of academic excellence engage students in cybersecurity research\n", "abstract": " Since fall 2012, several National Centers of Academic Excellence in Cyber Defense Research have fielded a collaborative course-the Information Security Research and Education (INSuRE) program-to engage students in applied cybersecurity research. Recent experiences with INSuRE are discussed, including an overview of the project-based research course, student projects, and outcomes and lessons learned.", "num_citations": "16\n", "authors": ["1190"]}
{"title": "Malware collection and analysis\n", "abstract": " This paper describes the various malware datasets that we have obtained permissions to host at the University of Arizona as part of a National Science Foundation funded project. It also describes some other malware datasets that we are in the process of obtaining permissions to host at the University of Arizona. We have also discussed some preliminary work we have carried out on malware analysis using big data platforms.", "num_citations": "16\n", "authors": ["1190"]}
{"title": "Online anomaly detection for multi\u2010source VMware using a distributed streaming framework\n", "abstract": " Anomaly detection refers to the identification of patterns in a dataset that do not conform to expected patterns. Such non\u2010conformant patterns typically correspond to samples of interest and are assigned to different labels in different domains, such as outliers, anomalies, exceptions, and malware. A daunting challenge is to detect anomalies in rapid voluminous streams of data. This paper presents a novel, generic real\u2010time distributed anomaly detection framework for multi\u2010source stream data. As a case study, we investigate anomaly detection for a multi\u2010source VMware\u2010based cloud data center, which maintains a large number of virtual machines (VMs). This framework continuously monitors VMware performance stream data related to CPU statistics (e.g., load and usage). It collects data simultaneously from all of the VMs connected to the network and notifies the resource manager to reschedule its CPU resources\u00a0\u2026", "num_citations": "16\n", "authors": ["1190"]}
{"title": "Secure data provenance and inference control with semantic web\n", "abstract": " With an ever-increasing amount of information on the web, it is critical to understand the pedigree, quality, and accuracy of your data. Using provenance, you can ascertain the quality of data based on its ancestral data and derivations, track back to sources of errors, allow automatic re-enactment of derivations to update data, and provide attribution of the data source. Secure Data Provenance and Inference Control with Semantic Web supplies step-by-step instructions on how to secure the provenance of your data to make sure it is safe from inference attacks. It details the design and implementation of a policy engine for provenance of data and presents case studies that illustrate solutions in a typical distributed health care system for hospitals. Although the case studies describe solutions in the health care domain, you can easily apply the methods presented in the book to a range of other domains. The book describes the design and implementation of a policy engine for provenance and demonstrates the use of Semantic Web technologies and cloud computing technologies to enhance the scalability of solutions. It covers Semantic Web technologies for the representation and reasoning of the provenance of the data and provides a unifying framework for securing provenance that can help to address the various criteria of your information systems. Illustrating key concepts and practical techniques, the book considers cloud computing technologies that can enhance the scalability of solutions. After reading this book you will be better prepared to keep up with the on-going development of the prototypes, products, tools, and standards for secure data\u00a0\u2026", "num_citations": "16\n", "authors": ["1190"]}
{"title": "Scalable complex query processing over large semantic web data using cloud\n", "abstract": " Cloud computing solutions continue to grow increasingly popular both in research and in the commercial IT industry. With this popularity comes ever increasing challenges for the cloud computing service providers. Semantic web is another domain of rapid growth in both research and industry. RDF datasets are becoming increasingly large and complex and existing solutions do not scale adequately. In this paper, we will detail a scalable semantic web framework built using cloud computing technologies. We define solutions for generating and executing optimal query plans. We handle not only queries with Basic Graph Patterns (BGP) but also complex queries with optional blocks. We have devised a novel algorithm to handle these complex queries. Our algorithm minimizes binding triple patterns and joins between them by identifying common blocks by algorithms to find sub graph isomorphism and building a\u00a0\u2026", "num_citations": "16\n", "authors": ["1190"]}
{"title": "End-to-end accountability in grid computing systems for coalition information sharing\n", "abstract": " Accountability is crucial for any computer system. It assures that every action executed in the system can be traced back to some entity. Accountability for grid computing systems is even more crucial given the very large number of users and active entities in these systems. However, so far no comprehensive approach to accountability for grid systems exists. Our work addresses such lack by developing a comprehensive accountability system driven by policies and supported by accountability agents. This paper first discusses the requirements that have driven the design of our accountability system. It then presents the key elements of our approach, namely the accountability data, the accountability policy language, and the agents.", "num_citations": "16\n", "authors": ["1190"]}
{"title": "Data and applications security: developments and directions\n", "abstract": " This paper first describes the developments in data and applications security with a special emphasis on database security. Then it discusses direction for data and applications security which includes secure semantic web, XML security, and security for emerging applications such as bioinformatics, peer-to-peer computing, and stream information management.", "num_citations": "16\n", "authors": ["1190"]}
{"title": "Evolvable real-time C3 systems\n", "abstract": " This paper describes MITRE's Evolvable Real-Time Command, Control, and Communications (C3) systems initiative that attempts to develop an approach that would enable current real-time systems to evolve into the systems of the future. In particular, this article describes the infrastructure requirements that we have developed. We first provide an overview of the current real-time C3 systems and describe the systems of the future. Next, we describe some candidate architectures that we have examined for future systems. Then a detailed discussion of the requirements for the infrastructure are given. The main focus is on operating systems, data management systems, and communication systems requirements. The discussion is based on the candidate architectures that we have examined. The project has chosen Airborne Warning and Control System (AWACS) as an example to test out the concepts and\u00a0\u2026", "num_citations": "16\n", "authors": ["1190"]}
{"title": "Multilevel secure object-oriented data model- issues on noncomposite objects, composite objects, and versioning.\n", "abstract": " While progress has been made in incorporating multilevel security into an object-oriented data model, much still remains to be done. This article discusses the issues involved in supporting noncomposite and composite objects and versioning, which have not yet been investigated in such models, because these features are essential for data-intensive applications in hypermedia systems, CAD/CAM, and knowledge-based systems.", "num_citations": "16\n", "authors": ["1190"]}
{"title": "A multilevel secure object-oriented data model\n", "abstract": " A multilevel secure object-oriented data model, S02, is described here. We first developed a multilevel type system and then defined it multilevel obycct-oriented database.|'. is this approach that could establish a theoretical framework for secure objcct~ on'cntcd systems. Also d: scussed here are the issues involved in (1) developing a security policy (2) handling polyinstantiation (3) using security constraints and (4) handling the inference problem for our model.", "num_citations": "16\n", "authors": ["1190"]}
{"title": "Towards a privacy-aware quantified self data management framework\n", "abstract": " Massive amounts of data are being collected, stored, and analyzed for various business and marketing purposes. While such data analysis is critical for many applications, it could also violate the privacy of individuals. This paper describes the issues involved in designing a privacy aware data management framework for collecting, storing, and analyzing the data. We also discuss behavioral aspects of data sharing as well as aspects of a formal framework based on rewriting rules that encompasses the privacy aware data management framework.", "num_citations": "15\n", "authors": ["1190"]}
{"title": "Big data analytics with applications in insider threat detection\n", "abstract": " Today's malware mutates randomly to avoid detection, but reactively adaptive malware is more intelligent, learning and adapting to new computer defenses on the fly. Using the same algorithms that antivirus software uses to detect viruses, reactively adaptive malware deploys those algorithms to outwit antivirus defenses and to go undetected. This book provides details of the tools, the types of malware the tools will detect, implementation of the tools in a cloud computing framework and the applications for insider threat detection.", "num_citations": "15\n", "authors": ["1190"]}
{"title": "Developing and securing the cloud\n", "abstract": " Although the use of cloud computing platforms and applications has expanded rapidly, most books on the subject focus on high-level concepts. There has long been a need for a book that provides detailed guidance on how to develop secure clouds. Filling this void, Developing and Securing the Cloud provides a comprehensive overview of cloud computing technology. Supplying step-by-step instruction on how to develop and secure cloud computing platforms and web services, it includes an easy-to-understand, basic-level overview of cloud computing and its supporting technologies. Presenting a framework for secure cloud computing development, the book describes supporting technologies for the cloud such as web services and security. It details the various layers of the cloud computing framework, including the virtual machine monitor and hypervisor, cloud data storage, cloud data management, and virtual network monitor. It also provides several examples of cloud products and prototypes, including private, public, and US government clouds. Reviewing recent developments in cloud computing, the book illustrates the essential concepts, issues, and challenges in developing and securing today\u2019s cloud computing platforms and applications. It also examines prototypes built on experimental cloud computing systems that the author and her team have developed at the University of Texas at Dallas. This diverse reference is suitable for those in industry, government, and academia. Technologists will develop the understanding required to select the appropriate tools for particular cloud applications. Developers will discover alternative designs\u00a0\u2026", "num_citations": "15\n", "authors": ["1190"]}
{"title": "Adaptive information coding for secure and reliable wireless telesurgery communications\n", "abstract": " Telesurgical Robot Systems (TRSs) have been the focus of research in academic, military, and commercial domains for many years. Contemporary TRSs address mission critical operations emerging in extreme fields such as battlefields, underwater, and disaster territories. The lack of wirelined communication infrastructure in such fields makes the use of wireless technologies including satellite and ad-hoc networks inevitable. TRSs over wireless environments pose unique challenges such as preserving a certain reliability threshold, adhering some maximum tolerable delay, and providing various security measures depending on the nature of the operation and communication environment. In this study, we present a novel approach that uses information coding to integrate both light-weight privacy and adaptive reliability in a single protocol called Secure and Statistically Reliable UDP (SSR-UDP). We prove\u00a0\u2026", "num_citations": "15\n", "authors": ["1190"]}
{"title": "PTAS for the minimum weighted dominating set in growth bounded graphs\n", "abstract": " The minimum weighted dominating set (MWDS) problem is one of the classic NP-hard optimization problems in graph theory with applications in many fields such as wireless communication networks. MWDS in general graphs has been showed not to have polynomial-time constant-approximation if  . Recently, several polynomial-time constant-approximation SCHEMES have been designed for MWDS in unit disk graphs. In this paper, using the local neighborhood-based scheme technique, we present a PTAS for MWDS in polynomial growth bounded graphs with bounded degree constraint.", "num_citations": "15\n", "authors": ["1190"]}
{"title": "Scalable and efficient reasoning for enforcing role-based access control\n", "abstract": " Today, many organizations generate large amount of data and have many users that need only partial access to resources at any time to collaborate in making critical decisions. Thus, there is a need for a scalable access control model that simplifies the management of security policies and handles the heterogeneity inherent in the information system. This paper proposes an ontology-based distributed solution to this problem, with the benefits of being scalable and producing acceptable response times.", "num_citations": "15\n", "authors": ["1190"]}
{"title": "Trustworthy semantic Web technologies for secure knowledge management\n", "abstract": " Semantic Web technologies have many applications due to their expressive and reasoning power. At the same time, secure knowledge management is becoming a crucial area for many corporations where the data, information and knowledge including the intellectual property and the expertise in the corporation have to be protected. In this paper we explore the applications of trustworthy semantic Web technologies for secure knowledge management.", "num_citations": "15\n", "authors": ["1190"]}
{"title": "DAGIS: A Geospatial Semantic Web services discovery and selection framework\n", "abstract": " The traditional Web services architecture uses a keyword based search to match a query to one or more service providers. However, a world-to-word matching to discover a service provider is too simplistic for geospatial data and fails to capture matches that advertise their functionality using domain-dependent terminology. In this paper, we present DAGIS (Discovering Annotated Geospatial Information Services) \u2013 a semantic Web services based framework for geospatial domain that has graphical interface to query and discover services. It handles the semantic heterogeneities involved in the discovery phase and we propose algorithms for selecting the best service through QoS (Quality of Service) based semantic matching. The framework is capable of performing dynamic compositions on the fly through a back chaining algorithm. The framework is evaluated by solving queries posed by users in various\u00a0\u2026", "num_citations": "15\n", "authors": ["1190"]}
{"title": "Applying omt for designing multilevel database applications\n", "abstract": " Applying OMT for Designing Multilevel Database Applications | Proceedings of the IFIP WG11.3 Working Conference on Database Security VII ACM Digital Library home ACM home Google, Inc. (search) Advanced Search Browse About Sign in Register Advanced Search Journals Magazines Proceedings Books SIGs Conferences People More Search ACM Digital Library SearchSearch Advanced Search Browse Browse Digital Library Collections More HomeBrowse by TitleProceedingsProceedings of the IFIP WG11.Working Conference on Database Security VIIApplying OMT for Designing Multilevel Database Applications ARTICLE Applying OMT for Designing Multilevel Database Applications Share on Authors: Peter J Sell profile image Peter J. Sell View Profile , Bhavani M Thuraisingham profile image Bhavani M. Thuraisingham View Profile Authors Info & Affiliations Publication: Proceedings of the IFIP WG11.\u2026", "num_citations": "15\n", "authors": ["1190"]}
{"title": "Evolving big data stream classification with mapreduce\n", "abstract": " Big Data Stream mining has some inherent challenges which are not present in traditional data mining. Not only Big Data Stream receives large volume of data continuously, but also it may have different types of features. Moreover, the concepts and features tend to evolve throughout the stream. Traditional data mining techniques are not sufficient to address these challenges. In our current work, we have designed a multi-tiered ensemble based method HSMiner to address aforementioned challenges to label instances in an evolving Big Data Stream. However, this method requires building large number of AdaBoost ensembles for each of the numeric features after receiving each new data chunk which is very costly. Thus, HSMiner may face scalability issue in case of classifying Big Data Stream. To address this problem, we propose three approaches to build these large number of AdaBoost ensembles using\u00a0\u2026", "num_citations": "14\n", "authors": ["1190"]}
{"title": "Ontology-driven query expansion methods to facilitate federated queries\n", "abstract": " In view of the need for a highly distributed and federated architecture, a robust query expansion in a specific domain has great impact on the performance of information retrieval. We aim to determine robust expansion terms using different weighting techniques and finding out the most k-top relevant terms. For this, first, we consider each individual ontology and user query keywords to determine the Basic Expansion Terms (BET) using a number of semantic measures namely Density Measure (DM), Betweenness Measure (BM), and Semantic Similarity Measure (SSM). Second, we specify New Expansion Terms (NET) by Ontology Alignment (OA). Third, we weight expanded terms using a combination of these semantic measures. Fourth, we use a Specific Interval(SI) to determine a set of Robust Expansion Terms (RET). Finally, we compare the result of our novel weighting approach with existing expansion\u00a0\u2026", "num_citations": "14\n", "authors": ["1190"]}
{"title": "Semantic web for content based video retrieval\n", "abstract": " This paper aims to provide a semantic Web based video search engine. Currently, we do not have scalable integration platforms to represent extracted features from videos, so that they could be indexed and searched. The task of indexing extracted features from videos is a difficult challenge, due to the diverse nature of the features and the temporal dimensions of videos. We present a semantic Web based framework for automatic feature extraction, storage, indexing and retrieval of videos. Videos are represented as interconnected set of semantic resources. Also, we suggest a new ranking algorithm for finding related resources which could be used in a semantic Web based search engine.", "num_citations": "14\n", "authors": ["1190"]}
{"title": "Data mining for security applications: Mining concept-drifting data streams to detect peer to peer botnet traffic\n", "abstract": " The presentation first provide an overview for data mining for security applications and then discuss our research to the botnet problem which follows from an important observation that network traffic (as well as botnet traffic) is a continuous flow of data stream. Conventional data mining techniques are not directly applicable to stream data because of two vital problems associated with them: potentially infinite in length, and concept drift. We propose a technique that can efficiently handle both problems. Our main focus is to adapt three major data mining techniques: classification, clustering, and outlier detection to handle stream data. Our preliminary study on the development of new stream classification techniques for P2P bothnet detection has generated encouraging results. In addition to botnet detection, we also discuss our research on data mining for malicious code detection and intrusion detection.", "num_citations": "14\n", "authors": ["1190"]}
{"title": "Building Secure survivable semantic webs\n", "abstract": " This paper describes some ideas for a secure survivable semantic web that follows some of our previous ideas on dependable semantic web. Semantic web is a technology for understanding Web pages. It is important that the semantic web is secure. In addition, data exchanged by the Web has to be of high quality and survive failures and errors. The processes that the Web supports have to meet certain timing constraints. This paper discusses these aspects, and describes how they provide a dependable semantic web.", "num_citations": "14\n", "authors": ["1190"]}
{"title": "Data quality: developments and directions\n", "abstract": " This paper first examines various issues on data quality and provides an overview of current research in the area. Then it focuses on research at the MITRE Corporation to use annotations to manage data quality. Next some of the emerging directions in data quality including managing quality for the semantic web and the relationships between data quality and data mining will be discussed. Finally some of the directions for data quality will be provided.", "num_citations": "14\n", "authors": ["1190"]}
{"title": "Query processing in ldv: A secure database system\n", "abstract": " An overview is given of the query processing of the multilevel secure database management system (MLS/DBMS), LOCK Data Views (LDV), for the secure distributed Data Views contract. The authors summarize design issues such as data distribution, polyinstantiation, and response assembly. They show the need for a security policy for a database system that builds on the classical security policies for operating systems. They describe some of the problems associated with multilevel databases and their approach to solving them. They also explain how a pipeline organization helps to minimize the amount of design and code that must be trusted and/or verified.<>", "num_citations": "14\n", "authors": ["1190"]}
{"title": "Multistream classification for cyber threat data with heterogeneous feature space\n", "abstract": " Under a newly introduced setting of multistream classification, two data streams are involved, which are referred to as source and target streams. The source stream continuously generates data instances from a certain domain with labels, while the target stream does the same task without labels from another domain. Existing approaches assume that domains for both data streams are identical, which is not quite true in real world scenario, since data streams from different sources may contain distinct features. Furthermore, obtaining labels for every instance in a data stream is often expensive and time-consuming. Therefore, it has become an important topic to explore whether labeled instances from other related streams can be helpful to predict those unlabeled instances in a given stream. Note that domains of source and target streams may have distinct features spaces and data distributions. Our objective is to\u00a0\u2026", "num_citations": "13\n", "authors": ["1190"]}
{"title": "Unsupervised deep embedding for novel class detection over data stream\n", "abstract": " Data streams are continuous flows of data points. Novel class detection is an important part of data stream mining. A novel class is a newly emerged class that has not previously been modeled by the classifier over the input stream. This paper proposes deep embedding for novel class detection - a novel approach that combines feature learning using denoising autoencoding with novel class detection. A denoising autoencoder is a neural network with hidden layers aiming to reconstruct the input vector from a corrupted version. A nonparametric multidimensional change point detection approach is also proposed, to detect concept-drift (the change of data feature values over time). Experiments on several real datasets show that the approach significantly improves the performance of novel class detection.", "num_citations": "13\n", "authors": ["1190"]}
{"title": "Honeypot based unauthorized data access detection in MapReduce systems\n", "abstract": " The data processing capabilities of MapReduce systems pioneered with the on-demand scalability of cloud computing have enabled the Big Data revolution. However, the data controllers/owners worried about the privacy and accountability impact of storing their data in the cloud infrastructures as the existing cloud computing solutions provide very limited control on the underlying systems. The intuitive approach - encrypting data before uploading to the cloud - is not applicable to MapReduce computation as the data analytics tasks are ad-hoc defined in the MapReduce environment using general programming languages (e.g, Java) and homomorphic encryption methods that can scale to big data do not exist. In this paper, we address the challenges of determining and detecting unauthorized access to data stored in MapReduce based cloud environments. To this end, we introduce alarm raising honeypots\u00a0\u2026", "num_citations": "13\n", "authors": ["1190"]}
{"title": "An individual-based model of information diffusion combining friends\u2019 influence\n", "abstract": " In many real-world scenarios, an individual accepts a new piece of information based on her intrinsic interest as well as friends\u2019 influence. However, in most of the previous works, the factor of individual\u2019s interest does not receive great attention from researchers. Here, we propose a new model which attaches importance to individual\u2019s interest including friends\u2019 influence. We formulate the problem of maximizing the acceptance of information (MAI) as: launch a seed set of acceptors to trigger a cascade such that the number of final acceptors under a time constraint T in a social network is maximized. We then prove that MAI is NP-hard, and for time , the objective function for information acceptance is sub-modular when the function for friends\u2019 influence is sub-linear in the number of friends who have accepted the information (referred to as active friends). Therefore, an approximation ratio  for MAI\u00a0\u2026", "num_citations": "13\n", "authors": ["1190"]}
{"title": "A cloud-based RDF policy engine for assured information sharing\n", "abstract": " In this paper, we describe a general-purpose, scalable RDF policy engine. The innovations in our work include seamless support for a diverse set of security policies enforced by a highly available and scalable policy engine designed using a cloud-based platform. Our main goal is to demonstrate how coalition agencies can share information stored in multiple formats, through the enforcement of appropriate policies.", "num_citations": "13\n", "authors": ["1190"]}
{"title": "Cloud-centric assured information sharing\n", "abstract": " In this paper we describe the design and implementation of cloud-based assured information sharing systems. In particular, we will describe our current implementation of a centralized cloud-based assured information sharing system and the design of a decentralized hybrid cloud-based assured information sharing system of the future. Our goal is for coalition organizations to share information stored in multiple clouds and enforce appropriate policies.", "num_citations": "13\n", "authors": ["1190"]}
{"title": "Privacy/analysis tradeoffs in sharing anonymized packet traces: Single-field case\n", "abstract": " Network data needs to be shared for distributed security analysis. Anonymization of network data for sharing sets up a fundamental tradeoff between privacy protection versus security analysis capability. This privacy/analysis tradeoff has been acknowledged by many researchers but this is the first paper to provide empirical measurements to characterize the privacy/analysis tradeoff for an enterprise dataset. Specifically we perform anonymization options on single-fields within network packet traces and then make measurements using intrusion detection system alarms as a proxy for security analysis capability. Our results show: (1) two fields have a zero sum tradeoff (more privacy lessens security analysis and vice versa) and (2) eight fields have a more complex tradeoff (that is not zero sum) in which both privacy and analysis can both be simultaneously accomplished.", "num_citations": "13\n", "authors": ["1190"]}
{"title": "Bimorphing: A bi-directional bursting defense against website fingerprinting attacks\n", "abstract": " Network traffic analysis has been increasingly used in various applications to either protect or threaten people, information, and systems. Website fingerprinting is a passive traffic analysis attack which threatens web navigation privacy. It is a set of techniques used to discover patterns from a sequence of network packets generated while a user accesses different websites. Internet users (such as online activists or journalists) may wish to hide their identity and online activity to protect their privacy. Typically, an anonymity network is utilized for this purpose. These anonymity networks such as Tor (The Onion Router) provide layers of data encryption which poses a challenge to the traffic analysis techniques. Although various defenses have been proposed to counteract this passive attack, they have been penetrated by new attacks that proved the ineffectiveness and/or impracticality of such defenses. In this work, we\u00a0\u2026", "num_citations": "12\n", "authors": ["1190"]}
{"title": "P2V: Effective website fingerprinting using vector space representations\n", "abstract": " Language vector space models (VSMs) have recently proven to be effective across a variety of tasks. In VSMs, each word in a corpus is represented as a real-valued vector. These vectors can be used as features in many applications in machine learning and natural language processing. In this paper, we study the effect of vector space representations in cyber security. In particular, we consider a passive traffic analysis attack (Website Fingerprinting) that threatens users' navigation privacy on the web. By using anonymous communication, Internet users (such as online activists) may wish to hide the destination of web pages they access for different reasons such as avoiding tyrant governments. Traditional website fingerprinting studies collect packets from the users' network and extract features that are used by machine learning techniques to reveal the destination of certain web pages. In this work, we propose the\u00a0\u2026", "num_citations": "12\n", "authors": ["1190"]}
{"title": "Services in the Cloud\n", "abstract": " Provides an in-depth review and analysis of cloud computing, applications for its use, cloud technologies, different kinds of cloud services, types of clouds, and major areas of research and development in the cloud computing field. Also provides an overview of the articles persented in this issue.", "num_citations": "12\n", "authors": ["1190"]}
{"title": "Sparse bayesian adversarial learning using relevance vector machine ensembles\n", "abstract": " Data mining tasks are made more complicated when adversaries attack by modifying malicious data to evade detection. The main challenge lies in finding a robust learning model that is insensitive to unpredictable malicious data distribution. In this paper, we present a sparse relevance vector machine ensemble for adversarial learning. The novelty of our work is the use of individualized kernel parameters to model potential adversarial attacks during model training. We allow the kernel parameters to drift in the direction that minimizes the likelihood of the positive data. This step is interleaved with learning the weights and the weight priors of a relevance vector machine. Our empirical results demonstrate that an ensemble of such relevance vector machine models is more robust to adversarial attacks.", "num_citations": "12\n", "authors": ["1190"]}
{"title": "Identity management for cloud computing: developments and directions\n", "abstract": " Cloud computing technologies have been rapidly adopted by organizations to lower costs and to enable flexible and efficient access to critical data. As these new cloud technologies emerge, cyber security challenges associated with these technologies have increased at a rapid pace. One of the critical areas that needs attention for secure cloud computing is identity management where the multiple identities of cloud users operating possibly in a federated environment have to be managed and maintained. In this paper, we first explore identity management technologies and secure cloud computing technologies. We will then discuss some of the security balances for cloud computing with respect to identity management.", "num_citations": "12\n", "authors": ["1190"]}
{"title": "Managing risks in RBAC employed distributed environments\n", "abstract": " Role Based Access Control (RBAC) has been introduced in an effort to facilitate authorization in database systems. It introduces roles as a new layer in between users and permissions. This not only provides a well maintained access granting mechanism, but also alleviates the burden to manage multiple users. While providing comprehensive access control, current RBAC models and systems do not take into consideration the possible risks that can be incurred with role misuse. In distributed environments a large number of users are a very common case, and a considerable number of them are first time users. This fact magnifies the need to measure risk before and after granting an access. We investigate the means of managing risks in RBAC employed distributed environments and introduce a probability based novel risk model. Based on each role, we use information about user credentials, current\u00a0\u2026", "num_citations": "12\n", "authors": ["1190"]}
{"title": "Security and privacy issues for sensor databases\n", "abstract": " The most compelling sensor technology advances of this decade are deploying wireless networks of heterogeneous smart sensor nodes for complex information-gathering tasks. Sensors in wireless sensor networks operate under a set of unique and fundamental constraints that make collaborative information-gathering tasks challenging. Sensors in the network simultaneously participate in the collaborative decision making required for aggregation of data in an efficient way. Thus, the study of security and privacy for sensor databases in the context of sensor networks is at the very heart of technology advances for the next decade. The intent of this article is to bring together the fundamental concepts of security and privacy for sensor databases so that the unifying principles and underlying concepts of algorithm design may more easily be structured. We discuss several security issues. These include security policy\u00a0\u2026", "num_citations": "12\n", "authors": ["1190"]}
{"title": "Restricting search domains to refine data analysis in semantic-conflict identification\n", "abstract": " This paper describes heuristics for identifying semantic conflicts in large, heterogeneous, distributed, and federated databases in which the combinatorics of comparisons become an issue. Data can be divided along several dimensions and the search space for conflicts can be narrowed considerably. The paper considers various search criteria, such as frequency of use, importance of data and error correction, ability to fix, and data categories. Tradeoffs in reducing the search domain and the relationship of this work to other areas of computer science also are described. The paper concludes with a discussion of future directions and applications.", "num_citations": "12\n", "authors": ["1190"]}
{"title": "A fine-grained access control model for object-oriented DBMSs\n", "abstract": " A Fine-grained Access Control Model for Object-Oriented DBMSs | Proceedings of the IFIP WG11.3 Working Conference on Database Security VII ACM Digital Library home ACM home Google, Inc. (search) Advanced Search Browse About Sign in Register Advanced Search Journals Magazines Proceedings Books SIGs Conferences People More Search ACM Digital Library SearchSearch Advanced Search Browse Browse Digital Library Collections More HomeBrowse by TitleProceedingsProceedings of the IFIP WG11.Working Conference on Database Security VIIA Fine-grained Access Control Model for Object-Oriented DBMSs ARTICLE A Fine-grained Access Control Model for Object-Oriented DBMSs Share on Authors: Arnon S Rosenthal profile image Arnon Rosenthal View Profile , James G Williams profile image James Williams View Profile , William R Herndon profile image William R. Herndon View Profile , \u2026", "num_citations": "12\n", "authors": ["1190"]}
{"title": "Deepsweep: An evaluation framework for mitigating dnn backdoor attacks using data augmentation\n", "abstract": " Public resources and services (eg, datasets, training platforms, pre-trained models) have been widely adopted to ease the development of Deep Learning-based applications. However, if the third-party providers are untrusted, they can inject poisoned samples into the datasets or embed backdoors in those models. Such an integrity breach can cause severe consequences, especially in safety-and security-critical applications. Various backdoor attack techniques have been proposed for higher effectiveness and stealthiness. Unfortunately, existing defense solutions are not practical to thwart those attacks in a comprehensive way.", "num_citations": "11\n", "authors": ["1190"]}
{"title": "Specification and Analysis of ABAC Policies via the Category-based Metamodel\n", "abstract": " The Attribute-Based Access Control (ABAC) model is one of the most powerful access control models in use. It subsumes popular models, such as the Role-Based Access Control (RBAC) model, and can also enforce dynamic policies where authorisations depend on values of user, resource or environment attributes. However, in its general form, ABAC does not lend itself well to some operations, such as review queries, and ABAC policies are in general more difficult to specify and analyse than simpler RBAC policies. In this paper we propose a formal specification of ABAC in the category-based metamodel of access control, which adds structure to ABAC policies, making them easier to design and understand. We provide an axiomatic and an operational semantics for ABAC policies, and show how to use them to analyse policies and evaluate review queries.", "num_citations": "11\n", "authors": ["1190"]}
{"title": "Tweeque: Spatio-temporal analysis of social networks for location mining using graph partitioning\n", "abstract": " Because of privacy and security reasons, most of the people on social networking sites like Twitter are unwilling to specify their locations in the profiles. In this paper, we present a completely novel approach, Tweeque which is a spatio-temporal mining algorithm that predicts the current location of the user purely on the basis of his social network. The algorithm goes beyond the previous approaches by linking geospatial proximity to friendship and understanding the social phenomenon of migration. The algorithm then performs graph partitioning for identifying social groups allowing us to implicitly consider time as a factor for prediction of user's most current city location. We perform extensive experiments to show the validity of our system in terms of both accuracy and running time.", "num_citations": "11\n", "authors": ["1190"]}
{"title": "Extracting semantic information structures from free text law enforcement data\n", "abstract": " A detective distributes information on a current case to his law enforcement peers. He quickly receives a computer generated response with leads identified within hundreds of thousands of previously distributed free text documents from thousands of other detectives. The challenges lie in the nature of free text - unstructured formats, confusing word usage, cut-andpaste additions, abbreviations, inserted html/xml tags, multimedia content, and domain-specific terminology. This research proposes a new data structure, the semantic information structure, which encapsulates the extracted content information on classes of information such as people, vehicles, events, organizations, objects, and locations as well as the contextual information about the connections and measures to enable prioritization of files containing related pieces of content. The structure is organized to be a result of automated natural language\u00a0\u2026", "num_citations": "11\n", "authors": ["1190"]}
{"title": "Extraction of expanded entity phrases\n", "abstract": " This research is part of a larger integrated approach for extraction of information of interest from free text and the visualization of semantic relatedness between phrases of interest. This paper defines a new structure which is a key component, the expanded entity phrase (EPx). This paper also presents an approach for extracting EPx's from free text. The structure of the EPx's facilitates quantitative comparison with other EPx's. A combination of part of speech-based template matching and ontology-driven NLP provides an effective technique for extracting complex entity structures that cross clause boundaries. This approach also uses ontology-based inferences to lay the ground work for linking EPx's for semantic relatedness assessments involving different named entities not explicitly stated in the text. The real world data used in this research were derived from a collection of law enforcement email messages\u00a0\u2026", "num_citations": "11\n", "authors": ["1190"]}
{"title": "Geospatial Resource Description Framework (GRDF) and security constructs\n", "abstract": " The Semantic Web enables an automated, ontology based information aggregation mechanism. In geographic domain, automatic aggregation is a particularly important task in light of the over-abundance of data formats and types. Because the formats and types are not necessarily uniform or adhere to a particular information structure, aggregating geospatial data with differing formats is a challenging task. The aggregation is extremely useful in many areas such as business, academic, homeland security and public awareness. In this paper, we propose a set of geospatial constructs written in Web Ontology Language (OWL) and collectively referred to as Geographic Resource Description Framework (GRDF).Our goal is to propose a broad, semantics-aware and expressive language for geospatial domain. The most important advantage GRDF has over other geospatial languages is the ability to use logical\u00a0\u2026", "num_citations": "11\n", "authors": ["1190"]}
{"title": "Secure, highly available, and high performance peer-to-peer storage systems\n", "abstract": " Storage system is an important component in many data intensive applications, including data grid. Security, availability, and high performance are important issues in the storage system design. In this paper we present a peer-to-peer (P2P) storage system design based on distributed hash table (DHT) and short secret sharing (SSS) to provide highly available, secure and efficient data storage services. Existing DHTs do not consider share location and search. Also, storage systems using data partitioning schemes (including SSS) does not consider the severe problems in share update. We develop three access protocols to maintain the share consistency in spite of concurrent update, partial update and compromised storage nodes by storing a limit number of history versions of the shares. We also conducted experimental studies to evaluate the performance and data availability and compare the behaviors of the\u00a0\u2026", "num_citations": "11\n", "authors": ["1190"]}
{"title": "Assured information sharing: Technologies, challenges and directions\n", "abstract": " This paper describes issues, technologies, challenges, and directions for Assured Information Sharing (AIS). AIS is about organizations sharing information but at the same time enforcing policies and procedures so that the data is integrated and mined to extract nuggets. This is the first in a series of papers we are writing on AIS. It provides an overview including architectures, functions and policies for AIS. We assume that the partners of a coalition may be trustworthy, semi-trustworthy or untrustworthy and investigate solutions for AIS to handle the different scenarios.", "num_citations": "11\n", "authors": ["1190"]}
{"title": "Hypersemantic data modeling for inference analysis\n", "abstract": " Hypersemantic Data Modeling for Inference Analysis | Proceedings of the IFIP WG11.3 Working Conference on Database Security VII ACM Digital Library home ACM home Google, Inc. (search) Advanced Search Browse About Sign in Register Advanced Search Journals Magazines Proceedings Books SIGs Conferences People More Search ACM Digital Library SearchSearch Advanced Search Browse Browse Digital Library Collections More HomeBrowse by TitleProceedingsProceedings of the IFIP WG11.Working Conference on Database Security VIIHypersemantic Data Modeling for Inference Analysis ARTICLE Hypersemantic Data Modeling for Inference Analysis Share on Authors: Donald George Marks profile image Donald G. Marks View Profile , Leonard J Binns profile image Leonard J. Binns View Profile , Bhavani M Thuraisingham profile image Bhavani M. Thuraisingham View Profile Authors Info & \u2026", "num_citations": "11\n", "authors": ["1190"]}
{"title": "Multilevel security for information retrieval systems\n", "abstract": " In this paper, we describe multilevel security issues for information retrieval database management systems. We first discuss security issues for document representation; in particular, issues on developing an appropriate data model for representing multilevel information retrieval applications are given. Then we consider the security issues for document manipulation.", "num_citations": "11\n", "authors": ["1190"]}
{"title": "SQL extensions for security assertions\n", "abstract": " We describe the extensions to the relational query language SQL that are necessary to support multilevel security in database management systems. In a multilevel secure database with data at a variety of sensitivity levels. The data classifications are provided by security assertions. Consequently, the data definition language and the data manipulation language should provide support for incorporating these assertions.", "num_citations": "11\n", "authors": ["1190"]}
{"title": "Automated threat report classification over multi-source data\n", "abstract": " With an increase in targeted attacks such as advanced persistent threats (APTs), enterprise system defenders require comprehensive frameworks that allow them to collaborate and evaluate their defense systems against such attacks. MITRE has developed a framework which includes a database of different kill-chains, tactics, techniques, and procedures that attackers employ to perform these attacks. In this work, we leverage natural language processing techniques to extract attacker actions from threat report documents generated by different organizations and automatically classify them into standardized tactics and techniques, while providing relevant mitigation advisories for each attack. A na\u00efve method to achieve this is by training a machine learning model to predict labels that associate the reports with relevant categories. In practice, however, sufficient labeled data for model training is not always readily\u00a0\u2026", "num_citations": "10\n", "authors": ["1190"]}
{"title": "Stream mining using statistical relational learning\n", "abstract": " Stream mining has gained popularity in recent years due to the availability of numerous data streams from sources such as social media and sensor networks. Data mining on such continuous streams possess a variety of challenges including concept drift and unbounded stream length. Traditional data mining approaches to these problems have difficulty incorporating relational domain knowledge and feature relationships, which can be used to improve the accuracy of a classifier. In this work, we model large data streams using statistical relational learning techniques for classification, in particular, we use a Markov Logic Network to capture relational features in structured data and show that this approach performs better for supervised learning than current state-of-the-art approaches. Additionally, we evaluate our approach with semi-supervised learning scenarios, where class labels are only partially available\u00a0\u2026", "num_citations": "10\n", "authors": ["1190"]}
{"title": "State Actors' Offensive Cyberoperations: The Disruptive Power of Systematic Cyberattacks\n", "abstract": " During the last few years, several nation states have entered cyberspace in an attempt to use the Internet for policy, geopolitical, and state gains. Using development of the battle tank as an analogy, the authors explore the militarization of the Internet and how the ability to think beyond the existing cybersecurity paradigm has led to new cyberweapons and the need for new defensive strategies. The existing paradigm says that cyberspace is becoming more secure; the entrance of state actors reverses this trajectory, making cyberspace less secure and heavily contested.", "num_citations": "10\n", "authors": ["1190"]}
{"title": "From Cyber Terrorism to State Actors\u2019 Covert Cyber Operations\n", "abstract": " Abstracts:Historically, since the Internet started to become a common feature in our lives, hackers have been seen as a major threat. This view has repeatedly been entrenched and distributed by media coverage and commentaries through the years. Instead the first 20 years of the Internet was acceptably secure due to the limited abilities of the attackers, compared to the threat generated from a militarized Internet with state actors conducting cyber operations. In reality, the Internet has a reversed trajectory for its security; it has become more unsafe over time and moved from a threat to the individual to a national security threat. The entry of state actors creates a contested cyberspace where intelligence, economic espionage, information operations, and psychological operations radically changed the fundamentals for Internet security. The state actor seeks to exploit weaknesses in the targeted national infrastructure\u00a0\u2026", "num_citations": "10\n", "authors": ["1190"]}
{"title": "Design and implementation of a cloud-based assured information sharing system\n", "abstract": " The advent of cloud computing and the continuing movement toward software as a service (SaaS) paradigms have posed an increasing need for assured information sharing (AIS) as a service in the cloud. This paper describes the first of its kind assured information sharing system that operates in a cloud. The idea is for each organization to store their data and the information sharing policies in a cloud. The information is shared according to the policies. We describe a cloud-based information sharing framework that utilizes Semantic Web technologies; our framework consists of a policy engine that reasons about the policies for information sharing purposes and a secure data engine that stores and queries data in the cloud. We also describe the operation of our system with example policies.", "num_citations": "10\n", "authors": ["1190"]}
{"title": "Ranking ontologies using verified entities to facilitate federated queries\n", "abstract": " In view of the need for highly distributed and federated architecture, ranking ontologies from different data sources in a specific domain have great impact on the performance of web applications. Since ontologies for a same domain usually overlap, we aim to rank ontologies based on the commonality of overlapping entities and distance between each pair of ontologies. Overlapping entities are determined by considering entities and relationships between them in ontology's graph. First we find out the Common Subset of Entities (CSE) between two ontologies using the lexical and structural similarity of entities. Second, we propose a novel strategy to find the similarity between ontologies by an Entropy Based Distribution (EBD) measurement. Third, we rank some synthetic and non-synthetic ontologies based on EBD values by naive and clustering approaches. Finally, we compare our approach with an existing\u00a0\u2026", "num_citations": "10\n", "authors": ["1190"]}
{"title": "Query processing techniques for compliance with data confidence policies\n", "abstract": " Data integrity and quality is a very critical issue in many data-intensive decision-making applications. In such applications, decision makers need to be provided with high quality data on which they can rely on with high confidence. A key issue is that obtaining high quality data may be very expensive. We thus need flexible solutions to the problem of data integrity and quality. This paper proposes one such solution based on four key elements. The first element is the association of a confidence value with each data item in the database. The second element is the computation of the confidence values of query results by using lineage propagation. The third element is the notion of confidence policies. Such a policy restricts access to the query results by specifying the minimum confidence level that is required for use in a certain task by a certain subject. The fourth element is an approach to dynamically\u00a0\u2026", "num_citations": "10\n", "authors": ["1190"]}
{"title": "Assured information sharing life cycle\n", "abstract": " This paper describes our approach to assured information sharing. The research is being carried out under a MURI 9Multiuniversity Research Initiative) project funded by the air force office of scientific research (AFOSR). The main objective of our project is: define, design and develop an assured information sharing lifecycle (AISL) that realizes the DoD's information sharing value chain. In this paper we describe the problem faced by the department of defense and our solution to developing an AISL system.", "num_citations": "10\n", "authors": ["1190"]}
{"title": "A better approximation for minimum average routing path clustering problem in 2-D underwater sensor networks\n", "abstract": " Previously, we proposed Minimum Average Routing Path Clustering Problem (MARPCP) in multi-hop USNs. The goal of this problem is to find a clustering of a USN so that the average clustering-based routing path from a node to it nearest underwater sink is minimized. We relaxed MARPCP to a special case of Minimum Weight Dominating Set Problem (MWDSP), namely MWDSP-R. In addition, we showed the Performance Ratio (PR) of \u03b1-approximation algorithm for MWDSP-R is 3\u03b1 for MARPCP. Based on this result, we showed the existence of a (15 + \u220a)-approximation algorithm for MARPCP. In this paper, we first establish the NP-completeness of both MARPCP and MWDSP-R. Then, we propose a PTAS for MWDSP-R. By combining this result with our previous one, we have a (3 + \u220a)-approximation algorithm for MARPCP.", "num_citations": "10\n", "authors": ["1190"]}
{"title": "A data mining technique to detect remote exploits\n", "abstract": " We design and implement DExtor, a Data Mining based Exploit code detector, to protect network services. The main assumption of our work is that normal traffic into the network services contain only data, whereas exploit code contains code. Thus, the \u201cexploit code detection\u201d problem reduces to \u201ccode detection\u201d problem. DExtor is an application-layer attack blocker, which is deployed between a web service and its corresponding firewall. The system is first trained with real training data containing both exploit code, and normal traffic. Training is performed by applying binary disassembly on the training data, extracting features, and training a classifier. Once trained, DExtor is deployed in the network to detect exploit code and protect the network service. We evaluate DExtor with a large collection of real exploit code and normal data. Our results show that DExtor can detect almost all exploit code with negligible false alarm rate. We also compare DExtor with other published works and prove its effectiveness.", "num_citations": "10\n", "authors": ["1190"]}
{"title": "Secure peer-to-peer networks for trusted collaboration\n", "abstract": " An overview of recent advances in secure peer-to-peer networking is presented, toward enforcing data integrity, confidentiality, availability, and access control policies in these decentralized, distributed systems. These technologies are combined with reputation-based trust management systems to enforce integrity-based discretionary access control policies. Particular attention is devoted to the problem of developing secure routing protocols that constitute a suitable foundation for implementing this security system. The research is examined as a basis for developing a secure data management system for trusted collaboration applications such as e-commerce, situation awareness, and intelligence analysis.", "num_citations": "10\n", "authors": ["1190"]}
{"title": "Security for Distributed Databases\n", "abstract": " Security for Distributed Databases \u00d7 Close The Infona portal uses cookies, ie strings of text saved by a browser on the user's device. The portal can access those files and use them to remember the user's data, such as their chosen settings (screen view, interface language, etc.), or their login data. By using the Infona portal the user accepts automatic saving and using this information for portal operation purposes. More information on the subject can be found in the Privacy Policy and Terms of Service. By closing this window the user confirms that they have read the information on cookie usage, and they accept the privacy policy and the way cookies are used by the portal. You can change the cookie settings in your browser. I accept Polski English Login or register account remember me Password recovery INFONA - science communication portal INFONA Search advanced search Browse series books journals articles \u2026", "num_citations": "10\n", "authors": ["1190"]}
{"title": "Concurrency control in trusted database management systems: A survey\n", "abstract": " Recently several algorithms have been proposed for concurrency control in a Trusted Database Management System (TDBMS). The various research efforts are examining the concurrency control algorithms developed for DBMSs and adapting them for a multilevel environment. This paper provides a survey of the concurrency control algorithms for a TDBMS and discusses future directions.", "num_citations": "10\n", "authors": ["1190"]}
{"title": "XIMKON: an expert simulation and control program\n", "abstract": " Artificial intelligence (AI) and object-oriented techniques are applied to the development of XIMKON, an expert software system for simulation and control engineering of dynamic processes. As an enhancement to a broad FORTRAN base of control design tools, XIMKON incorporates the principles of AI planners and advanced user interface technologies in an effort to automate the procedures involved in modeling, analysis and synthesis of control systems. This paper describes the current state of XIMKON and discusses early results of its implementation for process control applications.", "num_citations": "10\n", "authors": ["1190"]}
{"title": "A framework for secure data collection and management for Internet of Things\n", "abstract": " More and more current industrial control systems (eg, smart grids, oil and gas systems, connected cars and trucks) have the capability to collect and transmit users' data in order to provide services that are tailored to the specific needs of the customers. Such smart industrial control systems fall into the category of Internet of Things (IoT). However, in many cases, the data transmitted by such IoT devices includes sensitive information and users are faced with an all-or-nothing choice: either they adopt the proposed services and release their private data, or refrain from using services which could be beneficial but pose significant privacy risks. Unfortunately, encryption alone does not solve the problem, though techniques to counter these privacy risks are emerging (eg, by using applications that alter, merge or bundle data to ensure they cannot be linked to a particular user). In this paper, we propose a general\u00a0\u2026", "num_citations": "9\n", "authors": ["1190"]}
{"title": "Measuring expertise and bias in cyber security using cognitive and neuroscience approaches\n", "abstract": " Toward the ultimate goal of enhancing human performance in cyber security, we attempt to understand the cognitive components of cyber security expertise. Our initial focus is on cyber security attackers - often called \u201chackers\u201d. Our first aim is to develop behavioral measures of accuracy and response time to examine the cognitive processes of pattern-recognition, reasoning and decision-making that underlie the detection and exploitation of security vulnerabilities. Understanding these processes at a cognitive level will lead to theory development addressing questions about how cyber security expertise can be identified, quantified, and trained. In addition to behavioral measures our plan is to conduct a functional magnetic resonance imaging (fMRI) study of neural processing patterns that can differentiate persons with different levels of cyber security expertise. Our second aim is to quantitatively assess the impact of\u00a0\u2026", "num_citations": "9\n", "authors": ["1190"]}
{"title": "Redact: A framework for sanitizing rdf data\n", "abstract": " Resource Description Framework (RDF) is the foundational data model of the Semantic Web, and is essentially designed for integration of heterogeneous data from varying sources. However, lack of security features for managing sensitive RDF data while sharing may result in privacy breaches, which in turn, result in loss of user trust. Therefore, it is imperative to provide an infrastructure to secure RDF data. We present a set of graph sanitization operations that are built as an extension to SPARQL. These operations allow one to sanitize sensitive parts of an RDF graph and further enable one to build more sophisticated security and privacy features, thus allowing RDF data to be shared securely.", "num_citations": "9\n", "authors": ["1190"]}
{"title": "Secure information integration with a semantic web-based framework\n", "abstract": " In this paper we describe the design and implementation of a semantic web based framework for secure information integration. In particular, we have evaluated Amazon's Simple Storage Service's ability to provide storage support for large-scale semantic data used by a semantic web-based framework called Blackbook. We describe cryptographic techniques for enforcing the protection of published data on Amazon S3. We also explore access control issues associated with such services and provide a solution using Sun's implementation of eXtensible Access Control Markup Language (XACML).", "num_citations": "9\n", "authors": ["1190"]}
{"title": "Secure data processing in a hybrid cloud\n", "abstract": " Cloud computing has made it possible for a user to be able to select a computing service precisely when needed. However, certain factors such as security of data and regulatory issues will impact a user's choice of using such a service. A solution to these problems is the use of a hybrid cloud that combines a user's local computing capabilities (for mission- or organization-critical tasks) with a public cloud (for less influential tasks). We foresee three challenges that must be overcome before the adoption of a hybrid cloud approach: 1) data design: How to partition relations in a hybrid cloud? The solution to this problem must account for the sensitivity of attributes in a relation as well as the workload of a user; 2) data security: How to protect a user's data in a public cloud with encryption while enabling query processing over this encrypted data? and 3) query processing: How to execute queries efficiently over both, encrypted and unencrypted data? This paper addresses these challenges and incorporates their solutions into an add-on tool for a Hadoop and Hive based cloud computing infrastructure.", "num_citations": "9\n", "authors": ["1190"]}
{"title": "Bi-directional translation of relational data into virtual RDF stores\n", "abstract": " A vast majority of the world's valuable data currently exists in relational databases and other legacy storage systems. In order for Semantic Web applications to access such legacy data without replication or synchronization of the same, the gap between the two needs to be bridged. Several efforts exist that publish relational data as Resource Description Framework (RDF) triples, however almost all current work in this arena is uni-directional, presenting the existing and new data from an underlying relational database into a corresponding virtual RDF store in a read-only manner. This paper expands on previous relational-to-RDF bridging work, by enabling the bridge to be bi-directional and allowing data updates specified as triples to be propagated back to the relational database as tuples. Algorithms to translate the triples to be updated/inserted/deleted into equivalent relational attributes/tuples whenever possible\u00a0\u2026", "num_citations": "9\n", "authors": ["1190"]}
{"title": "Design and implementation of a secure social network system\n", "abstract": " Context-based anomaly tracking represents a new approach to security enhancement of communication streams. By creating a system that develops an understanding of normal and abnormal based on communication history, it is possible to detect fluctuations in an evolving social network. Although more research is necessary to overcome current obstacles, the combination of social network analysis and anomaly detection techniques yields a promising set of applications for enhancing communication security. In this paper we will describe a system for context-based anomaly detection and then describe experiments for message surveillance application.", "num_citations": "9\n", "authors": ["1190"]}
{"title": "Measuring anonymization privacy/analysis tradeoffs inherent to sharing network data\n", "abstract": " Sharing of network data between organizations is desperately needed as attackers bounce between targets in different security domains and launch attacks across security domains. Anonymization to protect private/sensitive information has emerged as a promising approach to sharing network data between security domains. However, a fundamental tradeoff exists between the anonymization of data for privacy protection and the utility of anonymized for security analysis. While many researchers have referred to this tradeoff, no one has characterized it with testing. In this paper we present a testing framework we have developed to characterize privacy/analysis anonymization tradeoffs along with some preliminary results.", "num_citations": "9\n", "authors": ["1190"]}
{"title": "Toward trusted sharing of network packet traces using anonymization: Single-field privacy/analysis tradeoffs\n", "abstract": " Network data needs to be shared for distributed security analysis. Anonymization of network data for sharing sets up a fundamental tradeoff between privacy protection versus security analysis capability. This privacy/analysis tradeoff has been acknowledged by many researchers but this is the first paper to provide empirical measurements to characterize the privacy/analysis tradeoff for an enterprise dataset. Specifically we perform anonymization options on single-fields within network packet traces and then make measurements using intrusion detection system alarms as a proxy for security analysis capability. Our results show: (1) two fields have a zero sum tradeoff (more privacy lessens security analysis and vice versa) and (2) eight fields have a more complex tradeoff (that is not zero sum) in which both privacy and analysis can both be simultaneously accomplished.", "num_citations": "9\n", "authors": ["1190"]}
{"title": "Confidentiality, privacy and trust policy enforcement for the semantic web\n", "abstract": " In this position paper we describe aspects of securing the semantic Web. In particular, we discuss ways of enforcing confidentiality privacy and trust polices. We also discuss our research on secure geospatial semantic Web. Our application of secure semantic Web technologies for assured information sharing is also discussed.", "num_citations": "9\n", "authors": ["1190"]}
{"title": "An adaptable perturbation model of privacy preserving data mining\n", "abstract": " Several approaches to privacy preserving data mining have been developed in recent years. These approaches can be classified into two main categories; they are based on perturbation and randomization techniques [1-4] and secure multi-party computation based techniques (SMC)[5-9]. The approach proposed by Kargupta et. al in [10] poses a challenge to the perturbation and randomization-based approaches. It claims that such approaches may lose information as well as not provide privacy by introducing random noise to the data. By using random matrix properties, Kargupta et. al successfully separates the data from the random noise and subsequently discloses the original data. Several approaches [5-9] fall into the second category (ie the multi-party computation), but they all require very high computation costs. Furthermore, these multi-party computation based approaches assume that each party uses the\u00a0\u2026", "num_citations": "9\n", "authors": ["1190"]}
{"title": "Privacy and civil liberties\n", "abstract": " This paper describes the proceedings of the panel on privacy and civil liberties. It presents the panelists\u2019 positions and considers the issue of privacy from the tecnological and legislative perspectives.", "num_citations": "9\n", "authors": ["1190"]}
{"title": "RT-OMT: A real-time object modeling technique for designing real-time database applications\n", "abstract": " This paper describes a methodology called RT-OMT (Real-time Object Modeling Technique) for designing real-time database applications. RT-OMT adapts the OMT (Object Modeling Technique) methodology for this purpose. OMT is one of the more popular object-oriented methodologies for designing applications for complex information systems. These include database systems, executive/enterprise information systems, collaborative computing systems, medical information systems, and hypermedia systems. This paper proposes an enhanced real-time OMT, by defining a real-time object model, a real-time dynamic model, and a real-time functional model for modeling and analysis of real-time database applications.< >", "num_citations": "9\n", "authors": ["1190"]}
{"title": "The inference problem in database security\n", "abstract": " The inference problem in database security | Cipher: IEEE Computer Society Technical Committee Newsletter on Security & Privacy ACM Digital Library home ACM home Google, Inc. (search) Advanced Search Browse About Sign in Register Advanced Search Journals Magazines Proceedings Books SIGs Conferences People More Search ACM Digital Library SearchSearch Advanced Search Cipher: IEEE Computer Society Technical Committee Newsletter on Security & Privacy Periodical Home Latest Issue Archive Authors Affiliations Award Winners More HomeBrowse by TitlePeriodicalsCipher: IEEE Computer Society Technical Committee Newsletter on Security & PrivacyWinter 1991The inference problem in database security article The inference problem in database security Share on Author: Bhavani M Thuraisingham profile image Bhavani Thuraisingham View Profile Authors Info & Affiliations Publication: \u2026", "num_citations": "9\n", "authors": ["1190"]}
{"title": "Saccos: A semi-supervised framework for emerging class detection and concept drift adaption over data streams\n", "abstract": " In this paper, we address the challenges of detecting instances from emerging classes over a non-stationary data stream during classification. In particular, instances from an entirely unknown class may appear in a data stream over time. Existing classification techniques utilize unsupervised clustering to identify the emergence of such data instances. Unfortunately, they make strong assumptions which are typically invalid in practice; (i) Most instances associated with a class are closer to each other in feature space than instances associated with different classes, (ii) Covariates of data are normalized through an oracle to overcome the effect of a few data instances having large feature values, and (iii) Labels of instances from emerging classes are readily available soon after detection. To address the challenges that occur in practice when the above assumptions are weak, we propose a practical semi-supervised\u00a0\u2026", "num_citations": "8\n", "authors": ["1190"]}
{"title": "Database security: Past, present, and future\n", "abstract": " Database security has received a great deal of attention since the mid-1970s starting from discretionary security for System R and Ingres to access control models to multilevel database systems to security for emerging data management systems to data privacy to privacy-preserving data mining to data mining for security applications to big data security and privacy. This paper describes the developments in database security since 1975 and provides a vision for the future.", "num_citations": "8\n", "authors": ["1190"]}
{"title": "Evolving stream classification using change detection\n", "abstract": " Classifying instances in evolving data stream is a challenging task because of its properties, e.g., infinite length, concept drift, and concept evolution. Most of the currently available approaches to classify stream data instances divide the stream data into fixed size chunks to fit the data in memory and process the fixed size chunk one after another. However, this may lead to failure of capturing the concept drift immediately. We try to determine the chunk size dynamically by exploiting change point detection (CPD) techniques on stream data. In general, the distribution families before and after the change point are unknown over the stream, therefore non-parametric CPD algorithms are used in this case. We propose a multi-dimensional non-parametric CPD technique to determine chunk boundary over data streams dynamically which leads to better models to classify instances of evolving data streams. Experimental\u00a0\u2026", "num_citations": "8\n", "authors": ["1190"]}
{"title": "Societal cyberwar theory applied: the disruptive power of state actor aggression for public sector information security\n", "abstract": " The modern welfare state faces significant challenges to be able to sustain a systematic cyber conflict that pursues the institutional destabilization of the targeted state. Cyber defense in these advanced democracies are limited, unstructured, and focused on anecdotal cyber interchanges of marginal geopolitical value. The factual reach of government activities once a conflict is initiated is likely to be miniscule. Therefore the information security activities, and assessments leading to cyber defense efforts, have to be strategically pre-event coordinated within the state. This coordination should be following a framework that ensures institutional stability, public trust, and limit challenges to the state. The paper presents a case to use societal cyber war theory to create a public sector cyber defense strategy beforehand facing a massive state actor initiated automated systematic cyber attacks to limit the risk for a societal\u00a0\u2026", "num_citations": "8\n", "authors": ["1190"]}
{"title": "Randomizing smartphone malware profiles against statistical mining techniques\n", "abstract": " The growing use of smartphones opens up new opportunities for malware activities such as eavesdropping on phone calls, reading e-mail and call-logs, and tracking callers\u2019 locations. Statistical data mining techniques have been shown to be applicable to detect smartphone malware. In this paper, we demonstrate that statistical mining techniques are prone to attacks that lead to random smartphone malware behavior. We show that with randomized profiles, statistical mining techniques can be easily foiled. Six in-house proof-of-concept malware programs are developed on the Android platform for this study. The malware programs are designed to perform privacy intrusion, information theft, and denial of service attacks. By simulating and tuning the frequency and interval of attacks, we aim to answer the following questions: 1) Can statistical mining algorithms detect smartphone malware by monitoring the\u00a0\u2026", "num_citations": "8\n", "authors": ["1190"]}
{"title": "Identification of related information of interest across free text documents\n", "abstract": " An approach is presented for finding information of interest in a free text document and then identifying and presenting related information of interest from other free text documents. The goal is to find specific related items of interest within documents whether the documents are of the same category or not. Information of interest is defined with respect to expanded entity phrases and their ontology mappings. Powerful techniques requiring minimal training are described for expanding an entity phrase to include attributes from components of a complex sentence; for measuring relatedness of same-name expanded entity phrases; and for detecting related expanded entity phrases through ontology inferences. A representative dataset is described and preliminary measurements of performance against ground truth are provided.", "num_citations": "8\n", "authors": ["1190"]}
{"title": "WS-Sim: A web service simulation toolset with realistic data support\n", "abstract": " Service-oriented computing has become the most promising paradigm for system integration and interoperation. Many different models and mechanisms have been proposed to achieve security, dependability, real-time, etc. capabilities in service composition and execution. To facilitate the evaluation of these mechanisms, especially their performance impacts, it is generally desirable to have a web service simulation environment. However, many existing simulations are based on specific cases and specialized for evaluating specific mechanisms. In some cases, the simulations are based on randomly generated data to simulate the property of the simulated web services without the support of realistic data from actual web services and/or applications. In this paper, we take the first step towards building a general and more practical web service simulation system, WS-Sim. We explore many existing web services and\u00a0\u2026", "num_citations": "8\n", "authors": ["1190"]}
{"title": "A construction of Cartesian authentication code from orthogonal spaces over a finite field of odd characteristic\n", "abstract": " In this paper, we construct a Cartesian authentication code from subspaces of orthogonal space  of odd characteristic and compute its parameters. Assuming that the encoding rules of the transmitter and the receiver are chosen according to a uniform probability distribution, the probabilities of successful impersonation attack and substitution attack are also computed.", "num_citations": "8\n", "authors": ["1190"]}
{"title": "Semanticaware data protection in web services\n", "abstract": " This paper presents a method to remove the dependency of XML access control models on the syntactic representation of the XML trees. We propose a semantics-based approach, expressing XML access control on ontologies that describe the XML documents. The semanticsbased model is transformed to the syntactic representation on the actual XML instance. Our method supports the uniform enforcement of an authentication model on syntactically different but semantically equivalent XML documents.", "num_citations": "8\n", "authors": ["1190"]}
{"title": "Security Management, Integrity, and Internal Control in Information Systems: IFIP TC-11 WG 11.1 & WG 11.5 Joint Working Conference\n", "abstract": " This is the first joint working conference between the IFIP Working Groups 11. 1 and 11. 5. We hope this joint conference will promote collaboration among researchers who focus on the security management issues and those who are interested in integrity and control of information systems. Indeed, as management at any level may be increasingly held answerable for the reliable and secure operation of the information systems and services in their respective organizations in the same manner as they are for financial aspects of the enterprise, there is an increasing need for ensuring proper standards of integrity and control in information systems in order to ensure that data, software and, ultimately, the business processes are complete, adequate and valid for intended functionality and expectations of the owner (ie the user organization). As organizers, we would like to thank the members of the international program committee for their review work during the paper selection process. We would also like to thank the authors of the invited papers, who added valuable contribution to this first joint working conference. Paul Dowland X. Sean Wang December 2005 Contents Preface vii Session 1-Security Standards Information Security Standards: Adoption Drivers (Invited Paper) 1 JEAN-NOEL EZINGEARD AND DAVID BIRCHALL Data Quality Dimensions for Information Systems Security: A Theorectical Exposition (Invited Paper) 21 GURVIRENDER TEJAY, GURPREET DHILLON, AND AMITA GOYAL CHIN From XML to RDF: Syntax, Semantics, Security, and Integrity (Invited Paper) 41 C. FARKAS, V. GowADiA, A. JAIN, AND D.", "num_citations": "8\n", "authors": ["1190"]}
{"title": "Dependable semantic web\n", "abstract": " This paper describes some ideas for a dependable Semantic Web. Semantic Web is a technology for understanding Web pages. It is important that the Semantic Web is secure. In addition, data exchanged by the Web has to be of high quality. The processes that the Web supports have to meet certain timing constraints. This paper discusses these aspects, and describes how they provide a dependable Semantic Web.", "num_citations": "8\n", "authors": ["1190"]}
{"title": "Classic work: The balanced scorecard: Learning and growth perspective\n", "abstract": " The fourth and final perspective on the Balanced Scorecard develops objectives and measures to drive organizational learning and growth. The objectives established in the financial, customer, and internal-businessprocess perspectives identify where the organization must excel to achieve breakthrough performance. The objectives in the learning and growth perspective provide the infrastructure to enable ambitious objectives in the other three perspectives to be achieved. Objectives in the learning and growth perspective are the drivers for achieving excellent outcomes in the first three scorecard perspectives. Managers in several organizations have noted that when they were evaluated solely on short-term financial performance, they often found it difficult to sustain investments to enhance the capability of their people, systems, and organizational processes. Expenditures on such investments are treated as period expenses by the financial accounting model so that cutbacks in these investments are an easy way to produce incremental short-term earnings. The adverse long-term consequences of consistent failure to enhance employee, systems, and organizational capabilities will not show up in the short run, and when they do, these managers reason, it may be on somebody else's\" watch.\" The Balanced Scorecard stresses the importance of investing for the future, and not just in traditional areas for investment, such as new equipment and new product research and development. Equipment and R&D investments are certainly important but they are unlikely to be sufficient by themselves. Organizations must also invest in their infrastructure\u2014", "num_citations": "8\n", "authors": ["1190"]}
{"title": "Security and privacy issues for the World Wide Web: Panel discussion\n", "abstract": " This is the second in a series of panels at the IFIP 11.3 Working Conference on Database and Application Security. While the first panel in 1997 focussed on data warehousing, data mining and security, the panel in 1998 focussed on web security with discussions on data warehousing and data mining.", "num_citations": "8\n", "authors": ["1190"]}
{"title": "Data allocation and spatio-temporal implications for video-on-demand systems\n", "abstract": " As the number of video streams to be supported by a digital video delivery system (DVDS) increases, we more and more start to understand that the ability to reliably and cost-efficiently support a considerable number of video streams (in the magnitude of tens of thousands) largely depends on software capabilities. Even in the presence of the best hardware configuration (not ignoring software vs hardware costs), the software exploitation of the hardware capabilities is of paramount importance. It is imperative that current software developments account for the eventual scalability of the number of video streams without commensurate increase in hardware. We present strategies for the management of video streams in order to maintain and satisfy their space and time requirements. We present a detailed analysis of the issues related to queuing I/O requests and data buffering. The designs for the arrangement and\u00a0\u2026", "num_citations": "8\n", "authors": ["1190"]}
{"title": "The Role of Artificial Intelligence and Cyber Security for Social Media\n", "abstract": " Social media systems such as Facebook and Twitter are playing a major role in society connecting over a billion people worldwide and enabling them to communicate and share information with each other as well as within a group of individuals. These social media systems can vastly help humanity such as spread information about infectious diseases and discuss solutions to problems faced by humanity including preventing child trafficking and violence against women. However, social media systems can also do harm, such as shared false information, more popularly known as fake news, as well as violating the privacy of individuals. With the proliferation of Artificial Intelligence (AI) systems together with powerful machine learning techniques as well as the cyber-attacks on information systems are changing the way the social media systems are being used by humans. This paper discusses the role of both AI and\u00a0\u2026", "num_citations": "7\n", "authors": ["1190"]}
{"title": "Large-scale realistic network data generation on a budget\n", "abstract": " Many novel problems in computer networking require relevant network trace data during the research process. Unfortunately, such data can often be hard to find, which becomes a problem within itself. While generating appropriate data using in-lab network testbeds and simulators are feasible solutions, the former has limitations in terms of network scale, while the latter has limitations in the generated data. To help address these issues, we present an approach for the generation of realistic network trace data in a contained, large-scale network environment. We use network emulation to enable large-scale, in-lab networking, and a software framework we developed to support autonomous client-side protocols and services, including user-behavioral models which scale in a shared CPU environment. Our framework also enables quick experiment setup and monitoring. We show through experimentation on a low\u00a0\u2026", "num_citations": "7\n", "authors": ["1190"]}
{"title": "Security and Privacy in Communication Networks: 11th International Conference, SecureComm 2015, Dallas, TX, USA, October 26-29, 2015, Revised Selected Papers\n", "abstract": " This volume constitutes the thoroughly refereed post-conference proceedings of the 11th International Conference on Security and Privacy in Communication Networks, SecureComm 2015, held in Dallas, TX, USA, in October 2015. The 29 regular and 10 poster papers presented were carefully reviewed and selected from 107 submissions. It also presents 9 papers accepted of the workshop on Applications and Techniques in Cyber Security, ATCS 2015. The papers are grouped in the following topics: mobile, system, and software security; cloud security; privacy and side channels; Web and network security; crypto, protocol, and model.", "num_citations": "7\n", "authors": ["1190"]}
{"title": "Risk-aware data processing in hybrid clouds\n", "abstract": " This paper explores query processing in a hybrid cloud model where a users local computing capability is exploited alongside public cloud services to deliver an efficient and secure data management solution. Hybrid clouds offer numerous economic advantages including the ability to better manage data privacy and confidentiality, as well as exerting control on monetary expenses of consuming cloud services by exploiting local resources. Nonetheless, query processing in hybrid clouds introduces numerous challenges, the foremost of which is, how to partition data and computation between the public and private components of the cloud. The solution must account for the characteristics of the workload that will be executed, the monetary costs associated with acquiringoperating cloud services as well as the risks affiliated with storing sensitive data on a public cloud. This paper proposes a principled framework for distributing data and processing in a hybrid cloud that meets the conflicting goals of performance, disclosure risk and resource allocation cost. The proposed solution is implemented as an add-on tool for a Hadoop and Hive based cloud computing infrastructure.Descriptors:", "num_citations": "7\n", "authors": ["1190"]}
{"title": "Relationalization of provenance data in complex RDF reification nodes\n", "abstract": " The plethora of information available to today\u2019s users due to the Internet phenomenon has brought forth an associated concern, namely, determination of the trustworthiness of information. Provenance information, such as who is responsible for the data or how the data came to be, plays a pivotal role in addressing this concern by providing additional facts that could serve as a basis for establishing the authenticity of information. Awareness of the importance of data provenance has ensured that current technologies include support for the ability to record provenance information. These include Semantic Web technologies such as Resource Description Framework (RDF) that records data provenance through the process of reification. Reification enables the association of a level of trust with RDF triples, thereby enabling the validation of the authenticity of the triples. RDF\u2019s rapid acceptance has created an\u00a0\u2026", "num_citations": "7\n", "authors": ["1190"]}
{"title": "Challenges and future directions of software technology: Secure software development\n", "abstract": " Developing large scale software systems has major security challenges. This paper describes the issues involved and then addresses two topics: formal methods for emerging secure systems and secure services modeling.", "num_citations": "7\n", "authors": ["1190"]}
{"title": "Using OWL to model role based access control\n", "abstract": " Current access control research follows two parallel themes: many efforts focus on developing novel access control models meeting the policy needs of real world application domains while others are exploring new policy languages. This paper is motivated by the desire to develop a synergy between these themes facilitated by OWL. Our vision for the future is a world where advanced access control concepts are embodied in models that are supported by policy languages in a natural intuitive manner, while allowing for details beyond the models to be further specified in the policy language. In this paper we specifically study the relationship between the Web Ontology Language (OWL) and the Role Based Access Control (RBAC) model. Although OWL is a web ontology language and not specifically designed for expressing authorization policies, it has been used successfully for this purpose in previous work. We show two different ways to support the NIST Standard RBAC model in OWL and then discuss how the OWL constructions can be extended to model attribute-based RBAC or more generally attribute-based access control.", "num_citations": "7\n", "authors": ["1190"]}
{"title": "Centralized security labels in decentralized P2P networks\n", "abstract": " This paper describes the design of a peer-to-peer network that supports integrity and confidentiality labeling of shared data. A notion of data ownership privacy is also enforced, whereby peers can share data without revealing which data they own. Security labels are global but the implementation does not require a centralized label server. The network employs a reputation-based trust management system to assess and update data labels, and to store and retrieve labels safely in the presence of malicious peers. The security labeling scheme preserves the efficiency of network operations; lookup cost including label retrieval is O(log N), where N is the number of agents in the network.", "num_citations": "7\n", "authors": ["1190"]}
{"title": "Dependable and secure TMO scheme\n", "abstract": " In a real-time distributed computing environment, security is critical to protect the system from unauthorized access especially since such systems are being used in time critical applications. Access control mechanisms have been introduced during the last several decades and have offered a basic and powerful means for enforcing security. In this paper, we examine the concepts of the TMO (time triggered message triggered object) scheme that provides guaranteed real-time services in a distributed object computing environment. We also examine access control mechanisms; such as the traditional model, the RBAC (role-based access control) model and the UCON (usage control) model. The main contribution of this paper is applying the traditional, RBAC and UCON models to the TMO scheme in order to provide a secure real-time distributed environment", "num_citations": "7\n", "authors": ["1190"]}
{"title": "Real-time data mining of multimedia objects\n", "abstract": " Whereas much of the previous work on data mining has focused on mining data in relational databases, we discuss mining objects. Object models are very popular for representing multimedia data, and therefore we need to mine object databases to extract useful information from the large quantities of multimedia data. We first describe the motivation for multimedia data mining with examples and then discuss object mining with focus on text, image, video and audio mining. We also address the need for real time data mining for multimedia applications.", "num_citations": "7\n", "authors": ["1190"]}
{"title": "CORBA-based real-time trader service for adaptable command and control systems\n", "abstract": " The paper describes an approach to building adaptable real time command and control (C2) systems. In particular it presents an overview of the Adaptable Real-Time Distributed Object Management (ARTDOM) project in progress at the MITRE Corporation. This project is currently developing real time extensions for the Common Object Request Broker Architecture (CORBA) Trading Object Service. The goal of the project is to demonstrate how current C2 systems can be more easily upgraded and made more adaptable by using emerging distributed computing technology. This goal is being accomplished by investigating and developing real time middleware that is reflexive (i.e., capable of examining its current state and processing demands) and self adapting (i.e., capable of reconfiguring itself based on its reflective findings).", "num_citations": "7\n", "authors": ["1190"]}
{"title": "Handbook of Data Management\n", "abstract": " Data management is the process of understanding the data needs of an enterprise and making that data optimally available to support the opera-tions of the enterprise. It includes methods for providing integrated access to one or more data bases, possibly heterogeneous in nature, as well as methods for designing and maintaining the data bases. Data management often includes many tasks, such as mass storage management, distributed processing, knowledge management, information management, and end-user support. The ultimate goal of data management is to provide the seamless access and fusion of massive amounts of data, information, and knowledge in a heterogeneous and real-time environment to carry out the functions of an enterprise. To achieve this vision, massive, multimedia, and heteroge-neous data bases must be integrated so that users can perform queries and obtain relevant information, as well as update the information in a timely manner.", "num_citations": "7\n", "authors": ["1190"]}
{"title": "Security issues for data warehousing and data mining\n", "abstract": " This paper describes security issues for data warehousing and data mining. It first provides an overview of data warehousing and security issues for data warehouses. Then a discussion of data mining, security implications of data mining, as well as data mining as a tool to handle security problems are given.", "num_citations": "7\n", "authors": ["1190"]}
{"title": "Application of object-oriented technology for integrating heterogeneous database systems\n", "abstract": " This paper describes the current challenges on the interoperability of heterogeneous database management systems and then focusses on the role of the objectoriented approach to facilitate the interconnection of multiple database systems.", "num_citations": "7\n", "authors": ["1190"]}
{"title": "Novel approaches to the inference problem\n", "abstract": " In this paper we describe the inference problem in database security and discuss four novel approaches which could possibly be used in order to build inference controllers in the long term which mimic certain aspects of human reasoning. These approaches utilize techniques from inductive inference, probabilistic deduction, mathematical programming, and game theory. Much research needs to be done before these approaches can be implemented. Therefore they can be regarded as\" nearly impossible problems\" given the current state-of-the-art technology. However, recent developments in artificial intelligence show much promise for these approaches.", "num_citations": "7\n", "authors": ["1190"]}
{"title": "Cyber security and artificial intelligence for cloud-based internet of transportation systems\n", "abstract": " The Internet of Things (IoT) has major implications in the transportation industry. Autonomous Vehicles (AVs) aim at improving day-to-day activities such as delivering packages, improving traffic, and the transportations of goods. AVs are not limited to ground vehicles but also include aerial and sea vehicles with a wide range of applications. The IoT systems consisting of a collection of AVs have come to be known as the Internet of Transportation systems. While such IoT systems manage large quantities of sensor data, much of the data is also sent to a cloud for offline analysis. While there is great potential in AVs and the improvements it can make to the transportation industry, security and privacy concerns pose new challenges that need to be addressed as we move forward. In addition, Artificial Intelligence techniques are also becoming crucial for such IoT systems to be able to intelligently manage the AVs. This\u00a0\u2026", "num_citations": "6\n", "authors": ["1190"]}
{"title": "Privacy-preserving architecture for cloud-IoT platforms\n", "abstract": " We propose a cloud-IoT architecture, called Data Bank, aiming at protecting users' sensitive data by allowing them to control which kind of data is transmitted by their devices and providing supportive tools for agreement visualisation and privacy-utility trade-off. The architecture consists of several layers, from IoT objects in the lower layer to web and mobile applications in the top layer, with regulated communication mechanisms to transfer data from the lower level to data processing services in the top level. We illustrate our proposal with an example in a smart vehicle environment.", "num_citations": "6\n", "authors": ["1190"]}
{"title": "Using deep learning to generate relational honeydata\n", "abstract": " Although there has been a plethora of work in generating deceptive applications, generating deceptive data that can easily fool attackers received very little attention. In this book chapter, we discuss our secure deceptive data generation framework that makes it hard for an attacker to distinguish between the real versus deceptive data. Especially, we discuss how to generate such deceptive data using deep learning and differential privacy techniques. In addition, we discuss our formal evaluation framework.", "num_citations": "6\n", "authors": ["1190"]}
{"title": "Lifting the smokescreen: Detecting underlying anomalies during a ddos attack\n", "abstract": " While DDoS attacks have become an ever-growing threat in the last decade, a new variation is taking root in which the DDoS is used as a distraction or smokescreen to hide other malicious activity. This variation, which we call DDoS as a Smokescreen (DaaSS), often result in data theft and financial loss, and often are only detected because the theft is discovered independently, long after the attack has ceased. In this work, we set out to describe these attacks and present a novel approach to detect them using real-world network trace data. We present experimental results showing promise that DaaSS attacks can be detected in a manner conducive to practical deployment.", "num_citations": "6\n", "authors": ["1190"]}
{"title": "Hacking social network data mining\n", "abstract": " Over the years social network data has been mined to predict individuals' traits such as intelligence and sexual orientation. While mining social network data can provide many beneficial services to the user such as personalized experiences, it can also harm the user when used in making critical decisions such as employment. In this work, we investigate the reliability of applying data mining techniques on social network data to predict various individual traits. In spite of the preliminary success of such data mining applications, in this paper, we demonstrate the vulnerabilities of existing state of the art social network data mining techniques when they are facing malicious attacks. Our results indicate that making critical decisions, such as employment or credit approval, based solely on social network data mining results is still premature at this stage. Specifically, we explore Facebook likes data for predicting the traits of\u00a0\u2026", "num_citations": "6\n", "authors": ["1190"]}
{"title": "Intelligent mapreduce based framework for labeling instances in evolving data stream\n", "abstract": " In our current work, we have proposed a multi-tiered ensemble based robust method to address all of the challenges of labeling instances in evolving data stream. Bottleneck of our current work is, it needs to build ADABOOST ensembles for each of the numeric features. This can face scalability issue as number of features can be very large at times in data stream. In this paper, we propose an intelligent approach to build these large number of ADABOOST ensembles with MapReduce based parallelism. We show that, this approach can help our base method to achieve significant scalability without compromising classification accuracy. We analyze different aspects of our design to depict advantages and disadvantages of the approach. We also compare and analyze performance of the proposed approach in terms of execution time, speedup and scale up.", "num_citations": "6\n", "authors": ["1190"]}
{"title": "Measuring relatedness and augmentation of information of interest within free text law enforcement documents\n", "abstract": " This paper defines and shows the merit of measures for quantifying the degree of relatedness of information of interest and the importance of new information found within a large number of free text documents. These measures are used for identifying and sorting free text documents that are found to contain related information of interest and, in some cases, new information of interest related to a reference document. The relatedness measures consider the semantic content (e.g., people, vehicles, events, organizations, objects, and locations with their descriptive attributes) as well as the semantic context between semantic content items and key entities such as events and temporal items. Additional links to related sub-graphs between a reference graph and a comparison graph identify augmented knowledge over the known semantic text. Graph structures are generated initially from syntactic links and ontological\u00a0\u2026", "num_citations": "6\n", "authors": ["1190"]}
{"title": "RDFKB: A semantic web knowledge base\n", "abstract": " There are many significant research projects focused on providing semantic web repositories that are scalable and efficient. However, the true value of the semantic web architecture is its ability to represent meaningful knowledge and not just data. Therefore, a semantic web knowledge base should do more than retrieve collections of triples. We propose RDFKB (Resource Description Knowledge Base), a complete semantic web knowledge case. RDFKB is a solution for managing, persisting and querying semantic web knowledge. Our experiments with real world and synthetic datasets demonstrate that RDFKB achieves superior query performance to other state-of-the-art solutions. The key features of RDFKB that differentiate it from other solutions are: 1) a simple and efficient process for data additions, deletions and updates that does not involve reprocessing the dataset; 2) materialization of inferred triples at addition time without performance degradation; 3) materialization of uncertain information and support for queries involving probabilities; 4) distributed inference across datasets; 5) ability to apply alignments to the dataset and perform queries against multiple sources using alignment. RDFKB allows more knowledge to be stored and retrieved; it is a repository not just for RDF datasets, but also for inferred triples, probability information, and lineage information. RDFKB provides a complete and efficient RDF data repository and knowledge base.", "num_citations": "6\n", "authors": ["1190"]}
{"title": "A comparison of approaches for large-scale data mining\n", "abstract": " Massive data sets that are generated in many applications ranging from astronomy to bioinformatics provide various opportunities and challenges. Especially, scalable mining of such massive data sets is an challenging issue that attracted some recent research. Some of these recent work use MapReduce paradigm to build data mining models on the entire data set. In this paper, we analyze existing approaches for large scale data mining and compare their performance to the MapReduce model. Based on our analysis, a data mining framework that integrates MapReduce and sampling is introduced and discussed.", "num_citations": "6\n", "authors": ["1190"]}
{"title": "Semantic web implementation scheme for national vulnerability database (Common Platform Enumeration Data)\n", "abstract": " In software systems, information modeling is of prime significance. The attributes of real world objects that are captured and the way they are represented in the software system determines the operations a system can perform and the queries it can answer. Traditional relational database model and semantic model are two popular information modeling techniques and they differ in several ways. A relational model is based on tables and columns whereas a semantic model is based on classes and properties. Due to the inadequate expressivity of a relational data model, it imposes limitations on semantic interoperability apart from certain syntactic interoperability issues. Semantic web technologies are based on an information model that is designed to facilitate easy data sharing and interoperability (syntactic as well as semantic). This report addresses the importance of a semantic model and subsequently illustrates an efficient way to migrate data from a relational model to a semantic model using a real world application based on National Vulnerability Database.", "num_citations": "6\n", "authors": ["1190"]}
{"title": "Building trustworthy semantic webs\n", "abstract": " Recent developments in information systems technologies have resulted in computerizing many applications in various business areas. Data has become a critical resource in many organizations, and therefore, efficient access to data, sharing the data, extracting information from the data, and making use of the information has become an urgent need. As a result, there have been many efforts on not only integrating the various data sources scattered across several sites, but extracting information from these databases in the form of patterns and trends has also become important. These data sources may be databases managed by database management systems, or they could be data warehoused in a repository from multiple data sources.", "num_citations": "6\n", "authors": ["1190"]}
{"title": "Relationalizing RDF stores for tools reusability\n", "abstract": " The emergence of Semantic Web technologies and standards such as Resource Description Framework (RDF) has introduced novel data storage models such as the RDF Graph Model. In this paper, we present a research effort called R2D, which attempts to bridge the gap between RDF and RDBMS concepts by presenting a relational view of RDF data stores. Thus, R2D is essentially a relational wrapper around RDF stores that aims to make the variety of stable relational tools that are currently in the market available to RDF stores without data duplication and synchronization issues.", "num_citations": "6\n", "authors": ["1190"]}
{"title": "design and Implementation of a framework for assured Information sharing across organizational Boundaries\n", "abstract": " In this article we have designed and developed a framework for sharing data in an assured manner in case of emergencies. We focus especially on a need to share environment. It is often required to divulge information when an emergency is flagged and then take necessary steps to handle the consequences of divulging information. This procedure involves the application of a wide range of policies to determine how much information can be divulges in case of an emergency depending on how trustworthy the requester of the information is.", "num_citations": "6\n", "authors": ["1190"]}
{"title": "E-Mail Worm Detection Using Data Mining\n", "abstract": " This work applies data mining techniques to detect e-mail worms. E-mail messages contain a number of different features such as the total number of words in message body/subject, presence/absence of binary attachments, type of attachments, and so on. The goal is to obtain an efficient classification model based on these features. The solution consists of several steps. First, the number of features is reduced using two different approaches: feature-selection and dimension-reduction. This step is necessary to reduce noise and redundancy from the data. The feature-selection technique is called Two-phase Selection (TPS), which is a novel combination of decision tree and greedy selection algorithm. The dimension-reduction is performed by Principal Component Analysis. Second, the reduced data is used to train a classifier. Different classification techniques have been used, such as Support Vector Machine (SVM\u00a0\u2026", "num_citations": "6\n", "authors": ["1190"]}
{"title": "Design and Simulation of Trust Management Techniques for a Coalition Data Sharing Environment\n", "abstract": " Effective communication among agents in large teams is crucial because the members share a common goal but only have partial views of the environment. Information sharing is difficult in a large team because, a team member may have a piece of valuable information but not know who needs the information, since it is infeasible to know what each other agent is doing. Information sharing is a main part of any system or organization. The information sharing needs to be foolproof. Only the legitimate receiver should be able to get hold of the information. This paper mainly deals with intelligent software agents for information sharing with confidentiality and trust. It clearly defines an intelligent software agent, background of information sharing in intelligent agents and the trust in the agents. Some part of the information needs confidentiality. The information that is shared requires security policy enforced based on the\u00a0\u2026", "num_citations": "6\n", "authors": ["1190"]}
{"title": "Administering the semantic web: Confidentiality, privacy, and trust management\n", "abstract": " The Semantic Web is essentially a collection of technologies to support machine-understandable Web pages as well as Information Interoperability. There has been much progress made on the Semantic Web, including standards for eXtensible Markup Language, Resource Description Framework, and Ontologies. However, administration policies and techniques for enforcing them have received little attention. These policies include policies for security, privacy, data quality, integrity, trust, and timely information processing. This article discusses administration policies for the Semantic Web as well as techniques for enforcing them. In particular, we will discuss an approach for ensuring confidentiality, privacy, and trust for the Semantic Web. We will also discuss the inference and privacy problems within the context of administration policies.", "num_citations": "6\n", "authors": ["1190"]}
{"title": "Assured Information Sharing: Volume 1: Overview\n", "abstract": " This paper describes issues, technologies, challenges, and directions for Assured Information Sharing (AIS). AIS is about organizations sharing information but at the same time enforcing policies and procedures so that the data is integrated and mined to extract nuggets. This is the first in a series of papers we are writing on AIS. It provides an overview including architectures, functions and policies for AIS. We assume that the partners of a coalition may be trustworthy, semi-trustworthy or untrustworthy and investigate solutions for AIS to handle the different scenarios.", "num_citations": "6\n", "authors": ["1190"]}
{"title": "Security issues in data warehousing and data mining: panel discussion\n", "abstract": " This paper describes the panel discussion on data warehousing, data mining and security.", "num_citations": "6\n", "authors": ["1190"]}
{"title": "Object-oriented implementation of an infrastructure and data manager for real-time command and control systems\n", "abstract": " MITRE's Evolvable Real-Time C3 (command, control, and communications) project attempts to develop an approach that would enable current real-time systems to evolve into the systems of the future. The project has chosen AWACS (Airborne Warning and Control System) as an example to test out the concepts and architectures to be developed. In this paper, we describe an object-oriented implementation of an infrastructure and data manager for next-generation real-time command-and-control systems.", "num_citations": "6\n", "authors": ["1190"]}
{"title": "Design and implementation of a distributed database inference controller\n", "abstract": " We describe an approach for controlling certain unauthorized inferences in a multilevel secure distributed database management system. In such a system, two or more multilevel secure database management systems are connected via a trusted network. Furthermore, the environment that we have considered is a limited heterogeneous one where not all of the nodes handle the same accreditation ranges. In our approach, security constraints, which are rules that assign security levels to the data, are processed during the distributed query, update, and database design operations in such a way that users do not acquire information to which they are not authorized via logical deductions. We describe the design and implementation of the distributed inference controller which functions during the query operation.", "num_citations": "6\n", "authors": ["1190"]}
{"title": "Current status of R&D in trusted database management systems\n", "abstract": " Since the Air Force Summer Study in 1982 [AFSB83], several R&D efforts in Trusted Database Management Systems (TDBMSs) have been initiated. These include efforts in (i) Trusted Relational DBMS,(ii) Trusted Object-Oriented DBMS,(iii) Trusted Distributed DBMS,(iv) Trusted High Performance DBMS, and (v) other TDBMS research topics such as inference and aggregation, polyinstantiation, concurrency control, and auditing. In this paper, we provide a global view of TDBMS R&D. 2TDBMS technology evolved from computer security technology, and relational database technology. 3 The earliest published work on TDBMS was the design of the Hinke Sebaefer architecture [HINK75], where database data is stored in single level operating system segments and access to these segments is controlled by the operating system. Some of the other early work in the late 70's and early 80's included the data model\u00a0\u2026", "num_citations": "6\n", "authors": ["1190"]}
{"title": "Handling security constraints during multilevel database design\n", "abstract": " 'In this paper we describe techniques for processing association-based constraints. simple constraints, and logical constraints during multilevel database design. l INTRODUCTIONIn a multilevel secure database management system (Ml.. S/DBMS) users cleared at different security levels access and share a database consisting of data at different sensitivity levels. A powerful and dynamic approach to assigning sensitivity levels, also called security levels, to data is one which utilizes security constraints or classi\ufb01cation rules. Security consuaints provide an effective and versatile classi\ufb01cation policy. They can be used to assign security levels to the data depending on their content and the context in which the data is displayed. They can also be used to dynamically reclassify the data. In other words. the security constraints are essential for describing multilevel applications.", "num_citations": "6\n", "authors": ["1190"]}
{"title": "Security constraint processing during the update operation in a multilevel secure database management system\n", "abstract": " In a multilevel secure database management system (MLSDBMS). users cleared at different security levels access and share a database consisting of data at different sensitivity levels. A powerful and dynamic approach to assigning sensitivity levels (also called security levels) to data is one which utilizes security constraints or classification rules. Security constraints provide an effective and versatile classification policy. They can be used to assign security levels to the data depending on the content, context, and time. In this paper, we argue that security constraints are a special form of integrity constraints enforced in a MLSDBMS. As such, they can be handled during query processing, during database updates, or during database design. We then describe in detail the design and implementation of a secure update processor which handles security constraints in a multilevel secure database management system.", "num_citations": "6\n", "authors": ["1190"]}
{"title": "Secure query processing in distributed database management systems-design and performance studies\n", "abstract": " Distributed systems are vital for the efficient processing required in military applications. For these applications it is especially important that the distributed database management systems (DDBMS) should operate in a secure manner. For example, the DDBMS should allow users, who are cleared to different levels, access to the database consisting of data at a variety of sensitivity levels without compromising security. The authors focus on secure query processing in a DDBMS. Implementation of secure query processing algorithms in a DDBMS as well as an analysis of the performance of the algorithms is described.< >", "num_citations": "6\n", "authors": ["1190"]}
{"title": "Heterogeneous Information Exchange and Organizational Hubs\n", "abstract": " Helene Bestougeff, Universite de Marne Ia Vallee, France Jacques-Emile Dubois, Universite Paris VII-Denis Diderot, France Bhavani Thuraisingham, MITRE Corporation, USA The last fifty years promoted the conceptual trio: Knowledge, Information and Data (KID) to the center of our present scientific technological and human activities. The intrusion of the Internet drastically modified the historical cycles of communication between authors, providers and users. Today, information is often the result of the interaction between data and the knowledge based on their comprehension, interpretation and prediction. Nowadays important goals involve the exchange of heterogeneous information, as many real life and even specific scientific and technological problems are all interdisciplinary by nature. For a specific project, this signifies extracting information, data and even knowledge from many different sources that must be addressed by interoperable programs. Another important challenge is that of corporations collaborating with each other and forming coalitions and partnerships. One development towards achieving this challenge is organizational hubs. This concept is new and still evolving. Much like an airport hub serving air traffic needs, organizational hubs are central platforms that provide information and collaboration specific to a group of users' needs. Now companies are creating hubs particular to certain types of industries. The users of hubs are seen as communities for which all related information is directly available without further searching efforts and often with value-added services.", "num_citations": "5\n", "authors": ["1190"]}
{"title": "Data supply chain management: supply chain management for incentive and risk-based assured information sharing\n", "abstract": " In this paper we introduce the notion of data supply chain management and draw parallels between supply chain management and developing a data product. Then we discuss information sharing in supply chain management and discuss risks and incentives for information sharing. Our objective is to implement the Department of Defense Information Sharing Strategy whose goal is to Recognize and leverage the Information Sharing Value Chain. This is the seventh in a series of reports we are writing on Security Studies and the application of information technology for providing security and combating terrorism. We will include papers on both cyber security and national security. The purpose of these series of reports is to guide us in the technologies we are developing for both cyber security and national security. The technologies include systems for assured information sharing and assured cloud computing and tools for secure social network analysis and data mining for security applications such as malware detection. Our research to develop these technologies is supported by the Air Force Office of Scientific Research.Descriptors:", "num_citations": "5\n", "authors": ["1190"]}
{"title": "Update-enabled triplification of relational data into virtual rdf stores\n", "abstract": " The current buzzword in the Internet community is the Semantic Web initiative proposed by the W3C to yield a Web that is more flexible and self-adapting. However, for the Semantic Web initiative to become a reality, heterogeneous data sources need to be integrated in order to enable access to them in a homogeneous manner. Since a vast majority of data currently resides in relational databases, integrating relational data sources with semantic web technologies is at the top of the list of activities required to realize the semantic web vision. Several efforts exist that publish relational data as Resource Description Framework (RDF) triples; however almost all current work in this arena is uni-directional, presenting data from an underlying relational database into a corresponding virtual RDF store in a read-only manner. An enhancement over previous relational-to-RDF bridging work in the form of bi-directionality\u00a0\u2026", "num_citations": "5\n", "authors": ["1190"]}
{"title": "R2D: A framework for the relational transformation of RDF data\n", "abstract": " The astronomical growth of the World Wide Web has resulted in data explosion that in turn has given rise to a need for data representation methodologies and standards to present required information in a rapid and automated manner. The Resource Description Framework (RDF) is one such standard proposed by W3C to address the above need. The ubiquitous acceptance of RDF on the Internet has resulted in the emergence of a new data storage paradigm, the RDF Graph Model, which, as with any data storage methodology, requires data modeling and visualization tools to aid with data management. This paper presents R2D (RDF-to-Database), a relational wrapper for RDF Data Stores, which aims to transform, at run-time, semi-structured RDF data into an equivalent domain-specific relational schema, thereby bridging the gap between RDF and RDBMS concepts and making the abundance of relational tools\u00a0\u2026", "num_citations": "5\n", "authors": ["1190"]}
{"title": "Multilevel Secure Database Management System.\n", "abstract": " This paper provides an overview of security constraints and describes an integrated approach to security constraint processing", "num_citations": "5\n", "authors": ["1190"]}
{"title": "Secure Grid Computing\n", "abstract": " Since late 1990s, Grid computing has become an increasingly important research topic within computer science. Grid computing is concerned how to share and coordinated use diverse resources in distributed environments. The dynamic and multi-institutional nature of these environments introduces challenging security issues, which include integration with existing systems and technologies, interoperability with different \u201chosting environments\u201d and trust relationships among interacting hosting environments. We need new technical approaches to handle those security issues. During those years, many prominent companies and research institutes have proposed and implemented several architectures for grid and grid security. In this survey, first, we introduce the Globus Toolkit, some commercial Grid productions and Grid Testbeds. Second we describe several Grid security architectures and research methods of security issues from research institutes and Universities. Next we discuss the application of grid computing to the Global Information Grid (GIG). Finally we give some potential research topics..", "num_citations": "5\n", "authors": ["1190"]}
{"title": "Data mining and cyber security\n", "abstract": " Data mining is the process of posing queries and extracting patterns, often previously unknown from large quantities of data using pattern matching or other reasoning techniques. Cyber security is the area that deals with protecting from cyber terrorism. Cyber attacks include access control violations, unauthorized intrusions, and denial of service as well as insider threat. The presentation provides an overview of data mining techniques and cyber threats and discusses developments in applying data mining for cyber security.", "num_citations": "5\n", "authors": ["1190"]}
{"title": "Benchmarking real-time distributed object management systems for evolvable and adaptable command and control applications\n", "abstract": " The paper describes benchmarking for evolvable and adaptable real time command and control systems. MITRE's Evolvable Real-Time C3 initiative developed an approach that would enable current real time systems to evolve into the systems of the future. We designed and implemented an infrastructure and data manager so that various applications could be hosted on the infrastructure. Then we completed a follow-on effort to design flexible adaptable distributed object management systems for command and control (C2) systems. Such an adaptable system would switch scheduling algorithms, policies, and protocols depending on the need and the environment. Both initiatives were carried out for the United States Air Force. One of the key contributions of the work is the investigation of real time features for distributed object management systems. Partly as a result of our work we are now seeing various real time\u00a0\u2026", "num_citations": "5\n", "authors": ["1190"]}
{"title": "Design and implementation of a query processor for a trusted distributed data base management system\n", "abstract": " Distributed systems are vital for the efficient processing required in military and commercial applications. For many of these applications, it is especially important that the distributed data base management systems (DDBMS) operate in a secure manner. For example, the DDBMS should allow users, who are cleared at different security levels access to the data base at different levels of data sensitivity without compromising security. A DDBMS with multilevel user/datahandling capability is called a trusted distributed data base management system (TDDBMS). This article focuses on query processing in a TDDBMS. It describes the security issues, algorithms for query processing, and design and implementation of a query processor prototype for a TDDBMS.", "num_citations": "5\n", "authors": ["1190"]}
{"title": "Foundations of Multilevel Databases\n", "abstract": " In this paper, formal logic is used as a basis for establishing concepts in multilevel databases. Issues covered include: model and proof theoretic approaches to formalizing multilevel database concepts, environments associated with security levels, the inference problem, handling negative information and the inclusion of formal semantics of time. Finally, issues related to the theory of multilevel relational databases, consistency and completeness of security constraints and assigning security levels to data are also briefly addressed.", "num_citations": "5\n", "authors": ["1190"]}
{"title": "Issues on the design and implementation of an intelligent database inference controller\n", "abstract": " The Inference Problem compromises database systems which are usually considered to be secure. Here, users pose sets of queries and infer unauthorized information from the responses that they obtain. An Inference Controller is a device that prevents and/or detects security violations via inference. This paper describes the issues involved in designing and implementing an intelligent database inference controller.", "num_citations": "5\n", "authors": ["1190"]}
{"title": "Progressive one-shot human parsing\n", "abstract": " Prior human parsing models are limited to parsing humans into classes pre-defined in the training data, which is not flexible to generalize to unseen classes, eg, new clothing in fashion analysis. In this paper, we propose a new problem named one-shot human parsing (OSHP) that requires to parse human into an open set of reference classes defined by any single reference example. During training, only base classes defined in the training set are exposed, which can overlap with part of reference classes. In this paper, we devise a novel Progressive One-shot Parsing network (POPNet) to address two critical challenges, ie, testing bias and small sizes. POPNet consists of two collaborative metric learning modules named Attention Guidance Module and Nearest Centroid Module, which can learn representative prototypes for base classes and quickly transfer the ability to unseen classes during testing, thereby reducing testing bias. Moreover, POPNet adopts a progressive human parsing framework that can incorporate the learned knowledge of parent classes at the coarse granularity to help recognize the descendant classes at the fine granularity, thereby handling the small sizes issue. Experiments on the ATR-OS benchmark tailored for OSHP demonstrate POPNet outperforms other representative one-shot segmentation models by large margins and establishes a strong baseline. Source code can be found at https://github. com/Charleshhy/One-shot-Human-Parsing.", "num_citations": "4\n", "authors": ["1190"]}
{"title": "A data access model for privacy-preserving cloud-iot architectures\n", "abstract": " We propose a novel data collection and data sharing model for cloud-IoT architectures with an emphasis on data privacy. This model has been implemented in Privasee, an open source platform for privacy-aware web-application development, which provides a plug-in module to support IoT application development. Privasee uses a cloud-IoT architecture called DataBank. We provide examples and discuss future extensions.", "num_citations": "4\n", "authors": ["1190"]}
{"title": "Analyzing and Securing Social Networks\n", "abstract": " BACKGROUNDRecent developments in information systems technologies have resulted in the computerization of many applications in various business areas. Data has become a critical resource in many organizations, and therefore, efficient access to data, sharing the data, extracting information from the data, and making use of the information has become an urgent need. As a result, there have been many efforts at not only integrating the various data sources scattered across several sites, but extracting information from these databases in the form of patterns and trends has also become important. These data sources may be databases managed by database management systems, or they could be data warehoused in a repository from multiple data sources. The advent of the World Wide Web (WWW) in the mid-1990s has resulted in even greater demand for effective management of data, information, and\u00a0\u2026", "num_citations": "4\n", "authors": ["1190"]}
{"title": "Integrating cyber security and data science for social media: A position paper\n", "abstract": " Cyber security and data science are two of the fastest growing fields in Computer Science and more recently they are being integrated for various applications. This position paper will review the developments in applying Data science for cyber security and cyber security for data science and then discuss the applications in Social Media.", "num_citations": "4\n", "authors": ["1190"]}
{"title": "A category-based model for ABAC\n", "abstract": " In Attribute-Based Access Control (ABAC) systems, access to resources is controlled by evaluating rules against the attributes of the user and the object involved in the access request, as well as the values of the relevant attributes from the environment. This is a powerful concept: ABAC is able to enforce DAC and RBAC policies, as well as more general, dynamic access control policies, where the decision to grant or deny an access request is based on the system's state. However, in its current definition, ABAC does not lend itself well to some operations, such as review queries, and it is in general more costly to specify and maintain than simpler systems such as RBAC. To address these issues, in this paper we propose a formal model of ABAC based on the notion of a category that underlies the general category-based metamodel of access control (CBAC). Our proposed approach adds structure to ABAC, so that\u00a0\u2026", "num_citations": "4\n", "authors": ["1190"]}
{"title": "Towards a framework for developing cyber privacy metrics: A vision paper\n", "abstract": " While progress has been made on techniques, tools and systems for providing data privacy, there is no general methodology for determining the extent to which these techniques, tools and systems reduce practical privacy risks. We need a comprehensive framework where the privacy and utility of multiple privacy-preserving techniques could be measured. This vision paper provides directions for designing such a framework.", "num_citations": "4\n", "authors": ["1190"]}
{"title": "Redaction based rdf access control language\n", "abstract": " We propose an access control language for securing RDF graphs which essentially leverages an underlying query language based redaction mechanism to provide fine grained RDF access control. The access control language presented is equipped with critical features such as policy resolution and cascading policies that are essential for fine grained RDF access control. We present the architecture of our system which primarily features a flexible, scalable and general purpose RDF access control mechanism.", "num_citations": "4\n", "authors": ["1190"]}
{"title": "Calculating edit distance for large sets of string pairs using MapReduce\n", "abstract": " Given two strings X and Y over a finite alphabet, the edit distance between X and Y, d (X, Y) is the number of elementary edit operations required to edit X into Y. A dynamic programming algorithm elegantly computes this distance. In this paper, we investigate the parallelization of calculating edit distance for a large set of strings using MapReduce, a popular parallel computing framework. We propose SIM MR and PRE MR algorithms, parallel versions of the dynamic programming solution, and present implementations of these algorithms. We study different cases by varying algorithm parameters, input size and number of parallel nodes, and analytically and experimentally confirm the superiority of our methods over the usual dynamic programming approach. This study demonstrates how MapReduce parallelization opens new avenues of designing for dynamic programming algorithms.", "num_citations": "4\n", "authors": ["1190"]}
{"title": "Rdf-x: A language for sanitizing rdf graphs\n", "abstract": " With the advent of Semantic Web and Resource Description Framework (RDF), the web is likely to witness an unprecedented wealth of knowledge, resulting from seamless integration of various data sources. Data integration is one of the key features of RDF, however, absence of secure means for managing sensitive RDF data may prevent sharing of critical data altogether or may cause serious damage. Towards this end we present a language for sanitizing RDF graphs, which comprises a set of sanitization operations that transform a graph by concealing the sensitive data. These operations are modeled into a new SPARQL query form known as SANITIZE, which can also be leveraged towards fine grained access control and building advanced anonymization features.", "num_citations": "4\n", "authors": ["1190"]}
{"title": "After the \u2018War on Terror\u2019\u2014how to maintain long-range terrorist deterrence\n", "abstract": " The American withdrawal from Afghanistan scheduled for 2014 signals an end to the first phase of the global \u2018War on Terror\u2019. Once the War on Terror enters a less intensive phase, a revised approach to terrorist deterrence would be necessary. The future global War on Terror is indicated to be focused on intelligence operations, surgical strikes to attack terrorist groups, increased surveillance and interception of terrorist activity and finally deterrence. Traditional deterrence requires a mutual understanding of a common rationality and a mutual interest of survival between the parties. These conditions do not exist in deterring terrorists. This paper is focused on how to establish and maintain long-range terrorist deterrence after the War on Terror. If traditional deterrence is applied on terrorism, it tends to end up in a tit-for-tat game, strike when struck and countervalue targeting. Traditional deterrence does not work against\u00a0\u2026", "num_citations": "4\n", "authors": ["1190"]}
{"title": "MapReduce-guided scalable compressed dictionary construction for evolving repetitive sequence streams\n", "abstract": " Users' repetitive daily or weekly activities may constitute user profiles. For example, a user's frequent command sequences may represent normative pattern of that user. To find normative patterns over dynamic data streams of unbounded length is challenging. For this, an unsupervised learning approach is proposed in our prior work by exploiting a compressed/quantized dictionary to model common behavior sequences. This work suffers scalability issues. Hence, in this paper, we propose and implement a MapReduce-based framework to construct a quantized dictionary. We show effectiveness of our distributed parallel solution on a benchmark dataset.", "num_citations": "4\n", "authors": ["1190"]}
{"title": "Evolving insider threat detection using stream analytics and big data\n", "abstract": " Evidence of malicious insider activity is often buried within large data streams, such as system logs accumulated over months or years. Ensemble-based stream mining leverages multiple classification models to achieve highly accurate anomaly detection in such streams, even when the stream is unbounded, evolving, and unlabeled. This makes the approach effective for identifying insiders who attempt to conceal their activities by varying their behaviors over time.", "num_citations": "4\n", "authors": ["1190"]}
{"title": "StormRider: harnessing\" storm\" for social networks\n", "abstract": " The focus of online social media providers today has shifted from\" content generation\" towards finding effective methodologies for\" content storage, retrieval and analysis\" in the presence of evolving networks. Towards this end, in this paper we present StormRider, a framework that uses existing cloud computing and semantic web technologies to provide application programmers with automated support for these tasks, thereby allowing a richer assortment of use cases to be implemented on the underlying evolving social networks.", "num_citations": "4\n", "authors": ["1190"]}
{"title": "Policy enforcement system for inter-organizational data sharing\n", "abstract": " Sharing data among organizations plays an important role in security and data mining. In this paper, the authors describe a Data Sharing Miner and Analyzer (DASMA) system that simulates data sharing among N organizations. Each organization has its own enforced policy. The N organizations share their data based on trusted third party. The system collects the released data from each organization, processes it, mines it, and analyzes the results. Sharing in DASMA is based on trusted third parties. However, organizations may encode some attributes, for example. Each organization has its own policy represented in XML format. This policy states what attributes can be released, encoded, and randomized. DASMA processes the data set and collects the data, combines it, and prepares it for mining. After mining, a statistical report is produced stating the similarities between mining with data sharing and mining\u00a0\u2026", "num_citations": "4\n", "authors": ["1190"]}
{"title": "An Evaluation of Privacy, Risks and Utility with Provenance\n", "abstract": " The web provides an open environment, where anyone can assert anything and publish it; therefore, it is important for users to be aware of the data quality in order to use the data appropriately. Many published databases are in fact legitimate and contain sensitive information that should not be disclosed to unauthorized users. To decide which databases have high quality data, we could use provenance information to accompany the information retrieved. Releasing provenance data, however, could have important privacy consequences. Therefore, we need to be careful in choosing what kind of provenance information to reveal. In our work, we propose a Semantic Webbased inferencing framework that provides a risk-based approach to decide what kind of provenance information to release.", "num_citations": "4\n", "authors": ["1190"]}
{"title": "Necessary and sufficient conditions for transaction-consistent global checkpoints in a distributed database system\n", "abstract": " Checkpointing and rollback recovery are well-known techniques for handling failures in distributed systems. The issues related to the design and implementation of efficient checkpointing and recovery techniques for distributed systems have been thoroughly understood. For example, the necessary and sufficient conditions for a set of checkpoints to be part of a consistent global checkpoint has been established for distributed computations. In this paper, we address the analogous question for distributed database systems. In distributed database systems, transaction-consistent global checkpoints are useful not only for recovery from failure but also for audit purposes. If each data item of a distributed database is checkpointed independently by a separate transaction, none of the checkpoints taken may be part of any transaction-consistent global checkpoint. However, allowing individual data items to be checkpointed\u00a0\u2026", "num_citations": "4\n", "authors": ["1190"]}
{"title": "Geospatial data mining for national security: Land cover classification and semantic grouping\n", "abstract": " Land cover classification for the evaluation of land cover changes over certain areas or time periods is crucial for geospatial modeling, environmental crisis evaluation and urban open space planning. Remotely sensed images of various spatial and spectral resolutions make it possible to classify land covers on the level of pixels. Semantic meanings of large regions consisting of hundreds of thousands of pixels cannot be revealed by discrete and individual pixel classes, but can be derived by integrating various groups of pixels using ontologies. This paper combines data of different resolutions for pixel classification by support vector classifiers, and proposes an efficient algorithm to group pixels based on classes of neighboring pixels. The algorithm is linear in the number of pixels of the target area, and is scalable to very large regions. It also re-evaluates imprecise classifications according to neighboring classes for\u00a0\u2026", "num_citations": "4\n", "authors": ["1190"]}
{"title": "Security and privacy in collaborative distributed systems\n", "abstract": " With the rapid development of various types of open infrastructures, including Internet, grid, and wireless networks, much attention has been focused on the development of distributed systems for collaborative applications in many areas, including collaborative research and development, healthcare, e-commerce, disaster management and homeland security. Besides reliability and timeliness, the great advantages of collaborative distributed systems for improved group awareness and collaboration opportunities are, however, often overshadowed by accompanying security and privacy concerns. In practice, it is desirable that security and privacy for collaborative distributed systems are flexible, scalable and adaptable to the changing and heterogeneous environments. Ensuring efficient collaboration with such security and privacy requirements is a great challenge due to the difficulties of: 1) Ensuring flexible and\u00a0\u2026", "num_citations": "4\n", "authors": ["1190"]}
{"title": "What\u2019s Next in XML and Databases?\n", "abstract": " Since the time XML became a W3C standard for document representation and exchange over the Web, many efforts have been devoted to the development of standards, methodologies, and tools for handling, storing, retrieving, and protecting XML documents. The purpose of this panel, held during the international EDBT\u20192004 workshop on \u201cdatabase technologies for handling XML information on the Web\u201d [3], is to discuss the current status of the research in XML data management and to foresee new trends towards the XML-ization of database research.", "num_citations": "4\n", "authors": ["1190"]}
{"title": "Dependable Infrastructures and Data Managers for Sensor Networks.\n", "abstract": " This paper provides some directions for developing infrastructures and data managers for dependable sensor networks. By dependable systems we mean systems that adapt to the environment and are secure, fault tolerant and process data in real-time as needed. We start with a discussion of the need for dependable sensor information management and then provide an overview of dependable infrastructures and data managers for such networks. We also discuss some security issues for sensor information management. Finally some directions for further research are given.", "num_citations": "4\n", "authors": ["1190"]}
{"title": "Data and applications security\n", "abstract": " Data and Applications Security Page 1 Data and Applications Security Developments and Directions Dr. Bhavani Thuraisingham The University of Texas at Dallas Lecture #1 Introduction to Data and Applications Security January 9, 2006 Page 2 Outline \uf06c Data and Applications Security - Developments and Directions \uf06c Secure Semantic Web - XML Security; Other directions \uf06c Some Emerging Secure DAS Technologies - Secure Sensor Information Management; Secure Dependable Information Management \uf06c Some Directions for Privacy Research - Data Mining for handling security problems; Privacy vs. National Security; Privacy Constraint Processing; Foundations of the Privacy Problem \uf06c What are the Challenges? Page 3 Secure Federated Database Management for Data Sharing: Policy Integration Policies at the Component level: eg, Component policies for components A, B, and C Generic policies for the \u2026", "num_citations": "4\n", "authors": ["1190"]}
{"title": "Dependable objects for databases, middleware and methodologies: a position paper\n", "abstract": " Describes work that needs to be done to integrate the features of security, fault tolerance and real-time computing into object-oriented technology to produce \"dependable objects\". This dependable technology can be applied in the areas of databases, middleware (including object request brokers) and object-based design and analysis methodologies. The accomplishments to date of various groups and organizations are reviewed and some preliminary ideas for new research directions are revealed.", "num_citations": "4\n", "authors": ["1190"]}
{"title": "Data Manager for Evolvable Real-Time Command and Control Systems\n", "abstract": " We describe the design and implementation of real-time data management services. These services combine technologies developed in the context of real-time distributed object management, object DBMSs, and scheduling. This combination simpli es many of the services, and produces a result which is greater than the sum of its parts, because it can be used to improve the portability and exibility of real-time applications.", "num_citations": "4\n", "authors": ["1190"]}
{"title": "Multimedia Database Management Systems: Research Issues and Future Directions\n", "abstract": " Multimedia Database Management Systems brings together in one place important contributions and up-to-date research results in this important area. Multimedia Database Management Systems serves as an excellent reference, providing insight into some of the most important research issues in the field.", "num_citations": "4\n", "authors": ["1190"]}
{"title": "On real-time extensions to object request brokers: a panel position paper\n", "abstract": " The paper describes some of the issues that need to be investigated in order to develop real time data processing extensions to CORBA. In particular, issues on extensions to the object model, ORB, and IDL are discussed. Finally a discussion of some real time services and applications hosted on real time ORBs are provided.", "num_citations": "4\n", "authors": ["1190"]}
{"title": "A new view of information modeling: A bridge between data and information\n", "abstract": " The information model was conceived to address the complexities of managing large volumes of data, processes, designs, and tools that are shared by many business users with differing requirements. Because an information model derives much of its features from data models, the distinction between information modeling and data modeling is sometimes unclear. One perspective is that information modeling is context dependent: when a model is viewed as a representation scheme for users to comprehend, it is an information model. When used as a representation scheme to be processed by a computer, it is a data model.", "num_citations": "4\n", "authors": ["1190"]}
{"title": "Knowledge-based support for the development of database-centered applications\n", "abstract": " Using the Application Development Toolkit (ADT) as an example, it is shown that by borrowing some techniques from the artificial-intelligence field, database-centered application-development productivity tools can be made more acceptable to end users and more useful to expert developers. Experience with ADT has indicated that a more end-user-oriented approach, and, in particular, more accommodating and application-oriented interface, is needed. A characteristic set of problems that are found in the class of productivity tools similar to ADT is presented. A series of possible improvements that shed some light on deficiencies in current state-of-the-art application-generation systems and productivity tools is suggested.<>", "num_citations": "4\n", "authors": ["1190"]}
{"title": "Next-location prediction using federated learning on a blockchain\n", "abstract": " Mobile devices are a rich source of sensitive location data. In this paper, we propose a method for harnessing this data to provide better location predictions without sacrificing the privacy of the users generating this data. To this end, we propose utilizing Federated Learning to train locally on a user's mobile device, while simultaneously identifying and combatting the possibility of bad actors or adversaries that may deliberately report problematic data to hurt the training process. Furthermore, we propose using a blockchain instead of a centralized server for the training process, to ensure that the process is secure.", "num_citations": "3\n", "authors": ["1190"]}
{"title": "Artificial Intelligence and Data Science Governance: Roles and Responsibilities at the C-Level and the Board\n", "abstract": " Corporate governance and the roles and responsibilities of the corporate officers and the board of directors have received an increasing interest since the Enron scandal of the early 2000s. This scandal resulted in enacting policies, laws and regulations such as the Sarbanes-Oxley and others. More recently, with almost every corporation focusing on the applications of Artificial Intelligence (AI) and Data Science (DS) for their businesses in numerous industries including finance and banking, healthcare and medicine, manufacturing and retail and defense and intelligence, it is critical that these corporations take a serious look at the roles and responsibilities of the corporate officers and the board with respect to the governance of the AI and DS operations. This paper discusses the issues and challenges for AI and DS governance with an emphasis on the potential roles and responsibilities of the corporate officers and\u00a0\u2026", "num_citations": "3\n", "authors": ["1190"]}
{"title": "GCI: A GPU Based Transfer Learning Approach for Detecting Cheats of Computer Game\n", "abstract": " Cheating in massive multiple online games (MMOGs) adversely affect the game's popularity and reputation among its users. Therefore, game developers invest large amount of efforts to detect and prevent cheats that provide an unfair advantage to cheaters over other naive users during game play. Particularly, MMOG clients share data with the server during game play. Game developers leverage this data to detect cheating. However, detecting cheats is challenging mainly due to the limited client-side information, along with unknown and complex cheating techniques. In this paper, we aim to leverage machine learning based models to predict cheats over encrypted game traffic during game play. Concretely, network game traffic during game play from each player can be used to determine whether a cheat is employed. A major challenge in developing such a prediction model is the availability of sufficient training\u00a0\u2026", "num_citations": "3\n", "authors": ["1190"]}
{"title": "Admin-CBAC: An Administration Model for Category-Based Access Control\n", "abstract": " We present Admin-CBAC, an administrative model for Category-Based Access Control (CBAC). Since most of the access control models in use nowadays are instances of CBAC, in particular the popular RBAC and ABAC models, from Admin-CBAC we derive administrative models for RBAC and ABAC too. We define Admin-CBAC using Barker's metamodel, and use its axiomatic semantics to derive properties of administrative policies. Using an abstract operational semantics for administrative actions, we show how properties (such as safety, liveness and effectiveness of policies) and constraints (such as separation of duties) can be checked, and discuss the impact of policy changes. Although the most interesting properties of policies are generally undecidable in dynamic access control models, we identify particular cases where reachability based properties are decidable and can be checked using our\u00a0\u2026", "num_citations": "3\n", "authors": ["1190"]}
{"title": "Where did the political news event happen? primary focus location extraction in different languages\n", "abstract": " Political news reports are populated all over the world in various languages. It has a great value to automatically detect the geolocation from these reports for a better understanding of the associated events. Although various open-source and commercial tools exist to identify geolocation, they fail to identify at a granular level such as locality or city and they do not support most languages. Most of the techniques view the problem in terms of Named Entity Recognition (NER) and identify geolocation information at the country level for a given text. In this paper, we consider English, Spanish and Arabic news articles from different publishers. We define primary focus location as the actual location where the event occurred amongst other focus locations mentioned in the report. Our aim is to extract the primary focus location regardless of the language from articles belonging to different news agencies. We propose a\u00a0\u2026", "num_citations": "3\n", "authors": ["1190"]}
{"title": "GCI: A transfer learning approach for detecting cheats of computer game\n", "abstract": " Cheating in massive multiple online games (MMOGs) adversely affect the game's popularity and reputation among its users. Therefore, game developers invest large amount of efforts to detect and prevent cheats that provide an unfair advantage to cheaters over other naive users during game play. Particularly, MMOG clients share data with the server during game play. Game developers leverage this data to detect cheating. However, detecting cheats is challenging mainly due to the limited client-side information, along with unknown and complex cheating techniques. In this paper, we aim to leverage machine learning based models to predict cheats over encrypted game traffic during game play. Concretely, network game traffic during game play from each player can be used to determine whether a cheat is employed. A major challenge in developing such a prediction model is the availability of sufficient training\u00a0\u2026", "num_citations": "3\n", "authors": ["1190"]}
{"title": "From myths to norms: Demystifying data mining models with instance-based transparency\n", "abstract": " The desire of moving from data to intelligence has become a trend that pushes the world we live in today fast forward. Machine learning and data mining techniques are being used as important tools to unlock the wealth of voluminous amounts of data owned by organizations. Despite the existing effort of explaining their underlying machinery in layman's terms, data mining models and their output remain as esoteric, discipline-based black boxes-viable only to experts with years of training and development experiences. As data mining techniques gain growing popularity in the real world, the ability to understand their intelligent decision-making artifacts has become increasingly important and critical, especially in areas such as criminal justice and law enforcement where transparency of decision-making is vital for ensuring fairness, justice, and equality. In this paper, we present a transparency model to help unmask\u00a0\u2026", "num_citations": "3\n", "authors": ["1190"]}
{"title": "Emerging Web Services [Guest editorial]\n", "abstract": " The articles in this special section focus on emerging Web Services. The papers address the latest advances in Web Services selection, discovery and recommendation. These advances consider context awareness, quality of service, quality of experience, service usage history and evolution, and cost effectiveness.", "num_citations": "3\n", "authors": ["1190"]}
{"title": "Security in cloud computing\n", "abstract": " Cloud Computing (CC) is the new trend in computing and resource management. The architectural shift towards thin clients and the conveniently centralized provision of computing resources that the CC paradigm introduces, offer significant economic benefits to its users. However, the remarkable CC benefits are not offered at no cost. As clients\u2019 lack of direct resource control, new security and privacy risks are introduced. The whole IT infrastructure is under the control of the cloud provider and the clients have to trust the security protection mechanisms that the cloud and the service providers offer. At the same time, the centralization of resources constitutes the cloud provider a very tempting target. The CC technology is evolving rapidly and the security and privacy protection mechanisms must keep this quick pace in order to support the acceptance of the cloud model. New security solutions are required, while well\u00a0\u2026", "num_citations": "3\n", "authors": ["1190"]}
{"title": "Guest Editors' Introduction: Special Section on Data and Applications Security and Privacy.\n", "abstract": " The four papers in this special section focus on the latest advancements in data and application systems in the information security and privacy industry.", "num_citations": "3\n", "authors": ["1190"]}
{"title": "Expanded semantic graph representation for matching related information of interest across free text documents\n", "abstract": " This research proposes an expanded semantic graph definition that serves as a basis for an expanded semantic graph representation and graph matching approach. This representation separates the content and context and adds a number of semantic structures that encapsulate inferred information. The expanded semantic graph approach facilitates finding additional matches, identifying and eliminating poor matches, and prioritizing matches based on how much new knowledge is provided. By focusing on information of interest, doing pre-processing, and reducing processing requirements, the approach is applicable to problems where related information of interest is sought across a massive body of free text documents. Key aspects of the approach include (1) expanding the nodes and edges through inference using DL-Safe rules, abductive hypotheses, and syntactic patterns, (2) separating semantic content\u00a0\u2026", "num_citations": "3\n", "authors": ["1190"]}
{"title": "Securing the execution environment applications and data from multi-trusted components\n", "abstract": " The Of\ufb01ce of the Deputy Assistant Secretary of Defense (Information and Identity Assurance) has stated that \u201cthe Department of Defense's (DoD) policy, planning, and war \ufb01ghting capabilities are heavily dependent on the information technology foundation provided by the Global Information Grid (GIG). However, the GIG was built for business ef\ufb01ciency instead of mission assurance against sophisticated adversaries who have demonstrated intent and proven their ability to use cyberspace as a tool for espionage and criminal theft of data. GIG mission assurance works to ensure the DoD is able to accomplish its critical missions when networks, services, or information are unavailable, degraded, or distrusted.\u201d To meet the needs of mission assurance challenges, President\u2019s cyber plan (CNCI) has listed the area of developing multipronged approaches to supply chain risk management as one of the priorities. CNCI\u00a0\u2026", "num_citations": "3\n", "authors": ["1190"]}
{"title": "Cost-based query processing for large RDF graph using hadoop and MapReduce\n", "abstract": " With the advent of Cloud Computing, handling large amount of data scalably has become the focus of research once again. As is the case with relational data, scalable storage and retrieval of large RDF (Resource Description Framework) graphs is always a challenge. Currently available seman-tic web frameworks do not fare well to this daunting task. In this paper, we describe a framework that we built using Hadoop to store and retrieve large number of RDF triples. We describe a scheme to store RDF data in Hadoop Dis-tributed File System. We present the summary statistics we collect about the data. We also describe our algorithms to generate best possible query plan to answer a SPARQL (SPARQL Protocol and RDF Query Language) query based on a cost model. We use Hadoop's MapPeduce framework to actually answer the queries. Our results show that we can scalably store large RDF graphs in Hadoop\u00a0\u2026", "num_citations": "3\n", "authors": ["1190"]}
{"title": "Keynote: Security engineering: Developments and directions\n", "abstract": " Abstract, Security, E, are put t, o, include g, a, identifyin, g, security t, e, carry out, on end to, operating, subsyste, m, More rec, direction., Identity A, s, capabiliti, e, Informati, o, against, s, cyberspa, c, DoD is a, b, degraded, Bush) cy, b, manage, m, significan, t, To overc, o, secure s, y, of missio, n, successf, u, This key, n, to model, and accr, e, we will di, s, of secure, Biograp, h, Dr. Bhav, a, Compute, r, Engineeri, IEEE (In, s, A, dvance, m, received, t, and inno, v, E, ngineering i, s, o, gether, one, n, a, thering the, s, g, the securit, y, e, sting. Befor, e, certification, a, end security, systems an, d, m, s are comp, o, ently with o, p, The Office, o, s, surance) h, a, e, s are heavi, l, o, n Grid (GIG,), s, ophisticated, c, e as a tool f, o, b, le to accom,, or distruste, b, er plan (CN, C, m, ent as one, t, challenges, o, me such ch, a, y, stems whos, e, n, critical sy, s, u, l mission as, s, n, ote address, to design to, e, ditation. Sy, s, s, cuss the ch, a\u00a0\u2026", "num_citations": "3\n", "authors": ["1190"]}
{"title": "Simulating bioterrorism through epidemiology approximation\n", "abstract": " Bioterrorism represents a significant threat to society. The lack of successful attacks that have resulted in true epidemics have created a need for data that can be generated from existing known factors. We have taken the popular susceptible-infected-recovery model and created a hybridized model that balances the simplicity of the original with an approximation of what more complex agent-based models already offer, with an emphasis on the exploration of large search spaces. Our experiments focus on more unconventional methods of intervention in the event of an epidemic, the results of which suggest a much more basic approach can be taken beyond the thoroughly explored realm of inoculation strategies.", "num_citations": "3\n", "authors": ["1190"]}
{"title": "Transaction-consistent global checkpoints in a distributed database system\n", "abstract": " Checkpointing and rollback recovery are well-known techniques for handling failures in distributed database systems. In this paper, we establish the necessary and sufficient conditions for the checkpoints on a set of data items to be part of a transaction-consistent global checkpoint of the distributed database. This can throw light on designing efficient, non-intrusive checkpointing techniques and transparent recovery techniques for distributed database systems.", "num_citations": "3\n", "authors": ["1190"]}
{"title": "Guest editorial: special issue on privacy preserving data management\n", "abstract": " Recent developments in information system technologies have resulted in computerizing many applications in various business areas. Data have become a critical resource in many organizations, and therefore, efficient access to data, sharing the data, extracting information from the data, and making use of the information have become an urgent need. The advent of the World Wide Web has resulted in even greater demand for managing data, information and knowledge effectively. As the demand for data and information management increases, there is also a critical need for maintaining the security of the data, applications and information systems. In such a scenario, one of the key issues is related to privacy. Information privacy relates to an individual\u2019s right to determine how, when, and to what extent personal information will be released to another person or organization. For example, since data mining tools make automatic associations, even na\u00efve users could deduce private information from the unclassified or public pieces of data, simply exploiting the associations made available by these tools. Note that while confidentiality deals with authorized release of data from servers to users, privacy deals with a user determining what data can be released about him or her to the public including to various web sites. A crucial need is therefore the development of privacy-preserving techniques for data management, able to trade-off between", "num_citations": "3\n", "authors": ["1190"]}
{"title": "Multilevel secure teleconferencing over public switched telephone network\n", "abstract": " Two-way group voice communications, otherwise known as teleconferencing are common in commercial and defense networks. One of the main features of military teleconferences is the need to provide means to enforce the Multilevel Security (MLS) model. In this paper we propose an architecture and protocols facilitating MLS conferences over Public Switched Telephone Network (PSTN). We develop protocols to establish secure telephone conferencing at a specific security level, add and drop conference participants, change the security level of an ongoing conference, and tear down a conference. These protocols enforce MLS requirements and prevent against eavesdropping. Our solution is based on encryption methods used for user and telephone authentication and message encryption, and trusted authentication centers and certificate authorities. We provide an initial estimate of signaling delays of\u00a0\u2026", "num_citations": "3\n", "authors": ["1190"]}
{"title": "Trust management in a distributed environment\n", "abstract": " Cybercrime as well as threats to national security are costing U.S. organizations billions of dollars each year. These organizations could be government organizations, financial corporations, medical hospitals and academic institutions. There is a critical need for organizations to share data within and across the organizations so that analysts could analyze the data, mine the data, and make effective decisions. Each organization could share information within the infosphere of that organization. An infosphere may consist of the data, applications and services that are needed for the operation of the organization. Organizations may share data with one another across what is called a global infosphere that spans multiple infospheres. While access control is an important security concern for organizational data sharing, managing trust is also an important consideration. For example, A may have the authorization to share\u00a0\u2026", "num_citations": "3\n", "authors": ["1190"]}
{"title": "Secure Knowledge Management\n", "abstract": " Secure Knowledge Management Page 1 Secure Knowledge Management Dr. Bhavani Thuraisingham The National Science Foundation September 2004 Page 2 2 1/3/2005 11:30 Outline 0 Background on Knowledge Management 0 Aspects of Secure Knowledge Management 0 Secure Knowledge Management Technologies - Eg, Secure Semantic Web Directions Page 3 3 1/3/2005 11:30 What is Knowledge Management? 0 Knowledge management, or KM, is the process through which organizations generate value from their intellectual property and knowledge-based assets 0 KM involves the creation, dissemination, and utilization of knowledge 0 Reference: http://www.commerce-database.com/knowledgemanagement.htm?source=google Page 4 4 1/3/2005 11:30 Aspects of Knowledge Management Components: \u2026", "num_citations": "3\n", "authors": ["1190"]}
{"title": "Object-oriented data management and distribution for real-time command and control systems\n", "abstract": " This paper describes requirements and a proposed design for object-oriented data management and distribution services for real-time command and control systems. These systems are used to manage situation awareness information in a military environment. They exhibit both soft and hard real-time characteristics. In the past, these systems have been especially difficult to build and maintain, due to their complexity and the general lack of commercial solutions.", "num_citations": "3\n", "authors": ["1190"]}
{"title": "KNOWLEDGE-BASED INFERENCE CONTROL IN A. MULTILEVEL SECURE? DATABASE MANAGEMENT SYSTEM\n", "abstract": " In\u017fcrcncc problem is the problem of users dcducing unauthorized information from the legitimate information that they acquirc. Wc arc particularly intcrcsted in thc infcrcncc problem which occurs in a multilevel operating environment. In such an environment, users arc clcarcd at different sccurity levels and they access a multilevel database where the data is classificd at different sensitivity levels. A multilevel sccurc database management system (MLS/DBMS) manages a multilevel database whcrc ils users cannot access data to which they are not authorized. However, providing a solution to the infcrcncc problem, where users issue multiple requests and consequently in\u017fer unauthorized knowlcdgc, is beyond the capability of currently availablc MLS/DBMSs.Duc to the complexity of the in\u017fercncc problem (scc for examplc (THUR90a)), we believe that a triple approach to rescarch is nccdcd to combat it; onc is to build infcrcncc controllers which act during transaction processing, the other is to build inference controllers for databasc design, and the third is to build in\u017fercncc controllers lo act as advisors to thc System Sccurity Officer (SSO). In our recent papers, we have described prolotypes for handling the in\u017fcrencc problem during qucry and update processing (FORD90, COLL90). In addition, Icchniques for handling this problem during databasc design have also been proposed (THUR91a). While the previous approaches cnablc thc dcicction and/or prevention of simplc in\u017fcrcnce stralcgics that users could utilize to draw in\u017fercnces, wc bclicvc that for an in\u017ference controller lo bc c\u017f\u017fectivc, it should be able to capture the complex reasoning strategics of\u00a0\u2026", "num_citations": "3\n", "authors": ["1190"]}
{"title": "Cloud Governance\n", "abstract": " Corporate governance has received a lot of prominence since the accounting scandals of the early 2000s. Corporate governance not only includes maintaining correct financial records, but also the information technology aspects of the corporation. These include cyber security governance, artificial intelligence governance and cloud governance. Most corporations are migrating their data, software and processes to the cloud and therefore it is critical that the cloud be governed by appropriate policies and procedures. We also need cloud governance frameworks. The roles and responsibilities of the people involved have to be clearly identified. In this paper we discuss some of the essential points of cloud governance.", "num_citations": "2\n", "authors": ["1190"]}
{"title": "Can AI be for Good in the Midst of Cyber Attacks and Privacy Violations? A Position Paper\n", "abstract": " Artificial Intelligence (AI) is affecting every aspect of our lives from healthcare to finance to driving to managing the home. Sophisticated machine learning techniques with a focus on deep learning are being applied successfully to detect cancer, to make the best choices for investments, to determine the most suitable routes for driving as well as to efficiently manage the electricity in our homes. We expect AI to have even more influence as advances are made with technology as well as in learning, planning, reasoning and explainable systems. While these advances will greatly advance humanity, organizations such as the United Nations have embarked on initiatives such as\" AI for Good\" and we can expect to see more emphasis on applying AI for the good of humanity especially in developing countries. However, the question that needs to be answered is Can AI be for Good when when the AI techniques can be\u00a0\u2026", "num_citations": "2\n", "authors": ["1190"]}
{"title": "Cyber Security and Data Governance Roles and Responsibilities at the C-Level and the Board\n", "abstract": " Corporate governance and the roles and responsibilities of the corporate officers and the board of directors have received an increasing interest since the Enron scandal of the early 2000s. This scandal resulted in enacting policies, laws and regulations such as the Sarbanes-Oxley and others. More recently, the near daily cyber security attacks on the infrastructures and data assets of corporations have resulted in cyber security professionals taking a serious look at the roles and responsibilities of the corporate officers and the board with respect to cyber security and data governance. This paper discusses the issues and challenges for cyber security governance with an emphasis on data governance and the potential roles and responsibilities of the corporate officers and the board of directors.", "num_citations": "2\n", "authors": ["1190"]}
{"title": "Graph-based data-collection policies for the internet of things\n", "abstract": " Smart industrial control systems (eg, smart grid, oil and gas systems, transportation systems) are connected to the internet, and have the capability to collect and transmit data; as such, they are part of the IoT. The data collected can be used to improve services; however, there are serious privacy risks. This concern is usually addressed by means of privacy policies, but it is often difficult to understand the scope and consequences of such policies. Better tools to visualise and analyse data collection policies are needed. Graph-based modelling tools have been used to analyse complex systems in other domains. In this paper, we apply this technique to IoT data-collection policy analysis and visualisation. We describe graphical representations of category-based data collection policies and show that a graph-based policy language is a powerful tool not only to specify and visualise the policy, but also to analyse policy\u00a0\u2026", "num_citations": "2\n", "authors": ["1190"]}
{"title": "Near real-time atrocity event coding\n", "abstract": " In recent years, mass atrocities, terrorism, and political unrest have caused much human suffering. Thousands of innocent lives have been lost to these events. With the help of advanced technologies, we can now dream of a tool that uses machine learning and natural language processing (NLP) techniques to warn of such events. Detecting atrocities demands structured event data that contain metadata, with multiple fields and values (e.g. event date, victim, perpetrator). Traditionally, humans apply common sense and encode events from news stories but this process is slow, expensive, and ambiguous. To accelerate it, we use machine coding to generate an encoded event. In this paper, we develop a near-real-time supervised machine coding technique with an external knowledge base, WordNet, to generate a structured event. We design a Spark-based distributed framework with a web scraper to gather news\u00a0\u2026", "num_citations": "2\n", "authors": ["1190"]}
{"title": "Emergency-Driven Assured Information Sharing in Secure Online Social Networks: A Position Paper\n", "abstract": " The United States and its Allied Forces have had tremendous success in combat operations. This includes combat in Germany, Japan and more recently in Iraq and Afghanistan. However not all of our stabilization and reconstruction operations (SARO) have been as successful. Several studies have been carried out on SARO by organizations such as the National Defense University and the Naval Post Graduate School. These studies have shown that security as well as power and jobs are key ingredients for success during SARO. One of the major conclusions is that we need to plan for SARO while we are planning for combat. These studies have also analyzed the various technologies that are needed for successfully carrying out SARO which include sensors, robotics and information management. As stated in the work by the Naval Postgraduate School, we need to determine the social, political and economic\u00a0\u2026", "num_citations": "2\n", "authors": ["1190"]}
{"title": "Real-Time Stream Data Analytics for Multi-purpose Social Media Applications\n", "abstract": " This paper describes a real-time information integration and analytics system called InXite for multi-purpose applications. InXite is designed to detect evolving patterns and trends in streaming data including social media data (e.g., tweets). InXite comprises of multiple modules including InXite Registration and Dashboard, InXite Real-time Data Streamer, InXite Information Integrator and InXite Analytics Engine each of which is outlined in this document. At present, InXite has been demonstrated for security, marketing and law enforcement applications.", "num_citations": "2\n", "authors": ["1190"]}
{"title": "Panel: Essential data analytics knowledge forcyber-security professionals and students\n", "abstract": " Increasingly, techniques from data analytics fields of statistics, machine learning, data mining, and natural language processing are being employed for challenges in cyber-security and privacy. This panel examines which techniques from these fields are essential for current and future cyber-security practioners and what are the related considerations involved in successfully solving security and privacy challenges of the future.", "num_citations": "2\n", "authors": ["1190"]}
{"title": "A generalized approach for social network integration and analysis with privacy preservation\n", "abstract": " Social network analysis is very useful in discovering the embedded knowledge in social network structures, which is applicable in many practical domains including homeland security, publish safety, epidemiology, public health, electronic commerce, marketing, and social science. However, social network data is usually distributed and no single organization is able to capture the global social network. For example, a law enforcement unit in Region A has the criminal social network data of her region; similarly, another law enforcement unit in Region B has another criminal social network data of Region B. Unfortunately, due the privacy concerns, these law enforcement units may not be allowed to share the data, and therefore, neither of them can benefit by analyzing the integrated social network that combines the data from the social networks in Region A and Region B. In this chapter, we discuss aspects of\u00a0\u2026", "num_citations": "2\n", "authors": ["1190"]}
{"title": "Behavioral sequence prediction for evolving data stream\n", "abstract": " Behavioral pattern prediction has many applications, ranging from consumer buying behavior analysis, web surfing prediction to network attack prediction. The traditional behavioral prediction technique works mainly on a fixed dataset. But recent advances in digital technology generates a huge amount of data which contributes to data stream. Data evolves over time due to the concept drift. Stream-based classification also needs to evolve over time. Our goal is not to predict a single action/behavior, but a sequence of actions that can occur later depending on the previous actions. We call this problem \u201cBehavioral Pattern Extrapolation\u201d. In our research, we exploited a stream mining based technique along with Markovian model, where we used an incremental and ensemble based technique for predicting a set of future actions. We have experimented using a number of benchmark datasets and shown the\u00a0\u2026", "num_citations": "2\n", "authors": ["1190"]}
{"title": "Design and implementation of SNODSOC: Novel class detection for social network analysis\n", "abstract": " This paper describes a framework, SNODSOC (Stream based novel class detection for social network analysis), that detects evolving patterns and trends in social microblogs. SNODSOC extends our powerful data mining system, SNOD (Stream-based Novel Class Detection) for now detecting novel patterns and trends within microblogs.", "num_citations": "2\n", "authors": ["1190"]}
{"title": "On-line anomaly detection based on relative entropy\n", "abstract": " Because the internet and computer networks are exposed to rapidly increasing number of serious security threats, efficient and effective anomaly detection techniques have become a necessity to secure the internet and computer networks. Traditional signature based anomaly detection techniques failed to detect polymorphic and new security threats. In this paper, we propose an online worm detection system based on relative entropy. The system effectively profiles network traffic features and then uses relative entropy to dynamically determine the traffic changes. It then applies adaptive filter to differentiate the traffic changes and determines whether the traffic is normal or contains worms. Our experimental results show that the proposed system is efficient for on-line anomaly detection, using traffic trace collected in high-speed links.", "num_citations": "2\n", "authors": ["1190"]}
{"title": "Information Sharing Strategies of the United States Federal Government and Its Allies and Our Contributions Towards Implementing these Strategies. Version 1\n", "abstract": " This report provides a survey of the major initiatives within the US Federal Government in Information Sharing. In particular, we discuss information sharing strategies and initiatives within the Department of Defense, Department of Justice Department of Homeland Security, and Director of National Intelligence. In addition we also discuss some of the initiatives within agencies such as the Department of Health and Human Services, as well as some of the international efforts within the United Kingdom and Australia. This report was prepared as part of the AFOSR MURI project on Assured Information Sharing. A deliverable under this project is to monitor the strategies of the US Federal Government and its allies so that we can develop effective solutions to the major problem of information sharing critical to fight the global war on terror. This report will be updated as progress is made on information sharing within the federal agencies in the United States and in the allied nations.Descriptors:", "num_citations": "2\n", "authors": ["1190"]}
{"title": "Information Operations Across Infospheres\n", "abstract": " The research reported in this final report was carried out mainly at the University of Texas at Dallas UTD between December I, 2005 and November 30, 2008. It describes the issues and challenges for information operations across infospheres and focuses on assured information sharing. We have examined three models In the first model the partners of the coalition are considered to be trustworthy. In the second model, the partners are semi-trustworthy. In the third model the partners are untrustworthy. George Mason University GMU received a subcontract from the University of Texas at Dallas to examine the use of Role-based Access Control RBAC and Usage Control models for Coalition data sharing. The PI moved to UTSA and received the next phase of the funding to work on group-based information sharing Part 1.Descriptors:", "num_citations": "2\n", "authors": ["1190"]}
{"title": "Secure semantic sensor web and pervasive computing\n", "abstract": " In this paper we discuss issues on developing a secure semantic sensor web. SensorML is the starting point for this work. We explore the layers for a semantic sensor web and discuss security issues. We also discuss secure peer-to-peer computing as it relates to sensor web.", "num_citations": "2\n", "authors": ["1190"]}
{"title": "Why is Interdisciplinary Research Hard\n", "abstract": " For those who have read \u201cMy Story\u201d or browsed through my website, you know that I was a Program Director at NSF between 2001 and 2004. That was when I was introduced to interdisciplinary research. Before that, while I was at MITRE, all of my research was in computer science including in information security and data management. However, when I went to NSF, I soon realized that interdisciplinary research was strongly encouraged. Furthermore, I was the representative from my division on programs in bioinformatics and geo-informatics. This was because Dr. Maria Zemankova invited me to join NSF as IPA and take over her programs while she was on sabbatical at NLM conducting interdisciplinary research herself. I learnt at that time that biologists, computer scientists and geologists had to work together to advance bioinformatics and geo-informatics. I learnt a lot about bioinformatics from Dr. Sylvia Spengler\u00a0\u2026", "num_citations": "2\n", "authors": ["1190"]}
{"title": "Building a Geosocial Semantic Web for Military Stabilization and Reconstruction Operations\n", "abstract": " The United States and its Allied Forces have had tremendous success in combat operations. This includes combat in Germany, Japan and more recently in Iraq and Afghanistan. However not all of our stabilization and reconstruction operations (SARO) have been as successful. Recently several studies have been carried out on SARO by National Defense University as well as for the Army Science and Technology. One of the major conclusions is that we need to plan for SARO while we are planning for combat. That is, we cannot start planning for SARO after the enemy regime has fallen. In addition, the studies have shown that security, power and jobs are key ingredients for success during SARO. For example, it is essential that security be maintained. Furthermore, it is important to give powerful positions to those from the fallen regime provided they are trustworthy. It is critical that investments are made to\u00a0\u2026", "num_citations": "2\n", "authors": ["1190"]}
{"title": "Preventing private information inference attacks on social networks\n", "abstract": " On-line social networks, such as Facebook, are increasingly utilized by many people. These networks allow users to publish details about themselves and connect to their friends. Some of the information revealed inside these networks is meant to be private. Yet it is possible that corporations could use learning algorithms on released data to predict undisclosed private information. In this paper, we explore how to launch inference attacks using released social networking data to predict undisclosed private information about individuals. We then devise three possible sanitization techniques that could be used in various situations. Then, we explore the effectiveness of these techniques by implementing them on a dataset obtained from the Dallas/Fort Worth, Texas network of the Facebook social networking application and attempting to use methods of collective inference to discover sensitive attributes of the data set\u00a0\u2026", "num_citations": "2\n", "authors": ["1190"]}
{"title": "Real-time knowledge discovery and dissemination for intelligence analysis\n", "abstract": " This paper describes the issues and challenges for real-time knowledge discovery and then discusses approaches and challenges for real-time data mining and stream mining. Our goal is to extract accurate information to support the emergency responder, the war fighter, as well as the intelligence analyst in a timely manner.", "num_citations": "2\n", "authors": ["1190"]}
{"title": "Data mining for security Applications and Its privacy implications\n", "abstract": " In this paper we first examine data mining applications in security and their implications for privacy. We then examine the notion of privacy and provide an overview of the developments especially those on privacy preserving data mining. We then provide an agenda for research on privacy and data mining.", "num_citations": "2\n", "authors": ["1190"]}
{"title": "Differences in Fitts\u2019 Law Task Performance Based on Environment Scaling\n", "abstract": " Haptics research has begun implementing haptic feedback in tasks of great precision and skill, such as robotic surgery. Haptic displays can represent task environments with arbitrary scaling. Fitts\u2019 Law suggests differences in the scale of a workspace rendered on a visual display and in a haptic display should not affect performance of those tasks. However, interactions of great precision and skill may require understanding and verifying the influence of perceiving an environment when the visual and haptic displays represent those environments with differing scales. This experiment measured the influence that mismatched haptic and visual display scalings had on movement times in. Each of five treatments used different scales in the visual and the haptic displays. A Friedman rank test showed a significant difference across all treatments. A post hoc pairwise comparison showed a nearly significant\u00a0\u2026", "num_citations": "2\n", "authors": ["1190"]}
{"title": "A Framework for the Relational Transformation of RDF Data\n", "abstract": " The astronomical growth and ubiquitous acceptance of the World Wide Web and its applications resulted in data explosion that has, in turn, has given rise to a need for data representation methodologies and standards that would enable intelligent autonomous software agents to access and present the required information in a rapid and automated manner. The Resource Description Framework is one such standard proposed by the World Wide Web consortium that aims to achieve the above goals by providing machine understandable semantics for describing and interchanging data about web resources. The rapid acceptance and widespread deployment of RDF on the Internet has increased the demand for efficient RDF data storage and retrieval options which has, in turn, resulted in the emergence of a new data storage paradigm, the RDF Graph Model. Any data storage methodology requires a rich suite of\u00a0\u2026", "num_citations": "2\n", "authors": ["1190"]}
{"title": "Multimedia systems security\n", "abstract": " Multimedia data and information systems manage, communicate, and present multimedia data including text, images, audio and video. We need to ensure that the data is protected from unauthorized access as well as malicious corruption. Digital watermarking techniques that insert hidden copyright messages into the multimedia data are needed. Furthermore, since multimedia data is being used for security applications such as surveillance and monitoring, protecting privacy of the individual is crucial.", "num_citations": "2\n", "authors": ["1190"]}
{"title": "Enforcing honesty in assured information sharing within a distributed system\n", "abstract": " The growing number of distributed information systems such as the internet has created a need for security in data sharing. When several autonomous parties attempt to share data, there is not necessarily any guarantee that the participants will share data truthfully. In fact, there is often a large incentive to engage in behavior that can sabotage the effectiveness of such a system. We analyze these situations in light of game theory, a mathematical model which permits us to consider behavior and choices for any autonomous party. This paper uses this theory to create a behavior enforcement method that does not need a central management system. We use a simple punishment method that is inherently available in most scenarios. Our approach is applicable to a variety of assured information sharing applications where the members of a coalition have to work together to solve a problem.", "num_citations": "2\n", "authors": ["1190"]}
{"title": "Design of secure CAMIN application system based on dependable and secure TMO and RT-UCON\n", "abstract": " Increasingly the need for protecting information from unauthorized access has lead to more attention in the field of information security. Access control mechanisms have been in place for the last four decades and are a powerful tool utilized to ensure security. In a real-time distributed computing environment, systems have to meet timing constraints for time sensitive applications, such as obtaining financial quotes and operating command and control systems. However, real-time distributed computing environment security is yet to be investigated. Earlier work examined a Time Triggered Message Triggered Object (TMO) scheme that provides real-time services in a distributed computing environment secured by applying a Role-Based Access Control (RBAC). In this paper we describe the application of a sophisticated Usage Control (UCON) model for a TMO and subsequently design an application that utilizes a\u00a0\u2026", "num_citations": "2\n", "authors": ["1190"]}
{"title": "Detecting New malicious Executables Using Data Mining\n", "abstract": " We have applied a novel approach in detecting new malicious codes. Our ap-proach applies a reverse engineering technique by disassembling the executable codes and extracting features from the disassembled code. We have extracted n-gram features from the assembly instructions and performed feature reduction using Principal Component Analysis and applied classification techniques such as Na\u00efve Bayes and Support Vector Machine on the data. Results indicate that this approach is promising. We wish to explore the possibility of extracting more features from the assembly code and improve accuracy of detection in our future work.", "num_citations": "2\n", "authors": ["1190"]}
{"title": "Access control for web data: models and policy languages\n", "abstract": " The web has made easily available an enormous amount of information in digital form and has facilitated the exchange of such information. In this environment, access control is a primary concern. The key issue is how to trade-off between maximizing the sharing of information and enforcing a controlled access to web data. In this paper we start by outlining which are the main access control requirements of web data. Then, we review researches carried on in the field, by mainly focusing on xml. Finally, we discuss policy languages for the semantic web, and outline which are the main research directions in this field.", "num_citations": "2\n", "authors": ["1190"]}
{"title": "Face Recognition Using Various Classifiers: Artificial Neural Network\n", "abstract": " Automatic identification of users enables intelligent servicing in an interconnected home environment. In this paper, we propose a near real-time effective face recognition system for consumer applications. Since the nature of application domain requires real time result and better accuracy, it poses a serious challenge. To address this challenge, we propose a feature reduction technique using Principle component analysis (PCA) to facilitate real time face recognition along with better accuracy. In addition, to improve face recognition classifier accuracy, we propose a hybrid model that combines various classification techniques namely, Artificial Neural Networks (ANN), and Linear Fisher Discriminant Analysis (LDA) to resolve recognition using linear average weighting. Such fusion overcomes the drawbacks of each classifier. We have investigated the effectiveness of our hybrid model by comparing our results with\u00a0\u2026", "num_citations": "2\n", "authors": ["1190"]}
{"title": "Towards access control for visual Web model management\n", "abstract": " With the advance of e-commerce over Web-based information, the interoperability of isolated XML repositories and databases over the Internet has drawn an increasing interest recently. Little effort, however, has been made to preserve necessary autonomy and security of each individual XML repository or database during information exchange or evolution. Generic model management has been intensively researched and also implemented in a prototype since its first introduction. Security related research is yet to be conducted for model management. This paper presents a uniform security model for access control specifications of heterogeneous data models over the Web. Based on the uniform representation, we present security extensions to our previous work on visual model management operators for managing access control specifications to allow heterogeneous Web data models to exchange information\u00a0\u2026", "num_citations": "2\n", "authors": ["1190"]}
{"title": "Grand challenges in data integrity and quality: Panel discussion\n", "abstract": " This paper provides a summary of the panel discussions on data quality at the IFIP Integrity and Internal Control in Information Systems Conference held in Lausanne, Switzerland, November 2003.", "num_citations": "2\n", "authors": ["1190"]}
{"title": "Dependable computing for national security: a position paper\n", "abstract": " This paper examines various security threats and then discusses the applications of dependable computing for national security. By a dependable computing system we mean a flexible system that enforces quality of service policies between security, real-time processing, integrity and fault tolerance. We also discuss infrastructures and business objects for counter-terrorism.", "num_citations": "2\n", "authors": ["1190"]}
{"title": "Simulation of join query processing algorithms for a trusted distributed database management system\n", "abstract": " The paper describes the simulation of join query processing algorithms for a trusted distributed database management system (TDDBMS). In a TDDBMS, users cleared at different security levels access and share a distributed database with data at different sensitivity levels without compromising security. The query processing algorithms should ensure that users only obtain the information classified at or below their security levels. The paper also describes two algorithms for the join operation and discusses the simulation of these algorithms. Some of the results obtained from the simulation with those obtained from a prototype implementation are then compared, and finally, the authors recommend an algorithm for join query processing in a TDDBMS.", "num_citations": "2\n", "authors": ["1190"]}
{"title": "Security for object-oriented systems\n", "abstract": " Object-oriented systems are gaining increasing popularity due to their inherent ability to represent conceptual entities as objects, which is similar to the way humans view the world. This power of representation has led to the development of new generation applications such as Multimedia information processing, Artificial Intelligence, CAD/CAM, and Process control systems. In addition to the power of representation, object-oriented approaches are also being used to design software components and to interconnect heterogeneous database systems.However, the increasing popularity of object-oriented systems should not obscure the need to maintain security of operation. That is, it is important that such systems operate securely in order to overcome any malicious corruption of data as well as to prohibit unauthorized access to and use of classified data. For applications such as C41, it is also important to provide\u00a0\u2026", "num_citations": "2\n", "authors": ["1190"]}
{"title": "The concept of -cylinder and its relationship to simple sets.\n", "abstract": " 1 Introduction The concept of'^-cylinder'was originally defined [4] in order to construct noncylindrical decision problems using System functions, a kind of function defined by Cleave [1]. It is a generalization of Young's [6] concept of a semicylinder and it forms a link between a semicylinder and a cylinder. Its definition is as follows:", "num_citations": "2\n", "authors": ["1190"]}
{"title": "Real-time, stream data information integration and analytics system\n", "abstract": " A real-time, stream data information integration and analytics system can include an information engine that performs real-time entity extraction to create key-value pairs of attributes for a personal profile and integrates similar personal profiles generated from same or different data sources into a single person-of-interest profile. The real-time, stream data information integration and analytics system can further include a real-time analytics module that performs a variety of analytics using the person-of-interest profiles and updates the person-of-interest profiles with scores and other results of the variety of analytics. The variety of real-time analytics can include sentiment analysis and at least some aspects of threat detection analysis. The sentiment analysis can be used to provide a recommender system and the threat detection analysis can be used to identify and predict threats and opportunities.", "num_citations": "1\n", "authors": ["1190"]}
{"title": "Blockchain Technologies and Their Applications in Data Science and Cyber Security\n", "abstract": " Blockchain technologies have been very effective in processing distributed transactions securely. They have many applications including in handling bitcoin cryptocurrencies and smart contracts. More recently the use of blockchain has been explored for data science applications. This paper examines blockchain technologies and discusses their applications in data science and cyber security.", "num_citations": "1\n", "authors": ["1190"]}
{"title": "Graphboot: Quantifying uncertainty in node feature learning on large networks\n", "abstract": " In recent years, as online social networks continue to grow in size, estimating node features, such as sociodemographics, preferences and health status, in a scalable and reliable way has become a primary research direction in social network mining. Although many techniques have been developed for estimating various node features, quantifying uncertainty in such estimations has received little attention. Furthermore, most existing methods study networks parametrically, which limits insights about necessary quantity of queried data, reliable feature estimation, and estimator uncertainty. Uncertainty quantification is critical for answering key questions, such as, given a limited availability of social network data, how much data should be queried from the network?, and which node features can be learned reliably? More importantly, how can we evaluate uncertainty of our estimators? Uncertainty quantification is not\u00a0\u2026", "num_citations": "1\n", "authors": ["1190"]}
{"title": "An OpenRBAC semantic model for access control in vehicular networks\n", "abstract": " Inter-vehicle communication has the potential to significantly improve driving safety, but also raises security concerns. The fundamental mechanism to govern information sharing behaviors is access control. Since vehicular networks have a highly dynamic and open nature, access control becomes very challenging. Existing works are not applicable to the vehicular world. In this paper, we develop a new access control model, openRBAC, and the corresponding mechanisms for access control in vehicular systems. Our approach lets the accessee define a relative role hierarchy, specifying all potential accessor roles in terms of their relative perception to the accessees. Access control policies are defined for the relative roles in the hierarchy. Since the accessee has a clear understanding of the relative roles defined by itself, the policy definitions can be precise and less flawed.", "num_citations": "1\n", "authors": ["1190"]}
{"title": "Attacklets: Modeling high dimensionality in real world cyberattacks\n", "abstract": " We introduce attacklets, a novel approach to model the high dimensional interactions in cyberattacks. Attacklets are implemented using a real-world dataset of cyberattacks from the Verizon Data Breach Investigation Report. Whereas the commonly used attack graphs model the action sequences of attackers for specific exploits, attacklets model general attributes and states of each attack separately. Attacklets may inform the number and types of attributes across a wide range of cyberattacks. These structural properties can then be used in machine learning models to classify and predict future cyberattacks.", "num_citations": "1\n", "authors": ["1190"]}
{"title": "Deploying malware detection software for smart phones\n", "abstract": " As mobile computing devices become more widely used and integrated with more crucial systems like infrastructure and military or law enforcement, it becomes more apparent that these mobile systems should be the target of malware and other forms of cyber-attack. Limited system resources make classic signature detection difficult. As new methods for detection develop, it is important that we maintain the ability to confidently block harmful applications from running. We look at using default system resources to maintain control of application installation and execution. The confirmation and proper detection of malicious activity still requires offrite connectivity; however, without this connectivity, the default mobile resources can still be used to maintain control of a clean system environment until verification is made. Though the entire malware detection system is not demonstrated here, the proof of a gateway protocol\u00a0\u2026", "num_citations": "1\n", "authors": ["1190"]}
{"title": "Assured information sharing (AIS) using private clouds\n", "abstract": " Assured Information Sharing (AIS) is a framework that allows cooperating organizations to share information in a manner that respects the privacy, confidentiality and security of the data of each individual in each organization. In this chapter, we present an overview of AIS by detailing the motivations behind AIS, a goal-oriented architecture for AIS and challenges that must be overcome before the adoption of AIS. In addition, we present historical as well as recent research advances that have been made towards addressing the challenges that lie within an AIS framework. Finally, we describe the details of two novel cloud-based AIS implementations that support the high availability, scalability, agility and efficiency required for realizing the vision of AIS.", "num_citations": "1\n", "authors": ["1190"]}
{"title": "Practical privacy preserving record integration and search\n", "abstract": " The integration of information dispersed among multiple repositories is a crucial step for accurate data analytics. In support of this goal, it is critical to devise procedures for identifying similar records across distinct data sources. At the same time, to adhere to privacy regulations and policies, such procedures should protect the confidentiality of the individuals to whom the information corresponds. Various techniques have been proposed to resolve the tension, involving secure multi-party computation (SMC) and similarity preserving data transformation. SMC methods provide secure and accurate solutions to the problem, but they are prohibitively expensive in practice. Data transformation techniques offer more practical solutions, but incur the cost of information leakage and false matches. Unfortunately, uncontrolled information leakage is subject to adversarial attacks. In fact, we introduce a novel cryptanalysis against\u00a0\u2026", "num_citations": "1\n", "authors": ["1190"]}
{"title": "Tag-based Information Flow Analysis for Document Classification in Provenance.\n", "abstract": " A crucial aspect of certain applications such as the ones pertaining to Intelligence domain or Health-care, is to manage and protect sensitive information effectively and efficiently. In this paper, we propose a tagging mechanism to track the flow of sensitive or valuable information in a provenance graph and automate the process of document classification. When provenance is initially recorded, the documents of a provenance graph are assumed to be annotated with tags representing their sensitivity or priority. We then propagate the tags appropriately on the newly generated documents using additional inference rules defined in this paper. This approach enables users to conveniently query to identify sensitive or valuable information, which can now be efficiently managed or protected once identified.", "num_citations": "1\n", "authors": ["1190"]}
{"title": "Efficient incentive compatible secure data sharing\n", "abstract": " As time goes on, humans collect more and more data. This data is used to create useful information which allows us to make decisions. Not all data is in one database, however. Humanity's data is split among millions of different owners: companies, individuals, and governments. In order to create better information for better decisions, data sharing can be employed. However, the different motivations of the owners can cause problems in the data sharing process. This dissertation examines the different motivations involved in data sharing, and proposes methods to both expedite and assure the data sharing process's completion. In particular, it considers approximations in secure data mining, enforcing honesty in data sharing through game theory, and improvements on the verification of outsourced data queries.", "num_citations": "1\n", "authors": ["1190"]}
{"title": "A database inference controller for 3D motion capture databases\n", "abstract": " This article presents a strategy for restructuring private human motion capture data to enforce access and inference controls within a relational database management system. Human 3D motion capture data is an important part of electronic health records for patients with motionrelated diseases and symptoms. There are significant privacy concerns regarding the safe storage and dissemination of such data. Access controls traditionally applied to other forms of medical data (eg, textual data) are not well suited to motion captures, which contain large quantities of data with complex interdependencies that divulge privacy-violating inferences. Encoding such motion data effectively within a relational database brings the large body of relational database security research to bear on this important problem.", "num_citations": "1\n", "authors": ["1190"]}
{"title": "Design and Implementation of a Data Mining System for Malware Detection\n", "abstract": " This paper describes the design and implementation of a data mining system called SNODMAL (Stream based novel class detection for malware) for malware detection. SNODMAL extends our data mining system called SNOD (Stream-based Novel Class Detection) for detecting malware. SNOD is a powerful system as it can detect novel classes. We also describe the design of SNODMAL++ which is an extended version of SNODMAL.", "num_citations": "1\n", "authors": ["1190"]}
{"title": "Secure semantic computing\n", "abstract": " This paper explores the integration of semantic computing technologies with security technologies. Past and current research on the application of semantic web technologies for policy management and inference control, the application of data mining technologies for intrusion and malware detection, and programming language-based approaches to mobile code certification and data confidentiality enforcement are discussed.", "num_citations": "1\n", "authors": ["1190"]}
{"title": "Learning-based geospatial schema matching guided by external knowledge\n", "abstract": " Resolving semantic heterogeneity across distinct data sources remains a highly relevant problem in the GIS domain requiring innovative solutions. Our approach, called GSim, semantically aligns tables from respective GIS databases by first choosing attributes for comparison. We then examine their instances and calculate a similarity value between them called entropy-based distribution (EBD) by combining two separate methods. Our primary method discerns the geographic types from instances of compared attributes. If successful, EBD is calculated using only this method. GSim further facilitates geographic type matching by using latlong values to further disambiguate between multiple types of a given instance and applying attribute weighting to quantify the uniqueness of mapped attributes. If geographic type matching is not possible, we then apply a generic schema matching method which employs normalized\u00a0\u2026", "num_citations": "1\n", "authors": ["1190"]}
{"title": "Exploiting modern hardware for secure data management\n", "abstract": " Databases and data warehouses facilitate the storage and retrieval of large amounts of information. These systems have been increasingly used by corporations to store data such as unit cost information, customers credit card numbers, or patients medical records that are confidential in nature. This trend has been accompanied by numerous incidents of data theft and has resulted in more corporations preferring to store their sensitive records in an encrypted format. Since traditional cryptographic operations entail significant amount of arithmetic calculations, latencies generally arise while recording or retrieving sensitive data. We propose novel and efficient techniques to minimize the cost of cryptographic operations while processing sensitive information. These techniques involve exploiting the parallel computation capability of multi-core CPUs and IO latencies.", "num_citations": "1\n", "authors": ["1190"]}
{"title": "Privacy protection of personal data in social networks\n", "abstract": " The Web has evolved from a simple tool for publishing text data, into a complex collaborative knowledge management system. This evolution is mainly due to the rapid spread of social computing services, such as blogs, wikis, social networks, social bookmarking, collaborative filtering, and recommendation and reputation systems. Social computing services are now starting to be used also at the enterprise level to communicate, share information, make decisions, and do business. This is in line with the emerging trend known as Enterprise 2.0, that is, the use of Web 2.0 technologies within the intranet, to allow for more spontaneous, knowledge-based collaborations. One of the main witnesses of this new trend is represented by online social networks, platforms that allow people to publish details about themselves and to connect to other members of the network through friendship or other kind of links. Recently, the\u00a0\u2026", "num_citations": "1\n", "authors": ["1190"]}
{"title": "Efficient processing of large RDF streams using memory management algorithms\n", "abstract": " The University of Texas at Dallas Abstract. As more RDF streaming applications are being developed, there is a growing need for an efficient mechanism for storing and performing inference over these streams. In this poster, we present a tool that stores these streams in a unified model by combining memory and disk based mechanisms. We explore various memory management algorithms and disk-persistence strategies to optimize query performance. Our unified model produces an optimized query execution and inference performance for RDF streams that benefit from the advantages of using both, memory and disk.", "num_citations": "1\n", "authors": ["1190"]}
{"title": "Editorial SACMAT 2007\n", "abstract": " (SACMAT) held in Sophia Antipolis, France, in June 2007. SACMAT has become the premier forum for presentation of research results and experience reports on leading edge issues of access control including models, systems, applications, and theory. The mission of the symposium is to share novel access control solutions that fulfill the needs of heterogeneous applications and environments as well as to identify new directions for future research and development.The article \u201cPrivacy-aware Role-Based Access Control\u201d by Q. Ni, E. Bertino, J. Lobo, C. Brodie, C.-M. Karat, J. Karat, and A. Trombetta extends the popular role-based access control model with complex and realistic privacy policies. The article describes the security model as well as the design and implementation of a system based on this privacy-aware role-based access control also known as P-RBAC. The authors also compare and contrast their\u00a0\u2026", "num_citations": "1\n", "authors": ["1190"]}
{"title": "Cyber physical systems security applied to surgical robotics\n", "abstract": " Next generation surgical robots capable of teleoperated surgery are being developed by the BioRobotics Laboratory at the University of Washington and SRI International. Using these robots, surgeons may perform surgery on patients at remote locations. The robots are being developed with the intended purpose of being deployed to the battlefield to provide immediate medical assistance to injured soldiers by surgeons in safe and remote locations. These two research groups have begun developing an open standard called the Interoperable Telesurgery Protocol (ITP) which provides a standardized framework for communications between the surgical robot and the surgical robot controller. A collaboration between the BioRobotics Lab and The University of Texas at Dallas has been established to design and implement a security enhancement for the surgical robot cyber physical system. The Advanced Encryption\u00a0\u2026", "num_citations": "1\n", "authors": ["1190"]}
{"title": "Geographically-typed semantic schema matching\n", "abstract": " Resolving semantic heterogeneity across distinct data sources remains a highly relevant problem in the GIS domain requiring innovative solutions. Our approach, called GSim, semantically aligns tables from respective GIS databases by first choosing attributes for comparison. We then examine their instances and calculate a similarity value between them called entropy-based distribution (EBD) by combining two separate methods. Our primary method discerns the geographic types from instances of compared attributes. If geographic type matching is not possible, we then apply a generic schema matching method which employs normalized Google distance. We show the effectiveness of our approach over the traditional N-gram approach across multi-jurisdictional datasets by generating impressive results.", "num_citations": "1\n", "authors": ["1190"]}
{"title": "Design of a temporal geosocial semantic web for military stabilization and reconstruction operations\n", "abstract": " The United States and its Allied Forces have had tremendous success in combat operations. This includes combat in Germany, Japan and more recently in Iraq and Afghanistan. However not all of our stabilization and reconstruction operations (SARO) have been as successful. Recently several studies have been carried out on SARO by National Defense University as well as for the Army Science and Technology. One of the major conclusions is that we need to plan for SARO while we are planning for combat. That is, we cannot start planning for SARO after the enemy regime has fallen. In addition, the studies have shown that security, power and jobs are key ingredients for success during SARO. It is important to give positions to some of the power players from the fallen regime provided they are trustworthy. It is critical that investments are made to stimulate the local economies. The studies have also analyzed the\u00a0\u2026", "num_citations": "1\n", "authors": ["1190"]}
{"title": "On the mitigation of bioterrorism through game theory\n", "abstract": " Bioterrorism represents a serious threat to the security of civilian populations. The nature of an epidemic requires careful consideration of all possible vectors over which an infection can spread. Our work takes the SIR model and creates a detailed hybridization of existing simulations to allow a large search space to be explored. We then create a Stackelberg game to evaluate all possibilities with respect to the investment of available resources and consider the resulting scenarios. Our analysis of our experimental results yields the opportunity to place an upper bound on the worst case scenario for a population center in the event of an attack, with consideration of defensive and offensive measures.", "num_citations": "1\n", "authors": ["1190"]}
{"title": "Assured information sharing between trustworthy, semi-trustworthy and untrustworthy coalition partners\n", "abstract": " \u2022 Daniel Wolfe (formerly of the NSA) defined assured information sharing (AIS) as a framework that \u201cprovides the ability to dynamically and securely share information at multiple classification levels among US, allied and coalition forces.\u201d\u2022 The DoD\u2019s vision for AIS is to \u201cdeliver the power of information to ensure mission success through an agile enterprise with freedom of maneuverability across the information environment\u201d\u2022 9/11 Commission report has stated that we need to migrate from a need-to-know to a need-to-share paradigm\u2022 Our objective is to help achieve this vision by defining an AIS lifecycle and developing a framework to realize it.", "num_citations": "1\n", "authors": ["1190"]}
{"title": "Ontology Alignment Using Multiple Contexts.\n", "abstract": " Ontology alignment involves determining the semantic heterogeneity between two or more domain specifications by considering their associated concepts. Our approach considers name, structural and content matching techniques for aligning ontologies. After comparing the ontologies using concept names, we examine the instance data of the compared concepts and perform content matching using value types based on N-grams and Entropy Based Distribution (EBD). Although these approaches are generally sufficient, additional methods may be required. Subsequently, we compare the structural characteristics between concepts using Expectation-Maximization (EM). To illustrate our approach, we conducted experiments using authentic geographic information systems (GIS) data and generate results which clearly demonstrate the utility of the algorithms while emphasizing the contribution of structural matching.", "num_citations": "1\n", "authors": ["1190"]}
{"title": "Making quantitative measurements of privacy/analysis tradeoffs inherent to packet trace anonymization\n", "abstract": " Anonymization provides a mechanism for sharing data while obscuring private/sensitive values within the shared data. However, anonymization for sharing also sets up a fundamental tradeoff \u2013 the stronger the anonymization protection, the less information remains for analysis. This privacy/analysis tradeoff has been descriptively acknowledged by many researchers but no one has yet attempted to quantify this tradeoff. We perform anonymization options on network packet traces and make empirical measurements using IDS alarms as an indicator for security analysis capability. Preliminary results show most packet fields have unexpected complex tradeoffs while only two fields exhibiting the classic zero sum tradeoff.", "num_citations": "1\n", "authors": ["1190"]}
{"title": "IEEE ISI 2008 Invited Talk (I) Data Mining for Security Applications: Mining Concept-Drifting Data Streams to Detect Peer to Peer Botnet Traffic\n", "abstract": " There has been much interest on using data mining for counter-terrorism and cyber security applications. For example, data mining can be used to detect unusual patterns, terrorist activities and fraudulent behavior. In addition data mining can also be sued for intrusion detection and malicious code detection. Our current research is focusing extensively on data mining for security applications in general and data mining for botnet detection in particular. Our presentation will addressed both aspects of data mining applications.The term bot comes from the word robot. A bot is usually referred to automated software capable of performing certain functions. A botnet is a network of bots that are used by a human operator or botmaster to carry out malicious actions. Botnet is one of the most extreme tools used in cyber-crime these days including DDoS attacks, phishing, spamming, and spying on remote computers. Often businesses, governments, and individuals are facing multi-million-dollar damages caused by hackers with the help of these botnets. It is a major challenge to the cyber-security research community to combat this threat.", "num_citations": "1\n", "authors": ["1190"]}
{"title": "Directions and trends of XML and web service security\n", "abstract": " Many building blocks of secure architectures based on Web services are currently in place. To name but a few, the XML Signature specification, a joint effort of W3C and IETF, is aimed at providing data integrity and authentication features in XML format. W3C's XML Encryption standard addresses the issue of data confidentiality using encryption, wrapping encrypted data inside standard XML tags.WS-Security from OASIS defines a mechanism for including integrity, confidentiality, and single message authentication features within a SOAP message. WS-Security makes use of the XML Signature and XML Encryption specifications and defines how to include digital signatures, message digests, and encrypted data in a SOAP message.", "num_citations": "1\n", "authors": ["1190"]}
{"title": "Administering the semantic web: cpt: confidentiality, privacy and trust management\n", "abstract": " The semantic web is essentially an intelligent web. There has been much progress made on the semantic web including standards for extensible Markup Language, Resource Description Framework and Information Interoperability. However administration policies and techniques for enforcing them have received little attention. These policies include policies for security, privacy, data quality, integrity, trust and timely information processing. This paper discusses administration policies for the semantic web as well as techniques for enforcing them. In particular we will discuss an approach for ensuring confidentiality, privacy and trust for the semantic web. We will also discuss the inference and privacy problems within the context of administration policies.", "num_citations": "1\n", "authors": ["1190"]}
{"title": "Secure model management operations for the web\n", "abstract": " The interoperability among different data formats over the Internet has drawn increasing interest recently due to more and more heterogeneous data models are used in different Web services. In order to ease the manipulation of data models for heterogeneous data, generic model management has been intensively researched and also implemented in a prototype since its first introduction. Access control specifications attached to each individual data model require significant amount of efforts to manually specify. Based on a general security model for access control specifications on heterogeneous data models and its visual representation, we present secure model management operators for managing access control specifications.The secure model management operators disccussed in the paper include a secure match operator and a secure merge operator. We introduce a novel graphical schema\u00a0\u2026", "num_citations": "1\n", "authors": ["1190"]}
{"title": "Managing Threats to Web Databases and Cyber Systems\n", "abstract": " This chapter provides an overview of some of the cyber threats information systems as well as data management systems and then discusses potential solutions and challenges. The threats include access control violations, unauthorized intrusions and inference and aggregation. Solutions include role-based access control, data mining techniques and security constraint processing.", "num_citations": "1\n", "authors": ["1190"]}
{"title": "Web and information security\n", "abstract": " The advent of the World Wide Web (WWW) in the mid 1990s has resulted in even greater demand for managing data, information and knowledge effectively. As this demand grows, it is also critical that access to the data is secure. Furthermore, it is also important that the integrity of the data is maintained. There have been many developments recently on web and information security. The research areas include secure ecommerce, access control models, secure database management, and secure digital libraries.To discuss the developments as well as to provide directions for research in web and information security, we have organized this workshop at COMPSAC 2002. The workshop will first provide an overview of some of the recent developments in web and information security and will then focus on directions for future work. The topics of the papers to be presented include role-based access control, data\u00a0\u2026", "num_citations": "1\n", "authors": ["1190"]}
{"title": "Using CORBA to Integrate Database Systems\n", "abstract": " Information has become the most critical resource in many organizations, and the rapid growth of networking and database technologies has had a major impact on information processing requirements. Efficient access to information, as well as sharing it, have become urgent needs. As a result, an increasing number of databases in different sites are being interconnected. In order to reconcile the contrasting requirements of the different database management systems (DBMSs), tools that enable users of one system to use another system\u2019s data are being developed. Efficient solutions for interconnecting and administering different database systems are also being investigated.There are two aspects to the object-oriented approach to integrating heterogeneous database systems. In one approach, an object-oriented data model could be used as a generic representation scheme so that the schema transformations between the different database systems could be facilitated. In the other approach, a distributed object management system could be used to interconnect heterogeneous database systems. This chapter explores the distributed object management system approach by focusing on a specific distributed object management system: the object management group\u2019s (OMG) Common Object Request Broker Architecture (CORBA).", "num_citations": "1\n", "authors": ["1190"]}
{"title": "Data mining for the e-business: developments and directions\n", "abstract": " This paper describes data mining and e-business and then shows how data mining may be applied to e-business to gather consumer/supplier intelligence so that targeted marketing and merchandising may be carried out.", "num_citations": "1\n", "authors": ["1190"]}
{"title": "Web Information Management and Its Application to Electronic Commerce\n", "abstract": " This paper describes various aspects of web information management with particular emphasis on its application to electronic commerce. We first provide a brief overview of the web. Then we discuss concepts for web database management, as database management is a key part of information management. These include data models and architectures, query processing, transaction management, metadata management, storage issues, and integrity and security. Then we discuss various web information management technologies such as multimedia, visualization, data mining and warehousing, and knowledge management. Then we discuss emerging standards such as Java Database Connectivity, Extended Markup Language (XML), and middleware standards such as Object Request Brokers (ORB) and Remote Method Invocation (RMI). Finally we discuss how web data management technologies can be\u00a0\u2026", "num_citations": "1\n", "authors": ["1190"]}
{"title": "Object technology for building adaptable and evolvable autonomous decentralized systems\n", "abstract": " CiNii \u8ad6\u6587 - Object technology for building adaptable and evolvable autonomous decentralized systems CiNii \u56fd\u7acb\u60c5\u5831\u5b66\u7814\u7a76\u6240 \u5b66\u8853\u60c5\u5831\u30ca\u30d3\u30b2\u30fc\u30bf[\u30b5\u30a4\u30cb\u30a3] \u65e5\u672c\u306e\u8ad6\u6587\u3092 \u3055\u304c\u3059 \u5927\u5b66\u56f3\u66f8\u9928\u306e\u672c\u3092\u3055\u304c\u3059 \u65e5\u672c\u306e\u535a\u58eb\u8ad6\u6587\u3092\u3055\u304c\u3059 \u65b0\u898f\u767b\u9332 \u30ed\u30b0\u30a4\u30f3 English \u691c\u7d22 \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u9589\u3058\u308b \u30bf\u30a4\u30c8\u30eb \u8457\u8005\u540d \u8457\u8005ID \u8457\u8005\u6240\u5c5e \u520a\u884c\u7269\u540d ISSN \u5dfb\u53f7\u30da\u30fc\u30b8 \u51fa\u7248\u8005 \u53c2\u8003\u6587\u732e \u51fa\u7248\u5e74 \u5e74\u304b\u3089 \u5e74\u307e\u3067 \u691c\u7d22 \u691c\u7d22 \u691c\u7d22 CiNii\u7a93\u53e3\u696d\u52d9\u306e\u518d\u958b\u306b\u3064\u3044\u3066 Object technology for building adaptable and evolvable autonomous decentralized systems THURAISINGHAM B. \u88ab\u5f15\u7528\u6587\u732e: 1\u4ef6 \u8457\u8005 THURAISINGHAM B. \u53ce\u9332\u520a\u884c\u7269 4th IEEE Symposium Autonomous Decentralized Systems (ISADS99) 4th IEEE Symposium Autonomous Decentralized Systems (ISADS99), 1999 \u88ab\u5f15\u7528\u6587\u732e: 1\u4ef6\u4e2d 1-1\u4ef6\u3092 \u8868\u793a 1 An Autonomous Decentralized Architecture for Distributed Data Management and Dissemination BLAKE Malworsth Brain , LIGUORI Patricia \u2026", "num_citations": "1\n", "authors": ["1190"]}
{"title": "Active real-time database management for command & control applications\n", "abstract": " Active real-time database management for command & control applications | Proceedings of the workshop on Databases: active and real-time ACM Digital Library Logo ACM Logo Google, Inc. (search) Advanced Search Browse About Sign in Register Advanced Search Journals Magazines Proceedings Books SIGs Conferences People More Search ACM Digital Library SearchSearch Advanced Search cikm Conference Proceedings Upcoming Events Authors Affiliations Award Winners More HomeConferencesCIKMProceedingsCIKM '96Active real-time database management for command & control applications ARTICLE Active real-time database management for command & control applications Share on Authors: Bhavani M Thuraisingham profile image Bhavani Thuraisingham MITRE Corp. MITRE Corp. View Profile , Eric R. Hughes profile image Eric Hughes MITRE Corp. MITRE Corp. View Profile , Peter C Krupp \u2026", "num_citations": "1\n", "authors": ["1190"]}
{"title": "Data management research at the MITRE Corporation\n", "abstract": " The MITRE Corporation provides technical assistance, system engineering, and acquisition support to large organizations, especially U.S. Government agencies. We help our customers to plan complex systems based on emerging technologies, and to implement systems based on commercial-off-the-shelf products. In MITRE's research program, instead of emphasizing concerns of DBMS or CASE vendors, our research emphasizes the issues of organizations who need to use such products. For example, we favor areas where we can build over commercial products, rather than changing their internals.Data management at MITRE goes beyond research, to include technology transition, system engineering, product evaluation, prototypes, tutorials, advice on customers' strategic directions, and participation in standards efforts. We use prototyping to illustrate potential improvements in customer systems, to\u00a0\u2026", "num_citations": "1\n", "authors": ["1190"]}
{"title": "ACM Multimedia'94 conference workshop on multimedia database management systems\n", "abstract": " This paper describes the ACM Multimedia '94 Conference Workshop on Multimedia Database Management Systems held on 21 October 1994 in San Francisco, California. The workshop consisted of four sessions: designing multimedia database management systems, video and continuous media service, multimedia storage and retrieval management, and miscellaneous topics in multimedia data management. The workshop concluded with a discussion session on directions for multimedia database management. Twenty-eight participants from U.S.A., U.K., Germany, Norway, and Egypt attended the workshop.", "num_citations": "1\n", "authors": ["1190"]}
{"title": "Consistent data access in a distributed database management system for command and control applications\n", "abstract": " Consistent data access in a distributed database management system for command and control applications | Proceedings of the 1994 simulation multiconference on Grand challenges in computer simulation ACM Digital Library home ACM home Google, Inc. (search) Advanced Search Browse About Sign in Register Advanced Search Journals Magazines Proceedings Books SIGs Conferences People More Search ACM Digital Library SearchSearch Advanced Search Browse Browse Digital Library Collections More HomeBrowse by TitleProceedingsProceedings of the 1994 simulation multiconference on Grand challenges in computer simulationConsistent data access in a distributed database management system for command and control applications Article Consistent data access in a distributed database management system for command and control applications Share on Authors: Dana L Small profile image \u2026", "num_citations": "1\n", "authors": ["1190"]}
{"title": "Security constraint processing in a distributed database environment\n", "abstract": " Publication: CSC'94: Proceedings of the 22nd annual ACM computer science conference on Scaling up: meeting the challenge of complexity in real-world computing applications: meeting the challenge of complexity in real-world computing applications March 1994 Pages 356\u2013363 https://doi. org/10.1145/197530.197651", "num_citations": "1\n", "authors": ["1190"]}
{"title": "Design and implementation of a distributed database\n", "abstract": " We describe an approach for controlling certain unauthorized inferences in a multilevel secure distributed database management system. In such a system, two or more multilevel secure database management systems are connected via a trusted network. Furthermore, the environment that we have considered is a limited heterogeneous one where not all of the nodes handle the same accreditation ranges. In our approach, security constraints, which are rules that assign security levels to the data, are processed during the distributed query, update, and database design operations in such a way that users do not acquire information to which they are not authorized via logical deductions. We describe the design and implementation of the distributed inference controller which functions during the query operation.< >", "num_citations": "1\n", "authors": ["1190"]}
{"title": "Towards developing a standard multilevel relational data model for representing a multilevel universe\n", "abstract": " In a multilevel universe, not all of the information is at the same security level. Furthermore, in such a universe, it is also possible for users at different levels to have different views of the same entity or event. The standard relational data model is not adequate to represent such a universe. In this paper we discuss the issues on developing a standard multilevel relational data model for representing a multilevel universe.", "num_citations": "1\n", "authors": ["1190"]}
{"title": "Secure interoperability of trusted database management systems\n", "abstract": " We describe issues on interconnecting trusted database management systems. In particular, issues on handling heterogeneity and autonomy are discussed.", "num_citations": "1\n", "authors": ["1190"]}
{"title": "Integrating intelligent database technology and trusted database technology\n", "abstract": " This article investigates the issues of applying trusted database technology to intelligent database technology toproduce trusted intelligent database management systems and applying intelligent database technology to trusted database technology to enhance the security of a trusted database management system.The demand for the efficient processing of ever larger quantities of information has made the implementation of novel concepts more urgent. Intelligent database management system technology, which has recently been thrust to the forefront, allows the representation of complex applications such as CAD/CAM, multimedia, arti\ufb01cial intelligence and process control. It also enables database systems to make intelligent deductions.", "num_citations": "1\n", "authors": ["1190"]}
{"title": "Inference and Aggregation\n", "abstract": " The inference problem recognizes that sensitive information must be protected not only from direct retrieval but also from indirect disclosure. Information flow analysis addresses such indirect disclosure within the system, such as signaling, observation of resource utilization, and other covert channels.", "num_citations": "1\n", "authors": ["1190"]}
{"title": "The inference problem in multilevel secure database management systems\n", "abstract": " In this paper we describe our research and development activities on the inference problem. In particular, we \u0428 Escribe our near-term research and implementation activities on handling security constraints during query processing,^ Me\u00e1base updates, and during database design. We also summarize our research activities on (i) developing a conceptual'% \u2116 t> delJor representing and reasoning about multilevel applications,(ii) developing a logic for secure dataiknowledge (foase management systems, and (Hi) designing an expert inference controller.", "num_citations": "1\n", "authors": ["1190"]}
{"title": "AI applications in distributed system design issues\n", "abstract": " The design of distributed systems using object-oriented techniques that support local area networking and distributed processing is proposed. The focus is on how these artificial intelligence techniques can enhance the design of the front-end subsystem of each host in a distributed environment. The functions of the front-end subsystem addressed are user interface management, interconnection in the case of heterogeneous systems, and security. It is contended that AI techniques will circumvent the problems that have beset traditional approaches.< >", "num_citations": "1\n", "authors": ["1190"]}