{"title": "QoS-aware middleware for web services composition\n", "abstract": " The paradigmatic shift from a Web of manual interactions to a Web of programmatic interactions driven by Web services is creating unprecedented opportunities for the formation of online business-to-business (B2B) collaborations. In particular, the creation of value-added services by composition of existing ones is gaining a significant momentum. Since many available Web services provide overlapping or identical functionality, albeit with different quality of service (QoS), a choice needs to be made to determine which services are to participate in a given composite service. This paper presents a middleware platform which addresses the issue of selecting Web services for the purpose of their composition in a way that maximizes user satisfaction expressed as utility functions over QoS attributes, while satisfying the constraints set by the user and by the structure of the composite service. Two selection approaches are\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "3951\n", "authors": ["21"]}
{"title": "Fundamentals of Business Process Management\n", "abstract": " \u252c\u2310 Springer-Verlag Berlin Heidelberg 2013", "num_citations": "2638\n", "authors": ["21"]}
{"title": "Quality driven web services composition\n", "abstract": " The process-driven composition of Web services is emerging as a promising approach to integrate business applications within and across organizational boundaries. In this approach, individual Web services are federated into composite Web services whose business logic is expressed as a process model. The tasks of this process model are essentially invocations to functionalities offered by the underlying component services. Usually, several component services are able to execute a given task, although with different levels of pricing and quality. In this paper, we advocate that the selection of component services should be carried out during the execution of a composite service, rather than at design-time. In addition, this selection should consider multiple criteria (eg, price, duration, reliability), and it should take into account global constraints and preferences set by the user (eg, budget constraints). Accordingly\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1727\n", "authors": ["21"]}
{"title": "Process-Aware Information Systems: Bridging People and Software Through Process Technology\n", "abstract": " A unifying foundation to design and implement process-aware information systems This publication takes on the formidable task of establishing a unifying foundation and set of common underlying principles to effectively model, design, and implement process-aware information systems. Authored by leading authorities and pioneers in the field, Process-Aware Information Systems helps readers gain a thorough understanding of major concepts, languages, and techniques for building process-aware applications, including:* UML and EPCs: two of the most widely used notations for business process modeling* Concrete techniques for process design and analysis* Process execution standards: WfMC and BPEL* Representative commercial tools: ARIS, TIBCO Staffware, and FLOWer Each chapter begins with a description of the problem domain and then progressively unveils relevant concepts and techniques. Examples and illustrations are used extensively to clarify and simplify complex material. Each chapter ends with a set of exercises, ranging from simple questions to thought-provoking assignments. Sample solutions for many of the exercises are available on the companion Web site. Armed with a new and deeper understanding, readers are better positioned to make their own contributions to the field and evaluate various approaches to a particular task or problem. This publication is recommended as a textbook for graduate and advanced undergraduate students in computer science and information systems, as well as for professionals involved in workflow and business process management, groupware and teamwork, enterprise\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1398\n", "authors": ["21"]}
{"title": "Semantics and analysis of business process models in BPMN\n", "abstract": " The Business Process Modelling Notation (BPMN) is a standard for capturing business processes in the early phases of systems development. The mix of constructs found in BPMN makes it possible to create models with semantic errors. Such errors are especially serious, because errors in the early phases of systems development are among the most costly and hardest to correct. The ability to statically check the semantic correctness of models is thus a desirable feature for modelling tools based on BPMN. Accordingly, this paper proposes a mapping from BPMN to a formal language, namely Petri nets, for which efficient analysis techniques are available. The proposed mapping has been implemented as a tool that, in conjunction with existing Petri net-based tools, enables the static analysis of BPMN models. The formalisation also led to the identification of deficiencies in the BPMN standard specification.", "num_citations": "896\n", "authors": ["21"]}
{"title": "Declarative composition and peer-to-peer provisioning of dynamic web services\n", "abstract": " The development of new services through the integration of existing ones has gained a considerable momentum as a means to create and streamline business-to-business collaborations. Unfortunately, as Web services are often autonomous and heterogeneous entities, connecting and coordinating them in order to build integrated services is a delicate and time-consuming task. In this paper, we describe the design and implementation of a system through which existing Web services can be declaratively composed, and the resulting composite services can be executed following a peer-to-peer paradigm, within a dynamic environment. This system provides tools for specifying composite services through. statecharts, data conversion rules, and provider selection, policies. These specifications are then translated into XML documents that can be interpreted by peer-to-peer inter-connected software components, in\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "652\n", "authors": ["21"]}
{"title": "Formal semantics and analysis of control flow in WS-BPEL\n", "abstract": " Web service composition refers to the creation of new (Web) services by combining functionalities provided by existing ones. A number of domain-specific languages for service composition have been proposed, with consensus being formed around a process-oriented language known as WS-BPEL (or BPEL). The kernel of BPEL consists of simple communication primitives that may be combined using control-flow constructs expressing sequence, branching, parallelism, synchronization, etc. We present a comprehensive and rigorously defined mapping of BPEL constructs onto Petri net structures, and use this for the analysis of various dynamic properties related to unreachable activities, conflicting messages, garbage collection, conformance checking, and deadlocks and lifelocks in interaction processes. We use a mapping onto Petri nets because this allows us to use existing theoretical results and analysis tools\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "602\n", "authors": ["21"]}
{"title": "UML activity diagrams as a workflow specification language\n", "abstract": " If UML activity diagrams are to succeed as a standard in the area of organisational process modeling, they need to compare well to alternative languages such as those provided by commercial Workflow Management Systems. This paper examines the expressiveness and the adequacy of activity diagrams for workflow specification, by systematically evaluating their ability to capture a collection of workflow patterns. This analysis provides insights into the relative strengths and weaknesses of activity diagrams. In particular, it is shown that, given an appropriate clarification of their semantics, activity diagrams are able to capture situations arising in practice, which cannot be captured by most commercial Workflow Management Systems. On the other hand, the study shows that activity diagrams fail to capture some useful situations, thereby suggesting directions for improvement.", "num_citations": "582\n", "authors": ["21"]}
{"title": "On the suitability of BPMN for business process modelling\n", "abstract": " In this paper we examine the suitability of the Business Process Modelling Notation (BPMN) for business process modelling, using the Workflow Patterns as an evaluation framework. The Workflow Patterns are a collection of patterns developed for assessing control-flow, data and resource capabilities in the area of Process Aware Information Systems (PAISs). In doing so, we provide a comprehensive evaluation of the capabilities of BPMN, and its strengths and weaknesses when utilised for business process modelling. The analysis provided for BPMN is part of a larger effort aiming at an unbiased and vendor-independent survey of the suitability and the expressive power of some mainstream process modelling languages. It is a sequel to previous work in which languages including BPEL and UML Activity Diagrams were evaluated.", "num_citations": "539\n", "authors": ["21"]}
{"title": "Service interaction patterns\n", "abstract": " With increased sophistication and standardization of modeling languages and execution platforms supporting business process management (BPM) across traditional boundaries, has come the need for consolidated insights into their exploitation from a business perspective. Key technology developments in BPM bear this out, with several web services-related initiatives investing significant effort in the collection of compelling use cases to heighten the exploitation of BPM in multi-party collaborative environments. In this setting, we present a collection of patterns of service interactions which allow emerging web services functionality, especially that pertaining to choreography and orchestration, to be benchmarked against abstracted forms of representative scenarios. Beyond bilateral interactions, these patterns cover multilateral, competing, atomic and causally related interactions. Issues related to the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "520\n", "authors": ["21"]}
{"title": "Graph matching algorithms for business process model similarity search\n", "abstract": " We investigate the problem of ranking all process models in a repository according to their similarity with respect to a given process model. We focus specifically on the application of graph matching algorithms to this similarity search problem. Since the corresponding graph matching problem is NP-complete, we seek to find a compromise between computational complexity and quality of the computed ranking. Using a repository of 100 process models, we evaluate four graph matching algorithms, ranging from a greedy one to a relatively exhaustive one. The results show that the mean average precision obtained by a fast greedy algorithm is close to that obtained with the most exhaustive algorithm.", "num_citations": "488\n", "authors": ["21"]}
{"title": "Analysis of web services composition languages: The case of BPEL4WS\n", "abstract": " Web services composition is an emerging paradigm for application integration within and across organizational boundaries. A landscape of languages and techniques for web services composition has emerged and is continuously being enriched with new proposals from different vendors and coalitions. However, little effort has been dedicated to systematically evaluate the capabilities and limitations of these languages and techniques. The work reported in this paper is a step in this direction. It presents an in-depth analysis of the Business Process Execution Language for Web Services (BPEL4WS) with respect to a framework composed of workflow and communication patterns.", "num_citations": "409\n", "authors": ["21"]}
{"title": "Design and implementation of the YAWL system\n", "abstract": " This paper describes the implementation of a system supporting YAWL (Yet Another Workflow Language). YAWL is based on a rigorous analysis of existing workflow management systems and related standards using a comprehensive set of workflow patterns. This analysis shows that contemporary workflow systems, relevant standards (e.g. XPDL, BPML, BPEL4WS), and theoretical models such as Petri nets have problems supporting essential patterns. This inspired the development of YAWL by taking Petri nets as a starting point and introducing mechanisms that provide direct support for the workflow patterns identified. As a proof of concept we have developed a workflow management system supporting YAWL. In this paper, we present the architecture and functionality of the system and zoom into the control-flow, data, and operational perspectives.", "num_citations": "368\n", "authors": ["21"]}
{"title": "Predictive business process monitoring with LSTM neural networks\n", "abstract": " Predictive business process monitoring methods exploit logs of completed cases of a process in order to make predictions about running cases thereof. Existing methods in this space are tailor-made for specific prediction tasks. Moreover, their relative accuracy is highly sensitive to the dataset at hand, thus requiring users to engage in trial-and-error and tuning when applying them in a specific setting. This paper investigates Long Short-Term Memory (LSTM) neural networks as an approach to build consistently accurate models for a wide range of predictive process monitoring tasks. First, we show that LSTMs outperform existing techniques to predict the next event of a running case and its timestamp. Next, we show how to use models for predicting the next task in order to predict the full continuation of a running case. Finally, we apply the same approach to predict the remaining time, and show that this\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "311\n", "authors": ["21"]}
{"title": "The rise of web service ecosystems\n", "abstract": " The convergence of Internet marketplaces and service-oriented architectures has spurred the growth of Web service ecosystems. This paper articulates a vision for Web service ecosystems, discusses early manifestations of this vision, and presents a unifying architecture to support the emergence of larger and more sophisticated ecosystems", "num_citations": "309\n", "authors": ["21"]}
{"title": "A critical overview of the web services choreography description language\n", "abstract": " There is an increasingly widespread acceptance of Service-Oriented Architectures (SOA) as a paradigm for integrating software applications within and across organizational boundaries. In this paradigm, independently developed and operated applications are exposed as (Web) services which are then interconnected using a stack of standards including SOAP, WSDL, UDDI, WS-Security, etc. While the technology for developing basic services and interconnecting them on a point-to-point basis has attained a certain level of maturity and adoption, there remain open challenges when it comes to managing service interactions that go beyond simple sequences of requests and responses or involve large numbers of participants.Standardization is a key aspect of the Web services paradigm. Web services standardization initiatives such as SOAP and WSDL, as well as the family of WS-* specifications (eg, WS-Policy, WS-Security, WS-Coordination) aim at ensuring interoperability between services developed using competing platforms. Figure 1 provides a quick (though partial) overview of the existing stack of Web services standards.", "num_citations": "297\n", "authors": ["21"]}
{"title": "From BPMN process models to BPEL web services\n", "abstract": " The business process modelling notation (BPMN) is a graph-oriented language in which control and action nodes can be connected almost arbitrarily. It is supported by various modelling tools but so far no systems can directly execute BPMN models. The business process execution language for Web services (BPEL) on the other hand is a mainly block-structured language supported by several execution platforms. In the current setting, mapping BPMN models to BPEL code is a necessary step towards unified and standards-based business process development environments. It turns out that this mapping is challenging from a scientific viewpoint as BPMN and BPEL represent two fundamentally different classes of languages. Existing methods for mapping BPMN to BPEL impose limitations on the structure of the source model. This paper proposes a technique that overcomes these limitations. Beyond its direct\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "293\n", "authors": ["21"]}
{"title": "Service-oriented design: A multi-viewpoint approach\n", "abstract": " As the technology associated with the \"Web Services\" trend gains significant adoption, the need for a corresponding design approach becomes increasingly important. This paper introduces a foundational model for designing (composite) services. The innovation of this model lies in the identification of four interrelated viewpoints (interface behaviour, provider behaviour, choreography, and orchestration) and their formalization from a control-flow perspective in terms of Petri nets. By formally capturing the interrelationships between these viewpoints, the proposal enables the static verification of the consistency of composite services designed in a cooperative and incremental manner. A proof-of-concept simulation and verification tool has been developed to test the possibilities of the proposed model.", "num_citations": "289\n", "authors": ["21"]}
{"title": "Business process model merging: An approach to business process consolidation\n", "abstract": " This article addresses the problem of constructing consolidated business process models out of collections of process models that share common fragments. The article considers the construction of unions of multiple models (called merged models) as well as intersections (called digests). Merged models are intended for analysts who wish to create a model that subsumes a collection of process models -- typically representing variants of the same underlying process -- with the aim of replacing the variants with the merged model. Digests, on the other hand, are intended for analysts who wish to identify the most recurring fragments across a collection of process models, so that they can focus their efforts on optimizing these fragments. The article presents an algorithm for computing merged models and an algorithm for extracting digests from a merged model. The merging and digest extraction algorithms have been\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "269\n", "authors": ["21"]}
{"title": "Self-serv: A platform for rapid composition of web services in a peer-to-peer environment\n", "abstract": " Publisher SummarySELF-SERV distinguishes three types of services: elementary services, composite services, and service communities. An elementary service is an individual Web accessible application (for example, a Java program) that does not explicitly rely on another Web service. A composite service aggregates multiple Web services that are referred to as its components. SELF-SERV relies on a declarative language for composing services based on state charts: a widely used formalism in the area of reactive systems that is emerging as a standard for process modeling following its integration into the Unified Modeling Language (UML). An operation of a composite service can be seen as having input parameters, output parameters, consumed and produced events, and a state chart gluing these elements together. The technology to compose Web services in appropriate time frames has not kept pace with\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "264\n", "authors": ["21"]}
{"title": "Pattern based analysis of BPEL4WS\n", "abstract": " Web services composition is an emerging paradigm for enabling application integration within and across organisational boundaries. A landscape of languages and techniques for web services composition has emerged and is continuously being enriched with new proposals from different vendors and coalitions. However, little or no effort has been dedicated to systematically evaluating the capabilities and limitations of these languages and techniques. The work reported in this paper is a first step in this direction. It presents an in-depth analysis of the Business Process Execution Language for Web Services (BPEL4WS). The framework used for this analysis is based on a collection of workflow and communication patterns.", "num_citations": "250\n", "authors": ["21"]}
{"title": "Let\u0393\u00c7\u00d6s dance: A language for service behavior modeling\n", "abstract": " In Service-Oriented Architectures (SOAs), software systems are decomposed into independent units, namely services, that interact with one another through message exchanges. To promote reuse and evolvability, these interactions are explicitly described right from the early phases of the development lifecycle. Up to now, emphasis has been placed on capturing structural aspects of service interactions. Gradually though, the description of behavioral dependencies between service interactions is gaining increasing attention as a means to push forward the SOA vision. This paper deals with the description of these behavioral dependencies during the analysis and design phases. The paper outlines a set of requirements that a language for modeling service interactions at this level should fulfill, and proposes a language whose design is driven by these requirements.", "num_citations": "237\n", "authors": ["21"]}
{"title": "Predictive monitoring of business processes\n", "abstract": " Modern information systems that support complex business processes generally maintain significant amounts of process execution data, particularly records of events corresponding to the execution of activities (event logs). In this paper, we present an approach to analyze such event logs in order to predictively monitor business constraints during business process execution. At any point during an execution of a process, the user can define business constraints in the form of linear temporal logic rules. When an activity is being executed, the framework identifies input data values that are more (or less) likely to lead to the achievement of each business constraint. Unlike reactive compliance monitoring approaches that detect violations only after they have occurred, our predictive monitoring approach provides early advice so that users can steer ongoing process executions towards the achievement of\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "230\n", "authors": ["21"]}
{"title": "Conformance checking of service behavior\n", "abstract": " A service-oriented system is composed of independent software units, namely services, that interact with one another exclusively through message exchanges. The proper functioning of such system depends on whether or not each individual service behaves as the other services expect it to behave. Since services may be developed and operated independently, it is unrealistic to assume that this is always the case. This article addresses the problem of checking and quantifying how much the actual behavior of a service, as recorded in message logs, conforms to the expected behavior as specified in a process model. We consider the case where the expected behavior is defined using the BPEL industry standard (Business Process Execution Language for Web Services). BPEL process definitions are translated into Petri nets and Petri net-based conformance checking techniques are applied to derive two\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "224\n", "authors": ["21"]}
{"title": "Adapt or perish: Algebra and visual notation for service interface adaptation\n", "abstract": " The proliferation of services on the web is leading to the formation of service ecosystems wherein services interact with one another in ways not necessarily foreseen during their development or deployment. A key challenge in this setting is service mediation: the act of retrofitting existing services by intercepting, storing, transforming, and (re-)routing messages going into and out of these services so they can interact in unforeseen manners. This paper addresses a sub-problem of service mediation, namely service interface adaptation, that arises when the interface that a service provides does not match the interface that it is expected to provide in a given interaction. The paper focuses on reconciling mismatches between behavioural interfaces, i.e. interfaces that capture ordering constraints between interactions. It presents a declarative approach to service interface adaptation based on: (i) an algebra over\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "219\n", "authors": ["21"]}
{"title": "Automated discovery of process models from event logs: Review and benchmark\n", "abstract": " Process mining allows analysts to exploit logs of historical executions of business processes to extract insights regarding the actual performance of these processes. One of the most widely studied process mining operations is automated process discovery. An automated process discovery method takes as input an event log, and produces as output a business process model that captures the control-flow relations between tasks that are observed in or implied by the event log. Various automated process discovery methods have been proposed in the past two decades, striking different tradeoffs between scalability, accuracy, and complexity of the resulting models. However, these methods have been evaluated in an ad-hoc manner, employing different datasets, experimental setups, evaluation measures, and baselines, often leading to incomparable conclusions and sometimes unreproducible results due to the use\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "211\n", "authors": ["21"]}
{"title": "Pattern-based translation of BPMN process models to BPEL web services\n", "abstract": " The business process modeling notation (BPMN) is a graph-oriented language primarily targeted at domain analysts and supported by many modeling tools. The business process execution language for Web services (BPEL) on the other hand is a mainly block-structured language targeted at software developers and supported by several execution platforms. Translating BPMN models into BPEL code is a necessary step towards standards-based business process development environments. This translation is challenging since BPMN and BPEL represent two fundamentally different classes of languages. Existing BPMN-to-BPEL translations rely on the identification of block-structured patterns in BPMN models that are mapped onto structured BPEL constructs. This article advances the state of the art in BPMN-to-BPEL translation by defining methods for identifying not only perfectly block-structured fragments in\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "211\n", "authors": ["21"]}
{"title": "Facilitating the rapid development and scalable orchestration of composite web services\n", "abstract": " The development of new Web services through the composition of existing ones has gained a considerable momentum as a means to realise business-to-business collaborations. Unfortunately, given that services are often developed in an ad hoc fashion using manifold technologies and standards, connecting and coordinating them in order to build composite services is a delicate and time-consuming task. In this paper, we describe the design and implementation of a system in which services are composed using a model-driven approach, and the resulting composite services are orchestrated following a peer-to-peer paradigm. The system provides tools for specifying composite services through statecharts, data conversion rules, and multi-attribute provider selection policies. These specifications are interpreted by software components that interact in a peer-to-peer way to coordinate the execution of the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "211\n", "authors": ["21"]}
{"title": "Translating bpmn to bpel\n", "abstract": " The Business Process Modelling Notation (BPMN) is a graph-oriented language in which control and action nodes can be connected almost arbitrarily. It is supported by various modelling tools but so far no systems can directly execute BPMN models. The Business Process Execution Language for Web Services (BPEL) on the other hand is a mainly block-structured language supported by several execution platforms. In the current setting, mapping BPMN models to BPEL code is a necessary step towards unified and standards-based business process development environments. It turns out that this mapping is challenging from a scientific viewpoint as BPMN and BPEL represent two fundamentally different classes of languages. Existing methods for mapping BPMN to BPEL impose limitations on the structure of the source model, especially with respect to cycles. This report proposes a technique that overcomes these limitations. Beyond its direct relevance in the context of BPMN and BPEL, this technique addresses difficult problems that arise generally when translating between floow-based languages with parallelism.", "num_citations": "206\n", "authors": ["21"]}
{"title": "Business process variability modeling: A survey\n", "abstract": " It is common for organizations to maintain multiple variants of a given business process, such as multiple sales processes for different products or multiple bookkeeping processes for different countries. Conventional business process modeling languages do not explicitly support the representation of such families of process variants. This gap triggered significant research efforts over the past decade, leading to an array of approaches to business process variability modeling. In general, each of these approaches extends a conventional process modeling language with constructs to capture customizable process models. A customizable process model represents a family of process variants in a way that a model of each variant can be derived by adding or deleting fragments according to customization options or according to a domain model. This survey draws up a systematic inventory of approaches to\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "202\n", "authors": ["21"]}
{"title": "Web Service Composition Languages: Old Wine in New Bottles?.\n", "abstract": " Recently, several languages for Web service composition have emerged (e.g., BPEL4WS and WSCI). The goal of these languages is to glue Web services together in a process-oriented way. For this purpose, these languages typically borrow concepts from workflow management systems and embed these concepts in the so-called \"Web services stack\". Up to now, little or no effort has been dedicated to systematically evaluate the capabilities and limitations of these languages. BPEL4WS for example is said to combine the best of other standards for Web service composition such as WSFL (IBM) andXLANG (Microsoft), and allows for a mixture of block structured and graph structured process models. However, aspects such as the expressiveness, adequacy, orthogonality, and formal characterization of BPEL4WS (e.g. reachability) have not yet been systematically investigated. Although BPEL4WS is not a bad\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "191\n", "authors": ["21"]}
{"title": "Questionnaire-based variability modeling for system configuration\n", "abstract": " Variability management is a recurrent issue in systems engineering. It arises for example in enterprise systems, where modules are configured and composed to meet the requirements of individual customers based on modifications to a reference model. It also manifests itself in the context of software product families, where variants of a system are built from a common code base. This paper proposes an approach to capture system variability based on questionnaire models that include order dependencies and domain constraints. The paper presents analysis techniques to detect circular dependencies and contradictory constraints in questionnaire models, as well as techniques to incrementally prevent invalid configurations by restricting the space of allowed answers to a question based on previous answers. The approach has been implemented as a toolset and has been used in practice to capture\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "185\n", "authors": ["21"]}
{"title": "Similarity search of business process models.\n", "abstract": " Similarity search is a general class of problems in which a given object, called a query object, is compared against a collection of objects in order to retrieve those that most closely resemble the query object. This paper reviews recent work on an instance of this class of problems, where the objects in question are business process models. The goal is to identify process models in a repository that most closely resemble a given process model or a fragment thereof.", "num_citations": "179\n", "authors": ["21"]}
{"title": "Deadline-based escalation in process-aware information systems\n", "abstract": " Decision making in process-aware information systems involves build-time and run-time decisions. At build-time, idealized process models are designed based on the organization's objectives, infrastructure, context, constraints, etc. At run-time, this idealized view is often broken. In particular, process models generally assume that planned activities happen within a certain period. When such assumptions are not fulfilled, users must make decisions regarding alternative arrangements to achieve the goal of completing the process within its expected time frame or to minimize tardiness. We refer to the required decisions as escalations. This paper proposes a framework for escalations that draws on established principles from the workflow management field. The paper identifies and classifies a number of escalation mechanisms such as changing the routing of work, changing the work distribution or changing the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "172\n", "authors": ["21"]}
{"title": "Optimized execution of business processes on blockchain\n", "abstract": " Blockchain technology enables the execution of collaborative business processes involving untrusted parties without requiring a central authority. Specifically, a process model comprising tasks performed by multiple parties can be coordinated via smart contracts operating on the blockchain. The consensus mechanism governing the blockchain thereby guarantees that the process model is followed by each party. However, the cost required for blockchain use is highly dependent on the volume of data recorded and the frequency of data updates by smart contracts. This paper proposes an optimized method for executing business processes on top of commodity blockchain technology. Our optimization targets three areas specifically: initialization cost for process instances, task execution cost by means of a space-optimized data structure, and improved runtime components for maximized throughput. The\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "167\n", "authors": ["21"]}
{"title": "Translating Standard Process Models to BPEL\n", "abstract": " Standardisation of languages in the field of business process management has long been an elusive goal. Recently though, consensus has built around one process implementation language, namely BPEL, and two fundamentally similar process modelling notations, namely UML Activity Diagram (UML AD) and BPMN. This paper presents a technique for generating BPEL code from process models expressed in a core subset of BPMN and UML AD. This model-to-code translation is a necessary ingredient to the emergence of model-driven business process development environments based on these standards. The proposed translation has been implemented as an open source tool.", "num_citations": "161\n", "authors": ["21"]}
{"title": "Merging business process models\n", "abstract": " This paper addresses the following problem: given two business process models, create a process model that is the union of the process models given as input. In other words, the behavior of the produced process model should encompass that of the input models. The paper describes an algorithm that produces a single configurable process model from a pair of process models. The algorithm works by extracting the common parts of the input process models, creating a single copy of them, and appending the differences as branches of configurable connectors. This way, the merged process model is kept as small as possible, while still capturing all the behavior of the input models. Moreover, analysts are able to trace back which model(s) a given element in the merged model originates from. The algorithm has been prototyped and tested against process models taken from several application domains.", "num_citations": "151\n", "authors": ["21"]}
{"title": "Standards for web service choreography and orchestration: Status and perspectives\n", "abstract": " Web service composition has been the subject of a number of standardisation initiatives. These initiatives have met various difficulties and had mixed degrees of success, and none of them has yet attained both de facto and de jure status. This paper reviews two of these initiatives with respect to a framework wherein service composition is approached from multiple interrelated perspectives. One conclusion is that standardisation initiatives in this area have not been built on top of an explicitly defined overarching conceptual foundation. The paper outlines a research agenda aimed at identifying requirements and concepts that should be addressed by and incorporated into these standards.", "num_citations": "151\n", "authors": ["21"]}
{"title": "WofBPEL: A tool for automated analysis of BPEL processes\n", "abstract": " The Business Process Execution Language for Web Service, known as BPEL4WS, more recently as WS-BPEL (or BPEL for short) [1], is a process definition language geared towards Service-Oriented Computing (SOC) and layered on top of the Web services technology stack. In BPEL, the logic of the interactions between a given service and its environment is described as a composition of communication actions. These communication actions are interrelated by control-flow dependencies expressed through constructs close to those found in workflow definition languages. In particular, BPEL incorporates two sophisticated branching and synchronisation constructs, namely \u0393\u00c7\u00a3control links\u0393\u00c7\u00a5 and \u0393\u00c7\u00a3join conditions\u0393\u00c7\u00a5, which can be found in a class of workflow models known as synchronising workflows formalised in terms of Petri nets in [3].", "num_citations": "147\n", "authors": ["21"]}
{"title": "Complex symbolic sequence encodings for predictive monitoring of business processes\n", "abstract": " This paper addresses the problem of predicting the outcome of an ongoing case of a business process based on event logs. In this setting, the outcome of a case may refer for example to the achievement of a performance objective or the fulfillment of a compliance rule upon completion of the case. Given a log consisting of traces of completed cases, given a trace of an ongoing case, and given two or more possible outcomes (e.g., a positive and a negative outcome), the paper addresses the problem of determining the most likely outcome for the case in question. Previous approaches to this problem are largely based on simple symbolic sequence classification, meaning that they extract features from traces seen as sequences of event labels, and use these features to construct a classifier for runtime prediction. In doing so, these approaches ignore the data payload associated to each event. This paper\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "143\n", "authors": ["21"]}
{"title": "Pattern-based analysis of BPML (and WSCI)\n", "abstract": " Web services composition is an emerging paradigm for enabling application integration within and across organizational boundaries. A landscape of languages and techniques for web services composition has emerged and is continuously being enriched with new proposals from different vendors and coalitions. However, little or no effort has been dedicated to systematically evaluating the capabilities and limitations of these languages and techniques. The work reported in this paper is a step in this direction. It presents an in-depth analysis of the Business Process modeling Language (BPML). The framework used for this analysis is based on a collection of workflow and communication patterns. In addition to BPML, the framework is also applied to the Web Services Choreography Interface (WSCI). WSCI and BPML have several routing constructs in common but aim at different levels of the web services stack.", "num_citations": "141\n", "authors": ["21"]}
{"title": "Service interaction modeling: Bridging global and local views\n", "abstract": " In a service-oriented architecture (SOA), a system is viewed as a collection of independent units (services) that interact with one another through message exchanges. Established languages such as the Web services description language and the business process execution language allow developers to capture the interactions in which an individual service can engage, both from a structural and from a behavioral perspective. However, in large service-oriented systems, stakeholders may require a global picture of the way services interact with each other, rather than multiple small pictures focusing on individual services. Such \"global models\" are especially useful when a set of services interact in such a way that none of them sees all messages being exchanged, yet interactions taking place between some services affect the way other services interact. An issue that arises when dealing with global models of\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "137\n", "authors": ["21"]}
{"title": "Pattern-based Analysis of BPMN - An extensive evaluation of the Control-flow, the Data and the Resource Perspectives\n", "abstract": " In this paper an evaluation of BPMN is presented, using the Workflow Patterns as an analysis framework. The analysis provided for BPMN is part of a larger effort aiming at an unbiased and vendor-independent survey of the expressive power of some mainstream modelling languages for process-aware information systems. It is a sequel to an analysis series where languages like BPEL and UML 2.0 A.D were evaluated. The results from the survey could both be used for the selection of a modelling technique, as well as for motivation and input to further development of any of the surveyed languages.", "num_citations": "137\n", "authors": ["21"]}
{"title": "Towards a semantic framework for service description\n", "abstract": " The inexpensive and global connectivity provided by the Internet has triggered a wave of interest in providing service-oriented electronic access to commercial activities. This pressure has led, in turn, to a need for accurate service description, so that we may advertise, locate, analyse and compare services. In this paper, we classify services by the context in which they are used. Next, we characterise both conventional and electronic services according to a range of domain independent attributes including price, payment method and availability. We examine possible representations for each of these service dimensions. By integrating these representations into a unified service description language, we hope to provide a means to lubricate the electronic services marketplace.", "num_citations": "137\n", "authors": ["21"]}
{"title": "Aligning business process models\n", "abstract": " This paper studies the following problem: given a pair of business process models, determine which elements in one model are related to which elements in the other model. This problem arises in the context of merging different versions or variants of a business process model or when comparing business process models in order to display their similarities and differences. The paper investigates two approaches to this alignment problem: one based purely on lexical matching of pairs of elements and another based on error-correcting graph matching. Using a set of models taken from real-life scenarios, the paper empirically shows that graph matching techniques yield a significantly higher precision than pure lexical matching, while achieving comparable recall.", "num_citations": "135\n", "authors": ["21"]}
{"title": "Outcome-oriented predictive process monitoring: Review and benchmark\n", "abstract": " Predictive business process monitoring refers to the act of making predictions about the future state of ongoing cases of a business process, based on their incomplete execution traces and logs of historical (completed) traces. Motivated by the increasingly pervasive availability of fine-grained event data about business process executions, the problem of predictive process monitoring has received substantial attention in the past years. In particular, a considerable number of methods have been put forward to address the problem of outcome-oriented predictive process monitoring, which refers to classifying each ongoing case of a process according to a given set of possible categorical outcomes\u0393\u00c7\u00f6e.g., Will the customer complain or not? Will an order be delivered, canceled, or withdrawn? Unfortunately, different authors have used different datasets, experimental settings, evaluation measures, and baselines to\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "121\n", "authors": ["21"]}
{"title": "Clustering-based predictive process monitoring\n", "abstract": " The enactment of business processes is generally supported by information systems that record data about each process execution (a.k.a. case). This data can be analyzed via a family of methods broadly known as process mining. Predictive process monitoring is a process mining technique concerned with predicting how running (uncompleted) cases will unfold up to their completion. In this paper, we propose a predictive process monitoring framework for estimating the probability that a given predicate will be fulfilled upon completion of a running case. The framework takes into account both the sequence of events observed in the current trace, as well as data attributes associated to these events. The prediction problem is approached in two phases. First, prefixes of previous (completed) cases are clustered according to control flow information. Second, a classifier is built for each cluster using event data attributes\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "116\n", "authors": ["21"]}
{"title": "Pattern-based analysis of the control-flow perspective of UML activity diagrams\n", "abstract": " The Unified Modelling Language (UML) is a well-known family of notations for software modelling. Recently, a new version of UML has been released. In this paper we examine the Activity Diagrams notation of this latest version of UML in terms of a collection of patterns developed for assessing control-flow capabilities of languages used in the area of process-aware information systems. The purpose of this analysis is to assess relative strengths and weaknesses of control-flow specification in Activity Diagrams and to identify ways of addressing potential deficiencies. In addition, the pattern-based analysis will yield typical solutions to practical process modelling problems and expose some of the ambiguities in the current UML 2.0 specification [9].", "num_citations": "115\n", "authors": ["21"]}
{"title": "Structuring acyclic process models\n", "abstract": " This article studies the problem of transforming a process model with an arbitrary topology into an equivalent well-structured process model. While this problem has received significant attention, there is still no full characterization of the class of unstructured process models that can be transformed into well-structured ones, nor an automated method for structuring any process model that belongs to this class. This article fills this gap in the context of acyclic process models. The article defines a necessary and sufficient condition for an unstructured acyclic process model to have an equivalent well-structured process model under fully concurrent bisimulation, as well as a complete structuring method. The method has been implemented as a tool that takes process models captured in the BPMN and EPC notations as input. The article also reports on an empirical evaluation of the structuring method using a repository of\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "114\n", "authors": ["21"]}
{"title": "Achieving performance and availability guarantees with spot instances\n", "abstract": " In the Infrastructure-as-a-Service (IaaS) cloud computing market, spot instances refer to virtual servers that are rented via an auction. Spot instances allow IaaS providers to sell spare capacity while enabling IaaS users to acquire virtual servers at a lower price than the regular market price (also called \"on demand\" price). Users bid for spot instances at their chosen limit price. Based on the bids and the available capacity, the IaaS provider sets a clearing price. A bidder acquires their requested spot instances if their bid is above the clearing price. However, these spot instances may be terminated by the IaaS provider impromptu if the auction's clearing price goes above the user's limit price. In this context, this paper addresses the following question: Can spot instances be used to run paid web services while achieving performance and availability guaran-tees? The paper examines the problem faced by a Software-as\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "114\n", "authors": ["21"]}
{"title": "Life after BPEL?\n", "abstract": " The Business Process Execution Language for Web Services (BPEL) has emerged as a standard for specifying and executing processes. It is supported by vendors such as IBM and Microsoft and positioned as the \u0393\u00c7\u00a3process language of the Internet\u0393\u00c7\u00a5. This paper provides a critical analysis of BPEL based on the so-called workflow patterns. It also discusses the need for languages like BPEL. Finally, the paper addresses several challenges not directly addressed by BPEL but highly relevant to the support of web services.", "num_citations": "114\n", "authors": ["21"]}
{"title": "Questionnaire-driven configuration of reference process models\n", "abstract": " Reference models are a widely accepted means to facilitate reusable information system and organizational design. At present, besides domain knowledge, the configuration of a reference model requires a thorough understanding of the notation it is captured in. This hinders the involvement of domain experts without specialized modeling background, in the configuration of reference models. In this paper, we propose a questionnaire-driven approach to reference model configuration which abstracts away from the modeling language. For illustration, we show how this approach can be applied to reference process models captured in the Configurable EPC notation. To demonstrate its applicability, the proposal has been implemented as a toolset that guides users through the configuration process by means of a form-based interface.", "num_citations": "112\n", "authors": ["21"]}
{"title": "Correlation patterns in service-oriented architectures\n", "abstract": " When a service engages in multiple interactions concurrently, it is generally required to correlate incoming messages with messages previously sent or received. Features to deal with this correlation requirement have been incorporated into standards and tools for service implementation, but the supported sets of features are ad hoc as there is a lack of an overarching framework from which their expressiveness can be evaluated. This paper introduces a set of patterns that provide a basis for evaluating languages and protocols for service implementation in terms of their support for correlation. The proposed correlation patterns are grounded in a formal model that views correlation mechanisms as means of grouping atomic message events into conversations and processes. The paper also provides an evaluation of relevant standards in terms of the patterns, specifically WS-Addressing and BPEL, and\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "112\n", "authors": ["21"]}
{"title": "Caterpillar: A Blockchain-Based Business Process Management System.\n", "abstract": " This demonstration introduces Caterpillar, an open-source Business Process Management System (BPMS) that runs on top of the Ethereum blockchain. Like any BPMS, Caterpillar supports the creation of instances of a process model (captured in the Business Process Model and Notation\u0393\u00c7\u00f4BPMN) and allows users to track the state of process instances and to execute tasks thereof. The specificity of Caterpillar is that the state of each process instance is maintained on the Ethereum blockchain, and the workflow routing is performed by smart contracts generated by a BPMN-to-Solidity compiler. The compiler supports a wide array of BPMN constructs, including user, script and service tasks, parallel and exclusive gateways, subprocesses, multi-instance activities and event handlers. The target audience of this demonstration includes researchers in the area of business process management and blockchain technology.", "num_citations": "108\n", "authors": ["21"]}
{"title": "Structuring acyclic process models\n", "abstract": " This paper addresses the problem of transforming a process model with an arbitrary topology into an equivalent well-structured process model. While this problem has received significant attention, there is still no full characterization of the class of unstructured process models that can be transformed into well-structured ones, nor an automated method to structure any process model that belongs to this class. This paper fills this gap in the context of acyclic process models. The paper defines a necessary and sufficient condition for an unstructured process model to have an equivalent structured model under fully concurrent bisimulation, as well as a complete structuring method.", "num_citations": "108\n", "authors": ["21"]}
{"title": "When are two workflows the same?\n", "abstract": " In the area of workflow management, one is confronted with a large number of competing languages and the relations between them (e.g. relative expressiveness) are usually not clear. Moreover, even within the same language it is generally possible to express the same workflow in different ways, a feature known as variability. This paper aims at providing some of the formal groundwork for studying relative expressiveness and variability by defining notions of equivalence capturing different views on how workflow systems operate. Firstly, a notion of observational equivalence in the absence of silent steps is defined and related to classical bisimulation. Secondly, a number of equivalence notions in the presence of silent steps are defined. A distinction is made between the case where silent steps are visible (but not controllable) by the environment and the case where silent steps are not visible, i.e., there is an alternation between system events and environment interactions. It is shown that these notions of equivalence are different and do not coincide with classical notions of bisimulation with silent steps (e.g. weak and branching).", "num_citations": "104\n", "authors": ["21"]}
{"title": "Enabling personalized composition and adaptive provisioning of web services\n", "abstract": " The proliferation of interconnected computing devices is fostering the emergence of environments where Web services made available to mobile users are a commodity. Unfortunately, inherent limitations of mobile devices still hinder the seamless access to Web services, and their use in supporting complex user activities. In this paper, we describe the design and implementation of a distributed, adaptive, and context-aware framework for personalized service composition and provisioning adapted to mobile users. Users specify their preferences by annotating existing process templates, leading to personalized service-based processes. To cater for the possibility of low bandwidth communication channels and frequent disconnections, an execution model is proposed whereby the responsibility of orchestrating personalized processes is spread across the participating services and user agents. In addition\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "104\n", "authors": ["21"]}
{"title": "Fast fully dynamic landmark-based estimation of shortest path distances in very large graphs\n", "abstract": " Computing the shortest path between a pair of vertices in a graph is a fundamental primitive in graph algorithmics. Classical exact methods for this problem do not scale up to contemporary, rapidly evolving social networks with hundreds of millions of users and billions of connections. A number of approximate methods have been proposed, including several landmark-based methods that have been shown to scale up to very large graphs with acceptable accuracy. This paper presents two improvements to existing landmark-based shortest path estimation methods. The first improvement relates to the use of shortest-path trees (SPTs). Together with appropriate short-cutting heuristics, the use of SPTs allows to achieve higher accuracy with acceptable time and memory overhead. Furthermore, SPTs can be maintained incrementally under edge insertions and deletions, which allows for a fully-dynamic algorithm. The\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "103\n", "authors": ["21"]}
{"title": "Introduction to business process management\n", "abstract": " Business Process Management (BPM) is the art and science of overseeing how work is performed in an organization to ensure consistent outcomes and to take advantage of improvement opportunities. In this context, the term \u0393\u00c7\u00a3improvement\u0393\u00c7\u00a5 may take different meanings depending on the objectives of the organization. Typical examples of improvement objectives include reducing costs, reducing execution times and reducing error rates. Improvement initiatives may be one-off, but also display a more continuous nature. Importantly, BPM is not about improving the way individual activities are performed. Rather, it is about managing entire chains of events, activities and decisions that ultimately add value to the organization and its customers. These \u0393\u00c7\u00a3chains of events, activities and decisions\u0393\u00c7\u00a5 are called processes.           In this chapter, we introduce a few essential concepts behind BPM. We will start with a\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "101\n", "authors": ["21"]}
{"title": "BPMN Miner: Automated discovery of BPMN process models with hierarchical structure\n", "abstract": " Existing techniques for automated discovery of process models from event logs generally produce flat process models. Thus, they fail to exploit the notion of subprocess as well as error handling and repetition constructs provided by contemporary process modeling notations, such as the Business Process Model and Notation (BPMN). This paper presents a technique, namely BPMN Miner, for automated discovery of hierarchical BPMN models containing interrupting and non-interrupting boundary events and activity markers. The technique employs approximate functional and inclusion dependency discovery techniques in order to elicit a process\u0393\u00c7\u00f4subprocess hierarchy from the event log. Given this hierarchy and the projected logs associated to each node in the hierarchy, parent process and subprocess models are discovered using existing techniques for flat process model discovery. Finally, the resulting models\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "93\n", "authors": ["21"]}
{"title": "Discovering data-aware declarative process models from event logs\n", "abstract": " A wealth of techniques are available to automatically discover business process models from event logs. However, the bulk of these techniques yield procedural process models that may be useful for detailed analysis, but do not necessarily provide a comprehensible picture of the process. Additionally, barring few exceptions, these techniques do not take into account data attributes associated to events in the log, which can otherwise provide valuable insights into the rules that govern the process. This paper contributes to filling these gaps by proposing a technique to automatically discover declarative process models that incorporate both control-flow dependencies and data conditions. The discovered models are conjunctions of first-order temporal logic expressions with an associated graphical representation (Declare notation). Importantly, the proposed technique discovers underspecified models\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "89\n", "authors": ["21"]}
{"title": "A formal approach to protocols and strategies for (legal) negotiation\n", "abstract": " We propose a formal and executable framework for expressing protocols and strategies for automated (legal) negotiation. In this framework a party involved in a negotiation is represented through a software agent composed of four modules:(i) a communication module which manages the interaction with the other agents;(ii) a control module;(iii) a reasoning module specified as a defeasible theory; and (iv) a knowledge base which bridges the control and the reasoning modules, while keeping track of past decisions and interactions. The choice of defeasible logic is justified against a set of desirable criteria for negotiation automation languages. Moreover, the suitability of the framework is illustrated through two case studies.", "num_citations": "89\n", "authors": ["21"]}
{"title": "CATERPILLAR: A business process execution engine on the Ethereum blockchain\n", "abstract": " Blockchain platforms, such as Ethereum, allow a set of actors to maintain a ledger of transactions without relying on a central authority and to deploy programs, called smart contracts, that are executed whenever certain transactions occur. These features can be used as basic building blocks for executing collaborative business processes between mutually untrusting parties. However, implementing business processes using the low\u0393\u00c7\u00c9level primitives provided by blockchain platforms is cumbersome and error\u0393\u00c7\u00c9prone. In contrast, established business process management systems (BPMSs), such as those based on the standard Business Process Model and Notation (BPMN), provide convenient abstractions for rapid development of process\u0393\u00c7\u00c9oriented applications. This article demonstrates how to combine the advantages of a BPMS with those of a blockchain platform. The article introduces a blockchain\u0393\u00c7\u00c9based BPMN\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "88\n", "authors": ["21"]}
{"title": "Structure and evolution of package dependency networks\n", "abstract": " Software developers often include available open-source software packages into their projects to minimize redundant effort. However, adding a package to a project can also introduce risks, which can propagate through multiple levels of dependencies. Currently, not much is known about the structure of open-source package ecosystems of popular programming languages and the extent to which transitive bug propagation is possible. This paper analyzes the dependency network structure and evolution of the JavaScript, Ruby, and Rust ecosystems. The reported results reveal significant differences across language ecosystems. The results indicate that the number of transitive dependencies for JavaScript has grown 60% over the last year, suggesting that developers should look more carefully into their dependencies to understand what exactly is included. The study also reveals that vulnerability to a removal of\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "87\n", "authors": ["21"]}
{"title": "Split miner: automated discovery of accurate and simple business process models from event logs\n", "abstract": " The problem of automated discovery of process models from event logs has been intensively researched in the past two decades. Despite a rich field of proposals, state-of-the-art automated process discovery methods suffer from two recurrent deficiencies when applied to real-life logs: (i) they produce large and spaghetti-like models; and (ii) they produce models that either poorly fit the event log (low fitness) or over-generalize it (low precision). Striking a trade-off between these quality dimensions in a robust and scalable manner has proved elusive. This paper presents an automated process discovery method, namely Split Miner, which produces simple process models with low branching complexity and consistently high and balanced fitness and precision, while achieving considerably faster execution times than state-of-the-art methods, measured on a benchmark covering twelve real-life event logs. Split\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "86\n", "authors": ["21"]}
{"title": "Business process simulation for operational decision support\n", "abstract": " Contemporary business process simulation environments are geared towards design-time analysis, rather than operational decision support over already deployed and running processes. In particular, simulation experiments in existing process simulation environments start from an empty execution state. We investigate the requirements for a process simulation environment that allows simulation experiments to start from an intermediate execution state. We propose an architecture addressing these requirements and demonstrate it through a case study conducted using the YAWL workflow engine and CPN simulation tools.", "num_citations": "80\n", "authors": ["21"]}
{"title": "Clone detection in repositories of business process models\n", "abstract": " Over time, process model repositories tend to accumulate duplicate fragments (also called clones) as new process models are created or extended by copying and merging fragments from other models. This phenomenon calls for methods to detect clones in process models, so that these clones can be refactored as separate subprocesses in order to improve maintainability. This paper presents an indexing structure to support the fast detection of clones in large process model repositories. The proposed index is based on a novel combination of a method for process model decomposition (specifically the Refined Process Structure Tree), with established graph canonization and string matching techniques. Experiments show that the algorithm scales to repositories with hundreds of models. The experimental results also show that a significant number of non-trivial clones can be found in process model\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "78\n", "authors": ["21"]}
{"title": "A flexible, object-centric approach for business process modelling\n", "abstract": " Mainstream business process modelling techniques often promote a design paradigm wherein the activities that may be performed within a case, together with their usual execution order, form the backbone on top of which other aspects are anchored. This Fordist paradigm, while effective in standardised and production-oriented domains, breaks when confronted with processes in which case-by-case variations and exceptions are the norm. We contend that the effective design of flexible processes calls for a substantially different modelling paradigm. Motivated by requirements from the human services domain, we explore the hypothesis that a framework consisting of a small set of coordination concepts, combined with established object-oriented modelling principles, provides a suitable foundation for designing highly flexible processes. Several human service delivery processes have been designed using\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "77\n", "authors": ["21"]}
{"title": "Process model transformation for event-based coordination of composite applications\n", "abstract": " A process model specified using, for example, UML activity diagrams can be translated into an event-based model that can be executed on top of a coordination middleware. For example, a process model may be encoded as a collection of coordinating objects that interact with each other through a coordination middleware including a shared memory space. This approach is suitable for undertaking post-deployment adaptation of process-oriented composite applications. In particular, new control dependencies can be encoded by dropping new (or enabling existing) coordinating objects into the space and/or disabling existing ones.", "num_citations": "77\n", "authors": ["21"]}
{"title": "Transforming BPMN diagrams into YAWL nets\n", "abstract": " While the Business Process Modeling Notation (BPMN) is the de facto standard for modeling business processes on a conceptual level, YAWL allows the specification of executable workflow models. A transformation between these two languages enables the integration of different levels of abstraction in process modeling. This paper discusses the transformation of BPMN diagrams to YAWL nets and presents a tool that carries out this transformation.", "num_citations": "77\n", "authors": ["21"]}
{"title": "Declarative process modeling in BPMN\n", "abstract": " Traditional business process modeling notations, including the standard Business Process Model and Notation (BPMN), rely on an imperative paradigm wherein the process model captures all allowed activity flows. In other words, every flow that is not specified is implicitly disallowed. In the past decade, several researchers have exposed the limitations of this paradigm in the context of business processes with high variability. As an alternative, declarative process modeling notations have been proposed (e.g., Declare). These notations allow modelers to capture constraints on the allowed activity flows, meaning that all flows are allowed provided that they do not violate the specified constraints. Recently, it has been recognized that the boundary between imperative and declarative process modeling is not crisp. Instead, mixtures of declarative and imperative process modeling styles are sometimes\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "76\n", "authors": ["21"]}
{"title": "Learning accurate LSTM models of business processes\n", "abstract": " Deep learning techniques have recently found applications in the field of predictive business process monitoring. These techniques allow us to predict, among other things, what will be the next events in a case, when will they occur, and which resources will trigger them. They also allow us to generate entire execution traces of a business process, or even entire event logs, which opens up the possibility of using such models for process simulation. This paper addresses the question of how to use deep learning techniques to train accurate models of business process behavior from event logs. The paper proposes an approach to train recurrent neural networks with Long-Short-Term Memory (LSTM) architecture in order to predict sequences of next events, their timestamp, and their associated resource pools. An experimental evaluation on real-life event logs shows that the proposed approach outperforms\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "73\n", "authors": ["21"]}
{"title": "Artifact lifecycle discovery\n", "abstract": " Artifact-centric modeling is an approach for capturing business processes in terms of so-called business artifacts \u0393\u00c7\u00f6 key entities driving a company's operations and whose lifecycles and interactions define an overall business process. This approach has been shown to be especially suitable in the context of processes where one-to-many or many-to-many relations exist between the entities involved in the process. As a contribution towards building up a body of methods to support artifact-centric modeling, this article presents a method for automated discovery of artifact-centric process models starting from logs consisting of flat collections of event records. We decompose the problem in such a way that a wide range of existing (non-artifact-centric) automated process discovery methods can be reused in a flexible manner. The presented methods are implemented as a package for ProM, a generic open-source\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "72\n", "authors": ["21"]}
{"title": "Predictive business process monitoring with structured and unstructured data\n", "abstract": " Predictive business process monitoring is concerned with continuously analyzing the events produced by the execution of a business process in order to predict as early as possible the outcome of each ongoing case thereof. Previous work has approached the problem of predictive process monitoring when the observed events carry structured data payloads consisting of attribute-value pairs. In practice, structured data often comes in conjunction with unstructured (textual) data such as emails or comments. This paper presents a predictive process monitoring framework that combines text mining with sequence classification techniques so as to handle both structured and unstructured event payloads. The framework has been evaluated with respect to accuracy, prediction earliness and efficiency on two real-life datasets.", "num_citations": "71\n", "authors": ["21"]}
{"title": "Survey and cross-benchmark comparison of remaining time prediction methods in business process monitoring\n", "abstract": " Predictive business process monitoring methods exploit historical process execution logs to generate predictions about running instances (called cases) of a business process, such as the prediction of the outcome, next activity, or remaining cycle time of a given process case. These insights could be used to support operational managers in taking remedial actions as business processes unfold, e.g., shifting resources from one case onto another to ensure the latter is completed on time. A number of methods to tackle the remaining cycle time prediction problem have been proposed in the literature. However, due to differences in their experimental setup, choice of datasets, evaluation measures, and baselines, the relative merits of each method remain unclear. This article presents a systematic literature review and taxonomy of methods for remaining time prediction in the context of business processes, as well as a\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "70\n", "authors": ["21"]}
{"title": "Fast and accurate business process drift detection\n", "abstract": " Business processes are prone to continuous and unexpected changes. Process workers may start executing a process differently in order to adjust to changes in workload, season, guidelines or regulations for example. Early detection of business process changes based on their event logs \u0393\u00c7\u00f4 also known as business process drift detection \u0393\u00c7\u00f4 enables analysts to identify and act upon changes that may otherwise affect process performance. Previous methods for business process drift detection are based on an exploration of a potentially large feature space and in some cases they require users to manually identify the specific features that characterize the drift. Depending on the explored feature set, these methods may miss certain types of changes. This paper proposes a fully automated and statistically grounded method for detecting process drift. The core idea is to perform statistical tests over the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "70\n", "authors": ["21"]}
{"title": "Semantics of standard process models with OR-joins\n", "abstract": " The Business Process Modeling Notation (BPMN) is an emerging standard for capturing business processes. Like its predecessors, BPMN lacks a formal semantics and many of its features are subject to interpretation. One construct of BPMN that has an ambiguous semantics is the OR-join. Several formal semantics of this construct have been proposed for similar languages such as EPCs and YAWL. However, these existing semantics are computationally expensive. This paper formulates a semantics of the OR-join in BPMN for which enablement of an OR-join in a process model can be evaluated in quadratic time in terms of the total number of elements in the model. This complexity can be reduced down to linear-time after materializing a quadratic-sized data structure at design-time. The paper also shows how to efficiently detect the enablement of an OR-join incrementally as the execution of a process\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "70\n", "authors": ["21"]}
{"title": "Handling transactional properties in web service composition\n", "abstract": " The development of new services by composition of existing ones has gained considerable momentum as a means of integrating heterogeneous applications and realising business collaborations. Services that enter into compositions with other services may have transactional properties, especially those in the broad area of resource management (e.g. booking services). These transactional properties may be exploited in order to derive composite services which themselves exhibit certain transactional properties. This paper presents a model for composing services that expose transactional properties and more specifically, services that support tentative holds and/or atomic execution. The proposed model is based on a high-level service composition operator that produces composite services that satisfy specified atomicity constraints. The model supports the possibility of selecting the services that enter\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "70\n", "authors": ["21"]}
{"title": "Choreography conformance checking: An approach based on bpel and petri nets\n", "abstract": " Recently, languages such as BPEL and CDL have been proposed to describe the way services can interact from a behavioral perspective. The emergence of these languages heralds an era where richer service descriptions, going beyond WSDL-like interfaces, will be available. However, what can these richer service descriptions serve for? This paper investigates a possible usage of behavioral service descriptions, namely as input for conformance checking. Conformance checking is the act of verifying whether one or more parties stick to the agreed-upon behavior by observing the actual behavior, e.g., the exchange of messages between all parties. This paper shows that it is possible to translate BPEL business protocols to Petri nets and to relate SOAP messages to transitions in the Petri net. As a result, Petri net-based conformance checking techniques can be used to quantify fitness (whether the observed behavior is possible in the business protocol) and appropriateness (whether the observed behavior \"overfits\" or \"underfits\" the business protocol). Moreover, non-conformance can be visualized to pinpoint deviations. The approach has been implemented in the context of the ProM framework.", "num_citations": "69\n", "authors": ["21"]}
{"title": "A formal approach to negotiating agents development\n", "abstract": " This paper presents a formal and executable approach to capture the behaviour of parties involved in a negotiation. A party is modeled as a negotiating agent composed of a communication module, a control module, a reasoning module, and a knowledge base. The control module is expressed as a statechart, and the reasoning module as a defeasible logic program. A strategy specification therefore consists of a statechart, a set of defeasible rules, and a set of initial facts. Such a specification can be dynamically plugged into an agent shell incorporating a statechart interpreter and a defeasible logic inference engine, in order to yield an agent capable of participating in a given type of negotiations. The choice of statecharts and defeasible logic with respect to other formalisms is justified against a set of desirable criteria, and their suitability is illustrated through concrete examples of bidding and multi-lateral\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "69\n", "authors": ["21"]}
{"title": "Complete and interpretable conformance checking of business processes\n", "abstract": " This article presents a method for checking the conformance between an event log capturing the actual execution of a business process, and a model capturing its expected or normative execution. Given a process model and an event log, the method returns a set of statements in natural language describing the behavior allowed by the model but not observed in the log and vice versa. The method relies on a unified representation of process models and event logs based on a well-known model of concurrency, namely event structures. Specifically, the problem of conformance checking is approached by converting the event log into an event structure, converting the process model into another event structure, and aligning the two event structures via an error-correcting synchronized product. Each difference detected in the synchronized product is then verbalized as a natural language statement. An empirical\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "68\n", "authors": ["21"]}
{"title": "Evolution and change in data management\u0393\u00c7\u00f6issues and directions\n", "abstract": " One of the fundamental aspects of information and database systems is that they change. Moreover, in so doing they evolve, although the manner and quality of this evolution is highly dependent on the mechanisms in place to handle it. While changes in data are handled well, changes in other aspects, such as structure, rules, constraints, the model, etc., are handled to varying levels of sophistication and completeness. In order to study this in more detail a workshop on Evolution and Change in Data Management was held in Paris in November 1999. It brought together researchers from a wide range of disciplines with a common interest in handling the fundamental characteristics and the conceptual modelling of change in information and database systems. This short report of the workshop concentrates on some of the general lessons that emerged during the four days.", "num_citations": "68\n", "authors": ["21"]}
{"title": "Approximate clone detection in repositories of business process models\n", "abstract": " Evidence exists that repositories of business process models used in industrial practice contain significant amounts of duplication. This duplication may stem from the fact that the repository describes variants of the same processes and/or because of copy/pasting activity throughout the lifetime of the repository. Previous work has put forward techniques for identifying duplicate fragments (clones) that can be refactored into shared subprocesses. However, these techniques are limited to finding exact clones. This paper analyzes the problem of approximate clone detection and puts forward two techniques for detecting clusters of approximate clones. Experiments show that the proposed techniques are able to accurately retrieve clusters of approximate clones that originate from copy/pasting followed by independent modifications to the copied fragments.", "num_citations": "67\n", "authors": ["21"]}
{"title": "Log delta analysis: Interpretable differencing of business process event logs\n", "abstract": " This paper addresses the problem of explaining behavioral differences between two business process event logs. The paper presents a method that, given two event logs, returns a set of statements in natural language capturing behavior that is present or frequent in one log, while absent or infrequent in the other. This log delta analysis method allows users to diagnose differences between normal and deviant executions of a process or between two versions or variants of a process. The method relies on a novel approach to losslessly encode an event log as an event structure, combined with a frequency-enhanced technique for differencing pairs of event structures. A validation of the proposed method shows that it accurately diagnoses typical change patterns and can explain differences between normal and deviant cases in a real-life log, more compactly and precisely than previously proposed methods.", "num_citations": "64\n", "authors": ["21"]}
{"title": "Split miner: Discovering accurate and simple business process models from event logs\n", "abstract": " The problem of automated discovery of process models from event logs has been intensively researched in the past two decades. Despite a rich field of proposals, state-of-the-art automated process discovery methods suffer from two recurrent deficiencies when applied to real-life logs: (i) they produce large and spaghetti-like models; and (ii) they produce models that either poorly fit the event log (low fitness) or highly generalize it (low precision). Striking a tradeoff between these quality dimensions in a robust and scalable manner has proved elusive. This paper presents an automated process discovery method that produces simple process models with low branching complexity and consistently high and balanced fitness, precision and generalization, while achieving execution times 2-6 times faster than state-of-the-art methods on a set of 12 real-life logs. Further, our approach guarantees deadlock-freedom for\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "63\n", "authors": ["21"]}
{"title": "Fast detection of exact clones in business process model repositories\n", "abstract": " As organizations reach higher levels of business process management maturity, they often find themselves maintaining very large process model repositories, representing valuable knowledge about their operations. A common practice within these repositories is to create new process models, or extend existing ones, by copying and merging fragments from other models. We contend that if these duplicate fragments, a.k.a. exact clones, can be identified and factored out as shared subprocesses, the repository's maintainability can be greatly improved. With this purpose in mind, we propose an indexing structure to support fast detection of clones in process model repositories. Moreover, we show how this index can be used to efficiently query a process model repository for fragments. This index, called RPSDAG, is based on a novel combination of a method for process model decomposition (namely the Refined\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "63\n", "authors": ["21"]}
{"title": "Aggregate quality of service computation for composite services\n", "abstract": " This paper addresses the problem of computing the aggregate QoS of a composite service given the QoS of the services participating in the composition. Previous solutions to this problem are restricted to composite services with well-structured orchestration models. Yet, in existing languages such as WS-BPEL and BPMN, orchestration models may be unstructured. This paper lifts this limitation by providing equations to compute the aggregate QoS for general types of irreducible unstructured regions in orchestration models. In conjunction with existing algorithms for decomposing business process models into single-entry-single-exit regions, these functions allow us to cover a larger set of orchestration models than existing QoS aggregation techniques.", "num_citations": "63\n", "authors": ["21"]}
{"title": "Execution semantics for service choreographies\n", "abstract": " A service choreography is a model of interactions in which a set of services engage to achieve a goal, seen from the perspective of an ideal observer that records all messages exchanged between these services. Choreographies have been put forward as a starting point for building service-oriented systems since they provide a global picture of the system\u0393\u00c7\u00d6s behavior. In previous work we presented a language for service choreography modeling targeting the early phases of the development lifecycle. This paper provides an execution semantics for this language in terms of a mapping to \u2567\u00c7-calculus. This formal semantics provides a basis for analyzing choreographies. The paper reports on experiences using the semantics to detect unreachable interactions.", "num_citations": "62\n", "authors": ["21"]}
{"title": "Genetic algorithms for hyperparameter optimization in predictive business process monitoring\n", "abstract": " Predictive business process monitoring aims at predicting the outcome of ongoing cases of a business process based on past execution traces. A wide range of techniques for this predictive task have been proposed in the literature. It turns out that no single technique, under a default configuration, consistently achieves the best predictive accuracy across all datasets. Thus, the selection and configuration of a technique needs to be done for each dataset. This paper presents a framework for predictive process monitoring that brings together a range of techniques, each with an associated set of hyperparameters. The framework incorporates two automatic hyperparameter optimization algorithms, which, given a dataset, select suitable techniques for each step in the framework and configure these techniques with minimal user input. The proposed framework and hyperparameter optimization algorithms have been\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "61\n", "authors": ["21"]}
{"title": "Mining business process deviance: a quest for accuracy\n", "abstract": " This paper evaluates the suitability of sequence classification techniques for analyzing deviant business process executions based on event logs. Deviant process executions are those that deviate in a negative or positive way with respect to normative or desirable outcomes, such as executions that undershoot or exceed performance targets. We evaluate a range of features and classification methods based on their ability to accurately discriminate between normal and deviant executions. We also analyze the ability of the discovered rules to explain potential causes of observed deviances. The evaluation shows that feature types extracted using pattern mining techniques only slightly outperform those based on individual activity frequency. It also suggest that more complex feature types ought to be explored to achieve higher levels of accuracy.", "num_citations": "60\n", "authors": ["21"]}
{"title": "Definition and execution of composite web services: The self-serv project\n", "abstract": " Web services composition is emerging as a promising technology for the effective automation of businessto-business collaborations. It allows organizations to form alliances by connecting their applications, databases, and systems, in order to offer \u0393\u00c7\u00a3one-stops shops\u0393\u00c7\u00a5 for their customers. The SELF-SERV project aims at providing tool support and middleware infrastructure for the definition and execution of composite Web services. A major outcome of the project has been a prototype system in which Web services are declaratively composed, and the resulting composite services can be orchestrated either in a peerto-peer or in a centralized way within a dynamic environment. Work is underway to extend this system in order to enable user-driven composition of Web services, and their execution in a mobile environment.", "num_citations": "57\n", "authors": ["21"]}
{"title": "On the notion of coupling in communication middleware\n", "abstract": " It is well accepted that different types of distributed architectures require different levels of coupling. For example, in client-server and three-tier architectures the application components are generally tightly coupled between them and with the underlying communication middleware. Meanwhile, in off-line transaction processing, grid computing and mobile application architectures, the degree of coupling between application components and with the underlying middleware needs to be minimised along different dimensions. In the literature, terms such as synchronous, asynchronous, blocking, non-blocking, directed, and non-directed are generally used to refer to the degree of coupling required by a given architecture or provided by a given middleware. However, these terms are used with various connotations by different authors and middleware vendors. And while several informal definitions of these terms\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "56\n", "authors": ["21"]}
{"title": "Pattern-based analysis of UML activity diagrams\n", "abstract": " The Unified Modelling Language (UML) is a well-known family of notations for software modelling. Recently, a new version of UML has been released. In this paper we examine the Activity Diagrams notation of this latest version of UML in terms of a collection of patterns developed for assessing control flow and data flow capabilities of languages used in the area of process-aware information systems. The purpose of this analysis is to assess relative strengths and weaknesses of control and data flow specification in Activity Diagrams and to identify ways of addressing potential deficiencies. In addition, the pattern-based analysis will yield typical solutions to practical process modelling problems and expose some of the ambiguities in the current UML 2.0 draft specification [9].", "num_citations": "56\n", "authors": ["21"]}
{"title": "Using dynamic and contextual features to predict issue lifetime in GitHub projects\n", "abstract": " Methods for predicting issue lifetime can help software project managers to prioritize issues and allocate resources accordingly. Previous studies on issue lifetime prediction have focused on models built from static features, meaning features calculated at one snapshot of the issue's lifetime based on data associated to the issue itself. However, during its lifetime, an issue typically receives comments from various stakeholders, which may carry valuable insights into its perceived priority and difficulty and may thus be exploited to update lifetime predictions. Moreover, the lifetime of an issue depends not only on characteristics of the issue itself, but also on the state of the project as a whole. Hence, issue lifetime prediction may benefit from taking into account features capturing the issue's context (contextual features). In this work, we analyze issues from more than 4000 GitHub projects and build models to predict, at\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "54\n", "authors": ["21"]}
{"title": "Modelling families of business process variants: A decomposition driven method\n", "abstract": " Business processes usually do not exist as singular entities that can be managed in isolation, but rather as families of business process variants. When modelling such families of variants, analysts are confronted with the choice between modelling each variant separately, or modelling multiple or all variants in a single model. Modelling each variant separately leads to a proliferation of models that share common parts, resulting in redundancies and inconsistencies. Meanwhile, modelling all variants together leads to less but more complex models, thus hindering on comprehensibility. This paper introduces a method for modelling families of process variants that addresses this trade-off. The key tenet of the method is to alternate between steps of decomposition (breaking down processes into sub-processes) and deciding which parts should be modelled together and which ones should be modelled separately. We\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "53\n", "authors": ["21"]}
{"title": "Slice, mine and dice: Complexity-aware automated discovery of business process models\n", "abstract": " Automated process discovery techniques aim at extracting models from information system logs in order to shed light into the business processes supported by these systems. Existing techniques in this space are effective when applied to relatively small or regular logs, but otherwise generate large and spaghetti-like models. In previous work, trace clustering has been applied in an attempt to reduce the size and complexity of automatically discovered process models. The idea is to split the log into clusters and to discover one model per cluster. The result is a collection of process models \u0393\u00c7\u00f4 each one representing a variant of the business process \u0393\u00c7\u00f4 as opposed to an all-encompassing model. Still, models produced in this way may exhibit unacceptably high complexity. In this setting, this paper presents a two-way divide-and-conquer process discovery technique, wherein the discovered process models are\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "53\n", "authors": ["21"]}
{"title": "The move to Web service ecosystems\n", "abstract": " This position paper envisions the emergence of a new generation of service-oriented software, namely Web Service Ecosystems, and how these systems will support large-scale collaborative business processes. The paper reviews early manifestations of these systems and identifies challenges that will need to be addressed on the road to this vision.", "num_citations": "53\n", "authors": ["21"]}
{"title": "Peer-to-peer traced execution of composite services\n", "abstract": " The connectivity generated by the Internet is opening unprecedented opportunities of automating business-to-business collaborations. As a result, organisations of all sizes are forming online alliances in order to deliver integrated value-added services. Unfortunately, due to a lack of tools and methodologies offering an adequate level of abstraction, the development of these integrated services is currently ad hoc and requires a considerable effort of low-level programming, especially when dealing with coordination, communication, and execution tracing issues. In this paper, we present a framework through which business services can be declaratively composed, and the resulting composite services can be executed in a fully traceable manner. The traces of a composite service executions are collected incrementally through peer-to-peer interactions between the involved providers. Once collected, these\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "52\n", "authors": ["21"]}
{"title": "Modelling flexible processes with business objects\n", "abstract": " Mainstream business process modelling techniques promote a design paradigm wherein the activities that may be performed within a case, together with their usual execution order, form the backbone on top of which other aspects are anchored. This Fordist paradigm, while effective in standardised and production-oriented domains, breaks when confronted with processes in which case-by-case variations and exceptions are the norm. We contend that the effective design of flexible processes calls for a substantially different modelling paradigm: one where processes are organized as interacting business objects rather than as chains of activities. This paper presents a meta-model for business process modelling based on business objects. The paper also presents a real-life case study in which a number of human service delivery processes were designed using the presented meta-model. The case study\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "51\n", "authors": ["21"]}
{"title": "Detecting sudden and gradual drifts in business processes from execution traces\n", "abstract": " Business processes are prone to unexpected changes, as process workers may suddenly or gradually start executing a process differently in order to adjust to changes in workload, season, or other external factors. Early detection of business process changes enables managers to identify and act upon changes that may otherwise affect process performance. Business process drift detection refers to a family of methods to detect changes in a business process by analyzing event logs extracted from the systems that support the execution of the process. Existing methods for business process drift detection are based on an explorative analysis of a potentially large feature space and in some cases they require users to manually identify specific features that characterize the drift. Depending on the explored feature space, these methods miss various types of changes. Moreover, they are either designed to detect\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "49\n", "authors": ["21"]}
{"title": "Blockchain and business process improvement\n", "abstract": " We\u0393\u00c7\u00d6re in the year of blockchain. Blockchain technology has gained widespread traction and is attracting investments like no other emerging technology [1]. A growing number of use cases are being discussed and tested across a range of industries, including finance, insurance, healthcare, logistics and supply chain management. Gartner recently placed blockchain technology just shy of the \u0393\u00c7\u00a3peak of inflated expectations\u0393\u00c7\u00a5 in their hype cycle for emerging technologies for 2016 [2] with another 5 to 10 years before we see mainstream adoption. But beyond the hype, should BPM practitioners care about it?Blockchain is perhaps best known as the technology underpinning bitcoin, but its potential applications go well beyond enabling digital currencies. Blockchain enables a potentially evolving and open set of parties to maintain a safe, permanent, and tamper-proof digital ledger of transactions, without a central authority. The key to the technology is that transactions are not recorded centrally. Instead, each party maintains a copy of the ledger. A majority of parties need to approve (verify) a new transaction before it can be recorded in the ledger\u0393\u00c7\u00f4according to a notion of majority that varies depending on the specific technology. Once a transaction is approved, it is practically impossible to change it or remove it. Hence, blockchain technology can be seen as a replicated append-only transactional data store, and hence it can be used as a substitute for centralized registers maintained by single trusted authorities.", "num_citations": "48\n", "authors": ["21"]}
{"title": "Complex symbolic sequence clustering and multiple classifiers for predictive process monitoring\n", "abstract": " This paper addresses the following predictive business process monitoring problem: Given the execution trace of an ongoing case, and given a set of traces of historical (completed) cases, predict the most likely outcome of the ongoing case. In this context, a trace refers to a sequence of events with corresponding payloads, where a payload consists of a set of attribute-value pairs. Meanwhile, an outcome refers to a label associated to completed cases, like, for example, a label indicating that a given case completed \u0393\u00c7\u00a3on time\u0393\u00c7\u00a5 (with respect to a given desired duration) or \u0393\u00c7\u00a3late\u0393\u00c7\u00a5, or a label indicating that a given case led to a customer complaint or not. The paper tackles this problem via a two-phased approach. In the first phase, prefixes of historical cases are encoded using complex symbolic sequences and clustered. In the second phase, a classifier is built for each of the clusters. To predict the outcome of an\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "48\n", "authors": ["21"]}
{"title": "Beyond tasks and gateways: Discovering BPMN models with subprocesses, boundary events and activity markers\n", "abstract": " Existing techniques for automated discovery of process models from event logs generally produce flat process models. Thus, they fail to exploit the notion of subprocess, as well as error handling and repetition constructs provided by contemporary process modeling notations, such as the Business Process Model and Notation (BPMN). This paper presents a technique for automated discovery of BPMN models containing subprocesses, interrupting and non-interrupting boundary events and activity markers. The technique analyzes dependencies between data attributes attached to events in order to identify subprocesses and to extract their associated logs. Parent process and subprocess models are then discovered using existing techniques for flat process model discovery. Finally, the resulting models and logs are heuristically analyzed in order to identify boundary events and markers. A validation with one\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "47\n", "authors": ["21"]}
{"title": "Cost-effective semantic annotation of XML schemas and web service interfaces\n", "abstract": " Research in the field of semantic Web services aims at automating the discovery, selection, composition and management of Web services based on semantic descriptions. However, the applicability of many solutions developed in this field is hampered by the costs associated with semantically annotating large repositories of Web services. To overcome this gap we propose a practical method for semantically annotating collections of XML Schemas and Web service interfaces. We have evaluated this method on a large repository of governmental Web services. The evaluation shows that relatively simple techniques are surprisingly cost-effective, saving hundreds of man-hours of semantic annotation effort. Moreover, the proposed method does not assume the availability of a preexisting ontology or controlled vocabulary. Instead, the space of annotations is dynamically built during the annotation process.", "num_citations": "47\n", "authors": ["21"]}
{"title": "Transforming object-oriented models to process-oriented models\n", "abstract": " Object-oriented modelling is an established approach to document the information systems. In an object model, a system is captured in terms of object types and associations, state machines, collaboration diagrams, etc. Process modeling on the other hand, provides a different approach whereby behaviour is captured in terms of activities, flow dependencies, resources, etc. These two approaches have their relative advantages. In object models, behaviour is split across object types, whereas in process models, behaviour is captured along chains of logically related tasks. Also, object models and process models lend themselves to different styles of implementation. There is an opportunity to leverage the relative advantages of object models and process models by creating integrated meta-models and transformations so that modellers can switch between these views. In this paper we define a transformation\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "47\n", "authors": ["21"]}
{"title": "Evaluating choreographies in BPMN 2.0 using an extended quality framework\n", "abstract": " The notion of choreography has emerged over the past years as a foundational concept for capturing and managing collaborative business processes. This concept has been adopted as a first-class citizen in the latest version of the Business Process Modeling Notation (BPMN\u252c\u00e12.0). However, it remains an open question whether or not BPMN\u252c\u00e12.0 is actually appropriate for capturing choreographies. In this paper, we shed light into this question by extending an existing language evaluation framework in order to cover the specificities of choreographies, and applying the extended evaluation framework to BPMN\u252c\u00e12.0. Among others, the evaluation identifies a number of issues in BPMN\u252c\u00e12.0 that affect the perceptual discriminability of certain choreography modelling constructs. These deficiencies could potentially affect the comprehensibility of models and lead to confusion, particularly among novice users\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "46\n", "authors": ["21"]}
{"title": "Linking domain models and process models for reference model configuration\n", "abstract": " Reference process models capture common practices in a given domain and variations thereof. Such models are intended to be configured in a specific setting, leading to individualized process models. Although the advantages of reference process models are widely accepted, their configuration still requires a high degree of modeling expertise. Thus users not only need to be domain experts, but also need to master the notation in which the reference process model is captured. In this paper we propose a framework for reference process modeling wherein the domain variability is represented separately from the actual process model. Domain variability is captured as a questionnaire that reflects the decisions that need to be made during configuration and their interrelationships. This questionnaire allows subject matter experts to configure the process model without requiring them to understand the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "46\n", "authors": ["21"]}
{"title": "Automated discovery of structured process models: Discover structured vs. discover and structure\n", "abstract": " This paper addresses the problem of discovering business process models from event logs. Existing approaches to this problem strike various tradeoffs between accuracy and understandability of the discovered models. With respect to the second criterion, empirical studies have shown that block-structured process models are generally more understandable and less error-prone than unstructured ones. Accordingly, several automated process discovery methods generate block-structured models by construction. These approaches however intertwine the concern of producing accurate models with that of ensuring their structuredness, sometimes sacrificing the former to ensure the latter. In this paper we propose an alternative approach that separates these two concerns. Instead of directly discovering a structured process model, we first apply a well-known heuristic that discovers more accurate but\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "44\n", "authors": ["21"]}
{"title": "Framework for monitoring and testing web application scalability on the cloud\n", "abstract": " By allowing resources to be acquired on-demand and in variable amounts, cloud computing provides an appealing environment for deploying pilot projects and for performance testing of Web applications and services. However, setting up cloud environments for performance testing still requires a significant amount of manual effort. To aid performance engineers in this task, we developed a framework that integrates several common benchmarking and monitoring tools. The framework helps performance engineers to test applications under various configurations and loads. Furthermore, the framework supports dynamic server allocation based on incoming load using a response-time-aware heuristics. We validated the framework by deploying and stress-testing the MediaWiki application. An experimental evaluation was conducted aimed at comparing the response-time-aware heuristics against Amazon Auto-Scale.", "num_citations": "44\n", "authors": ["21"]}
{"title": "On the convergence of data and process engineering\n", "abstract": " It is common practice in contemporary information systems engineering to combine data engineering methods with process engineering methods. However, these methods are applied rather independently and at different layers of an information system. This situation engenders an impedance mismatch between the process layer and the business logic and data layers in contemporary information systems. We expose some of the issues that this impedance mismatch raises by means of a concrete example. We then discuss emerging paradigms for seamlessly integrating data and process engineering.", "num_citations": "44\n", "authors": ["21"]}
{"title": "Probabilistic automated bidding in multiple auctions\n", "abstract": " This paper presents an approach to develop bidding agents that participate in multiple auctions with the goal of obtaining an item with a given probability. The approach consists of a prediction method and a planning algorithm. The prediction method exploits the history of past auctions to compute probability functions capturing the belief that a bid of a given price may win a given auction. The planning algorithm computes a price and a set of compatible auctions, such that by sequentially bidding this price in each of the auctions, the agent can obtain the item with the desired probability. Experiments show that the approach increases the payoff of their users and the welfare of the market.", "num_citations": "44\n", "authors": ["21"]}
{"title": "Predictive business process monitoring framework with hyperparameter optimization\n", "abstract": " Predictive business process monitoring exploits event logs to predict how ongoing (uncompleted) traces will unfold up\u252c\u00e1to their completion. A predictive process monitoring framework collects a range of techniques that allow users to get accurate predictions about the achievement of a goal for a given ongoing trace. These techniques can be combined and their parameters configured in different framework instances. Unfortunately, a unique framework instance that is general enough to outperform others for every dataset, goal or type of prediction is elusive. Thus, the selection and configuration of a framework instance needs to be done for a given dataset. This paper presents a predictive process monitoring framework armed with a hyperparameter optimization method to select a suitable framework instance for a given dataset.", "num_citations": "43\n", "authors": ["21"]}
{"title": "Behavioral comparison of process models based on canonically reduced event structures\n", "abstract": " We address the problem of diagnosing behavioral differences between pairs of business process models. Specifically, given two process models, we seek to determine if they are behaviorally equivalent, and if not, we seek to describe their differences in terms of behavioral relations captured in one model but not in the other. The proposed solution is based on a translation from process models to Asymmetric Event Structures (AES). A na\u251c\u00bbve version of this translation suffers from two limitations. First, it produces redundant difference diagnostic statements because an AES may contain unnecessary event duplication. Second, it is not applicable to process models with cycles. To tackle the first limitation, we propose a technique to reduce event duplication in an AES while preserving canonicity. For the second limitation, we propose a notion of unfolding that captures all possible causes of each event in a cycle\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "42\n", "authors": ["21"]}
{"title": "Modeling Business Process Variability for Design-Time Configuration\n", "abstract": " A reference process model represents multiple variants of a common business process in an integrated and reusable manner. It is intended to be individualized in order to fit the requirements of a specific organization or project. This practice of individualizing reference process models provides an attractive alternative with respect to designing process models from scratch; in particular, it enables the reuse of proven practices. This chapter introduces techniques for representing variability in the context of reference process models, as well as techniques that facilitate the individualization of reference process models with respect to a given set of requirements.", "num_citations": "42\n", "authors": ["21"]}
{"title": "The service adaptation machine\n", "abstract": " The reuse of software services often requires the introduction of adapters. In the case of coarse-grained services, and especially services that engage in long-running conversations, these adapters must deal not only with mismatches at the level of individual interactions, but also across interdependent interactions. Existing techniques support the synthesis of adapters at design-time by comparing pairs of service interfaces. However, these techniques only work under certain restrictions. This paper explores a runtime approach to service interface adaptation. The paper proposes an adaptation machine that sits between pairs of services and manipulates the exchanged messages according to a repository of mapping rules. The paper formulates an operational semantics for the adaptation machine, including algorithms to compute rule firing sequences and criteria for detecting deadlocks and information loss. The\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "42\n", "authors": ["21"]}
{"title": "Discovering branching conditions from business process execution logs\n", "abstract": " Process mining is a family of techniques to discover business process models and other knowledge of business processes from event logs. Existing process mining techniques are geared towards discovering models that capture the order of execution of tasks, but not the conditions under which tasks are executed \u0393\u00c7\u00f4 also called branching conditions. One existing process mining technique, namely ProM\u0393\u00c7\u00d6s Decision Miner, applies decision tree learning techniques to discover branching conditions composed of atoms of the form \u0393\u00c7\u00a3v op c\u0393\u00c7\u00a5 where \u0393\u00c7\u00a3v\u0393\u00c7\u00a5 is a variable, \u0393\u00c7\u00a3op\u0393\u00c7\u00a5 is a comparison predicate and \u0393\u00c7\u00a3c\u0393\u00c7\u00a5 is a constant. This paper puts forward a more general technique to discover branching conditions where the atoms are linear equations or inequalities involving multiple variables and arithmetic operators. The proposed technique combine invariant discovery techniques embodied in the Daikon system with decision\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "39\n", "authors": ["21"]}
{"title": "Event-based coordination of process-oriented composite applications\n", "abstract": " A process model specified using, for example, UML activity diagrams can be translated into an event-based model that can be executed on top of a coordination middleware. For example, a process model may be encoded as a collection of coordinating objects that interact with each other through a coordination middleware including a shared memory space. This approach is suitable for undertaking post-deployment adaptation of process-oriented composite applications. In particular, new control dependencies can be encoded by dropping new (or enabling existing) coordinating objects into the space and/or disabling existing ones.", "num_citations": "39\n", "authors": ["21"]}
{"title": "Reserved or on-demand instances? A revenue maximization model for cloud providers\n", "abstract": " We examine the problem of managing a server farm in a way that attempts to maximize the net revenue earned by a cloud provider by renting servers to customers according to a typical Platform-as-a-Service model. The Cloud provider offers its resources to two classes of customers: 'premium' and 'basic'. Premium customers pay upfront fees to reserve servers for a specified period of time (e.g. a year). Premium customers can submit jobs for their reserved servers at any time and pay a fee for the server-hours they use. The provider is liable to pay a penalty every time a 'premium' job can not be executed due to lack of resources. On the other hand, 'basic' customers are served on a best-effort basis, and pay a server-hour fee that may be higher than the one paid by premium customers. The provider incurs energy costs when running servers. Hence, it has an incentive to turn off idle servers. The question of how to\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "38\n", "authors": ["21"]}
{"title": "Unraveling unstructured process models\n", "abstract": " A BPMN model is well-structured if splits and joins are always paired into single-entry-single-exit blocks. Well-structuredness is often a desirable property as it promotes readability and makes models easier to analyze. However, many process models found in practice are not well-structured, and it is not always feasible or even desirable to restrict process modelers to produce only well-structured models. Also, not all processes can be captured as well-structured process models. An alternative to forcing modelers to produce well-structured models, is to automatically transform unstructured models into well-structured ones when needed and possible. This talk reviews existing results on automatic transformation of unstructured process models into structured ones.", "num_citations": "38\n", "authors": ["21"]}
{"title": "Scalable conformance checking of business processes\n", "abstract": " Given a process model representing the expected behavior of a business process and an event log recording its actual execution, the problem of business process conformance checking is that of detecting and describing the differences between the process model and the log. A desirable feature is to produce a minimal yet complete set of behavioral differences. Existing conformance checking techniques that achieve these properties do not scale up\u252c\u00e1to real-life process models and logs. This paper presents an approach that addresses this shortcoming by exploiting automata-based techniques. A log is converted into a deterministic automaton in a lossless manner, the input process model is converted into another minimal automaton, and a minimal error-correcting synchronized product of both automata is calculated using an A* heuristic. The resulting automaton is used to extract alignments between\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "37\n", "authors": ["21"]}
{"title": "Generalized aggregate Quality of Service computation for composite services\n", "abstract": " This article addresses the problem of estimating the Quality of Service (QoS) of a composite service given the QoS of the services participating in the composition. Previous solutions to this problem impose restrictions on the topology of the orchestration models, limiting their applicability to well-structured orchestration models for example. This article lifts these restrictions by proposing a method for aggregate QoS computation that deals with more general types of unstructured orchestration models. The applicability and scalability of the proposed method are validated using a collection of models from industrial practice.", "num_citations": "37\n", "authors": ["21"]}
{"title": "Pattern Based Analysis of Eai Languages-The Case of the Business Modeling Language.\n", "abstract": " Enterprise Application Integration (EAI) is a challenging area that is attracting growing attention from the software industry and the research community. A landscape of languages and techniques for EAI has emerged and is continuously being enriched with new proposals from different software vendors and coalitions. However, little or no effort has been dedicated to systematically evaluate and compare these languages and techniques. The work reported in this paper is a first step in this direction. It presents an in-depth analysis of a language, namely the Business Modeling Language, specifically developed for EAI. The framework used for this analysis is based on a number of workflow and communication patterns. This framework provides a basis for evaluating the advantages and drawbacks of EAI languages with respect to recurrent problems and situations.", "num_citations": "37\n", "authors": ["21"]}
{"title": "A probabilistic approach to automated bidding in alternative auctions\n", "abstract": " This paper presents an approach to develop bidding agents that participate in multiple alternative auctions, with the goal of obtaining an item at the lowest price. The approach consists of a prediction method and a planning algorithm. The prediction method exploits the history of past auctions in order to build probability functions capturing the belief that a bid of a given price may win a given auction. The planning algorithm computes the lowest price, such that by sequentially bidding in a subset of the relevant auctions, the agent can obtain the item at that price with an acceptable probability. The approach addresses the case where the auctions are for substitutable items with different values. Experimental results are reported, showing that the approach increases the payoff of their users and the welfare of the market.", "num_citations": "37\n", "authors": ["21"]}
{"title": "Detecting approximate clones in business process model repositories\n", "abstract": " Empirical evidence shows that repositories of business process models used in industrial practice contain significant amounts of duplication. This duplication arises for example when the repository covers multiple variants of the same processes or due to copy-pasting. Previous work has addressed the problem of efficiently retrieving exact clones that can be refactored into shared subprocess models. This paper studies the broader problem of approximate clone detection in process models. The paper proposes techniques for detecting clusters of approximate clones based on two well-known clustering algorithms: DBSCAN and Hierarchical Agglomerative Clustering (HAC). The paper also defines a measure of standardizability of an approximate clone cluster, meaning the potential benefit of replacing the approximate clones with a single standardized subprocess. Experiments show that both techniques, in\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "36\n", "authors": ["21"]}
{"title": "Bridging global and local models of service-oriented systems\n", "abstract": " A service-oriented system is a collection of independent services that interact with one another through message exchanges. Languages such as the Web Services Description Language (WSDL) and the Business Process Execution Language (BPEL) allow developers to capture the interactions in which an individual service can engage, both from a structural and from a behavioral perspective. However, in large service-oriented systems, stakeholders may require a global picture of the way services interact with each other, rather than multiple small pictures focusing on individual services. Such global models are especially useful when a set of services interact in such a way that none of them sees all messages being exchanged, yet interactions between some services may affect the way other services interact. Unfortunately, global models of service interactions may sometimes capture behavioral constraints that\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "36\n", "authors": ["21"]}
{"title": "Process mining reloaded: Event structures as a unified representation of process models and event logs\n", "abstract": " Process mining is a family of methods to analyze event logs produced during the execution of business processes in order to extract insights regarding their performance and conformance with respect to normative or expected behavior. The landscape of process mining methods and use cases has expanded considerably in the past decade. However, the field has evolved in a rather ad hoc manner without a unifying foundational theory that would allow algorithms and theoretical results developed for one process mining problem to be reused when addressing other related problems. In this paper we advocate a foundational approach to process mining based on a well-known model of concurrency, namely event structures. We outline how event structures can serve as a unified representation of behavior captured in process models and behavior captured in event logs. We then sketch how process mining\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "35\n", "authors": ["21"]}
{"title": "Toward web-scale workflows for film production\n", "abstract": " The screen business encompasses all creative and management aspects related to film, television, and new media content, from concept to production and distribution. Companies in this industry face increasing competition due to market globalization. To stay competitive, they're turning to contemporary technology-enabled business improvement methods, such as business process management. Despite its potential benefits, the use of workflow systems for automating film production processes is largely unexplored. The authors' case study highlights some of the key challenges that lie ahead for Web-scale workflows for film production.", "num_citations": "35\n", "authors": ["21"]}
{"title": "Multi-staged and multi-viewpoint service choreography modelling\n", "abstract": " Recent approaches to service-oriented systems engineering start by capturing the interactions between services from the perspective of a global observer, leading to so-called service choreographies. The rationale is that such choreographies allow stakeholders to agree on the overall structure and behaviour of the system prior to developing new services or adapting existing ones. However, existing languages for choreography modelling, such as WS-CDL, are implementation-focused. Also, these proposals treat choreographies as monolithic models, with no support for multiple viewpoints. This paper proposes a multi-staged and multi-viewpoint approach to choreography modelling. For the initial stages, the approach promotes the partitioning of choreography models and the design of role-based views; while for subsequent stages, milestone and scenario models are used as an entry point into detailed interaction models. The paper presents analysis techniques to manage the consistency between viewpoints. The proposal is illustrated using a sales and logistics model.", "num_citations": "34\n", "authors": ["21"]}
{"title": "Automated discovery of structured process models from event logs: the discover-and-structure approach\n", "abstract": " This article tackles the problem of discovering a process model from an event log recording the execution of tasks in a business process. Previous approaches to this reverse-engineering problem strike different tradeoffs between the accuracy of the discovered models and their structural complexity. With respect to the latter property, empirical studies have demonstrated that block-structured process models are generally more understandable and less error-prone than unstructured ones. Accordingly, several methods for automated process model discovery generate block-structured models only. These methods however intertwine the objective of producing accurate models with that of ensuring their structuredness, and often sacrifice the former in favour of the latter. In this paper we propose an alternative approach that separates these concerns. Instead of directly discovering a structured process model, we first\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "33\n", "authors": ["21"]}
{"title": "Controlled automated discovery of collections of business process models\n", "abstract": " Automated process discovery techniques aim at extracting process models from information system logs. Existing techniques in this space are effective when applied to relatively small or regular logs, but generate spaghetti-like and sometimes inaccurate models when confronted to logs with high variability. In previous work, trace clustering has been applied in an attempt to reduce the size and complexity of automatically discovered process models. The idea is to split the log into clusters and to discover one model per cluster. This leads to a collection of process models \u0393\u00c7\u00f4 each one representing a variant of the business process \u0393\u00c7\u00f4 as opposed to an all-encompassing model. Still, models produced in this way may exhibit unacceptably high complexity and low fitness. In this setting, this paper presents a two-way divide-and-conquer process discovery technique, wherein the discovered process models are split on the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "33\n", "authors": ["21"]}
{"title": "Generating business process models from object behavior models\n", "abstract": " Object-oriented modeling is an established approach to document information systems. In an object model, a system is captured in terms of object types and associations, state machines and collaboration diagrams, among others. Process modeling on the other hand, provides a different approach whereby behavior is captured in terms of activities, flow dependencies, resources, etc. These two approaches have their relative advantages. Also, object models and process models lend themselves to different styles of implementation. In this paper we define a transformation from a meta-model for object behavior modeling to a meta-model for process modeling. The transformation relies on the identification of causal relations in the object model. These relations are encoded in a heuristics net from which a process model is derived and then simplified. Using this transformation, it becomes possible to apply\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "33\n", "authors": ["21"]}
{"title": "Robotic process mining: vision and challenges\n", "abstract": " Robotic process automation (RPA) is an emerging technology that allows organizations automating repetitive clerical tasks by executing scripts that encode sequences of fine-grained interactions with Web and desktop applications. Examples of clerical tasks include opening a file, selecting a field in a Web form or a cell in a spreadsheet, and copy-pasting data across fields or cells. Given that RPA can automate a wide range of routines, this raises the question of which routines should be automated in the first place. This paper presents a vision towards a family of techniques, termed robotic process mining (RPM), aimed at filling this gap. The core idea of RPM is that repetitive routines amenable for automation can be discovered from logs of interactions between workers and Web and desktop applications, also known as user interactions (UI) logs. The paper defines a set of basic concepts underpinning RPM and presents a pipeline of processing steps that would allow an RPM tool to generate RPA scripts from UI logs. The paper also discusses research challenges to realize the envisioned pipeline.", "num_citations": "32\n", "authors": ["21"]}
{"title": "Diagnosing behavioral differences between business process models: An approach based on event structures\n", "abstract": " Companies operating in multiple markets or segments often need to manage multiple variants of the same business process. Such multiplicity may stem for example from distinct products, different types of customers or regulatory differences across countries in which the companies operate. During the management of these processes, analysts need to compare models of multiple process variants in order to identify opportunities for standardization or to understand performance differences across variants. To support this comparison, this paper proposes a technique for diagnosing behavioral differences between process models. Given two process models, it determines if they are behaviorally equivalent, and if not, it describes their differences in terms of behavioral relations \u0393\u00c7\u00f4 like causal dependencies or conflicts \u0393\u00c7\u00f4 that hold in one model but not in the other. The technique is based on a translation from process\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "32\n", "authors": ["21"]}
{"title": "Multi-perspective process model discovery for robotic process automation\n", "abstract": " Robotic Process Automation (RPA) is a novel approach for immediate cost reduction and gaining operational efficiency. RPA tools can automate repeatable tasks, thus reducing the error rates and increasing overall process performance. Even more, RPA improves the quality of the data (data completeness, data consistency/correctness, etc.). Although, being widely used in many organizations, RPA suffers from high time consumption allocated to the training of software robots (bots for short). Moreover, the models used for training are often inaccurate, which leads to increase of time spent on testing the bots. One of the possible solutions is to apply process mining in order to extract the information about the processes from UI logs such as clickstreams and keylogs, which can then be used to train the bots. However, traditional process discovery techniques are not suitable for the purpose of RPA, as they discover only control-flow perspective of the process and cannot deal well with the UI logs, producing huge and complex models. The proposed research project aims at shifting process mining techniques from working on event logs to working on UI logs as well as developing multi-perspective automated discovery technique, which can then be applied to train the RPA bots.", "num_citations": "31\n", "authors": ["21"]}
{"title": "Interactive and incremental business process model repair\n", "abstract": " It is common for the observed behavior of a business process to differ from the behavior captured in its corresponding model, as workers devise workarounds to handle special circumstances, which over time become part of the norm. Process model repair methods help modelers to realign their models with the observed behavior as recorded in an event log. Given a process model and an event log, these methods produce a new process model that more closely matches the log, while resembling the original model as close as possible. Existing repair methods identify points in the process where the log deviates from the model, and fix these deviations by adding behavior to the model locally. In their quest for automation, these methods often add too much behavior to the model, resulting in models that over-generalize the behavior in the log. This paper advocates for an interactive and incremental approach\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "30\n", "authors": ["21"]}
{"title": "Temporal stability in predictive process monitoring\n", "abstract": " Predictive process monitoring is concerned with the analysis of events produced during the execution of a business process in order to predict as early as possible the final outcome of an ongoing case. Traditionally, predictive process monitoring methods are optimized with respect to accuracy. However, in environments where users make decisions and take actions in response to the predictions they receive, it is equally important to optimize the stability of the successive predictions made for each case. To this end, this paper defines a notion of temporal stability for binary classification tasks in predictive process monitoring and evaluates existing methods with respect to both temporal stability and accuracy. We find that methods based on XGBoost and LSTM neural networks exhibit the highest temporal stability. We then show that temporal stability can be enhanced by hyperparameter-optimizing random\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "29\n", "authors": ["21"]}
{"title": "Synthesis of orchestrators from service choreographies\n", "abstract": " With service interaction modelling, it is customary to distinguish between two types of models: choreographies and orchestrations. A choreography describes interactions within a collection of services from a global perspective, where no service plays a privileged role. Instead, services interact in a peer-to-peer manner. In contrast, an orchestration describes the interactions between one particular service, the orchestrator, and a number of partner services. The main proposition of this work is an approach to bridge these two modelling viewpoints by synthesising orchestrators from choreographies. To start with, choreographies are defined using a simple behaviour description language based on communicating finite state machines. From such a model, orchestrators are initially synthesised in the form of state machines. It turns out that state machines are not suitable for orchestration modelling, because orchestrators generally need to engage in concurrent interactions. To address this issue, a technique is proposed to transform state machines into process models in the Business Process Modelling Notation (BPMN). Orchestrations represented in BPMN can then be augmented with additional business logic to achieve value-adding mediation. In addition, techniques exist for refining BPMN models into executable process definitions. The transformation from state machines to BPMN relies on Petri nets as an intermediary representation and leverages techniques from theory of regions to identify concurrency in the initial Petri net. Once concurrency has been identified, the resulting Petri net is transformed into a BPMN model. The original contributions\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "29\n", "authors": ["21"]}
{"title": "Business process management\n", "abstract": " The use of general descriptive names, registered names, trademarks, service marks, etc. in this publication does not imply, even in the absence of a specific statement, that such names are exempt from the relevant protective laws and regulations and therefore free for general use. The publisher, the authors and the editors are safe to assume that the advice and information in this book are believed to be true and accurate at the date of publication. Neither the publisher nor the authors or the editors give a warranty, express or implied, with respect to the material contained herein or for any errors or omissions that may have been made. The publisher remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.", "num_citations": "29\n", "authors": ["21"]}
{"title": "Automated discovery of business process simulation models from event logs\n", "abstract": " Business process simulation is a versatile technique to estimate the performance of a process under multiple scenarios. This, in turn, allows analysts to compare alternative options to improve a business process. A common roadblock for business process simulation is that constructing accurate simulation models is cumbersome and error-prone. Modern information systems store detailed execution logs of the business processes they support. Previous work has shown that these logs can be used to discover simulation models. However, existing methods for log-based discovery of simulation models do not seek to optimize the accuracy of the resulting models. Instead they leave it to the user to manually tune the simulation model to achieve the desired level of accuracy. This article presents an accuracy-optimized method to discover business process simulation models from execution logs. The method decomposes\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "28\n", "authors": ["21"]}
{"title": "Towards model transformation between SecureUML and UMLsec for role-based access control\n", "abstract": " Nowadays security has become an important aspect in information systems engineering. A mainstream method for information system security is Role-based Access Control (RBAC), which restricts system access to authorised users. Recently different authors have proposed a number of modelling languages (eg, abuse cases, misuse cases, secure i*, secure Tropos, and KAOS extensions to security) that facilitate the documentation and analysis of security aspects. However it is unclear if these languages support the full spectrum of RBAC specification needs. In this paper we selected two security modelling languages, namely SecureUML and UMLsec. Based on the literature study and on the running example we systematically investigate how these languages could be used for RBAC. Our observations indicate that, although both approaches originate from the de-facto industry standard UML, they are not\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "28\n", "authors": ["21"]}
{"title": "Heuristics for composite web service decentralization\n", "abstract": " A composite service is usually specified by means of a process model that captures control-flow and data-flow relations between activities that are bound to underlying component services. In mainstream service orchestration platforms, this process model is executed by a centralized orchestrator through which all interactions are channeled. This architecture is not optimal in terms of communication overhead and has the usual problems of a single point of failure. In previous work, we proposed a method for executing composite services in a decentralized manner. However, this and similar methods for decentralized composite service execution do not optimize the communication overhead between the services participating in the composition. This paper studies the problem of optimizing the selection of services assigned to activities in a decentralized composite service, both in terms of communication\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "27\n", "authors": ["21"]}
{"title": "Browserbite: Accurate cross-browser testing via machine learning over image features\n", "abstract": " Cross-browser compatibility testing is a time consuming and monotonous task. In its most manual form, Web testers open Web pages one-by-one on multiple browser-platform combinations and visually compare the resulting page renderings. Automated cross-browser testing tools speed up this process by extracting screenshots and applying image processing techniques so as to highlight potential incompatibilities. However, these systems suffer from insufficient accuracy, primarily due to a large percentage of false positives. Improving accuracy in this context is challenging as the criteria for classifying a difference as an incompatibility are to some extent subjective. We present our experience building a cross-browser testing tool (Browser bite) based on image segmentation and differencing in conjunction with machine learning. An experimental evaluation involving a dataset of 140 pages, each rendered in 14\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "27\n", "authors": ["21"]}
{"title": "Configurable process models: how to adopt standard practices in your own way?\n", "abstract": " Configurable process models enable a systematic documentation and reuse of standardized \"best\" practices, while allowing process analysts to understand possible variations contemplated by these standards, and to link these variations to business decisions. This article discusses the potential benefits of configurable process models and introduces a method and a toolset for process design based on configurable process models.", "num_citations": "27\n", "authors": ["21"]}
{"title": "Handling temporal grouping and pattern-matching queries in a temporal object model\n", "abstract": " This paper presents a language for expressing temporal pattern-matching queries, and a set of temporal grouping Operators for structuring histories following calendar-based criteria. Pattern-matching queries are shown to be useful for reasoning about successive events in time while temporal grouping may be either used to aggregate data along the time dimension or to display histories. The combination of these capabilities allows to express complex queries involv-ing succession in time and calendar-based conditions simultaneously. These operators are embedded into the TEMPOS temporal data model and their use is illustrated through ex-amples taken from a geographical application. The proposal has been validated by a prototype on top of the O, DBMS.", "num_citations": "27\n", "authors": ["21"]}
{"title": "Dynamic role binding in blockchain-based collaborative business processes\n", "abstract": " Blockchain technology enables the execution of collaborative business processes involving mutually untrusted parties. Existing tools allow such processes to be modeled using high-level notations and compiled into smart contracts that can be deployed on blockchain platforms. However, these tools brush aside the question of who is allowed to execute which tasks in the process, either by deferring the question altogether or by adopting a static approach where all actors are bound to roles upon process instantiation. Yet, a key advantage of blockchains is their ability to support dynamic sets of actors. This paper presents a model for dynamic binding of actors to roles in collaborative processes and an associated binding policy specification language. The proposed language is endowed with a Petri net semantics, thus enabling policy consistency verification. The paper also outlines an approach to compile\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "26\n", "authors": ["21"]}
{"title": "Discovering causal factors explaining business process performance variation\n", "abstract": " Business process performance may be affected by a range of factors, such as the volume and characteristics of ongoing cases or the performance and availability of individual resources. Event logs collected by modern information systems provide a wealth of data about the execution of business processes. However, extracting root causes for performance issues from these event logs is a major challenge. Processes may change continuously due to internal and external factors. Moreover, there may be many resources and case attributes influencing performance. This paper introduces a novel approach based on time series analysis to detect cause-effect relations between a range of business process characteristics and process performance indicators. The scalability and practical relevance of the approach has been validated by a case study involving a real-life insurance claims handling process.", "num_citations": "26\n", "authors": ["21"]}
{"title": "Towards a model for cloud computing cost estimation with reserved instances\n", "abstract": " Cloud computing has been touted as a lower-cost alternative to in-house IT infrastructure recently. However, case studies and anecdotal evidence suggest that it is not always cheaper to use cloud computing resources in lieu of in-house infrastructure. Also, several factors influence the cost of sourcing computing resources from the cloud. For example, cloud computing providers offer virtual machine instances of different types. Each type of virtual machine strikes a different tradeoff between memory capacity, processing power and cost. Also, some providers offer discounts over the hourly price of renting a virtual machine instance if the instance is reserved in advance and an upfront reservation fee is paid. The choice of virtual machine types and the reservation schedule have a direct impact on the running costs. Therefore, IT decision-makers need tools that allow them to determine the potential cost of sourcing their computing resources from the cloud and the optimal sourcing strategy. This paper presents an initial model for estimating the optimal cost of replacing in-house servers with cloud computing resources. The model takes as input the load curve, RAM, storage and network usage observed in the in-house servers over a representative season. Based on this input, the model produces an estimate of the amount of virtual machine instances required across the planning time, in order to replace the in-house infrastructure. As an initial validation of the model, we have applied it to assess the cost of replacing an HPC cluster with virtual machine instances sourced from Amazon EC2.", "num_citations": "26\n", "authors": ["21"]}
{"title": "Event-based coordination of process-oriented composite applications\n", "abstract": " A process-oriented composite application aggregates functionality from a number of other applications and coordinates these applications according to a process model. Traditional approaches to develop process-oriented composite application rely on statically defined process models that are deployed into a process management engine. This approach has the advantage that application designers and users can comprehend the dependencies between the applications involved in the composition by referring to the process model. A major disadvantage however is that once deployed the behaviour of every execution of the composite application is expected to abide by its process model until this model is changed and re-deployed. This makes it difficult to enrich the application with even minor features, to plug-in new applications into the composition, or to hot-fix the composite application to meet special\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "26\n", "authors": ["21"]}
{"title": "Business process performance mining with staged process flows\n", "abstract": " Existing business process performance mining tools offer various summary views of the performance of a process over a given period of time, allowing analysts to identify bottlenecks and their performance effects. However, these tools are not designed to help analysts understand how bottlenecks form and dissolve over time nor how the formation and dissolution of bottlenecks \u0393\u00c7\u00f4 and associated fluctuations in demand and capacity \u0393\u00c7\u00f4 affect the overall process performance. This paper presents an approach to analyze the evolution of process performance via a notion of Staged Process Flow (SPF). An SPF abstracts a business process as a series of queues corresponding to stages. The paper defines a number of stage characteristics and visualizations that collectively allow process performance evolution to be analyzed from multiple perspectives. It demonstrates the advantages of the SPF approach over\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "25\n", "authors": ["21"]}
{"title": "Code churn estimation using organisational and code metrics: An experimental comparison\n", "abstract": " ContextSource code revision control systems contain vast amounts of data that can be exploited for various purposes. For example, the data can be used as a base for estimating future code maintenance effort in order to plan software maintenance activities. Previous work has extensively studied the use of metrics extracted from object-oriented source code to estimate future coding effort. In comparison, the use of other types of metrics for this purpose has received significantly less attention.ObjectiveThis paper applies machine learning techniques to unveil predictors of yearly cumulative code churn of software projects on the basis of metrics extracted from revision control systems.MethodThe study is based on a collection of object-oriented code metrics, XML code metrics, and organisational metrics. Several models are constructed with different subsets of these metrics. The predictive power of these models is\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "25\n", "authors": ["21"]}
{"title": "Visual Exploration of Temporal Object Databases.\n", "abstract": " Two complementary families of users\u0393\u00c7\u00d6 tasks may be identified during database visualization: data browsing and data analysis. On the one hand, data browsing involves extensively exploring a subset of the database using navigational interaction techniques. Classical object database browsers provide means for navigating within a collection of objects and amongst objects by way of their relationships. In temporal object databases, these techniques are not sufficient to adequately support time-related tasks, such as studying a snapshot of a collection of objects at a given instant, or detecting changes within temporal attributes and relationships. Visual data analysis on the other hand, is dedicated to the extraction of valuable knowledge by exploiting the human visual perception capabilities. In temporal databases, examples of data analysis tasks include observing the layout of a history, detecting regularities and trends, and comparing the evolution of the values taken by two or more histories. In this paper, we identify several users\u0393\u00c7\u00d6 tasks related to temporal database exploration, and we propose three novel visualization techniques addressing them. The first of them is dedicated to temporal object browsing, while the two others are oriented towards the analysis of quantitative histories. All three techniques are shown to satisfy several ergonomic properties.", "num_citations": "25\n", "authors": ["21"]}
{"title": "Semantic DMN: Formalizing decision models with domain knowledge\n", "abstract": " The Decision Model and Notation (DMN) is a recent OMG standard for the elicitation and representation of decision models. DMN builds on the notion of decision table, which consists of columns representing the inputs and outputs of a decision, and rows denoting rules. DMN models work under the assumption of complete information, and do not support integration with background domain knowledge. In this paper, we overcome these issues, by proposing decision knowledge bases (DKBs), where decisions are modeled in DMN, and domain knowledge is captured by means of first-order logic with datatypes. We provide a logic-based semantics for such an integration, and formalize how the different DMN reasoning tasks introduced in the literature can be lifted to DKBs. We then consider the case where background knowledge is expressed as an  description logic ontology equipped with datatypes\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "24\n", "authors": ["21"]}
{"title": "On the expressive power of behavioral profiles\n", "abstract": " Behavioral profiles have been proposed as a behavioral abstraction of dynamic systems, specifically in the context of business process modeling. A behavioral profile can be seen as a complete graph over a set of task labels, where each edge is annotated with one relation from a given set of binary behavioral relations. Since their introduction, behavioral profiles were argued to provide a convenient way for comparing pairs of process models with respect to their behavior or computing behavioral similarity between process models. Still, as of today, there is little understanding of the expressive power of behavioral profiles. Via counter-examples, several authors have shown that behavioral profiles over various sets of behavioral relations cannot distinguish certain systems up to trace equivalence, even for restricted classes of systems represented as safe workflow nets. This paper studies the expressive\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "24\n", "authors": ["21"]}
{"title": "Discovering automatable routines from user interaction logs\n", "abstract": " The complexity and rigidity of legacy applications in large organizations engender situations where workers need to perform repetitive routines to transfer data from one application to another via their user interfaces, e.g. moving data from a spreadsheet to a Web application or vice-versa. Discovering and automating such routines can help to eliminate tedious work, reduce cycle times, and improve data quality. Advances in Robotic Process Automation (RPA) technology make it possible to automate such routines, but not to discover them in the first place. This paper presents a method to analyse user interactions in order to discover routines that are fully deterministic and thus amenable to automation. The proposed method identifies sequences of actions that are always triggered when a given activation condition holds and such that the parameters of each action can be deterministically derived from data\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "23\n", "authors": ["21"]}
{"title": "Systems and methods for adapting service interface behaviors\n", "abstract": " In one embodiment, the present invention includes a computer-implemented method of adapting software component interfaces by providing a user interface for specifying a provided interface, a required interface and interface operators. In another embodiment the present invention includes a computer-implemented method of adapting interfaces comprising receiving an adapter specification, receiving a plurality of communication actions from a first software component, transforming one or more of the communication actions in accordance with predefined interface operations; and sending transformed communication actions to a second software component.", "num_citations": "23\n", "authors": ["21"]}
{"title": "Strategies in supply chain management for the Trading Agent Competition\n", "abstract": " Negotiating with suppliers and with customers is a key part of supply chain management. However, with recent technological advances, the mechanisms available to carry out such activities have become increasingly sophisticated, and the environment in which these activities take place has become highly dynamic. As a consequence, the overall planning of these complex trades, and the coordination of the various production and scheduling activities, need to be carefully considered by the businesses involved in the supply chain management. In order to guide the overall planning, production, scheduling, and allocation of resources, especially designed strategies are increasingly used by the businesses. In this setting, it is crucial that the intended behaviour, and through that, the desired outcomes, of these strategies be precisely understood. Using an empirical analysis, this paper investigates two fundamental\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "23\n", "authors": ["21"]}
{"title": "A process-based methodology for designing event-based mobile composite applications\n", "abstract": " Mobile application developers should be able to specify how applications can adapt to changing conditions, and to later reconfigure the application to suit new circumstances. Event-based communication have been advocated to facilitate such dynamic changes. Event-based models, however, are fragmented, which makes it difficult to understand the dependencies between components. A process-oriented methodology overcomes this issue, by specifying dependencies according to a process model. This paper describes a methodology that combines the comprehensibility and manageability of control from process-oriented methodologies, with the flexibility of event-based communication. This enables fine-grained adaptation of process-oriented applications.", "num_citations": "23\n", "authors": ["21"]}
{"title": "A comparison of secureUML and UMLsec for rolebased access control\n", "abstract": " Nowadays security has become an important aspect in information systems engineering. A mainstream method for information system security is Role-based Access Control (RBAC), which restricts system access to authorised users. Recently different authors have proposed a number of modelling languages (eg, abuse cases, misuse cases, secure i*, secure Tropos, and KAOS extensions to security) that facilitate the documentation and analysis of security aspects. However it is unclear if these languages support the full spectrum of RBAC specification needs. In this paper we selected two security modelling languages, namely SecureUML and UMLsec. Based on the literature study and on the running example we systematically investigate how these languages could be used for RBAC. Our observations indicate that, although both approaches originate from the de-facto industry standard UML, they are not competitors. Rather they complement each other: SecureUML helps defining static RBAC aspects; UMLsec is recommended for dynamic RBAC analysis. Hopefully our study will help practitioners to understand these two approaches better, especially when selecting them for modelling purposes. We also believe that the combination of both approaches would ease secure information system development.", "num_citations": "22\n", "authors": ["21"]}
{"title": "Case study: BPMN to BPEL model transformation\n", "abstract": " This case study considers the definition of model transformations between two languages for business process modeling, namely BPMN and BPEL. The model transformations should achieve four evaluation criteria: completeness, correctness, readability and reversibility.", "num_citations": "22\n", "authors": ["21"]}
{"title": "The 3dma middleware for mobile applications\n", "abstract": " Research in mobile devices have received extensive interest in recent years. Mobility raises new issues such as more dynamic context, limited computing resources, and frequent disconnections. To handle these issues, we propose a middleware, called 3DMA, which addresses three requirements: 1) decomposition, 2) distribution and 3) decoupling. 3DMA uses a space based approach combined with a set of \u0393\u00c7\u00a3workers\u0393\u00c7\u00a5 which are able to act on the users behalf either to reduce load on the mobile device, or to support disconnected behavior. In order to demonstrate aspects of the middleware architecture we consider the development of a mobile application.", "num_citations": "22\n", "authors": ["21"]}
{"title": "White-box prediction of process performance indicators via flow analysis\n", "abstract": " Predictive business process monitoring methods exploit historical process execution logs to provide predictions about running instances of a process, which enable process workers and managers to preempt performance issues or compliance violations. A number of approaches have been proposed to predict quantitative process performance indicators, such as remaining cycle time, cost, or probability of deadline violation. However, these approaches adopt a black-box approach, insofar as they predict a single scalar value without decomposing this prediction into more elementary components. In this paper, we propose a white-box approach to predict performance indicators of running process instances. The key idea is to first predict the performance indicator at the level of activities, and then to aggregate these predictions at the level of a process instance by means of flow analysis techniques. The paper\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "21\n", "authors": ["21"]}
{"title": "Minimizing overprocessing waste in business processes via predictive activity ordering\n", "abstract": " Overprocessing waste occurs in a business process when effort is spent in a way that does not add value to the customer nor to the business. Previous studies have identified a recurrent overprocessing pattern in business processes with so-called \u0393\u00c7\u00a3knockout checks\u0393\u00c7\u00a5, meaning activities that classify a case into \u0393\u00c7\u00a3accepted\u0393\u00c7\u00a5 or \u0393\u00c7\u00a3rejected\u0393\u00c7\u00a5, such that if the case is accepted it proceeds forward, while if rejected, it is cancelled and all work performed in the case is considered unnecessary. Thus, when a knockout check rejects a case, the effort spent in other (previous) checks becomes overprocessing waste. Traditional process redesign methods propose to order knockout checks according to their mean effort and rejection rate. This paper presents a more fine-grained approach where knockout checks are ordered at runtime based on predictive machine learning models. Experiments on two real-life processes show\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "21\n", "authors": ["21"]}
{"title": "Criteria and heuristics for business process model decomposition\n", "abstract": " It is generally agreed that large process models should be decomposed into sub-processes in order to enhance understandability and maintainability. Accordingly, a number of process decomposition criteria and heuristics have been proposed in the literature. This paper presents a review of the field revealing distinct classes of criteria and heuristics. The study raises the question of how different decomposition heuristics affect process model understandability and maintainability. To address this question, an experiment is conducted where two different heuristics, one based on breakpoints and the other on data objects, were used to decompose a flat process model. The results of the experiment show that, although there are minor differences, the heuristics cause very similar results in regard to understandability and maintainability as measured by various process model metrics.", "num_citations": "21\n", "authors": ["21"]}
{"title": "Squeezing out the cloud via profit-maximizing resource allocation policies\n", "abstract": " We study the problem of maximizing the average hourly profit earned by a Software-as-a-Service (SaaS) provider who runs a software service on behalf of a customer using servers rented from an Infrastructure-as-a-Service (IaaS) provider. The SaaS provider earns a fee per successful transaction and incurs costs pro-portional to the number of server-hours it uses. A number of resource allocation policies for this or similar problems have been proposed in previous work. However, to the best of our knowledge, these policies have not been comparatively evaluated in a cloud environment. This paper reports on an empirical evaluation of three policies using a replica of Wikipedia deployed on the Amazon EC2 cloud. Experimental results show that a policy based on a solution to an optimization problem derived from the SaaS provider's utility function outperforms well-known heuristics that have been proposed for\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "20\n", "authors": ["21"]}
{"title": "Optimized decentralization of composite web services\n", "abstract": " Composite services are usually specified by means of orchestration models that capture control and data-flow relations between activities. Concrete services are then assigned to each activity based on various criteria. In mainstream service orchestration platforms, the orchestration model is executed by a centralized orchestrator through which all interactions are channeled. This architecture is not optimal in terms of communication overhead and has the usual problems of a single point of failure. In previous work, we proposed a method for executing service orchestrations in a decentralized manner while fulfilling collocation and separation constraints. However, this and similar methods for decentralized orchestration do not seek to optimize the communication overhead between services participating in the orchestration. This paper presents a method for optimizing the selection of services assigned to activities in a\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "20\n", "authors": ["21"]}
{"title": "The business process modeling notation\n", "abstract": " Business processes may be analyzed and designed at different levels of abstraction. In this respect, it is common to distinguish between business process models intended for business analysis and improvement, and those intended for automation by means, for example, of a workflow engine such as YAWL. At the business analysis level, stakeholders focus on strategic and tactical issues such as cost, risks, resource utilization, and other nonfunctional aspects of process models. At the automation level, stakeholders are interested in making their models executable, which entails the need to provide detailed specifications of data types, data extraction and conversion steps, application bindings, resource allocation, and distribution policies, among others. The requirements for process modeling notations at these two levels of abstraction are significantly different. This in turn has resulted in different languages\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "20\n", "authors": ["21"]}
{"title": "Using CEP technology to adapt messages exchanged by web services\n", "abstract": " Web service may be unable to interact with each other because of incompatibilities between their interfaces. In this paper, we present an event driven approach which aims at adapting messages exchanged during service interactions. The proposed framework relies on the Complex Event Processing (CEP) technology, which provides an environment for the development of applications that need to continuously process, analyse and respond to event streams. Our main contribution is a system that enables developers to design and implement CEP-based adapters. These latter are deployed in a CEP engine which is responsible for continuously receiving messages and processing them according to rules implemented by the adapters. Resulting transformed messages are thus forwarded to their original service recipient.", "num_citations": "20\n", "authors": ["21"]}
{"title": "Understanding the challenges in getting together: The semantics of decoupling in middleware\n", "abstract": " It is well accepted that different types of distributed architectures require different levels of coupling. For example, in client-server and three-tier architectures, application components are generally tightly coupled, both to one-another and with the underlying middleware. Meanwhile, in off-line transaction processing, grid computing and mobile application architectures, the degree of coupling between application components and with the underlying middleware needs to be minimised. Terms such as \u0393\u00c7\u00a3synchronous\u0393\u00c7\u00a5,\u0393\u00c7\u00a3asynchronous\u0393\u00c7\u00a5,\u0393\u00c7\u00a3blocking\u0393\u00c7\u00a5,\u0393\u00c7\u00a3non-blocking\u0393\u00c7\u00a5,\u0393\u00c7\u00a3directed\u0393\u00c7\u00a5, and \u0393\u00c7\u00a3non-directed\u0393\u00c7\u00a5 are often used to refer to the level of coupling required by an architecture or provided by a middleware. However, these terms are used with various connotations. And while various informal definitions have been provided, there is a lack of an overarching formal framework upon which software architects can rely to unambiguously communicate architectural requirements with respect to (de-) coupling. This article addresses this gap by:(i) formally defining three dimensions of decoupling;(ii) relating these dimensions to existing middleware;(iii) proposing notational elements to represent various coupling integration patterns; and (iv) proposing an API that supports all the identified coupling patterns.", "num_citations": "20\n", "authors": ["21"]}
{"title": "Alarm-based prescriptive process monitoring\n", "abstract": " Predictive process monitoring is concerned with the analysis of events produced during the execution of a process in order to predict the future state of ongoing cases thereof. Existing techniques in this field are able to predict, at each step of a case, the likelihood that the case will end up in an undesired outcome. These techniques, however, do not take into account what process workers may do with the generated predictions in order to decrease the likelihood of undesired outcomes. This paper proposes a framework for prescriptive process monitoring, which extends predictive process monitoring approaches with the concepts of alarms, interventions, compensations, and mitigation effects. The framework incorporates a parameterized cost model to assess the cost-benefit tradeoffs of applying prescriptive process monitoring in a given setting. The paper also outlines an approach to optimize the generation\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "19\n", "authors": ["21"]}
{"title": "Decomposition driven consolidation of process models\n", "abstract": " Oftentimes business processes exist not as singular entities that can be managed in isolation, but as families of variants that need to be managed together. When it comes to modelling these variants, analysts are faced with the dilemma of whether to model each variant separately or to model multiple or all variants as a single model. The former option leads to a proliferation of models that share common parts, leading to redundancy and possible inconsistency. The latter approach leads to less but more complex models, thus hindering on their comprehensibility. This paper presents a decomposition driven method to capture a family of process variants in a consolidated manner taking into account the above trade-off. We applied our method on a case study in the banking sector. A reduction of 50% of duplication was achieved in this case study.", "num_citations": "19\n", "authors": ["21"]}
{"title": "Process-oriented assessment of web services\n", "abstract": " Though Web services offer unique opportunities for the design of new business processes, the assessment of the potential impact of Web services is often reduced to technical aspects. This paper proposes a four-phase methodology which facilitates the evaluation of the potential use of Web services in e-business systems both from a technical and from a strategic viewpoint. It is based on business process models, which are used to frame the adoption of Web services and to assess their impact on existing business processes. The application of this methodology is described using a procurement scenario.", "num_citations": "19\n", "authors": ["21"]}
{"title": "A programming language for web service development\n", "abstract": " There is now widespread acceptance of Web services and service-oriented architectures. But despite the agreement on key Web services standards there remain many challenges. Programming environments based on WSDL support go some way to facilitating Web service development. However Web services fundamentally rely on XML and Schema, not on contemporary programming language type systems such as those of Java or .NET. Moreover, Web services are based on a messaging paradigm and hence bring forward the traditional problems of messaging systems including concurrency control and message correlation. It is easy to write simple synchronous Web services using traditional programming languages; however more realistic scenarios are surprisingly difficult to implement. To alleviate these issues we propose a programming language which directly supports Web service development. The language leverages XQuery for native XML processing, supports implicit message correlation and has high level join calculus-style concurrency control. We illustrate the features of the language through a motivating example.", "num_citations": "19\n", "authors": ["21"]}
{"title": "Action logger: Enabling process mining for robotic process automation\n", "abstract": " This paper presents a tool, called Action Logger, for recording user interface (UI) logs, i.e., logs of user interactions with information systems. By generating output suitable for process mining, the tool aims to introduce process mining methods, techniques, and tools for supporting Robotic Process Automation (RPA) activities, e.g., robot discovery and implementation. Action Logger offers unique capabilities, including logging relevant user actions at a granularity level suitable for RPA, data-awareness, and context-independence.", "num_citations": "18\n", "authors": ["21"]}
{"title": "Abstract-and-Compare: A Family of Scalable Precision Measures for Automated Process Discovery\n", "abstract": " Automated process discovery techniques allow us to extract business process models from event logs. The quality of models discovered by these techniques can be assessed with respect to various criteria related to simplicity and accuracy. One of these criteria, namely precision, captures the extent to which the behavior allowed by a process model is observed in the log. While several measures of precision have been proposed, a recent study has shown that none of them fulfills a set of five axioms that capture intuitive properties behind the concept of precision. In addition, existing precision measures suffer from scalability issues when applied to models discovered from real-life event logs. This paper presents a family of precision measures based on the idea of comparing the k-th order Markovian abstraction of a process model against that of an event log. We demonstrate that this family of measures\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "18\n", "authors": ["21"]}
{"title": "Explaining international migration in the skype network: The role of social network features\n", "abstract": " In recent years, several new ways have appeared for quantifying human migration such as location based smartphone applications and tracking user activity from websites. To show usefulness of these new approaches, we present the results of a study of cross-country migration as observed via login events in the Skype network. We explore possibility to extract human migration and correlate it with institutional statistics. The study demonstrates that a number of social network features are strongly related to net migration from and to a given country, as well as net migration between pairs of countries. Specifically, we find that percentage of international calls, percentage of international links and foreign logins in a country, complemented by gross domestic product, can be used as relatively accurate proxies for estimating migration.", "num_citations": "18\n", "authors": ["21"]}
{"title": "Patterns of Process Modeling.\n", "abstract": " The previous chapters have presented different languages and approaches to process modeling. In this chapter, we review some issues in process modeling from a more language-independent perspective. To this end, we rely on the concept of pattern: an \u0393\u00c7\u00a3abstraction from a concrete form which keeps recurring in specific non-arbitrary contexts\u0393\u00c7\u00a5[18]. The use of patterns is a proven practice in the context of objectoriented design, as evidenced by the impact made by the design patterns of Gamma et al.[10].Process Aware Information Systems (PAISs) address a number of perspectives. Jablonski and Bussler [11] identify several such perspectives in the context of workflow management. These include the process perspective (describing the controlflow), organization perspective (structuring of resources), data/information perspective (to structure data elements), operation perspective (to describe the atomic process elements) and integration perspective (to \u0393\u00c7\u00a3glue\u0393\u00c7\u00a5 things together). 1 In a typical workflow management system, the process perspective is described in terms of some graphical model, eg a variant of Petri nets (see Chapter 7), the organization perspective is described by specifying and populating roles and organizational units, the data/information perspective is described by associating data elements to workflow instances (these may be typed and have a scope), the operation perspective is described by some scripting language used to launch external applications, and the integration perspective is described by some hierarchy of processes and activities. In principle, it is possible to define design patterns for each of these perspectives\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "18\n", "authors": ["21"]}
{"title": "Ontology markup for web forms generation\n", "abstract": " Ontologies are promising to become the keystones of the Semantic Web. The realisation of this promise requires that adequate approaches to model, represent, and mark up ontologies on the Web are developed. This paper presents an approach to model ontologies as populated conceptual schemas and to implement them using XML tools and relational databases. The issue of marking up these ontologies with presentation information is then addressed. The approach is applied to the generation of Web forms from marked up ontologies, in such a way that the generated forms encode the constraints expressed in the ontology.", "num_citations": "18\n", "authors": ["21"]}
{"title": "A Petri Nets based Generic Genetic Algorithm framework for resource optimization in business processes\n", "abstract": " Business process simulation (BPS) enables detailed analysis of resource allocation schemes prior to actually deploying and executing the processes. Although BPS has been widely researched in recent years, less attention has been devoted to intelligent optimization of resource allocation in business processes by exploiting simulation outputs. This paper endeavors to combine the power of a genetic algorithm (GA) in finding optimum resource allocation scheme and the benefits of the process simulation. Although GA has been successfully used for finding optimal resource allocation schemes in manufacturing processes, in this previous work the design of these algorithms is ad hoc, meaning that the chromosomes, crossover and selection operators, and fitness functions need to be manually tailored for each problem. In this research, we pioneer to design and implement a Petri Nets based Generic Genetic\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "17\n", "authors": ["21"]}
{"title": "Discovering process maps from event streams\n", "abstract": " Automated process discovery is a class of process mining methods that allow analysts to extract business process models from event logs. Traditional process discovery methods extract process models from a snapshot of an event log stored in its entirety. In some scenarios, however, events keep coming with a high arrival rate to the extent that it is impractical to store the entire event log and to continuously re-discover a process model from scratch. Such scenarios require online process discovery approaches. Given an event stream produced by the execution of a business process, the goal of an online process discovery method is to maintain a continuously updated model of the process with a bounded amount of memory while at the same time achieving similar accuracy as offline methods. However, existing online discovery approaches require relatively large amounts of memory to achieve levels of accuracy\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "17\n", "authors": ["21"]}
{"title": "Process-aware information systems\n", "abstract": " In the previous chapters, we learned how to use qualitative and quantitative analysis techniques in order to identify issues of existing business processes. We also saw that many processes in practice have problems with flow-time efficiency. Various redesign heuristics emphasize the potential of using information systems to improve process performance. This chapter deals with information systems that support process automation. First, we briefly explain what an automated business process is, after which we focus on two types of systems that are particularly suitable to achieve process automation, i.e., Process-Aware Information Systems (PAISs) and Business Process Management Systems (BPMSs). We will present the different variants of these systems and explain their features. Finally, we will discuss some of the advantages and challenges that are involved with introducing a BPMS in an organization.", "num_citations": "17\n", "authors": ["21"]}
{"title": "TEMPOS: A temporal database model seamlessly extending ODMG\n", "abstract": " This paper presents Tempos, a set of models and languages intended to seamlessly extend the ODMG object database standard with temporal functionalities. The proposed models exploit object-oriented technology to meet some important, yet traditionally neglected design criteria, related to legacy code migration and representation independence. Tempos has been fully formalized both at the syntactical and the semantical level and implemented on top of the O2 DBMS. Its suitability in regard to applications' requirements has been validated through concrete case studies from various contexts.", "num_citations": "17\n", "authors": ["21"]}
{"title": "Analyse de donn\u251c\u2310es g\u251c\u2310ographiques: application des Bases de Donn\u251c\u2310es Temporelles.\n", "abstract": " Nous pr\u251c\u2310sentons une application concernant l'analyse des processus d'utilisation des ressources et de l'espace au cours du temps, dans une station touristique des Alpes fran\u251c\u00baaises. Elle vise \u251c\u00e1 contribuer aux r\u251c\u2310flexions sur la r\u251c\u2310habilitation des infrastructures de la station. Dans cet article les aspects temporels de cette application sont \u251c\u2310tudi\u251c\u2310s en s'appuyant sur TEMPOS qui est un mod\u251c\u00bfle d'historique extensible int\u251c\u2310grant les concepts principaux et les facilit\u251c\u2310s n\u251c\u2310cessaires \u251c\u00e1 la gestion de la dimension historique des donn\u251c\u2310es au-dessus d'un SGBD.", "num_citations": "17\n", "authors": ["21"]}
{"title": "Predicting process performance: A white\u0393\u00c7\u00c9box approach based on process models\n", "abstract": " Predictive business process monitoring methods exploit historical process execution logs to provide predictions about running instances of a process. These predictions enable process workers and managers to preempt performance issues or compliance violations. A number of approaches have been proposed to predict quantitative process performance indicators for running instances of a process, including remaining cycle time, cost, or probability of deadline violation. However, these approaches adopt a black\u0393\u00c7\u00c9box approach, insofar as they predict a single scalar value without decomposing this prediction into more elementary components. In this paper, we propose a white\u0393\u00c7\u00c9box approach to predict performance indicators of running process instances. The key idea is to first predict the performance indicator at the level of activities and then to aggregate these predictions at the level of a process instance by\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "16\n", "authors": ["21"]}
{"title": "Browserbite: cross\u0393\u00c7\u00c9browser testing via image processing\n", "abstract": " Cross\u0393\u00c7\u00c9browser compatibility testing is concerned with identifying perceptible differences in the way a Web page is rendered across different browsers or configurations thereof. Existing automated cross\u0393\u00c7\u00c9browser compatibility testing methods are generally based on document object model (DOM) analysis, or in some cases, a combination of DOM analysis with screenshot capture and image processing. DOM analysis, however, may miss incompatibilities that arise not during DOM construction but rather during rendering. Conversely, DOM analysis produces false alarms because different DOMs may lead to identical or sufficiently similar renderings. This paper presents a novel method for cross\u0393\u00c7\u00c9browser testing based purely on image processing. The method relies on image segmentation to extract \u0393\u00c7\u00ffregions\u0393\u00c7\u00d6 from a Web page and computer vision techniques to extract a set of characteristic features from each region\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "16\n", "authors": ["21"]}
{"title": "Issue dynamics in github projects\n", "abstract": " Issue repositories are used to keep of track of bugs, development tasks and feature requests in software development projects. In the case of open source projects, everyone can submit a new issue in the tracker. This practice can lead to situations where more issues are created than what can be effectively handled by the project members, raising the question of how issues are treated as the capacity of the project members is exceeded. In this paper, we study the temporal dynamics of issues in a popular open source development platform, namely Github, based on a sample of 4000 projects. We specifically analyze how the rate of issue creation, the amount of pending issues, and their average lifetime evolve over the course of time. The results show that more issues are opened shortly after the creation of a project repository and that the amount of pending issues increases inexorably due to forgotten\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "16\n", "authors": ["21"]}
{"title": "From petri nets to guard-stage-milestone models\n", "abstract": " Artifact-centric modeling is an approach for modeling business processes based on business artifacts, i.e., entities that are central for the company\u0393\u00c7\u00d6s operations. Existing process mining methods usually focus on traditional process-centric rather than artifact-centric models. Furthermore, currently no methods exist for discovering models in Guard-Stage-Milestone (GSM) notation from event logs. To bridge this gap, we propose a method for translating Petri Net models into GSM which gives the possibility to use the numerous existing algorithms for mining Petri Nets for discovering the life cycles of single artifacts and then generating GSM models.", "num_citations": "16\n", "authors": ["21"]}
{"title": "Towards an open and extensible business process simulation engine\n", "abstract": " This paper outlines the architecture and initial proof-ofconcept implementation of an open and extensible business process model simulator based on CPN tools. The key component of the simulator is a transformation from BPMN process models to CPNs. This transformation is structured as a set of templates that can be extended and modified by developers in order to incorporate new functionality into the simulator. The paper illustrates how this templating mechanism can be used to capture different types of tasks and resource allocation policies.", "num_citations": "16\n", "authors": ["21"]}
{"title": "Middleware support for mobile applications\n", "abstract": " Mobile devices have received much research interest in recent years. Mobility raises new issues such as more dynamic context, limited computing resources, and frequent disconnections. A middleware infrastructure for mobile computing must handle all these issues properly. In this project we propose a middleware, called 3DMA, to support mobile computing. We introduce three requirements, distribution, decoupling and decomposition as central issues for mobile middleware. 3DMA uses a space based middleware, which facilitates the implementation of decoupled behavior and support for disconnected operation and context awareness. This is done by defining a set of \u0393\u00c7\u00a3workers\u0393\u00c7\u00a5 which are able to act on the users behalf either: to reduce load on the mobile device, and/or to support disconnected behavior. In order to demonstrate aspects of the middleware architecture we then consider the development of a\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "16\n", "authors": ["21"]}
{"title": "Experience using a coordination-based architecture for adaptive web content provision\n", "abstract": " There are many ways of achieving scalable dynamic web content. In previous work we have focused on dynamic content degradation using a standard architecture and a design-time \u0393\u00c7\u00a3Just In Case\u0393\u00c7\u00a5 methodology. In this paper, we address certain shortcomings witnessed in our previous work by using an alternate coordination-based architecture, which has interesting applicability to run-time web server adaptation. We first establish the viability of using this architecture for high-volume dynamic web content generation. In doing so, we establish its ability to maintain high throughput in overload conditions. We go on to show how we used the architecture to achieve a \u0393\u00c7\u00a3Just In Time\u0393\u00c7\u00a5 adaptation to achieve dynamic web content degradation in a running web application server.", "num_citations": "16\n", "authors": ["21"]}
{"title": "A Representation-Independent Temporal Extension of ODMG's Object Query Language.\n", "abstract": " TEMPOS is a set of models providing a framework for extending database systems with temporal functionalities. Based on this framework, an extension of the ODMG\u0393\u00c7\u00d6s object database standard has been defined. This extension includes a hierarchy of abstract datatypes for managing temporal values and histories, as well as temporal extensions of ODMG\u0393\u00c7\u00d6s object model, schema definition language and query language. This paper focuses on the latter, namely TEMPOQL. With respect to related proposals, the main originality of TEMPOQL is that it allows to manipulate histories regardless of their representations, by composition of functional constructs. Thereby, the abstraction principle of object-orientation is fulfilled, and the functional nature of OQL is enforced. In fact, TEMPOQL goes further in preserving OQL\u0393\u00c7\u00d6s structure, by generalizing most standard OQL constructs to deal with histories. The overall proposal has been fully formalized both at the syntactical and the semantical level and implemented as a preprocessor.", "num_citations": "16\n", "authors": ["21"]}
{"title": "Business process variant analysis: Survey and classification\n", "abstract": " It is common for business processes to exhibit a high degree of internal heterogeneity, in the sense that the executions of the process differ widely from each other due to contextual factors, human factors, or deliberate business decisions. For example, a quote-to-cash process in a multinational company is typically executed differently across different countries or even across different regions in the same country. Similarly, an insurance claims handling process might be executed differently across different claims handling centers or across multiple teams within the same claims handling center. A subset of executions of a business process that can be distinguished from others based on a given predicate (e.g. the executions of a process in a given country) is called a process variant. Understanding differences between process variants helps analysts and managers to make informed decisions as to how to standardize\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "15\n", "authors": ["21"]}
{"title": "Business process deviance mining: review and evaluation\n", "abstract": " Business process deviance refers to the phenomenon whereby a subset of the executions of a business process deviate, in a negative or positive way, with respect to its expected or desirable outcomes. Deviant executions of a business process include those that violate compliance rules, or executions that undershoot or exceed performance targets. Deviance mining is concerned with uncovering the reasons for deviant executions by analyzing business process event logs. This article provides a systematic review and comparative evaluation of deviance mining approaches based on a family of data mining techniques known as sequence classification. Using real-life logs from multiple domains, we evaluate a range of feature types and classification methods in terms of their ability to accurately discriminate between normal and deviant executions of a process. We also analyze the interestingness of the rule sets extracted using different methods. We observe that feature sets extracted using pattern mining techniques only slightly outperform simpler feature sets based on counts of individual activity occurrences in a trace.", "num_citations": "15\n", "authors": ["21"]}
{"title": "Comparative evaluation of process mining tools\n", "abstract": " Process mining is relatively young research area that meets the gap between businesses processes and various IT systems. Event logs are the primary sources for a process mining project and they are captured by different data sources including databases, ERP systems, CRM systems, audit trails, hospital information systems, bank transaction logs, etc. The extracted knowledge from this log enable us to discover the actual process and existing process model for further analysis, evaluation and continuous improvement in their quality. This way, various process mining tools have been developed in the market. Nevertheless, there is a lack of sufficent and comprehensive evaluation frameworks that assist users in selecting the right tool.This thesis proposes a framework that enables the comparison of process mining tools in terms of their functional features. The proposed operations are linked to typical problems reported in existing process mining use cases. Using this framework, the thesis compares three process mining tools, namely ProM, Disco and Celonis The comparison shows that while these tools provide comparable functionality they differ in terms of the way the functionality is provided.", "num_citations": "15\n", "authors": ["21"]}
{"title": "Enabling process innovation via deviance mining and predictive monitoring\n", "abstract": " A long-standing challenge in the field of business process management is how to deal with processes that exhibit high levels of variability, such as customer lead management, product design or healthcare processes. One thing that is understood about these processes is that they require process designs and support environments that leave considerable freedom so that process workers can readily deviate from pre-established paths. At the same time, consistent management of these processes requires workers and process owners to understand the implications of their actions and decisions on the performance of the process. We present two emerging techniques\u0393\u00c7\u00f6deviance mining and predictive monitoring\u0393\u00c7\u00f6that leverage information hidden in business process execution logs in order to provide guidance to stakeholders so that they can steer the process towards consistent and compliant outcomes and\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "15\n", "authors": ["21"]}
{"title": "Bursty egocentric network evolution in skype\n", "abstract": " In this study we analyze the dynamics of the contact list evolution of millions of users of the Skype communication network. We find that egocentric networks evolve heterogeneously in time as events of edge additions and deletions of individuals are grouped in long bursty clusters, which are separated by long inactive periods. We classify users by their link creation dynamics and show that bursty peaks of contact additions are likely to appear shortly after user account creation. We also study possible relations between bursty contact addition activity and other user-initiated actions like free and paid service adoption events. We show that bursts of contact additions are associated with increases in activity and adoption\u0393\u00c7\u00f6an observation that can inform the design of targeted marketing tactics.", "num_citations": "15\n", "authors": ["21"]}
{"title": "Identifying and classifying variations in business processes\n", "abstract": " Many business processes exist not as singular entities but rather as a plurality of variants that need to be collectively managed. The spectrum of approaches for managing collections of process variants range from capturing all variants in a large consolidated model, down to capturing each variant as a separate model. Most of these approaches are built on the assumption that the variation points and variation drivers are given as input. The question of how process variation is elicited and conceptualized in the first place has received relatively little attention. As a step to filling this gap, this paper puts forward a framework for identifying and classifying variation drivers in business processes. We apply the framework on two collections of process models: one consisting of a collection of process models implicitly clustered along product type and the other one along market type. In both cases, the framework\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "15\n", "authors": ["21"]}
{"title": "BESERIAL: Behavioural service interface analyser\n", "abstract": " In a service-oriented architecture, software services interact by means of message exchanges that follow certain patterns documented in the form of behavioural interfaces. As any software artifact, a service interface evolves over time. When this happens, incompatibility problems may arise. We demonstrate a tool, namely BESERIAL, that can pinpoint incompatibilities between behavioural interfaces.", "num_citations": "15\n", "authors": ["21"]}
{"title": "TEMPOS: a platform for developing temporal applications on top of object DBMS\n", "abstract": " We present TEMPOS: a set of models and languages supporting the manipulation of temporal data on top of object DBMS. The proposed models exploit object-oriented technology to meet some important, yet traditionally neglected design criteria related to legacy code migration and representation independence. Two complementary ways for accessing temporal data are offered: a query language and a visual browser. The query language, namely TEMPOQL, is an extension of OQL supporting the manipulation of histories regardless of their representations, through fully composable functional operators. The visual browser offers operators that facilitate several time-related interactive navigation tasks, such as studying a snapshot of a collection of objects at a given instant, or detecting and examining changes within temporal attributes and relationships. TEMPOS models and languages have been formalized both at\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "15\n", "authors": ["21"]}
{"title": "Process mining meets causal machine learning: Discovering causal rules from event logs\n", "abstract": " This paper proposes an approach to analyze an event log of a business process in order to generate case-level recommendations of treatments that maximize the probability of a given outcome. Users classify the attributes in the event log into controllable and non-controllable, where the former correspond to attributes that can be altered during an execution of the process (the possible treatments). We use an action rule mining technique to identify treatments that co-occur with the outcome under some conditions. Since action rules are generated based on correlation rather than causation, we then use a causal machine learning technique, specifically uplift trees, to discover subgroups of cases for which a treatment has a high causal effect on the outcome after adjusting for confounding variables. We test the relevance of this approach using an event log of a loan application process and compare our findings with\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "14\n", "authors": ["21"]}
{"title": "Automated discovery of declarative process models with correlated data conditions\n", "abstract": " Automated process discovery techniques enable users to generate business process models from event logs extracted from enterprise information systems. Traditional techniques in this field generate procedural process models (e.g., in the BPMN notation). When dealing with highly variable processes, the resulting procedural models are often too complex to be practically usable. An alternative approach is to discover declarative process models, which represent the behavior of the process as a set of constraints. Declarative process discovery techniques have been shown to produce simpler models than procedural ones, particularly for processes with high variability. However, the bulk of approaches for automated discovery of declarative process models focus on the control-flow perspective, ignoring the data perspective. This paper addresses the problem of discovering declarative process models with data\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "14\n", "authors": ["21"]}
{"title": "Multi-perspective comparison of business process variants based on event logs\n", "abstract": " A process variant represents a collection of cases with certain shared characteristics, e.g. cases that exhibit certain levels of performance. The comparison of business process variants based on event logs is a recurrent operation in the field of process mining. Existing approaches focus on comparing variants based on directly-follows relations such as \u0393\u00c7\u00a3a task directly follows another one\u0393\u00c7\u00a5 or a \u0393\u00c7\u00a3resource directly hands-off to another resource\u0393\u00c7\u00a5. This paper presents a more general approach to log-based process variant comparison based on so-called perspective graphs. A perspective graph is a graph-based abstraction of an event log where a node represents any entity referred to in the log (e.g. task, resource, location) and an arc represents a relation between these entities within or across cases (e.g. directly-follows, co-occurs, hands-off to, works-together with). Statistically significant differences between two\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "14\n", "authors": ["21"]}
{"title": "Reverse-engineering conference rankings: what does it take to make a reputable conference?\n", "abstract": " In recent years, several national and community-driven conference rankings have been compiled. These rankings are often taken as indicators of reputation and used for a variety of purposes, such as evaluating the performance of academic institutions and individual scientists, or selecting target conferences for paper submissions. Current rankings are based on a combination of objective criteria and subjective opinions that are collated and reviewed through largely manual processes. In this setting, the aim of this paper is to shed light into the following question: to what extent existing conference rankings reflect objective criteria, specifically submission and acceptance statistics and bibliometric indicators? The paper specifically considers three conference rankings in the field of Computer Science: an Australian national ranking, a Brazilian national ranking and an informal community-built ranking. It is\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "14\n", "authors": ["21"]}
{"title": "Data warehouse model for audit trail analysis in workflows\n", "abstract": " Business process performance evaluation is a key step towards assessing and improving e-business operations. In real-scale scenarios, such evaluation requires the collection, aggregation and processing of vast amounts of data, in particular audit trails. This paper aims at enabling such evaluation by integrating workflow technology with data warehousing. We first present a data model for capturing workflow audit trail data relevant to process performance evaluation. We then construct logical models that characterize the derivation of performance evaluation data from workflow audit trails. Based on these models, we apply dimensional modeling techniques to define schemas for storing workflow audit trail data in data warehouses. Using data warehouse technology, decision makers are able to query large volumes of audit trail data for business process performance evaluation.", "num_citations": "14\n", "authors": ["21"]}
{"title": "Secure multi-party computation for inter-organizational process mining\n", "abstract": " Process mining is a family of techniques for analyzing business processes based on event logs extracted from information systems. Mainstream process mining tools are designed for intra-organizational settings, insofar as they assume that an event log is available for processing as a whole. The use of such tools for inter-organizational process analysis is hampered by the fact that such processes involve independent parties who are unwilling to, or sometimes legally prevented from, sharing detailed event logs with each other. In this setting, this paper proposes an approach for constructing and querying a common artifact used for process mining, namely the frequency and time-annotated Directly-Follows Graph\u252c\u00e1(DFG), over multiple event logs belonging to different parties, in such a way that the parties do not share the event logs with each other. The proposal leverages an existing platform for secure multi\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "13\n", "authors": ["21"]}
{"title": "Interpreted execution of business process models on blockchain\n", "abstract": " Blockchain technology provides a tamper-proof mechanism to execute inter-organizational business processes involving mutually untrusted parties. Existing approaches to blockchain-based process execution are based on code generation. In these approaches, a process model is compiled into one or more smart contracts, which are then deployed on a blockchain platform. Given the immutability of the deployed smart contracts, these compiled approaches ensure that all process instances conform to the process model. However, this advantage comes at the price of inflexibility. Any changes to the process model require the redeployment of the smart contracts (a costly operation). In addition, changes cannot be applied to running process instances. To address this lack of flexibility, this paper presents an interpreter of BPMN process models based on dynamic data structures. The proposed interpreter is embedded\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "13\n", "authors": ["21"]}
{"title": "Evaluation of synchronization gateways in process models\n", "abstract": " A system may include a thread monitor that is arranged and configured to monitor progress of multiple threads of a workflow process at a synchronization point with each of the threads having a state, and configured to generate at least one inspection trigger for inspection of the threads. A thread inspector may inspect the threads at the synchronization point for a change in the state in any of the threads in response to the inspection trigger. A firing rules engine may determine whether or not the synchronization point should fire based at least in part on the change in the state of at least one of the threads.", "num_citations": "13\n", "authors": ["21"]}
{"title": "Manipulation de valeurs temporelles dans un SGBD a objets\n", "abstract": " Sauf mention contraire ci-dessus, le contenu de cette notice bibliographique peut \u251c\u00actre utilis\u251c\u2310 dans le cadre d\u0393\u00c7\u00d6une licence CC BY 4.0 Inist-CNRS/Unless otherwise stated above, the content of this bibliographic record may be used under a CC BY 4.0 licence by Inist-CNRS/A menos que se haya se\u251c\u2592alado antes, el contenido de este registro bibliogr\u251c\u00edfico puede ser utilizado al amparo de una licencia CC BY 4.0 Inist-CNRS", "num_citations": "13\n", "authors": ["21"]}
{"title": "Nirdizati: A web-based tool for predictive process monitoring\n", "abstract": " This paper introduces Nirdizati: A web-based application for generating predictions about running cases of a business process. Nirdizati is a configurable full-stack web application that supports users in selecting and tuning prediction methods from a list of implemented algorithms and enables the continuous prediction of various performance indicators at runtime. The tool can be used to predict the outcome, the next events, the remaining time, or the overall workload per day of each case of a process. For example, in a lead-to-order process, Nirdizati can predict which customer leads will convert to purchase orders and when. In a claim handling process, it can predict if a claim decision will be made on time or late. The predictions, as well as real-time summary statistics about the process executions, are presented in a dashboard that offers multiple visualization options. Based on these predictions, process participants can act proactively to resolve or mitigate potential process performance violations. The target audience of this demonstration includes process mining researchers as well as practitioners interested in exploring the potential of predictive process monitoring.", "num_citations": "12\n", "authors": ["21"]}
{"title": "Modeling Software Processes Using BPMN: When and When Not?\n", "abstract": " Software process models capture structural and behavioral properties of software development activities, supporting the elicitation, analysis, simulation, and improvement of software development processes. Various approaches for the modeling and model-driven analysis of software development processes have been proposed but little progress has been made regarding standardization. With increasing demands regarding flexibility and adaptability of development processes, the constant evolution of development methods and tools, and the trend toward continuous product deployment, better support for process engineers in terms of universally applicable modeling notations as well as simulation and enactment mechanisms has become more desirable than ever. In contrast to software process modeling, the discipline of business process modeling has attained a greater level of consensus and\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "12\n", "authors": ["21"]}
{"title": "The Rise of the Estonian Start-up sphere\n", "abstract": " Twenty years ago, when the local economy of Estonia was in shatters following the Soviet collapse, few would have predicted that this small country on the border between the European Union and Russia would become an ecosystem of IT start-ups. How did it happened, and what conditions enabled it?", "num_citations": "12\n", "authors": ["21"]}
{"title": "Management and engineering of process-aware information systems: Introduction to the special issue\n", "abstract": " In this editorial letter, we provide the readers of Information Systems with a birds-eye introduction to Process-aware Information Systems (PAIS) \u0393\u00c7\u00f4 a sub-field of Information Systems that has drawn growing attention in the past two decades, both as an engineering and as a management discipline. Against this backdrop, we briefly discuss how the papers included in this special issue contribute to extending the body of knowledge in this field.", "num_citations": "12\n", "authors": ["21"]}
{"title": "Orthogonally modeling video structuration and annotation: exploiting the concept of granularity\n", "abstract": " In this paper, we exploit the concept of granularity to design a video metadata model that addresses both logical structuration and content annotation in an orthogonal way. We then show that thanks to this orthogonality, the proposed model properly captures the interactions between these two aspects, in the sense that annotations may be independently attached to any level of video structuration. In other words, an annotation attached to a scene is not treated in the same way as an annotation attached to every frame of that scene. We also investigate what are the implications of this orthogonality regarding querying and composition.", "num_citations": "12\n", "authors": ["21"]}
{"title": "Automated discovery of data transformations for robotic process automation\n", "abstract": " Robotic Process Automation (RPA) is a technology for automating repetitive routines consisting of sequences of user interactions with one or more applications. In order to fully exploit the opportunities opened by RPA, companies need to discover which specific routines may be automated, and how. In this setting, this paper addresses the problem of analyzing User Interaction (UI) logs in order to discover routines where a user transfers data from one spreadsheet or (Web) form to another. The paper maps this problem to that of discovering data transformations by example - a problem for which several techniques are available. The paper shows that a naive application of a state-of-the-art technique for data transformation discovery is computationally inefficient. Accordingly, the paper proposes two optimizations that take advantage of the information in the UI log and the fact that data transfers across applications typically involve copying alphabetic and numeric tokens separately. The proposed approach and its optimizations are evaluated using UI logs that replicate a real-life repetitive data transfer routine.", "num_citations": "11\n", "authors": ["21"]}
{"title": "Mining business process stages from event logs\n", "abstract": " Process mining is a family of techniques to analyze business processes based on event logs recorded by their supporting information systems. Two recurrent bottlenecks of existing process mining techniques when confronted with real-life event logs are scalability and interpretability of the outputs. A common approach to tackle these limitations is to decompose the process under analysis into a set of stages, such that each stage can be mined separately. However, existing techniques for automated discovery of stages from event logs produce decompositions that are very different from those that domain experts would produce manually. This paper proposes a technique that, given an event log, discovers a\u252c\u00e1stage decomposition that maximizes a measure of modularity borrowed from the field of social network analysis. An empirical evaluation on real-life event logs shows that the produced decompositions\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "11\n", "authors": ["21"]}
{"title": "Community-centric analysis of user engagement in skype social network\n", "abstract": " Traditional approaches to user engagement analysis focus on individual users. In this paper we address user engagement analysis at the level of groups of users (social communities). From the entire Skype social network we extract communities by means of representative community detection methods each one providing node partitions having their own peculiarities. We then examine user engagement in the extracted communities putting into evidence clear relations between topological and geographic features of communities and their mean user engagement. In particular we show that user engagement can be to a great extent predicted from such features. Moreover, from the analysis it clearly emerges that the choice of community definition and granularity deeply affect the predictive performance.", "num_citations": "11\n", "authors": ["21"]}
{"title": "Dimensions of coupling in middleware\n", "abstract": " It is well accepted that different types of distributed architectures require different degrees of coupling. For example, in client\u0393\u00c7\u00c9server and three\u0393\u00c7\u00c9tier architectures, application components are generally tightly coupled, both with one another and with the underlying middleware. Meanwhile, in off\u0393\u00c7\u00c9line transaction processing, grid computing and mobile applications, the degree of coupling between application components and with the underlying middleware needs to be minimized. Terms such as \u0393\u00c7\u00ffsynchronous\u0393\u00c7\u00d6, \u0393\u00c7\u00ffasynchronous\u0393\u00c7\u00d6, \u0393\u00c7\u00ffblocking\u0393\u00c7\u00d6, \u0393\u00c7\u00ffnon\u0393\u00c7\u00c9blocking\u0393\u00c7\u00d6, \u0393\u00c7\u00ffdirected\u0393\u00c7\u00d6, and \u0393\u00c7\u00ffnon\u0393\u00c7\u00c9directed\u0393\u00c7\u00d6 are often used to refer to the degree of coupling required by an architecture or provided by a middleware. However, these terms are used with various connotations. Although various informal definitions have been provided, there is a lack of an overarching formal framework to unambiguously communicate architectural requirements with\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "11\n", "authors": ["21"]}
{"title": "Communication abstractions for distributed business processes\n", "abstract": " Languages for business process definition generally suffer from myopic approaches to capturing communication between distributed processes. Effective communication between processes requires: support for conversations involving interrelated interactions spread over time; ability to select and group messages based on their content, regardless of format and transport technology; and resolving contention between processes or tasks for common sets of messages. This paper presents a set of communication abstractions that provide a \u0393\u00c7\u00a3glue\u0393\u00c7\u00a5 between the process layer and the middleware. The paper also reports on an implementation of these abstractions and an experimental evaluation.", "num_citations": "11\n", "authors": ["21"]}
{"title": "Getting started with YAWL\n", "abstract": " Nowadays, organisations are challenged to continuously improve their efficiency and to respond quickly to changes in their environment, such as new business opportunities, competition threats, and evolving customer expectations. It is not surprising then that organisations are paying more attention to capturing, analysing and improving their work practices in a systematic manner. The methods, techniques and tools to do this are collectively known as Business Process Management (BPM).For IT departments, BPM provides an opportunity to align IT systems with business requirements, and to re-organise existing application infrastructure to better support the day-to-day operations of the organisation. BPM initiatives often translate into requirements for IT systems. Here is where workflow technology comes into play. Business process models produced by business experts are taken as a starting point by software architects to produce a blueprint for a software application that coordinates, monitors and controls some or all of the tasks that make up these business processes. Such software applications are called workflows. An example of a business process is an order-to-cash process: one that goes from the moment a purchase order for a product or service is received by an organisation to the moment the customer pays for the products, including aspects such as invoicing and shipment. After capturing this process from beginning to end, an organisation may choose to add further details about the people, legacy applications, messages and documents involved, and to deploy a workflow application to coordinate this process. You can build a\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "11\n", "authors": ["21"]}
{"title": "Correlating activation and target conditions in data-aware declarative process discovery\n", "abstract": " Automated process discovery is a branch of process mining that allows users to extract process models from event logs. Traditional automated process discovery techniques are designed to produce procedural process models as output (e.g., in the BPMN notation). However, when confronted to complex event logs, automatically discovered process models can become too complex to be practically usable. An alternative approach is to discover declarative process models, which represent the behavior of the process in terms of a set of business constraints. These approaches have been shown to produce simpler process models, especially in the context of processes with high levels of variability. However, the bulk of approaches for automated discovery of declarative process models are focused on the control-flow perspective of business processes and do not cover other perspectives, e.g., the data, time\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "10\n", "authors": ["21"]}
{"title": "Differential privacy analysis of data processing workflows\n", "abstract": " Differential privacy is an established paradigm to measure and control private information leakages occurring as a result of disclosures of derivatives of sensitive data sources. The bulk of differential privacy research has focused on designing mechanisms to ensure that the output of a program or query is -differentially private with respect to its input. In an enterprise environment however, data processing generally occurs in the context of business processes consisting of chains of tasks performed by multiple IT system components, which disclose outputs to multiple parties along the way. Ensuring privacy in this setting requires us to reason in terms of series of disclosures of intermediate and final outputs, derived from multiple data sources. This paper proposes a method to quantify the amount of private information leakage from each sensitive data source vis-a-vis of each party involved in a business\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "10\n", "authors": ["21"]}
{"title": "Cross-browser testing in browserbite\n", "abstract": " Cross-browser compatibility testing aims at verifying that a web page is rendered as intended by its developers across multiple browsers and platforms. Browserbite is a tool for cross-browser testing based on comparison of screenshots with the aim of identifying differences that a user may perceive as incompatibilities. Browserbite is based on segmentation and image comparison techniques adapted from the field of computer vision. The key idea is to first extract web page regions via segmentation and then to match and compare these regions pairwise based on geometry and pixel density distribution. Additional accuracy is achieved by post-processing the output of the region comparison step via supervised machine learning techniques. In this way, compatibility checking is performed based purely on screenshots rather than relying on the Document Object Model (DOM), an alternative that often leads to\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "10\n", "authors": ["21"]}
{"title": "Memory-efficient fast shortest path estimation in large social networks\n", "abstract": " As the sizes of contemporary social networks surpass billions of users, so grows the need for fast graph algorithms to analyze them. A particularly important basic operation is the computation of shortest paths between nodes. Classical exact algorithms for this problem are prohibitively slow on large graphs, which motivates the development of approximate methods. Of those, landmark-based methods have been actively studied in recent years. Landmark-based estimation methods start by picking a fixed set of landmark nodes, precomputing the distance from each node in the graph to each landmark, and storing the precomputed distances in a data structure. Prior work has shown that the number of landmarks required to achieve a given level of precision grows with the size of the graph. Simultaneously, the size of the data structure is proportional to the product of the size of the graph and the number of landmarks. In this work we propose an alternative landmark-based distance estimation approach that substantially reduces space requirements by means of pruning: computing distances from each node to only a small subset of the closest landmarks. We evaluate our method on the DBLP, Orkut, Twitter and Skype social networks and demonstrate that the resulting estimation algorithms are comparable in query time and potentially superior in approximation quality to equivalent non-pruned landmark-based methods, while requiring less memory or disk space.", "num_citations": "10\n", "authors": ["21"]}
{"title": "Process intelligence\n", "abstract": " It is a central idea of BPM that processes are explicitly defined, then executed, and that information about process execution is prepared and analyzed. In this way, this information provides a feedback loop on how the process might be redesigned. Data about the execution of processes can stem from BPMSs in which processes are specified, but also from systems that do not work with an explicit process model, for instance ERP systems or ticketing systems. Data from those systems have to be transformed to meet the requirements of intelligent process execution analysis. This field is typically referred to as process mining.           This chapter deals with intelligently using the data generated from the execution of the process. We refer to such data as event logs, covering what has been done when by whom in relation to which process instance. First, we investigate the structure of event logs, their relationship to\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "10\n", "authors": ["21"]}
{"title": "Predicting the maintainability of XSL transformations\n", "abstract": " XSLT is a popular language for implementing both presentation templates in Web applications as well as document and message converters in enterprise applications. The widespread adoption and popularity of XSLT raises the challenge of efficiently managing the evolution of significant amounts of XSLT code. This challenge calls for guidelines and tool support for developing maintainable XSLT code. In this setting, this paper addresses the following question: Can the maintainability of XSL transformations, measured in terms of code churn in the next revision of a transformation, be predicted using a combination of simple metrics? This question is studied using a dataset extracted from open-source software project repositories. An outcome of this empirical study is a set of statistical models for predicting the maintainability of XSL transformations with relatively high accuracy. In addition, by analyzing the major\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "10\n", "authors": ["21"]}
{"title": "Configurable SOAP proxy cache for data provisioning web services\n", "abstract": " This paper presents a proxy cache for data provisioning SOAP-based Web services. The proposed solution relies on a novel caching scheme to efficiently identify cache hits and provides fine-grained configurability by supporting the definition of caching policies at the service and at the operation levels. The proposal has been evaluated using the logs of service calls of a Web service marketplace. The evaluation results show that an appropriately configured SOAP cache provides major performance benefits in practical settings. Furthermore, it is shown that the LRU (Least Recently Used) replacement policy provides the most effective hit ratio, byte hit ratio and delay savings ratio compared to LFU and Size in case of Web services--a result that is in line with similar results in traditional Web page caching.", "num_citations": "10\n", "authors": ["21"]}
{"title": "Service-enabled process management\n", "abstract": " This chapter discusses some relationships between service-oriented architecture and Business Process Management. In particular, the chapter presents a method for analyzing a business process to enable its execution on top of a service-oriented application landscape, thereby leading to the notion of service-enabled business process. The chapter also provides an overview of contemporary technology standards for implementing service-enabled processes.", "num_citations": "10\n", "authors": ["21"]}
{"title": "GPSL: A programming language for service implementation\n", "abstract": " At present, there is a dichotomy of approaches to supporting web service implementation: extending mainstream programming languages with libraries and metadata notations vs. designing new languages. While the former approach has proven suitable for interconnecting services on a simple point-to-point fashion, it turns to be unsuitable for coding concurrent, multi-party, and interrelated interactions requiring extensive XML manipulation. As a result, various web service programming languages have been proposed, most notably (WS-)BPEL. However, these languages still do not meet the needs of highly concurrent and dynamic interactions due to their bias towards statically-bounded concurrency. In this paper we introduce a new web service programming language with a set of features designed to address this gap. We describe the implementations in this language of non-trivial scenarios of service\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "10\n", "authors": ["21"]}
{"title": "An architecture for assembling agents that participate in alternative heterogeneous auctions\n", "abstract": " This paper addresses the issue of developing agents capable of participating in several potentially simultaneous auctions of different kinds (English, First-Price, Vickrey), with the goal of finding the best price for an item on behalf of their users. Specifically, a multi-agent architecture is proposed, in which a manager agent cooperates with several expert agents, each specialised in a specific kind of auction. The expert agents communicate their knowledge to the manager agent in the form of probability functions, capturing the likelihood that a bid of a given price may win an auction. Given a set of such functions, the manager agent builds a bidding plan that it executes in concert with the expert agents.", "num_citations": "10\n", "authors": ["21"]}
{"title": "Pointwise temporal object database browsing\n", "abstract": " Visual object database browsers are essentially based on two kinds of interactions: navigation within a collection of objects, and navi- gation between objects by the way of their relationships. These two inter- actional operators have proven to adequately support the main user task addressed by these tools, that is, exploring the states of a set of related objects. In temporal object databases, visual browsing tools should addi- tionally support users tasks such as examining a snapshot of a collection of objects at a given instant, or detecting changes within object states. In this paper, we show that the two interactional operators supported by classical object browsers do not adequately address these tasks. We consequently propose an interactional operator dedicated to navigation through time, and we study how it may be orthogonally integrated with the above two.", "num_citations": "10\n", "authors": ["21"]}
{"title": "Fire now, fire later: alarm-based systems for prescriptive process monitoring\n", "abstract": " Predictive process monitoring is a family of techniques to analyze events produced during the execution of a business process in order to predict the future state or the final outcome of running process instances. Existing techniques in this field are able to predict, at each step of a process instance, the likelihood that it will lead to an undesired outcome.These techniques, however, focus on generating predictions and do not prescribe when and how process workers should intervene to decrease the cost of undesired outcomes. This paper proposes a framework for prescriptive process monitoring, which extends predictive monitoring with the ability to generate alarms that trigger interventions to prevent an undesired outcome or mitigate its effect. The framework incorporates a parameterized cost model to assess the cost-benefit trade-off of generating alarms. We show how to optimize the generation of alarms given an event log of past process executions and a set of cost model parameters. The proposed approaches are empirically evaluated using a range of real-life event logs. The experimental results show that the net cost of undesired outcomes can be minimized by changing the threshold for generating alarms, as the process instance progresses. Moreover, introducing delays for triggering alarms, instead of triggering them as soon as the probability of an undesired outcome exceeds a threshold, leads to lower net costs.", "num_citations": "9\n", "authors": ["21"]}
{"title": "A ProM Operational Support Provider for Predictive Monitoring of Business Processes.\n", "abstract": " Predictive process monitoring is concerned with exploiting event logs to predict how running (uncompleted) cases will unfold up to their completion. In this paper, we propose an implementation in the ProM toolset of a predictive process monitoring framework for estimating the probability that an ongoing case will lead to a certain outcome among a set of possible outcomes. An outcome refers to a label associated to completed cases, like, for example, a label indicating that a given case completed \u0393\u00c7\u00a3on time\u0393\u00c7\u00a5(with respect to a given desired duration) or \u0393\u00c7\u00a3late\u0393\u00c7\u00a5, or a label indicating that a given case led to a customer complaint or not. The framework takes into account both the sequence of events observed in the current trace, as well as data attributes associated to these events. The prediction problem is approached in two phases. First, prefixes of previous traces are clustered according to control flow information. Secondly, a classifier is built for each cluster to discriminate among a set of possible outcomes. At runtime, a prediction is made on a running case by mapping it to a cluster and applying the corresponding classifier.", "num_citations": "9\n", "authors": ["21"]}
{"title": "Processing search queries using a data structure\n", "abstract": " According to an embodiment, there is provided a method of generating a data structure stored in computer memory for processing a search query in a network of interconnected nodes, wherein the method comprises selecting landmark nodes by the following steps and storing the selected landmark nodes in the data structure: sampling from the network nodes a first sample of vertex pairs, computing the shortest path for each vertex pair, each shortest path comprising a set of vertices between each vertex in the vertex pair; identifying a first landmark node which occurs in more of the shortest paths more often than any other vertex; removing from the network vertices shortest paths including the first landmark node and identifying a second landmark node which occurs in more of the remaining shortest paths than any other remaining vertex.", "num_citations": "9\n", "authors": ["21"]}
{"title": "Discovering unbounded synchronization conditions in artifact-centric process models\n", "abstract": " Automated process discovery methods aim at extracting business process models from execution logs of information systems. Existing methods in this space are designed to discover synchronization conditions over a set of events that is fixed in number, such as for example discovering that a task should wait for two other tasks to complete. However, they fail to discover synchronization conditions over a variable-sized set of events such as for example that a purchasing decision is made only if at least three out of an a priori undetermined set of quotes have been received. Such synchronization conditions arise in particular in the context of artifact-centric processes, which consist of collections of interacting artifacts, each with its own life cycle. In such processes, an artifact may reach a state in its life cycle where it has to wait for a variable-sized set of artifacts to reach certain states before proceeding. In this\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "9\n", "authors": ["21"]}
{"title": "Specification and execution of composite trading activities\n", "abstract": " With the growing number of trading opportunities available online, software tools designed to act on behalf of human traders are increasingly being used to automate trading activities. The next logical step in this evolution is the automation of composite trading activities designed to fulfill complex user goals and requirements. In this paper, we describe a model for specifying composite trading activities involving concurrent and interrelated negotiations with multiple parties and heterogeneous protocols. The model supports the specification of several types of constraints, such as the number of required successful negotiations, the limit price for the items to be traded, and the temporal constraints imposed by all trading parties. In order to guide the execution of the trading activities, we describe a novel planning and execution model for composite trading activities which is designed to maximize the expected\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "9\n", "authors": ["21"]}
{"title": "Service-oriented processes: an introduction to BPEL\n", "abstract": " The Business Process Execution Language for Web Services (BPEL) is an emerging standard for specifying the behaviour of Web services at different levels of details using business process modeling constructs. It represents a convergence between Web services and business process technology. This chapter introduces the main concepts and constructs of BPEL and illustrates them by means of a comprehensive example. In addition, the chapter reviews some perceived limitations of BPEL and discusses proposals to address these limitations. The chapter also considers the possibility of applying formal methods and Semantic Web technology to support the rigorous development of service-oriented processes using BPEL.", "num_citations": "9\n", "authors": ["21"]}
{"title": "Programming and compiling web services in GPSL\n", "abstract": " Implementing web services that participate in long-running, multi-lateral conversations is diffcult because traditional programming languages are poor at manipulating XML data and handling concurrent and interrelated interactions. We have designed a programming language to deliberately address these problems. In this paper we describe how to use this language to consume a popular web service, and discuss the compiler, including the kinds of semantic checks it performs, and the runtime environment.", "num_citations": "9\n", "authors": ["21"]}
{"title": "Applying temporal databases to geographical data analysis\n", "abstract": " This paper reports an experience in which a temporal database was used to analyze the results of a survey on human behaviors and displacements in a ski resort. This survey was part of a broader study about the use of the resort's infrastructure, based on the time-geography methodology. As such, the presented experience may be seen as an attempt to implement some concepts of the time-geography using temporal database technology. Throughout the paper some shortcomings of current temporal data models regarding human displacements analysis are pointed out, and possible solutions are briefly sketched.", "num_citations": "9\n", "authors": ["21"]}
{"title": "Measuring fitness and precision of automatically discovered process models: A principled and scalable approach\n", "abstract": " Automated process discovery techniques allow us to generate a process model from an event log. The quality of automatically discovered process models can be assessed with respect to several criteria, including fitness, which captures the degree to which the process model is able to recognize the traces in the event log, and precision, which captures the extent to which the behavior allowed by the process model is observed in the event log. Many fitness and precision measures have been proposed in the literature. However, recent studies have shown that none of the existing measures fulfil a set of intuitive properties. In addition, existing fitness and precision measures suffer from scalability issues when applied to models discovered from real-life event logs. This article presents a family of fitness and precision measures based on the idea of comparing the k-th order Markovian abstractions of a process model\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "8\n", "authors": ["21"]}
{"title": "Metaheuristic optimization for automated business process discovery\n", "abstract": " The problem of automated discovery of process models from event logs has been intensely investigated in the past two decades, leading to a range of approaches that strike various trade-offs between accuracy, model complexity, and execution time. A few studies have suggested that the accuracy of automated process discovery approaches can be enhanced by using metaheuristic optimization. However, these studies have remained at the level of proposals without validation on real-life logs or they have only considered one metaheuristics in isolation. In this setting, this paper studies the following question: To what extent can the accuracy of automated process discovery approaches be improved by applying different optimization metaheuristics? To address this question, the paper proposes an approach to enhance automated process discovery approaches with metaheuristic optimization. The\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "8\n", "authors": ["21"]}
{"title": "Stage-based discovery of business process models from event logs\n", "abstract": " An automated process discovery technique generates a process model from an event log recording the execution of a business process. For it to be useful, the generated process model should be as simple as possible, while accurately capturing the behavior recorded in, and implied by, the event log. Most existing automated process discovery techniques generate flat process models. When confronted to large event logs, these approaches lead to overly complex or inaccurate process models. An alternative is to apply a divide-and-conquer approach by decomposing the process into stages and discovering one model per stage. It turns out, however, that existing divide-and-conquer process discovery approaches often produce less accurate models than flat discovery techniques, when applied to real-life event logs. This article proposes an automated method to identify business process stages from an event log\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "8\n", "authors": ["21"]}
{"title": "Business Process Privacy Analysis in Pleak\n", "abstract": " Pleak is a tool to capture and analyze privacy-enhanced business process models to characterize and quantify to what extent the outputs of a process leak information about its inputs. Pleak incorporates an extensible set of analysis plugins, which enable users to inspect potential leakages at multiple levels of detail.", "num_citations": "8\n", "authors": ["21"]}
{"title": "Process discovery\n", "abstract": " Various methods can be used to create a process model from information inferred within the organization, e.g., by interviewing process participants or by observing how these operate in practice. Meanwhile, it is important to ensure that a model is not only syntactically correct, but that it also accurately reflects the actual business process being modeled. In this chapter, we first present the challenges faced by the stakeholders involved in the lead-up to a process model. Then, we discuss methods to facilitate effective communication and information gathering about business processes. We then show step-by-step how to construct a process model based on the gathered information, and what quality criteria should be checked before the model can be accepted as an authoritative representation of a business process.", "num_citations": "8\n", "authors": ["21"]}
{"title": "Processing search queries using a data structure\n", "abstract": " The disclosure relates to of generating a data structure stored in a computer memory for use in performing a search query to determine a separation between nodes in a network of interconnected nodes, wherein the method comprises: selecting a set of landmark nodes from the network; and for at least two of the landmark nodes in the set; generating a path tree for each landmark node that indicates a separation between the landmark node and each of a plurality of nodes; wherein the generating is configured to limit the number of path trees each of said plurality of nodes may appear in to no more than a predetermined number of path trees. A method of processing a data structure is also disclosed.", "num_citations": "8\n", "authors": ["21"]}
{"title": "Bp-diff: a tool for behavioral comparison of business process models\n", "abstract": " BP-Diff is a tool for identifying and diagnosing behavioral differences between pairs of business process models. BP-Diff identifies behavioral discrepancies involving pairs of tasks and provides both verbal and visual feedback to help users understand each discrepancy. The verbal feedback explains how a given pair of tasks is related in one model in contrast to the other model. Meanwhile, the visual feedback allows users to pinpoint the exact state where the discrepancy occurs. Unlike existing techniques, BP-Diff abstracts away from syntactical differences, focusing instead on behavior.", "num_citations": "8\n", "authors": ["21"]}
{"title": "Processing search queries in a network of interconnected nodes\n", "abstract": " A search query to provide a search result may be received, which identifies source and target nodes and an application for generating the search result. The application accesses a data structure holding landmark nodes, which store a shortest path tree in the form of a set of parent links. Each parent link can identify an adjacent vertex node in a shortest path between each node in the data structure and the landmark node. The location of the source node and the target node in the shortest path trees may be identified to the landmark node. For each landmark node, using the identified locations of the target node and source node, a measure of distance between the source node and the target may be generated. The landmark node with the shortest distance may be determined. A search result related to the shortest path tree of that landmark node may be provided.", "num_citations": "8\n", "authors": ["21"]}
{"title": "Event structures as a foundation for process model differencing, part 1: Acyclic processes\n", "abstract": " This paper considers the problem of comparing process models in terms of their behavior. Given two process models, the problem addressed is that of explaining their differences in terms of simple and intuitive statements. This model differencing operation is needed for example in the context of process consolidation, where analysts need to reconcile differences between process variants in order to produce consolidated process models. The paper presents an approach to acyclic process model differencing based on event structures. First the paper considers the use of prime event structures. It is found that the high level of node duplication inherent to prime event structures hinders on the usefulness of the difference diagnostics that can be extracted thereon. Accordingly, the paper defines a method for producing (asymmetric) event structures with reduced duplication.", "num_citations": "8\n", "authors": ["21"]}
{"title": "Towards a formalization of contracts for service substitution\n", "abstract": " In a service-oriented system, service substitution can be used to repair faults, improve performance, and/or enhance resilience via graceful degradation. Correct service substitution must preserve the essential properties of the system, particularly the proper termination and/or liveness. However, ensuring correct service substitution poses some challenges. The main obstacle is that, in the general case, the behavior of services is opaque to service designers due to the autonomy of services. While the bulk of prior researches on service substitution assume that services advertise their interaction protocols (e.g., in the form of BPEL), existing services (i.e., in the form of WSDL) rarely do so. In this paper, we study the problem of service substitution under the assumption that services do not expose their interaction protocols. To this end, we characterize the behavior of interacting services by means of contracts consisting\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "8\n", "authors": ["21"]}
{"title": "Simulation-based evaluation of workflow escalation strategies\n", "abstract": " Workflows in the service industry sometimes need to deal with multi-fold increases in customer demand within a short period of time. Such spikes in service demand may be caused by a variety of events including promotional deals, launching of new products, major news or natural disasters. Escalation strategies can be incorporated into the design of a workflow so that it can cope with sudden spikes in the number of service requests while providing acceptable execution times. In this paper, we propose a method for evaluating escalation strategies using simulation technology. The effectiveness of the proposed method is demonstrated on a workflow from an insurance company.", "num_citations": "8\n", "authors": ["21"]}
{"title": "Predictive process monitoring in apromore\n", "abstract": " This paper discusses the integration of Nirdizati, a tool for predictive process monitoring, into the Web-based process analytics platform Apromore. Through this integration, Apromore\u0393\u00c7\u00d6s users can use event logs stored in the Apromore repository to train a range of predictive models, and later use the trained models to predict various performance indicators of running process cases from a live event stream. For example, one can predict the remaining time or the next events until case completion, the case outcome, or the violation of compliance rules or internal policies. The predictions can be presented graphically via a dashboard that offers multiple visualization options, including a range of summary statistics about ongoing and past process cases. They can also be exported into a text file for periodic reporting or to be visualized in third-parties business intelligence tools. Based on these predictions, operations\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "7\n", "authors": ["21"]}
{"title": "Process identification\n", "abstract": " Process identification refers to those management activities that aim to systematically define the set of business processes of an organization and establish clear criteria for selecting specific processes for improvement. The output is a process architecture, which represents the processes and their interrelations. This artifact serves as a framework for defining the priorities and the scope of process modeling and redesign projects. In this chapter, we start by discussing the context of process identification. Then, we present a method for process identification that is based on two steps: process architecture definition and process selection.", "num_citations": "7\n", "authors": ["21"]}
{"title": "Homophilic network decomposition: a community-centric analysis of online social services\n", "abstract": " In this paper we formulate the homophilic network decomposition problem: Is it possible to identify a network partition whose structure is able to characterize the degree of homophily of its nodes? The aim of our work is to understand the relations between the homophily of individuals and the topological features expressed by specific network substructures. We apply several community detection algorithms on three large-scale online social networks\u0393\u00c7\u00f6Skype, LastFM and Google+\u0393\u00c7\u00f6and advocate the need of identifying the right algorithm for each specific network in order to extract a homophilic network decomposition. Our results show clear relations between the topological features of communities and the degree of homophily of their nodes in three online social scenarios: product engagement in the Skype network, number of listened songs on LastFM and homogeneous level of education among users of\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "7\n", "authors": ["21"]}
{"title": "Community-based prediction of activity change in Skype\n", "abstract": " A key problem for facilitators of online communication and social networks is to identify users whose activity is likely to change in the near future. Such predictions may serve as basis for targeted campaigns aimed at sustaining or increasing overall user engagement in the network. A common approach to this problem is to apply machine learning methods to make predictions at the level of individuals. These approaches consider only information about each individual user and, thus, do not exploit the social connections and structure of the network. In this paper, we approach the problem of activity change prediction at the level of communities rather than individuals. We develop predictive models of activity change over communities obtained using state-of-art community detection methods and compare their predictive power with each other and against the single-user baseline and ego networks. The results show\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "7\n", "authors": ["21"]}
{"title": "On the suitability of generalized behavioral profiles for process model comparison\n", "abstract": " Given two process models, the problem of behavioral comparison is that of determining if these models are behaviorally equivalent (e.g., by trace equivalence) and, if not, identifying how can the differences be presented in a compact manner? Behavioral profiles have been proposed as a convenient abstraction for this problem. A behavioral profile is a matrix, where each cell encodes a behavioral relation between a pair of tasks (e.g., causality or conflict). Thus, the problem of behavioral comparison can be reduced to matrix comparison. It has been observed that while behavioral profiles can be efficiently computed, they are not accurate insofar as behaviorally different process models may map to the same behavioral profile. This paper investigates the question of how accurate existing behavioral profiles are. The paper shows that behavioral profiles are fully behavior preserving for the class of acyclic\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "7\n", "authors": ["21"]}
{"title": "Essential process modeling\n", "abstract": " Business process models are important at various stages of the BPM lifecycle. Before starting to model a process, it is crucial to understand why we are modeling it. The models we produce will look quite differently depending on the reason for modeling them in the first place. There are many reasons for modeling a process. The first one is simply to understand the process and to share our understanding of the process with the people who are involved with the process on a daily basis. Indeed, process participants typically perform quite specialized activities in a process such that they are hardly confronted with the complexity of the whole process. Therefore, process modeling helps to better understand the process and to identify and prevent issues. This step towards a thorough understanding is the prerequisite to conduct process analysis, redesign or automation.           In this chapter we will become familiar\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "7\n", "authors": ["21"]}
{"title": "Beyond control-flow: extending business process configuration to resources and objects\n", "abstract": " A configurable process model is an integrated representation of multiple variants of a business process. It is designed to be individualized to meet a particular set of requirements. As such, configurable process models promote systematic reuse of proven or common practices. Existing notations for configurable process modeling focus on capturing tasks and control-flow dependencies, neglecting equally important aspects of business processes such as data flow, material flow and resource management. This paper fills this gap by proposing an integrated meta-model for configurable processes with advanced features for capturing resources involved in the performance of tasks (through task-role associations) as well as flow of data and physical artifacts (through task-object associations). Although embodied as an extension of a popular process modeling notation, namely EPC, the meta-model is defined in an abstract and formal manner to make it applicable to other notations.", "num_citations": "7\n", "authors": ["21"]}
{"title": "A sequence-based object-oriented model for video databases\n", "abstract": " Structuration, annotation and composition are amidst the most crucial modeling issues that video editing and querying in the context of a database entail. In this paper, we propose a sequence-based, object-oriented data model that addresses them in an unified, yet orthogonal way. Thanks to this orthogonality, the interactions between these three aspects are properly captured, i.e., annotations may be attached to any level of video structuration, and all the composition operators preserve the structurations and annotations of the argument videos. We also propose to query both the structuration and the annotations of videos using an extension of ODMG's OQL which integrates a set of algebraic operators on sequences. The overall proposal is formalized and implemented on top of an object-oriented DBMS.", "num_citations": "7\n", "authors": ["21"]}
{"title": "Shareprom: A Tool for Privacy-Preserving Inter-Organizational Process Mining.\n", "abstract": " Process mining is a set of techniques to analyze business processes based on event logs extracted from information systems. Existing process mining techniques are designed for intra-organizational settings, as they assume that the entire event log of a process is available for analysis at once. In an intra-organizational process, each party only has access to its own private event log, which gives only a partial picture of the whole process. Moreover, the involved parties may be unwilling or unable to share their private logs due to confidentiality or privacy imperatives. In this context, this paper presents a tool, namely Shareprom, that enables independent parties to execute process mining operations without revealing any data other than the output of the analysis. Specifically, Shareprom uses secure multi-party computation techniques to compute the frequency or time-annotated Directly-Follows Graph (DFG) of event logs held by multiple parties, without these parties having to share any information other than the resulting DFG. The tool applies a differentially private release mechanism before revealing the output DFG in order to provide an additional layer of privacy protection.", "num_citations": "6\n", "authors": ["21"]}
{"title": "Adaptations of data mining methodologies: a systematic literature review\n", "abstract": " The use of end-to-end data mining methodologies such as CRISP-DM, KDD process, and SEMMA has grown substantially over the past decade. However, little is known as to how these methodologies are used in practice. In particular, the question of whether data mining methodologies are used \u0393\u00c7\u00ffas-is\u0393\u00c7\u00d6 or adapted for specific purposes, has not been thoroughly investigated. This article addresses this gap via a systematic literature review focused on the context in which data mining methodologies are used and the adaptations they undergo. The literature review covers 207 peer-reviewed and \u0393\u00c7\u00ffgrey\u0393\u00c7\u00d6 publications. We find that data mining methodologies are primarily applied \u0393\u00c7\u00ffas-is\u0393\u00c7\u00d6. At the same time, we also identify various adaptations of data mining methodologies and we note that their number is growing rapidly. The dominant adaptations pattern is related to methodology adjustments at a granular level (modifications) followed by extensions of existing methodologies with additional elements. Further, we identify two recurrent purposes for adaptation: (1) adaptations to handle Big Data technologies, tools and environments (technological adaptations); and (2) adaptations for context-awareness and for integrating data mining solutions into business processes and IT systems (organizational adaptations). The study suggests that standard data mining methodologies do not pay sufficient attention to deployment issues, which play a prominent role when turning data mining models into software products that are integrated into the IT architectures and business processes of organizations. We conclude that refinements of existing methodologies aimed at\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "6\n", "authors": ["21"]}
{"title": "Qualitative Process Analysis\n", "abstract": " Analyzing business processes is both an art and a science. In this respect, qualitative analysis is the artistic side of process analysis. Qualitative process analysis techniques allow us to identify, classify, and understand weaknesses and improvement opportunities in a process. In this chapter, we introduce a selected set of principles and techniques for qualitative process analysis. First, we present two techniques to identify unnecessary steps of the process (value-added analysis) and sources of waste (waste analysis). Next, we present techniques to identify and document issues in a process from multiple perspectives and to analyze the root causes of these issues.", "num_citations": "6\n", "authors": ["21"]}
{"title": "Business process graphs: similarity search and matching\n", "abstract": " Organizations create collections of hundreds or even thousands of business process models to describe their operations. This chapter explains how graphs can be used as underlying formalism to develop techniques for managing such collections. To this end it defines the business process graph formalism. On this formalism it defines techniques for determining similarity of business process graphs. Such techniques can be used to quickly search through a collection of business process graphs to find the graph that is most relevant to a given query. These techniques can be used by tool builders that develop tools for managing large collections of business process models. The aim of the chapter is to provide an overview of the research area of using graphs to do similarity search and matching of business processes.", "num_citations": "6\n", "authors": ["21"]}
{"title": "Consolidated management of business process variants\n", "abstract": " In business processes within large organizations, one will often find variations stemming from segmentation along customer types, product lines, business units or geographical regions. For example, a business process for handling claims in an insurance company will vary depending on whether the claim relates to a car accident, a property damage or a personal incident. Also, in an insurance company that operates in several jurisdictions or countries, one is likely to observe variations in the way insurance claims are handled across these political boundaries. Similarly, in company mergers, the merged organization often ends up with multiple models describing \u0393\u00c7\u00a3equivalent\u0393\u00c7\u00a5 processes previously executed separately in each organization prior to their merger.", "num_citations": "6\n", "authors": ["21"]}
{"title": "Improving Web service survivability via gracefully degraded substitution\n", "abstract": " The ability to substitute a service for another is one of the features of Service-Oriented Computing (SOC). In this paper, we study the problem of degraded service substitution assuming that services have explicitly-defined interaction protocols, e.g., in the form of WS-BPEL business protocols. To this end, we characterize the behavior of interacting services by means of contracts specifying the allowed sequence of message sending and receiving operations. The contribution of this paper is a theory of contracts for service substitution which improve system survivability by gracefully degraded substitution.", "num_citations": "6\n", "authors": ["21"]}
{"title": "Vers un mod\u251c\u00bfle de composition de services web avec propri\u251c\u2310t\u251c\u2310s transactionnelles.\n", "abstract": " Le d\u251c\u2310veloppement et l\u0393\u00c7\u00d6adoption de la technologie associ\u251c\u2310e aux services web permettent d\u0393\u00c7\u00d6implanter de nouvelles applications avec valeur ajout\u251c\u2310e en composant des services existants. Certains des services web, qui pourraient intervenir dans de telles compositions, ont des propri\u251c\u2310t\u251c\u2310s transactionnelles inh\u251c\u2310rentes. Ceci est le cas notamment des services associ\u251c\u2310s \u251c\u00e1 la r\u251c\u2310servation de ressources, comme par exemple la r\u251c\u2310servation de chambres d\u0393\u00c7\u00d6h\u251c\u2524tel, de services professionnels, etc. Les propri\u251c\u2310t\u251c\u2310s transactionnelles de ces services peuvent \u251c\u00actre exploit\u251c\u2310es lors de leur composition, pour r\u251c\u2310pondre \u251c\u00e1 des contraintes et des pr\u251c\u2310f\u251c\u2310rences \u251c\u2310tablies par le concepteur. Cet article pr\u251c\u2310sente un mod\u251c\u00bfle de composition de services poss\u251c\u2310dant certaines propri\u251c\u2310t\u251c\u2310s transactionnelles, plus sp\u251c\u2310cifiquement des fonctionnalit\u251c\u2310s de tentative de r\u251c\u2310servation et d\u0393\u00c7\u00d6ex\u251c\u2310cution atomique. Le mod\u251c\u00bfle est bas\u251c\u2310 sur un op\u251c\u2310rateur de haut niveau qui permet aux concepteurs de composer des services cens\u251c\u2310s s\u0393\u00c7\u00d6 ex\u251c\u2310cuter en parall\u251c\u00bfle, et de prendre en compte des contraintes li\u251c\u2310es \u251c\u00e1 l\u0393\u00c7\u00d6atomicit\u251c\u2310 au travers de param\u251c\u00bftres et de fonctions de restriction et de pr\u251c\u2310f\u251c\u2310rence.ABSTRACT. The development of new services by composition of existing ones has gained considerable momentum as a means of integrating heterogeneous applications and realising business collaborations. Services that enter into compositions with other services may have transactional properties, especially those in the broad area of resource management (eg booking", "num_citations": "6\n", "authors": ["21"]}
{"title": "TEMPOS: managing time and histories on top of OO DBMS\n", "abstract": " TEMPOS (Temporal Extension Model for Persistent Object Servers)[1, 2] is a multigranular temporal model integrating the main functionalities required to manage the data historical dimensions on top of a DBMS. It is composed of a time model, which defines a hierarchy of simple and complex datatypes for modelling temporal values, and a historical model, which allows to update and query timestamped object properties. The main originalities of this model are:", "num_citations": "6\n", "authors": ["21"]}
{"title": "Scalable alignment of process models and event logs: An approach based on automata and S-components\n", "abstract": " Given a model of the expected behavior of a business process and given an event log recording its observed behavior, the problem of business process conformance checking is that of identifying and describing the differences between the process model and the event log. A desirable feature of a conformance checking technique is that it should identify a minimal yet complete set of differences. Existing conformance checking techniques that fulfill this property exhibit limited scalability when confronted to large and complex process models and event logs. One reason for this limitation is that existing techniques compare each execution trace in the log against the process model separately, without reusing computations made for one trace when processing subsequent traces. Yet, the execution traces of a business process typically share common fragments (e.g. prefixes and suffixes). A second reason is that these\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "5\n", "authors": ["21"]}
{"title": "Identifying candidate routines for robotic process automation from unsegmented UI logs\n", "abstract": " Robotic Process Automation (RPA) is a technology to develop software bots that automate repetitive sequences of interactions between users and software applications (a.k. a. routines). To take full advantage of this technology, organizations need to identify and to scope their routines. This is a challenging endeavor in large organizations, as routines are usually not concentrated in a handful of processes, but rather scattered across the process landscape. Accordingly, the identification of routines from User Interaction (UI) logs has received significant attention. Existing approaches to this problem assume that the UI log is segmented, meaning that it consists of traces of a task that is presupposed to contain one or more routines. However, a UI log usually takes the form of a single unsegmented sequence of events. This paper presents an approach to discover candidate routines from unsegmented UI logs in the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "5\n", "authors": ["21"]}
{"title": "Local concurrency detection in business process event logs\n", "abstract": " Process mining techniques aim at analyzing records generated during the execution of a business process in order to provide insights on the actual performance of the process. Detecting concurrency relations between events is a fundamental primitive underpinning a range of process mining techniques. Existing approaches to this problem identify concurrency relations at the level of event types under a global interpretation. If two event types are declared to be concurrent, every occurrence of one event type is deemed to be concurrent to one occurrence of the other. In practice, this interpretation is too coarse-grained and leads to over-generalization. This article proposes a finer-grained approach, whereby two event types may be deemed to be in a concurrency relation relative to one state of the process, but not relative to other states. In other words, the detected concurrency relation holds locally, relative to a set of\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "5\n", "authors": ["21"]}
{"title": "Disclosure Analysis of SQL Workflows.\n", "abstract": " In the context of business process management, the implementation of data minimization requirements requires that analysts are able to assert what private data each worker is able to access, not only directly via the inputs of the tasks they perform in a business process, but also indirectly via the chain of tasks that lead to the production of these inputs. In this setting, this paper presents a technique which, given a workflow that transforms a set of input tables into a set of output tables via a set of inter-related SQL statements, determines what information from each input table is disclosed by each output table, and under what conditions this disclosure occurs. The result of this disclosure analysis is a summary representation of the possible computations leading from the inputs of the workflow to a given output thereof.", "num_citations": "5\n", "authors": ["21"]}
{"title": "Building payment classification models from rules and crowdsourced labels: a case study\n", "abstract": " The ability to classify customer-to-business payments enables retail financial institutions to better understand their customers\u0393\u00c7\u00d6 expenditure patterns and to customize their offerings accordingly. However, payment classification is a difficult problem because of the large and evolving set of businesses and the fact that each business may offer multiple types of products, e.g. a business may sell both food and electronics. Two major approaches to payment classification are rule-based classification and machine learning-based classification on transactions labeled by the customers themselves (a form of crowdsourcing). The rules-based approach is not scalable as it requires rules to be maintained for every business and type of transaction. The crowdsourcing approach leads to inconsistencies and is difficult to bootstrap since it requires a large number of customers to manually label their transactions for an\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "5\n", "authors": ["21"]}
{"title": "Process redesign\n", "abstract": " The thorough analysis of a business process may lead to the identification of a range of issues. For example, bottlenecks slow down the process or the cost of process execution is too high. These issues spark various directions for redesign. The problem is, however, that redesign is often approached as an ad hoc activity. The downside to this is that interesting redesign opportunities may be overlooked. For this reason, it is important to become aware of redesign methods, which can be used to systematically generate redesign options. This chapter deals with the methods that help to rethink and re-organize business processes to make them perform better. We first clarify the motivation for redesign and delve deeper into what improving process performance actually means. Then, we present the spectrum of redesign methods and discuss representative sample methods in some detail. More specifically, we\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "5\n", "authors": ["21"]}
{"title": "Incremental and interactive business process model repair in apromore\n", "abstract": " This paper presents the integration of a plugin for interactive and incremental business process repair into the Apromore advanced business process analytics platform. Given a model and an event log representing the behavior of a process as input, the plugin describes encountered behavioral differences between them as natural language statements, as well as graphical representations displayed over the model. The graphical representation of the differences provides visual guidance on how to repair the model so that it better reflects the observed behavior. Subsequently, the users have the option to select the discrepancies that need to be resolved and how to resolve them. The users can choose whether to follow the suggestion provided by the tool or to repair the discrepancies differently. The differences are updated with every repair applied over the model, resulting in an iterative procedure where the user has the ultimate control over the extent and the implementation of the changes over the model.", "num_citations": "5\n", "authors": ["21"]}
{"title": "Factors Affecting the Sustained Use of Process Models\n", "abstract": " The documentation of business processes via modelling notations is a well-accepted and widespread practice. While a given process model is created in a specific project and sometimes for a specific purpose, it is generally preserved so that it can be used subsequently, beyond the context where it was created. In this setting, the aim of the paper at hand is to uncover factors that affect the sustained use of process models in an organization. First, the paper outlines an a priori model of sustained process model use derived from existing factor models of business process modelling success and knowledge reuse. This a priori model is packaged as an assessment instrument and applied to four organizations from different domains. Based on these case studies, we identify a subset of factors and relationships that explain differences in the observed sustained use of process models across the organizations in\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "5\n", "authors": ["21"]}
{"title": "Redundancy detection in service-oriented systems\n", "abstract": " This paper addresses the problem of identifying redundant data in large-scale service-oriented information systems. Specifically, the paper puts forward an automated method to pinpoint potentially redundant data attributes from a given collection of semantically-annotated Web service interfaces. The key idea is to construct a service network to represent all input and output dependencies between data attributes and operations captured in the service interfaces, and to apply centrality measures from network theory in order to quantify the degree to which an attribute belongs to a given subsystem. The proposed method was tested on a federated governmental information system consisting of 58 independently-maintained information systems providing altogether about 1000 service operations described in WSDL. The accuracy of the method is evaluated in terms of precision and recall.", "num_citations": "5\n", "authors": ["21"]}
{"title": "Designing maintainable xml transformations\n", "abstract": " Modern applications often rely on XML to represent data internally and to interact with other applications and with end users. XSL transformations are commonly employed to transform between the internal representations of XML documents manipulated by an application and representations used for interaction with end-users and with other applications. These XSL transformations need to be updated whenever the underlying XML formats evolve. To address this maintenance problem, we formulate a number of guidelines for designing XSL transformations that are resilient to changes in the schema of the input XML documents. These guidelines are evaluated experimentally on the basis of three case studies. The evaluation shows that the use of these guidelines leads to more concise XSL transformations and to significant reductions in the amount of changes required to adapt existing XSL transformations in\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "5\n", "authors": ["21"]}
{"title": "Modelling business process variability\n", "abstract": " A reference process model represents multiple variants of a common business process in an inte-grated and reusable manner. It is intended to be individualized in order to fit the requirements of a specific organization or project. This practice of individualizing reference process models provides an attractive alternative with respect to designing process models from scratch. In particular, it en-ables the reuse of proven practices. This chapter introduces techniques for representing variability in the context of reference process models, as well as techniques that facilitate the individualization of reference process model with respect to a given set of requirements.", "num_citations": "5\n", "authors": ["21"]}
{"title": "Reconciling object-oriented and process-oriented approaches to information systems engineering\n", "abstract": " Object-oriented modelling is an established approach to document the structure and behaviour of information systems. In an object model, a system is captured in terms of object types and associations, state machines, collaboration diagrams, etc. Process modeling on the other hand, provides a different approach whereby behaviour is captured in terms of tasks, flow dependencies, resources, etc. These two approaches have their relative strengths and weaknesses. In object models, behaviour is split across object types, whereas in process models, behaviour is seen holistically along chains of logically related tasks. Also, object models and process models lend themselves to different styles of implementation. There is an opportunity to leverage the relative advantages of object models and process models by creating integrated meta-models and transformations, so that modellers can switch between these views. In this paper we define a transformation from a meta-model for object behavior modeling to a meta-model for process modeling. The transformation relies on the identification of elementary causal relations in the object model. These relations are encoded in a heuristics net from which a process model is derived.", "num_citations": "5\n", "authors": ["21"]}
{"title": "Scaling dynamic web content provision using elapsed-time-based content degradation\n", "abstract": " Dynamic Web content is increasing in popularity and, by its nature, is harder to scale than static content. As a result, dynamic Web content delivery degrades more rapidly than static content under similar client request rates. Many techniques have been explored for effectively handling heavy Web request traffic. In this paper, we concentrate on dynamic content degradation, believing that it offers a good balance between minimising total cost of ownership and maximising scalability.               We describe an algorithm for dynamic content degradation that is easily implemented on top of existing mainstream Web application architectures. The algorithm is based on measuring the elapsed time of content generation. We demonstrate the algorithm\u0393\u00c7\u00d6s adaptability against two traffic request patterns, and explore behavioural changes when varying the algorithm\u0393\u00c7\u00d6s key parameters. We find our elapsed time based\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "5\n", "authors": ["21"]}
{"title": "Property propagation rules for prioritizing and synchronizing trading activities\n", "abstract": " With the growing number of marketplaces and trading partners in the e-commerce (electronic commerce) environment, software tools designed to act on behalf of human traders are increasingly used to automate trading activities. We describe a model for constructing trading engines, which are capable of concurrently participating in multiple interrelated negotiations with heterogeneous protocols. These tree-structured engines are configured by means of a single generic synchronization construct, which enables the incremental composition of complex trading schemes, including a number of well-known strategies from the financial trading domain. The construct is augmented by a priority-based scheduling algorithm, which selects a set of nodes for negotiation based on their estimated profit, the time remaining and the desired degree of concurrency. The model also provides iterative negotiation, which is essential in\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "5\n", "authors": ["21"]}
{"title": "From conceptual models to constrained web forms\n", "abstract": " Constraints expressed within document definitions, data models, ontologies, and domain models, are a key constituent of the Semantic Web. Indeed, Semantic Web applications will rely on these constraints when reasoning about Web contents. This paper deals with the enforcement by Web interfaces, of constraints expressed at the conceptual level. Specifically, the paper presents an approach to mark up a conceptual model in order to generate a Web form capable of enforcing the constraints captured in the model. The feasibility and practicality of this approach have been validated through the development of a system that generates XForms code from ORM conceptual schemas.", "num_citations": "5\n", "authors": ["21"]}
{"title": "Discovering business process simulation models in the presence of multitasking\n", "abstract": " Business process simulation is a versatile technique for analyzing business processes from a quantitative perspective. A well-known limitation of process simulation is that the accuracy of the simulation results is limited by the faithfulness of the process model and simulation parameters given as input to the simulator. To tackle this limitation, several authors have proposed to discover simulation models from process execution logs so that the resulting simulation models more closely match reality. Existing techniques in this field assume that each resource in the process performs one task at a time. In reality, however, resources may engage in multitasking behavior. Traditional simulation approaches do not handle multitasking. Instead, they rely on a resource allocation approach wherein a task instance is only assigned to a resource when the resource is free. This inability to handle multitasking leads to an\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "4\n", "authors": ["21"]}
{"title": "Subscriptions for routing incoming messages to process instances in a process execution engine\n", "abstract": " An orchestration engine may execute multiple, concurrently running instances of a process model, where each of the instances of the process model includes one or more receiving objects. A messaging endpoint may be shared by the instances of the process model and may be configured to receive messages. A subscription handler may handle multiple subscriptions for the instances and may be configured to define a first subscription for a first instance of the multiple instances, where the first subscription includes a correlation expression, an association with the messaging endpoint, and an association with a first receiving object within the first instance. The subscription handler also may associate the first subscription with the first instance and initialize the first subscription associated with the first instance. A routing manager may route received messages that match the first subscription to the first receiving object.", "num_citations": "4\n", "authors": ["21"]}
{"title": "Tell me what\u0393\u00c7\u00d6s ahead? predicting remaining activity sequences of business process instances\n", "abstract": " Predictive business process monitoring is a family of techniques to determine how running instances of a business process are likely to unfold in the future. Techniques in this space differ according to their object of prediction. Some predict whether or not a running process instance will fulfill a compliance rule, others predict whether or not a given activity will occur, while others predict the remaining execution time. These and other predictive process monitoring problems are subsumed by the problem of predicting the remaining sequence of activities of a given process instance. In this paper, we tackle this latter problem using two alternative approaches. In the first one, we statically construct a transition system from an event log of completed process instances and annotate each transition with a probability calculated using a k-nearest neighbors classifier. At runtime, we map the (incomplete) trace of a process instance to a state in the transition system. To predict the remaining activity sequence of a process instance, we calculate a highest-probability path starting from the current state. In the second approach, we treat the problem of activity sequence prediction as a structured output prediction problem and apply recurrent neural networks. The accuracy of the two proposed approaches is evaluated on real-life and synthetic datasets and compared against an existing baseline technique.", "num_citations": "4\n", "authors": ["21"]}
{"title": "Enforcing policies and guidelines in web portals: a case study\n", "abstract": " Customizability is generally considered a desirable feature of web portals. However, if left uncontrolled, customizability may come at the price of lack of uniformity or lack of maintainability. Indeed, as the portal content and services evolve, they can break assumptions made in the definition of customized views. Also, uncontrolled customization may lead to certain content considered important by the web portal owners (e.g. advertisements), to not be displayed to end users. Thus, web portal customization is hindered by the need to enforce customization policies and guidelines with minimal overhead. This paper presents a case study where a combination of techniques was employed to semi-automatically enforce policies and guidelines on community-built presentation components in a web portal. The study shows that a combination of automated verification and semantics extraction techniques can reduce\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "4\n", "authors": ["21"]}
{"title": "Specification and analysis of adapters for conversational services\n", "abstract": " Over its lifetime, a Web service is likely to be reused across several development projects, such that in each of them different interfaces are required from it. Implementing, testing, deploying, and maintaining adapters to deal with this multiplicity of interfaces can be costly and error-prone. The problem is compounded in the case of services that do not follow simple request-response interactions, but instead engage in conversations comprising arbitrary patterns of message exchanges. This paper proposes a language for specifying adapters for conversational services. The language is based on six composable operators that are endowed with a formal semantics defined in terms of Petri nets. The formal semantics is used as a basis to statically check the correctness of the specified adapters. The proposal has been validated through a prototype implementation of an execution engine and a tool that converts adapter specifications into Petri nets.", "num_citations": "4\n", "authors": ["21"]}
{"title": "A model for the configurable composition and synchronization of complex trading activities\n", "abstract": " With the increasing number of market places and potential trading partners across the e-commerce environment, it will become natural for multiple trading activities to be deployed as part of single trading strategy. This paper describes a multi-process model for controlling interrelated trading activities. The model includes a powerful generic synchronization construct for building a variety of executable trading engines. The modular design of the construct enables the realization of complex trading schemes, including a number of well known strategies from the financial trading domain. A homogeneous interface is defined to allow seamless integration of control modules built using this construct with elementary trading tasks which directly interact with the trading partners using various negotiation protocols. The model also enables iterative negotiation capabilities, which are essential in any complex trading environment\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "4\n", "authors": ["21"]}
{"title": "Verification of Privacy-Enhanced Collaborations\n", "abstract": " In a distributed scenario it is possible to find systems consisting of independent parties that collaboratively execute a business process, but cannot disclose a subset of the data used in this process to each other. Such systems can be modelled using the PE-BPMN notation: a privacy-enhanced extension of the BPMN process modeling notation. Given a PE-BPMN model, we address the problem of verifying that the content of certain data objects is not leaked to unauthorized parties. To this end, we formalise the semantics of PE-BPMN collaboration diagrams via a translation into process algebraic specifications. This formalisation enables us to apply model checking to detect unintended data leakages in a PE-BPMN model. We specifically consider data leakages in the context of secret sharing technology. The approach has been implemented on top of the mCRL2 toolset, and integrated into the Pleak toolset\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "3\n", "authors": ["21"]}
{"title": "Business process execution on blockchain\n", "abstract": " Nowadays, in traditional Business Process Management Systems (BPMSs) supporting inter-organizational collaborations issues like the lack of trust and the flexibility to adapt to changes quickly are still a roadblock. Blockchain technology provides a tamper-proof mechanism for decentralized execution of collaborative business processes. In that context, a blockchain can serve not only to keep track of the exchanged information but also to coordinate, via smart contracts, the execution of steps as prescribed by a process model. Nevertheless, current solutions in this field are still in an early stage or at a high level of abstraction. This research aims at developing a BPMS on blockchain to handle inter-organizational collaborations that relies on the translation of process models captured in the Business Process Model and Notation (BPMN) into smart contracts. Unlike existing solutions, we consider the blockchain as the platform supporting the full execution of a process. This work presents the main challenges to be tackled, outlines the research approach to follow and describes some preliminary results on designing the architecture of the prototype and compiling process models into smart contracts.", "num_citations": "3\n", "authors": ["21"]}
{"title": "BPM as an Enterprise Capability\n", "abstract": " Due to the need to improve different business processes, chances are that multiple BPM projects are being conducted at the same time within the same organization. Collectively, we call the set of BPM projects within a company, including its specific management structure, a BPM program. This chapter deals with the following question: \u0393\u00c7\u00a3What does it take to successfully manage a BPM program?\u0393\u00c7\u00a5 To answer this question, we consider BPM as an enterprise capability, at the same level as other organizational management disciplines such as risk management and human performance management. After introducing the typical reasons for BPM programs to fail, we introduce transversal aspects of BPM, such as governance and strategic alignment, and discuss how these are critical to avoid the fail reasons. Next, we organize these aspects in a BPM maturity model and show how to use this model to assess the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "3\n", "authors": ["21"]}
{"title": "Quantitative Process Analysis\n", "abstract": " Qualitative analysis is a valuable tool to gain systematic insights into a process. However, the results obtained from qualitative analysis are sometimes not detailed enough to provide a solid basis for decision making. To understand the impact of issues, we need to go beyond qualitative analysis. This chapter introduces techniques for analyzing business processes quantitatively in terms of process performance measures such as cycle time, waiting time, cost, and resource utilization. The chapter focuses on three techniques: flow analysis, queueing analysis, and simulation. The chapter shows how these techniques can be used to measure the cycle time and capacity of a process, to detect critical paths and bottlenecks, and to estimate the performance impact of a given process change.", "num_citations": "3\n", "authors": ["21"]}
{"title": "Predictive business process monitoring with LSTMs\n", "abstract": " Predictive business process monitoring techniques are concerned with predicting the evolution of running cases of a business process based on models extracted from historical event logs. A range of such techniques have been proposed for a variety of business process prediction tasks, eg predicting the next activity (Becker et al., 2014), predicting the future path (continuation) of a running case (Polato et al., 2016), predicting the remaining cycle time (Rogge-Solti & Weske, 2013), and predicting deadline violations (Metzger et al., 2015). Existing predictive process monitoring approaches are tailor-made for specific prediction tasks and not readily generalizable. Moreover, their relative accuracy varies significantly depending on the input dataset and the point in time when the prediction is made. Long Short-Term Memory networks (Hochreiter & Schmidhuber, 1997) have been shown to deliver consistently high accuracy in several sequence modeling application domains, eg natural language processing and speech recognition. Recently,(Evermann et al., 2016) applied LSTMs specifically to predict the next activity in a case. This paper explores the application of LSTMs for three predictive business process monitoring tasks:(i) the next activity in a running case and its timestamp;(ii) the continuation of a case up to completion; and (iii) the remaining cycle time. The outlined LSTM architectures are empirically compared against tailor-made approaches using four real-life event logs.", "num_citations": "3\n", "authors": ["21"]}
{"title": "Stage-based business process mining\n", "abstract": " Evidence-based BPM has gained significant momentum in recent years, thanks to the widespread adoption of enterprise systems that store detailed business process execution data in event logs. Techniques for analyzing business processes using event logs are termed \u0393\u00c7\u00a3process mining\u0393\u00c7\u00a5 techniques. Their objective is to aid business analysts in improving business processes by learning knowledge from massive data. To date, techniques for process mining abound. For example, one can measure processing time and waiting time, diagnose process delays and quality issues, and replay an entire event log over a process model dis- covered from the log itself. However, these techniques often suffer from limited applicability, particularly when used on top of unpredictable processes such as patient treatment processes in healthcare as opposed to predictable processes such as a car manufacturing process. They failed to extract a highly fit process model, awkward in measuring process performance, and inaccurate in predictive monitoring. In addition, they are confused at how to divide the problem into sub-problems for better solutions. This research aims at designing a novel set of techniques based on a notion of business process stages which can improve over existing process mining techniques.", "num_citations": "3\n", "authors": ["21"]}
{"title": "On the Effect of Mixing Text and Diagrams on Business Process Model Use\n", "abstract": " A picture is worth a thousand words, but a few words can greatly enhance a picture. It is common to find textual and diagrammatic components complement each other in enterprise models in general, and business process models in particular. Previous work has considered the question of the relative understandability of diagrammatic versus textual representations of process models for different types of users. However, the effect of combining textual and diagrammatic components on the actual use of process models has to the best of our knowledge not been considered. This paper addresses the question of how the mix of diagrammatic and textual components in business process models affects their sustained use. This question is approached via a case study in a telecommunications company where models with different mixtures of text and diagrams have been collected over time. The study shows that models, in which the ordering relations between tasks are captured in diagrammatic form, while the details of each task are captured in textual form, are more likely to be used on a sustained basis.", "num_citations": "3\n", "authors": ["21"]}
{"title": "The Process Documentation Cube: A Model for Process Documentation Assessment\n", "abstract": " This paper presents a model for organizing and assessing business process documentation with the aim of identifying gaps and inconsistencies. The proposed model \u0393\u00c7\u00f4 namely the Process Documentation Cube (PDC) \u0393\u00c7\u00f4 has been tested in six public sector organizations in Estonia \u0393\u00c7\u00f4 three of them with years of process modeling engagement and three others in early stages of process modeling adoption. In the organizations where process modeling is already well established, the PDC allowed the relevant stakeholders to identify gaps in their documentation and directions for improving the integration between process models and other documentation. In the remaining organizations, the PDC was perceived as a useful tool for planning process documentation efforts.", "num_citations": "3\n", "authors": ["21"]}
{"title": "Detecting approximate clones in process model repositories with apromore\n", "abstract": " Approximate clone detection is the process of identifying similar process fragments in business process model collections. The tool presented in this paper can efficiently cluster approximate clones in large process model repositories. Once a repository is clustered, users can filter and browse the clusters using different filtering parameters. Our tool can also visualize clusters in the 2D space, allowing a better understanding of clusters and their member fragments. This demonstration will be useful for researchers and practitioners working on large process model repositories, where process standardization is a critical task for increasing the consistency and reducing the complexity of the repository.", "num_citations": "3\n", "authors": ["21"]}
{"title": "Detecting behavioural incompatibilities between pairs of services\n", "abstract": " We present a technique to analyse successive versions of a service interface in order to detect changes that cause clients using an earlier version not to interact properly with a later version. We focus on behavioural incompatibilities and adopt the notion of simulation as a basis for determining if a new version of a service is behaviourally compatible with a previous one. Unlike prior work, our technique does not simply check if the new version of the service simulates the previous one. Instead, in the case of incompatible versions, the technique provides detailed diagnostics, including a list of incompatibilities and specific states in which these incompatibilities occur. The technique has been implemented in a tool that visually pinpoints a set of changes that cause one behavioural interface not to simulate another one.", "num_citations": "3\n", "authors": ["21"]}
{"title": "Web Services and Formal Methods: 4th International Workshop, WS-FM 2007, Brisbane, Australia, September 28-29, 2007, Proceedings\n", "abstract": " This volume contains the papers presented at WS-FM 2007, the 4th International Workshop on Web Services and Formal Methods, held on September 28 and 29, 2007 in Brisbane, Australia. Web service technology aims at empowering providers of services, in the broad sense, with the ability to package and deliver their services by means of software applications available on the Web. Existing infrastructures for Web services-ready enable providers to describe services in terms of structure, access policy and behaviour, to locate services, to interact with them, and to bundle simpler services into more complex ones. However, innovations are needed to seamlessly extend this technology in order to deal with challenges such as managing int-actions with stateful and long-running Web services, managing large numbers of Web services each with multiple interfaces and versions, managing the quality of Web service delivery, etc. Formal methods have a fundamental role to play in shaping innovations in Web service technology. For instance, formal methods help to de? ne and to understand the semantics of languages and protocols that underpin existing infrastructures for Web services, and to formulate features that are found to be lacking. They also provide a basis for reasoning about Web service behaviour, for example to discover individual services that can ful? la given goal, or even to compose multiple services that can collectively ful? la goal. Finally, formal analysis of security properties and performance are relevant in many application areas of Web services such as e-commerce and e-business.", "num_citations": "3\n", "authors": ["21"]}
{"title": "Les services web\n", "abstract": " D\u251c\u2310finition A Web service is a software system designed to support interoperable machine-to-machine interaction over a network. It has an interface described in a machine-processable format (specifically WSDL1). Other systems interact with the Web service in a manner prescribed by its description using SOAP2 messages, typically conveyed using HTTP with an XML serialization in conjunction with other Web-related standards.", "num_citations": "3\n", "authors": ["21"]}
{"title": "Specification of composite trading activities in supply chain management\n", "abstract": " Negotiating with suppliers and with customers is a key part of supply chain management. However, with recent technological advances, the mechanisms available to carry out such activities have become increasingly sophisticated, and the environment in which these activities take place has become highly dynamic. As a consequence, the overall planning of these complex trades, and the coordination of the various component activities, need to be carefully considered. In this setting, it is crucial that the intended behaviour, and through that, the desired outcomes, of these composite trading activities be expressed in a suitably precise manner. Using an approach based on the generation of negotiation plans, this paper describes (i) an approach to the specification of such complex activities, and (ii) a corresponding execution model. The proposal is illustrated and validated by means of a scenario taken from a recent\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "3\n", "authors": ["21"]}
{"title": "Life after BPEL\n", "abstract": " The Business Process Execution Language for Web Services (BPEL) has emerged as a standard for specifying and executing processes. It is supported by vendors such as IBM and Microsoft and positioned as the \u0393\u00c7\u00a3process language of the Internet\u0393\u00c7\u00a5. This paper provides a critical analysis of BPEL based on the so-called workflow patterns. Italso discusses the need for languages like BPEL. Finally, the paper addresses several challenges not directly addressed by BPEL but highly relevant to the support of web services. 1", "num_citations": "3\n", "authors": ["21"]}
{"title": "Orchestrating interrelated trading activities\n", "abstract": " With the growing number of marketplaces and trading partners in the e-commerce environment, software tools designed to act on behalf of human traders are increasingly being used to automate trading activities. This paper describes a model for constructing trading engines which are capable of concurrently participating in multiple interrelated negotiations with heterogeneous protocols. The behaviour of these trading engines is specified by means of a generic synchronisation construct which enables the incremental composition of complex trading schemes, including a number of well known strategies from the financial trading domain. The construct is augmented with a priority-based scheduling algorithm which selects a set of nodes for negotiation based on their estimated profit, the time remaining and the desired degree of concurrency. The model also supports iterative negotiation, which is essential in any\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "3\n", "authors": ["21"]}
{"title": "Varying resource consumption to achieve scalable web services\n", "abstract": " Web service deployment is hampered by the possibility of sudden variations in request volumes. Mechanisms exist to enhance scalability in times of heavy load when the delivered content is static. However, web services typically involve dynamic content, delivered through application servers which may have little to no support for adapting to varying loads in order to ensure timely delivery. In this paper we discuss why scaling dynamic content delivery under load is difficult, we present a technique for controlled service degradation to achieve this scalability, and we present experimental results evaluating its benefits.", "num_citations": "3\n", "authors": ["21"]}
{"title": "Updates and application migration support in an ODMG temporal extension\n", "abstract": " A substantial number of temporal extensions to data models and query languages have been proposed. However, little attention has been paid to the migration of data and applications from a \u0393\u00c7\u00a3snapshot\u0393\u00c7\u00a5 DBMS to a temporal extension of it. In this paper, we analyze this issue and precisely formulate some requirements related to it. We then present a temporal extension of the ODMG\u0393\u00c7\u00d6s object database standard fulfilling these requirements. Throughout this presentation, we underscore the importance of providing adequate update and interpolation modalities in achieving application migration support.", "num_citations": "3\n", "authors": ["21"]}
{"title": "Prescriptive Process Monitoring for Cost-Aware Cycle Time Reduction\n", "abstract": " Reducing cycle time is a recurrent concern in the field of business process management. Depending on the process, various interventions may be triggered to reduce the cycle time of a case, for example, using a faster shipping service in an order-to-delivery process or giving a phone call to a customer to obtain missing information rather than waiting passively. Each of these interventions comes with a cost. This paper tackles the problem of determining if and when to trigger a time-reducing intervention in a way that maximizes the total net gain. The paper proposes a prescriptive process monitoring method that uses orthogonal random forest models to estimate the causal effect of triggering a time-reducing intervention for each ongoing case of a process. Based on this causal effect estimate, the method triggers interventions according to a user-defined policy. The method is evaluated on two real-life logs.", "num_citations": "2\n", "authors": ["21"]}
{"title": "Controlled flexibility in blockchain-based collaborative business processes\n", "abstract": " Blockchain technology enables the execution of collaborative business processes involving mutually untrusted parties. Existing tools allow such processes to be modeled using high-level notations and compiled into smart contracts that can be deployed on blockchain platforms. However, these tools do not provide mechanisms to cope with the flexibility requirements inherent to open and dynamic collaboration environments. In particular, existing tools adopt a static role binding approach wherein roles are bound to actors upfront when a process instance is created. Also, these tools do not allow participants to collectively make choices regarding alternative sub-processes or branches in the process model, at runtime. This paper presents a model for dynamic binding of actors to roles in collaborative processes and an associated binding policy specification language. The proposed language is endowed with a Petri\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "2\n", "authors": ["21"]}
{"title": "Business Process Management: 18th International Conference, BPM 2020, Seville, Spain, September 13-18, 2020, Proceedings\n", "abstract": " This book constitutes the proceedings of the 18th International Conference on Business Process Management, BPM 2020, held in Seville, Spain, in September 2020. The conference was held virtually due to the COVID-19 pandemic. The 27 full papers included in this volume were carefully reviewed and selected from 125 submissions. Two full keynote papers are also included. The papers are organized in topical sections named: foundations; engineering; and management.", "num_citations": "2\n", "authors": ["21"]}
{"title": "Robidium: automated synthesis of robotic process automation scripts from UI logs\n", "abstract": " This paper presents Robidium: a tool that discovers au- tomatable routine tasks from User Interactions (UI) logs and generates Robotic Process Automation (RPA) scripts to automate such routines. Unlike record-and-replay features provided by commercial RPA tools, Robidium may take as input an UI log that is not specifically recorded to capture a pre-identified task. Instead, the log may contain mixtures of automatable and non-automatable routines, interspersed with events that are not part of any routine as well as redundant or irrelevant events.", "num_citations": "2\n", "authors": ["21"]}
{"title": "Data mining methodologies in the banking domain: A systematic literature review\n", "abstract": " Data mining and advanced analytics methods and techniques usage in research and in business settings have increased exponentially over the last decade. Development and implementation of complex Big Data and advanced analytics projects requires well-defined methodology and processes. However, it remains unclear for what purposes and how data mining methodologies are used in practice and across different industry domains. This paper addresses the need and provides survey in the field of data mining and advanced data analytics methodologies, focusing on their application in the banking domain. By means of systematic literature review we have identified 102 articles and analyzed them in view of addressing three research questions: for what purposes data mining methodologies are used in the banking domain? How are they applied (\u0393\u00c7\u00a3as-is\u0393\u00c7\u00a5 vs adapted)? And what are the goals of\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "2\n", "authors": ["21"]}
{"title": "Multi-perspective comparison of business process variants based on event Logs (extended paper)\n", "abstract": " The comparison of business process variants based on event logs is a recurrent operation in the field of process mining. Existing approaches are restricted to intra-case relations, and more specifically, directly-follows relations such as \"a task directly follows another one\" or a \"resource directly hands-off to another resource\" within the same case. This paper presents a more general approach for log-based process variant comparison based on so-called perspective graphs. A perspective graph is a graph-based abstraction of an event log where a node represents any entity in an event log (task, resource, location, etc.) and an arc represents an arbitrary relation between these entities (e.g. directly-follows, co-occurs, hands-off to, works-together with, etc.) within or across cases. Statistically significant differences between two perspective graphs are captured in a so-called differential perspective graph, which allows us to compare two event logs from any given perspective. The proposed approach has been implemented as a proof of concept prototype in ProM. We illustrate the possibilities of the approach on real-life event logs and compare it to existing approaches for log-based process variant comparison.", "num_citations": "2\n", "authors": ["21"]}
{"title": "Mining non-redundant sets of generalizing patterns from sequence databases\n", "abstract": " Sequential pattern mining techniques extract patterns corresponding to frequent subsequences from a sequence database. A practical limitation of these techniques is that they overload the user with too many patterns. Local Process Model (LPM) mining is an alternative approach coming from the field of process mining. While in traditional sequential pattern mining, a pattern describes one subsequence, an LPM captures a set of subsequences. Also, while traditional sequential patterns only match subsequences that are observed in the sequence database, an LPM may capture subsequences that are not explicitly observed, but that are related to observed subsequences. In other words, LPMs generalize the behavior observed in the sequence database. These properties make it possible for a set of LPMs to cover the behavior of a much larger set of sequential patterns. Yet, existing LPM mining techniques still suffer from the pattern explosion problem because they produce sets of redundant LPMs. In this paper, we propose several heuristics to mine a set of non-redundant LPMs either from a set of redundant LPMs or from a set of sequential patterns. We empirically compare the proposed heuristics between them and against existing (local) process mining techniques in terms of coverage, precision, and complexity of the produced sets of LPMs.", "num_citations": "2\n", "authors": ["21"]}
{"title": "Behavior-based Process Comparison in Apromore.\n", "abstract": " This paper presents the integration of three behavior-based comparison operations between logs and/or models into the Apromore process model repository. Each of these operations takes as input a pair of process artifacts (two event logs, two models, or one log and one model) and describes their behavioral differences by means of natural language statements. The generated difference diagnosis has a range of applications of interest to both practitioners and researchers. For example, the difference diagnosis can offer guidance for reconciling discrepancies between business process variants (model2model comparison); can be used to pinpoint and explain differences between actual and expected behavior for conformance checking purposes (model2log comparison); or can explain dissimilarities between normal and deviant executions of a process (log2log comparison).", "num_citations": "2\n", "authors": ["21"]}
{"title": "Guest editorial: special issue on data and artifact-centric business processes\n", "abstract": " Traditionally, researchers in the field of Business Process Management (BPM) have focused on studying control-flow aspects of business processes independently from data aspects. The separation of concerns between control-flow and data has been fruitful and has enabled the development of various foundational theories and methods for BPM. However, the limits of theories and methods built on this separation of concerns have now become evident, particularly with the increasing pressure to support ad hoc and flexible business processes, where control-flow is often intermingled with data. In the past decade, various approaches have emerged that emphasize the integration of data and control as key pillars to support the specification, analysis, enactment, and monitoring of rich and flexible business processes. These include object-centric and artifact-centric approaches, where data and behavior are bundled\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "2\n", "authors": ["21"]}
{"title": "Evidence-Based Business Process Management.\n", "abstract": " Evidence-based business process management seeks to instill a data-driven approach to the traditional lifecycle of business process improvement. A family of techniques, known as process mining, supports this data-driven approach by extracting useful information from business process event logs. This chapter gives an overview of process mining techniques and demonstrates via several case studies, how these techniques enable evidencebased business process management in practice. The chapter is of interest particularly to business process analysts seeking to ground their hypotheses on evidence extracted from business process execution logs.", "num_citations": "2\n", "authors": ["21"]}
{"title": "Business process conformance checking based on event structures\n", "abstract": " This paper addresses the problem of business process conformance checking defined as follows: Given an event log recording the actual execution of a business process, and given a process model capturing its expected or normative execution, describe the differences between the behavior captured in the event log and that captured in the process model. In this setting, a log consists of a set of traces, where each trace is a sequence of events. An event refers to the execution of an activity in the process. This problem has been approached using replay [4] and trace alignment [7]. Replay takes as input one trace at a time and determines the maximal prefix of the trace (if any) that can be parsed by the model. When it is found that a prefix can no longer be parsed by the model, error-recovery techniques are used to correct the parsing error and continue parsing as much as possible the remaining input trace. Trace alignment identifies, for each trace in the log, the closest corresponding trace (s) produced by the model and then highlights the points where the trace and the model diverge. However, trace alignment cannot characterize the exact differences observed in a given state of the process in a concise and understandable way, particularly for processes with a large number of possible traces. In this abstract, we outline a method that, given a process model and an event log, returns a set of statements in natural language describing all the behavior observed in the log but not allowed by the process model (and vice versa). The method relies on a well-known model of concurrency, namely prime event structures. We show that the stated problem of\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "2\n", "authors": ["21"]}
{"title": "From business process models to service interfaces\n", "abstract": " Service-oriented architectures are often at the heart of modern enterprise information systems. Given that these systems are intended to support the day-to-day operations of an organization, a natural question to ask is how do we ensure that an organization\u0393\u00c7\u00d6s service-oriented architecture is aligned with its business processes? This chapter dives into this question by sketching a method for analyzing a business process in view of enabling its execution on top of a service-oriented application landscape. The chapter also provides an overview of technology standards and platforms for implementing business processes in the context of service-oriented architectures.", "num_citations": "2\n", "authors": ["21"]}
{"title": "Evaluation of trade-offs between workflow escalation strategies\n", "abstract": " Workflows in the service industry are subject to exceptional circumstances that affect the ability to complete work in a timely manner. For instance, workflows may need to deal with sudden spikes in customer demand due to a variety of events such as promotional deals, product launches, major news, or natural disasters. Escalation strategies can be incorporated into the design of a workflow so that it can cope with sudden spikes in the number of service requests while mitigating the effects of missed deadlines. In this article, we propose a method for evaluating escalation strategies using simulation technology. The effectiveness of the proposed method is demonstrated on a workflow from an insurance company.", "num_citations": "2\n", "authors": ["21"]}
{"title": "Predicting coding effort in projects containing xml\n", "abstract": " This paper studies the problem of predicting the coding effort for a subsequent year of development by analysing metrics extracted from project repositories, with an emphasis on projects containing XML code. The study considers thirteen open source projects and applies machine learning algorithms to generate models to predict one-year coding effort, measured in terms of lines of code added, modified and deleted. Both organisational and code metrics associated to revisions are taken into account. The results show that coding effort is highly determined by the expertise of developers while source code metrics have little effect on improving the accuracy of estimations of coding effort. The study also shows that models trained on one project are unreliable at estimating effort in other projects.", "num_citations": "2\n", "authors": ["21"]}
{"title": "Extended choice relation framework for workflow testing\n", "abstract": " Testing workflow applications is important given their critical role in an organization\u0393\u00c7\u00d6s everyday operations. At the same time, systematic workflow application testing requires a significant amount of effort since these applications usually manipulate non-trivial data structures and interact with many other application components. Therefore, devising efficient and effective ways of generating test cases for workflow applications is a highly relevant problem. In this paper, we propose a method for workflow test case generation based on the Choice Relation Framework (CRF). The proposed method aims at reducing the amount of input required from the sofware tester, while allowing test cases to be generated incrementally. To this end, test cases are generated in such a way as to cover the most frequently-occurring workflow execution traces, which are identified based on conditional branching probabilities typically available in workflow models. An empirical evaluation demonstrates that the proposed method significantly reduces the amount of input that testers need to provide in order to produce test cases that cover the most frequent paths of the process.", "num_citations": "2\n", "authors": ["21"]}
{"title": "Predicting coding effort in projects containing xml code\n", "abstract": " This paper studies the problem of predicting the coding effort for a subsequent year of development by analysing metrics extracted from project repositories, with an emphasis on projects containing XML code. The study considers thirteen open source projects and applies machine learning algorithms to generate models to predict one-year coding effort, measured in terms of lines of code added, modified and deleted. Both organisational and code metrics associated to revisions are taken into account. The results show that coding effort is highly determined by the expertise of developers while source code metrics have little effect on improving the accuracy of estimations of coding effort. The study also shows that models trained on one project are unreliable at estimating effort in other projects.", "num_citations": "2\n", "authors": ["21"]}
{"title": "Guest editorial: Business process management\n", "abstract": " Guest editorial: Business process management: Data & Knowledge Engineering: Vol 68, No 9 ACM Digital Library home ACM home Google, Inc. (search) Advanced Search Browse About Sign in Register Advanced Search Journals Magazines Proceedings Books SIGs Conferences People More Search ACM Digital Library SearchSearch Advanced Search Data & Knowledge Engineering Periodical Home Latest Issue Archive Authors Affiliations Award Winners More HomeBrowse by TitlePeriodicalsData & Knowledge EngineeringVol. , No. Guest editorial: Business process management article Guest editorial: Business process management Share on Authors: M. Dumas profile image Marlon Dumas Institute of Computer Science, University of Tartu, J Liivi 2, Tartu, Estonia Institute of Computer Science, University of Tartu, J Liivi 2, Tartu, Estonia View Profile , Manfred Reichert profile image Manfred Reichert Institute of , \u0393\u00c7\u00aa", "num_citations": "2\n", "authors": ["21"]}
{"title": "Product flow analysis in distribution networks with fixed time horizon\n", "abstract": " The movement of items through a product distribution network is a complex dynamic process which depends not only on the network's static topology but also on a knowledge of how each node stores, handles and forwards items. Analysing this time-dependent behaviour would normally require a simulation algorithm which maintains a globally-synchronised system state. For a certain class of problem, however, where the simulation is required to stop in a consistent state but not necessarily maintain consistency at all times, we show that an algorithm that makes localised decisions only is sufficient. As a motivating example we consider the practical problem of product recalls, in which our primary concern is the state of the distribution network at a specific time after a batch of suspect items was released, but we do not necessarily care about intermediate states leading up to the final one.", "num_citations": "2\n", "authors": ["21"]}
{"title": "Intergiciels et Construction d'Applications R\u251c\u2310parties\n", "abstract": " L'acc\u251c\u00bfs aux syst\u251c\u00bfmes d'information s'appuie aujourd'hui de plus en plus sur des technologies Internet. Les efforts de standardisation dans ce contexte ont accentu\u251c\u2310 l'engouement des personnes et des organisations (aussi bien acad\u251c\u2310miques, qu'industrielles, commerciales, ou institutionnelles) pour l'utilisation de l'Internet et ont permis l'\u251c\u2310mergence des services Web comme support de d\u251c\u2310veloppement des applications accessibles par Internet. Ainsi, les technologies associ\u251c\u2310es aux services Web sont devenues incontournables pour le d\u251c\u2310veloppement d'applications interagissant les unes avec les autres par le biais de l'Internet. Nous proposons dans ce chapitre de faire un point sur le sujet des services Web. L'objectif de la discussion est de traiter des aspects conceptuels de la mod\u251c\u2310lisation des services aussi bien que des aspects li\u251c\u2310s \u251c\u00e1 leur implantation.", "num_citations": "2\n", "authors": ["21"]}
{"title": "Interactively exploring temporal object databases\n", "abstract": " CiteSeerX \u0393\u00c7\u00f6 Interactively exploring temporal object databases Documents Authors Tables Log in Sign up MetaCart DMCA Donate CiteSeerX logo Documents: Advanced Search Include Citations Authors: Advanced Search Include Citations Tables: DMCA Interactively exploring temporal object databases (2000) Cached Download as a PDF Download Links [www-clips.imag.fr] [www-mrim.imag.fr] [www.cse.unsw.edu.au] [iihm.imag.fr] [ieg.ifs.tuwien.ac.at] Save to List Add to Collection Correct Errors Monitor Changes by Chaouki Daassi , Marlon Dumas , Marie-christine Fauvet , Laurence Nigay , Pierre-claude Scholl Venue: In Actes des 16e Journ ees Bases de Donn ees Avanc ees (BDA Citations: 8 - 2 self Summary Citations Active Bibliography Co-citation Clustered Documents Version History Share Facebook Twitter Reddit Bibsonomy OpenURL Abstract \u252c\u00f1 Firstname.Lastname \u252c\u00d1 Keyphrases temporal object by: Solr -\u0393\u00c7\u00aa", "num_citations": "2\n", "authors": ["21"]}
{"title": "Grundlagen des Gesch\u251c\u00f1ftsprozessmanagements-\u251c\u255dbersetzt von Thomas Grisold, Steven Gro\u251c\u0192, Jan Mendling, Bastian Wurm, 1\n", "abstract": " Grundlagen des Gesch\u251c\u00f1ftsprozessmanagements | SpringerLink Skip to main content Skip to table of contents Advertisement Hide SpringerLink Search SpringerLink Search Home Log in \u252c\u2310 2021 Grundlagen des Gesch\u251c\u00f1ftsprozessmanagements \u251c\u255dbersetzt von Thomas Grisold, Steven Gro\u251c\u0192, Jan Mendling, Bastian Wurm Authors (view affiliations) Marlon Dumas Marcello La Rosa Jan Mendling Hajo A. Reijers Benefits Zeigt den gesamten Lebenszyklus des Gesch\u251c\u00f1ftsprozessmanagements Konzeptionelle Grundlagen und praktisch anwendbare Methoden Zus\u251c\u00f1tzliche Lehrmaterialien auf der begleitenden Webseite Textbook 1.7k Downloads Download book PDF Download book EPUB Chapters Table of contents (12 chapters) About About this book Table of contents Search within book 1.Front Matter Pages I-XXXI PDF 2.Einf\u251c\u255dhrung in das Gesch\u251c\u00f1ftsprozessmanagement Marlon Dumas, Marcello La Rosa, Jan Mendling, A. \u0393\u00c7\u00aa", "num_citations": "2\n", "authors": ["21"]}
{"title": "Opportunities and Challenges for Process Mining in Organizations: Results of a Delphi Study\n", "abstract": " Process mining is an active research domain and has been applied to understand and improve business processes. While significant research has been conducted on the development and improvement of algorithms, evidence on the application of process mining in organizations has been far more limited. In particular, there is limited understanding of the opportunities and challenges of using process mining in organizations. Such an understanding has the potential to guide research by highlighting barriers for process mining adoption and, thus, can contribute to successful process mining initiatives in practice. In this respect, the paper provides a holistic view of opportunities and challenges for process mining in organizations identified in a Delphi study with 40 international experts from academia and industry. Besides proposing a set of 30 opportunities and 32 challenges, the paper conveys insights into\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["21"]}
{"title": "Discovering generative models from event logs: data-driven simulation vs deep learning\n", "abstract": " A generative model is a statistical model capable of generating new data instances from previously observed ones. In the context of business processes, a generative model creates new execution traces from a set of historical traces, also known as an event log. Two types of generative business process models have been developed in previous work: data-driven simulation models and deep learning models. Until now, these two approaches have evolved independently, and their relative performance has not been studied. This paper fills this gap by empirically comparing a data-driven simulation approach with multiple deep learning approaches for building generative business process models. The study sheds light on the relative strengths of these two approaches and raises the prospect of developing hybrid approaches that combine these strengths.", "num_citations": "1\n", "authors": ["21"]}
{"title": "Discovering business process simulation models in the presence of multitasking and availability constraints\n", "abstract": " Business process simulation is a versatile technique for quantitative analysis of business processes. A well-known limitation of process simulation is that the accuracy of the simulation results is limited by the faithfulness of the process model and simulation parameters given as input to the simulator. To tackle this limitation, various authors have proposed to discover simulation models from process execution logs, so that the resulting simulation models more closely match reality. However, existing techniques in this field make certain assumptions about resource behavior that do not typically hold in practice, including: (i) that each resource performs one task at a time; and (ii) that resources are continuously available (24/7). In reality, resources may engage in multitasking behavior and they work only during certain periods of the day or the week. This article proposes an approach to discover process simulation models\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["21"]}
{"title": "Mine Me but Don't Single Me Out: Differentially Private Event Logs for Process Mining\n", "abstract": " The applicability of process mining techniques hinges on the availability of event logs capturing the execution of a business process. In some use cases, particularly those involving customer-facing processes, these event logs may contain private information. Data protection regulations restrict the use of such event logs for analysis purposes. One way of circumventing these restrictions is to anonymize the event log to the extent that no individual can be singled out using the anonymized log. This paper addresses the problem of anonymizing an event log in order to guarantee that, upon disclosure of the anonymized log, the probability that an attacker may single out any individual represented in the original log, does not increase by more than a threshold. The paper proposes a differentially private disclosure mechanism, which oversamples the cases in the log and adds noise to the timestamps to the extent required to achieve the above privacy guarantee. The paper reports on an empirical evaluation of the proposed approach using 14 real-life event logs in terms of data utility loss and computational efficiency.", "num_citations": "1\n", "authors": ["21"]}
{"title": "Constructing Digital Twins for Accurate and Reliable What-If Business Process Analysis\n", "abstract": " A long-standing problem in the field of Business Process Management (BPM) is that of constructing accurate and reliable models for \u0393\u00c7\u00a3what-if\u0393\u00c7\u00a5 business process analysis (digital process twins). This paper formulates this problem in a general setting and spells out associated challenges. The paper suggests that this problem can be addressed by combining observational data, experimental data, and domain knowledge using hybrid modeling methods drawing from the fields of discrete event simulation, machine learning, and causal inference.", "num_citations": "1\n", "authors": ["21"]}
{"title": "Privacy-Preserving Directly-Follows Graphs: Balancing Risk and Utility in Process Mining\n", "abstract": " Process mining techniques enable organizations to analyze business process execution traces in order to identify opportunities for improving their operational performance. Oftentimes, such execution traces contain private information. For example, the execution traces of a healthcare process are likely to be privacy-sensitive. In such cases, organizations need to deploy Privacy-Enhancing Technologies (PETs) to strike a balance between the benefits they get from analyzing these data and the requirements imposed onto them by privacy regulations, particularly that of minimizing re-identification risks when data are disclosed to a process analyst. Among many available PETs, differential privacy stands out for its ability to prevent predicate singling out attacks and its composable privacy guarantees. A drawback of differential privacy is the lack of interpretability of the main privacy parameter it relies upon, namely epsilon. This leads to the recurrent question of how much epsilon is enough? This article proposes a method to determine the epsilon value to be used when disclosing the output of a process mining technique in terms of two business-relevant metrics, namely absolute percentage error metrics capturing the loss of accuracy (a.k.a. utility loss) resulting from adding noise to the disclosed data, and guessing advantage, which captures the increase in the probability that an adversary may guess information about an individual as a result of a disclosure. The article specifically studies the problem of protecting the disclosure of the so-called Directly-Follows Graph (DFGs), which is a process mining artifact produced by most process mining tools\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["21"]}
{"title": "Automated Discovery of Process Models with True Concurrency and Inclusive Choices\n", "abstract": " Enterprise information systems allow companies to maintain detailed records of their business process executions. These records can be extracted in the form of event logs, which capture the execution of activities across multiple instances of a business process. Event logs may be used to analyze business processes at a fine level of detail using process mining techniques. Among other things, process mining techniques allow us to discover a process model from an event log\u0393\u00c7\u00f4an operation known as automated process discovery. Despite a rich body of research in the field, existing automated process discovery techniques do not fully capture the concurrency inherent in a business process. Specifically, the bulk of these techniques treat two activities A and B as concurrent if sometimes A completes before B and other times B completes before A. Typically though, activities in a business process are executed in a true\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["21"]}
{"title": "Scalable and Imbalance-Resistant Machine Learning Models for Anti-money Laundering: A Two-Layered Approach\n", "abstract": " In this paper, we address the problem of detecting potentially illicit behavior in the context of Anti-Money Laundering (AML). We specifically address two requirements that arise when training machine learning models for AML: scalability and imbalance-resistance. By scalability we mean the ability to train the models to very large transaction datasets. By imbalance-resistance we mean the ability for the model to achieve suitable accuracy despite high class imbalance, i.e. the low number of instances of potentially illicit behavior relative to a large number of features that may characterize potentially illicit behavior. We propose a two-layered modelling concept. The first layer consists of a Logistic Regression model with simple features, which can be computed with low overhead. These features capture customer profiles as well as global aggregates of transaction histories. This layer filters out a proportion of\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["21"]}
{"title": "Process Monitoring\n", "abstract": " After implementing and deploying a redesigned business process, it may happen that the new process does not meet our expectations. For example, certain types of unforeseen exceptions may arise, the processing time of some tasks may be much higher than expected due to these exceptions, and queues may build up to the extent that process participants start taking shortcuts due to high pressure, while customers become unsatisfied due to long waiting times. A first step to address these issues is to understand what is actually happening during the execution of the process. This is the goal of the process monitoring phase of the BPM lifecycle. This chapter gives an overview of process monitoring techniques and tools. The chapter first focuses on performance dashboards, both for offline and online monitoring. Next, it dives into process mining techniques, including methods for automated process discovery\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["21"]}
{"title": "1849-2016-IEEE Standard for eXtensible Event Stream (XES) for achieving interoperability in event logs and event streams\n", "abstract": " A grammar for a tag-based language whose aim is to provide designers of information systems with a unified and extensible methodology for capturing systems behaviors by means of event logs and event streams is defined in the XES standard. An XML Schema describing the structure of an XES event log/stream and a XML Schema describing the structure of an extension of such a log/stream are included in this standard. Moreover, a basic collection of so-called XES extension prototypes that provide semantics to certain attributes as recorded in the event log/stream is included in this standard. Scope: This standard defines World Wide Web Consortium (W3C) Extensible Markup Language (XML) structure and constraints on the contents of XML 1.1 documents that can be used to represent extensible event stream (XES) instances.1 A XES instance corresponds to a file-based event log or a formatted event stream that can be used to transfer event-driven data in a unified and extensible manner from a first site to a second site. Typically, the first site will be the site generating this event-driven data (for example, workflow systems, case handling systems, procurement systems, devices like wafer steppers and X-ray machines, and hospitals) while the second site will be the site analyzing this data (for example, by data scientists and/or advanced software systems). To transfer event-driven data in a unified manner, this standard includes a W3C XML Schema describing the structure of a XES instance. To transfer this data in an extensible manner, this standard also includes a W3C XML Schema describing the structure of an extension to such a XES\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["21"]}
{"title": "Discovering Local Concurrency Relations in Business Process Event Logs\n", "abstract": " Detecting concurrency relations between events is a fundamental primitive in process mining. Existing approaches to this problem identify concurrency relations between pairs of event types under a global interpretation. If two event types are found to be concurrent, every occurrence of one event type is deemed to be concurrent to one occurrence of the other. In practice, this assumption not always holds. This paper proposes a finer-grained approach, whereby two event types may be in a concurrency relation relative to one state of the process, but not relative to other states, i.e. the concurrency relation holds locally. Experimental results both with artificial and real-life logs show that the proposed local concurrency detection approach improves the accuracy of existing concurrency detection techniques.", "num_citations": "1\n", "authors": ["21"]}
{"title": "BPMN Miner 2.0: Discovering hierarchical and block-structured BPMN process models\n", "abstract": " We present BPMN Miner 2.0: a tool that extracts hierarchical and block-structured BPMN process models from event logs. Given an event log in XES format, the tool partitions it into sub-logs (one per subprocess) and discovers a BPMN process model from each sub-log using existing techniques for discovering BPMN process models via heuristics nets or Petri nets. A drawback of these techniques is that they often produce spaghetti-like models and in some cases unsound models. Accordingly, BPMN Miner 2.0 applies post-processing steps to remove unsound constructions as well as a technique to block-structrure the resulting process models in a behavior-preserving manner. The tool is available as a standalone Java tool as well as a ProM and an Apromore plugin. The target audience of this demonstration includes process mining researchers as well as practitioners interested in exploring the potential of process mining using BPMN.", "num_citations": "1\n", "authors": ["21"]}
{"title": "Combining propensity and influence models for product adoption prediction\n", "abstract": " This paper studies the problem of selecting users in an online social network for targeted advertising so as to maximize the adoption of a given product. In previous work, two families of models have been considered to address this problem: direct targeting and network-based targeting. The former approach targets users with the highest propensity to adopt the product, while the latter approach targets users with the highest influence potential--that is users whose adoption is most likely to be followed by subsequent adoptions by peers. This paper proposes a hybrid approach that combines a notion of propensity and a notion of influence into a single utility function. We show that targeting a fixed number of high-utility users results in more adoptions than targeting either highly influential users or users with high propensity.", "num_citations": "1\n", "authors": ["21"]}
{"title": "Towards an assessment model for balancing process model production and use\n", "abstract": " This paper presents a framework for assessing the balance between process modeling effort and process model usage in order to optimize the value of process modeling in an organization. The framework has been tested through case studies in five organizations. The case studies demonstrate different degrees of imbalance in favor of model production as compared to usage. Three of the studied organizations have active and structured process modeling programs but underuse the models. We contend that the proposed framework can form a basis for a value-aware process modeling governance framework.", "num_citations": "1\n", "authors": ["21"]}
{"title": "Analyzing web services networks: Theory and practice\n", "abstract": " This paper addresses the problem of applying the general network theory for analyzing qualitatively Web services networks. The paper reviews current approaches to analyzing Web services networks, generalizes the published approaches into a formal framework for analyzing Web services networks and demonstrates its applicability in practice. More specifically, two case studies are described where the presented framework has been applied. The first one considers identification of redundant data in large-scale service-oriented information systems, while the second one measures information diffusion between individual information systems.", "num_citations": "1\n", "authors": ["21"]}
{"title": "Process Redesign\n", "abstract": " The thorough analysis of a business process typically sparks various ideas and directions for redesign. The problem is, however, that redesign is often not approached in a systematic way, but rather considered as a purely creative activity. The critical point with creative techniques is that parts of the spectrum of potential redesign options could be missed. As an alternative, suitable methods can be utilized to yield more and, hopefully, better redesign options.           This chapter deals with rethinking and re-organizing business processes with the specific purpose of making them perform better. We clarify the motivation and the trade-offs of redesign. Then, we present two methods for systematically redesigning processes. First, we introduce Heuristic Process Redesign as a method that builds upon an extensive set of redesign options. The method is illustrated by the help of a case of a health care institute. Second\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["21"]}
{"title": "Advanced Process Modeling\n", "abstract": " In this chapter we will learn how to model complex business processes with BPMN. The constructs presented in this chapter build on top of the knowledge acquired in Chap.\u252c\u00e1               3                            . In particular, we will expand on activities, events and gateways. We will learn how to use activities to model sub-processes and how to reuse these sub-processes across different processes. We will also extend activities to model more sophisticated forms of rework and repetition. As per events, we will expand on message events, present temporal events and show how race conditions can be modeled among these event types via a new type of gateway. We will also learn how to use events to handle business process exceptions. Finally, we will show how a collaboration diagram can be abstracted into a choreography diagram that only focuses on the interactions between the involved business parties.", "num_citations": "1\n", "authors": ["21"]}
{"title": "D\u251c\u2310centralisation Optimis\u251c\u2310e de Services Web Compos\u251c\u2310s.\n", "abstract": " Dans le contexte des services web, une orchestration repr\u251c\u2310sente une collaboration entre plusieurs services. Elle d\u251c\u2310finit la logique interne qui r\u251c\u2310git l\u0393\u00c7\u00d6ordre d\u0393\u00c7\u00d6ex\u251c\u2310cution des activit\u251c\u2310s ainsi que les interactions entre elles. Cette architecture centralis\u251c\u2310e n\u0393\u00c7\u00d6est toutefois pas optimale en termes de co\u251c\u2557t de communication dans le sens o\u251c\u2563 toutes les interactions entre partenaires passent par l\u0393\u00c7\u00d6orchestrateur centralis\u251c\u2310. Dans des travaux pr\u251c\u2310c\u251c\u2310dents, nous avons propos\u251c\u2310 une approche pour la d\u251c\u2310centralisation des compositions de services web. Toutefois, cette approche et les approches similaires ne cherchent pas \u251c\u00e1 optimiser les interactions entres les partenaires en collaboration. Ce papier pr\u251c\u2310sente une m\u251c\u2310thode pour l\u0393\u00c7\u00d6optimisation de la d\u251c\u2310centralisation au niveau de la s\u251c\u2310lection des services affect\u251c\u2310s aux activit\u251c\u2310s (en termes de QdS) et de la charge globale de communication. La m\u251c\u2310thode prend en consid\u251c\u2310ration la charge de communication entre les paires de services, la quantit\u251c\u2310 de donn\u251c\u2310es que ces derniers vont \u251c\u2310changer et les contraintes de co-localisation et de s\u251c\u2310paration impos\u251c\u2310es par le concepteur.ABSTRACT. Composite services are usually specified by means of orchestration models that capture control and data-flow relations between activities. Concrete services are then assigned to each activity based on various criteria. In mainstream service orchestration platforms, the orchestration model is executed by a centralized orchestrator through which all interactions are channeled. This architecture is not optimal in terms of communication overhead and has the usual problems of a single point of failure. In previous work, we proposed a method for\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["21"]}
{"title": "Assessing the potential impact of web services on business processes\n", "abstract": " Though web services offer unique opportunities for the design of new business processes, the assessment of the potential impact of Web services on existing business information systems is often reduced to technical aspects. This paper proposes a four-phase methodology which facilitates the evaluation of the potential use of Web services on business information systems both from a technical and from a strategic viewpoint. It is based on business process models, which are used to frame the adoption and deployment of Web services and to assess their impact on existing business processes. The application of this methodology is described using a procurement scenario.", "num_citations": "1\n", "authors": ["21"]}
{"title": "Generating Interactive Questionnaires From Configuration Models\n", "abstract": " Configuration, be it at the level of models or at the level of code, is a recurrent issue in systems engineering. It arises for example in enterprise systems, where modules are adapted and composed to meet the needs of individual customers based on modifications to a reference model. It also manifests itself in the context of software product families, where variants of a system are built from a common code base. Configuration of such generic systems generally involves two phases: (i) collecting data by answering a set of questions; and (ii) performing certain actions on an existing model or code base to produce an individualized model or system. This paper focuses on the first of these phases. The paper proposes a formal foundation for representing configuration models as well as methods to ensure the consistency of these models and to generate questionnaires from them. The generated questionnaires are interactive, in the sense that questions are only posed if and when they can be answered, and the space of allowed answers to a question is determined by previous answers. The approach has been implemented and tested against a reference model from the logistics domain.", "num_citations": "1\n", "authors": ["21"]}
{"title": "Donn\u251c\u2310es temporelles Mod\u251c\u2310lisation, interrogation et visualisation\n", "abstract": " Les donn\u251c\u2310es temporelles interviennent dans un grand nombre de domaines d\u0393\u00c7\u00d6application utilisant des SGBD: analyse financiere, gestion de documents multim\u251c\u2310dias, systemes d\u0393\u00c7\u00d6information environnementale, etc. Aussi, la plupart des SGBD fournissent des types correspondant aux concepts de date et de dur\u251c\u2310e, gr\u251c\u00f3ce auxquels il est possible de mod\u251c\u2310liser des associations temporelles simples, comme par exemple la date de naissance ou l\u0393\u00c7\u00d6\u251c\u00f3ge d\u0393\u00c7\u00d6une personne. Toutefois,a quelques nuances pres, aucun de ces systemes n\u0393\u00c7\u00d6offre des abstractions d\u251c\u2310di\u251c\u2310esa la mod\u251c\u2310lisation d\u0393\u00c7\u00d6associations temporelles plus complexes, telles que l\u0393\u00c7\u00d6historique du salaire d\u0393\u00c7\u00d6un employ\u251c\u2310, la suite d\u0393\u00c7\u00d6activit\u251c\u2310s d\u0393\u00c7\u00d6un individu au cours d\u0393\u00c7\u00d6une journ\u251c\u2310e, ou la s\u251c\u2310quence d\u0393\u00c7\u00d6annotations attach\u251c\u2310esa un document vid\u251c\u2310o. Lorsque la prise en compte de l\u0393\u00c7\u00d6\u251c\u2310volution des donn\u251c\u2310es est requise, leur historique doit alors \u251c\u00actre cod\u251c\u2310a partir des types temporels \u251c\u2310l\u251c\u2310mentaires, et la s\u251c\u2310mantique de ce codage doit \u251c\u00actre int\u251c\u2310gr\u251c\u2310e dans la logique des programmes applicatifs. Ces programmes sont rendus ainsi plus complexes, et la connaissance que possede le SGBD sur la s\u251c\u2310mantique des donn\u251c\u2310es qu\u0393\u00c7\u00d6il manipule est r\u251c\u2310duite, ce qui restreint la port\u251c\u2310e des optimisations qu\u0393\u00c7\u00d6il est susceptible d\u0393\u00c7\u00d6effectuer. Par ailleurs, au fur eta mesure de la multiplication des sources de donn\u251c\u2310es temporelles, de l\u0393\u00c7\u00d6accroissement de leur taille, du nombre d\u0393\u00c7\u00d6utilisateurs qui y accedent et de la vari\u251c\u2310t\u251c\u2310 de leurs profils, le d\u251c\u2310veloppement d\u0393\u00c7\u00d6outils multi-paradigmes pour l\u0393\u00c7\u00d6exploitation interactive de ces sources devient un besoin de plus en plus crucial. D\u0393\u00c7\u00d6une fa\u251c\u00baon ou d\u0393\u00c7\u00d6une autre, ces outils font intervenir une interface d\u0393\u00c7\u00d6interrogation (un\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["21"]}
{"title": "Universit\u251c\u2310s am\u251c\u2310ricaine et fran\u251c\u00baaise; Quelques \u251c\u2310l\u251c\u2310ments de comparaison\n", "abstract": " Que l\u0393\u00c7\u00d6instruction et la possibilit\u251c\u2310 de r\u251c\u2310ussir dans la vie soient pour les citoyens du monde un droit parmi d'autres clairement \u251c\u2310nonc\u251c\u2310s, c'est l\u251c\u00e1 une id\u251c\u2310e ma\u251c\u00abtresse qui fut formul\u251c\u2310e dans beaucoup de pays. L'ONU depuis sa cr\u251c\u2310ation en 1945 a largement contribu\u251c\u2310 au d\u251c\u2310veloppement de l'\u251c\u2310ducation. N\u251c\u2310anmoins, les modes d'enseignements sont tr\u251c\u00bfs diff\u251c\u2310rents selon les pays. Nous avons choisi de comparer le syst\u251c\u00bfme universitaire Fran\u251c\u00baais et Am\u251c\u2310ricain en t\u251c\u00f3chant de montrer les points forts et les points faibles de ces deux types d'\u251c\u2310ducation.Pourquoi avoir choisi ces deux pays? O Notons que Thomas Jefferson et d'autres avec lui avaient l'intime conviction que le progr\u251c\u00bfs de l'intellect \u251c\u2310tait aussi important que celui de l'esprit et que l'un ne devait pas se concevoir sans l'autre. Ils comprenaient ainsi que dans une nation libre o\u251c\u2563 le pouvoir appartient au peuple, l'attachement \u251c\u00e1 l'\u251c\u2310ducation d\u251c\u2310finit le progr\u251c\u00bfs de cette d\u251c\u2310mocratie\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["21"]}
{"title": "Suivi d\u0393\u00c7\u00d6ex\u251c\u2310cution de services accessibles par l\u0393\u00c7\u00d6Internet\n", "abstract": " De nombreuses collaborations inter-organisationnelles s\u0393\u00c7\u00d6 appuient aujourd\u0393\u00c7\u00d6hui sur le d\u251c\u2310veloppement de nouveaux services accessibles par l\u0393\u00c7\u00d6Internet,a partir d\u0393\u00c7\u00d6autres d\u251c\u2310ja existants. Ces services composites ainsi obtenus, peuventa leur tour entrer dans d\u0393\u00c7\u00d6autres compositions. Les organisations offrant ce type de services, aussi bien que leurs clients ou partenaires, ont besoin de disposer de compte-rendus d\u0393\u00c7\u00d6ex\u251c\u2310cutions de ces services pour mesurer leurs performances, expliquer d\u0393\u00c7\u00d6\u251c\u2310ventuelles pannes et finalement fournir un support d\u0393\u00c7\u00d6aidea la d\u251c\u2310cision. Cet article pr\u251c\u2310sente une approche pour la gestion d\u0393\u00c7\u00d6un tel suivi. Les points abord\u251c\u2310s concernent la mod\u251c\u2310lisation et la collection des traces dans un environnement distribu\u251c\u2310, ainsi que l\u0393\u00c7\u00d6\u251c\u2310valuation des requ\u251c\u00actes portant sur ces traces.", "num_citations": "1\n", "authors": ["21"]}