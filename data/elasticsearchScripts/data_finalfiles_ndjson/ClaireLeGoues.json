{"title": "Genprog: A generic method for automatic software repair\n", "abstract": " This paper describes GenProg, an automated method for repairing defects in off-the-shelf, legacy programs without formal specifications, program annotations, or special coding practices. GenProg uses an extended form of genetic programming to evolve a program variant that retains required functionality but is not susceptible to a given defect, using existing test suites to encode both the defect and required functionality. Structural differencing algorithms and delta debugging reduce the difference between this variant and the original program to a minimal repair. We describe the algorithm and report experimental results of its success on 16 programs totaling 1.25 M lines of C code and 120K lines of module code, spanning eight classes of defects, in 357 seconds, on average. We analyze the generated repairs qualitatively and quantitatively to demonstrate that the process efficiently produces evolved programs that\u00a0\u2026", "num_citations": "874\n", "authors": ["544"]}
{"title": "Automatically finding patches using genetic programming\n", "abstract": " Automatic program repair has been a longstanding goal in software engineering, yet debugging remains a largely manual process. We introduce a fully automated method for locating and repairing bugs in software. The approach works on off-the-shelf legacy applications and does not require formal specifications, program annotations or special coding practices. Once a program fault is discovered, an extended form of genetic programming is used to evolve program variants until one is found that both retains required functionality and also avoids the defect in question. Standard test cases are used to exercise the fault and to encode program requirements. After a successful repair has been discovered, it is minimized using structural differencing algorithms and delta debugging. We describe the proposed method and report experimental results demonstrating that it can successfully repair ten different C programs\u00a0\u2026", "num_citations": "793\n", "authors": ["544"]}
{"title": "A systematic study of automated program repair: Fixing 55 out of 105 bugs for $8 each\n", "abstract": " There are more bugs in real-world programs than human programmers can realistically address. This paper evaluates two research questions: \u201cWhat fraction of bugs can be repaired automatically?\u201d and \u201cHow much does it cost to repair a bug automatically?\u201d In previous work, we presented GenProg, which uses genetic programming to repair defects in off-the-shelf C programs. To answer these questions, we: (1) propose novel algorithmic improvements to GenProg that allow it to scale to large programs and find repairs 68% more often, (2) exploit GenProg's inherent parallelism using cloud computing resources to provide grounded, human-competitive cost measurements, and (3) generate a large, indicative benchmark set to use for systematic evaluations. We evaluate GenProg on 105 defects from 8 open-source programs totaling 5.1 million lines of code and involving 10,193 test cases. GenProg automatically\u00a0\u2026", "num_citations": "621\n", "authors": ["544"]}
{"title": "A genetic programming approach to automated software repair\n", "abstract": " Genetic programming is combined with program analysis methods to repair bugs in off-the-shelf legacy C programs. Fitness is defined using negative test cases that exercise the bug to be repaired and positive test cases that encode program requirements. Once a successful repair is discovered, structural differencing algorithms and delta debugging methods are used to minimize its size. Several modifications to the GP technique contribute to its success:(1) genetic operations are localized to the nodes along the execution path of the negative test case;(2) high-level statements are represented as single nodes in the program tree;(3) genetic operators use existing code in other parts of the program, so new code does not need to be invented. The paper describes the method, reviews earlier experiments that repaired 11 bugs in over 60,000 lines of code, reports results on new bug repairs, and describes experiments\u00a0\u2026", "num_citations": "276\n", "authors": ["544"]}
{"title": "Is the cure worse than the disease? overfitting in automated program repair\n", "abstract": " Automated program repair has shown promise for reducing the significant manual effort debugging requires. This paper addresses a deficit of earlier evaluations of automated repair techniques caused by repairing programs and evaluating generated patches' correctness using the same set of tests. Since tests are an imperfect metric of program correctness, evaluations of this type do not discriminate between correct patches and patches that overfit the available tests and break untested but desired functionality. This paper evaluates two well-studied repair tools, GenProg and TrpAutoRepair, on a publicly available benchmark of bugs, each with a human-written patch. By evaluating patches using tests independent from those used during repair, we find that the tools are unlikely to improve the proportion of independent tests passed, and that the quality of the patches is proportional to the coverage of the test suite\u00a0\u2026", "num_citations": "247\n", "authors": ["544"]}
{"title": "Automatic program repair with evolutionary computation\n", "abstract": " There are many methods for detecting and mitigating software errors but few generic methods for automatically repairing errors once they are discovered. This paper highlights recent work combining program analysis methods with evolutionary computation to automatically repair bugs in off-the-shelf legacy C programs. The method takes as input the buggy C source code, a failed test case that demonstrates the bug, and a small number of other test cases that encode the required functionality of the program. The repair procedure does not rely on formal specifications, making it applicable to a wide range of extant software for which formal specifications rarely exist.", "num_citations": "222\n", "authors": ["544"]}
{"title": "Repairing programs with semantic code search (t)\n", "abstract": " Automated program repair can potentially reduce debugging costs and improve software quality but recent studies have drawn attention to shortcomings in the quality of automatically generated repairs. We propose a new kind of repair that uses the large body of existing open-source code to find potential fixes. The key challenges lie in efficiently finding code semantically similar (but not identical) to defective code and then appropriately integrating that code into a buggy program. We present SearchRepair, a repair technique that addresses these challenges by(1) encoding a large database of human-written code fragments as SMT constraints on input-output behavior, (2) localizing a given defect to likely buggy program fragments and deriving the desired input-output behavior for code to replace those fragments, (3) using state-of-the-art constraint solvers to search the database for fragments that satisfy that desired\u00a0\u2026", "num_citations": "215\n", "authors": ["544"]}
{"title": "Current challenges in automatic software repair\n", "abstract": " The abundance of defects in existing software systems is unsustainable. Addressing them is a dominant cost of software maintenance, which in turn dominates the life cycle cost of a system. Recent research has made significant progress on the problem of automatic program repair, using techniques such as evolutionary computation, instrumentation and run-time monitoring, and sound synthesis with respect to a specification. This article serves three purposes. First, we review current work on evolutionary computation approaches, focusing on GenProg, which uses genetic programming to evolve a patch to a particular bug. We summarize algorithmic improvements and recent experimental results. Second, we review related work in the rapidly growing subfield of automatic program repair. Finally, we outline important open research challenges that we believe should guide future research in the area.", "num_citations": "195\n", "authors": ["544"]}
{"title": "Who should test whom?\n", "abstract": " Examining the use and abuse of personality tests in software engineering.", "num_citations": "170\n", "authors": ["544"]}
{"title": "Specification mining with few false positives\n", "abstract": " Formal specifications can help with program testing, optimization, refactoring, documentation, and, most importantly, debugging and repair. Unfortunately, formal specifications are difficult to write manually, while techniques that infer specifications automatically suffer from 90\u201399% false positive rates. Consequently, neither option is currently practical for most software development projects.               We present a novel technique that automatically infers partial correctness specifications with a very low false positive rate. We claim that existing specification miners yield false positives because they assign equal weight to all aspects of program behavior. By using additional information from the software engineering process, we are able to dramatically reduce this rate. For example, we grant less credence to duplicate code, infrequently-tested code, and code that exhibits high turnover in the version control\u00a0\u2026", "num_citations": "91\n", "authors": ["544"]}
{"title": "Representations and operators for improving evolutionary software repair\n", "abstract": " Evolutionary computation is a promising technique for automating time-consuming and expensive software maintenance tasks, including bug repair. The success of this approach, however, depends at least partially on the choice of representation, fitness function, and operators. Previous work on evolutionary software repair has employed different approaches, but they have not yet been evaluated in depth. This paper investigates representation and operator choices for source-level evolutionary program repair in the GenProg framework [17], focusing on:(1) representation of individual variants,(2) crossover design,(3) mutation operators, and (4) search space definition. We evaluate empirically on a dataset comprising 8 C programs totaling over 5.1 million lines of code and containing 105 reproducible, human-confirmed defects. Our results provide concrete suggestions for operator and representation design\u00a0\u2026", "num_citations": "79\n", "authors": ["544"]}
{"title": "Designing better fitness functions for automated program repair\n", "abstract": " Evolutionary methods have been used to repair programs automatically, with promising results. However, the fitness function used to achieve these results was based on a few simple test cases and is likely too simplistic for larger programs and more complex bugs. We focus here on two aspects of fitness evaluation: efficiency and precision. Efficiency is an issue because many programs have hundreds of test cases, and it is costly to run each test on every individual in the population. Moreover, the precision of fitness functions based on test cases is limited by the fact that a program either passes a test case, or does not, which leads to a fitness function that can take on only a few distinct values. This paper investigates two approaches to enhancing fitness functions for program repair, incorporating (1) test suite selection to improve efficiency and (2) formal specifications to improve precision. We evaluate test suite\u00a0\u2026", "num_citations": "79\n", "authors": ["544"]}
{"title": "Static automated program repair for heap properties\n", "abstract": " Static analysis tools have demonstrated effectiveness at finding bugs in real world code. Such tools are increasingly widely adopted to improve software quality in practice. Automated Program Repair (APR) has the potential to further cut down on the cost of improving software quality. However, there is a disconnect between these effective bug-finding tools and APR. Recent advances in APR rely on test cases, making them inapplicable to newly discovered bugs or bugs difficult to test for deterministically (like memory leaks). Additionally, the quality of patches generated to satisfy a test suite is a key challenge. We address these challenges by adapting advances in practical static analysis and verification techniques to enable a new technique that finds and then accurately fixes real bugs without test cases. We present a new automated program repair technique using Separation Logic. At a high-level, our technique\u00a0\u2026", "num_citations": "59\n", "authors": ["544"]}
{"title": "Overfitting in semantics-based automated program repair\n", "abstract": " The primary goal of Automated Program Repair (APR) is to automatically fix buggy software, to reduce the manual bug-fix burden that presently rests on human developers. Existing APR techniques can be generally divided into two families: semantics- vs. heuristics-based. Semantics-based APR uses symbolic execution and test suites to extract semantic constraints, and uses program synthesis to synthesize repairs that satisfy the extracted constraints. Heuristic-based APR generates large populations of repair candidates via source manipulation, and searches for the best among them. Both families largely rely on a primary assumption that a program is correctly patched if the generated patch leads the program to pass all provided test cases. Patch correctness is thus an especially pressing concern. A repair technique may generate overfitting patches, which lead a program to pass all existing test cases\u00a0\u2026", "num_citations": "58\n", "authors": ["544"]}
{"title": "Measuring code quality to improve specification mining\n", "abstract": " Formal specifications can help with program testing, optimization, refactoring, documentation, and, most importantly, debugging and repair. However, they are difficult to write manually, and automatic mining techniques suffer from 90-99 percent false positive rates. To address this problem, we propose to augment a temporal-property miner by incorporating code quality metrics. We measure code quality by extracting additional information from the software engineering process and using information from code that is more likely to be correct, as well as code that is less likely to be correct. When used as a preprocessing step for an existing specification miner, our technique identifies which input is most indicative of correct program behavior, which allows off-the-shelf techniques to learn the same number of specifications using only 45 percent of their original input. As a novel inference technique, our approach has few\u00a0\u2026", "num_citations": "55\n", "authors": ["544"]}
{"title": "Learning to rank for bug report assignee recommendation\n", "abstract": " Projects receive a large number of bug reports, and resolving these reports take considerable time and human resources. To aid developers in the resolution of bug reports, various automated techniques have been proposed to identify and recommend developers to address newly reported bugs. Two families of bug assignee recommendation techniques include those that recommend developers who have fixed similar bugs before (a.k.a. activity-based techniques) and those recommend suitable developers based on the location of the bug (a.k.a. location-based techniques). Previously, each of these techniques has been investigated separately. In this work, we propose a unified model that combines information from both developers' previous activities and suspicious program locations associated with a bug report in the form of similarity features. We have evaluated our proposed approach on more than 11,000\u00a0\u2026", "num_citations": "53\n", "authors": ["544"]}
{"title": "A deeper look into bug fixes: patterns, replacements, deletions, and additions\n", "abstract": " Many implementations of research techniques that automatically repair software bugs target programs written in C. Work that targets Java often begins from or compares to direct translations of such techniques to a Java context. However, Java and C are very different languages, and Java should be studied to inform the construction of repair approaches to target it. We conduct a large-scale study of bug-fixing commits in Java projects, focusing on assumptions underlying common search-based repair approaches. We make observations that can be leveraged to guide high quality automatic software repair to target Java specifically, including common and uncommon statement modifications in human patches and the applicability of previously-proposed patch construction operators in the Java context.", "num_citations": "53\n", "authors": ["544"]}
{"title": "The boogie verification debugger (tool paper)\n", "abstract": " The Boogie Verification Debugger (BVD) is a tool that lets users explore the potential program errors reported by a deductive program verifier. The user interface is like that of a dynamic debugger, but the debugging happens statically without executing the program. BVD integrates with the program-verification engine Boogie. Just as Boogie supports multiple language front-ends, BVD can work with those front-ends through a plug-in architecture. BVD plugins have been implemented for two state-of-the-art verifiers, VCC and Dafny.", "num_citations": "50\n", "authors": ["544"]}
{"title": "Enhancing automated program repair with deductive verification\n", "abstract": " Automated program repair (APR) is a challenging process of detecting bugs, localizing buggy code, generating fix candidates and validating the fixes. Effectiveness of program repair methods relies on the generated fix candidates, and the methods used to traverse the space of generated candidates to search for the best ones. Existing approaches generate fix candidates based on either syntactic searches over source code or semantic analysis of specification, e.g., test cases. In this paper, we propose to combine both syntactic and semantic fix candidates to enhance the search space of APR, and provide a function to effectively traverse the search space. We present an automated repair method based on structured specifications, deductive verification and genetic programming. Given a function with its specification, we utilize a modular verifier to detect bugs and localize both program statements and sub-formulas\u00a0\u2026", "num_citations": "48\n", "authors": ["544"]}
{"title": "Examining programmer practices for locally handling exceptions\n", "abstract": " Many have argued that the current try/catch mechanism for handling exceptions in Java is flawed. A major complaint is that programmers often write minimal and low quality handlers. We used the Boa tool to examine a large number of Java projects on GitHub to provide empirical evidence about how programmers currently deal with exceptions. We found that programmers handle exceptions locally in catch blocks much of the time, rather than propagating by throwing an Exception. Programmers make heavy use of actions like Log, Print, Return, or Throw in catch blocks, and also frequently copy code between handlers. We found bad practices like empty catch blocks or catching Exception are indeed widespread. We discuss evidence that programmers may misjudge risk when catching Exception, and face a tension between handlers that directly address local program statement failure and handlers that consider\u00a0\u2026", "num_citations": "35\n", "authors": ["544"]}
{"title": "Using a probabilistic model to predict bug fixes\n", "abstract": " Automatic Software Repair (APR) has significant potential to reduce software maintenance costs by reducing the human effort required to localize and fix bugs. State-of-the-art generate-and-validate APR techniques select between and instantiate various mutation operators to construct candidate patches, informed largely by heuristic probability distributions. This may reduce effectiveness in terms of both efficiency and output quality. In practice, human developers have many options in terms of how to edit code to fix bugs, some of which are far more common than others (e.g., deleting a line of code is more common than adding a new class). We mined the most recent 100 bug-fixing commits from each of the 500 most popular Java projects in GitHub (the largest dataset to date) to create a probabilistic model describing edit distributions. We categorize, compare and evaluate the different mutation operators used in\u00a0\u2026", "num_citations": "33\n", "authors": ["544"]}
{"title": "Using execution paths to evolve software patches\n", "abstract": " We present an evolutionary approach using genetic programming (GP) to automatically create software repairs. By concentrating the modifications on regions related to where the bug occurs, we effectively minimize the search space complexity and hence increase the performance of the GP process. To preserve the core functionalities of the program, we evolve programs only from code in the original program. Early experimental results show our GP approach is able to fix various program defects in reasonable time.", "num_citations": "31\n", "authors": ["544"]}
{"title": "Empirical study on synthesis engines for semantics-based program repair\n", "abstract": " Automatic Program Repair (APR) is an emerging and rapidly growing research area, with many techniques proposed to repair defective software. One notable state-of-the-art line of APR approaches is known as semantics-based techniques, e.g., Angelix, which extract semantics constraints, i.e., specifications, via symbolic execution and test suites, and then generate repairs conforming to these constraints using program synthesis. The repair capability of such approaches-expressive power, output quality, and scalability-naturally depends on the underlying synthesis technique. However, despite recent advances in program synthesis, not much attention has been paid to assess, compare, or leverage the variety of available synthesis engine capabilities in an APR context. In this paper, we empirically compare the effectiveness of different synthesis engines for program repair. We do this by implementing a framework\u00a0\u2026", "num_citations": "30\n", "authors": ["544"]}
{"title": "Improved representation and genetic operators for linear genetic programming for automated program repair\n", "abstract": " Genetic improvement for program repair can fix bugs or otherwise improve software via patch evolution. Consider GenProg, a prototypical technique of this type. GenProg\u2019s crossover and mutation operators manipulate individuals represented as patches. A patch is composed of high-granularity edits that indivisibly comprise an edit operation, a faulty location, and a fix statement used in replacement or insertions. We observe that recombination and mutation of such high-level units limits the technique\u2019s ability to effectively traverse and recombine the repair search spaces. We propose a reformulation of program repair representation, crossover, and mutation operators such that they explicitly traverse the three subspaces that underlie the search problem: the Operator, Fault and Fix Spaces. We provide experimental evidence validating our insight, showing that the operators provide considerable improvement\u00a0\u2026", "num_citations": "25\n", "authors": ["544"]}
{"title": "Semantic crash bucketing\n", "abstract": " Precise crash triage is important for automated dynamic testing tools, like fuzzers. At scale, fuzzers produce millions of crashing inputs. Fuzzers use heuristics, like stack hashes, to cut down on duplicate bug reports. These heuristics are fast, but often imprecise: even after deduplication, hundreds of uniquely reported crashes can still correspond to the same bug. Remaining crashes must be inspected manually, incurring considerable effort. In this paper we present Semantic Crash Bucketing, a generic method for precise crash bucketing using program transformation. Semantic Crash Bucketing maps crashing inputs to unique bugs as a function of changing a program (i.e., a semantic delta). We observe that a real bug fix precisely identifies crashes belonging to the same bug. Our insight is to approximate real bug fixes with lightweight program transformation to obtain the same level of precision. Our approach uses (a\u00a0\u2026", "num_citations": "25\n", "authors": ["544"]}
{"title": "Improved crossover operators for genetic programming for program repair\n", "abstract": " GenProg is a stochastic method based on genetic programming that presents promising results in automatic software repair via patch evolution. GenProg\u2019s crossover operates on a patch representation composed of high-granularity edits that indivisibly comprise an edit operation, a faulty location, and a fix statement used in replacement or insertions. Recombination of such high-level minimal units limits the technique\u2019s ability to effectively traverse and recombine the repair search spaces. In this work, we propose a reformulation of program repair operators such that they explicitly traverse three subspaces that underlie the search problem: Operator, Fault Space and Fix Space. We leverage this reformulation in the form of new crossover operators that faithfully respect this subspace division, improving search performance. Our experiments on 43 programs validate our insight, and show that the Unif1Space\u00a0\u2026", "num_citations": "24\n", "authors": ["544"]}
{"title": "Toward semantic foundations for program editors\n", "abstract": " Programming language definitions assign formal meaning to complete programs. Programmers, however, spend a substantial amount of time interacting with incomplete programs -- programs with holes, type inconsistencies and binding inconsistencies -- using tools like program editors and live programming environments (which interleave editing and evaluation). Semanticists have done comparatively little to formally characterize (1) the static and dynamic semantics of incomplete programs; (2) the actions available to programmers as they edit and inspect incomplete programs; and (3) the behavior of editor services that suggest likely edit actions to the programmer based on semantic information extracted from the incomplete program being edited, and from programs that the system has encountered in the past. As such, each tool designer has largely been left to develop their own ad hoc heuristics. This paper serves as a vision statement for a research program that seeks to develop these \"missing\" semantic foundations. Our hope is that these contributions, which will take the form of a series of simple formal calculi equipped with a tractable metatheory, will guide the design of a variety of current and future interactive programming tools, much as various lambda calculi have guided modern language designs. Our own research will apply these principles in the design of Hazel, an experimental live lab notebook programming environment designed for data science tasks. We plan to co-design the Hazel language with the editor so that we can explore concepts such as edit-time semantic conflict resolution mechanisms and mechanisms that allow\u00a0\u2026", "num_citations": "22\n", "authors": ["544"]}
{"title": "A novel fitness function for automated program repair based on source code checkpoints\n", "abstract": " Software maintenance, especially bug fixing, is one of the most expensive problems in software practice. Bugs have global impact in terms of cost and time, and they also reflect negatively on a company's brand. GenProg is a method for Automated Program Repair based on an evolutionary approach. It aims to generate bug repairs without human intervention or a need for special instrumentation or source code annotations. Its canonical fitness function evaluates each variant as the weighted sum of the test cases that a modified program passes. However, it evaluates distinct individuals with the same fitness score (plateaus). We propose a fitness function that minimizes these plateaus using dynamic analysis to increase the granularity of the fitness information that can be gleaned from test case execution, increasing the diversity of the population, the number of repairs found (expressiveness), and the efficiency of the\u00a0\u2026", "num_citations": "21\n", "authors": ["544"]}
{"title": "Automatic program repair using genetic programming\n", "abstract": " Software quality is an urgent problem. There are so many bugs in industrial program source code that mature software projects are known to ship with both known and unknown bugs [1], and the number of outstanding defects typically exceeds the resources available to address them [2]. This has become a pressing economic problem whose costs in the United States can be measured in the billions of dollars annually [3]. A dominant reason that software defects are so expensive is that fixing them remains a manual process. The process of identifying, triaging, reproducing, and localizing a particular bug, coupled with the task of understanding the underlying error, identifying a set of code changes that address it correctly, and then verifying those changes, costs both time [4] and money, and the cost of repairing a defect can increase by orders of magnitude as development progresses [5]. As a result, many defects, including critical security defects [6], remain unaddressed for long periods of time [7]. Moreover, humans are error-prone, and many human fixes are imperfect, in that they are either incorrect or lead to crashes, hangs, corruption, or security problems [8]. As a result, defect repair has become a major component of software maintenance, which in turn consumes up to 90% of the total lifecycle cost of a given piece of software [9]. Although considerable research attention has been paid to supporting various aspects of the manual debugging process [10, 11], and also to preempting or dynamically addressing particular classes of vulnerabilities, such as buffer overruns [12, 13], there exist virtually no previous automated solutions that address the\u00a0\u2026", "num_citations": "16\n", "authors": ["544"]}
{"title": "Defending against the attack of the micro-clones\n", "abstract": " Micro-clones are small pieces of redundant code, such as repeated subexpressions or statements. In this paper, we establish the considerations and value toward automated detection and removal of micro-clones at scale. We leverage the Boa software mining infrastructure to detect micro-clones in a data set containing 380,125 Java repositories, and yield thousands of instances where redundant code may be safely removed. By filtering our results to target popular Java projects on GitHub, we proceed to issue 43 pull requests that patch micro-clones. In summary, 95% of our patches to active GitHub repositories are merged rapidly (within 15 hours on average). Moreover, none of our patches were contested; they either constituted a real flaw, or have not been considered due to repository inactivity. Our results suggest that the detection and removal of micro-clones is valued by developers, can be automated at scale\u00a0\u2026", "num_citations": "15\n", "authors": ["544"]}
{"title": "Towards s/engineer/bot: Principles for program repair bots\n", "abstract": " Of the hundreds of billions of dollars spent on developer wages, up to 25% accounts for fixing bugs. Companies like Google save significant human effort and engineering costs with automatic bug detection tools, yet automatically fixing them is still a nascent endeavour. Very recent work (including our own) demonstrates the feasibility of automatic program repair in practice. As automated repair technology matures, it presents great appeal for integration into developer workflows. We believe software bots are a promising vehicle for realizing this integration, as they bridge the gap between human software development and automated processes. We envision repair bots orchestrating automated refactoring and bug fixing. To this end, we explore what building a repair bot entails. We draw on our understanding of patch generation, validation, and real world software development interactions to identify six principles\u00a0\u2026", "num_citations": "14\n", "authors": ["544"]}
{"title": "Evaluating the flexibility of the Java sandbox\n", "abstract": " The ubiquitously-installed Java Runtime Environment (JRE) provides a complex, flexible set of mechanisms that support the execution of untrusted code inside a secure sandbox. However, many recent exploits have successfully escaped the sandbox, allowing attackers to infect numerous Java hosts. We hypothesize that the Java security model affords developers more flexibility than they need or use in practice, and thus its complexity compromises security without improving practical functionality. We describe an empirical study of the ways benign open-source Java applications use and interact with the Java security manager. We found that developers regularly misunderstand or misuse Java security mechanisms, that benign programs do not use all of the vast flexibility afforded by the Java security model, and that there are clear differences between the ways benign and exploit programs interact with the security\u00a0\u2026", "num_citations": "14\n", "authors": ["544"]}
{"title": "BugZoo: a platform for studying software bugs\n", "abstract": " Proposing a new method for automatically detecting, localising, or repairing software faults requires a fair, reproducible evaluation of the effectiveness of the method relative to existing alternatives. Measuring effectiveness requires both an indicative set of bugs, and a mechanism for reliably reproducing and interacting with those bugs. We present BugZoo: a decentralised platform for distributing, reproducing, and interacting with historical software bugs. BugZoo connects existing datasets and tools to developers and researchers, and provides a controlled environment for conducting experiments. To ensure reproducibility, extensibility, and usability, BugZoo uses Docker containers to package, deliver, and interact with bugs and tools. Adding BugZoo support to existing datasets and tools is simple and non-invasive, requiring only a small number of supplementary files. BugZoo is open-source and available to\u00a0\u2026", "num_citations": "11\n", "authors": ["544"]}
{"title": "A qualitative study on framework debugging\n", "abstract": " Features of frameworks, such as inversion of control and the structure of framework applications, require developers to adjust their programming and debugging strategies as compared to sequential programs. However, the benefits and challenges of framework debugging are not fully understood, and gaining this knowledge could provide guidance in debugging strategies and framework tool design. To gain insight into the framework application debugging process, we performed two human studies investigating how developers fix applications that use a framework API incorrectly. These studies focused on the Android Fragment class and the ROS framework. We analyzed the results of the studies using a mixed-methods approach, using techniques from qualitative approaches. Our analysis found that participants benefited from the structure of frameworks and the pre-made solutions to common problems in the\u00a0\u2026", "num_citations": "10\n", "authors": ["544"]}
{"title": "Lightweight multi-language syntax transformation with parser parser combinators\n", "abstract": " Automatically transforming programs is hard, yet critical for automated program refactoring, rewriting, and repair. Multi-language syntax transformation is especially hard due to heterogeneous representations in syntax, parse trees, and abstract syntax trees (ASTs). Our insight is that the problem can be decomposed such that (1) a common grammar expresses the central context-free language (CFL) properties shared by many contemporary languages and (2) open extension points in the grammar allow customizing syntax (eg, for balanced delimiters) and hooks in smaller parsers to handle language-specific syntax (eg, for comments). Our key contribution operationalizes this decomposition using a Parser Parser combinator (PPC), a mechanism that generates parsers for matching syntactic fragments in source code by parsing declarative user-supplied templates. This allows our approach to detach from translating\u00a0\u2026", "num_citations": "9\n", "authors": ["544"]}
{"title": "Trusted software repair for system resiliency\n", "abstract": " We describe ongoing work to increase trust in resilient software systems. Automated software repair techniques promise to increase system resiliency, allowing missions to continue in the face of software defects. While a number of program repair approaches have been proposed, the most scalable and applicable of those techniques can be the most difficult to trust. Using approximate solutions to the oracle problem, we consider three approaches by which trust can be re-established in a post-repair system. Each approach learns or infers a different form of partial model of correct behavior from pre-repair observations; post-repair systems are evaluated with respect to those models. We focus on partial oracles modeled from external execution signals, derived from similar code fragment behavior, and inferred from invariant relations over local variables. We believe these three approaches can provide an expanded\u00a0\u2026", "num_citations": "8\n", "authors": ["544"]}
{"title": "The case for software evolution\n", "abstract": " Many software systems exceed our human ability to comprehend and manage, and they continue to contain unacceptable errors. This is an unintended consequence of Moore's Law, which has led to increases in system size, complexity, and interconnectedness. Yet, software is still primarily created, modified, and maintained by humans. The interactions among heterogeneous programs, machines and human operators has reached a level of complexity rivaling that of some biological ecosystems. By viewing software as an evolving complex system, researchers could incorporate biologically inspired mechanisms and employ the quantitative analysis methods of evolutionary biology. This approach could improve our understanding and analysis of software; it could lead to robust methods for automatically writing, debugging and improving code; and it could improve predictions about functional and structural\u00a0\u2026", "num_citations": "8\n", "authors": ["544"]}
{"title": "Common statement kind changes to inform automatic program repair\n", "abstract": " The search space for automatic program repair approaches is vast and the search for mechanisms to help restrict this search are increasing. We make a granular analysis based on statement kinds to find which statements are more likely to be modified than others when fixing an error. We construct a corpus for analysis by delimiting debugging regions in the provided dataset and recursively analyze the differences between the Simplified Syntax Trees associated with EditEvent's. We build a distribution of statement kinds with their corresponding likelihood of being modified and we validate the usage of this distribution to guide the statement selection. We then build association rules with different confidence thresholds to describe statement kinds commonly modified together for multi-edit patch creation. Finally we evaluate association rule coverage over a held out test set and find that when using a 95% confidence\u00a0\u2026", "num_citations": "7\n", "authors": ["544"]}
{"title": "Empirical study of restarted and flaky builds on Travis CI\n", "abstract": " Continuous Integration (CI) is a development practice where developers frequently integrate code into a common codebase. After the code is integrated, the CI server runs a test suite and other tools to produce a set of reports (eg, the output of linters and tests). If the result of a CI test run is unexpected, developers have the option to manually restart the build, re-running the same test suite on the same code; this can reveal build flakiness, if the restarted build outcome differs from the original build.", "num_citations": "6\n", "authors": ["544"]}
{"title": "Search-based software engineering\n", "abstract": " SBSE is growing up! In its sixth edition, the conference left home and expanded its reach in a process of becoming a truly global forum. Brazil was proudly chosen to kick off this process, mainly in recognition of its strong and still growing SBSE community. Besides innovating in its location, SSBSE 2014 implemented a series of novelties. First, it stood alone once again. As a test of maturity, this decision sheds light on how independent and solid the SBSE field has become. Second, it brought an all-inclusive experience, allowing for a much higher level of integration among participants, in turn strengthening the community and helping create a much more cooperative environment. Finally, it implemented a double-blind submission and review process for the first time, providing as fair and objective an evaluation of the submitted papers as possible. Obviously, this historical event would not have been possible without\u00a0\u2026", "num_citations": "6\n", "authors": ["544"]}
{"title": "Analyzing the impact of social attributes on commit integration success\n", "abstract": " As the software development community makes it easier to contribute to open source projects, the number of commits and pull requests keep increasing. However, this exciting growth renders it more difficult to only accept quality contributions. Recent research has found that both technical and social factors predict the success of project contributions on GitHub. We take this question a step further, focusing on predicting continuous integration build success based on technical and social factors involved in a commit. Specifically, we investigated if social factors (such as being a core member of the development team, having a large number of followers, or contributing a large number of commits) improve predictions of build success. We found that social factors cause a noticeable increase in predictive power (12%), core team members are more likely to pass the build tests (10%), and users with 1000 or more followers\u00a0\u2026", "num_citations": "5\n", "authors": ["544"]}
{"title": "A panel data set of cryptocurrency development activity on GitHub\n", "abstract": " Cryptocurrencies are a significant development in recent years, featuring in global news, the financial sector, and academic research. They also hold a significant presence in open source development, comprising some of the most popular repositories on GitHub. Their openly developed software artifacts thus present a unique and exclusive avenue to quantitatively observe human activity, effort, and software growth for cryptocurrencies. Our data set marks the first concentrated effort toward high-fidelity panel data of cryptocurrency development for a wide range of metrics. The data set is foremost a quantitative measure of developer activity for budding open source cryptocurrency development. We collect metrics like daily commits, contributors, lines of code changes, stars, forks, and subscribers. We also include financial data for each cryptocurrency: the daily price and market capitalization. The data set includes\u00a0\u2026", "num_citations": "4\n", "authors": ["544"]}
{"title": "Detecting execution anomalies as an oracle for autonomy software robustness\n", "abstract": " We propose a method for detecting execution anomalies in robotics and autonomy software. The algorithm uses system monitoring techniques to obtain profiles of executions. It uses a clustering algorithm to create clusters of those executions, representing nominal execution. A distance metric determines whether additional execution profiles belong to the existing clusters or should be considered anomalies. The method is suitable for identifying faults in robotics and autonomy systems. We evaluate the technique in simulation on two robotics systems, one of which is a real-world industrial system. We find that our technique works well to detect possibly unsafe behavior in autonomous systems.", "num_citations": "3\n", "authors": ["544"]}
{"title": "A case for double-blind reviewing in software engineering\n", "abstract": " This letter is to urge organizers of software engineering conferences to implement double-blind paper reviewing. I urge the organizers to do so for three reasons:(1) the scientific evidence that there exists bias in reviewing, and that double-blind reviewing reduces such bias, is overwhelming, and this is particularly important in our diversity-challenged field,(2) software engineering is falling behind the top conferences in other areas of computer science on this important issue, and (3) while the potential benefits of double-blind reviewing are significant, the costs and risks are minor.First, there is a large, and growing body of evidence that subconscious biases influence one\u2019s ability to objectively evaluate work. This evidence points to bias against national origin, gender, sexual orientation, and race. The evidence further supports the claim that double-blind reviewing mitigates these bias effects and thus improves the quality of the review process. A sampling of the relevant studies includes:", "num_citations": "3\n", "authors": ["544"]}
{"title": "SOAR: A Synthesis Approach for Data Science API Refactoring\n", "abstract": " With the growth of the open-source data science community, both the number of data science libraries and the number of versions for the same library are increasing rapidly. To match the evolving APIs from those libraries, open-source organizations often have to exert manual effort to refactor the APIs used in the code base. Moreover, due to the abundance of similar open-source libraries, data scientists working on a certain application may have an abundance of libraries to choose, maintain and migrate between. The manual refactoring between APIs is a tedious and error-prone task. Although recent research efforts were made on performing automatic API refactoring between different languages, previous work relies on statistical learning with collected pairwise training data for the API matching and migration. Using large statistical data for refactoring is not ideal because such training data will not be available for\u00a0\u2026", "num_citations": "2\n", "authors": ["544"]}
{"title": "Tailoring programs for static analysis via program transformation\n", "abstract": " Static analysis is a proven technique for catching bugs during software development. However, analysis tooling must approximate, both theoretically and in the interest of practicality. False positives are a pervading manifestation of such approximations---tool configuration and customization is therefore crucial for usability and directing analysis behavior. To suppress false positives, developers readily disable bug checks or insert comments that suppress spurious bug reports. Existing work shows that these mechanisms fall short of developer needs and present a significant pain point for using or adopting analyses. We draw on the insight that an analysis user always has one notable ability to influence analysis behavior regardless of analyzer options and implementation: modifying their program. We present a new technique for automated, generic, and temporary code changes that tailor to suppress spurious\u00a0\u2026", "num_citations": "2\n", "authors": ["544"]}
{"title": "Clarifications on the Construction and Use of the ManyBugs Benchmark\n", "abstract": " Automated repair techniques produce variant php interpreters, which should naturally serve as the tested interpreters. However, the answer to the question of what should serve as the testing interpreter is less obvious. php's default test harness configuration uses the same version of the interpreter for both the tested and testing interpreter. However, php may be configured via a command-line argument to use a different interpreter, such as the unmodified defective version, or a separate, manually-repaired version.", "num_citations": "2\n", "authors": ["544"]}
{"title": "VarFix: balancing edit expressiveness and search effectiveness in automated program repair\n", "abstract": " Automatically repairing a buggy program is essentially a search problem, searching for code transformations that pass a set of tests. Various search strategies have been explored, but they either navigate the search space in an ad hoc way using heuristics, or systemically but at the cost of limited edit expressiveness in the kinds of supported program edits. In this work, we explore the possibility of systematically navigating the search space without sacrificing edit expressiveness. The key enabler of this exploration is variational execution, a dynamic analysis technique that has been shown to be effective at exploring many similar executions in large search spaces. We evaluate our approach on IntroClassJava and Defects4J, showing that a systematic search is effective at leveraging and combining fixing ingredients to find patches, including many high-quality patches and multi-edit patches.", "num_citations": "1\n", "authors": ["544"]}
{"title": "An Empirical Study of OSS-Fuzz Bugs\n", "abstract": " Continuous fuzzing is an increasingly popular technique for automated quality and security assurance. Google maintains OSS-Fuzz: a continuous fuzzing service for open source software. We conduct the first empirical study of OSS-Fuzz, analyzing 23,907 bugs found in 316 projects. We examine the characteristics of fuzzer-found faults, the lifecycles of such faults, and the evolution of fuzzing campaigns over time. We find that OSS-Fuzz is often effective at quickly finding bugs, and developers are often quick to patch them. However, flaky bugs, timeouts, and out of memory errors are problematic, people rarely file CVEs for security vulnerabilities, and fuzzing campaigns often exhibit punctuated equilibria, where developers might be surprised by large spikes in bugs found. Our findings have implications on future fuzzing research and practice.", "num_citations": "1\n", "authors": ["544"]}
{"title": "Framefix: Automatically repairing statically-detected directive violations in framework applications\n", "abstract": " Software frameworks make developing applications for a specific domain easier than doing so from scratch. Unfortunately, frameworks can also place unexpected requirements on a developer\u2019s application, which can, in turn, lead to application bugs in the development process. We propose an automated technique for repairing violations of state-based framework requirements, FrameFix. First, developers of a framework can encode state-based framework requirements. Then, FrameFix automatically checks whether a developer\u2019s application follows the encoded requirements. Once a violation of these requirements has been detected, FrameFix tries three different approaches \u2014 reordering method calls, moving method calls to different method definitions, and comparing the faulty method to similarly defined methods on GitHub. These repair approaches are based on the principles of how frameworks interact with\u00a0\u2026", "num_citations": "1\n", "authors": ["544"]}
{"title": "Cooperative, trusted software repair for cyber physical system resiliency\n", "abstract": " Cyber physical systems CPS form a ubiquitous, networked computing substrate, which is increasingly essential to our nations civilian and military infrastructure. These systems must be highly resilient to adversaries, perform mission critical functions despite known and unknown vulnerabilities, and protect and repair themselves during or after operational failures and cyber-attacks. We believe that an automated CPS repair approach that can prevent failures of related, mission-critical systems is a necessary component to support the resiliency and survivability of our nations infrastructure. We developed and evaluated techniques to cooperatively repair certain general classes of cyber physical systems, and to increase the confidence of human operators in the trustworthiness of the repairs and the subsequent system behavior. We used embedded systems platforms, including quadrotor autonomous vehicles, to demonstrate and validate our approach.Descriptors:", "num_citations": "1\n", "authors": ["544"]}
{"title": "Cross-architecture lifter synthesis\n", "abstract": " Code translation is a staple component of program analysis. A lifter is a code translation unit that translates low-level code to a higher-level intermediate representation (IR). Lifters thus enable a host of static and dynamic analyses for such low-level code. However, writing a lifter is a tedious manual process which must be repeated for every architecture an analysis aims to support. We introduce cross-architecture lifter synthesis, a novel approach that automatically synthesizes lifters for previously unsupported architectures. Our insight is that lifter synthesis can be bootstrapped with existing IR sketches that exploit the shared semantic properties of heterogeneous architecture instruction sets. We show that our approach automates a significant amount of translation effort for a previously unsupported instruction set, and that it enables discovery of new bugs on new architecture targets through reuse of an\u00a0\u2026", "num_citations": "1\n", "authors": ["544"]}
{"title": "Behavior Metrics for Prioritizing Investigations of Exceptions\n", "abstract": " Many software development teams collect product defect reports, which can either be manually submitted or automatically created from product logs. Periodically, the teams use the collected defect reports to prioritize which defect to address next. We present a set of behavior-based metrics that can be used in this process. These metrics are based on the insight that development teams can estimate user inconvenience from user and application behavior in interaction logs. To estimate user inconvenience, the behavior metrics capture important user and application behavior after exceptions (the defects of interest in our case). We validated these metrics through a survey of how developers would incorporate the behavior metrics into their prioritization decisions. We found that developers change their priority of investigating an exception about 31% of the time after including the behavior metrics in the priority decision\u00a0\u2026", "num_citations": "1\n", "authors": ["544"]}
{"title": "Is the cure worse than the disease? A large-scale analysis of overfitting in automated program repair\n", "abstract": " Recent research in automated program repair has shown promise for reducing the significant manual effort debugging requires. This paper addresses a deficit of earlier evaluations of automated repair techniques caused by repairing programs and evaluating generated patches\u2019 correctness using the same set of tests. Since tests are an imperfect metric of program correctness, evaluations of this type do not discriminate between correct patches, and patches that overfit the available tests and break untested but desired functionality. This paper evaluates two well-studied repair tools, GenProg and TSPRepair, on a 956-bug dataset, each with a human-written patch. By evaluating patches on tests independent from those used during repair, we find that the tools are unlikely to improve the proportion of independent tests passed, and that the quality of the patches is proportional to the coverage of the test suite used during repair. For programs with fewer bugs, the tools are as likely to break tests as to fix them. However, novice developers also overfit, and automated repair can, under the right conditions, outperform these developers. In addition to overfitting, we measure the effects of test suite coverage, test suite provenance, starting program quality, and the difference in quality between novice-developer-written and toolgenerated patches when quality is assessed with an independent test suite from patch generation. We have released the 956-bug dataset to allow future evaluations of new repair tools.", "num_citations": "1\n", "authors": ["544"]}
{"title": "Fixing Real Bugs in Real Programs using Evolutionary Computing\n", "abstract": " We present an automatic method to create software repairs using Evolutionary Computing (EC) techniques. By concentrating the modifications on regions related to where the bug occurs, we effectively minimize the search space complexity and hence increase the performance of the EC process. To preserve the core functionalities of the program, we evolve programs only from code in the original program. Positive and negative testcases are used in the fitness function to determine the correctness of the programs. In addition, various techniques are considered to speed up the process and delta debugging with structural differencing algorithms are applied to optimize the repair found. Early experimental results show our EC approach is able to fix various program defects in reasonable time.", "num_citations": "1\n", "authors": ["544"]}