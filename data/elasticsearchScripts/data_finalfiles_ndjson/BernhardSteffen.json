{"title": "Lazy code motion\n", "abstract": " We present a bit-vector algorithm for the optimal and economical placement of computations within flow graphs, which is as efficient as standard uni-directional analyses. The point of our algorithm is the decomposition of the bi-directional structure of the known placement algorithms into a sequence of a backward and a forward analysis, which directly implies the efficiency result. Moreover, the new compositional structure opens the algorithm for modification: two further uni-directional analysis components exclude any unnecessary code motion. This laziness of our algorithm minimizes the register pressure, which has drastic effects on the run-time behaviour of the optimized programs in practice, where an economical use of registers is essential.", "num_citations": "370\n", "authors": ["1616"]}
{"title": "Optimal code motion: Theory and practice\n", "abstract": " An implementation-oriented algorithm for lazy code motion is presented that minimizes the number of computations in programs while suppressing any unnecessary code motion in order to avoid superfluous register pressure. In particular, this variant of the original algorithm for lazy code motion works on flowgraphs whose nodes are basic blocks rather than single statements, since this format is standard in optimizing compilers. The theoretical foundations of the modified algorithm are given in the first part, where t-refined flowgraphs are introduced for simplifying the treatment of flow graphs whose nodes are basic blocks. The second part presents the \u201cbasic block\u201d algorithm in standard notation and gives directions for its implementation in standard compiler environments.", "num_citations": "276\n", "authors": ["1616"]}
{"title": "Partial dead code elimination\n", "abstract": " A new aggressive algorithm for the elimination of partially dead code is presented, i.e., of code which is only dead on some program paths. Besides being more powerful than the usual approaches to dead code elimination, this algorithm is optimal in the following sense: partially dead code remaining in the resulting program cannot be eliminated without changing the branching structure or the semantics of the program, or without impairing some program executions. Our approach is based on techniques for partial redundancy elimination. Besides some new technical problems there is a significant difference here: partial dead code elimination introduces second order effects, which we overcome by means of exhaustive motion and elimination steps. The optimality and  the uniqueness of the program obtained is proved by means of a new technique which is universally applicable and particularly useful in the case of\u00a0\u2026", "num_citations": "268\n", "authors": ["1616"]}
{"title": "Verification on infinite structures\n", "abstract": " In this chapter, we present a hierarchy of infinite-state systems based on the primitive operations of sequential and parallel composition; the hierarchy includes a variety of commonly-studied classes of systems such as context-free and pushdown automata, and Petri net processes. We then examine the equivalence and regularity checking problems for these classes, with special emphasis on bisimulation equivalence, stressing the structural techniques which have been devised for solving these problems. Finally, we explore the model checking problem over these classes with respect to various linear- and branching-time temporal logics.", "num_citations": "260\n", "authors": ["1616"]}
{"title": "Model-checking\n", "abstract": " In the past two decades, model-checking has emerged as a promising and powerful approach to fully automatic verification of hardware systems. But model checking technology can be usefully applied to other application areas, and this article provides fundamentals that a practitioner can use to translate verification problems into model-checking questions. A taxonomy of the notions of \u201cmodel,\u201d \u201cproperty,\u201d and \u201cmodel checking\u201d are presented, and three standard model-checking approaches are described and applied to examples.", "num_citations": "249\n", "authors": ["1616"]}
{"title": "Model-driven development with the jABC\n", "abstract": " We present the jABC, a framework for model driven application development based on Lightweight Process Coordination. With jABC, users (product developers and system/software designers) easily develop services and applications by composing reusable building-blocks into hierarchical (flow-) graph structures that are executable models of the application. This process is supported by an extensible set of plugins providing additional functionalities, so that the jABC models can be animated, analyzed, simulated, verified, executed and compiled. This way of handling the collaborative design of complex software systems has proven to be effective and adequate for the cooperation of non-programmers and technical people, and it is now being rolled out in the operative practice.", "num_citations": "247\n", "authors": ["1616"]}
{"title": "Data flow analysis as model checking\n", "abstract": " The paper develops a framework that is based on the idea that modal logic provides an appropriate framework for the specification of data flow analysis (DFA) algorithms as soon as programs are represented as models of the logic. This can be exploited to construct a DFA-generator that generates efficient implementations of DFA-algorithms from modal specifications by partially evaluating a specific model checker with respect to the specifying modal formula. Moreover, the use of a modal logic as specification language for DFA-algorithms supports the compositional development of specifications and structured proofs of properties of DFA-algorithms. \u2014 The framework is illustrated by means of a real life example: the problem of determining optimal computation points within flow graphs.", "num_citations": "194\n", "authors": ["1616"]}
{"title": "The interprocedural coincidence theorem\n", "abstract": " We present an interprocedural generalization of the well-known (intraprocedural) Coincidence Theorem of Kam and Ullman, which provides a sufficient condition for the equivalence of the meet over all paths (MOP) solution and the maximal fixed point (MFP) solution to a data flow analysis problem. This generalization covers arbitrary imperative programs with recursive procedures, global and local variables, and formal value parameters. In the absence of procedures, it reduces to the classical intraprocedural version. In particular, our stack-based approach generalizes the coincidence theorems of Barth and Sharir/Pnueli for the same setup, which do not properly deal with local variables of recursive procedures.", "num_citations": "188\n", "authors": ["1616"]}
{"title": "Model checking for context-free processes\n", "abstract": " We develop a model-checking algorithm that decides for a given context-free process whether it satisfies a property written in the alternation-free modal mu-calculus. The central idea behind this algorithm is to raise the standard iterative model-checking techniques to higher order. in contrast to the usual approaches, in which the set of formulas that are satisfied by a certain state are iteratively computed, our algorithm iteratively computes a property transformer for each state class of the finite process representation. These property transformers can then simply be applied to solve the model-checking problem. The complexity of our algorithm is linear in the size of the system's representation and exponential in the size of the property being investigated.", "num_citations": "179\n", "authors": ["1616"]}
{"title": "Program Analysis as Model Checking of Abstract Interpretations\n", "abstract": " This paper presents a collection of techniques, a methodology, in which abstract interpretation, flow analysis, and model checking are employed in the representation, abstraction, and analysis of programs. The methodology shows the areas of intersection of the different techniques as well as the opportunites that exist when one technique is used in support of another. The methodology is presented as a three-step process: First, from a (small-step) operational semantics definition and a program, one constructs a program model, which is a state-transition system that encodes the program\u2019s executions. Second, abstraction upon the program model is performed, reducing the detail of information in the model\u2019s nodes and arcs. Finally, the program model is analyzed for properties of its states and paths.", "num_citations": "176\n", "authors": ["1616"]}
{"title": "Learnlib: A library for automata learning and experimentation\n", "abstract": " In this paper we present the LearnLib, a library for automata learning and experimentation. Its modular structure allows users to configure their tailored learning scenarios, which exploit specific properties of the envisioned applications. As has been shown earlier, exploiting application-specific structural features enables optimizations that may lead to performance gains of several orders of magnitude, a necessary precondition to make automata learning applicable to realistic scenarios.", "num_citations": "167\n", "authors": ["1616"]}
{"title": "A linear-time model-checking algorithm for the alternation-free modal mu-calculus\n", "abstract": " We develop a model-checking algorithm for a logic that permits propositions to be defined with greatest and least fixed points of mutually recursive systems of equations. This logic is as expressive as the alternation-free fragment of the modal mu-calculus identified by Emerson and Lei, and it may therefore be used to encode a number of temporal logics and behavioral preorders. Our algorithm determines whether a process satisfies a formula in time proportional to the product of the sizes of the process and the formula; this improves on the best known algorithm for similar fixed-point logics.", "num_citations": "157\n", "authors": ["1616"]}
{"title": "Parallelism for free: Efficient and optimal bitvector analyses for parallel programs\n", "abstract": " We consider parallel programs with shared memory and interleaving semantics, for which we show how to construct for unidirectional bitvector problems optimal analysis algorithms that are as efficient as their purely sequential counterparts and that can easily be implemented. Whereas the complexity result is rather obvious, our optimality result is a consequence of a new Kam/Ullman-style Coincidence Theorem. Thus using our method, the standard algorithms for sequential programs computing liveness, availability, very busyness, reaching definitions, definition-use chains, or the analyses for performing code motion, assignment motion, partial dead-code elimination or strength reduction, can straightforward be transferred to the parallel setting at almost no cost.", "num_citations": "154\n", "authors": ["1616"]}
{"title": "Introduction to active automata learning from a practical perspective\n", "abstract": " In this chapter we give an introduction to active learning of Mealy machines, an automata model particularly suited for modeling the behavior of realistic reactive systems. Active learning is characterized by its alternation of an exploration phase and a testing phase. During exploration phases so-called membership queries are used to construct hypothesis models of a system under learning. In testing phases so-called equivalence queries are used to compare respective hypothesis models to the actual system. These two phases are iterated until a valid model of the target system is produced.               We will step-wisely elaborate on this simple algorithmic pattern, its underlying correctness arguments, its limitations, and, in particular, ways to overcome apparent hurdles for practical application. This should provide students and outsiders of the field with an intuitive account of the high potential of this\u00a0\u2026", "num_citations": "147\n", "authors": ["1616"]}
{"title": "Domain-specific optimization in automata learning\n", "abstract": " Automatically generated models may provide the key towards controlling the evolution of complex systems, form the basis for test generation and may be applied as monitors for running applications. However, the practicality of automata learning is currently largely preempted by its extremely high complexity and unrealistic frame conditions. By optimizing a standard learning method according to domain-specific structural properties, we are able to generate abstract models for complex reactive systems. The experiments conducted using an industry-level test environment on a recent version of a telephone switch illustrate the drastic effect of our optimizations on the learning efficiency. From a conceptual point of view, the developments can be seen as an instance of optimizing general learning procedures by capitalizing on specific application profiles.", "num_citations": "146\n", "authors": ["1616"]}
{"title": "Model generation by moderated regular extrapolation\n", "abstract": " This paper introduces regular extrapolation, a technique that provides descriptions of systems or system aspects a posteriori in a largely automatic way. The descriptions come in the form of models which offer the possibility of mechanically producing system tests, grading test suites and monitoring running systems. Regular extrapolation builds models from observations via techniques from machine learning and finite automata theory. Also expert knowledge about the system enters the model construction in a systematic way. The power of this approach is illustrated in the context of a test environment for telecommunication systems.", "num_citations": "138\n", "authors": ["1616"]}
{"title": "LearnLib: a framework for extrapolating behavioral models\n", "abstract": " In this paper, we present the LearnLib, a library of tools for automata learning, which is explicitly designed for the systematic experimental analysis of the profile of available learning algorithms and corresponding optimizations. Its modular structure allows users to configure their own tailored learning scenarios, which exploit specific properties of their envisioned applications. As has been shown earlier, exploiting application-specific structural features enables optimizations that may lead to performance gains of several orders of magnitude, a necessary precondition to make automata learning applicable to realistic scenarios.", "num_citations": "134\n", "authors": ["1616"]}
{"title": "The TTT algorithm: a redundancy-free approach to active automata learning\n", "abstract": " In this paper we present TTT, a novel active automata learning algorithm formulated in the Minimally Adequate Teacher (MAT) framework. The distinguishing characteristic of TTT is its redundancy-free organization of observations, which can be exploited to achieve optimal (linear) space complexity. This is thanks to a thorough analysis of counterexamples, extracting and storing only the essential refining information. TTT is therefore particularly well-suited for application in a runtime verification context, where counterexamples (obtained, e.g., via monitoring) may be excessively long: as the execution time of a test sequence typically grows with its length, this would otherwise cause severe performance degradation. We illustrate the impact of TTT\u2019s consequent redundancy-free approach along a number of examples.", "num_citations": "132\n", "authors": ["1616"]}
{"title": "Agile IT: thinking in user-centric models\n", "abstract": " We advocate a new teaching direction for modern CS curricula: extreme model-driven development (XMDD), a new development paradigm designed to continuously involve the customer/application expert throughout the whole systems\u2019 life cycle. Based on the \u2018One-Thing Approach\u2019, which works by successively enriching and refining one single artifact, system development becomes in essence a user-centric orchestration of intuitive service functionality. XMDD differs radically from classical software development, which, in our opinion is no longer adequate for the bulk of application programming \u2013 in particular when it comes to heterogeneous, cross organizational systems which must adapt to rapidly changing market requirements. Thus there is a need for new curricula addressing this model-driven, lightweight, and cooperative development paradigm that puts the user process in the center of the\u00a0\u2026", "num_citations": "132\n", "authors": ["1616"]}
{"title": "Business process modeling in the jABC: the one-thing approach\n", "abstract": " The one thing approach is designed to overcome the classical communication hurdles between application experts and the various levels of IT experts. Technically, it is realized in terms of eXtreme Model Driven Design, a technique that puts the user-level process in the center of the development. It enables customers/users to design, animate, validate, and control their processes throughout the whole life cycle, starting with the first requirement analysis, and ending with the demand driven process evolution over its entire life span. This strict way of top-down thinking emphasizes the primary goal of every development: customer satisfaction.", "num_citations": "126\n", "authors": ["1616"]}
{"title": "The open-source LearnLib\n", "abstract": " In this paper, we present LearnLib, a library for active automata learning. The current, open-source version of LearnLib was completely rewritten from scratch, incorporating the lessons learned from the decade-spanning development process of the previous versions of LearnLib. Like its immediate predecessor, the open-source LearnLib is written in Java to enable a high degree of flexibility and extensibility, while at the same time providing a performance that allows for large-scale applications. Additionally, LearnLib provides facilities for visualizing the progress of learning algorithms in detail, thus complementing its applicability in research and industrial contexts with an educational\u00a0aspect.", "num_citations": "123\n", "authors": ["1616"]}
{"title": "Characteristic formulas for processes with divergence\n", "abstract": " Characteristic formulae have been introduced by Graf and Sifakis to relate equational reasoning about proceses to reasoning in a modal logic, and therefore to allow proofs about processes to be carried out in a logical framework. This work, which concerned finite processes and bisimulation-like equivalences, has later on been extended to finite state processes and further equivalences. Based upon an intuitionistic understanding of Hennessy-Milner Logic (HML) with mutual recursion, we extend these results to cover bisimulation-like preorders, which are sensitive to liveness properties. This demonstrates the expressive power of intuitionistically interpreted HML with mutual recursion, and it builds the theoretical basis for a uniform and efficient method to automatically verify bisimulation-like relations between processes by means of model checking.", "num_citations": "123\n", "authors": ["1616"]}
{"title": "Generating data flow analysis algorithms from modal specifications\n", "abstract": " The paper develops a framework that is based on the idea that modal logic provides an appropriate framework for the specification of data flow analysis (DFA) algorithms as soon as programs are represented as models of the logic. This can be exploited to construct a DFA-generator that generates efficient implementations of DFA-algorithms from modal specifications by partially evaluating a specific model checker with respect to the specifying modal formula. Moreover, the use of a modal logic as specification language for DFA-algorithms supports the compositional development of specifications and structured proofs of properties of DFA-algorithms. The framework is illustrated by means of a real-life example: the problem of determining optimal computation points within flow graphs.", "num_citations": "123\n", "authors": ["1616"]}
{"title": "Dynamic testing via automata learning\n", "abstract": " This paper presents dynamic testing, a method that exploits automata learning to systematically test (black box) systems almost without prerequisites. Based on interface descriptions and optional sample test cases, our method successively explores the system under test (SUT), in order to extrapolate a behavioural model. This is in turn used to steer the further exploration process. Due to the applied learning technique, our method is optimal in the sense that the extrapolated models are most concise (i.e. state minimal) in consistently representing all the information gathered during the exploration. Using the LearnLib, our framework for automata learning, our method can be elegantly combined with numerous optimisations of the learning procedure, with various choices of model structures, and with the option of dynamically/interactively enlarging the alphabet underlying the learning process. The latter is\u00a0\u2026", "num_citations": "122\n", "authors": ["1616"]}
{"title": "Inferring canonical register automata\n", "abstract": " In this paper, we present an extension of active automata learning to register automata, an automaton model which is capable of expressing the influence of data on control flow. Register automata operate on an infinite data domain, whose values can be assigned to registers and compared for equality. Our active learning algorithm is unique in that it directly infers the effect of data values on control flow as part of the learning process. This effect is expressed by means of registers and guarded transitions in the resulting register automata models. The application of our algorithm to a small example indicates the impact of learning register automata models: Not only are the inferred models much more expressive than finite state machines, but the prototype implementation also drastically outperforms the classic L               * algorithm, even when exploiting optimal data abstraction and symmetry reduction.", "num_citations": "119\n", "authors": ["1616"]}
{"title": "On the correspondence between conformance testing and regular inference\n", "abstract": " Conformance testing for finite state machines and regular inference both aim at identifying the model structure underlying a black box system on the basis of a limited set of observations. Whereas the former technique checks for equivalence with a given conjecture model, the latter techniques addresses the corresponding synthesis problem by means of techniques adopted from automata learning. In this paper we establish a common framework to investigate the similarities of these techniques by showing how results in one area can be transferred to results in the other and to explain the reasons for their differences.", "num_citations": "119\n", "authors": ["1616"]}
{"title": "Service engineering: Linking business and it\n", "abstract": " With its strong emphasis on modularization, service-oriented computing radically alters the way business processes are modeled, realized, and maintained. Domain-specific services virtualize complex functions of the underlying business applications so that they can be loosely coupled to form transorganizational processes. This level of abstraction fosters agility and lessens traditional provider dependence. Service-oriented design has long driven the development of the telecommunications infrastructure and applications, especially intelligent network services. Applying the same principles of domain specificity, visualization, loose coupling, and seamless vertical integration to business processes has the potential to lead to a new generation of personalized, secure, and highly available Web services", "num_citations": "117\n", "authors": ["1616"]}
{"title": "Model checking the full modal mu-calculus for infinite sequential processes\n", "abstract": " In this paper we develop a new exponential algorithm for model-checking infinite sequential processes, including context-free processes, pushdown processes, and regular graphs, that decides the full modal mu-calculus. Whereas the actual model checking algorithm results from considering conditional semantics together with backtracking caused by alternation, the corresponding correctness proof requires a stronger framework, which uses dynamic environments modelled by finite-state automata.", "num_citations": "117\n", "authors": ["1616"]}
{"title": "The Electronic Tool Integration platform: concepts and design\n", "abstract": " The Electronic Tool Integration platform (ETI) associated with STTT is designed for the interactive experimentation with and coordination of heterogeneous tools. ETI users are supported by an advanced, personalized Online Service guiding experimentation, coordination, and simple browsing of the available tool repository according to their degree of experience. In particular, this allows even newcomers to orient themselves in the wealth of existing tools and to identify the most appropriate collection of tools to solve their own application-specific tasks.", "num_citations": "107\n", "authors": ["1616"]}
{"title": "Service-orientation: conquering complexity with XMDD\n", "abstract": " We advocate a new direction for mastering complexity in service-oriented design of complex applications: eXtreme Model-Driven Development (XMDD). It is a new application development paradigm that is extreme in that it is designed to involve the customer/application expert continuously throughout the whole systems\u2019 life cycle, and it is model-driven because it is based on the \u2018One-Thing Approach\u2019, which works by successively enriching and refining one single artifact that is a rich model. With XMDD, system development becomes in essence a pathway to user-centric orchestration of intuitive service functionality. XMDD differs radically from classical software development, which in our opinion, is no longer adequate for the bulk of application programming, because the profile of todays\u2019s applications has changed and demands agility and a leaner development style. This need is particularly evident when\u00a0\u2026", "num_citations": "101\n", "authors": ["1616"]}
{"title": "An elementary bisimulation decision procedure for arbitrary context-free processes\n", "abstract": " We present an elementary algorithm for deciding bisimulation between arbitrary context-free processes. This improves on the state of the art algorithm of Christensen, H\u00fcttel and Stirling consisting of two semi-decision procedures running in parallel, which prohibits any complexity estimation. The point of our algorithm is the effective construction of a finite relation characterizing all bisimulation equivalence classes, whose mere existence was exploited for the above mentioned decidability result.", "num_citations": "101\n", "authors": ["1616"]}
{"title": "Active learning for extended finite state machines\n", "abstract": " We present a black-box active learning algorithm for inferring extended finite state machines (EFSM)s by dynamic black-box analysis. EFSMs can be used to model both data flow and control behavior of software and hardware components. Different dialects of EFSMs are widely used in tools for model-based software development, verification, and testing. Our algorithm infers a class of EFSMs called register automata. Register automata have a finite control structure, extended with variables (registers), assignments, and guards. Our algorithm is parameterized on a particular theory, i.e., a set of operations and tests on the data domain that can be used in guards.               Key to our learning technique is a novel learning model based on so-called tree queries. The learning algorithm uses tree queries to infer symbolic data constraints on parameters, e.g., sequence numbers, time stamps, identifiers, or even\u00a0\u2026", "num_citations": "100\n", "authors": ["1616"]}
{"title": "Service-oriented design: the roots\n", "abstract": " Service-Oriented Design has driven the development of telecommunication infrastructure and applications, in particular the so-called Intelligent Network (IN) Services, since the early 90s. A service-oriented, feature-based architecture, a corresponding standardization of basic services and applications in real standards, and adequate programming environments enabled flexibilization of services, and dramatically reduced the time to market. Today the current trend toward triple-play services, which blend voice, video, and data on broadband wireline and wireless services builds on this successful experience when reaching for new technological and operational challenges. In this paper, we review our 10 years of experience in service engineering for telecommunication systems from the point of view of Service-Oriented Design then and now.", "num_citations": "96\n", "authors": ["1616"]}
{"title": "Lightweight coarse-grained coordination: a scalable system-level approach\n", "abstract": " In this paper, our solution to the problem of modelling functionally complex communication systems at the application level, based on lightweight coordination, is extended to seamlessly capture system-level testing as well. This extension could be realized simply by self-application: the bulk of the work for integrating system-level testing into our development environment, the ABC, concerned domain modelling, which can be done using the ABC. Therefore, the extension of the ABC to cover system-level testing was merely an application development on the basis of the ABC, illustrated here in the domain of Computer Telephony Integration. Here the adoption of a coarse-grained approach to test design, which is central to the scalability of the overall testing environment, is the enabling aspect for system-level test automation. Together with our lightweight coordination approach this induces an understandable\u00a0\u2026", "num_citations": "95\n", "authors": ["1616"]}
{"title": "Simplicity as a driver for agile innovation\n", "abstract": " Software and hardware vendors long avoided interoperation for fear of opting out of their own product lines. Yet decisive change came to the automobile industry from a holistic evolution and maturation on many fronts.", "num_citations": "93\n", "authors": ["1616"]}
{"title": "The value flow graph: A program representation for optimal program transformations\n", "abstract": " Data flow analysis algorithms for imperative programming languages can be split into two groups: first, into the semantic algorithms that determine semantic equivalence between terms, and second, into the syntactic algorithms that compute complex program properties based on syntactic term identity, which support powerful optimization techniques like for example partial redundancy elimination. Value Flow Graphs represent semantic equivalence of terms syntactically. This allows us to feed the knowledge of semantic equivalence into syntactic algorithms. The power of this technique, which leads to modularly extendable algorithms, is demonstrated by developing a two stage algorithm for the optimal placement of computations within a program wrt the Herbrand interpretation.", "num_citations": "93\n", "authors": ["1616"]}
{"title": "Next generation learnlib\n", "abstract": " The Next Generation LearnLib (NGLL) is a framework for model-based construction of dedicated learning solutions on the basis of extensible component libraries, which comprise various methods and tools to deal with realistic systems including test harnesses, reset mechanisms and abstraction/refinement techniques. Its construction style allows application experts to control, adapt, and evaluate complex learning processes with minimal programming expertise.", "num_citations": "90\n", "authors": ["1616"]}
{"title": "The power of assignment motion\n", "abstract": " Assignment motion (AM) and expression motion (EM) are the basis of powerful and at the first sight incomparable techniques for removing partially redundant code from a program. Whereas AM aims at the elimination of complete assignments, a transformation which is always desirable, the more flexible EM requires temporaries to remove partial redundancies. Based on the observation that a simple program transformation enhances AM to subsume EM, we develop an algorithm that for the first time captures all second order effects between AM and EM transformations. Under usual structural restrictions, the worst case time complexity of our algorithm is essentially quadratic, a fact which explains the promising experience with our implementation.", "num_citations": "90\n", "authors": ["1616"]}
{"title": "Automata learning with automated alphabet abstraction refinement\n", "abstract": " Abstraction is the key when learning behavioral models of realistic systems, but also the cause of a major problem: the introduction of non-determinism. In this paper, we introduce a method for refining a given abstraction to automatically regain a deterministic behavior on-the-fly during the learning process. Thus the control over abstraction becomes part of the learning process, with the effect that detected non-determinism does not lead to failure, but to a dynamic alphabet abstraction refinement. Like automata learning itself, this method in general is neither sound nor complete, but it also enjoys similar convergence properties even for infinite systems as long as the concrete system itself behaves deterministically, as illustrated along a concrete example.", "num_citations": "88\n", "authors": ["1616"]}
{"title": "jETI: A tool for remote tool integration\n", "abstract": " We present jETI, a redesign of the Electronic Tools Integration platform (ETI), that addresses the major issues and concerns accumulated over seven years of experience with tool providers, tool users and students. Most important was here the reduction of the effort for integrating and updating tools. jETI combines Eclipse with Web Services functionality in order to provide (1) lightweight remote component (tool) integration, (2) distributed component (tool) libraries, (3) a graphical coordination environment, and (4) a distributed execution environment. These features will be illustrated in the course of building and executing remote heterogeneous tool sequences.", "num_citations": "87\n", "authors": ["1616"]}
{"title": "A constraint oriented proof methodology based on modal transition systems\n", "abstract": " We present a constraint-oriented state-based proof methodology for concurrent software systems which exploits compositionality and abstraction for the reduction of the verification problem under investigation. Formal basis for this methodology are Modal Transition Systems allowing loose state-based specifications, which can be refined by successively adding constraints. Key concepts of our method are projective views, separation of proof obligations, Skolemization and abstraction. Central to the method is the use of Parametrized Modal Transition Systems. The method easily transfers to real-time systems, where the main problem are parameters in timing constraints.", "num_citations": "85\n", "authors": ["1616"]}
{"title": "Property-oriented expansion\n", "abstract": " The paper develops a framework for property-oriented expansion, which is much more powerful than the state of the art (motion-based) approaches, supports the combination of transformations, and is open to automatic generation by means of synthesis. The power of our method comes at the price of an exponential worst case complexity, which, however, hardly shows up in practice: usually the algorithm behaves very moderately and provides results, which are essentially of the same size as the argument program. Power and limitations of property-oriented expansion are illustrated by means of algorithms, which are unique in eliminating all partial redundancies and all partially dead code.", "num_citations": "84\n", "authors": ["1616"]}
{"title": "Bio-jETI: a framework for semantics-based service composition\n", "abstract": " The development of bioinformatics databases, algorithms, and tools throughout the last years has lead to a highly distributed world of bioinformatics services. Without adequate management and development support, in silico researchers are hardly able to exploit the potential of building complex, specialized analysis processes from these services. The Semantic Web aims at thoroughly equipping individual data and services with machine-processable meta-information, while workflow systems support the construction of service compositions. However, even in this combination, in silico researchers currently would have to deal manually with the service interfaces, the adequacy of the semantic annotations, type incompatibilities, and the consistency of service compositions. In this paper, we demonstrate by means of two examples how Semantic Web technology together with an adequate domain modelling frees in silico researchers from dealing with interfaces, types, and inconsistencies. In Bio-jETI, bioinformatics services can be graphically combined to complex services without worrying about details of their interfaces or about type mismatches of the composition. These issues are taken care of at the semantic level by Bio-jETI's model checking and synthesis features. Whenever possible, they automatically resolve type mismatches in the considered service setting. Otherwise, they graphically indicate impossible/incorrect service combinations. In the latter case, the workflow developer may either modify his service composition using semantically similar services, or ask for help in developing the missing mediator that correctly bridges the detected\u00a0\u2026", "num_citations": "82\n", "authors": ["1616"]}
{"title": "Hierarchical service definition\n", "abstract": " The most innovative feature of INXpress, Siemens' solution to global advanced Intelligent Networks, is its advanced Service Definition environment ASD. In this paper we present a tool for the hierarchical definition of intelligent network services, which is realized by means of IN-MetaFrame. Technically, hierarchical service structure is introduced by means of a powerful macro mechanism, which allows developers to define whole subservices as primitive entities, which can be used just as Service Independent Building Blocks. As macros may be defined on-line and expanded whenever their interna become relevant, this supports a truly hierarchical service construction. Moreover, as macros have formally the same interfaces as SIBs, this supports the reuse of already designed (sub-) services. The macro concept of IN-MetaFrame goes hand in hand with its formal verification and abstract views features.", "num_citations": "81\n", "authors": ["1616"]}
{"title": "Efficient test-based model generation for legacy reactive systems\n", "abstract": " We present the effects of using an efficient algorithm for behavior-based model synthesis which is specifically tailored to reactive (legacy) system behaviors. Conceptual backbone is the classical automata learning procedure L*, which we adapt according to the considered application profile. The resulting learning procedure L*Meal , which directly synthesizes generalized Mealy automata from behavioral observations gathered via an automated test environment, drastically outperforms the classical learning algorithm for deterministic finite automata. Thus it marks a milestone towards opening industrial legacy systems to model-based test suite enhancement, test coverage analysis, and online testing.", "num_citations": "80\n", "authors": ["1616"]}
{"title": "Pushdown processes: Parallel composition and model checking\n", "abstract": " In this paper, we consider a strict generalization of context-free processes, the pushdown processes, which are particularly interesting for three reasons: First, in contrast to context-free processes that do not support the construction of distributed systems, they are closed under parallel composition with finite state systems. Second, they are the smallest extension of context-free processes allowing parallel composition with finite state processes. Third, they can be model checked by means of an elegant adaptation to pushdown automata of the second order model checker introduced in [BuS92]. As arbitrary parallel composition between context-free processes provides Turing power, and therefore destroys every hope for automatic verification, pushdown processes can be considered as the appropriate generalization of context-free processes for frameworks for automatic verification.", "num_citations": "79\n", "authors": ["1616"]}
{"title": "Continuous model-driven engineering\n", "abstract": " CMDE has been successfully applied in several industrial projects, including telecommunication services, supply-chain management, bioinformatics, logistics, and healthcare. In all these cases, agility at the customer, user, and application level proved key to aligning and linking business and IT. We now expect an additional boost when integrating this approach into a processor project-management environment that oversees development and evolution. This environment will include deadline management and progress reports, automatically informing all relevant parties when certain actions are required, managing different versions and product lines, and automatically steering the build and quality-management process. Perhaps not surprisingly, the development of this management environment proved to be a prime application of CMDE.", "num_citations": "78\n", "authors": ["1616"]}
{"title": "Remote integration and coordination of verification tools in jETI\n", "abstract": " We present jETI, a redesign of the Electronic Tools Integration platform (ETI), that addresses the major issues and concerns accumulated over seven years of experience with tool providers, tool users and students. Most important was here the reduction of the effort for integrating and updating tools. jETI combines Eclipse and Web Services functionality with our Application Building Center (ABC), a framework supporting coordination based application development. This provides (1) lightweight remote component (tool) integration, (2) distributed component (tool) libraries, (3) a graphical coordination environment, and (4) a distributed execution environment. We illustrate these features by discussing their impact on the typical jETI user groups: the Tool Providers, the Standard Tool Users, and the Advanced Tool Users.", "num_citations": "77\n", "authors": ["1616"]}
{"title": "METAFrame in Practice: Design of Intelligent Network Services\n", "abstract": " In this paper we present Meta Frame, an environment for formal methods-based, application-specific software design. Characteristic for Meta Frame are the following features: library-based development, meaning software construction by combination of components on a coarse granular level, incremental formalization, through successive enrichment of a special-purpose development environment, and library-based consistency checking, allowing continuous verification of application-and purpose-specific properties by means of model checking. These features and their impact for application developers and end users will be illustrated along an industrial application, the design of intelligent network (IN) services.", "num_citations": "76\n", "authors": ["1616"]}
{"title": "The fixpoint-analysis machine\n", "abstract": " We present a fixpoint-analysis machine, for the efficient computation of homogeneous, hierarchical, and alternating fixpoints over regular, context-free/push-down and macro models. Applications of such fix-point computations include intra- and interprocedural data flow analysis, model checking for various temporal logics, and the verification of behavioural relations between distributed systems. The fixpoint-analysis machine identifies an adequate (parameterized) level for a uniform treatment of all those problems, which, despite its uniformity, outperforms the \u2018standard iteration based\u2019 special purpose tools usually by factors around 10, even if the additional compilation time is taken into account.", "num_citations": "74\n", "authors": ["1616"]}
{"title": "Bisimulation collapse and the process taxonomy\n", "abstract": " We consider the factorization (collapse) of infinite transition graphs wrt. bisimulation equivalence. It turns out that almost none of the more complex classes of the process taxonomy, which has been established in the last years, are preserved by this operation. However, for the class of BPA graphs (i.e. prefix transition graphs of context-free grammars) we can show that the factorization is effectively a regular graph, i.e. finitely representable by means of a deterministic hypergraph grammar. Since finiteness of regular graphs is decidable, this yields, as a corollary, a decision procedure for the finiteness problem of context-free processes wrt. bisimulation equivalence.", "num_citations": "73\n", "authors": ["1616"]}
{"title": "Composition, decomposition and model checking of pushdown processes\n", "abstract": " In this paper, we consider a strict generalization of context-free processes, the pushdown processes, which are particularly interesting for three reasons: First, they are closed under parallel composition with finite state systems. This is shown by proving a new expansion theorem, whose implied 'representation explosion' is no worse than for finite state systems. Second, they axe the smallest extension of context-free processes allowing parallel composition with finite state processes, which we prove by showing that every pushdown process is bisimilar to a (relabelled) parallel composition of a context-free process (namely a stack) with some finite process. Third, they can be model checked by means of an elegant adaptation to pushdown automata of the second order model checker known for context-free processes. As arbitrary parallel composition between context-free processes provides Turing power, and\u00a0\u2026", "num_citations": "71\n", "authors": ["1616"]}
{"title": "Characteristic formulae\n", "abstract": " Characteristic formulae have been introduced by Graf and Sifakis to relate equational reasoning about processes to reasoning in a modal logic, and therefore to allow proofs about processes to be carried out in a logical framework. Based upon an intuitionistic understanding of the modal mu-calculus, this paper extends the results of Graf and Sifakis in two respects. First, it covers not only finite processes, but finite state processes. Second, it handles not only bisimulation-like equivalences but also preorders, which are sensitive to liveness properties.", "num_citations": "69\n", "authors": ["1616"]}
{"title": "Bio-jETI: a service integration, design, and provisioning platform for orchestrated bioinformatics processes\n", "abstract": " With Bio-jETI, we introduce a service platform for interdisciplinary work on biological application domains and illustrate its use in a concrete application concerning statistical data processing in R and xcms for an LC/MS analysis of FAAH gene knockout. Bio-jETI uses the jABC environment for service-oriented modeling and design as a graphical process modeling tool and the jETI service integration technology for remote tool execution. As a service definition and provisioning platform, Bio-jETI has the potential to become a core technology in interdisciplinary service orchestration and technology transfer. Domain experts, like biologists not trained in computer science, directly define complex service orchestrations as process models and use efficient and complex bioinformatics tools in a simple and intuitive way.", "num_citations": "65\n", "authors": ["1616"]}
{"title": "Learning register automata: from languages to program structures\n", "abstract": " This paper reviews the development of Register Automaton learning, an enhancement of active automata learning to deal with infinite-state systems. We will revisit the precursor techniques and influences, which in total span over more than a decade. A large share of this development was guided and motivated by the increasingly popular application of grammatical inference techniques in the field of software engineering. We specifically focus on a key problem to achieve practicality in this field: the adequate treatment of data values ranging over infinite domains, a major source of undecidability. Starting with the first case studies, in which data was completely abstracted away, we revisit different steps towards dealing with data explicitly at a model level: we discuss Mealy machines as a model for systems with (data) output, automated alphabet abstraction refinement techniques as a two-dimensional\u00a0\u2026", "num_citations": "63\n", "authors": ["1616"]}
{"title": "Test-based model generation for legacy systems\n", "abstract": " We study the extension of applicability of system-level testing techniques to the construction of a consistent model of (legacy) systems under test, which are seen as black boxes. We gather observations via an automated test environment and systematically extend available test suites according to learning procedures. Testing plays two roles here: (i) as an application domain and (ii) as the enabling technology for the adopted learning technique. The benefits include enhanced error detection and diagnosis, both during the testing phase and the online test of deployed systems at customer sites.", "num_citations": "63\n", "authors": ["1616"]}
{"title": "Efficient regression testing of CTI-systems: Testing a complex call-center solution\n", "abstract": " In this paper we show how the workfllow-oriented, library-based test design and execution environment presented in [8] fulfills the promise of enabling efficient integrated system-level tests of complex industrial applications. We exemplify on the basis of a concete case study (Siemens HPCO Application, a complex Call-Center Solution) how test engineers can now work with the Integrated Test Environment. Efficiency measurements in the field document an average performance gain in the execution of functional regression tests of factors beyond 30: setting up the test scenario is now the most expensive activity!", "num_citations": "61\n", "authors": ["1616"]}
{"title": "Module configuration by minimal model construction\n", "abstract": " We present a framework for the automatic configuration of large systems from a library of reusable software components. Core of the framework is a modal logic that uniformly and elegantly captures type descriptions, module specifications and relative time. Whereas the first twodimensions' are treated similarly by means of a simple logic over a taxonomy of types and modules respectively, time is expressed by means of modalities. Besides allowing an elegant and transparent specification of module configurations, our framework also provides a minimal model generator that automatically generates minimal solutions to a specification problem. All this is illustrated for a practically relevant application: the automatic configuration of heterogeneous analysis systems from loose specifications.", "num_citations": "59\n", "authors": ["1616"]}
{"title": "Lazy strength reduction\n", "abstract": " We present a bit-vector algorithm that uniformly combines code motion and strength reduction, avoids superfluous register pressure due to unnecessary code motion, and is as efficient as standard unidirectional analyses. The point of this algorithm is to combine the concept of lazy code motion of [1] with the concept of unifying code motion and strength reduction of [2, 3, 4]. This results in an algorithm for lazy strength reduction, which consists of a sequence of unidirectional analyses, and is unique in its transformational power. Keywords: Data flow analysis, program optimization, partial redundancy elimination, code motion, strength reduction, bit-vector data flow analyses. 1 Motivation Code motion improves the runtime efficiency of a program by avoiding unnecessary recomputations of a value at runtime. Strength reduction improves runtime efficiency by reducing\" expensive\" recomputations to less expensive ones, eg, by reducing computations involving multiplication to computatio...", "num_citations": "57\n", "authors": ["1616"]}
{"title": "Detecting equalities of variables: Combining efficiency with precision\n", "abstract": " Detecting whether different variables have the same value at a program point is generally undecidable. Though the subclass of equalities, whose validity holds independently from the interpretation of operators (Herbrand-equivalences), is decidable, the technique which is most widely implemented in compilers, value numbering, is restricted to basic blocks. Basically, there are two groups of algorithms aiming at globalizations of value numbering: first, a group of algorithms based on the algorithm of Kildall, which uses data flow analysis to gather information on value equalities. These algorithms are complete in detecting Herbrand-equivalences, however, expensive in terms of computational complexity. Second, a group of algorithms influenced by the algorithm of Alpern, Wegman and Zadeck. They do not fully interpret the control flow, which allows them to be particularly efficient, however, at the price of\u00a0\u2026", "num_citations": "56\n", "authors": ["1616"]}
{"title": "Genesys: service-oriented construction of property conform code generators\n", "abstract": " This paper presents Genesys, a framework for the high-level construction of property conform code generators. Genesys is an integral part of jABC, a flexible framework designed to enable a systematic model-driven development of systems and applications on the basis of an (extensible) library of well-defined, reusable building blocks. Within jABC, Genesys manages the construction of code generators for jABC\u2019s models. So far, Genesys has been used to develop code generators for a variety of different target platforms, like a number of Java-based platforms, mobile devices, BPEL engines, etc. Since the code generators are themselves built within the jABC in a model-driven way, concepts like bootstrapping and reuse of existing components enable a fast evolution of Genesys\u2019 code generation library, and a high degree of self-application. Due to its increasing complexity and its high degree of reuse\u00a0\u2026", "num_citations": "55\n", "authors": ["1616"]}
{"title": "Incremental formalization: a key to industrial success\n", "abstract": " CiteSeerX \u2014 Incremental Formalization: a Key to Industrial Success Documents Authors Tables Log in Sign up MetaCart DMCA Donate CiteSeerX logo Documents: Advanced Search Include Citations Authors: Advanced Search Include Citations Tables: DMCA Incremental Formalization: a Key to Industrial Success (1995) Cached Download as a PDF Download Links [www.fmi.uni-passau.de] [sunshine.cs.uni-dortmund.de] Other Repositories/Bibliography DBLP Save to List Add to Collection Correct Errors Monitor Changes by Bernhard Steffen , Tiziana Margaria , Andreas Cla\u00dfen , Volker Braun Citations: 7 - 6 self Summary Citations Active Bibliography Co-citation Clustered Documents Version History Share Facebook Twitter Reddit Bibsonomy OpenURL Abstract Keyphrases incremental formalization industrial success Powered by: Apache Solr About CiteSeerX Submit and Index Documents Privacy Policy Help \u2026", "num_citations": "55\n", "authors": ["1616"]}
{"title": "GeneFisher-P: variations of GeneFisher as processes in Bio-jETI\n", "abstract": " PCR primer design is an everyday, but not trivial task requiring state-of-the-art software. We describe the popular tool GeneFisher and explain its recent restructuring using workflow techniques. We apply a service-oriented approach to model and implement GeneFisher-P, a process-based version of the GeneFisher web application, as a part of the Bio-jETI platform for service modeling and execution. We show how to introduce a flexible process layer to meet the growing demand for improved user-friendliness and flexibility. Within Bio-jETI, we model the process using the jABC framework, a mature model-driven, service-oriented process definition platform. We encapsulate remote legacy tools and integrate web services using jETI, an extension of the jABC for seamless integration of remote resources as basic services, ready to be used in the process. Some of the basic services used by GeneFisher are in fact already provided as individual web services at BiBiServ and can be directly accessed. Others are legacy programs, and are made available to Bio-jETI via the jETI technology. The full power of service-based process orientation is required when more bioinformatics tools, available as web services or via jETI, lead to easy extensions or variations of the basic process. This concerns for instance variations of data retrieval or alignment tools as provided by the European Bioinformatics Institute (EBI). The resulting service- and process-oriented GeneFisher-P demonstrates how basic services from heterogeneous sources can be easily orchestrated in the Bio-jETI platform and lead to a flexible family of specialized processes tailored to specific tasks.", "num_citations": "50\n", "authors": ["1616"]}
{"title": "Library-based design and consistency checking of system-level industrial test cases\n", "abstract": " In this paper we present a new coarse grain approach to automated integrated (functional) testing, which combines three paradigms: library-based test design, meaning construction of test graphs by combination of test case components on a coarse granular level, incremental formalization, through successive enrichment of a special-purpose environment for application-specific test development and execution, and library-based consistency checking, allowing continuous verification of application- and aspect-specific properties by means of model checking. These features and their impact for the test process and the test engineers are illustrated along an industrial application: an automated integrated testing environment for CTI-Systems.", "num_citations": "49\n", "authors": ["1616"]}
{"title": "Constraint-based inter-procedural analysis of parallel programs\n", "abstract": " We provide a uniform framework for the analysis of programs with procedures and explicit, unbounded, fork/join parallelism covering not only bitvector problems like reaching definitions or live variables but also non-bitvector problems like simple constant propagation. Due to their structural similarity to the sequential case, the resulting algorithms are as efficient as their widely accepted sequential counterparts, and they can easily be integrated in existing program analysis environments like e.g. MetaFrame or PAG. We are therefore convinced that our method will soon find its way into industrial-scale computer systems.", "num_citations": "49\n", "authors": ["1616"]}
{"title": "The MetaFrame'95 Environment\n", "abstract": " METAFrame is a meta-level framework designed to offer a sophisticated support for the systematic and structured computer aided generation of applicationspecific complex objects from collections of reusable components. Figure 1 shows its overall organization. Special care has been taken in the design of an adequate, almost natural-language specification language, of a userJriendly graphical interface, of a hypertext based navigation tool, and a of semi-automatic synthesis process and repository management. This application-independent core is complemented by application-specific libraries of components, which constitute the objects of the synthesis. The principle of separating the component implementation from its description is systematically enforced: for each application we have a distinct Meta-Data repository containing a logic view of the components. The tools themselves and their documentation are\u00a0\u2026", "num_citations": "48\n", "authors": ["1616"]}
{"title": "An environment for the creation of intelligent network services\n", "abstract": " This paper presents a Service Creation Environment which is unique in offering global correctness and consistency checks. These guarantee frame conditions for the design concerning implementability, country specific standards and network specific features. Frame conditions are formulated in a modal logic and verified via model checking. The described environment offers a lazy and incremental use of formal methods: if no formal constraints are defined, the system behaves like standard systems for service creation. However, the more constraints are added, the more reliable are the created services. This provides asoft'entry into the world of formal methods. 1 Service Creation Intelligent Network (IN) services are customized telephone services, like eg, 1)FreePhone', where the receiver of the call can be billed if some conditions are met, 2)Universal Private Telephone', enabling groups of customers to define their own private net within the public net, or 3)Partner Lines', wh...", "num_citations": "48\n", "authors": ["1616"]}
{"title": "Active continuous quality control\n", "abstract": " We present Active Continuous Quality Control (ACQC), a novel approach that employs incremental active automata learning technology periodically in order to infer evolving behavioral automata of complex applications accompanying the development process. This way we are able to closely monitor and steer the evolution of applications throughout their whole life-cycle with minimum manual effort. Key to this approach is to establish a stable level for comparison via an incrementally growing behavioral abstraction in terms of a user-centric communication alphabet: The letters of this alphabet, which may correspond to whole use cases, are intended to directly express the functionality from the user perspective. At the same time their choice allows one to focus on specific aspects, which establishes tailored abstraction levels on demand, which may be refined by adding new letters in the course of the systems\u00a0\u2026", "num_citations": "47\n", "authors": ["1616"]}
{"title": "Incremental requirement specification for evolving systems\n", "abstract": " We propose a technique for hierarchically structuring requirement specifications in a way that simplifies change management. Our technique decomposes the process of requirement specification and validation. Specifications, added or modified incrementally on various levels of abstraction, can directly be subject to automatic verification, thus providing early feedback concerning design decisions or the impact of adding or changing service functionality. Thus it is possible to maintain consistency between (incomplete) requirement specifications and the underlying evolving implementation model.", "num_citations": "46\n", "authors": ["1616"]}
{"title": "A succinct canonical register automaton model\n", "abstract": " We present a novel canonical automaton model, based on register automata, that can easily be used to specify protocol or program behavior. More concretely, register automata are reminiscent of control flow graphs: they comprise a finite control structure, assignments, and conditionals, allowing to assign values of an infinite domain to registers (variables) and to compare them for equality. A major contribution is the definition of a canonical automaton representation of any language recognizable by a deterministic register automaton, by means of a Nerode congruence. Not only is this canonical form easier to comprehend than previous proposals, but it can also be exponentially more succinct than these. Key to the canonical form is the symbolic treatment of data languages, which overcomes the structural restrictions in previous formalisms, and opens the way to new practical applications.", "num_citations": "45\n", "authors": ["1616"]}
{"title": "Seven variations of an alignment workflow-An illustration of agile process design and management in Bio-jETI\n", "abstract": " This paper shows how the agility provided by the Bio-jETI platform helps to interactively design bioinformatics analysis processes. Bio-jETI is a platform for the integration, orchestration and provision of services. The agility in design and execution is demonstrated by developing seven variations on a multiple sequence alignment workflow.", "num_citations": "44\n", "authors": ["1616"]}
{"title": "Model-based design of distributed collaborative bioinformatics processes in the jABC\n", "abstract": " Our approach to the model-driven collaborative design of workflows for bioinformatic applications uses the jABC for model driven mediation and choreography to complement a Web service-based elementary service provision. jABC is a framework for service development based on lightweight process coordination. Users (product developers and system/software designers) develop services and applications by composing reusable building-blocks into (flow-)graph structures that can be animated, analyzed, simulated, verified, executed, and compiled. This way of handling the collaborative design of complex processes has proven to be effective and adequate for the cooperation of non-programmers (in this case biologists) and technical people, and it is now being rolled out in the operative practice", "num_citations": "44\n", "authors": ["1616"]}
{"title": "Full life-cycle support for end-to-end processes\n", "abstract": " Fully supporting end-to-end processes requires combining service orientation-which takes an engineering approach to reducing the gap between software requirements and implementation-with model-driven design-which addresses the same problem in terms of well-understood mathematical structures. The Centre of Innovation for Service-Centered Continuous Engineering (www.scce.info) adopts a holistic approach to closing the classical gap between business-driven requirements and IT-based realization. We provide a seamless method and a matching toolset based on the Java Application Building Center (jABC) framework (www. jabc.de) to cover the business development phase and the business-to-IT transition. The NetBeans Integrated Development Environment (IDE) supports IT development and deployment (www.netbeans.org).", "num_citations": "43\n", "authors": ["1616"]}
{"title": "Continuous modeling of real-time and hybrid systems: from concepts to tools\n", "abstract": " The past decade has witnessed arapid development in the field of formal methods for the specification, analysis and verification of real-time systems. Particularly striking is the progress in continuous time modeling, which, despite its unquestioned expressiveness, turned out to be surprisingly tractable: practically relevant classes of continuous time systems can be analyzed and verified fully automatically. This has led to the development of anumber of corresponding analysis and verification tools of different application profiles. In this paper we concentrate on the two key concepts underlying these tools, known as timed automata and hybrid systems. Their role can be best appreciated in the context of formal methods in general, and specifically of specification of real-time systems in terms of tailored process calculi and real-time logics. All these concepts will be presented in an intuitive fashion, avoiding as much formalism as possible.", "num_citations": "43\n", "authors": ["1616"]}
{"title": "Heterogeneous analysis and verification for distributed systems\n", "abstract": " In this paper we present an environment for the development of special purpose heterogeneous analysis and verification tools, which is unique in 1) constituting a framework for the development of application specific heterogeneous tools and 2) providing facilities for the automation of the synthesis process. Based on a specification language that uniformly combines taxonomic component specifications, interface conditions, and ordering constraints, our method adds a global view to conventional single component retrieval. Following a user session, we illustrate the interactive synthesis process, which supports the inclusion of a satisfactory new software component into the repository by proposing an appropriately precomputed default taxonomic classification. This guarantees convenient retrieval for later reuse.", "num_citations": "43\n", "authors": ["1616"]}
{"title": "Prototype-driven development of web applications with DyWA\n", "abstract": " In this paper we present an approach to the user-driven development of process-oriented web applications that combines business process modeling with user-side application domain evolution. In the center is the DyWA framework that accompanies the prototype-driven web-application development from the domain modeling through the development and deployment phase to the actual runtime and later product evolution: Using DyWA, application experts without programming knowledge are able to model (according to their professional knowledge and understanding) both domain-specific data models and the business process models that act on the data via automatically generated elementary data operations. The resulting business processes integrate data access and manipulation, and directly constitute executable prototypes of the resulting web application. All this is illustrated for OCS-lite, a cut\u00a0\u2026", "num_citations": "42\n", "authors": ["1616"]}
{"title": "Inferring semantic interfaces of data structures\n", "abstract": " In this paper, we show how to fully automatically infer semantic interfaces of data structures on the basis of systematic testing. Our semantic interfaces are a generalized form of Register Automata (RA), comprising parameterized input and output, allowing to model control- and data-flow in component interfaces concisely. Algorithmic key to the automated synthesis of these semantic interfaces is the extension of an active learning algorithm for Register Automata to explicitly deal with output. We evaluated our algorithm on a complex data structure, a \u201cstack of stacks\u201d, the largest of which we could learn in merely 20 seconds with less than 4000 membership queries, resulting in a model with rougly 800 nodes. In contrast, even when restricting the data domain to just four values, the corresponding plain Mealy machine would have more than 109 states and presumably require billions of membership queries.", "num_citations": "42\n", "authors": ["1616"]}
{"title": "From zulu to rers\n", "abstract": " This paper summarizes our experience with the ZULU challenge on active learning without equivalence queries, presents our winning solution, investigates the character of ZULU\u2019s rating approach, and discusses how this approach can be taken further to establish a framework for the systematic investigation of domain-specific, scalable learning solutions for practically relevant application. In particular, it discusses the RERS initiative, which provides a community platform together with a learning framework that allows users to interactively compose complex learning solutions on the basis of libraries for various learning components, system connectors, and other auxiliary functionality. This framework will be the backbone for an extended challenge on learning in 2011.", "num_citations": "42\n", "authors": ["1616"]}
{"title": "DFA&OPT-MetaFrame: a tool kit for program analysis and optimization\n", "abstract": " Whereas the construction process of a compiler for the early and late phases like syntactic analysis and code generation is well-supported by powerful tools, the optimizer, the key component for achieving highly efficient code is usually still hand-coded. The tool kit presented here supports this essential step in the construction of a compiler. The two key features making it exceptional are (1) that it automatically generates global program analyses for intraprocedural, interprocedural and parallel data flow problems, and (2) that it supports the combination of the results obtained to program optimizations.", "num_citations": "42\n", "authors": ["1616"]}
{"title": "Backtracking-free design planning by automatic synthesis in metaframe\n", "abstract": " We present an environment supporting the flexible and application-specific construction of design plans, which avoids the insurgence of unsuccessful design plans at design time, and is thus backtracking-free. During a planning phase the collection of all complete, executable design plans is automatically synthesized on the basis of simple constraint-like specifications and the library of available tools. The designer's choice of the best alternative is eased by a user friendly graphical interface and by hypertext support for the generation and management of plans, as illustrated along a user session. Example application field is the generation of design plans in a CAD environment for hardware design.", "num_citations": "40\n", "authors": ["1616"]}
{"title": "A constraint-oriented service creation environment\n", "abstract": " Intelligent Network (IN) services are customized telephone services, like eg, 1)'Free-Phone', where the receiver of the call can be billed if some conditions are met, 2)'Universal Private Telephone', enabling groups of customers to define their own private net within the public net, or 3)'Partner Lines', where a number of menus leads to the satisfaction of all desires. The realization of these services is quite complex and error prone. The current trend in advanced IN services clearly evolves towards decoupling Service Processing Systems from the switch network (see eg [3]). The reasons for this tendency lie in the growing need for decentralization of the service processing, in the demand for quick customization of the offered services, and in the requirement of rapid availability of the modified or reconfigured services. Service Processing Systems are those elements of the IN architecture which provide service processing\u00a0\u2026", "num_citations": "40\n", "authors": ["1616"]}
{"title": "Tool-supported enhancement of diagnosis in model-driven verification\n", "abstract": " We show on a case study from an autonomous aerospace context how to apply a game-based model-checking approach as a powerful technique for the verification, diagnosis, and adaptation of system behaviors based on temporal properties. This work is part of our contribution within the SHADOWS project, where we provide a number of enabling technologies for model-driven self-healing. We propose here to use GEAR, a game-based model checker, as a user-friendly tool that can offer automatic proofs of critical properties of such systems. Although it is a model checker for the full modal\u00a0\u03bc-calculus, it also supports derived, more user-oriented logics. With GEAR, designers and engineers can interactively investigate automatically generated winning strategies for the games, by this way exploring the connection between the property, the system, and the proof.", "num_citations": "38\n", "authors": ["1616"]}
{"title": "Knowledge-based relevance filtering for efficient system-level test-based model generation\n", "abstract": " Test-based model generation by classical automata learning is very expensive. It requires an impractically large number of queries to the system, each of which must be implemented as a system-level test case. Key in the tractability of observation-based model generation are powerful optimizations exploiting different kinds of expert knowledge in order to drastically reduce the number of required queries, and thus the testing effort. In this paper, we present a thorough experimental analysis of the second-order effects between such optimizations in order to maximize their combined impact.", "num_citations": "38\n", "authors": ["1616"]}
{"title": "Active automata learning in practice\n", "abstract": " Active automata learning is slowly becoming a standard tool in the toolbox of the software engineer. As systems become ever more complex and development becomes more distributed, inferred models of system behavior become an increasingly valuable asset for understanding and analyzing a system\u2019s behavior. Five years ago (in 2011) we have surveyed the then current state of active automata learning research and applications of active automata learning in practice. We predicted four major topics to be addressed in the then near future: efficiency, expressivity of models, bridging the semantic gap between formal languages and analyzed components, and solutions to the inherent problem of incompleteness of active learning in black-box scenarios. In this paper we review the progress that has been made over the past five years, assess the status of active automata learning techniques with respect to\u00a0\u2026", "num_citations": "37\n", "authors": ["1616"]}
{"title": "Learning extended finite state machines\n", "abstract": " We present an active learning algorithm for inferring extended finite state machines (EFSM)s, combining data flow and control behavior. Key to our learning technique is a novel learning model based on so-called tree queries. The learning algorithm uses the tree queries to infer symbolic data constraints on parameters, e.g., sequence numbers, time stamps, identifiers, or even simple arithmetic. We describe sufficient conditions for the properties that the symbolic constraints provided by a tree query in general must have to be usable in our learning model. We have evaluated our algorithm in a black-box scenario, where tree queries are realized through (black-box) testing. Our case studies include connection establishment in TCP and a priority queue from the Java Class Library.", "num_citations": "35\n", "authors": ["1616"]}
{"title": "Code motion for explicitly parallel programs\n", "abstract": " In comparison to automatic parallelization, which is thoroughly studied in the literature [31, 33], classical analyses and optimizations of explicitly parallel programs were more or less neglected. This may be due to the fact that naive adaptations of the sequential techniques fail [24], and their straightforward correct ones have unacceptable costs caused by the interleavings, which manifest the possible executions of a parallel program. Recently, however, we showed that unidirectional bitvector analyses can be performed for parallel programs as easily and as efficiently as for sequential ones [17], a necessary condition for the successful transfer of the classical optimizations to the parallel setting.In this article we focus on possible subsequent code motion transformations, which turn out to require much more care than originally conjectured [17]. Essentially, this is due to the fact that interleaving semantics, although being\u00a0\u2026", "num_citations": "35\n", "authors": ["1616"]}
{"title": "Hybrid test of web applications with webtest\n", "abstract": " In this paper, we present hybrid testing, a method that combines replay-testing (static testing) with automata learning techniques that generate models of black box systems (dynamic testing). This combination helps bridging the gap towards model based testing also for legacy systems. Webtest is an implementation of hybrid testing that builds on top of preexisting technology such as the LearnLib, a framework for automata learning, and the jABC, a framework for model-driven and service-oriented design, that we use here for modelling, executing, and managing test suites for and models of the web applications under analysis. In particular we intend to move towards Rich Internet Applications (RIAs), that include eg advanced client side capabilities and access to heavy resources (eg database access) over the Web.", "num_citations": "34\n", "authors": ["1616"]}
{"title": "LTL guided planning: Revisiting automatic tool composition in ETI\n", "abstract": " We revisit the automatic tool composition feature of the electronic tool integration platform under the perspective of planning. It turns out that in todays terminology, ETIs temporal logic-based synthesis of tool sequences is a form of planning-based automated orchestration. In contrast to Al-based planning approaches, our synthesis approach is not restricted to compute one solution, but it may compute all (shortest/minimal) solutions, with the intent to provide maximum insight into the potential design space.", "num_citations": "34\n", "authors": ["1616"]}
{"title": "Unifying models\n", "abstract": " In this paper we illustrate the unifying power and flexibility of an operational model-based approach by treating the problem dilemma of lack of consistency between the various description methods used in software systems design. The success of this approach strongly relies on the definition of adequate unifying model structures, which must be powerful enough to capture the interference potential between the different description methods, while remaining simple enough to support (automatic) verification, the key for formal methods to enter industrial practice.", "num_citations": "34\n", "authors": ["1616"]}
{"title": "Automatic synthesis of linear process models from temporal constraints: An incremental approach\n", "abstract": " We present PM-MetaFrame, a tool for automatic synthesis (ie prior to any enaction) of linear process models from specifications written as global constraints in linear time temporal logic. Key to the automatic synthesis is its level of abstraction: process model components are described logically by a taxonomic classification, expressing semantic properties, and the flow of data is only considered on the level of types, which is sufficient to guarantee executability of synthesized process models. Thus our approach can be regarded as a high-level extension of existing process model approaches, where, if at all, global constraints are described on the data level. The result of the synthesis is the set of all executable linear process models satisfying the, typically very loose, temporal constraints. Incremental refinement of global constraints by strengthening/loosening via conjunction/disjunction helps minimizing the search space.", "num_citations": "33\n", "authors": ["1616"]}
{"title": "Towards a tool kit for the automatic generation of interprocedural data flow analyses\n", "abstract": " Frameworks for interprocedural data ow analysis (DFA) often have a foundational character: designing concrete applications requires usually a deep understanding of the framework. Here, we reconsider interprocedural DFA from an application-oriented point of view, where all details irrelevant for application are hidden. In this view the underlying framework, which captures programs with mutually recursive procedures, global and local variables, formal value and reference parameters, reduces to a cook book of (1) how to specify interprocedural DFAs and (2) how to prove them precise with respect to a program property of interest. Thus only knowledge about the speci cation level is required. Moreover, this presentation also yields the basis for a tool kit implementation: speci cation parameters and generic algorithms of the framework become speci cation tools allowing concise speci cations and computation tools processing them. A tool kit prototype has been implemented within MetaFrame, a programming environment for the construction and veri cation of complex software systems. The bene ts of the approach are demonstrated by a collection of practically relevant precise interprocedural DFAs ranging from classical bit-vector problems like available expressions and live variables to sophisticated and powerful optimizations like the optimal elimination of interprocedural partial redundancies.", "num_citations": "33\n", "authors": ["1616"]}
{"title": "Code motion and code placement: Just synonyms?\n", "abstract": " We prove that there is no difference between code motion (CM) and code placement (CP) in the traditional syntactic setting, however, a dramatic difference in the semantic setting. We demonstrate this by re-investigating semantic CM under the perspective of the recent development of syntactic CM. Besides clarifying and highlighting the analogies and essential differences between the syntactic and the semantic approach, this leads as a side-effect to a drastical reduction of the conceptual complexity of the value-flow based procedure for semantic CM of [20], as the original bidirectional analysis is decomposed into purely unidirectional components. On the theoretical side, this establishes a natural semantical understanding in terms of the Herbrand interpretation (transparent equivalence), and thus eases the proof of correctness; moreover, it shows the frontier of semantic CM, and gives reason for the lack of\u00a0\u2026", "num_citations": "32\n", "authors": ["1616"]}
{"title": "Data-flow analysis as model checking within the jABC\n", "abstract": " This paper describes how the jABC, a generic framework for library-based program development, and two of its plugins\u00a0- the Model Checker and a flow graph converter \u2013 form a framework for intraprocedural data-flow analysis via model checking. Based on functionalities provided by the Soot program analysis platform, the converter generates graph structures from Java classes. Data flow analyses are then expressed as formulas in the modal \u03bc-calculus. Executing the analysis is carried out by checking the validity of the formulas on the flow graph.               The tool demonstration will illustrate the interplay of the involved components, which elegantly provides a fully integrated implementation of Data-Flow Analysis as Model Checking in a software development environment.", "num_citations": "31\n", "authors": ["1616"]}
{"title": "Efficient code motion and an adaption to strength reduction\n", "abstract": " Common subexpression elimination, partia] redundancy elimination and loop invariant code motion, are all instances of the same general run-time optimization problem: how to optimally place computations within a program. In [SKR1] we presented a modular algorithm for this problem, which optimally moves computations within programs wrt Herbrand equivalence. In this paper we consider two elaborations of this algorithm~ which are dealt with in Part I and Part II, respectively. Part I deals with the problem that the full variant of the algorithm of [SKR1] may excessively introduce trivial redefinitions of registers in order to cover a single computation. Rosen, Wegman\u2022 and Zadeck avoided such a too excessive introduction of trivial redefinitions by means of some practically oriented restrictions, and they proposed an effffcient algorithm, which optimally moves the computations of acyclic flow graphs under these\u00a0\u2026", "num_citations": "31\n", "authors": ["1616"]}
{"title": "Higher-order process modeling: product-lining, variability modeling and beyond\n", "abstract": " We present a graphical and dynamic framework for binding and execution of business) process models. It is tailored to integrate 1) ad hoc processes modeled graphically, 2) third party services discovered in the (Inter)net, and 3) (dynamically) synthesized process chains that solve situation-specific tasks, with the synthesis taking place not only at design time, but also at runtime. Key to our approach is the introduction of type-safe stacked second-order execution contexts that allow for higher-order process modeling. Tamed by our underlying strict service-oriented notion of abstraction, this approach is tailored also to be used by application experts with little technical knowledge: users can select, modify, construct and then pass (component) processes during process execution as if they were data. We illustrate the impact and essence of our framework along a concrete, realistic (business) process modeling scenario: the development of Springer's browser-based Online Conference Service (OCS). The most advanced feature of our new framework allows one to combine online synthesis with the integration of the synthesized process into the running application. This ability leads to a particularly flexible way of implementing self-adaption, and to a particularly concise and powerful way of achieving variability not only at design time, but also at runtime.", "num_citations": "30\n", "authors": ["1616"]}
{"title": "Behavior-based model construction\n", "abstract": " In this paper, we review behavior-based model construction from a point of view characterized by verification, model checking and abstract interpretation. It turns out that abstract interpretation is the key for scaling known learning techniques for practical applications, model checking may serve as a teaching aid in the learning process underlying the model construction, and that there are also synergies with other validation and verification techniques. We will illustrate our discussion by means of a realistic telecommunication scenario, where the underlying system has grown over the last two decades, the available system documentation consists of not much more than user manuals and protocol standards, and the revision cycle times are extremely short. In this situation, behavior-based model construction provides a sound basis e.g. for test-suite design and maintenance, test organization, and test evaluation.", "num_citations": "30\n", "authors": ["1616"]}
{"title": "Sparse code motion\n", "abstract": " In this article, we add a third dimension to partial redundancy elimination by considering code size as a further optimization goal in addition to the more classical consideration of computation costs and register pressure. This results in a family of sparse code motion algorithms coming as modular extensions of the algorithms for busy and lazy code motion. Each of them optimally captures a predefined choice of priority between these three optimization goals, eg code size can be minimized while (1) guaranteeing at least the performance of the argument program, or (2) even computational optimality. Each of them can further be refined to simultaneously reduce the lifetimes of temporaries to a minimum. These algorithms are well-suited for size-critical application areas like smart cards and embedded systems, as they provide a handle to control the code replication problem of classical code motion techniques. In fact\u00a0\u2026", "num_citations": "29\n", "authors": ["1616"]}
{"title": "On handling data in automata learning\n", "abstract": " Most communication with real-life systems involves data values being relevant to the communication context and thus influencing the observable behavior of the communication endpoints. When applying methods from the realm of automata learning, it is necessary to handle such data-occurrences. In this paper, we consider how the techniques of automata learning can be adapted to the problem of learning interaction models in which data parameters are an essential element. Especially, we will focus on how test-drivers for real-word systems can be generated automatically. Our main contribution is an analysis of (1) the requirements on information contained in models produced by the learning enabler in the Connect project and (2) the resulting preconditions for generating test-drivers automatically.", "num_citations": "28\n", "authors": ["1616"]}
{"title": "Verification, Diagnosis and Adaptation: Tool supported enhancement of the model-driven verification process.\n", "abstract": " In this paper, we use a case study from an autonomous aerospace context as running example to show how to apply a game-based model-checking approach a as a powerful technique for the verification, diagnosis and adaptation of temporal properties. This work is part of our contribution within the SHADOWS project, where we provide a number of enabling technologies for model-driven self-healing. We propose here to use GEAR, a game-based model checker for the full modal \u00b5-calculus and derived, more user-oriented logics, as a user friendly tool that can offer automatic proofs of critical properties of such systems. Designers and engineers can interactively investigate automatically generated winning strategies for the games, this way exploring the connection between the property, the system, and the proof. 1", "num_citations": "28\n", "authors": ["1616"]}
{"title": "jMosel: A stand-alone tool and jABC plugin for M2L (Str)\n", "abstract": " jMosel is a tool-set for the analysis and verification of linear parametric systems in monadic second-order logic on strings. In this paper we give a short introduction to the underlying concepts, as well as an overview of the implementation and the usage of jMosel.", "num_citations": "28\n", "authors": ["1616"]}
{"title": "Flux-P: automating metabolic flux analysis\n", "abstract": " Quantitative knowledge of intracellular fluxes in metabolic networks is invaluable for inferring metabolic system behavior and the design principles of biological systems. However, intracellular reaction rates can not often be calculated directly but have to be estimated; for instance, via 13 C-based metabolic flux analysis, a model-based interpretation of stable carbon isotope patterns in intermediates of metabolism. Existing software such as FiatFlux, OpenFLUX or 13CFLUX supports experts in this complex analysis, but requires several steps that have to be carried out manually, hence restricting the use of this software for data interpretation to a rather small number of experiments. In this paper, we present Flux-P as an approach to automate and standardize 13 C-based metabolic flux analysis, using the Bio-jETI workflow framework. Exemplarily based on the FiatFlux software, it demonstrates how services can be created that carry out the different analysis steps autonomously and how these can subsequently be assembled into software workflows that perform automated, high-throughput intracellular flux analysis of high quality and reproducibility. Besides significant acceleration and standardization of the data analysis, the agile workflow-based realization supports flexible changes of the analysis workflows on the user level, making it easy to perform custom analyses. View Full-Text", "num_citations": "27\n", "authors": ["1616"]}
{"title": "Towards an architecture for runtime interoperability\n", "abstract": " Interoperability remains a fundamental challenge when connecting heterogeneous systems which encounter and spontaneously communicate with one another in pervasive computing environments. This challenge is exasperated by the highly heterogeneous technologies employed by each of the interacting parties, i.e., in terms of hardware, operating system, middleware protocols, and application protocols. This paper introduces Connect, a software framework which aims to resolve this interoperability challenge in a fundamentally different way. Connect dynamically discovers information about the running systems, uses learning to build a richer view of a system\u2019s behaviour and then uses synthesis techniques to generate a connector to achieve interoperability between heterogeneous systems. Here, we introduce the key elements of Connect and describe its application to a distributed marketplace\u00a0\u2026", "num_citations": "27\n", "authors": ["1616"]}
{"title": "Triumphs and challenges for the industrial application of model-oriented formal methods\n", "abstract": " The \u201clightweight formal methods\u201d paradigm emphasises the use of abstract modelling as an aid to understanding and design of computer-based systems. It advocates careful targeting of formal methods technology on specific system parts or aspects, rather than large-scale application. The challenge of implementing the lightweight paradigm was taken up a decade ago by the community working with the Vienna Development Method (VDM), developing its semantics, tools and encouraging industry application. This paper reports industrial successes over that period, and identifies challenges for industrial formal methods application in the near future. This paper is to appear in the proceedings of the 2nd International Symposium on Leveraging Applications of Formal Methods, Verification and Validation (ISoLA 2006).", "num_citations": "27\n", "authors": ["1616"]}
{"title": "Maintaining semantic mappings between database schemas and ontologies\n", "abstract": " There is a growing need to define a semantic mapping from a database schema to an ontology. Such a mapping is an integral part of the data integration systems that use an ontology as a unified global view. However, both ontologies and database schemas evolve over time in order to accommodate updated information needs. Once the ontology and the database schema associated with a semantic mapping evolved, it is necessary and important to maintain the validity of the semantic mapping to reflect the new semantics in the ontology and the schema. In this paper, we propose a formulation of the mapping maintenance problem and outline a possible solution using illustrative examples. The main points of this paper are: (1) to differentiate the semantic mapping maintenance problem from the schema mapping adaptation problem which only adapts mappings when schemas change; (2) to develop an\u00a0\u2026", "num_citations": "26\n", "authors": ["1616"]}
{"title": "The jABC approach to mediation and choreography\n", "abstract": " Our approach to the SWS-Challenge 2006 Phase I uses the JavaABC [1] for mediation and choreography. jABC is a flexible and powerful framework for service development based on Lightweight Process Coordination. Users easily develop services and applications by composing reusable building-blocks into (flow-) graph structures that can be animated, analyzed, simulated, verified, executed, and compiled. We show here briefly how to handle the mediator design and the remote integration of web services.", "num_citations": "26\n", "authors": ["1616"]}
{"title": "Formulabuilder: A tool for graph-based modelling and generation of formulae\n", "abstract": " In this paper we present the FormulaBuilder, a flexible tool for graph-based modelling and generation of formulae. The FormulaBuilder allows easy and intuitive creation of formulae by using basic components called Formula Building Blocks (FBBs) and arranging them as graphs according to the syntactic structure of a formula. Such a graph can then be validated and used to generate the corresponding formula on the basis of a specific syntax which is chosen from a list of syntaxes supported by the FormulaBuilder. An important application of the FormulaBuilder is the formal specification of properties that describe the requirements of a system. Such property specifications are usually needed by verification tools like model checkers, that help software engineers to detect errors in a specified system. The FormulaBuilder allows users to model property specifications as formula graphs by using commonly-occurring\u00a0\u2026", "num_citations": "26\n", "authors": ["1616"]}
{"title": "Basic-block graphs: Living dinosaurs?\n", "abstract": " Since decades, basic-block (BB) graphs have been the state- of-the-art means for representing programs in advanced industrial compiler environments. The usual justification for introducing the intermediate BB-structures in the program representation is performance: analyses on BB-graphs are generally assumed to outperform their counterparts on single-instruction (SI) graphs, which, undoubtedly, are conceptually much simpler, easier to implement, and more straightforward to verify. In this article, we discuss the difference between the two program representations and show by means of runtime measurements that, according to the new computer generations, performance is no longer on the side of the more complex BB-graphs. In fact, it turns out that no sensible reason for the BB-structure remains. Rather, we will demonstrate that edge-labeled SI-graphs, which model statements in their edges instead of\u00a0\u2026", "num_citations": "26\n", "authors": ["1616"]}
{"title": "Fischer's protocol revisited: A simple proof using modal constraints\n", "abstract": " As a case study, we apply a constraint-oriented state-based proof methodology to Fischer's protocol. The method exploits compositionality and abstraction to reduce the investigated verification problem. This reduction avoids state space explosion. Key concepts of the reduction process are modal constraints, separation of proof obligations, Skolemization and abstraction. Formal basis for the method are Timed Modal Specifications (TMS) allowing loose state-based specifications, which can be refined by successively adding constraints. TMS's can be easily translated into Modal Timed Automata, thus enabling automatic verification. A central issue of the method is the use of Parametrized TMS's.", "num_citations": "26\n", "authors": ["1616"]}
{"title": "Leveraging Applications of Formal Methods, Verification, and Validation: 4th International Symposium on Leveraging Applications, ISoLA 2010, Heraklion, Crete, Greece, October\u00a0\u2026\n", "abstract": " Annotation The two volume set LNCS 6415 and LNCS 6416 constitutes the refereed proceedings of the 4th International Symposium on Leveraging Applications of Formal Methods, ISoLA 2010, held in Heraklion, Crete, Greece, in October 2010. The 100 revised full papers presented were carefully revised and selected from numerous submissions and discuss issues related to the adoption and use of rigorous tools and methods for the specification, analysis, verification, certification, construction, test, and maintenance of systems. The 46 papers of the first volume are organized in topical sections on new challenges in the development of critical embedded systems, formal languages and methods for designing and verifying complex embedded systems, worst-case traversal time (WCTT), tools in scientific workflow composition, emerging services and technologies for a converging telecommunications/Web world in smart environments of the internet of things, Web science, model transformation and analysis for industrial scale validation, and learning techniques for software verification and validation. The second volume presents 54 papers addressing the following topics: EternalS: mission and roadmap, formal methods in model-driven development for service-oriented and cloud computing, quantitative verification in practice, CONNECT: status and plans, certification of software-driven medical devices, modeling and formalizing industrial software for verification, validation and certification, and resource and timing analysis.", "num_citations": "25\n", "authors": ["1616"]}
{"title": "Plug-and-play higher-order process integration\n", "abstract": " Unexpected hurdles in dealing with integration, variability, and interoperability in software development can be overcome using higher-order process integration. Even in its simplicity-oriented, tamed version, this approach fosters a powerful plug-and-play discipline, where processes and services are moved around like data.", "num_citations": "24\n", "authors": ["1616"]}
{"title": "Inferring automata with state-local alphabet abstractions\n", "abstract": " A major hurdle for the application of automata learning to realistic systems is the identification of an adequate alphabet: it must be small enough, in particular finite, for the learning procedure to converge in reasonable time, and it must be expressive enough to describe the system at a level where its behavior is deterministic. In this paper, we combine our automated alphabet abstraction approach, which refines the global alphabet of the system to be learned on the fly during the learning process, with the principle of state-local alphabets: rather than determining a single global alphabet, we infer the optimal alphabet abstraction individually for each state. Our experimental results show that this does not only lead to an increased comprehensibility of the learned models, but also to a better performance of the learning process: indeed, besides the drastic \u2013 yet foreseeable \u2013 reduction in terms of membership\u00a0\u2026", "num_citations": "24\n", "authors": ["1616"]}
{"title": "Never-stop learning: Continuous validation of learned models for evolving systems through monitoring\n", "abstract": " Archive ouverte HAL - Never-stop Learning: Continuous Validation of Learned Models for Evolving Systems through Monitoring Acc\u00e9der directement au contenu Acc\u00e9der directement \u00e0 la navigation Toggle navigation CCSD HAL HAL HALSHS TEL M\u00e9diHAL Liste des portails AUR\u00e9HAL API Data Documentation Episciences.org Episciences.org Revues Documentation Sciencesconf.org Support hal Accueil D\u00e9p\u00f4t Consultation Les derniers d\u00e9p\u00f4ts Par type de publication Par discipline Par ann\u00e9e de publication Par structure de recherche Les portails de l'archive Recherche Documentation hal-00661027, version 1 Article dans une revue Never-stop Learning: Continuous Validation of Learned Models for Evolving Systems through Monitoring A. Bertolino 1 A. Calabr\u00f2 1 M. Merten 2 B. Steffen 2 D\u00e9tails 1 ISTI - Istituto di Scienza e Tecnologie dell'Informazione \u201cA. Faedo\" 2 TU - Technische Universit\u00e4t Dortmund [Dortmund] \u2026", "num_citations": "24\n", "authors": ["1616"]}
{"title": "The SWS Mediator with WEBML/WEBRATIO and JABC/JETI: A Comparison.\n", "abstract": " In this paper we compare the SWS Challenge Mediator solutions provided using the WebML/Webratio and the jABC/jETI approaches.", "num_citations": "24\n", "authors": ["1616"]}
{"title": "Deciding testing equivalence for real-time processes with dense time\n", "abstract": " We present a decision algorithm for testing equivalence of realtime systems with a dense time domain. Real-time systems are modelled by timed graphs, while the decision algorithm uses\u201c mutually refined\u201d timer region graphs. The mutual refinement is important for the synchronization of the timers of different real-time systems. Key to our decision algorithm is the fact that \u2014 despite the dense time domain \u2014 testing can be reduced to \u03a0-bisimulation in very much the same way as in the untimed case.", "num_citations": "24\n", "authors": ["1616"]}
{"title": "Local model checking for context-free processes\n", "abstract": " We present a local model checking algorithm that decides for a given contextfree process whether it satisfies a property written in the alternation-free modal mu-calculus. Heart of this algorithm is a purely syntactical sound and complete formal system, which in contrast to the known tableau techniques, uses intermediate higher-order assertions. These assertions provide a finite representation of all the infinite state sets which may arise during the proof in terms of the finite representation of the context-free argument process. This is the key to the effectiveness of our local model checking procedure.", "num_citations": "24\n", "authors": ["1616"]}
{"title": "Interprocedural herbrand equalities\n", "abstract": " We present an aggressive interprocedural analysis for inferring value equalities which are independent of the concrete interpretation of the operator symbols. These equalities, called Herbrand equalities, are therefore an ideal basis for truly machine-independent optimizations as they hold on every machine. Besides a general correctness theorem, covering arbitrary call-by-value parameters and local and global variables, we also obtain two new completeness results: one by constraining the analysis problem to Herbrand constants, and one by allowing side-effect-free functions only. Thus if we miss a constant/equality in these two scenarios, then there exists a separating interpretation of the operator symbols.", "num_citations": "23\n", "authors": ["1616"]}
{"title": "An approach to intelligent software library management\n", "abstract": " Although very important, solely supporting the retrieval of single components from a software library is not enough to make efforts towards software reuse a success. The DaCapo software library system presented in this paper supports the automatic construction of software from existing components according to a (possibly incomplete) specification. Our method adds a global view to conventional single component retrieval because taxonomic classification, interface constraints, and the relative order of components together form the selection condition for an entire sequence of components. The system is built on top of a deductive database system. It can easily be extended to integrate most software library management strategies found in the literature. 1 Introduction Systematic component reuse is essential to improve the productivity as well as the quality of software development. However, practitioners and researchers agree that efforts in software reuse will not achieve their goals if...", "num_citations": "23\n", "authors": ["1616"]}
{"title": "Towards a unified view of modeling and programming\n", "abstract": " In this paper we argue that there is a value in providing a unified view of modeling and programming. Models are meant to describe a system at a high level of abstraction for the purpose of human understanding and analysis. Programs, on the other hand, are meant for execution. However, programming languages are becoming increasingly higher-level, with convenient notation for concepts that in the past would only be reserved formal specification languages. This leads to the observation, that programming languages could be used for modeling, if only appropriate modifications were made to these languages. At the same time, model-based engineering formalisms such as UML and SysML are highly popular in engineering communities due to their graphical nature. However, these communities are, due to the complex nature of these formalisms, struggling to find grounds in textual formalisms with\u00a0\u2026", "num_citations": "22\n", "authors": ["1616"]}
{"title": "Risk-based testing via active continuous quality control\n", "abstract": " In this paper, we show how our approach to active continuous quality control (ACQC), which employs learning technology to automatically maintain test models along the whole life cycle, can be extended to include risk analysts for supporting risk-based testing. Key to this enhancement is the tailoring of ACQC\u2019s characteristic automata learning-based model extraction to prioritize critical aspects. Technically, risk analysts are provided with an abstract modeling level tailored to design test components (learning symbols) that encompass data-flow constraints reflecting a given risk profile. The resulting alphabet models are already sufficient to steer the ACQC process in a fashion that increases the risk coverage, while it at the same time radically reduces the testing effort. We illustrate our approach by means of case studies with Springer\u2019s Online Conference Service (OCS) which show the impact of the risk\u00a0\u2026", "num_citations": "22\n", "authors": ["1616"]}
{"title": "Effectively addressing complex proteomic search spaces with peptide spectrum matching\n", "abstract": " Summary: Protein identification by mass spectrometry is commonly accomplished using a peptide sequence matching search algorithm, whose sensitivity varies inversely with the size of the sequence database and the number of post-translational modifications considered. We present the Spectrum Identification Machine, a peptide sequence matching tool that capitalizes on the high-intensity b1-fragment ion of tandem mass spectra of peptides coupled in solution with phenylisotiocyanate to confidently sequence the first amino acid and ultimately reduce the search space. We demonstrate that in complex search spaces, a gain of some 120% in sensitivity can be achieved.                    Availability: All data generated and the software are freely available for academic use at http://proteomics.fiocruz.br/software/sim.                    Contact:           paulo@pcarvalho.com                             Supplementary information\u00a0\u2026", "num_citations": "22\n", "authors": ["1616"]}
{"title": "Machine learning for emergent middleware\n", "abstract": " Highly dynamic and heterogeneous distributed systems are challenging today\u2019s middleware technologies. Existing middleware paradigms are unable to deliver on their most central promise, which is offering interoperability. In this paper, we argue for the need to dynamically synthesise distributed system infrastructures according to the current operating environment, thereby generating \u201cEmergent Middleware\u201d to mediate interactions among heterogeneous networked systems that interact in an ad hoc way. The paper outlines the overall architecture of Enablers underlying Emergent Middleware, and in particular focuses on the key role of learning in supporting such a process, spanning statistical learning to infer the semantics of networked system functions and automata learning to extract the related behaviours of networked systems.", "num_citations": "22\n", "authors": ["1616"]}
{"title": "Biological LC/MS Preprocessing and Analysis with jABC, jETI and xcms\n", "abstract": " LC/MS is a successful analysis technique for the statistical analysis used in several branches of biology. It requires an intense screening and combination of the raw data, which is usually done with programs and libraries invoked by scripts in the domain-specific statistics language S or R. We show here how to model and implement this complex workflow in a service-oriented fashion, using the jABC service definition environment and jETI for remote service integration and execution.", "num_citations": "22\n", "authors": ["1616"]}
{"title": "Safe service customization\n", "abstract": " We present a new technique for Safe Service Customization, which flexibly supports subscribers in their desire to change the Service logic in a controlled fashion, while guaranteeing that the modified services can immediately be activated, without previous intervention of specialists like service designers or testers. This flexibilization and simplification of the service customization process reduces the costs for tailored intelligent network services and therefore provides a key to a service-on-demand market.", "num_citations": "22\n", "authors": ["1616"]}
{"title": "Optimal run time optimization proved by a new look at abstract interpretations\n", "abstract": " A two stage run time optimization algorithm is presented that combines two well-known techniques in a Herbrand optimal manner:                                    - Kildall's iterative method for data flow analysis and                                                     - Morel/Renvoise's partial redundancy elimination algorithm.                                                          To combine these techniques in such an optimal way, we firstly have to elaborate Kildall's approach. This is done by means of a new classification method for abstract interpretations which has to be introduced before             Secondly we have to extend Morel/Renovise's technique, which is only conceived to treat the occurrences of a single term, to work on the value equivalence classes delivered by the Kildall-like data flow analysis algorithm mentioned above.             Our algorithm being optimal with respect to the Herbrand interpretation, it is a well-founded basis for the construction of\u00a0\u2026", "num_citations": "22\n", "authors": ["1616"]}
{"title": "Model-driven design of secure high assurance systems: an introduction to the open platform from the user perspective\n", "abstract": " We present DIME, an integrated solution for the rigorous model-driven development of sophisticated web applications based on the Dynamic Web Application (DyWA) Framework, that is designed to flexibly integrate features such as high assurance and security. DIME provides a family of Graphical Domain-Specific Languages (GDSL), each of which tailored towards a specific aspect of typical web applications, including persistent entities (ie, a data model), data retrieval (ie, search queries), business logic in form of various types of process models, the structure of the user interface, and security. They are modeled on a high level of abstraction in a simplicity-driven fashion that focuses on describing what application is sought, instead of how the application is realized. The choice of platform, programming language, and frameworks is moved to the corresponding (full) code generator which may be changed without touching the models leading to high assurance systems.", "num_citations": "21\n", "authors": ["1616"]}
{"title": "A succinct canonical register automaton model\n", "abstract": " We present a novel canonical automaton model, based on register automata, that can be used to specify protocol or program behavior. Register automata have a finite control structure and a finite number of registers (variables), and process sequences of terms that carry data values from an infinite domain. We consider register automata that compare data values for equality. A major contribution is the definition of a canonical automaton representation of any language recognizable by a deterministic register automaton, by means of a Nerode congruence. This canonical form is well suited for modeling, e.g., protocols or program behavior. Our model can be exponentially more succinct than previous proposals, since it filters out \u2018accidental\u2019 relations between data values. This opens the way to new practical applications, e.g., in automata learning.", "num_citations": "21\n", "authors": ["1616"]}
{"title": "Reusing system states by active learning algorithms\n", "abstract": " In this paper we present a practical optimization to active automata learning that reduces the average execution time per query as well as the number of actual tests to be executed. Key to our optimization are two observations: (1) establishing well-defined initial conditions for a test (reset) is a very expensive operation on real systems, as it usually involves modifications to the persisted state of the system (e.g., a database). (2) In active learning many of the (sequentially) produced queries are extensions of previous queries. We exploit these observations by using the same test run on a real system for multiple \u201ccompatible\u201d queries. We maintain a pool of runs on the real system (system states), and execute only suffixes of queries on the real system whenever possible. The optimizations allow us to apply active learning to an industry-scale web-application running on an enterprise platform: the Online\u00a0\u2026", "num_citations": "21\n", "authors": ["1616"]}
{"title": "Behavior-based model construction\n", "abstract": " In this paper, we review behavior-based model construction from a point of view characterized by verification, model checking, and abstraction. It turns out that abstract interpretation is the key to scaling known learning techniques for practical applications, that model checking may serve as a teaching aid in the learning process underlying the model construction, and that there are various synergies with other validation and verification techniques. We will illustrate our discussion by means of a realistic telecommunications scenario, where the underlying system has grown over the last two decades, the available system documentation consists of not much more than user manuals and protocol standards, and the revision cycle times are extremely short. In this situation, behavior-based model construction provides a sound basis, e.g., for test-suite design and maintenance, test organization, and test evaluation.", "num_citations": "21\n", "authors": ["1616"]}
{"title": "Demonstrating learning of register automata\n", "abstract": " We will demonstrate the impact of the integration of our most recently developed learning technology for inferring Register Automata into the LearnLib, our framework for active automata learning. This will not only illustrate the unique power of Register Automata, which allows one to faithfully model data independent systems, but also the ease of enhancing the LearnLib with new functionality.", "num_citations": "20\n", "authors": ["1616"]}
{"title": "The learnlib in fmics-jeti\n", "abstract": " The FMICS-jETI platform is a collaborative, service- based demonstrator of tools and techniques for the analysis of industrial critical systems. It is the FMICS Working Group contribution to the Verified Software Initiative. In this paper, we extend the scope of the FMICS-jETI platform to address the integration of heterogeneous and legacy tools and technologies. We show how to integrate 1) CORBA, a language independent standard for the inter-operability of heterogeneous functionalities distributed over a network, 2) active model learning technologies, via the LearnLib, as a model extrapolation technique that uses testing to explore a black box system and CORBA as a communication mechanism, and 3) third party applications built on top of the LearnLib, in this case Smyle, a tool that synthesizes design models by learning from examples, that uses the LearnLib as learner core.", "num_citations": "20\n", "authors": ["1616"]}
{"title": "Automated regression testing of CTI-systems.\n", "abstract": " In this paper we present an integrated testing environment for the automated regression test of Computer Telephony Integrated applications. Its novelty consists of a coordinative test management layer that instantiates a general-purpose environment for the specification and verification of workflows in the testing domain. This results in a test environment that controls not only the individual test tools, but also the whole life-cycle of functional systemlevel tests, comprising test design, test generation, test execution, test evaluation and test reporting. Special attention is devoted to the simplification of the test case design and the checking of admissibility criteria, of interdependencies between the actions of test cases, and of assumptions about the state of the system\u2019s resources. We discuss the key features of our testing environment along a concrete industrial application, which illustrates in particular the coarse grain, workflow-like test case representation and the validation and formal verification capabilities. Field results document an efficiency improvement of factors during the test execution phase.", "num_citations": "20\n", "authors": ["1616"]}
{"title": "A formal requirements engineering method for specification, synthesis, and verification\n", "abstract": " This paper presents a formal requirements engineering method capturing specification, synthesis, and verification. Being multi-paradigm, our approach integrates individual established formal methods: temporal logics are used to express abstract specifications in the form of loose global constraints, like ordering requirements, or abstract safety and liveness properties, whereas Statecharts are used to support the development of a detailed, hierarchical specification at the concrete level. The link between, these two specification layers is established by means of 1) a semi-automatic synthesis procedure, where sequential portions of Statecharts, automatically synthesized from abstract specifications, can be manually composed into structured Statecharts, and 2) by automatic formal verification via model checking, which validates the global constraints for the resulting overall Statechart specification. The method is\u00a0\u2026", "num_citations": "20\n", "authors": ["1616"]}
{"title": "Active automata learning: from DFAs to interface programs and beyond\n", "abstract": " This paper reviews the development of active learning in the last decade under the perspective of treating of data, a major source of undecidability, and therefore a key problem to achieve practicality. Starting with the first case studies, in which data was completely disregarded, we revisit different steps towards dealing with data explicitly in active learning: We discuss Mealy Machines as a model for systems with (data) output, automated alphabet abstraction refinement as a two-dimensional extension of the partition-refinement based approach of active learning for inferring not only states but also optimal alphabet abstractions, and Register Mealy Machines, which can be regarded as programs restricted to data-independent data processing as it is typical for protocols or interface programs. We are convinced that this development has the potential to transform active automata learning into a technology of high practical importance.", "num_citations": "19\n", "authors": ["1616"]}
{"title": "Synthesizing semantic web service compositions with jMosel and Golog\n", "abstract": " In this paper we investigate different technologies to attack the automatic solution of orchestration problems based on synthesis from declarative specifications, a semantically enriched description of the services, and a collection of services available on a testbed. In addition to our previously presented tableaux-based synthesis technology, we consider two structurally rather different approaches here: using jMosel, our tool for Monadic Second-Order Logic on Strings and the high-level programming language Golog, that internally makes use of planning techniques. As a common case study we consider the Mediation Scenario of the Semantic Web Service Challenge, which is a benchmark for process orchestration. All three synthesis solutions have been embedded in the jABC/jETI modeling framework, and used to synthesize the abstract mediator processes as well as their concrete, running (Web) service\u00a0\u2026", "num_citations": "19\n", "authors": ["1616"]}
{"title": "eXtreme model-driven design with jABC\n", "abstract": " eXtreme Model-Driven Design (XMDD) is a new development paradigm designed to continuously involve the customer/application expert throughout the whole system\u2019s life cycle. In technical practice, user-level models are successively enriched and refined from the user perspective, until a sufficient level of detail is reached, at which elementary services can be implemented that solve tasks at the application level. The realization of the individual services should be typically simple: they are often based on functionality provided by third-party and by standard software systems. We demonstrate jABC, a flexible framework designed to support systematic development according to the XMDD paradigm. jABC allows (end-) users to develop service-oriented systems by composing reusable building blocks into flow graph structures. As a case study we present a heterogeneous service mashup that makes extensive use of today\u2019s service technologies like REST or Web services. Furthermore we introduce a temporal logicbased synthesis approach that automatically delivers process flows that conform to declarative formal specifications on the basis of semantic information annotated to the service components.", "num_citations": "19\n", "authors": ["1616"]}
{"title": "Service-oriented mediation with jABC/jETI\n", "abstract": " This chapter shows how we solved the Mediation task in a model driven, service oriented fashion using the jABC framework for model driven development and its jETI extension for seamless integration of remote (Web) services. In particular we illustrate how atomic services and orchestrations are modelled in the jABC, how legacy services and their proxies are represented within our framework, and how they are imported into our framework, how the mediator arises as orchestrations of the testbed's remote services and of local services, how vital properties of the Mediator are verified via model checking in the jABC, and how jABC/jETI orchestrated services are exported as Web services. Besides providing a solution to the mediation problem, this also illustrates the agility of jABC-based solutions, which is due to what we call eXtreme Model Driven Design, a new paradigm that puts the user process in the center of\u00a0\u2026", "num_citations": "19\n", "authors": ["1616"]}
{"title": "System level testing of virtual switch (re-) configuration over ip\n", "abstract": " We show how our integrated test environment can be used for the validation of evolving Internet services, since it captures the (re-)configuration phase preceding the 'classical' steady state usage. This capability is extremely important to test applications and products based on the high-end HICOM switch family: via a specific application they allow the remote, web-browser and Internet-based configuration of services built upon a new-generation, virtual switch. The paper illustrates this capability while focussing on the challenge of dealing with high availability systems.", "num_citations": "19\n", "authors": ["1616"]}
{"title": "An automated testing environment for CTI systems using concepts for specification and verification of workflows\n", "abstract": " In this paper we present an automated testing environment for the system level test of Computer Telephony Integrated applications. Its novelty consists of a coordinative test management layer that instantiates a general-purpose environment for the specification and verification of workflows in the testing domain. This results in a test environment that does not only control the individual test tools but the whole life-cycle of functional system-level tests, comprising test design, test generation, test execution, test evaluation and test reporting. Special attention is devoted to the simplification of the test cases design and the checking of admissibility criteria, interdependencies between the actions of test cases, and assumptions about the state of the system's resources. The key features of our testing environment, coarse grain, workflow-like test case representation and the validation and formal verification capabilities, are\u00a0\u2026", "num_citations": "19\n", "authors": ["1616"]}
{"title": "A pragmatic approach to software synthesis\n", "abstract": " We present a practice oriented tool for software synthesis that supports the interface-correct configuration of complex systems from a library of reusable software components. Besides simply checking the interface-correctness of a link by means of type constraints, the tool is also designed to propose software components for solving a (loosely) specified problem within a certain context. In particular, it identifies possible interfacing modules that in case of an interface-conflict may serve for the right conversion, transformation or parameter configuration. We illustrate our tool, which is based on the deductive database system LOLA, in three application specific settings.", "num_citations": "19\n", "authors": ["1616"]}
{"title": "Automata learning algorithms and processes for providing more complete systems requirements specification by scenario generation, CSP-based syntax-oriented model construction\u00a0\u2026\n", "abstract": " Systems, methods and apparatus are provided through which in some embodiments, automata learning algorithms and techniques are implemented to generate a more complete set of scenarios for requirements based programming. More specifically, a CSP-based, syntax-oriented model construction, which requires the support of a theorem prover, is complemented by model extrapolation, via automata learning. This may support the systematic completion of the requirements, the nature of the requirement being partial, which provides focus on the most prominent scenarios. This may generalize requirement skeletons by extrapolation and may indicate by way of automatically generated traces where the requirement specification is too loose and additional information is required.", "num_citations": "18\n", "authors": ["1616"]}
{"title": "SCA and jABC: Bringing a service-oriented paradigm to web-service construction\n", "abstract": " Extensibility, flexibility, easy maintainability, and long-term robustness are core requirements for modern, highly distributed information and computation systems. Such systems in turn show a steady increase in complexity. In pursuit of these goals, software engineering has seen a rapid evolution of architectural paradigms aiming towards increasingly modular, hierarchical, and compositional approaches. Object-orientation, component orientation, middleware components, product-lines, and - recently - service orientation.               We compare two approaches towards a service-oriented paradigm, the Service Component Architecture (SCA) and the jABC.", "num_citations": "18\n", "authors": ["1616"]}
{"title": "An evaluation of service integration approaches of business process management systems\n", "abstract": " In this paper, we evaluate and categorize how five representative, Java-based, state-of-the-art business process management systems, namely jBPM (4.x and 5.x), Activiti, AristaFlow and jABC, realize the integration of services. In particular, we show that the use of domain specific business activities is the currently most sophisticated technique for integrating services in business processes, and describe what they consist of, how they are created, how they can be organized and how they are used. This sheds light on the corresponding state-of-the-art, and shows that, in particular the supposedly big players, have quite some room for improvement.", "num_citations": "17\n", "authors": ["1616"]}
{"title": "Requirement-driven evaluation of remote ERP-system solutions: a service-oriented perspective\n", "abstract": " This paper systematically evaluates four different APIs provided by popular ERP systems (SAP BAPI, SAP eSOA, Microsoft Dynamics NAV and Intuit Quick books) according to their potential for service-oriented enterprise application integration. Central is here the quality of access to and control of their business objects for developing complex business processes in a service oriented landscape. In an ideal world this development should be directly in the hands of the application/business experts, and thus be possible without coding. This can only work if the technical APIs can (semi-) automatically be bridged to a level an application expert can master, which leads to additional API requirements. Our evaluation reveals that the currently still quite unsatisfactory quality of the APIs very much reflects the business profile of each ERP solution. We are therefore convinced that, with changing market conditions, future ERP\u00a0\u2026", "num_citations": "17\n", "authors": ["1616"]}
{"title": "Automata, Languages and Programming: 36th International Colloquium, ICALP 2009, Rhodes, Greece, July 5-12, 2009, Proceedings, Part I\n", "abstract": " The two-volume set LNCS 5555 and LNCS 5556 constitutes the refereed proceedings of the 36th International Colloquium on Automata, Languages and Programming, ICALP 2009, held in Rhodes, Greece, in July 2009. The 126 revised full papers (62 papers for track A, 24 for track B, and 22 for track C) presented were carefully reviewed and selected from a total of 370 submissions. The papers are grouped in three major tracks on algorithms, automata, complexity and games; on logic, semantics, theory of programming; as well as on foundations of networked computation: models, algorithms and information management. LNCS 5555 contains 62 contributions of track A selected from 223 submissions as well as 2 invited lectures. This two-volume set lauches the new subline of Lecture Notes in Computer Science, entitled LNCS Advanced Research in Computing and Software Science (ARCoSS).", "num_citations": "17\n", "authors": ["1616"]}
{"title": "Automated functional testing of web-based applications\n", "abstract": " Modern web-based applications are multitiered, distributed applications that typically run on heterogeneous platforms. Their correct operation depends increasingly on the interoperability of single software modules, rather than on their intrinsic algorithmic correctness. We present a scalable, integrated test methodology capable of granting adequate coverage of functional testing, yet still easy to use. It bases on our previous work on system level test of Computer Telephony Integrated applications, where our coordination-based coarse grain approach has been applied very successfully. The practicability of the approach is illustrated by testing complex role based functional features of our Online Conference Service, an advanced online management system for the organization of scientific conferences.", "num_citations": "17\n", "authors": ["1616"]}
{"title": "RERS 2018: CTL, LTL, and Reachability\n", "abstract": " This paper is dedicated to the Rigorous Examination of Reactive Systems (RERS) Challenge 2018. We focus on changes and improvements compared to previous years. RERS again provided a large variety of verification benchmarks that foster the comparison of validation tools while featuring both sequential and parallel programs. In addition to reachability questions, the RERS Challenge is known for its linear temporal logic (LTL) properties, and RERS\u201918 extends the portfolio of verification tasks to computational tree logic (CTL). Modifications compared to the previous iteration include an enhanced generation of sequential benchmarks, an improved automation of the construction of parallel benchmarks, a redesigned penalty for wrong answers, and the addition of CTL properties. We illustrate our newly applied generation of parallel benchmarks in detail.", "num_citations": "16\n", "authors": ["1616"]}
{"title": "Leveraging Applications of Formal Methods, Verification and Validation: Discussion, Dissemination, Applications: 7th International Symposium, ISoLA 2016, Imperial, Corfu\u00a0\u2026\n", "abstract": " The two-volume set LNCS 9952 and LNCS 9953 constitutes the refereed proceedings of the 7th International Symposium on Leveraging Applications of Formal Methods, Verification and Validation, ISoLA 2016, held in Imperial, Corfu, Greece, in October 2016. The papers presented in this volume were carefully reviewed and selected for inclusion in the proceedings. Featuring a track introduction to each section, the papers are organized in topical sections named: statistical model checking; evaluation and reproducibility of program analysis and verification; ModSyn-PP: modular synthesis of programs and processes; semantic heterogeneity in the formal development of complex systems; static and runtime verification: competitors or friends?; rigorous engineering of collective adaptive systems; correctness-by-construction and post-hoc verification: friends or foes?; privacy and security issues in information systems; towards a unified view of modeling and programming; formal methods and safety certification: challenges in the railways domain; RVE: runtime verification and enforcement, the (industrial) application perspective; variability modeling for scalable software evolution; detecting and understanding software doping; learning systems: machine-learning in software products and learning-based analysis of software systems; testing the internet of things; doctoral symposium; industrial track; RERS challenge; and STRESS.", "num_citations": "16\n", "authors": ["1616"]}
{"title": "Automata learning with on-the-fly direct hypothesis construction\n", "abstract": " We present an active automata learning algorithm for Mealy state machines that directly constructs a state machine hypothesis according to observations, while other algorithms generate a state machine as output from information gathered in an observation table. Our DHC algorithm starts with a one-state hypothesis that it successively extends using a direct construction approach. This approach enables direct observation of the automata construction process: the learning algorithm continues to complete its hypothesis, providing intuition to a field of formal methods otherwise dominated by algorithms that largely operate on internal data structures without visible feedback.               The DHC algorithm is competitive in cases where memory is the critical issue, e.g., in embedded networked systems. It is also well-suited as educational tool to teach the underlying well-established theoretical methods in a totally\u00a0\u2026", "num_citations": "16\n", "authors": ["1616"]}
{"title": "Dynamic and formal verification of embedded systems: A comparative survey\n", "abstract": " Embedded Systems, by their nature, constitute a meeting point for communities with extremely different background. In particular, the high demands for quality and reliability for embedded systems have led to complementary quality assurance efforts: hardware engineers have developed techniques for dynamic verification in terms of co-simulation, which, in particular, addresses the different nature of hardware and software components. Thus these techniques are tailored for the transactional level, which comprises dedicated models for the hardware and the software parts. On the other hand, there is a bulk of work on formal verification techniques, which typically address higher levels of abstraction. These techniques are exhaustive in the sense that they cover all the infinite possible paths of their models, however at the price of neglecting many of the low-level aspects treated by co-simulation. It is the goal of\u00a0\u2026", "num_citations": "16\n", "authors": ["1616"]}
{"title": "Analyzing second-order effects between optimizations for system-level test-based model generation\n", "abstract": " Test-based model generation by classical automata learning is very expensive. It requires an impractically large number of queries to the system, each of which must be implemented as a system-level test case. Key towards the tractability of observation based model generation are powerful optimizations exploiting different kinds of expert knowledge in order to drastically reduce the number of required queries, and thus the testing effort. In this paper, we present a thorough experimental analysis of the second-order effects between such optimizations in order to maximize their combined impact", "num_citations": "16\n", "authors": ["1616"]}
{"title": "Graphs in MetaFrame: The unifying power of polymorphism\n", "abstract": " We present a highly polymorphic tool for the construction, synthesis, structuring, manipulation, investigation, and (symbolic) execution of graphs. The flexibility of this tool, which mainly arises as a consequence of combining complex graph labelings expressing the intended semantics with hierarchy and customized graphical node representations, is illustrated along a representative choice of application scenarios.", "num_citations": "16\n", "authors": ["1616"]}
{"title": "Non-monotone fixpoint iterations to resolve second order effects\n", "abstract": " We present a new fixpoint theorem which guarantees the existence and the finite computability of the least common solution of a countable system of recursive equations over a wellfounded domain. The functions are only required to be increasing and delay-monotone, the latter being a property much weaker than monotonicity. We hold that wellfoundedness is a natural condition as it guarantees termination of every fixpoint computation algorithm. Our fixpoint theorem covers, under the wellfoundedness condition, all the known \u2018synchronous\u2019 versions of fixpoint theorems. To demonstrate its power and versatility we contrast an application in data flow analysis, where known versions are applicable as well, to a practically relevant application in program optimization, which due to its second order effects, requires the full strength of our new theorem. In fact, the new theorem is central for establishing the optimality\u00a0\u2026", "num_citations": "16\n", "authors": ["1616"]}
{"title": "A succinct canonical register automaton model for data domains with binary relations\n", "abstract": " We present a novel canonical automaton model for languages over infinite data domains, that is suitable for specifying the behavior of services, protocol components, interfaces, etc. The model is based on register automata. A major contribution is a construction of succinct canonical register automata, which is parameterized on the set of relations by which elements in the data domain can be compared. We also present a Myhill Nerode-like theorem, from which minimal canonical automata can be constructed. This canonical form is as expressive as general deterministic register automata, but much better suited for modeling in practice since we lift many of the restrictions on the way variables can be accesed and stored: this allows our automata to be significantly more succinct than previously proposed canonical forms. Key to the canonical form is a symbolic treatment of data languages, which allows us to\u00a0\u2026", "num_citations": "15\n", "authors": ["1616"]}
{"title": "Database systems for advanced applications\n", "abstract": " It is our great pleasure to welcome you to DASFAA 2015, the 20th edition of the International Conference on Database Systems for Advanced Applications (DASFAA 2015), which was held in Hanoi, Vietnam during April 20\u201323, 2015. Hanoi (Vietnamese: H\u00e0 N\u00f4. i), the capital of Vietnam, is the second largest city in Vietnam and has collected all the essence, unique features, and diversification of Vietnamese culture. The city is preserving more than 4000 historical and cultural relics, architecture and beauty spots, in which nearly 900 relics have been nationally ranked with hundreds of pagodas, temples, architectural works, and sceneries. Handcraft streets and traditional handcraft villages remain prominent and attractive to tourists when visiting Hanoi, many of which centered around the Hoan Kiem Lake in the Old Quarter, close to the conference venue. Hanoi has recently been included on TripAdvisor\u2019s list of best\u00a0\u2026", "num_citations": "15\n", "authors": ["1616"]}
{"title": "The jABC approach to rigorous collaborative development of SCM applications\n", "abstract": " Our approach to the model-driven collaborative design of IKEA\u2019s P3 Delivery Management Process uses the jABC [9] for model driven mediation and choreography to complement a RUP-based (Rational Unified Process) development process. jABC is a framework for service development based on Lightweight Process Coordination. Users (product developers and system/software designers) easily develop services and applications by composing reusable building-blocks into (flow-) graph structures that can be animated, analyzed, simulated, verified, executed, and compiled. This way of handling the collaborative design of complex embedded systems has proven to be effective and adequate for the cooperation of non-programmers and non-technical people, which is the focus of this contribution, and it is now being rolled out in the operative practice.", "num_citations": "15\n", "authors": ["1616"]}
{"title": "Trust in agent societies\n", "abstract": " This special issue is the result of the selection and re-submission of advanced and revised versions of papers from the workshop on\" Trust in Agent Societies\"(11th edition), held in Estoril (Portugal) on May 10, 2008 as part of the Autonomous Agents and Multi-Agent Systems 2008 Conference (AAMAS 2008), and organized by Rino Falcone, Suzanne Barber, Jordi Sabater-Mir, and Munindar Singh. The aim of the workshop was to bring together researchers from different fields (artificial intelligence, multi-agent systems, cognitive science, game theory, and social and organizational sciences) that could contribute to a better understanding of trust and reputation in agent societies. The workshop scope included theoretical results as well their applications in human\u2013computer interaction and electronic commerce. It was constituted by a main session integrated with two others: the first on the formal models of trust, and the\u00a0\u2026", "num_citations": "15\n", "authors": ["1616"]}
{"title": "Completing and adapting models of biological processes\n", "abstract": " We present a learning-based method for model completion and adaptation, which is based on the combination of two approaches: 1) R2D2C, a technique for mechanically transforming system requirements via provably equivalent models to running code, and 2) automata learning-based model extrapolation. The intended impact of this new combination is to make model completion and adaptation accessible to experts of the field, like biologists or engineers. The principle is briefly illustrated by generating models of biological procedures concerning gene activities in the production of proteins, although the main application is going to concern autonomic systems for space exploration.", "num_citations": "15\n", "authors": ["1616"]}
{"title": "Expansion-based removal of semantic partial redundancies\n", "abstract": " We develop an expansion-based algorithm for semantic partial redundancy elimination (SPRE), which overcomes the central drawbacks of the state-of-the-art approaches, which leave the program structure invariant: they fail to eliminate all partial redundancies even for acyclic programs. Besides being optimal for acyclic programs, our algorithm is unique in eliminating all partial k-redundancies, a new class of redundancies which is characterised by the number k of loop iterations across which values have to be kept. These optimality results come at the price of an in the worst case exponential program growth. The new technique is thus tailored for optimizing the typically considerably small computational \u201chot\u201d spots of a program. Here it is particularly promising because its structural simplicity supports extensions to uniformly capture further powerful optimisations like constant propagation or strength\u00a0\u2026", "num_citations": "15\n", "authors": ["1616"]}
{"title": "Tools and Algorithms for the Construction and Analysis of Systems\n", "abstract": " Conference on Tools and Algorithms for the Construction and Analysis ofSystems. TACAS'98 took place at the Gulbenkian Foundation in Lisbon, Portugal, March 31st to April 3rd, 1998, as part of the First European Joint Conferences on Theory and Practice of Software (ETAPS), whose aims, organization, and history are detailed in the separate foreword by Donald Sannella.", "num_citations": "15\n", "authors": ["1616"]}
{"title": "Service Creation: Formal Verification and Abstract Views\n", "abstract": " This paper presents a Service Creation Environment which is unique in featuring formal verification of global correctness and consistency as well as abstract views. The verification guarantees frame conditions for the design, concerning implementability, country specific standards, and network specific features. Abstract views focus the development process, and support error correction. The described environment allows a lazy and incremental use of formal methods: if no formal constraints are defined, the system behaves like standard systems for service creation. However, the more constraints are added, the more reliable are the created services.", "num_citations": "15\n", "authors": ["1616"]}
{"title": "Chaotic fixed point iterations\n", "abstract": " In this paper we present a new fixed point theorem applicable for a countable system of recursive equations. We demonstrate the power and versatility of our result by applications in automata theory (AT), data flow analysis (DFA), and program optimization (PO). In AT and DFA this is demonstrated by proving the correctness of partitioning algorithms computing (strong) bisimulation and of workset algorithms computing solutions of data flow analysis problems, repectively, and in PO for proving the optimality of an algorithm for partial dead code elimination. Moreover, we show that our approach is more general than vector iterations. Keywords Fixed point, chaotic iteration, vector iteration, automata theory, data flow analysis, program optimization, partitioning algorithms, bisimulation, workset algorithms, partial dead code elimination. Contents 1 Introduction 1 2 Theory 2 2.1 The Main Theorem::::::::::::::::::::::::::::::: 2 2.2 Vector Iterations:::...", "num_citations": "15\n", "authors": ["1616"]}
{"title": "eXtreme Model-Driven Development (XMDD) Technologies as a Hands-On Approach to Software Development Without Coding\n", "abstract": " The Technological Domain and the Geoinformation Society Human action takes place in space, produces and reproduces space and place taking into account societal and physical rules, and appropriates space (Werlen 1993). As means of action, humans have been using geoinformation for centuries. A description of a specific place, explaining a route to someone else, using the sun and stars for orientation, using signposts or producing a sketch map to depict the supposed location of a treasure all contain geoinformation. This type of geoinformation usually consists of a location that is more or less exactly defined, as well as attribute data detailing specific qualities of the location, area, or route described. Thinking of a verbal description of the last holiday, people usually look into where it was, what the specific qualities of the place were, how they got there. Early rulers both in Asia and Europe kept track of their\u00a0\u2026", "num_citations": "14\n", "authors": ["1616"]}
{"title": "International Competition on Software Testing (Test-Comp).\n", "abstract": " Tool competitions are a special form of comparative evaluation, where each tool has a team of developers or supporters associated that makes sure the tool is properly configured to show its best possible performance. Tool competitions have been a driving force for the development of mature tools that represent the state of the art in several research areas. This paper describes the International Competition on Software Testing (Test-Comp), a comparative evaluation of automatic tools for software test generation. Test-Comp 2019 is presented as part of TOOLympics 2019, a satellite event of the conference TACAS.", "num_citations": "14\n", "authors": ["1616"]}
{"title": "An abstract framework for counterexample analysis in active automata learning\n", "abstract": " Counterexample analysis has emerged as one of the key challenges in Angluin-style active automata learning. Rivest and Schapire (1993) showed for the\\mathrmL^* algorithm that a single suffix of the counterexample was sufficient to ensure progress. This suffix can be obtained in a binary search fashion, requiring \u0398 (\\log m) membership queries for a counterexample of length m. Correctly implementing this algorithm can be quite tricky, and its correctness sometimes even has been disputed. In this paper, we establish an abstract framework for counterexample analysis, which basically reduces the problem of finding a suffix to finding distinct neighboring elements in a 0/1 sequence, where the first element is 0 and the last element is 1. We demonstrate the conciseness and simplicity of our framework by using it to present new counterexample analysis algorithms, which, while maintaining the worst-case complexity of O (\\log m), perform significantly better in practice. Furthermore, we contribute\u2014in a second instantiation of our framework, highlighting its generality\u2014the first sublinear counterexample analysis procedures for the algorithm due to Kearns and Vazirani (1994).", "num_citations": "14\n", "authors": ["1616"]}
{"title": "Second-order servification\n", "abstract": " In this paper we present second-order servification, a business process modeling paradigm for variability. Key to this paradigm is to consider services and even whole subprocesses as \u2018resources\u2019 of a (second-order) business process, which can be created, selected, and moved around just like data. This does not only allow us to easily define new variants of a business process simply via second-order parameterization, but also to exchange its constituent services (and even sub-processes) dynamically at runtime. In fact, the concrete implementation of a second-order activity in a process model may be unknown when the process starts, and built-up and exchanged while the process is running. We will illustrate the ease of the new paradigm along a flight booking scenario, where our corresponding second-order process model allows us to dynamically instantiate the payment process even with process\u00a0\u2026", "num_citations": "14\n", "authors": ["1616"]}
{"title": "The teachers\u2019 crowd: the impact of distributed oracles on active automata learning\n", "abstract": " In this paper we address the major bottleneck of active automata learning, the typically huge number of required tests, by investigating the impact of using a distributed testing environment (a crowd of teachers) to execute test cases (membership queries) in parallel. This kind of parallelization of automata learning has the best potential when the time for test case execution is dominant, an assumption valid for most practical applications. Our investigation explicitly focuses on the impact of the structure of the system under learning (number of states, size of alphabet) and the degree of supported parallelism. It comprises three variants of active learning algorithms with different test case generation profiles. These differences can be observed directly at the level of the run-times, which all show a linear speedup for moderate degrees of parallelization, but with different saturation points beyond which further\u00a0\u2026", "num_citations": "14\n", "authors": ["1616"]}
{"title": "Advances in Neural Networks-ISNN 2009: 6th International Symposium on Neural Networks, ISNN 2009 Wuhan, China, May 26-29, 2009 Proceedings, Part II\n", "abstract": " This book and its companion volumes, LNCS vols. 5551, 5552 and 5553, constitute the proceedings of the 6th International Symposium on Neural Networks (ISNN 2009), held during May 26\u201329, 2009 in Wuhan, China. Over the past few years, ISNN has matured into a well-established premier international symposium on neural n-works and related fields, with a successful sequence of ISNN symposia held in Dalian (2004), Chongqing (2005), Chengdu (2006), Nanjing (2007), and Beijing (2008). Following the tradition of the ISNN series, ISNN 2009 provided a high-level inter-tional forum for scientists, engineers, and educators to present state-of-the-art research in neural networks and related fields, and also to discuss with international colleagues on the major opportunities and challenges for future neural network research. Over the past decades, the neural network community has witnessed tremendous-forts and developments in all aspects of neural network research, including theoretical foundations, architectures and network organizations, modeling and simulation,-pirical study, as well as a wide range of applications across different domains. The recent developments of science and technology, including neuroscience, computer science, cognitive science, nano-technologies and engineering design, among others, have provided significant new understandings and technological solutions to move the neural network research toward the development of complex, large-scale, and n-worked brain-like intelligent systems. This long-term goal can only be achieved with the continuous efforts of the community to seriously investigate different\u00a0\u2026", "num_citations": "14\n", "authors": ["1616"]}
{"title": "Second-order semantic web\n", "abstract": " We propose a framework for top-down Web service interoperation based on an aggressive version of model-driven development (AMDD). The point here is to govern the construction and customization of complex Web applications at the model level in a framework that allows application experts to directly formulate their desires in an adequate way. Adequate means in this context that applications can be automatically validated, executed, tested, and deployed by the application experts, inside a framework that takes care also of second-order concerns. Our approach, which focuses on functionalities as the basic entities of the design space is tailored to make second order issues like interoperation, distribution, and compatibility simple for the many, difficult for the few: simple for the many, as the advocated approach hides most of the intricate second-order issues from the application designer, and difficult for the few\u00a0\u2026", "num_citations": "14\n", "authors": ["1616"]}
{"title": "Extending automata learning to extended finite state machines\n", "abstract": " Automata learning is an established class of techniques for inferring automata models by observing how they respond to a sample of input words. Recently, approaches have been presented that extend these techniques to infer extended finite state machines (EFSMs) by dynamic black-box analysis. EFSMs model both data flow and control behavior, and their mutual interaction. Different dialects of EFSMs are widely used in\u00a0tools for model-based software development, verification, and testing.                 This survey paper presents general principles behind some of these recent extensions. The goal is to elucidate how the principles behind classic automata learning can be maintained and guide extensions to more general automata models, and to situate some extensions with respect to these principles.", "num_citations": "13\n", "authors": ["1616"]}
{"title": "Property-preserving parallel decomposition\n", "abstract": " We propose a systematic approach to generate highly parallel benchmark systems with guaranteed temporal properties. Key to our approach is the iterative property-preserving parallel decomposition of an initial Modal Transition System, which is based on lightweight assumption commitment. Property preservation is guaranteed on the basis of Modal Contracts that permit a refinement into a component and its context while supporting the chaining of dependencies that are vital for the validity of considered properties. We illustrate our approach, which can be regarded as a simplicity-oriented variant of correctness by construction, by means of an accompanying example.", "num_citations": "13\n", "authors": ["1616"]}
{"title": "ALEX: mixed-mode learning of web applications at ease\n", "abstract": " In this paper, we present ALEX, a web application that enables non-programmers to fully automatically infer models of web applications via active automata learning. It guides the user in setting up dedicated learning scenarios, and invites her to experiment with the available options in order to infer models at adequate levels of abstraction. In the course of this process, characteristics that go beyond a mere \u201csite map\u201d can be revealed, such as hidden states that are often either specifically designed or indicate errors in the application logic. Characteristic for ALEX is its support for mixed-mode learning: REST and web services can be executed simultaneously in one learning experiment, which is ideal when trying to compare back-end and front-end functionality of a web application. ALEX has been evaluated in a comparative study with 140 undergraduate students, which impressively highlighted its potential\u00a0\u2026", "num_citations": "13\n", "authors": ["1616"]}
{"title": "Automated learning setups in automata learning\n", "abstract": " Test drivers are an essential part of any practical active automata learning setup. These components to accomplish the translation of abstract learning queries into concrete system invocations while managing runtime data values in the process. In current practice test drivers typically are created manually for every single system to be learned. This, however, can be a very time-consuming and thus expensive task, making it desirable to find general solutions that can be reused.               This paper discusses how test drivers can be created for LearnLib, a flexible automata learning framework. Starting with the construction of application-specific test drivers by hand, we will discuss how a generic test driver can be employed by means of configuration. This configuration is created manually or (semi-)automatically by analysis of the target system\u2019s interface.", "num_citations": "13\n", "authors": ["1616"]}
{"title": "Automated continuous quality assurance\n", "abstract": " We present a case study that illustrates the power of active learning for enabling the automated quality assurance of complex and distributed evolving systems. We illustrate how the development of the OCS, Springer Verlag's Online Conference System, is supported by continuous learning-based testing, that by its nature maintains the synchrony of the running application and the learned (test) model. The evolution of the test model clearly indicates which portions of the system remain stable and which are altered. Thus our approach includes classical regression testing and feature interaction detection. We show concretely how model checking, automata learning, and quantitative analysis concur with the holistic quality assurance of this product.", "num_citations": "13\n", "authors": ["1616"]}
{"title": "IT simply works: simplicity and embedded systems design\n", "abstract": " The central challenge in resource constrained systems is indeed that there is often a dramatic problem of resources: little memory, no display (i.e., mediated control and interaction), and weak processors. These constraints are due to various reasons, including cost (in economies of scale little amounts can make a big difference) and constraints mandated by the host environment such as limitations on weight, size, bandwidth, or power consumption. In addition to structural issues, that already force designers to stretch and squeeze pushing the design to the limit, other problems may affect the design process itself, e.g. when developers must program for a system that is itself still under design: such concurrent development includes easily critical unknown unknowns. Design efforts under these conditions are difficult at best, and it is not clear how much of these constraints and pressures are really unavoidable. We\u00a0\u2026", "num_citations": "13\n", "authors": ["1616"]}
{"title": "An approach to Discovery with miAamics and jABC\n", "abstract": " We address the discovery scenario using miAamics, a framework for rule-based evaluation originally developed for efficient and scalable personalization purposes, as a reasoning engine. The discovery service is implemented in the jABC framework.", "num_citations": "13\n", "authors": ["1616"]}
{"title": "Compositional characterization of observable program properties\n", "abstract": " Dans cet article nous mod\u00ealisons \u00e0 la fois les comportements des programmes et les abstractions entre eux comme des fonctions qui g\u00e9n\u00e9ralisent les interpr\u00e9tations abstraites en tirant profit de l'ordre naturel des propri\u00e9t\u00e9s des programmes. Cette g\u00e9n\u00e9ralisation offre un cadre dans lequel la correction (s\u00fbret\u00e9) et la completude des interpr\u00e9tations abstraites r\u00e9sultent naturellement de cet ordre. De plus, elle autorise le raffinement modulaire et pas \u00e0 pas: \u00e9tant donn\u00e9 le comportement d'un programme, sa caract\u00e9risation, qui est une s\u00e9mantique d\u00e9notationnelle compl\u00e8te et aussi correcte que possible, peut \u00eatre d\u00e9termin\u00e9e par composition.", "num_citations": "13\n", "authors": ["1616"]}
{"title": "Scientific workflows with the jABC framework\n", "abstract": " The jABC is a framework for process modelling and execution according to the XMDD (eXtreme model-driven design) paradigm, which advocates the rigorous use of user-level models in the software development process and software life cycle. We have used the jABC in the domain of scientific workflows for more than a decade now\u2014an occasion to look back and take stock of our experiences in the field. On the one hand, we discuss results from the analysis of a sample of nearly 100 scientific workflow applications that have been implemented with the jABC. On the other hand, we reflect on our experiences and observations regarding the workflow development process with the framework. We then derive and discuss ongoing further developments and future perspectives for the framework, all with an emphasis on simplicity for end users through increased domain specificity. Concretely, we describe how\u00a0\u2026", "num_citations": "12\n", "authors": ["1616"]}
{"title": "Simplicity driven application development\n", "abstract": " In this article we discuss a simplicity-oriented approach to application development along a case study for engineering a web platform for orchestrating predefined service modules. Focus here is the growing demand for agility: 1) earlier solutions typically economically outperform later, more mature solutions, and 2) the half-value times of applications are drastically shrinking. Our simplicity-oriented approach therefore addresses both, the ease of application development, exploiting the so-called 80/20 principle, and the ease of take-up, aiming at removing potential hurdles for user adoption. We will discuss how this reflects on design decisions, in particular concerning the sweet spot where to change from adopting and adapting existing software to custom development.", "num_citations": "12\n", "authors": ["1616"]}
{"title": "Customer-oriented business process management: vision and obstacles\n", "abstract": " The role of ERP systems is to add value to customers through efficient processing of an organization\u2019s transactions (note that we consider transactions in a broad sense to include such events such as communication exchanges with customers and vendors) and data associated with those transactions. They also provide a platform for critical added value services and for the integration of business processes often well beyond a single organization\u2019s borders. Difficulties arise when ERP systems become monolithic, unable to respond to changes that impact the critical interests of the ERP user. Re-engineered systems which allow strong user-driven and dynamic redesign of business processes are an important goal in today\u2019s rapidly changing business environment. To tame the complexity of achieving this goal, we need new methods and guidelines on how to develop the necessary modules, how to fit them\u00a0\u2026", "num_citations": "12\n", "authors": ["1616"]}
{"title": "Property-driven functional healing: Playing against undesired behavior\n", "abstract": " In this paper, we show how to use GEAR, a game-based model checker, for property-driven functional healing of high-assurance systems. Designers and engineers can interactively investigate the winning strategies resulting from the games. These reveal in-depth information about the connection between the property, the system, and the proof, both as explanation in case of a successful proof, and as detailed, fine-granular error diagnostics in the case of failure. This results in an interactive use of the tool where debugging and redesign are carried out by playing against undesired behavior. The benefits of the approach are illustrated on a case study that concerns the design of the task-level control part of the processes of the ExoMars Rover [Kap05], which was designed as part of a European Space Agency (ESA) project.", "num_citations": "12\n", "authors": ["1616"]}
{"title": "Optimal data flow analysis via observational equivalence\n", "abstract": " In [18] a three level model was presented to establish a concept of completeness or optimality for data flow analysis algorithms in the framework of abstract interpretation [2]. The notion of observational equivalence introduced here generalizes the idea of the three level model, which can only deal with hierarchies of abstract interpretations. Investigating this more general notion, it actually turns out that the three level model is general in a theoretical sense: it determines the most abstract computation level which delivers complete results. However, consideration of other aspects of data flow analysis profit from the extra generality of our observation directed approach. For example the completeness or optimality proof for a \u201creal life\u201d optimizer could be shortened significantly this way.", "num_citations": "12\n", "authors": ["1616"]}
{"title": "The physics of software tools: SWOT analysis and vision\n", "abstract": " This paper reviews the seemingly inevitable trend that software tools are no longer just a means for supporting the design, construction, and analysis of (large-scale) systems, but become so complex that each of them turns into a reality of their own, with its own \u201cphysics\u201d, that needs to be studied in its own right. The true effects of combining methodologies as diverse as classical static analysis, model checking, SAT and SMT solving, and dynamic methods such as simulation, runtime verification, testing, and learning, with their dedicated means of optimizations in terms of, e.g., BDD coding, parallelization, and various forms of abstraction and reduction, are very dependent on the particular tools and typically hardly predictable. Corresponding experimental investigations, today often supported by diverse and frequent tool challenges, provide interesting indications about the applied technology, but typically fail\u00a0\u2026", "num_citations": "11\n", "authors": ["1616"]}
{"title": "LearnLib tutorial: from finite automata to register interface programs\n", "abstract": " Motivation               In the past decade, active automata learning, an originally merely theoretical enterprise, got attention as a method for dealing with black-box or third party systems. Applications ranged from the support of formal verification, e.g. for assume guarantee reasoning [4], to usage of learned models as the basis for regression testing. In the meantime, a number of approaches exploiting active learning for validation [17,20,6,7,2,1] emerged.", "num_citations": "11\n", "authors": ["1616"]}
{"title": "Quality engineering: leveraging heterogeneous information\n", "abstract": " In this paper we present a flexible framework for fine tuning the quality of program analysis based on variations, generalizations, and pragmatic extensions of Plotkin\u2019s Structured Operational Semantics (SOS). Key to these variations is the idea of Property-Oriented Expansion, here the non-standard use of the data component in SOS configurations, which ranges from simple abstract interpretations, over arbitrary data flow information, to e.g., temporal constraints. In its most general form, which is characterized by the notion of unifying models, this results in a framework not only for fine-tuning program analysis according to an aspect (quality) of choice, but also for synthesizing orchestrations for service-oriented applications based on loose temporal specifications. From an engineering perspective, the simple interface pattern underlying the unifying models approach was key for realizing our experimental\u00a0\u2026", "num_citations": "11\n", "authors": ["1616"]}
{"title": "Evolution support in heterogeneous service-oriented landscapes\n", "abstract": " We present an approach that provides automatic or semi-automatic support for evolution and change management in heterogeneous legacy landscapes where (1)\u00a0legacy heterogeneous, possibly distributed platforms are integrated in a service oriented fashion, (2)\u00a0the coordination of functionality is provided at the service level, through orchestration, (3)\u00a0compliance and correctness are provided through policies and business rules, (4)\u00a0evolution and correctness-by-design are supported by the eXtreme Model Driven Development paradigm (XMDD) offered by the jABC (Margaria and Steffen in Annu. Rev. Commun. 57, 2004)\u2014the model-driven service oriented development platform we use here for integration, design, evolution, and governance. The artifacts are here semantically enriched, so that automatic synthesis plugins can field the vision of Enterprise Physics: knowledge driven business process development for the end user. We demonstrate this vision along a concrete case study that became over the past three years a benchmark for Semantic Web Service discovery and mediation. We enhance the Mediation Scenario of the Semantic Web Service Challenge along the 2 central evolution paradigms that occur in practice: (a)\u00a0Platform migration: platform substitution of a legacy system by an ERP system and (b)\u00a0Backend extension: extension of the legacy Customer Relationship Management (CRM) and Order Management System (OMS) backends via an additional ERP layer.", "num_citations": "11\n", "authors": ["1616"]}
{"title": "Semantics-Based Composition of EMBOSS Services with Bio-jETI.\n", "abstract": " Bio-jETI is a framework for model-based, graphical design, execution and management of bioinformatics analysis processes. Formal methodology like automatic service composition extends the framework and, in particular, allows for semantically aware workflow development. In this study we apply the workflow synthesis methodology to the EMBOSS suite of sequence analysis tools. As neither the tool suite itself nor its various interfaces provide ready-to-use semantic annotations, we set up a domain model that uses a high-level, semantically meaningful type nomenclature to describe the input/output behavior of the single EMBOSS tools. Based on this domain model, we demonstrate how working with the large, heterogeneous, and hence manually intractable EMBOSS collection is simplified by our service composition methodology.", "num_citations": "11\n", "authors": ["1616"]}
{"title": "Building code generators with Genesys: a tutorial introduction\n", "abstract": " Automatic code generation is a key feature of model-driven approaches to software engineering. In previous publications on this topic, we showed that constructing code generators in a model-driven way provides a lot of advantages. We presented Genesys, a code generation framework which supports the model-driven construction of code generators based on service-oriented principles. With this methodology, concepts like bootstrapping and reuse of existing components enable a fast evolution of the code generation library. Furthermore, the robustness of the code generators profits from the application of formal methods. In this paper, we will show in detail how code generators are constructed with Genesys, in a tutorial-like fashion. As an example, we will build a code generator for HTML documentation from scratch.", "num_citations": "11\n", "authors": ["1616"]}
{"title": "Parallelism for free: Bitvector analyses\u21d2 no state explosion!\n", "abstract": " One of the central problems in the automatic analysis of distributed or parallel systems is the combinatorial state explosion leading to models, which are exponential in the number of their parallel components. The only known cure for this problem are application specific techniques, which avoid the state explosion problem under special frame conditions. In this paper we present a new such technique, which is tailored to bitvector analyses, which are very common in data flow analysis. In fact, our method allows to adapt most of the practically relevant optimizations for sequential programs, for a parallel setting with shared variables and arbitrary interference between parallel components.", "num_citations": "11\n", "authors": ["1616"]}
{"title": "Local model checking for context-free processes\n", "abstract": " We present a local model checking algorithm that decides for a given context-free process whether it satisfies a property written in the alternation-free modal mu-calculus. Heart of this algorithm is a purely syntactical sound and complete formal system, which in contrast to the known tableaux techniques, uses intermediate second-order assertions. These assertions provide a finite representation of all the infinite state sets which may arise during the proof in terms of the finite representation of the context-free argument process. This is the key to the effectiveness of our local model checking procedure.", "num_citations": "11\n", "authors": ["1616"]}
{"title": "Intelligent software synthesis in the DaCapo environment\n", "abstract": " The DaCapo software synthesis environment presented in this paper supports the automatic construction of software from existing components according to a (possibly incomplete) specification. Based on a specification language that uniformly combines taxonomic component specifications, interface conditions, and ordering constraints, our method adds a global view to conventional single component retrieval. Together with its semi-automatic archiving facility for theretrievable'inclusion of a satisfactory new software component into the software repository, our systems supports a full software lifecycle, as we illustrate along a user session.", "num_citations": "11\n", "authors": ["1616"]}
{"title": "Finite constants: Characterizations of a new decidable set of constants\n", "abstract": " Constant propagation, the replacement of program terms which represent a unique value at run time by their values, is a classical program optimization method. In spite of being treated for years, constant propagation still has been in the unsatisfactory phase of heuristics. We enhance the known constant propagation techniques to obtain an algorithm which is optimal for programs without loops. Fundamental is the introduction of a new decidable set of constants, the finite constants. This set has two different characterizations: a denotational one, which directly specifies our iterative algorithm and an operational one, which delivers the completeness or optimality of this algorithm for programs without loops. The algorithm is implemented in a commercial compiler project.", "num_citations": "11\n", "authors": ["1616"]}
{"title": "Finite constants: Characterizations of a new decidable set of constants\n", "abstract": " Constant propagation \u2014 the replacement of program terms which represent a unique value at run time by their values \u2014 is a classical program optimization method. In spite of being treated for years, constant propagation still has been in the unsatisfactory phase of heuristics. We enhance the known constant propagation techniques to obtain an algorithm which is optimal for programs without loops. Fundamental is the introduction of a new decidable set of constants, the finite constants. This set has two different characterizations: a denotational one, which directly specifies our iterative algorithm and an operational one, which delivers the completeness or optimality of this algorithm for programs without loops. The algorithm is implemented in a commercial compiler project.", "num_citations": "11\n", "authors": ["1616"]}
{"title": "Secube (tm): an open security platform-general approach and strategies\n", "abstract": " Abstract The SEcube\u2122(Secure Environment cube) platform presented in this special track is an open source securityoriented hardware and software platform constructed with ease of integration and service-orientation in mind. Its hardware component is a SoC platform: a single-chip design embedding three main cores: a highly powerful processor, a Common Criteria certified smartcard, and a flexible FPGA. The software components include several libraries of readyto use components that provide developers with different entry levels to adoption. The software is modular, and available as API or as services in an advanced model driven design environment. This way, security experts can avail of the open source character, and verify, change, or write from scratch the entire system, starting from the elementary lowlevel blocks, but at the same time we support also developers who use the predefined primitives and can experience the SEcube\u2122 as a high-security black box. This paper explains its aims and architecture, while the other papers detail the unique aspects of the overall platform.", "num_citations": "10\n", "authors": ["1616"]}
{"title": "Assuring property conformance of code generators via model checking\n", "abstract": " Automatic code generation is an essential cornerstone of today\u2019s model-driven approaches to software engineering. Thus a key requirement for the success of this technique is the reliability and correctness of code generators. This article describes how we employ standard model checking-based verification to check that code generator models developed within our code generation framework Genesys conform to (temporal) properties. Genesys is a graphical framework for the high-level construction of code generators on the basis of an extensible library of well-defined building blocks along the lines of the Extreme Model-Driven Development paradigm. We will illustrate our verification approach by examining complex constraints for code generators, which even span entire model hierarchies. We also show how this leads to a knowledge base of rules for code generators, which we constantly extend by e.g\u00a0\u2026", "num_citations": "10\n", "authors": ["1616"]}
{"title": "Game-based model checking for reliable autonomy in space\n", "abstract": " Autonomy is an emerging paradigm for the design and implementation of managed services and systems. Self-managed aspects frequently concern the communication of systems with their environment. Self-management subsystems are critical, they should thus be designed and implemented as high-assurance components. Here, we propose to use GEAR, a game-based model checker for the full modal \u03bc-calculus, and derived, more user-oriented logics, as a user friendly tool that can offer automatic proofs of critical properties of such systems. Designers and engineers can interactively investigate automatically generated winning strategies resulting from the games, this way exploring the connection between the property, the system, and the proof. The benefits of the approach are illustrated on a case study that concerns the ExoMars Rover.", "num_citations": "10\n", "authors": ["1616"]}
{"title": "Aggressive Model-Driven Development: Synthesizing Systems from Models viewed as Constraints.\n", "abstract": " We propose an aggressive version of model-driven development (AMDD) 1, which moves most of the recurring problems of compatibility and consistency of software (mass-) construction and customization from the coding and integration level to the modelling level. AMDD requires a complex preparation of adequate settings, supporting the required automation. However, the effort to create these settings can be easily paid off by immense cost reductions in software mass-construction and maintenance. In fact, besides reducing the costs, AMDD will also lead towards a kind of normed software development, making software engineering a true engineering activity.", "num_citations": "10\n", "authors": ["1616"]}
{"title": "Interprocedural analysis (almost) for free\n", "abstract": " We consider a procedural language with assignments and C-like functions with call-by-value parameters and single return values and present the rst algorithm for precise inter-procedural value-numbering. The algorithm is precise in that it infers for all program points all inter-procedurally valid Herbrand equalities. Moreover, it is asymptotically no more expensive than the best known algorithm for the same intra-procedural problem. Instead of un-interpreted operator symbols in assignments, we also consider ane assignments and the problem of inferring all inter-procedurally valid ane relationships between program variables. For this problem, we obtain a new polynomial algorithm whose complexity matches the complexity of the intra-procedural analysis in [21] and thus improves on the corresponding inter-procedural algorithm in [20] by a factor of k 5 where k is the number of variables.", "num_citations": "10\n", "authors": ["1616"]}
{"title": "Automatic error location for in service definition\n", "abstract": " The paper presents a new unique feature of the IN-METAFrame Service Definition Environment: the automatic generation of diagnostic location information as a consequence of detecting an error in the design phase of a Service Logic. Violations of constraints which express frame conditions for the design (concerning e.g. implementability, country specific standards, and network specific features) are detected by formal verification techniques. The subsequent error diagnosis and correction is now supported by a new kind of abstract views, which not only give hints on the possible source of trouble, tut additionally automatically locate the exact occurrence of the constraint violation in the Service Logic.", "num_citations": "10\n", "authors": ["1616"]}
{"title": "The methodology of modal constraints\n", "abstract": " We present a complete solution of the RPC-Memory Specification Problem, by applying a constraint-oriented state-based proof methodology for concurrent software systems. Our methodolgy exploits compositionality and abstraction for the reduction of the verification problem under investigation. Formal basis for this methodology are Modal Transition Systems allowing loose state-based specifications, which can be refined by successively adding constraints. Key concepts of our method are projective views, separation of proof obligations, Skolemization and abstraction. Central to the method is the use of Parametrized Modal Transition Systems. The method extends elegantly to real-time systems.", "num_citations": "10\n", "authors": ["1616"]}
{"title": "The META-Frame: An environment for flexible tool management\n", "abstract": " META-Frame is designed for the flexible management of a large library of tools. It supports the semi-antomatic construction of application-specific tools from natural language-like'profiles', as well as the inclusion of new tools into the library, in a way which does not require much expertise. Special care has been taken in the design of an adequate, almost natural-language specification language, a user-friendly graphical interface, a hypertext based navigation tool, the repository management, and in the automation of the synthesis process. This application-independent core has a firm theoretical basis: the synthesis is conceptually based on a model construction algorithm for a linear time temporal logic and supports an advanced form of Meta-level Intelligent Software Synthesis (MISS [SFCM94]). Altogether, M ETA-Fra me supports a full synthesis lifecycle (Figure 1) by1. providing a flexible specification language that\u00a0\u2026", "num_citations": "10\n", "authors": ["1616"]}
{"title": "Back-to-back testing of model-based code generators\n", "abstract": " In this paper, we present the testing approach of the Genesys code generator framework. The employed approach is based on back-to-back-testing, which tests the translation performed by a code generator from a semantic perspective rather than just checking for syntactic correctness of the generation result. We describe the basic testing framework and show that it scales in three dimensions: parameterized tests, testing across multiple target platforms and testing on multiple meta-levels.               In particular, the latter is only possible due to the fact that Genesys code generators are constructed as models. Furthermore, in order to facilitate simplicity, Genesys consistently employs one single notation for all artifacts involved in this testing approach: Test data, test cases, the code generators under test, and even the testing framework itself are all modeled using the same graphical modeling language.", "num_citations": "9\n", "authors": ["1616"]}
{"title": "Comparison: Mediation on webML/webRatio and jABC/jETI\n", "abstract": " In this chapter we compare two solutions to the mediation scenario of the SWS challenge that are based on the use of WebML [1] and of the jABC [2, 3] as modeling and execution platforms. In particular, first we give a general overview of the differences among the to approaches, and then we compare in the details the two solutions for the SWS challenge.               We use selected parts from the mediation scenarios to keep the comparison simple but expressive. Looking on the advanced scenarios would be more complex but would obviously lead to the same results.", "num_citations": "9\n", "authors": ["1616"]}
{"title": "Component-oriented behavior extraction for autonomic system design\n", "abstract": " Rich and multifaceted domain specific specification languages like the Autonomic System Specification Language (ASSL) help to design reliable systems with self-healing capabilities. The GEAR game-based Model Checker has been used successfully to investigate properties of the ESA Exo-Mars Rover in depth. We show here how to enable GEAR\u2019s game-based verification techniques for ASSL via systematic model extraction from a behavioral subset of the language, and illustrate it on a description of the Voyager II space mission.", "num_citations": "9\n", "authors": ["1616"]}
{"title": "Leveraging Applications of Formal Methods, Verification and Validation\n", "abstract": " It is our responsibility, as general and program chairs, to welcome the participants to the 9th International Symposium on Leveraging Applications of Formal Methods, Verification and Validation (ISoLA), planned to take place in Rhodes, Greece, during October 20\u201330, 2020, endorsed by the European Association of Software Science and Technology (EASST).This year\u2019s event follows the tradition of its symposia forerunners held in Paphos, Cyprus (2004 and 2006), Chalkidiki, Greece (2008), Crete, Greece (2010 and 2012), Corfu, Greece (2014 and 2016), and most recently in Limassol, Cyprus (2018), and the series of ISoLA workshops in Greenbelt, USA (2005), Poitiers, France (2007), Potsdam, Germany (2009), Vienna, Austria (2011), and Palo Alto, USA (2013). Considering that this year\u2019s situation is unique and unlike any previous one due to the ongoing COVID-19 pandemic, and that ISoLA\u2019s symposium touch\u00a0\u2026", "num_citations": "9\n", "authors": ["1616"]}
{"title": "Demonstration of an operational procedure for the model-based testing of CTI systems\n", "abstract": " In this demonstration we illustrate how a posteriori modeling of complex, heterogeneous, and distributed systems is practically performed within an automated integrated testing environment (ITE) to give improved support to the testing process of steadily evolving systems. The conceptual background of the modeling technique, called moderated regular extrapolation is described in a companion paper [3].", "num_citations": "9\n", "authors": ["1616"]}
{"title": "Distinguishing formulas for free\n", "abstract": " A system for the efficient verification of the input/output correctness of finite state machines with data path and control unit is presented. This system, which is based on First-Order Logic theorem proving, automatically provides distinguishing formulas expressing all test patterns that witness a behavioral difference between two descriptions. Experimental results illustrate the test pattern generation feature for stuck-at faults, functional faults, and initialization faults, and the efficiency which results from the compositionality of the verification.< >", "num_citations": "9\n", "authors": ["1616"]}
{"title": "Active mining of document type definitions\n", "abstract": " In this paper, we present the application of our active learning algorithm for Systems of Procedural Automata (SPAs) for inferring Document Type Definitions (DTDs) via testing of corresponding document validators. The point of this specification mining approach is to reveal unknown (lost or hidden) syntactic document constraints that are automatically imposed by document validators in order to support document writers or to validate whether a certain validator implementation does indeed satisfy its specification. This is particularly interesting in the context of today\u2019s General Data Protection Regulation (GDPR) as their violation might lead to substantial penalties. The practicality of this approach is supported by the fact that for inferred complex DTDs, context-free model checking may be used to automatically validate whether business-critical rules are enforced by a validator and therefore automatically\u00a0\u2026", "num_citations": "8\n", "authors": ["1616"]}
{"title": "Cinco products for the web\n", "abstract": " The development of today's software systems, which use several different technologies, is a difficult and complex task. It is necessary to define a software with the close collaboration of a domain expert, who has the idea of the software and it's behavior and the developer, who tries to analyze, interpret the earned information and finally plan and realize it. This translation of domain specific ideas of a software to an abstract model, which can be understood by a developer, is a difficult matter to deal with. To handle this problem, model-based development paradigms were introduced. These paradigms are based on the specification of an abstract model, which hides conceptual insignificant details from the developer. Depending on the manufactured model the software can be implemented. As an advantage of the model-based development and it's degree of abstraction the requirements are more precise, simple described and become less redundant. This leads to an accelerated realization and well understood domain specific concepts to the developers. To enhance the paradigms, modeling tools were established, which provide abilities to generate code from a given model. On the other hand of these model-based paradigms there are drawbacks like the high effort for the initial specification of a domain-model [10]. In addition to this it is necessary to implement further features and modifications for specialized requirements, which have to be reestablished to the superior model, so that the model and its code are synchronized. This additional effort leads to a disregarding of the model after its initial definition. Furthermore the tools for the model-driven\u00a0\u2026", "num_citations": "8\n", "authors": ["1616"]}
{"title": "Synthesizing the Mediator with jABC/ABC.\n", "abstract": " In this paper we show how to apply a tableau-based software composition technique to automatically generate the mediator\u2019s service logic. This uses an LTL planning (or configuration) algorithm originally embedded in the ABC and in the ETI platforms. The algorithm works on the basis of the existing jABC library of available services (SIB library) and of an enhanced description of their semantics given in terms of a taxonomic classification of their behaviour (modules) and abstract interfaces/messages (types).", "num_citations": "8\n", "authors": ["1616"]}
{"title": "Coarse-granular model checking in practice\n", "abstract": " In this paper, we illustrate the power of \u2018classical\u2019 iterative model checking for verifying quality of service requirements at the coarse granular level. Characteristic for our approach is the library-based modelling of applications in terms of service logic graphs, the incremental formalization of the underlying application domain in terms of temporal quality of service constraints, and the library-based consistency checking allowing continuous verification of application- and purpose-specific properties by means of model checking. This enables application experts to safely configure their own applications without requiring any specific knowledge about the underlying technology. The impact of this approach will be demonstrated in three orthogonal industrial application scenarios.", "num_citations": "8\n", "authors": ["1616"]}
{"title": "Tool Coordination in METAFrame\n", "abstract": " The MetaFrame tool integration platform is designed for the interactive combination and coordination of heterogeneous tools: complex tool combinations can be (semi-) automatically or interactively constructed and tested by on-linemeta-programming'in the interpreted, sequential tool coordination language HLL. The point of our design is the strict separation of a highly enhanced integration phase and a strongly supported coordination phase. This allows even non-experts to profitably coordinate the integrated tools to solve their own application-specific tasks. It is this profile which qualifies our coordination environment to serve as the Electronic Tool Integration (ETI) platform in the new international Springer Journal Software Tools for Technology Transfer (STTT).", "num_citations": "8\n", "authors": ["1616"]}
{"title": "Synthesizing subtle bugs with known witnesses\n", "abstract": " This paper presents a new technique for the generation of verification benchmarks that are automatically guaranteed to be hard, or as we say, to contain subtle bugs/property violations: (i) Identifying a bug requires to match many computation steps and (ii) corresponding counterexamples are sparse among all feasible executions. Key idea is to iteratively synthesize B\u00fcchi automata for variations of a set of LTL properties and to combine these automata in a fashion that each property can be individually controlled in the resulting model: Based on our notion of a counterexample handle, it is possible to switch the satisfaction of a given property on and off without affecting that of the other considered properties. This orthogonality of our treatment of counterexamples is vital for the subsequent parts of the benchmark generation process. Together with the mentioned hardness, it helps to overcome the undesired\u00a0\u2026", "num_citations": "7\n", "authors": ["1616"]}
{"title": "Securing C/C++ applications with a SEcube\u2122-based model-driven approach\n", "abstract": " In this paper we demonstrate the power and flexibility of extreme model-driven design using C-IME, our integrated modelling environment for C/C++ by showing how easily an application modelled in C-IME can be enhanced with hardware security features. In fact, our approach does not require any changes of the application model. Rather, C-IME provides a dedicated modelling language for code generators which embodies a palette of security primitives that are implemented based on the SEcube\u2122 API. We will illustrate how the required code generator can be modelled for a to-do list management application in our case study. It should be noted that this code generator is not limited to the considered application but it can be used to secure the file handling of any application modelled in C-IME.", "num_citations": "7\n", "authors": ["1616"]}
{"title": "Exploiting ecore's reflexivity for bootstrapping domain-specific code-generators\n", "abstract": " This paper shows how the reflexivity of Ecore can be exploited for incrementally bootstrapping domain-specific code generators in the model-driven and service-oriented code generation framework Genesys. Key to this technology is the EMF SIB Generator, which, based on a very small set of manually written code generator services called SIBs, incrementally generates services in a bootstrapping fashion. To this end, it leverages Ecore's metamodel, which is specified in Ecore itself, to iteratively enlarge the set of SIBs until all concepts of Ecore are covered. On this basis, the EMF SIB Generator can then be used to generate all services required for constructing a corresponding code generator for any given metamodel specified in Ecore. This approach can be staightforwardly applied to arbitrary metalevels and elegantly enables the model-driven and service-oriented construction of code generators for Ecore-based\u00a0\u2026", "num_citations": "7\n", "authors": ["1616"]}
{"title": "Comparison: handling preferences with DIANE and miAamics\n", "abstract": " In this chapter we compare the DIANE and miAamics solutions to service discovery along a specific feature supported by those solutions: preferences. Although quite different in their theoretical and technical background, both techniques have in fact the ability to express user preferences, that are used internally to rank the evaluation results. These preferences are used here to incorporate functional aspects as defined by the SWS Challenge tasks, but they can also be used to express non-functional properties like quality aspects. Here we take a closer look at how preferences are realized in the two different approaches and we briefly compare their profiles.", "num_citations": "7\n", "authors": ["1616"]}
{"title": "An Approach to Discovery with miAamics and jABC\n", "abstract": " We present a hybrid approach to service discovery that uses miAamics, a rule-based selection engine, as a matchmaker within the jABC, a framework for service-oriented process modelling, execution, and evolution. This approach aims at tailoring the service discovery process in such a way that different users with different technical and domain competence can efficiently participate at their level of expertise. We shape the collaboration between business experts and IT team following the well-known 80/20 principle: more than 80% of the discovery management, control, and use should not require any special IT knowledge. In particular, the specification of the set of weighted rules, which is miAamics' way of describing the aspect-oriented relevance of data/products/offers, can in our experience be dealt with by business experts without IT knowledge after a short training. Entering the predicates that describe the\u00a0\u2026", "num_citations": "7\n", "authors": ["1616"]}
{"title": "Supporting Process Development in Bio-jETI by Model Checking and Synthesis.\n", "abstract": " Bio-jETI is a platform for the intuitive graphical design and execution of bioinformatics workflows composed from heterogeneous remote services. In this paper we use a simple phylogenetic analysis process to show how formal approaches like model checking and process synthesis can be applied to further support the workflow development in Bio-jETI. To unfold their full potential these methods need a comprehensive knowledge base about the domain, containing semantic information about the single services as well as ontological classifications of the used terms. We outline how to systematically integrate these semantic web concepts into our framework and discuss the implications on checking and synthesis.", "num_citations": "7\n", "authors": ["1616"]}
{"title": "Model generation for legacy systems\n", "abstract": " We propose the use of (semi-) automatically extrapolated models as a means for coping with legacy systems: a focused way of testing systems for their behavioral properties allows the construction of expressive behavioral hypothesis models, and therefore extends the range of formal methods to \u2018black box\u2019 scenarios, which are dominant in industrial practice. Keeping these models up to date by continuous adaptation may provide an ideal way for controlling the evolution of large systems during their whole life cycles. Bottleneck of this approach is the size of the extrapolated models: particularly for distributed systems the state explosion problem strikes back. This paper focusses on a particularly promising cure: view-oriented model construction allows a new way of size control that complements other powerful techniques, which together have the potential to scale to systems of realistic size. This is illustrated\u00a0\u2026", "num_citations": "7\n", "authors": ["1616"]}
{"title": "Correct system design: recent insights and advances\n", "abstract": " Computers are gaining more and more control over systems that we use or rely on in our daily lives, privately as well as professionally. In safety-critical applications, as well as in others, it is of paramount importance that systems controled by a computer or computing systems themselves reliably behave in accordance with the specification and requirements, in other words: here correctness of the system, of its software and hardware is crucial. In order to cope with this callenge, software engineers and computer scientists need to understand the foundations of programming, how different formal theories are linked together, how compilers correctly translate high-level programs into machine code, and why transformations performed are justifiable. This book presents 17 mutually reviewed invited papers organized in sections on methodology, programming, automation, compilation, and application.", "num_citations": "7\n", "authors": ["1616"]}
{"title": "The eti online service in action\n", "abstract": " The Electronic Tool Integration platform (ETI) associated to the Intern. Journal on Software Tools for Technology Transfer (STTT) [7] is designed for the interactive experimentation with and coordination of heterogeneous tools1. ETI users are assisted by an advanced, personalized Online Service guiding experimentation, coordination and simple browsing of the available tool repository according to their degree of experience. In particular, this allows even newcomers to orient themselves in the wealth of existing tools and to identify the most appropriate collection of tools to solve their own application-specific tasks.", "num_citations": "7\n", "authors": ["1616"]}
{"title": "Coarse-grain component based software development: the MetaFrame approach\n", "abstract": " According to Joseph Goguen, the success of coarse-grain, system level programming in practice is conditioned by the availability of adequate, domain specific modelling and tool support: on the one hand side this enhances the reliability by automatic control, and on the other hand it speeds up the development process by supporting reuse (cf.[3]). The principles of generative programming listed in the statute of the Arbeitskreis \u201cGenerative und komponentenbasierte Softwareentwicklung\u201d of the GI-Fachgruppe 2.1. 9 \u201cObjektorientierte Softwareentwicklung\u201d closely reflect this point of view. They ask for simplification of the visible complexity of systems and require mechanisms for detecting semantically forbidden configurations. In addition they address more concrete aspects, like eg. the possibility of separation of concerns during the development, the adequate parameterization of components as a means for\u00a0\u2026", "num_citations": "7\n", "authors": ["1616"]}
{"title": "Design for \u2018X\u2019through model transformation\n", "abstract": " In this paper we sketch a transformation-oriented framework for establishing system characteristics like model-checkability, learnability, or performance. Backbone of our framework is Cinco, our meta tooling suite for generating DSL-specific development environments on the basis of specifications in terms of metamodels. Cinco is used here to specify the considered source and target languages, as well as the transformation language that allows one to transform source systems/models into semantically equivalent, X-conform target systems. In this paper, we focus on illustrating the power of domain-specific transformation languages by presenting a multi-level transformation pattern that allows one to elegantly capture refinement and aggregation aspects in a rule-based fashion. All this is explained along a number of examples.", "num_citations": "6\n", "authors": ["1616"]}
{"title": "M3C: Modal Meta Model Checking\n", "abstract": " M3C is a method and tool supporting meta-level product lining and evolution that comprises both context free system structure and modal refinement. The underlying Context-Free Modal Transition Systems can be regarded as loose specifications of meta models, and modal refinement as a way to increase the specificity of allowed DSLs by constraining the range of allowed syntax specifications. Model checking with M3C allows one to verify properties specified in a branching-time logic for all DSLs of a given level of specificity in one go, which is illustrated by looking at variations of an elementary programming language. Technically, M3C is based on second-order model checking which determines how procedure calls affect the validity of the properties of interest. The inherent compositionality of the second-order approach leads to a runtime complexity linear in the size of the procedural system\u00a0\u2026", "num_citations": "6\n", "authors": ["1616"]}
{"title": "Model-based testing without models: the TodoMVC case study\n", "abstract": " Web applications define the interface to many of the businesses and services that we interact with and use on a daily basis. The technology stack enabling these applications is constantly changing and applications are accessed from a plethora of different devices. Automated testing of the behavior of applications is a promising strategy for reducing the manual effort that has to be spent on ensuring a consistent user experience across devices. Unfortunately, specifications or models of the desired behavior often do not exist. Model-based testing without models (aka learning-based testing) tries to overcome this hurdle by integrating model learning and model-based testing. In this paper, we sketch the ALEX tool\u00a0[1, 11] for learning-based testing of web application and demonstrate its operation on benchmarks from the TodoMVC project. Our learning-based conformance analysis reveals that 7 of 27 Todo\u00a0\u2026", "num_citations": "6\n", "authors": ["1616"]}
{"title": "Tutorial: automata learning in practice\n", "abstract": " The paper reviews active automata learning with a particular focus on sources of redundancy. In particular, it gives an intuitive account of TTT, an algorithm based on three tree structures which concisely capture all the required information. This guarantees minimal memory consumption and it drastically reduces the length of membership queries, in particular in application scenarios like monitoring-based learning, where long counter examples arise. The essential steps and the impact of TTT are illustrated via experimentation with LearnLib, a free, open source Java library for active automata learning.", "num_citations": "6\n", "authors": ["1616"]}
{"title": "Leveraging Applications of Formal Methods, Verification and Validation. Specialized Techniques and Applications: 6th International Symposium, ISoLA 2014, Imperial, Corfu\u00a0\u2026\n", "abstract": " The two-volume set LNCS 8802 and LNCS 8803 constitutes the refereed proceedings of the 6th International Symposium on Leveraging Applications of Formal Methods, Verification and Validation, ISoLA 2014, held in Imperial, Corfu, Greece, in October 2014. The total of 67 full papers was carefully reviewed and selected for inclusion in the proceedings. Featuring a track introduction to each section, the papers are organized in topical sections named: evolving critical systems; rigorous engineering of autonomic ensembles; automata learning; formal methods and analysis in software product line engineering; model-based code generators and compilers; engineering virtualized systems; statistical model checking; risk-based testing; medical cyber-physical systems; scientific workflows; evaluation and reproducibility of program analysis; processes and data integration in the networked healthcare; semantic heterogeneity in the formal development of complex systems. In addition, part I contains a tutorial on automata learning in practice; as well as the preliminary manifesto to the LNCS Transactions on the Foundations for Mastering Change with several position papers. Part II contains information on the industrial track and the doctoral symposium and poster session.", "num_citations": "6\n", "authors": ["1616"]}
{"title": "Second-order value numbering\n", "abstract": " We present second-order value numbering, a new optimization method for suppressing redundancy, in a version tailored to the application for optimizing the decision procedure of jMosel, a verification tool set for monadic second-order logic on strings (M2L (Str)). The method extends the well-known concept of value", "num_citations": "6\n", "authors": ["1616"]}
{"title": "Demonstration of an automated integrated test environment for web-based applications\n", "abstract": " The state of the art of test tools for web-based applications is still dominated by static approaches, focussing on a posteriori link-structure reconstruction and interpretation [2],[6], e.g. to determine unreachable pages, rather than on functional aspects directly related to the application\u2019s behaviour. As it has happened in the areas of telecommunications, evolution in this direction is however necessary in order to test applications also for their intention, rather than just for environmental accidents.", "num_citations": "6\n", "authors": ["1616"]}
{"title": "Service Definition for Intelligent Networks: Experience in a Leading-edge Technological Project Based on Constraint Techniques\n", "abstract": " The paper reports on the use and impact of constraint-based verification and synthesis techniques in Siemens' INXpress Advanced Service Design tool, an industrial product for the design of IntelligentNetwork services (customized added-value telephone services) which reached the market in early 1996. Indeed, the introduction of model checking and synthesis wrt. constraints, here written in a modal logic, were the key to the the flexibilization of design and customization phases, and to the enhanced reliability of the services, which are central in today's customer's requirements.", "num_citations": "6\n", "authors": ["1616"]}
{"title": "Towards explainability in machine learning: The formal methods way\n", "abstract": " Explainable Al is a new direction aiming at the maturation of a fi eld that has experienced a boost in particular because of its fancy heuristics and corresponding breakthroughs in specific applications like the AlphaGo program for the game Go. In this context, the typical concept of\" explanation\" is still comparatively weak. For example, highlighting the most important pixel for a certain image classification is not really a comprehensive explanation, but rather a hint, an indication that helps pinpoint situations where things went drastically wrong. In contrast we take a formal methods-based path, originally established in STTT, 5 where the concept of\" explanation\" is interpreted as a precise characterization of the considered phenomenon. Our illustration on how much information about the how and why can be extracted with exact methods from a random forest consisting of 100 trees indicates that such characterization may\u00a0\u2026", "num_citations": "5\n", "authors": ["1616"]}
{"title": "Add-lib: Decision diagrams in practice\n", "abstract": " In the paper, we present the ADD-Lib, our efficient and easy to use framework for Algebraic Decision Diagrams (ADDs). The focus of the ADD-Lib is not so much on its efficient implementation of individual operations, which are taken by other established ADD frameworks, but its ease and flexibility, which arise at two levels: the level of individual ADD-tools, which come with a dedicated user-friendly web-based graphical user interface, and at the meta level, where such tools are specified. Both levels are described in the paper: the meta level by explaining how we can construct an ADD-tool tailored for Random Forest refinement and evaluation, and the accordingly generated Web-based domain-specific tool, which we also provide as an artifact for cooperative experimentation. In particular, the artifact allows readers to combine a given Random Forest with their own ADDs regarded as expert knowledge and to experience the corresponding effect.", "num_citations": "5\n", "authors": ["1616"]}
{"title": "Methods for generating selection structures, for making selections according to selection structures and for creating selection descriptions\n", "abstract": " A real-time system receives a selection structure formed by weighted rules joined by links, each of the weighted rules including a condition part, a conclusion part and a weight. The selection structure is an Algebraic Decision Diagram (ADD) that includes internal nodes, each representing a decision point and terminal nodes, each representing a subset of candidate results having an accumulated weight. In response to a query that specifies a set of conditions, the ADD is traversed by making a series of decisions at decision points of the ADD to reach at least one terminal node having a maximum accumulated weight among terminal nodes that satisfy the set of conditions. The subset of candidate results represented by the at least one terminal node is selected as the output for responding to the query.", "num_citations": "5\n", "authors": ["1616"]}
{"title": "Rigorous examination of reactive systems\n", "abstract": " In this paper we present the RERS challenge 2015, a free-style program analysis challenge on reactive systems to evaluate the effectiveness of different validation and verification techniques. It brings together researchers from different areas including static analysis, model checking, theorem proving, symbolic execution, and testing. The challenge characteristics and set-up are discussed, while special attention is given to the Runtime Verification track that was newly introduced.", "num_citations": "5\n", "authors": ["1616"]}
{"title": "Fuzzy logic and applications\n", "abstract": " The 10th International Workshop on Fuzzy Logic and Applications, WILF 2013, held in Genoa (Italy) during November 19\u201322, 2013, covered topics related to theoretical, experimental, and applied fuzzy techniques and systems with emphasis on their applications in the analysis of high-dimensional data. This event represents the pursuance of an established tradition of biannual interdisciplinary meetings. The previous editions of WILF have been held, with an increasing number of participants, in Naples (1995), Bari (1997), Genoa (1999), Milan (2001), Naples (2003), Crema (2005), Camogli (2007), Palermo (2009), and Trani (2011). Each event has focused on distinct main thematic areas of fuzzy logic and related applications. From this perspective, one of the main goals of the WILF workshops series is to bring together researchers and developers from both academia and high-tech companies and foster\u00a0\u2026", "num_citations": "5\n", "authors": ["1616"]}
{"title": "FISER: An effective method for detecting interactions between topic persons\n", "abstract": " Discovering the interactions between the persons mentioned in a set of topic documents can help readers construct the background of the topic and facilitate document comprehension. To discover person interactions, we need a detection method that can identify text segments containing information about the interactions. Information extraction algorithms then analyze the segments to extract interaction tuples and construct an interaction network of topic persons. In this paper, we define interaction detection as a classification problem. The proposed interaction detection method, called FISER, exploits nineteen features covering syntactic, context-dependent, and semantic information in text to detect interactive segments in topic documents. Empirical evaluations demonstrate the efficacy of FISER, and show that it significantly outperforms many well-known Open IE methods.", "num_citations": "5\n", "authors": ["1616"]}
{"title": "Mashup Development for Everybody A Planning-Based Approach\n", "abstract": " Today\u2019s service mashup technologies usually focus on assisting programmers to provide more powerful and valuable integrated applications to the users. A significant set of scripting languages, graphical tools and web services are used for this purpose, all addressing users with significant IT background. This paper aims at extending the power of mashup development to end users and application experts by automatically taking care of the tedious technical details like interface specifications, types, and syntactic constraints. In detail we support simple and intuitive mashup specifications which are automatically completed to runnable mashups by means of service discovery-like methods and planning. We illustrate our approach by means of a concrete case study executed within our jABC/jETI development and (remote) execution framework. 1", "num_citations": "5\n", "authors": ["1616"]}
{"title": "From bio-jETI process models to native code\n", "abstract": " Bio-jETI is a framework for model-based, graphical development and execution of bioinformatics analysis processes. With the GeneSys code generation framework we can automatically compile the workflow models into native, stand-alone program code. We show via a phylogenetic analysis workflow designed by the DNA Data Bank of Japan (DDBJ) how we generate 6 variants of Java code from the corresponding process model realized in Bio-jETI. Performance measurements show that 1) the overall workflow execution time is dominated by the remote services it uses, and thus 2)all 6 variants are almost as fast as the handwritten Java of DDBJ. This way, we obtain efficient native code essentially without program- ming. Thus, we demonstrate in this paper that model- based workflow development in Bio-jETI offers several advantages over manual implementation \u2013 including higher agility, greater transparency and\u00a0\u2026", "num_citations": "5\n", "authors": ["1616"]}
{"title": "Web Information Systems Engineering-WISE 2008: 9th International Conference, Auckland, New Zealand, September 1-3, 2008, Proceedings\n", "abstract": " This book constitutes the proceedings of the 9th International Conference on Web Information Systems Engineering, WISE 2008, held in Auckland, New Zealand, in September 2008. The 17 revised full papers and 14 revised short papers presented together with two keynote talks were carefully reviewed and selected from around 110 submissions. The papers are organized in topical sections on grid computing and peer-to-peer systems; Web mining; rich Web user interfaces; semantic Web; Web information retrieval; Web data integration; queries and peer-to-peer systems; and Web services.", "num_citations": "5\n", "authors": ["1616"]}
{"title": "A constraint oriented proof methodology\n", "abstract": " A Constraint Oriented Proof Methodology \u2014 Aalborg University's Research Portal Skip to main navigation Skip to search Skip to main content Aalborg University's Research Portal Logo Dansk English Home Profiles Projects Publications Activities Research Units Facilities Press / Media Prizes Datasets Impacts Search by keywords, name or affiliation A Constraint Oriented Proof Methodology Kim Guldstrand Larsen, B. Steffen, C. Weise Department of Computer Science Department of Electronic Systems Research output: Contribution to journal \u203a Journal article \u203a Research \u203a peer-review Overview Original language English Book series Lecture Notes in Computer Science Volume No. 1019 ISSN 0302-9743 Publication status Published - 1996 Cite this APA Author BIBTEX Harvard Standard RIS Vancouver Larsen, KG, Steffen, B., & Weise, C. (1996). A Constraint Oriented Proof Methodology. Lecture Notes in Computer \u2026", "num_citations": "5\n", "authors": ["1616"]}
{"title": "Optimal code motion for parallel programs\n", "abstract": " Code motion is well-known as a powerful technique for the optimization of sequential programs. It improves the run-time efficiency by avoiding unnecessary recomputations of values, and it is even possible to obtain computationally optimal results, ie, results where no program path can be improved any further by means of semantics preserving code motion. In this paper we present a code motion algorithm that for the first time achieves this optimality result for parallel programs. Fundamental is the framework of KSV1] showing how to perform optimal bitvector analyses for parallel programs as easily and as efficiently as for sequential ones. Moreover, the analyses can easily be adapted from their sequential counterparts. This is demonstrated here by constructing a computationally optimal code motion algorithm for parallel programs by systematically extending its counterpart for sequential programs, the busy code motion transformation of KRS1, KRS2].", "num_citations": "5\n", "authors": ["1616"]}
{"title": "A Model-driven Approach to Continuous Practices for Modern Cloud-based Web Applications\n", "abstract": " In this paper, we propose a model-driven approach to Continuous Software Integration and Deployment (CI/CD) for modern cloud-based applications. Key to our approach is a formal graphical modelling language for the specification of the processes and tasks involved. Based on these specifications the complete CI/CD configurations are generated fully and automatically guaranteeing their correctness with regard to the specification by construction. This way typical sources of critical errors can be avoided lowering the hurdle to introduce CI/CD especially in mature projects. We demonstrate the power of our model-driven approach with the help of an industrial web application - a prime example for cloud-based applications.", "num_citations": "4\n", "authors": ["1616"]}
{"title": "Product line verification via modal meta model checking\n", "abstract": " Modal Meta Model Checking (M3C) is a method and tool supporting meta-level product lining and evolution that comprises both context-free system structure and modal refinement. The underlying Context-Free Modal Transition Systems (CFMTSs) can be regarded as loose specifications of meta models, and modal refinement as a way to increase the specificity of allowed domain specific languages (DSLs) by constraining the range of allowed syntax specifications. Model checking with M3C allows one to verify properties specified in a branching-time logic for all DSLs of a given level of specificity in one go. The paper illustrates the impact of M3C in an industrial setting where well-formed documents serve as contracts between a provider and its customers in two steps: it establishes CFMTS as a formalism to specify product lines of document description types (DTDs \u2013 or related formalisms like JSON\u00a0\u2026", "num_citations": "4\n", "authors": ["1616"]}
{"title": "A model-driven and generative approach to holistic security\n", "abstract": " Functional and technical cyber-resilience gain increasing relevance for the health and integrity of connected and interoperating systems. In this chapter we demonstrate the power and flexibility of extreme model-driven design to provide holistic security to security-agnostic applications. Using C-IME, our integrated modelling environment for C/C++, we show how easily a modelled application can be enhanced with hardware security features fully automatically during code generation. We illustrate how to use this approach and design environment to make any modelled application ready to securely store its data in potentially insecure environments. The same approach can be used to secure communication over potentially insecure channels. In fact, our approach does not require any changes of the application model. Rather, our integrated modelling environment provides a dedicated modelling language\u00a0\u2026", "num_citations": "4\n", "authors": ["1616"]}
{"title": "Towards a unified view of modeling and programming (ISoLA 2018 track introduction)\n", "abstract": " The article provides an introduction to the track: Towards a Unified View of Modeling and Programming, organized by the authors of this paper as part of ISoLA 2018: the 8th International Symposium On Leveraging Applications of Formal Methods, Verification and Validation. A total of 19 researchers presented their views on the two questions: what are the commonalities between modeling and programming languages?, and should we strive towards a unified view of modeling and programming? The idea behind the track, which is a continuation of a similar track at ISoLA 2016, emerged as a result of experiences gathered in the three fields: formal methods, model-based software engineering, and programming languages, and from the observation that these technologies share a large common part, to the extent where one may ask, does the following equation hold:", "num_citations": "4\n", "authors": ["1616"]}
{"title": "Leveraging Applications of Formal Methods, Verification and Validation. Distributed Systems: 8th International Symposium, ISoLA 2018, Limassol, Cyprus, November 5-9, 2018\u00a0\u2026\n", "abstract": " The four-volume set LNCS 11244, 11245, 11246, and 11247 constitutes the refereed proceedings of the 8th International Symposium on Leveraging Applications of Formal Methods, Verification and Validation, ISoLA 2018, held in Limassol, Cyprus, in October/November 2018. The papers presented were carefully reviewed and selected for inclusion in the proceedings. Each volume focusses on an individual topic with topical section headings within the volume: Part I, Modeling: Towards a unified view of modeling and programming; X-by-construction, STRESS 2018. Part II, Verification: A broader view on verification: from static to runtime and back; evaluating tools for software verification; statistical model checking; RERS 2018; doctoral symposium. Part III, Distributed Systems: rigorous engineering of collective adaptive systems; verification and validation of distributed systems; and cyber-physical systems engineering. Part IV, Industrial Practice: runtime verification from the theory to the industry practice; formal methods in industrial practice-bridging the gap; reliable smart contracts: state-of-the-art, applications, challenges and future directions; and industrial day.", "num_citations": "4\n", "authors": ["1616"]}
{"title": "Leveraging Applications of Formal Methods, Verification and Validation: Foundational Techniques: 7th International Symposium, ISoLA 2016, Imperial, Corfu, Greece, October 10\u201314\u00a0\u2026\n", "abstract": " The two-volume set LNCS 9952 and LNCS 9953 constitutes the refereed proceedings of the 7th International Symposium on Leveraging Applications of Formal Methods, Verification and Validation, ISoLA 2016, held in Imperial, Corfu, Greece, in October 2016. The papers presented in this volume were carefully reviewed and selected for inclusion in the proceedings. Featuring a track introduction to each section, the papers are organized in topical sections named: statistical model checking; evaluation and reproducibility of program analysis and verification; ModSyn-PP: modular synthesis of programs and processes; semantic heterogeneity in the formal development of complex systems; static and runtime verification: competitors or friends?; rigorous engineering of collective adaptive systems; correctness-by-construction and post-hoc verification: friends or foes?; privacy and security issues in information systems; towards a unified view of modeling and programming; formal methods and safety certification: challenges in the railways domain; RVE: runtime verification and enforcement, the (industrial) application perspective; variability modeling for scalable software evolution; detecting and understanding software doping; learning systems: machine-learning in software products and learning-based analysis of software systems; testing the internet of things; doctoral symposium; industrial track; RERS challenge; and STRESS.", "num_citations": "4\n", "authors": ["1616"]}
{"title": "Grundlagen der h\u00f6heren Informatik\n", "abstract": " Inspiriert ist der Titel dieser in drei B\u00e4nden erscheinenden Reihe vom Begriff der h\u00f6heren Mathematik, der den kompetenten natur-und ingenieurwissenschaftlichen Umgang mit der Mathematik auf universit\u00e4rem Niveau umschreibt. Dort geht es also um eine mathematische Pragmatik jenseits von Rechnen und Einsetzen, aber diesseits involvierter Beweise und den j\u00fcngsten mathematischen Resultaten. In Analogie umfasst die h\u00f6here Informatik eine konzeptorientierte, informatische Pragmatik jenseits von Programmieren, Skripten und Klicken, aber diesseits der theorieorientierten Informatikforschung. Die h\u00f6here Informatik ist also klar anwendungsorientiert. Die zugeh\u00f6rigen Grundlagen geh\u00f6ren jedoch ebenso wie in der Mathematik in das eigentliche Curriculum. Verglichen werden kann die inhaltliche Ausrichtung der Trilogie daher zB mit der Mathematik-typischen Beherrschung von Unendlichkeit \u00fcber\u00a0\u2026", "num_citations": "4\n", "authors": ["1616"]}
{"title": "LocalChecker Plugin for the jABC\n", "abstract": " The LocalChecker plugin is a robust tool informing the application expert about incorrect usage of SIBs. In continous check mode, the plugin gives direct information about the status of a SIB via the overlay icon. For the actual selection, more details are available in the plugin inspector. The info window provides information about the status of all SIBs in a model. The LocalChecker is a powerful and easy to use addition to the model checking plugin GEAR.", "num_citations": "4\n", "authors": ["1616"]}
{"title": "Practical aspects of active automata learning\n", "abstract": " This chapter reviews the essence of practical automata learning, its major challenges, its variants, possible solutions, and illustrative case studies. These suggest that the theoretically quite well\u2010studied active learning technology can be a specifically enhanced application and become a powerful tool for practical system development. This requires application\u2010specific optimizations to increase scalability, the adequate treatment of parameters and data flow, tailored abstraction mechanisms, and an adequate adaptation of testing technology. The chapter first sketches the principle of regular extrapolation in its basic form and for Mealy machine models. It then sketches the major challenges of learning/extrapolating models for realistic (reactive) systems. The chapter continues with a presentation of the Next Generation LearnLib (NGLL), a framework for flexible modeling of application\u2010specific learning algorithms and\u00a0\u2026", "num_citations": "4\n", "authors": ["1616"]}
{"title": "Runtime Verification\n", "abstract": " Starting from a definition of runtime verification, we develop a taxonomy that explains the different aspects of runtime verification. We explain the core idea of runtime verification by showing how monitors can be attached to existing programs, be used to verify certain aspects of the underlying program as well as be used to guide the program execution. The main part of the presentation deals with synthesis techniques that, starting from a high level correctness specifications, derive suitable monitors automatically. We start with properties expressed in linear temporal logic (LTL), first with a semantics on finite traces and then extended to a semantics over infinite traces.", "num_citations": "4\n", "authors": ["1616"]}
{"title": "Simplified validation of emergent systems through automata learning-based testing\n", "abstract": " In this paper we present a novel approach to the test-based validation of complex heterogeneous applications which is tailored to simplify the requirements for the responsible personnel. Key to our approach is to automate the corresponding testing procedure by means of active automata learning. This replaces typical prerequisites like manual test construction or the provision of adequate test models by the definition of an adequate learning alphabet. In practice this typically means by providing representative data for the parameters of the relevant API calls. Besides its simplicity this approach also guarantees that the testing procedure automatically adapts to system modifications, which makes it an ideal tool for dealing with evolving systems of unknown emergent behaviour, as will be illlustrated along the Online Conference System (OCS), a model-driven and service-oriented online conference manuscript\u00a0\u2026", "num_citations": "4\n", "authors": ["1616"]}
{"title": "Leveraging service-orientation for combining code generation frameworks\n", "abstract": " In this paper, we leverage service-orientation as a means for combining the strengths of the UML-based code generator framework AndroMDA for generating static application aspects with code generators focussing on the dynamic aspects. The latter are developed with Genesys, a code generation framework that combines the ideas of model-driven development and service-orientation in order to enable a high-level engineering of code generators. This demonstrates a new level of reusability and even has the potential for full code generation, which elegantly eliminates the need for the typical round-trip engineering in model-driven development environments. We demonstrate the applicability of our approach by means of an example from the field of bioinformatics.", "num_citations": "4\n", "authors": ["1616"]}
{"title": "Bioinformatics: Processes and Workflows.\n", "abstract": " Today, research projects in molecular biology rely on increasingly complex combinations of computational methods to handle the data that is produced in the life science laboratories. A variety of bioinformatics databases, algorithms, and tools is available for specific analysis tasks. Their combination to solve a specific biological question defines more or less complex analysis workflows or processes. Software systems that facilitate the systematic development and automation of such processes have begun to enjoy great popularity in the community. In this entry we give a survey of popular bioinformatics services and workflow management systems. Furthermore, we present a case study in which we realize small, yet typical analysis processes in two different frameworks, namely, Taverna and Bio-jETI.", "num_citations": "4\n", "authors": ["1616"]}
{"title": "Keynote: Continuous model driven engineering\n", "abstract": " Agility is a must, in particular for business applications. Complex systems and processes must be continuously updated in order to meet the ever changing market conditions. Continuous Model Driven Engineering is based on our eXtreme Model-Driven Design (XMDD) framework, which has been designed to continuously involve the customer/application expert throughout the whole systems' life cycle including software maintenance and evolution. Conceptually it is based on the One Thing Approach (OTA), which combines the simplicity of the waterfall development paradigm with a maximum of agility. The key to OTA is to view the whole development process simply as a complex hierarchical and interactive decision process, where each stakeholder, including the application expert, is allowed to continuously place his/her decisions in term of constraints. Thus semantically, at any time, the state of the development or\u00a0\u2026", "num_citations": "4\n", "authors": ["1616"]}
{"title": "Software Technologies for Embedded and Ubiquitous Systems: 6th IFIP WG 10.2 International Workshop, SEUS 2008, Anacarpi, Capri Island, Italy, October 1-3, 2008, Revised Papers\n", "abstract": " Embedded and ubiquitous computing systems have considerably increased their scope of application over the past few years, and they now also include missi-and business-critical scenarios. The advances call for a variety of compelling-sues, including dependability, real-time, quality-of-service, autonomy, resource constraints, seamless interaction, middleware support, modeling, veri? cation, validation, etc. The International Workshop on Software Technologies for Future Embedded and Ubiquitous Systems (SEUS) brings together experts in the? eld of emb-ded and ubiquitous computing systems with the aim of exchanging ideas and advancing the state of the art about the above-mentioned issues. I was honored to chair the sixth edition of the workshop, which continued the tradition of past editions with high-quality research results. I was particularly pleased to host the workshop in the wonderful scenario of Capri, with its stunning views and traditions. The workshop started in 2003 as an IEEE event, and then in 2007 it became a? agship event of the IFIP Working Group 10.2 on embedded systems. The last few editions, held in Hakodate (Japan), Vienna (Austria), Seattle (USA), Gyeongju (Korea), and Santorini (Greece), were co-located with the IEEE-ternationalSymposiumonObject/Component/Service-OrientedReal-TimeD-tributed Computing (ISORC). This year, SEUS was held as a stand-alone event for the? rst time, and,-spite the additionalorganizationaldi? culties, it resultedina high-qualityevent, with papers from four continents (from USA, Europe, East Asia and Australia),(co-) authored and presented from senior scientists coming from\u00a0\u2026", "num_citations": "4\n", "authors": ["1616"]}
{"title": "Pairing-Based Cryptography\u2013Pairing 2008: Second International Conference, Egham, UK, September 1-3, 2008, Proceedings\n", "abstract": " This book constitutes the thoroughly refereed proceedings of the Second International Conference on Pairing-Based Cryptography, Pairing 2008, held in London, UK, in September 2008. The 20 full papers, presented together with the contributions resulting from 3 invited talks, were carefully reviewed and selected from 50 submissions. The contents are organized in topical sections on cryptography, mathematics, constructing pairing-friendly curves, implementation of pairings, and hardware implementation.", "num_citations": "4\n", "authors": ["1616"]}
{"title": "Device for generating selection structures, for making selections according to selection structures, and for creating selection descriptions\n", "abstract": " The invention relates to a device for selecting data elements that contains a selection description device for creating a selection description data element, a generator device, which generates a selection structure data element from the selection description data element, and contains a selection device, which makes a selection from the input data elements when an inquiry data element is present. Selection description data elements can be created by non-electronic data processing specialists. The generation of the selection structure data element is of significant importance. Due to its nature, the selection device can be designed with a low level of complexity, can be operated while detached from the other devices, and can rapidly make selections. The invention can be used in many areas of application in the field of information technology, particularly in embedded systems, in the personalization of web pages, in e\u00a0\u2026", "num_citations": "4\n", "authors": ["1616"]}
{"title": "Leveraging Applications of Formal Methods: First International Symposium, ISoLA 2004, Paphos, Cyprus, October 30-November 2, 2004, Revised Selected Papers\n", "abstract": " This book constitutes the thoroughly refereed post-proceedings of the First International Symposium on Leveraging Applications of Formal Methods, ISoLA 2004, held in Paphos, Cyprus in October/November 2004. The 12 revised full papers discuss issues related to the adoption and use of rigorous tools and methods for the specification, analysis, verification, certification, construction, test, and maintenance of systems.", "num_citations": "4\n", "authors": ["1616"]}
{"title": "Parallelism for free: E cient and optimal bitvector analyses for parallel programs\n", "abstract": " In this paper we show how to construct optimal bitvector analysis algorithms for parallel programs with shared memory that are as efficient as their purely sequential counterparts, and which can easily be implemented. Whereas the complexity result is rather obvious, our optimality result is a consequence of a new Kam/Ullman-style Coincidence Theorem. Thus, the important merits of sequential bitvector analyses survive the introduction of parallel statements.", "num_citations": "4\n", "authors": ["1616"]}
{"title": "Aligned, purpose-driven cooperation: The future way of system development\n", "abstract": " Collaborative system development requires a three-dimensional alignment: in space, in time, and in mindset: Traditionally, different developers typically have their own, local development environments, each of which may change over time due to updates and other version changes. The third dimension concerns so-called semantic gaps, which we proposed to address via Language-Driven Engineering using Purpose-Specific Language. In this paper we argue that web-based, collaborative development environments that support Language-Driven Engineering are capable of solving the three-dimensional alignment problem. Our illustration via a corresponding prototypical solution aims at illustrating that this vision has the power to radically improve the effectiveness of collaborative development and that it is realistic even in near future.", "num_citations": "3\n", "authors": ["1616"]}
{"title": "Pyrus: an online modeling environment for no-code data-analytics service composition\n", "abstract": " We present Pyrus, a domain-specific online modeling environment for building graphical processes for data analysis, machine learning and artificial intelligence. Pyrus aims at bridging the gap between de facto (often Python-based) standards as established by the Jupyter platform, and the tradition to model data analysis workflows in a dataflow-driven fashion. Technically, Pyrus integrates established online IDEs like Jupyter and allows users to graphically combine available functional components to dataflow-oriented workflows in a collaborative fashion without writing a single line of code. Following a controlflow/dataflow conversion and compilation, the execution is then delegated to the underlying platforms. Both the inputs to a modeled workflow and the results of its execution can be specified and viewed without leaving Pyrus which supports a seamless cooperation between data science experts and\u00a0\u2026", "num_citations": "3\n", "authors": ["1616"]}
{"title": "The RERS challenge: towards controllable and scalable benchmark synthesis\n", "abstract": " This paper (1) summarizes the history of the RERS challenge for the analysis and verification of reactive systems, its profile and intentions, its relation to other competitions, and, in particular, its evolution due to the feedback of participants, and (2) presents the most recent development concerning the synthesis of hard benchmark problems. In particular, the second part proposes a way to tailor benchmarks according to the depths to which programs have to be investigated in order to find all errors. This gives benchmark designers a method to challenge contributors that try to perform well by excessive guessing.", "num_citations": "3\n", "authors": ["1616"]}
{"title": "Characteristic invariants in Hennessy\u2013Milner logic\n", "abstract": " In this paper, we prove that Hennessy\u2013Milner Logic (HML), despite its structural limitations, is sufficiently expressive to specify an initial property  and a characteristic invariant  for an arbitrary finite-state process P such that  is a characteristic formula for P. This means that a process Q, even if infinite state, is bisimulation equivalent to P iff . It follows, in particular, that it is sufficient to check an HML formula for each state of a finite-state process to verify that it is bisimulation equivalent to P. In addition, more complex systems such as context-free processes can be checked for bisimulation equivalence with P using corresponding model checking algorithms. Our characteristic invariant is based on so called class-distinguishing formulas that identify bisimulation equivalence classes in P and which are expressed in HML. We extend Kanellakis and Smolka\u2019s partition refinement algorithm for\u00a0\u2026", "num_citations": "3\n", "authors": ["1616"]}
{"title": "Generating Hard Benchmark Problems for Weak Bisimulation\n", "abstract": " In this paper, we propose a method to automatically generate arbitrarily complex benchmark problems for bisimulation checking. Technically, this method is a variant of an incremental generation approach for model checking benchmarks where given benchmark scenarios of controllable size are expanded to arbitrarily complex benchmark problems. This expansion concerns both the number of parallel components and the component sizes. Whereas our property-preserving parallel decomposition is maintained in this variant, the alphabet extension is flexibilized as, in contrast to temporal logics, weak bisimulation is not sensitive to liveness properties.", "num_citations": "3\n", "authors": ["1616"]}
{"title": "Mathematical Foundations of Advanced Informatics: Volume 1: Inductive Approaches\n", "abstract": " The books in this trilogy capture the foundational core of advanced informatics. The authors make the foundations accessible, enabling students to become effective problem solvers. This first volume establishes the inductive approach as a fundamental principle for system and domain analysis. After a brief introduction to the elementary mathematical structures, such as sets, propositional logic, relations, and functions, the authors focus on the separation between syntax (representation) and semantics (meaning), and on the advantages of the consistent and persistent use of inductive definitions. They identify compositionality as a feature that not only acts as a foundation for algebraic proofs but also as a key for more general scalability of modeling and analysis. A core principle throughout is invariance, which the authors consider a key for the mastery of change, whether in the form of extensions, transformations, or abstractions. This textbook is suitable for undergraduate and graduate courses in computer science and for self-study. Most chapters contain exercises and the content has been class-tested over many years in various universities.", "num_citations": "3\n", "authors": ["1616"]}
{"title": "User-level synthesis: treating product lines as systems of constraints\n", "abstract": " In this paper, we sketch how treating product lines as systems of possibly heterogeneous constraints allows one to elegantly and consistently manage product lines in terms of a product line of product lines. In fact, as will also be illustrated along our example scenarios, this leads to a framework for a consistent division of labour in an\" easy for the many difficult for the few\" fashion which supports correctness by construction. Central for this approach are our powerful model-based synthesis and code generation technologies, which turn systems of constraints into executable models or target code.", "num_citations": "3\n", "authors": ["1616"]}
{"title": "Learning models for verification and testing\u2014special track at ISoLA 2014 track introduction\n", "abstract": " Specifications play an important role in modern-day software engineering research. Formal specifications, e.g., are the basis for automated verification and testing techniques. In spite of their potentially great positive impact, formal specifications are notoriously hard to come by in practice. One reason seems to be that writing precise formal specifications is not an easy task for most of us. As a consequence, e.g., many software systems in use today lack adequate specifications or make use of un/under-specified components. Moreover, in many practical contexts, revision cycle times are often extremely short, which makes the maintenance of formal specifications unrealistic. At the same time, component-based design and short development cycles necessitate extensive testing and verification effort. Problems of this kind are inherent in systems that continuously undergo change as the ones specifically\u00a0\u2026", "num_citations": "3\n", "authors": ["1616"]}
{"title": "Modeling and Execution of Scientific Workflows with the jABC Framework\n", "abstract": " We summarize here the main characteristics and features of the jABC framework, used in the case studies as a graphical tool for modeling scientific processes and workflows. As a comprehensive environment for service-oriented modeling and design according to the XMDD (eXtreme Model-Driven Design) paradigm, the jABC offers much more than the pure modeling capability. Associated technologies and plugins provide in fact means for a rich variety of supporting functionality, such as remote service integration, taxonomical service classification, model execution, model verification, model synthesis, and model compilation. We describe here in short both the essential jABC features and the service integration philosophy followed in the environment. In our work over the last years we have seen that this kind of service definition and provisioning platform has the potential to become a core technology in\u00a0\u2026", "num_citations": "3\n", "authors": ["1616"]}
{"title": "Leveraging Applications of Formal Methods, Verification and Validation: 5th International Symposium, ISoLA 2012, Heraklion, Crete, Greece, October 15-18, 2012, Proceedings, Part I\n", "abstract": " The two-volume set LNCS 7609 and 7610 constitutes the thoroughly refereed proceedings of the 5th International Symposium on Leveraging Applications of Formal Methods, Verification and Validation, held in Heraklion, Crete, Greece, in October 2012. The two volumes contain papers presented in the topical sections on adaptable and evolving software for eternal systems, approaches for mastering change, runtime verification: the application perspective, model-based testing and model inference, learning techniques for software verification and validation, LearnLib tutorial: from finite automata to register interface programs, RERS grey-box challenge 2012, Linux driver verification, bioscientific data processing and modeling, process and data integration in the networked healthcare, timing constraints: theory meets practice, formal methods for the development and certification of X-by-wire control systems, quantitative modelling and analysis, software aspects of robotic systems, process-oriented geoinformation systems and applications, handling heterogeneity in formal development of HW and SW Systems.", "num_citations": "3\n", "authors": ["1616"]}
{"title": "The xmdd approach to the semantic web services challenge\n", "abstract": " The Semantic Web Services Challenge addresses since 2006 the issue of finding adequate domain modeling formalisms that help taming the complexity of service orchestration and service discovery. In this chapter we sketch briefly our XMDD (eXtreme Model Driven Design) approach to the development of large service-oriented applications and describe how it was used to address the Challenge. Our approach gave rise so far to a collection of six solutions with different engines, methods, and profiles. We examine in this technological landscape the concrete settings, the dimensions of complexity that appear in the Challenge, and reflect on the essence of the evaluations and observations so far.", "num_citations": "3\n", "authors": ["1616"]}
{"title": "Establishing basis for learning algorithms\n", "abstract": " The CONNECT Integrated Project aims at enabling continuous composition of networked systems, by developing techniques for synthesizing connectors. A prerequisite for synthesis is to learn about the interaction behavior of networked peers. The role of WP4 is to develop techniques for learning models of networked peers and middleware through exploratory interaction. In this deliverable, we survey the CONNECT process in order to derive requirements for the participating learning techniques. We also report on a number of case studies from which such requirements are extracted. These include requirements on the ability to interact with the networked peer, whose behavior is being learned, as well as requirements on the learned model, in order that it can be used for subsequent manipulation in the CONNECT process, such as connector synthesis. A major challenge is to extract and maintain detailed information about the interface of the networked peer, as well as to relate this information to the produced model, in which abstractions have been employed in order to make synthesis tractable. We describe our approach for representing and maintaining this information, and how we have adopted existing learning techniques to make use of it. Ontologies are proposed as a suitable vehicle for representing the information. We also report on work performed to develop and adapt existing learning tools, in order that they be suitably useful in the CONNECT process. This deliverable summarizes the progress and achievements during Year 1 in WP4.", "num_citations": "3\n", "authors": ["1616"]}
{"title": "Rule Representation, Interchange and Reasoning on the Web: International Symposium, RuleML 2008, Orlando, FL, USA, October 30-31, 2008. Proceedings\n", "abstract": " The 2008 International Symposium on Rule Interchange and Applications (RuleML th 2008), collocated in Orlando, Florida, with the 11 International Business Rules-rum, was the premier place to meet and to exchange ideas from all fields of rules te-nologies. The aim of RuleML 2008 was both to present new and interesting research results and to show successfully deployed rule-based applications. This annual sym-sium is the flagship event of the Rule Markup and Modeling Initiative (RuleML). The RuleML Initiative (www. ruleml. org) is a non-profit umbrella organization of several technical groups organized by representatives from academia, industry and government working on rule technologies and applications. Its aim is to promote the study, research and application of rules in heterogeneous distributed environments such as the Web. RuleML maintains effective links with other major international societies and acts as intermediary between various \u2018specialized\u2019rule vendors, appli-tions, industrial and academic research groups, as well as standardization efforts from, for example, W3C, OMG, and OASIS.", "num_citations": "3\n", "authors": ["1616"]}
{"title": "Formal Modeling and Analysis of Timed Systems: 6th International Conference, FORMATS 2008, Saint Malo, France, September 15-17, 2008, Proceedings\n", "abstract": " This book constitutes the refereed proceedings of the 6th International Conference on Formal Modeling and Analysis of Timed Systems, FORMATS 2008, held in Saint Malo, France, September 2008. The 17 revised full papers presented together with 3 invited talks were carefully reviewed and selected from 37 submissions. The papers are organized in topical sections on extensions of timed automata and semantics; timed games and logic; case studies; model-checking of probabilistic systems; verification and test; timed petri nets.", "num_citations": "3\n", "authors": ["1616"]}
{"title": "Major threat: From formal methods without tools to tools without formal methods\n", "abstract": " Writing secure code is critical because a large fraction of security incidents result from flaws in the code. In order to effectively teaching knowledge of secure software engineering we have developed a course module titled\" Introduction to Writing Secure Code\". This paper presents the content of this module and reports our teaching experiences. This module has been successfully taught in GEEN 163 Introduction to Java Programming class and GEEN 165 Computer Programming Design class in Spring 2011 in the Department of Computer Science at xxx University. Our experience exhibits that teaching this module in freshman and sophomore levels help students not only understand the impacts of insecure code, but also gain significant knowledge of safe programming practice. Students' survey and feedback reflected that this module is very valuable in their educational experience. This module could be taught in\u00a0\u2026", "num_citations": "3\n", "authors": ["1616"]}
{"title": "Verification, Model Checking, and Abstract Interpretation: 5th International Conference, VMCAI 2004, Venice, January 11-13, 2004, Proceedings\n", "abstract": " This volume contains the proceedings of the 5th International Conference on Veri? cation, Model Checking, and Abstract Interpretation (VMCAI 2004), held inVenice, January11-13, 2004, inconjunctionwithPOPL2004, the31stAnnual SymposiumonPrinciplesofProgrammingLanguages, January14-16, 2004. The purposeofVMCAIistoprovideaforumforresearchersfromthreecommunities--veri? cation, model checking, and abstract interpretation--which will facilitate interaction, cross-fertilization, and the advance of hybrid methods that combine thethreeareas. Withthegrowingneedforformaltoolstoreasonaboutcomplex, in? nite-state, and embedded systems, such hybrid methods are bound to be of great importance. Topics covered by VMCAI include program veri? cation, static analysis te-niques, model checking, program certi? cation, type systems, abstract domains, debugging techniques, compiler optimization, embedded systems, and formal analysis of security protocols. This year's meeting follows the four previous events in Port Je? erson (1997), Pisa (1998), Venice (2002), LNCS 2294 and New York (2003), LNCS 2575. In particular, we thank VMCAI 2003's sponsor, the Courant Institute at New York University, for allowing us to apply a monetary surplus from the 2003 meeting to this one. The program committee selected 22 papers out of 68 on the basis of three-views. Theprincipalcriteriawererelevanceandquality. TheprogramofVMCAI 2004 included, in addition to the research papers,-a keynote speech by David Harel (Weizmann Institute, Israel) onAGrand Challenge for Computing: Full Reactive Modeling of a Multicellular Animal\u00a0\u2026", "num_citations": "3\n", "authors": ["1616"]}
{"title": "A practical approach for the regression testing of IP-based applications\n", "abstract": " Modern IP-based applications are multitiered, distributed applications that typically run on heterogeneous platforms. Their correct operation depends increasingly on the interoperability of single software modules, rather than on their intrinsic algorithmic correctness. We present a scalable, integrated test methodology capable of granting adequate coverage of functional testing, yet still easy to use. It bases on our previous work on system level test of Computer Telephony Integrated (CTI) and Web-based applications, where our coordination-based coarse grain approach has been applied very successfully.", "num_citations": "3\n", "authors": ["1616"]}
{"title": "The electronic tool integration platform (ETI) and the Petri net technology\n", "abstract": " The paper presents the Electronic Tool Integration platform (ETI), a community platform designed for project-oriented, domainspecific, public or private interactive experimentation with heterogeneous tools. ETI managers, developers, and users are assisted by an advanced, personalized Online Service guiding experimentation, coordination and simple browsing of the available tool repository according to their degree of experience. This enables even newcomers to master the wealth of existing tools in a short time span, and to identify the most appropriate collection of tools to solve their own application-specific tasks. Concepts and current realization are illustrated along a concrete instantiation: the ETI community associated with the International Journal on Software Tools for Technology Transfer (STTT).             The ETI technology is application-independent, and we are convinced that it can offer a useful\u00a0\u2026", "num_citations": "3\n", "authors": ["1616"]}
{"title": "Tools get formal methods into practice\n", "abstract": " Even after almost twenty years of formal methods, there is still a major gap between, on the one hand, foundational research formally developing new theories, methods, and algorithms and, on the other hand, industrial practice solving day-to-day problems usually heuristically and often even ad hoc: the results of our foundational research hardly reach practice. Many of us have experienced situations in which practitioners refused to take any form of advice. However, the bridge between theory and practice is also weak from the other side, as nicely pointed out by Gerard Holzmann in his invited TACAS lecture:Many practitioners are interested in new solutions. They are prepared to listen to you and to try. However, the key to their problems delivered by researchers usually does not fit, and when the practitioner comes back complaining, he is told that it is not the key which is wrong, but the lock,.... and the door, and the\u00a0\u2026", "num_citations": "3\n", "authors": ["1616"]}
{"title": "Method engineering for real-life concurrent systems\n", "abstract": " After almost twenty years of concurrency theory, we face a wide spectrum of formalisms--process algebras, models for concurrency, and application-specific languages--as well as various paradigms for communication and extensions covering real time, priority and probability, and there is currently no sign that this tendency towards diversity will end. In response to this state of the art, various tools have been developed, each addressing very specific scenarios.Real-life applications usually do not fall just into one of the well-studied scenarios, and thus one method alone is hardly sufficient for their treatment. Rather, heterogeneous methods exploiting the various application-specific characteristics of the problem must be used. Thus computer-aided method engineering is required, in order to understand and solve the problems heterogeneously on a meta-level, where whole methods and paradigms are combined\u00a0\u2026", "num_citations": "3\n", "authors": ["1616"]}
{"title": "Incremental formalization\n", "abstract": " In his invited talk at TAPSOFT'95, Joseph Goguen claimed that practical success of formal methods is bound to a tool-supported domain specific paradigm (cf.[1]), which he characterized as follows:1. a narrow, well defined, well understood problem domain is addressed; there may already be a successful library for this domain, 2. there is a community of users who understand the domain, have good communication among themselves, and have potential financial re-sources, 3. the tool has a graphical user interface that is intuitive to the user community, embodying their own language and conventions, 4. the tool takes a large grain approach; rather than synthesizing procedures out of statements, it synthesizes systems out of modules; it may use a library of components and synthesize code for putting them together, 5. inside the tool is a powerful engine that encapsulates formal methods concepts and/or algorithms; it\u00a0\u2026", "num_citations": "3\n", "authors": ["1616"]}
{"title": "Finite model checking and beyond\n", "abstract": " This paper reviews finite and infinite-state model checking form two pragmatic perspectives: the application to the static analysis of imperative programs or data flow analysis, and the implementational aspect. Data flow analysis provides an interesting area of application for model checking, which is particularly challenging in the interprocedural case, since infinite state spaces need to be considered. Moreover, as it is a typical case of a structurally unrestricted problem, one should use global iterative methods for solution. This can be exploited for a uniform and efficient treatment based on a fixpoint analysis machine that covers all kinds of logics and model structures important for the considered application. 1 Introduction and Motivation Over the past decade model-checking has emerged as a powerful tool for the automatic analysis of concurrent systems. Whereas model-checking for finitestate systems has already a long history (cf. eg [EmLe86, ClES86, Lars88, StWa89, Wins89,...", "num_citations": "3\n", "authors": ["1616"]}
{"title": "An introduction to graphical modeling of CI/CD workflows with rig\n", "abstract": " We present an introduction to the usage of Rig, our Cinco product for the graphical modeling of CI/CD workflows. While CI/CD has become a de facto standard in modern software engineering (e.g. DevOps) and the benefits of its practice are without a doubt, developers are still facing inconvenient solutions. We will briefly outline the basic concept of CI/CD and discuss the challenges involved in maintaining such workflows with current implementations before we explain and illustrate the advantages of our model-driven approach step by step along on the treatment of a typical web application.", "num_citations": "2\n", "authors": ["1616"]}
{"title": "Agile Business Engineering: From Transformation Towards ContinuousInnovation\n", "abstract": " We discuss how to overcome the often fatal impact of violating integral quality constraints: seemingly successful (software) development projects turn into failures because of a mismatch with the business context. We investigate the similarities and differences between the today popular DevOps scenarios for aligning development and operations and the more general alignment problem concerning software and business engineering based on 33 structured expert interviews. It appears that both scenarios are driven by creativity in a continuous collaboration process relying on continuous goal validation. On the other hand, differences appear when considering Thorngate\u2019s trade-off between accuracy, generality and simplicity: the different level of accuracy is the main hurdle for transferring the automation-driven DevOps technology. The paper closes with the hypothesis that this hurdle may be overcome by\u00a0\u2026", "num_citations": "2\n", "authors": ["1616"]}
{"title": "Compositional learning of mutually recursive procedural systems\n", "abstract": " This paper presents a compositional approach to active automata learning of Systems of Procedural Automata (SPAs), an extension of Deterministic Finite Automata (DFAs) to systems of DFAs that can mutually call each other. SPAs are of high practical relevance, as they allow one to efficiently learn intuitive recursive models of recursive programs after an easy instrumentation that makes calls and returns observable. Key to our approach is the simultaneous inference of individual DFAs for each of the involved procedures via expansion and projection: membership queries for the individual DFAs are expanded to membership queries of the entire SPA, and global counterexample traces are transformed into counterexamples for the DFAs of concerned procedures. This reduces the inference of SPAs to a simultaneous inference of the DFAs for the involved procedures for which we can utilize various existing\u00a0\u2026", "num_citations": "2\n", "authors": ["1616"]}
{"title": "Large random forests: Optimisation for rapid evaluation\n", "abstract": " Random Forests are one of the most popular classifiers in machine learning. The larger they are, the more precise is the outcome of their predictions. However, this comes at a cost: their running time for classification grows linearly with the number of trees, i.e. the size of the forest. In this paper, we propose a method to aggregate large Random Forests into a single, semantically equivalent decision diagram. Our experiments on various popular datasets show speed-ups of several orders of magnitude, while, at the same time, also significantly reducing the size of the required data structure.", "num_citations": "2\n", "authors": ["1616"]}
{"title": "Leveraging Applications of Formal Methods, Verification and Validation. Modeling: 8th International Symposium, ISoLA 2018, Limassol, Cyprus, November 5-9, 2018, Proceedings, Part I\n", "abstract": " The four-volume set LNCS 11244, 11245, 11246, and 11247 constitutes the refereed proceedings of the 8th International Symposium on Leveraging Applications of Formal Methods, Verification and Validation, ISoLA 2018, held in Limassol, Cyprus, in October/November 2018. The papers presented were carefully reviewed and selected for inclusion in the proceedings. Each volume focusses on an individual topic with topical section headings within the volume: Part I, Modeling: Towards a unified view of modeling and programming; X-by-construction, STRESS 2018. Part II, Verification: A broader view on verification: from static to runtime and back; evaluating tools for software verification; statistical model checking; RERS 2018; doctoral symposium. Part III, Distributed Systems: rigorous engineering of collective adaptive systems; verification and validation of distributed systems; and cyber-physical systems engineering. Part IV, Industrial Practice: runtime verification from the theory to the industry practice; formal methods in industrial practice-bridging the gap; reliable smart contracts: state-of-the-art, applications, challenges and future directions; and industrial day.", "num_citations": "2\n", "authors": ["1616"]}
{"title": "Simplicity as a Driver for Agile Innovation\n", "abstract": " Exponential organizations [1] are the most radical witnesses of simplicity-based agile innovation, colorfully illustrating what we mean by simplicity and agile innovation. Simplicity is not an absolute notion. Rather it strongly depends on the current circumstances, like the global and local state of the art and infrastructure and the mindset of the addressed audience. Agile innovation is the exploitation of the current circumstances with comparatively little effort. It is characterized more by its sensitivity to new externally provided (infrastructural) potential and (commercial) impact than by the specific technology involved. Reference [1] discusses a number of impressive success stories that typically heralded the end of some traditional business era. When in 2007, Nokia acquired the navigation and roadmap company Navteq for $8.1 billion with its market leading road sensor technology, it aimed at regaining market share as a dominating online traffic information provider. Essentially at the same time, Waze was founded. Its business idea exploits the ubiquitous GPS sensors in smartphones to crowdsource the collection of traffic information. In contrast to Nokia\u2019s own dedicated and highly expensive infrastructure, Waze had no infrastructural investments: It was simply built on globally available and steadily growing volunteer resources. A few years later, Nokia was sold to Microsoft for essentially the same price it paid for Navteq while Waze attracted 50 million users and was sold to Google for $1.1 billion. A posteriori, Nokia as an established company was too fixed in its mind set of resource ownership to", "num_citations": "2\n", "authors": ["1616"]}
{"title": "Analyzing ambient assisted living solutions: a research perspective\n", "abstract": " Typical AAL solutions rely on integrating capabilities for health monitoring, fall detection, communication and social inclusion, supervised physical exercises, vocal interfaces, robotic platforms etc. Ensuring the safe function and quality of service with respect to various extra-functional requirements like timing and security of such AAL solutions is of highest importance. To facilitate analysis, latest system development platforms provide underlying infrastructures for model-driven design (e.g., via the DIME tool), timing and resource-usage specification (e.g., via the REMES tool), security features (e.g., by employing SECube), and statistical model-checking techniques (e.g, via Plasma). In this paper, we discuss the challenges associated with analyzing complex AAL solutions, from relevant properties to semantic interoperability issues raised by employing various frameworks for modeling and analysis, and applicability to\u00a0\u2026", "num_citations": "2\n", "authors": ["1616"]}
{"title": "LearnLib Tutorial\n", "abstract": " Active automata learning is a promising technique to generate formal behavioral models of systems by experimentation. The practical applicability of active learning, however, is often hampered by the impossibility of realizing so-called equivalence queries, which are vital for ensuring progress during learning and finally resulting in correct models. This paper discusses the proposed approach of using monitoring as a means of generating counterexamples, explains in detail why virtually all existing learning algorithms are not suited for this approach, and gives an intuitive account of TTT, an algorithm designed to cope with counterexamples of extreme length. The essential steps and the impact of TTT are illustrated via experimentation with LearnLib, a free, open source Java library for active automata learning.", "num_citations": "2\n", "authors": ["1616"]}
{"title": "Leveraging Applications of Formal Methods, Verification and Validation. Applications and Case Studies: 5th International Symposium, ISoLA 2012, Heraklion, Crete, Greece\u00a0\u2026\n", "abstract": " The two volumes contain papers presented in the topical sections on adaptable and evolving software for eternal systems, approaches for mastering change, runtime verification: the application perspective, model-based testing and model inference, learning techniques for software verification and validation, LearnLib tutorial: from finite automata to register interface programs, RERS grey-box challenge 2012, Linux driver verification, bioscientific data processing and modeling, process and data integration in the networked healthcare, timing constraints: theory meets practice, formal methods for the developent and certification of X-by-wire control systems, quantitative modelling and analysis, software aspects of robotic systems, process-oriented geoinformation systems and applications, handling heterogeneity in formal development of HW and SW Systems.", "num_citations": "2\n", "authors": ["1616"]}
{"title": "Special Session on\" Simplification through Change of Perspective\"\n", "abstract": " Leading goal for the EU-project ITSy is to explore the power of simplicity for achieving robustness, flexibility and trust. A number of scenarios have been investigated in this direction, comprising looking at different notions of complexity, in particular distinguishing the actual complexity, the objective notion of complexity associated with a task as such, from the felt complexity, the subjective notion of complexity associated with the difficulty for some person satisfy his personal task. Division of labor is the typical way of decomposing the actual complexity in manageable or small piece of felt complexity. Or in other words, standing on the shoulder of giants we may reach impressive goals. A completely different way to achieve simplicity is to change the rules of the game. A good example here is the change from object tracking via image recognition to RFID-based object tracking. Whereas the former is intrinsically difficult, the\u00a0\u2026", "num_citations": "2\n", "authors": ["1616"]}
{"title": "Extracting component-oriented behaviour for self-healing enabling\n", "abstract": " Rich and multifaceted domain specific specification languages like the Autonomic System Specification Language (ASSL) help to design reliable systems with self-healing capabilities. The GEAR game-based Model Checker has been used successfully to investigate in depth properties of the ESA ExoMars Rover. We show here how to enable GEAR's game-based verification techniques for ASSL via systematic model extraction from a behavioral subset of the language, and illustrate it on a description of the Voyager II space mission. This way, we close the gap between the design-time and the run-time techniques provided in the SHADOWS platform for self-healing of concurrency, performance, and functional issues.", "num_citations": "2\n", "authors": ["1616"]}
{"title": "Maintenance, or the 3rd dimension of eXtreme model-driven design\n", "abstract": " Service orientation leads to a completely new understanding and a much more end-user oriented tailoring of software design. We advocate a new software development paradigm: eXtreme Model-Driven Design (XMDD), designed to continuously involve the customer/application expert throughout the whole system's life cycle, including development and software maintenance. As maintenance is predominantly an adaption to new user requirements or to other global conditions, empowering the application expert would change the scene: Customer/application experts could rapidly adapt the system to their changing requirements. Source code becomes \"only\" a by-product and the development focuses on the model level. This paper presents a new development paradigm which realizes these ideas.", "num_citations": "2\n", "authors": ["1616"]}
{"title": "Multiple Classifier Systems: 8th International Workshop, MCS 2009, Reykjavik, Iceland, June 10-12, 2009, Proceedings\n", "abstract": " These proceedings are a record of the Multiple Classi? er Systems Workshop, MCS 2009, held at the University of Iceland, Reykjavik, Iceland in June 2009. Being the eighth in a well-established series of meetings providing an inter-tional forum for the discussion of issues in multiple classi? er system design, the workshop achieved its objective of bringing together researchers from diverse communities (neural networks, pattern recognition, machine learning and stat-tics) concerned with this research topic. From more than 70 submissions, the Program Committee selected 54 papers to create an interesting scienti? c program. The special focus of MCS 2009 was on the application of multiple classi? er systems in remote sensing. This part-ular application uses multiple classi? ers for raw data fusion, feature level fusion and decision level fusion. In addition to the excellent regular submission in the technical program, outstanding contributions were made by invited speakers Melba Crawford from Purdue University and Zhi-Hua Zhou of Nanjing Univ-sity. Papers of these talks are included in these workshop proceedings. With the workshop\u2019sapplicationfocusbeingonremotesensing, Prof. Crawford\u2019sexpertise in the use of multiple classi? cation systems in this context made the discussions on this topic at MCS 2009 particularly fruitful.", "num_citations": "2\n", "authors": ["1616"]}
{"title": "Reconfigurable Computing: Architectures, Tools and Applications: 5th International Workshop, ARC 2009, Karlsruhe, Germany, March 16-18, 2009, Proceedings\n", "abstract": " This book constitutes the refereed proceedings of the 5th International Workshop on Applied Reconfigurable Computing, ARC 2009, held in Karlsruhe, Germany, in March 2009. The 21 full papers and 21 short papers presented together with the abstracts of 3 keynote lectures were carefully reviewed and selected from about 100 submissions. The papers are organized in topical sections on FPGA security and bitstream analysis, fault tolerant systems, architectures, place and route techniques, cryptography, and resource allocation and scheduling, as well as on applications.", "num_citations": "2\n", "authors": ["1616"]}
{"title": "Information Theoretic Security\n", "abstract": " ICITS 2008, the Third International Conference on Information Theoretic Security, was held in Calgary, Alberta, Canada, during August 10\u201313, 2008, at the University of Calgary. This series of conferences was started with the 2005 IEEE Information Theory Workshop on Theory and Practice in Information-Theoretic Security (ITW 2005, Japan), held on Awaji Island, Japan, October 16\u201319, 2005. The conference series aims at bringing focus to security research when there is no unproven computational assumption on the adversary. This is the framework proposed by Claude Shannon in his seminal paper formalizing modern unclassified research on cryptography. Over the last few decades, Shannon\u2019s approach to formalizing security has been used in various other areas including authentication, secure communication, key exchange, multiparty computation and information hiding to name a few. Coding theory has\u00a0\u2026", "num_citations": "2\n", "authors": ["1616"]}
{"title": "Smart Card Research and Advanced Applications\n", "abstract": " Since 1994, CARDIS has been the foremost international conference dedicated to smart card research and applications. Every two years, the scientific community congregates to present new ideas and discuss recent developments with both an academic and industrial focus. Following the increased capabilities of smart cards and devices, CARDIS has become a major event for the discussion of the various issues related to the use of small electronic tokens in the process of human-machine interactions. The scope of the conference includes numerous subfields such as networking, efficient implementations, physical security, biometrics, and so on. This year\u2019s CARDIS was held in London, UK, on September 8\u201311, 2008. It was organized by the Smart Card Centre, Information Security Group of the Royal Holloway, University of London. The present volume contains the 21 papers that were selected from the 51\u00a0\u2026", "num_citations": "2\n", "authors": ["1616"]}
{"title": "Service based enabling service availability in the MaTRICS: A model-driven approach\n", "abstract": " In today's business the availability of services is of central importance. To guarantee a higher availability of a service, like a Web-service, it can be installed on several machines, which are running in a hot-standby operation. In case of a fault one hot-standby service can take over the work of the faulty service. Novel to our approach is that this kind of redundancy can be applied to services, that normally do not support service availability concepts. The switching of one machine running in hot-standby mode to active can be done from an external system monitoring the service. MaTRICS is an architecture that allows the configuration of any service provided by a specific server. The specification is done by service logic graphs, which can be validated by model checking.", "num_citations": "2\n", "authors": ["1616"]}
{"title": "The ETI Online Service: Concepts and Design\n", "abstract": " The Electronic Tool Integration (ETI) online service is designed for orientation, experimentation, and evaluation purposes concerning the use of heterogeneous tools, and it is explicitely not intended to be a software distribution environment. Users may access the functionality described hereafter to run their trials and coordination tasks remotely via the Internet. Thus neither the underlying ETI platform nor the tools need to be (or even can be) downloaded to the user\u2019s site. This remote utilization policy is vital for our goals for several reasons, ranging from access performance (tools can be big and clog the transmission channels) to legal aspects (copyright, licensing, violations of commercial distribution policies).", "num_citations": "2\n", "authors": ["1616"]}
{"title": "Services and Visualization: Towards User-Friendly Design: ACos' 98, VISUAL'98, AIN'97, Selected Papers\n", "abstract": " This book comprises a strictly refereed selection of papers presented at three international workshops on advanced communication services (ACoS'98), visualization issues for formal methods (VISUAL'98), and advanced intelligent networks (AIN'97). The 20 revised full papers included in the book together with three invited presentations are centered around the user-friendly design of software systems and services, in particular telecommunication and Internet services, and visualization support for the design and administration of such systems. Among the topics addressed are telecommunication services, multimedia networking, user interfaces, intelligent networking protocols, formal specification and verification, visual formalisms, mobile computing, intelligent agents, and Java.", "num_citations": "2\n", "authors": ["1616"]}
{"title": "Automatic Synthesis of Design Plans in MetaFrame\n", "abstract": " We present CAD-MetaFrame, an environment supporting the flexible CAD tool integration and application-specific construction of design plans, which avoids the insurgence of unsuccessful design plans at design time. During a planning phase the collection of all complete, executable design plans is automatically synthesized on the basis of simple constraint-like specifications and the library of available tools. The designer's choice of the best alternative is eased by a user friendly graphical interface and by hypertext support for the generation and management of plans, as illustrated along a user session.", "num_citations": "2\n", "authors": ["1616"]}
{"title": "E cient and Optimal Bit-Vector Data Flow Analyses: A Uniform Interprocedural Framework\n", "abstract": " We present a uniform framework for solving bit-vector data ow analysis problems for imperative programs with recursive procedures, global and local variables, and formal value parameters. The point of our framework is (1) that it guarantees the optimality of the analysis algorithms, in that they yield the same results as their meet over all (interprocedural) paths counterparts, and,(2) that they are as e cient as their intraprocedural versions. The algorithm for interprocedural partial redundancy elimination, which we develop here for illustration, is in fact not only unique in its optimality, but also in its e ciency.", "num_citations": "2\n", "authors": ["1616"]}
{"title": "Optimal code motion within flow graphs: A practical approach\n", "abstract": " Common subexpression elimination, partial redundancy elimination and loop invariant code motion are all instances of the same general run-time optimization problem: how to optimally place computations within a program. In [SKR1] we presented a modular algorithm for this problem, which optimally moves computations within programs wrt Herbrand equivalence. However, the full variant of this algorithm may excessively introduce trivial redefinitions of registers in order to cover a single computation. Rosen, Wegman and Zadeck avoided such a too excessive introduction of trivial redefinitions by means of some practically oriented restrictions, and they proposed an efficient algorithm, which optimally moves the computations of acyclic flow graphs under these additional constraints (the algorithm is\" RWZ-optimal\" for acyclic flow graphs)[RWZ]. Here we adapt our algorithm to this notion of optimality. The result is a modular and efficient algorithm, which avoids a too excessive introduction of...", "num_citations": "2\n", "authors": ["1616"]}
{"title": "Asking why\n", "abstract": " In this paper, we illustrate the impact of simple Why questions as a means to reveal global aspects that may easily be forgotten during traditional requirement analysis. For illustration we use the introduction of the General Data Protection Regulations (GDPR), a prime example to observe that adequate solutions may require to think out of the box, beyond just stepwise trying to fulfill individual requirements. Our Why analysis revealed the traditional, scattered data handling as the essential bottleneck, which we believe can be overcome by a cloud-based knowledge management across departments and applications.", "num_citations": "1\n", "authors": ["1616"]}
{"title": "Programming-What is Next?\n", "abstract": " The paper provides an introduction to the track: \u201cProgramming - What is Next?\u201d, organized by the authors as part of ISoLA 2021: the 9th International Symposium On Leveraging Applications of Formal Methods, Verification and Validation. A total of 14 papers were presented in the track, with responses to the question: what are the trends in current more recent programming languages, and what can be expected of future languages?. The track covers such topics as general-purpose programming languages, domain-specific languages, formal methods and modeling languages, textual versus graphical languages, and application programming versus embedded programming.", "num_citations": "1\n", "authors": ["1616"]}
{"title": "Algebraic aggregation of random forests: towards explainability and rapid evaluation\n", "abstract": " Random Forests are one of the most popular classifiers in machine learning. The larger they are, the more precise the outcome of their predictions. However, this comes at a cost: it is increasingly difficult to understand why a Random Forest made a specific choice, and its running time for classification grows linearly with the size (number of trees). In this paper, we propose a method to aggregate large Random Forests into a single, semantically equivalent decision diagram which has the following two effects: (1)\u00a0minimal, sufficient explanations for Random Forest-based classifications can be obtained by means of a simple three step reduction, and (2)\u00a0the running time is radically improved. In fact, our experiments on various popular datasets show speed-ups of several orders of magnitude, while, at the same time, also significantly reducing the size of the required data structure.", "num_citations": "1\n", "authors": ["1616"]}
{"title": "A Generative Approach for User-Centered, Collaborative, Domain-Specific Modeling Environments\n", "abstract": " The use of low- and no-code modeling tools is today an established way in practice to give non-programmers an opportunity to master their digital challenges independently, using the means of model-driven software development. However, the existing tools are limited to a very small number of different domains such as mobile app development, which can be attributed to the enormous demands that a user has on such a tool today. These demands exceed the mere use of a modeling environment as such and require cross-cutting concerns such as: easy access, direct usability and simultaneous collaboration, which result in additional effort in the realization of such tools. Our solution is based on the idea to support and simplify the creation of new domain-specific holistic tools by generating it entirely based on a declarative specification with a domain-specific meta-tool. The meta-tool Pyro demonstrated and analyzed here focuses on graph-based graphical languages to fully generate a complete, directly executable tool starting from a meta-model in order to meet all cross-cutting requirements.", "num_citations": "1\n", "authors": ["1616"]}
{"title": "Never-stop context-free learning\n", "abstract": " In this paper, we revisit the concept of never-stop learning, a combination of active automata learning and runtime monitoring. Published research focuses on regular systems and became practical with the development of the TTT algorithm and its redundancy-free approach of storing information. With the recent development of our active learning algorithm for systems of procedural automata (SPAs), we can infer instrumented context-free/procedural systems via a simultaneous inference of individual (regular) procedures. In this paper, we combine these two concepts to lift the concept of never-stop (or life-long) learning to the level of context-free/procedural systems. In an empirical evaluation we show that using the TTT algorithm for procedural learning allows us to tackle internal (procedural) redundancy whereas the inherent compositional structure and instrumentation of our SPA approach allows us to tackle\u00a0\u2026", "num_citations": "1\n", "authors": ["1616"]}
{"title": "Every component matters: Generating parallel verification benchmarks with hardness guarantees\n", "abstract": " In this paper, we show how to automatically generate hard verification tasks in order to support events like the Model Checking Contest or the Rigorous Examination of Reactive Systems Challenge with tailored benchmark problems for analyzing the validity of linear-time properties in parallel systems. Characteristic of the generated benchmarks are two hardness guarantees: (i) every parallel component is relevant and (ii) the state space of the analyzed system is exponential in the number of its parallel components. Generated benchmarks can be made available, e.g., as Promela code or Petri nets.", "num_citations": "1\n", "authors": ["1616"]}
{"title": "Guaranteeing type consistency in collective adaptive systems\n", "abstract": " Collective adaptive systems whose entities are loosely coupled by their exchange of complex data structures became a very common architecture for distributed web-based systems. As HTTP-based APIs transfer data as plain text, this exchange is very error prone: API changes and malicious data modifications may remain unnoticed. GraphQL addresses this concern at the server side with strong typing but leaves the clients untouched. In this paper we present an approach to align the type schemas provided by GraphQL and type definitions at the client side on three levels during the systems\u2019 life cycles: At generation time by verifying queries against the GraphQL schema, at compile time by leveraging TypeScript\u2019s type system, and at run time by using decoders to validate payloads. Key to our solution are a functional, type-safe domain-specific language for the definition of GraphQL queries and a corresponding\u00a0\u2026", "num_citations": "1\n", "authors": ["1616"]}
{"title": "Aggressive aggregation: a new paradigm for program optimization\n", "abstract": " In this paper, we propose a new paradigm for program optimization which is based on aggressive aggregation, i.e., on a partial evaluation-based decomposition of acyclic program fragments into a pair of computationally optimal structures: an Algebraic Decision Diagram (ADD) to capture conditional branching and a parallel assignment that refers to an Expression DAG (ED) which realizes redundancy-free computation. The point of this decomposition into, in fact, side-effect-free component structures allows for powerful optimization that semantically comprise effects traditionally aimed at by SSA form transformation, code specialization, common subexpression elimination, and (partial) redundancy elimination. We illustrate our approach along an optimization of the well-known iterative Fibonacci program, which, typically, is considered to lack any optimization potential. The point here is that our technique supports loop unrolling as a first class optimization technique and is tailored to optimally aggregate large program fragments, especially those resulting from multiple loop unrollings. For the Fibonacci program, this results in a performance improvement beyond an order of magnitude.", "num_citations": "1\n", "authors": ["1616"]}
{"title": "Erratum to: Leveraging Applications of Formal Methods, Verification and Validation (Part I)\n", "abstract": " Erratum to: Leveraging Applications of Formal Methods, Verification and Validation (Part I) Page 1 Erratum to: Leveraging Applications of Formal Methods, Verification and Validation (Part I) Tiziana Margaria1(&) and Bernhard Steffen2 1 Lero, Limerick, Ireland tiziana.margaria@lero.ie 2 TU Dortmund, Dortmund, Germany Erratum to: T. Margaria and B. Steffen (Eds.) Leveraging Applications of Formal Methods, Verification and Validation (Part I) DOI: 10.1007/978-3-319-47166-2 In the initially published version of chapter 61 of Part II (Leveraging Applications of Formal Methods, Verification and Validation), two author names were erroneously omitted. This has been updated. Consequently, the Table of Contents and the Author Index have also been updated in this volume (LNCS 9952), Part I (Leveraging Applications of Formal Methods, Verification and Validation). The updated original online version for this Book can \u2026", "num_citations": "1\n", "authors": ["1616"]}
{"title": "Learning-Based Cross-Platform Conformance Testing\n", "abstract": " In this paper we present learning-based cross-platform conformance testing (LCCT), an approach specifically designed to validate successful system migration. Key to our approach is the combination of (1) adequate user-level system abstraction, (2) higher-order integration of executable test-blocks, and (3) learning-based automatic model inference and comparison. The impact of LCCT will be illustrated along the migration of Springer\u2019s Online Conference Service (OCS) from a browser-based implementation to using a RESTful web service API. Continuous LCCT allowed us in particular to systematically pinpoint spots where the original OCS depended on browser-based access control mechanisms, to eliminate them, and thus to maintain the OCS access control policy for the RESTFul API.", "num_citations": "1\n", "authors": ["1616"]}
{"title": "Rehasport: The Challenge of Small Margin Healthcare Accounting\n", "abstract": " The paper presents the development of a Web-based accounting system for rehabilitations sports, which, due to the small profit margins, requires a very economical approach, both for its development and for its later use. The development process was therefore driven by simplicity in two dimensions: the accounting process itself was reduced to the minimum under the given legal circumstances, and the software development was clearly guided by total-cost-of-ownership concerns. In particular, standards where taken and artifacts reused wherever possible.", "num_citations": "1\n", "authors": ["1616"]}
{"title": "Transactions on Foundations for Mastering Change I\n", "abstract": " The LNCS Transactions on Foundations for Mastering Change, FoMaC, aims to establish a forum for formal-methods-based research, dealing with the nature of today\u2019s agile system development, which is characterized by unclear premises, unforeseen change, and the need for fast reaction, in a context of hard-to-control frame conditions, such as third-party components, network problems, and attacks. Submissions are evaluated according to these goals. This book, the first volume in the series, contains contributions by the members of the editorial board. These contributions indicate the envisioned style and range of papers of topics covered by the transactions series. They cross-cut various traditional research directions and are characterized by a clear focus on change.", "num_citations": "1\n", "authors": ["1616"]}
{"title": "Playing with abstraction and representation\n", "abstract": " In this paper, we discuss partition refinement as an algorithmic pattern for explicating semantic properties of a system directly in the corresponding model structure in a co-inductive fashion. In particular, we review a landscape of analysis and verification approaches under this unifying perspective, which enables us to highlight their mutual profiles, while it at the same time establishes a basis for their combination: The common pattern establishes comparability, which reveals complementarity, and indicates where and under which circumstances the considered approaches may profit from one another. It can thus be regarded as a guideline for systematically exploring the benefits of the corresponding methods and their combinations.", "num_citations": "1\n", "authors": ["1616"]}
{"title": "Proceedings of the 5th international conference on Leveraging Applications of Formal Methods, Verification and Validation: technologies for mastering change-Volume Part I\n", "abstract": " Proceedings of the 5th international conference on Leveraging Applications of Formal Methods, Verification and Validation: technologies for mastering change - Volume Part I | Guide Proceedings ACM Digital Library home ACM home Google, Inc. (search) Advanced Search Browse About Sign in Register Advanced Search Journals Magazines Proceedings Books SIGs Conferences People More Search ACM Digital Library SearchSearch Advanced Search Browse Browse Digital Library Collections More HomeBrowse by TitleProceedingsISoLA'12 ABSTRACT No abstract available. Comments Login options Check if you have access through your login credentials or your institution to get full access on this article. Sign in Full Access Get this Publication Information Contributors Published in Guide Proceedings cover image ISoLA'12: Proceedings of the 5th international conference on Leveraging Applications of Formal \u2026", "num_citations": "1\n", "authors": ["1616"]}
{"title": "Further development of learning techniques\n", "abstract": " The CONNECT Integrated Project aims at enabling continuous composition of networked systems, by developing techniques for synthesizing connectors. A prerequisite for synthesis is to learn about the interaction behavior of networked peers. The role of WP4 is to develop techniques for learning models of networked peers and middleware through exploratory interaction. During Y1 of CONNECT, exploratory work was performed to understand the requirements for learning techniques in the CONNECT process, and to develop concepts for using a priori knowledge about component interfaces as a basis for learning. During Y2, a major goal has been to develop techniques for automatically learning models of networked peers and middleware, based on the concepts developed during Y1 (cf. D4.1). This deliverable surveys the significant progress made on problems that are important for realizing this goal. We have developed techniques for drastically improving the efficiency of active learning, meaning to explore significantly larger parts of component behavior within a given time. We verified the power of our approaches by winning the ZULU challenge in competition with several very strong groups in the language learning community. We have also made significant breakthroughs for learning of rich models with data. We have developed a novel compact and intuitive automaton model for representing learned behavior in a canonical way. Canonicity is very helpful for organizing the resuls of exploratory interactions, since it allows the extension of stable techniques for learning classical finite-state automata (such as L ) to richer models with data. We\u00a0\u2026", "num_citations": "1\n", "authors": ["1616"]}
{"title": "Wettbewerbsvorteile durch Prozesskompetenzen: Kalkulation von IT Services und wirtschaftliche Erfolgskontrolle durch Prozessautomatisierung\n", "abstract": " Eine Studie der Universit\u00e4t Oxford aus dem Jahre 2003 beziffert die Erfolgsquote bei durchgef\u00fchrten IT-Projekten in Unternehmen mit ca.16 % und bescheinigt damit einen sehr geringen Anteil. Misslungene Projekte sind nicht nur \u00e4rgerlich, sondern mit (zum Teil erheblichen) wirtschaftlichen Konsequenzen behaftet.", "num_citations": "1\n", "authors": ["1616"]}
{"title": "Information Security and Privacy: 14th Australasian Conference, ACISP 2009 Brisbane, Australia, July 1-3, 2009 Proceedings\n", "abstract": " The 2009 Australasian Conference on Information Security and Privacy was the 14th in an annual series that started in 1996. Over the years ACISP has grown froma relativelysmall conferencewith a largeproportionof paperscoming from Australia into a truly international conference with an established reputation. ACISP 2009 was held at Queensland University of Technology in Brisbane, d-ing July 1\u20133, 2009. This year there were 106 paper submissions and from those 30 papers were accepted for presentation, but one was subsequently withdrawn. Authors of-cepted papers came from 17 countries and 4 continents, illustrating the inter-tional? avorof ACISP. We would like to extend our sincere thanks to all authors who submitted papers to ACISP 2009. The contributed papers were supplemented by two invited talks from e-nent researchers in information security. Basie von Solms (University of Joh-nesburg), currently President of IFIP, raised the question of how well dressed is the information security king. L. Jean Camp (Indiana University) talked about how to harden the network from the friend within. We are grateful to both of them for sharing their extensive knowledge and setting challenging questions for the ACISP 2009 delegates. We were fortunate to have an energetic team of experts who formed the Program Committee. Their names may be found overleaf, and we thank them warmly for their considerable e? orts. This team was helped by an even larger number of individuals who reviewedpapers in their particularareasof expertise.", "num_citations": "1\n", "authors": ["1616"]}
{"title": "Transactions on computational science V: special issue on cognitive knowledge representation\n", "abstract": " The LNCS journal Transactions on Computational Science reflects recent developments in the field of Computational Science, conceiving the field not as a mere ancillary science but rather as an innovative approach supporting many other scientific disciplines. The journal focuses on original high-quality research in the realm of computational science in parallel and distributed environments, encompassing the facilitating theoretical foundations and the applications of large-scale computations and massive data processing. It addresses researchers and practitioners in areas ranging from aerospace to biochemistry, from electronics to geosciences, from mathematics to software architecture, presenting verifiable computational methods, findings and solutions and enabling industrial users to apply techniques of leading-edge, large-scale, high performance computational methods. The fifth volume of the Transactions on Computational Science journal, edited by Yingxu Wang and Keith CC Chan, is devoted to the subject of cognitive knowledge representation. This field of study focuses on the internal knowledge representation mechanisms of the brain and how these can be applied to computer science and engineering. The issue includes the latest research results in internal knowledge representation at the logical, functional, physiological, and biological levels and describes their impacts on computing, artificial intelligence, and computational intelligence.", "num_citations": "1\n", "authors": ["1616"]}
{"title": "Integration of AI and OR Techniques in Constraint Programming for Combinatorial Optimization Problems: 6th International Conference, CPAIOR 2009 Pittsburgh, PA, USA, May 27-31\u00a0\u2026\n", "abstract": " This book constitutes the refereed proceedings of the 6th International Conference on Integration of AI and OR Techniques in Constraint Programming for Combinatorial Optimization Problems, CPAIOR 2009, held in Pittsburgh, PA, USA, in May 2009. The 20 revised full papers and 10 extended abstracts presented together with 2 invited talks were carefully reviewed and selected from 65 submissions. The papers describe current research in the fields of constraint programming, artificial intelligence, and operations research and present new techniques or new applications in combinatorial optimization, thus exploring ways of solving large-scale, practical optimization problems through integration and hybridization of the fields' different techniques.", "num_citations": "1\n", "authors": ["1616"]}
{"title": "Language and Automata Theory and Applications: Third International Conference, LATA 2009, Tarragona, Spain, April 2-8, 2009. Proceedings\n", "abstract": " This book constitutes the refereed proceedings of the Third International Conference on Language and Automata Theory and Applications, LATA 2009, held in Tarragona, Spain, in April 2009. The 58 revised full papers presented together with 3 invited lectures and two tutorials were carefully reviewed and selected from 121 submissions. The papers address all the various issues related to automata theory and formal languages.", "num_citations": "1\n", "authors": ["1616"]}
{"title": "Human Machine Interaction: Research Results of the MMI Program\n", "abstract": " Human Machine Interaction : Research Results of the MMI Program | Titel Title external user (warning warning ) Log in as language uk Home Library My Library Search result Record nr. 1244159 Record number 1244159 Title Human Machine Interaction : Research Results of the MMI Program show extra info. edited by David Hutchison, Takeo Kanade, Josef Kittler, Jon M. Kleinberg, Friedemann Mattern, John C. Mitchell, Moni Naor, Oscar Nierstrasz, C. Pandu Rangan, Bernhard Steffen, Madhu Sudan, Demetri Terzopoulos, Doug Tygar, Moshe Y. Vardi, Gerhard Weikum, Denis Lalanne, J\u00fcrg Kohlas Author(s) Hutchison, David ; Pandu Rangan, C ; Kittler, Josef ; Weikum, Gerhard ; Vardi, Moshe Y ; Tygar, Doug ; Terzopoulos, Demetri ; Sudan, Madhu ; Steffen, Bernhard ; Naor, Moni ; Mitchell, John C ; Mattern, Friedemann ; Lalanne, Denis ; Kleinberg, Jon M ; Kanade, Takeo ; Nierstrasz, Oscar ; Kohlas, J\u00fcrg Publisher \u2026", "num_citations": "1\n", "authors": ["1616"]}
{"title": "Leveraging Applications of Formal Methods, Verification and Validation: Third International Symposium, ISoLA 2008, Porto Sani, Greece, October 13-15, 2008, Proceedings\n", "abstract": " This volume contains the conference proceedings of ISoLA 2008, the Third International Symposium on Leveraging Applications of Formal Methods, Verification and Validation, which was held in Porto Sani (Kassandra, Chalkidiki), Greece during October 13\u201315, 2008, sponsored by EASST and in cooperation with the IEEE Technical Committee on Complex Systems. Following the tradition of its forerunners in 2004 and 2006 in Cyprus, and the ISoLA Workshops in Greenbelt (USA) in 2005 and in Poitiers (France) in 2007, ISoLA 2008 provided a forum for developers, users, and researchers to discuss issues related to the adoption and use of rigorous tools and methods for the specification, analysis, verification, certification, construction, test, and maintenance of systems from the point of view of their different application domains. Thus, the ISoLA series of events serves the purpose of bridging the gap between designers and developers of rigorous tools, and users in engineering and in other disciplines, and to foster and exploit synergetic relationships among scientists, engineers, software developers, decision makers, and other critical thinkers in companies and organizations. In p-ticular, by providing a venue for the discussion of common problems, requirements, algorithms, methodologies, and practices, ISoLA aims at supporting researchers in their quest to improve the utility, reliability, flexibility, and efficiency of tools for building systems, and users in their search for adequate solutions to their problems.", "num_citations": "1\n", "authors": ["1616"]}
{"title": "Middleware: just another level for orchestration\n", "abstract": " In this paper we advocate to introduce a common modelling pattern for all the different layers of middleware based on services. This opens the possibility to flexibly choose the adequate levels for realizing specific features, and it allows all stakeholders to get a global picture of the overall scenario, a central factor when considering the convergence in networks and applications. Moreover, it allows to apply validations methods like model-based testing and model checking homogeneously, throughout the whole modelling hierarchy, from the underlying converging platforms to the user level.", "num_citations": "1\n", "authors": ["1616"]}
{"title": "ViDoC - Visual Design of Optimizing Compilers\n", "abstract": " Designing optimizing compilers is a challenging task that involves numerous mutually interdependent transformations. Often, these interdependencies are only captured in an ad-hoc manner, relying on the ingenuity and experience of the compiler engineers. ViDoC is a tool-kit for the specification-driven, interactive development of program optimizers in a service oriented way. In particular, ViDoC facilitates the specification of dependencies between transformations in terms of modal logic properties and requirements. These specifications can be used for checking, as well as synthesizing, suitable optimization sequences , which are expressed in terms of a workflow (graph) model. ViDoC also offers various kinds of visual support, like the display of flow graphs, call graphs and analysis information, and the visualization and even manipulation of the graphs expressing the optimization workflows. These\u00a0\u2026", "num_citations": "1\n", "authors": ["1616"]}
{"title": "Service-Oriented Design: The jABC Approach\n", "abstract": " Reviewing our 10 years of experience in service engineering for telecommunication systems from the point of view of Service-Oriented Design then and now, we observe that much is common to the two communities. We aim in our current research at establishing a link to the notions used by the service-oriented programming (SO) community. We are convinced that combined approaches, that blend the flexibility of the current SO-scenario with the rigour and semantic standardization culture of the telecommunication community will dramatically increase the productivity of the development of a large class of software systems. Incremental formalization and automatic verification techniques may be again the key to achieving confidence and reliability for services that interact and interoperate on a large distributed scale.", "num_citations": "1\n", "authors": ["1616"]}
{"title": "Service-Oriented Approaches for Highly Available Triple-Play Telecommunication Services\n", "abstract": " This paper discusses how to address the new need in the telecommunication area to marry standard library-based platform development and carrier grade quality. In particular, we propose to complement standardized interfacing and redundant hardware/software (HW/SW) clustering with semantic feature descriptions and orchestration at the level of the application expert. Together with our method of incremental formalization, which aims at maintaining vital service properties in the inevitable flow of change, this generalizes a methodology proven for the intelligent network (IN) to the tripleplay scenario.", "num_citations": "1\n", "authors": ["1616"]}
{"title": "Run-time agents as a means of reconciling flexibility and scalability of services\n", "abstract": " In this paper, we present our approach to flexibly modelling user processes: lightweight run time agents. These agents can be thought of as essentially being \u2018session handlers\u2019 with persistent memory, which steer the execution of generic global services according to the users\u2019 profile and its current context and goals. We illustrate this paradigm in two application scenarios, the Online Conference Service, and MaTRICS, a remote configuration management environment.", "num_citations": "1\n", "authors": ["1616"]}
{"title": "Scalable System-level CTI Testing through Lightweight Coarse-grained Coordination\n", "abstract": " We propose a solution to the problem of system-level testing of functionally complex communication systems based on lightweight coordination. The enabling aspect is here the adoption of a coarse-grained approach to test design, which is central to the scalability of the overall testing environment. This induces an understandable modelling paradigm of system-wide test cases which is adequate for the needs and requirements of industrial test engineers. The approach is coarse-grained in the sense that it renounces a detailed model of the system functionality (which would be unfeasible in the considered industrial setting). The coordination is lightweight in the sense that it allows a programming-free definition of system-level behaviours (in this case complex test cases) based on the coarse models of the functionalities. These features enable test engineers to graphically design complex test cases, which, in addition\u00a0\u2026", "num_citations": "1\n", "authors": ["1616"]}
{"title": "An Operational Procedure for Model-Based Testing of CTI Systems\n", "abstract": " In this paper we illustrate how a posteriori modelling of complex, heterogeneous, and distributed systems is practically performed within an automated integrated testing environment (ITE) to give improved support to the testing process of steadily evolving systems. The conceptual background is a modelling technique called moderated regular extrapolation, a technique that provides descriptions of systems or system aspects a posteriori in a largely automatic way. The descriptions come in the form of models which offer the possibility of mechanically producing system tests, grading test suites and monitoring running systems. Regular extrapolation builds models from observations via techniques from machine learning and finite automata theory. Also expert knowledge about the system enters the model construction in a systematic way. The power of this approach is illustrated in the context of a test environment for\u00a0\u2026", "num_citations": "1\n", "authors": ["1616"]}
{"title": "An Open Environment for Automated Integrated Testing\n", "abstract": " The increasing complexity of today\u2019s testing scenarios demands for an integrated, open and flexible approach to support the management of the overall test process. Furthermore systems under test become composite (eg including Computer Telephony Integrated platform aspects), embedded (eg with hardware/software codesign) and run on distributed architectures (eg client/server architectures). In addition, it is increasingly unrealistic to restrict the consideration of the testing activities to single units, since complex subsystems affect each other and require scalable, integrated test methodologies. In this paper, we present a test management layer driving the generation, execution and evaluation of system-level tests in a highly heterogeneous landscape. The management layer introduces the required flexibilization of the overall architecture of the test environment: it is a modular and open environment, so that\u00a0\u2026", "num_citations": "1\n", "authors": ["1616"]}
{"title": "Code-size sensitive partial redundancy elimination\n", "abstract": " Program optimization focuses usually on improving the runtime efficiency of a program. Its impact on the code size is typically not considered a concern. In fact, classical optimizations often cause code replication without providing any means allowing a user to control this. This limits their adequacy for applications, where code size is critical, too, like embedded systems or smart cards. In this article, we demonstrate this by means of partial redundancy elimination (PRE), one of the most powerful und widespread optimizations in contemporay compilers, which intuitively aims at avoiding multiple computations of a value at runtime. By modularly extending the classical PRE-approaches we develop a family of code-size sensitive PRE-transformations, whose members in addition to the two traditional goals of PRE (1) reducing the number of computations and (2) avoiding unnecessary register pressure, are unique for taking also (3) code size as a third optimization goal into account. Each of them optimally captures a predefined choice of priority between these three goals. The flexibility and aptitude of these techniques for size-critical applications is demonstrated by various examples.", "num_citations": "1\n", "authors": ["1616"]}
{"title": "The Programming Language Ada. Reference Manual: American National Standards Institute, Inc. ANSI/MIL-STD-1815A-1983. Approved 17 February 1983\n", "abstract": " 1 Ada is a programming language designed in accordance with requirements defined by the United States Department of Defense: the so-called Steelman requirements. Overall, these requirements call for a language with considerable expressive power covering a wide application domain. As a result, the language includes facilities offered by classical languages such as Pascal as well as facilities often found only in specialized languages. Thus the language is a modern algorithmic language with the usual control structures, and with the ability to define types and subprograms. It also serves the need for modularity, whereby data, types, and subprograms can be packaged. It treats modularity in the physical sense as well, with a facility to support separate compilation.", "num_citations": "1\n", "authors": ["1616"]}
{"title": "Graph-Based Representations in Pattern Recognition\n", "abstract": " These proceedings present the papers accepted for the 9th IAPR-TC-15 Workshop on Graph-based Representations in Pattern Recognition (GbR) 2013. For more than 15 years, GbR has been providing a forum for researchers from the fields of pattern recognition, image processing, and computer vision who build their works on the basis of graph theory. This year it was a great pleasure for us to organize the GbR 2013 in the heart of Europe\u2013Vienna, Austria. The Technical Committee 15 (TC15) of the International Association for Pattern Recognition (IAPR) was created in 1996. It encourages elaboration of graph-based research works, is an integral partner in organizing biennial GbR workshops, sponsors related special sessions at conferences, and promotes special issues in journals.Traditionally the work presented at GbR covers a wide range of topics. The scope of the papers varies from theoretical contributions\u00a0\u2026", "num_citations": "1\n", "authors": ["1616"]}
{"title": "Mathematik f\u00fcr Informatiker\n", "abstract": " Vektorr\u00e4ume bilden eine zentrale Struktur in der linearen Algebra. Ihre Struktur erlaubt es, die Elemente eines Vektorraums mit denen eines K\u00f6rpers zu multiplizieren oder untereinander zu addieren. Damit die Addition und die Multiplikation auf intuitive Weise erfolgen kann, werden einige Regeln zur Rechenweise in Vektorr\u00e4umen eingef\u00fchrt. So stellt zum Beispiel in einem R-Vektorraum die Distributivit\u00e4t bezgl. des K\u00f6rpers uA sicher, dass v+ v= 2\u00b7 v ist (und nicht etwa 3\u00b7 v o. \u00c4.) f\u00fcr ein beliebiges Element v des R-Vektorraums. Es gibt geometrisch anschauliche Beispiele eines Vektorraums, wie zB die euklidische Ebene, aber auch abstraktere Strukturen wie den Vektorraum der linearen Abbildungen oder kurios anmutende Strukturen wie den Vektorraum aller magischen Quadrate. In der Informatik finden Vektorr\u00e4ume der anschaulichen Art Anwendung zur Wegebestimmung, wie es z. Bsp. f\u00fcr einen Roboter, der Regale einr\u00e4umen soll, notwendig ist, aber auch abstraktere Formen der Vektorr\u00e4ume finden Anwendung, wie z. Bsp. in der linearen Codierungstheorie.", "num_citations": "1\n", "authors": ["1616"]}