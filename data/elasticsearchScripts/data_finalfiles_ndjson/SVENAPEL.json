{"title": "Granularity in software product lines\n", "abstract": " Building software product lines (SPLs) with features is a challenging task. Many SPL implementations support features with coarse granularity - e.g., the ability to add and wrap entire methods. However, fine-grained extensions, like adding a statement in the middle of a method, either require intricate workarounds or obfuscate the base code with annotations. Though many SPLs can and have been implemented with the coarse granularity of existing approaches, fine-grained extensions are essential when extracting features from legacy applications. Furthermore, also some existing SPLs could benefit from fine-grained extensions to reduce code replication or improve readability. In this paper, we analyze the effects of feature granularity in SPLs and present a tool, called Colored IDE (CIDE), that allows features to implement coarse-grained and fine-grained extensions in a concise way. In two case studies, we show\u00a0\u2026", "num_citations": "615\n", "authors": ["151"]}
{"title": "An overview of feature-oriented software development\n", "abstract": " Feature-oriented software development (FOSD) is a paradigm for the construction, customization, and synthesis of large-scale software systems. In this survey, we give an overview and a personal perspective on the roots of FOSD, connections to other software development paradigms, and recent developments in this field. Our aim is to point to connections between different lines of research and to identify open issues.", "num_citations": "557\n", "authors": ["151"]}
{"title": "FeatureHouse: Language-independent, automated software composition\n", "abstract": " Superimposition is a composition technique that has been applied successfully in many areas of software development. Although superimposition is a general-purpose concept, it has been (re)invented and implemented individually for various kinds of software artifacts. We unify languages and tools that rely on superimposition by using the language-independent model of feature structure trees (FSTs). On the basis of the FST model, we propose a general approach to the composition of software artifacts written in different languages, Furthermore, we offer a supporting framework and tool chain, called FEATUREHOUSE. We use attribute grammars to automate the integration of additional languages, in particular, we have integrated Java, C#, C, Haskell, JavaCC, and XML. Several case studies demonstrate the practicality and scalability of our approach and reveal insights into the properties a language must have in\u00a0\u2026", "num_citations": "299\n", "authors": ["151"]}
{"title": "Performance-influence models for highly configurable systems\n", "abstract": " Almost every complex software system today is configurable. While configurability has many benefits, it challenges performance prediction, optimization, and debugging. Often, the influences of individual configuration options on performance are unknown. Worse, configuration options may interact, giving rise to a configuration space of possibly exponential size. Addressing this challenge, we propose an approach that derives a performance-influence model for a given configurable system, describing all relevant influences of configuration options and their interactions. Our approach combines machine-learning and sampling heuristics in a novel way. It improves over standard techniques in that it (1) represents influences of options and their interactions explicitly (which eases debugging),(2) smoothly integrates binary and numeric configuration options for the first time,(3) incorporates domain knowledge, if available\u00a0\u2026", "num_citations": "216\n", "authors": ["151"]}
{"title": "Views on internal and external validity in empirical software engineering\n", "abstract": " Empirical methods have grown common in software engineering, but there is no consensus on how to apply them properly. Is practical relevance key? Do internally valid studies have any value? Should we replicate more to address the tradeoff between internal and external validity? We asked the community how empirical research should take place in software engineering, with a focus on the tradeoff between internal and external validity and replication, complemented with a literature review about the status of empirical research in software engineering. We found that the opinions differ considerably, and that there is no consensus in the community when to focus on internal or external validity and how to conduct and review replications.", "num_citations": "184\n", "authors": ["151"]}
{"title": "Type safety for feature-oriented product lines\n", "abstract": " A feature-oriented product line is a family of programs that share a common set of features. A feature implements a stakeholder\u2019s requirement and represents a design decision or configuration option. When added to a program, a feature involves the introduction of new structures, such as classes and methods, and the refinement of existing ones, such as extending methods. A feature-oriented decomposition enables a generator to create an executable program by composing feature code solely on the basis of the feature selection of a user\u2014no other information needed. A key challenge of product line engineering is to guarantee that only well-typed programs are generated. As the number of valid feature combinations grows combinatorially with the number of features, it is not feasible to type check all programs individually. The only feasible approach is to have a type system check the entire code base of\u00a0\u2026", "num_citations": "155\n", "authors": ["151"]}
{"title": "Cost-efficient sampling for performance prediction of configurable systems (t)\n", "abstract": " A key challenge of the development and maintenanceof configurable systems is to predict the performance ofindividual system variants based on the features selected. It isusually infeasible to measure the performance of all possible variants, due to feature combinatorics. Previous approaches predictperformance based on small samples of measured variants, butit is still open how to dynamically determine an ideal samplethat balances prediction accuracy and measurement effort. Inthis paper, we adapt two widely-used sampling strategies forperformance prediction to the domain of configurable systemsand evaluate them in terms of sampling cost, which considersprediction accuracy and measurement effort simultaneously. Togenerate an initial sample, we introduce a new heuristic based onfeature frequencies and compare it to a traditional method basedon t-way feature coverage. We conduct experiments on six\u00a0\u2026", "num_citations": "141\n", "authors": ["151"]}
{"title": "Type-checking software product lines-a formal approach\n", "abstract": " A software product line (SPL) is an efficient means to generate a family of program variants for a domain from a single code base. However, because of the potentially high number of possible program variants, it is difficult to test all variants and ensure properties like type-safety for the entire SPL. While first steps to type-check an entire SPL have been taken, they are informal and incomplete. In this paper, we extend the Featherweight Java (FJ) calculus with feature annotations to be used for SPLs. By extending FJ's type system, we guarantee that - given a well-typed SPL - all possible program variants are well- typed as well. We show how results from this formalization reflect and help implementing our own language-independent SPL tool CIDE.", "num_citations": "141\n", "authors": ["151"]}
{"title": "Language-independent and automated software composition: The FeatureHouse experience\n", "abstract": " Superimposition is a composition technique that has been applied successfully in many areas of software development. Although superimposition is a general-purpose concept, it has been (re)invented and implemented individually for various kinds of software artifacts. We unify languages and tools that rely on superimposition by using the language-independent model of feature structure trees (FSTs). On the basis of the FST model, we propose a general approach to the composition of software artifacts written in different languages. Furthermore, we offer a supporting framework and tool chain, called FEATUREHOUSE. We use attribute grammars to automate the integration of additional languages. In particular, we have integrated Java, C#, C, Haskell, Alloy, and JavaCC. A substantial number of case studies demonstrate the practicality and scalability of our approach and reveal insights into the properties that a\u00a0\u2026", "num_citations": "140\n", "authors": ["151"]}
{"title": "Virtual separation of concerns-a second chance for preprocessors\n", "abstract": " Conditional compilation with preprocessors like cpp is a simple but effective means to implement variability. By annotating code fragments with# ifdef and# endif directives, different program variants with or without these fragments can be created, which can be used (among others) to implement software product lines. Although, preprocessors are frequently used in practice, they are often criticized for their negative effect on code quality and maintainability. In contrast to modularized implementations, for example using components or aspects, preprocessors neglect separation of concerns, are prone to introduce subtle errors, can entirely obfuscate the source code, and limit reuse. Our aim is to rehabilitate the preprocessor by showing how simple tool support can address these problems and emulate some benefits of modularized implementations. At the same time we emphasize unique benefits of preprocessors, like simplicity and language independence. Although we do not have a definitive answer on how to implement variability, we want highlight opportunities to improve preprocessors and encourage research toward novel preprocessor-based approaches.", "num_citations": "131\n", "authors": ["151"]}
{"title": "Types and modularity for implicit invocation with implicit announcement\n", "abstract": " Through implicit invocation, procedures are called without explicitly referencing them. Implicit announcement adds to this implicitness by not only keeping implicit which procedures are called, but also where or when\u2014under implicit invocation with implicit announcement, the call site contains no signs of that, or what it calls. Recently, aspect-oriented programming has popularized implicit invocation with implicit announcement as a possibility to separate concerns that lead to interwoven code if conventional programming techniques are used. However, as has been noted elsewhere, as currently implemented it establishes strong implicit dependencies between components, hampering independent software development and evolution. To address this problem, we present a type-based modularization of implicit invocation with implicit announcement that is inspired by how interfaces and exceptions are realized in Java\u00a0\u2026", "num_citations": "104\n", "authors": ["151"]}
{"title": "Superimposition: A language-independent approach to software composition\n", "abstract": " Superimposition is a composition technique that has been applied successfully in several areas of software development. In order to unify several languages and tools that rely on superimposition, we present an underlying language-independent model that is based on feature structure trees (FSTs). Furthermore, we offer a tool, called FSTComposer, that composes software components represented by FSTs. Currently, the tool supports the composition of components written in Java, Jak, XML, and plain text. Three nontrivial case studies demonstrate the practicality of our approach.", "num_citations": "104\n", "authors": ["151"]}
{"title": "Integrating compositional and annotative approaches for product line engineering\n", "abstract": " Software product lines can be implemented with many different approaches. However, there are common underlying mechanisms which allow a classification into compositional and annotative approaches. While research focuses mainly on composition approaches like aspect-or feature-oriented programming because those support feature traceability and modularity, in practice annotative approaches like preprocessors are common as they are easier to adopt. In this paper, we compare both groups of approaches and find complementary strengths. We propose an integration of compositional and annotative approaches to combine advantages, increase flexibility for the developer, and ease adoption.", "num_citations": "103\n", "authors": ["151"]}
{"title": "When to use features and aspects?: a case study\n", "abstract": " Aspect-Oriented Programming (AOP) and Feature-Oriented Programming (FOP) are complementary technologies that can be combined to overcome their individual limitations. Aspectual Mixin Layers (AML) is a representative approach that unifies AOP and FOP. We use AML in a non-trivial case study to create a product line of overlay networks. We also present a set of guidelines to assist programmers in how and when to use AOP and FOP techniques for implementing product lines in a stepwise and generative manner.", "num_citations": "103\n", "authors": ["151"]}
{"title": "An algebra for features and feature composition\n", "abstract": " Feature-Oriented Software Development (FOSD) provides a multitude of formalisms, methods, languages, and tools for building variable, customizable, and extensible software. Along different lines of research, different notions of a feature have been developed. Although these notions have similar goals, no common basis for evaluation, comparison, and integration exists. We present a feature algebra that captures the key ideas of feature orientation and provides a common ground for current and future research in this field, in which also alternative options can be explored.", "num_citations": "99\n", "authors": ["151"]}
{"title": "Toward variability-aware testing\n", "abstract": " We investigate how to execute a unit test for all products of a product line without generating each product in isolation in a brute-force fashion. Learning from variability-aware analyses, we (a) design and implement a variability-aware interpreter and, alternatively,(b) reencode variability of the product line to simulate the test cases with a model checker. The interpreter internally reasons about variability, executing paths not affected by variability only once for the whole product line. The model checker achieves similar results by reusing powerful off-the-shelf analyses. We experimented with a prototype implementation for each strategy. We compare both strategies and discuss trade-offs and future directions. In the long run, we aim at finding an efficient testing approach that can be applied to entire product lines with millions of products.", "num_citations": "96\n", "authors": ["151"]}
{"title": "Scalable prediction of non-functional properties in software product lines: Footprint and memory consumption\n", "abstract": " ContextA software product line is a family of related software products, typically created from a set of common assets. Users select features to derive a product that fulfills their needs. Users often expect a product to have specific non-functional properties, such as a small footprint or a bounded response time. Because a product line may have an exponential number of products with respect to its features, it is usually not feasible to generate and measure non-functional properties for each possible product.ObjectiveOur overall goal is to derive optimal products with respect to non-functional requirements by showing customers which features must be selected.MethodWe propose an approach to predict a product\u2019s non-functional properties based on the product\u2019s feature selection. We aggregate the influence of each selected feature on a non-functional property to predict a product\u2019s properties. We generate and\u00a0\u2026", "num_citations": "95\n", "authors": ["151"]}
{"title": "A model of refactoring physically and virtually separated features\n", "abstract": " Physical separation with class refinements and method refinements \u00e0 la AHEAD and virtual separation using annotations \u00e0 la #ifdef or CIDE are two competing implementation approaches for software product lines with complementary advantages. Although both approaches have been mainly discussed in isolation, we strive for an integration to leverage the respective advantages. In this paper, we lay the foundation for such an integration by providing a model that supports both physical and virtual separation and by describing refactorings in both directions. We prove the refactorings complete, so every virtually separated product line can be automatically transformed into a physically separated one (replacing annotations by refinements) and vice versa. To demonstrate the feasibility of our approach, we have implemented the refactorings in our tool CIDE and conducted four case studies.", "num_citations": "92\n", "authors": ["151"]}
{"title": "Visualizing software product line variabilities in source code\n", "abstract": " Implementing software product lines is a challenging task. Depending on the implementation technique the code that realizes a feature is often scattered across multiple code units. This way it becomes difficult to trace features in source code which hinders maintenance and evolution. While previous effort on visualization technologies in software product lines has focused mainly on the feature model, we suggest tool support for feature traceability in the code base. With our tool CIDE, we propose an approach based on filters and views on source code in order to visualize and trace features in source code.", "num_citations": "90\n", "authors": ["151"]}
{"title": "Exploring feature interactions in the wild: the new feature-interaction challenge\n", "abstract": " The feature-interaction problem has been keeping researchers and practitioners in suspense for years. Although there has been substantial progress in developing approaches for modeling, detecting, managing, and resolving feature interactions, we lack sufficient knowledge on the kind of feature interactions that occur in real-world systems. In this position paper, we set out the goal to explore the nature of feature interactions systematically and comprehensively, classified in terms of order and visibility. Understanding this nature will have significant implications on research in this area, for example, on the efficiency of interaction-detection or performance-prediction techniques. A set of preliminary results as well as a discussion of possible experimental setups and corresponding challenges give us confidence that this endeavor is within reach but requires a collaborative effort of the community.", "num_citations": "89\n", "authors": ["151"]}
{"title": "Feature Featherweight Java: A calculus for feature-oriented programming and stepwise refinement\n", "abstract": " Feature-oriented programming (FOP) is a paradigm that incorporates programming language technology, program generation techniques, and stepwise refinement. In their GPCE'07 paper, Thaker et al. suggest the development of a type system for FOP to guarantee safe feature composition, ie, to guarantee the absence of type errors during feature composition. We present such a type system along with a calculus for a simple feature-oriented, Java-like language, called Feature Featherweight Java (FFJ). Furthermore, we explore four extensions of FFJ and how they affect type soundness.", "num_citations": "86\n", "authors": ["151"]}
{"title": "Tool support for feature-oriented software development: FeatureIDE: an Eclipse-based approach\n", "abstract": " Software program families have a long tradition and will gain momentum in the future. Today's research tries to move software development to a new quality of industrial production. Several solutions concerning different phases of the software development process have been proposed in order to cope with different problems of program family development. A major problem of program family engineering is still the missing tool support. The vision is an IDE that brings all phases of the development process together consistently and in a user-friendly manner. This paper focuses on AHEAD, a prominent design methodology and architectural model for feature-based program families. We present our first results on developing an Eclipse-based IDE that supports building program families following the AHEAD architecture model. Starting from current weaknesses and pitfalls in implementing program families we outline\u00a0\u2026", "num_citations": "86\n", "authors": ["151"]}
{"title": "Classifying developers into core and peripheral: An empirical study on count and network metrics\n", "abstract": " Knowledge about the roles developers play in a software project is crucial to understanding the project's collaborative dynamics. In practice, developers are often classified according to the dichotomy of core and peripheral roles. Typically, count-based operationalizations, which rely on simple counts of individual developer activities (e.g., number of commits), are used for this purpose, but there is concern regarding their validity and ability to elicit meaningful insights. To shed light on this issue, we investigate whether count-based operationalizations of developer roles produce consistent results, and we validate them with respect to developers' perceptions by surveying 166 developers. Improving over the state of the art, we propose a relational perspective on developer roles, using fine-grained developer networks modeling the organizational structure, and by examining developer roles in terms of developers'\u00a0\u2026", "num_citations": "85\n", "authors": ["151"]}
{"title": "Scalable prediction of non-functional properties in software product lines\n", "abstract": " A software product line is a family of related software products, typically, generated from a set of common assets. Users can select features to derive a product that fulfills their needs. Often, users expect a product to have specific non-functional properties, such as a small footprint or a minimum response time. Because a product line can contain millions of products, it is usually not feasible to generate and measure non-functional properties for each possible product of a product line. Hence, we propose an approach to predict a product's non-functional properties, based on the product's feature selection. To this end, we generate and measure a small set of products, and by comparing the measurements, we approximate each feature's non-functional properties. By aggregating the approximations of selected features, we predict the product's properties. Our technique is independent of the implementation approach and\u00a0\u2026", "num_citations": "82\n", "authors": ["151"]}
{"title": "Preprocessor-based variability in open-source and industrial software systems: An empirical study\n", "abstract": " Almost every sufficiently complex software system today is configurable. Conditional compilation is a simple variability-implementation mechanism that is widely used in open-source projects and industry. Especially, the C preprocessor (CPP) is very popular in practice, but it is also gaining (again) interest in academia. Although there have been several attempts to understand and improve CPP, there is a lack of understanding of how it is used in open-source and industrial systems and whether different usage patterns have emerged. The background is that much research on configurable systems and product lines concentrates on open-source systems, simply because they are available for study in the first place. This leads to the potentially problematic situation that it is unclear whether the results obtained from these studies are transferable to industrial systems. We aim at lowering this gap by comparing\u00a0\u2026", "num_citations": "81\n", "authors": ["151"]}
{"title": "Structured merge with auto-tuning: balancing precision and performance\n", "abstract": " Software-merging techniques face the challenge of finding a balance between precision and performance. In practice, developers use unstructured-merge (ie, line-based) tools, which are fast but imprecise. In academia, many approaches incorporate information on the structure of the artifacts being merged. While this increases precision in conflict detection and resolution, it can induce severe performance penalties. Striving for a proper balance between precision and performance, we propose a structured-merge approach with auto-tuning. In a nutshell, we tune the merge process on-line by switching between unstructured and structured merge, depending on the presence of conflicts. We implemented a corresponding merge tool for Java, called JDime. Our experiments with 8 real-world Java projects, involving 72 merge scenarios with over 17 million lines of code, demonstrate that our approach indeed hits a\u00a0\u2026", "num_citations": "80\n", "authors": ["151"]}
{"title": "The road to feature modularity?\n", "abstract": " Modularity of feature representations has been a long standing goal of feature-oriented software development. While some researchers regard feature modules and corresponding composition mechanisms as a modular solution, other researchers have challenged the notion of feature modularity and pointed out that most feature-oriented implementation mechanisms lack proper interfaces and support neither modular type checking nor separate compilation. We step back and reflect on the feature-modularity discussion. We distinguish two notions of modularity, cohesion without interfaces and information hiding with interfaces, and point out the different expectations that, we believe, are the root of many heated discussions. We discuss whether feature interfaces should be desired and weigh their potential benefits and costs, specifically regarding crosscutting, granularity, feature interactions, and the distinction\u00a0\u2026", "num_citations": "73\n", "authors": ["151"]}
{"title": "Detecting dependences and interactions in feature-oriented design\n", "abstract": " Feature-oriented software development (FOSD) aims at the construction, customization, and synthesis of large-scale software systems. We propose a novel software design paradigm, called feature-oriented design, that takes the distinguishing characteristics of FOSD into account, especially the clean and consistent mapping between features and their implementations as well as the tendency of features to interact inadvertently. We extend the lightweight modeling language Alloy with support for feature-oriented design and call the extension Feature Alloy. By means of an implementation and four case studies, we demonstrate how feature-oriented design with Feature Alloy facilitates separation of concerns, variability, and reuse of models of individual features and helps defining and detecting semantic dependences and interactions between features.", "num_citations": "73\n", "authors": ["151"]}
{"title": "The role of features and aspects in software development\n", "abstract": " In the 60s and 70s the software engineering offensive emerged from long-standing problems in software development, which are captured by the term software crisis. Though there has been significant progress since then, the current situation is far from satisfactory. According to the recent report of the Standish Group, still only 34% of all software projects succeed.Since the early days, two fundamental principles drive software engineering research to cope with the software crisis: separation of concerns and modularity. Building software according to these principles is supposed to improve its understandability, maintainability, reusability, and customizability. But it turned out that providing adequate concepts, methods, formalisms, and tools is difficult.", "num_citations": "70\n", "authors": ["151"]}
{"title": "An algebraic foundation for automatic feature-based program synthesis\n", "abstract": " Feature-Oriented Software Development provides a multitude of formalisms, methods, languages, and tools for building variable, customizable, and extensible software. Along different lines of research, different notions of a feature have been developed. Although these notions have similar goals, no common basis for evaluation, comparison, and integration exists. We present a feature algebra that captures the key ideas of feature orientation and that provides a common ground for current and future research in this field, on which also alternative options can be explored. Furthermore, our algebraic framework is meant to serve as a basis for the development of the technology of automatic feature-based program synthesis and architectural metaprogramming.", "num_citations": "69\n", "authors": ["151"]}
{"title": "Tailoring dynamic software product lines\n", "abstract": " Software product lines (SPLs) and adaptive systems aim at variability to cope with changing requirements. Variability can be described in terms of features, which are central for development and configuration of SPLs. In traditional SPLs, features are bound statically before runtime. By contrast, adaptive systems support feature binding at runtime and are sometimes called dynamic SPLs (DSPLs). DSPLs are usually built from coarse-grained components, which reduces the number of possible application scenarios. To overcome this limitation, we closely integrate static binding of traditional SPLs and runtime adaptation of DSPLs. We achieve this integration by statically generating a tailor-made DSPL from a highly customizable SPL. The generated DSPL provides only the runtime variability required by a particular application scenario and the execution environment. The DSPL supports self-configuration based on\u00a0\u2026", "num_citations": "68\n", "authors": ["151"]}
{"title": "ExaStencils: Advanced stencil-code engineering\n", "abstract": " Project ExaStencils pursues a radically new approach to stencil-code engineering. Present-day stencil codes are implemented in general-purpose programming languages, such as Fortran, C, or Java, or derivates thereof, and harnesses for parallelism, such as OpenMP, OpenCL or MPI. ExaStencils favors a much more domain-specific approach with languages at several layers of abstraction, the most abstract being the mathematical formulation, the most concrete the optimized target code. At every layer, the corresponding language expresses not only computational directives but also domain knowledge of the problem and platform to be leveraged for optimization. This approach will enable a highly automated code generation at all layers and has been demonstrated successfully before in the U.S. projects FFTW and SPIRAL for certain linear transforms.", "num_citations": "64\n", "authors": ["151"]}
{"title": "A calculus for uniform feature composition\n", "abstract": " The goal of feature-oriented programming (FOP) is to modularize software systems in terms of features. A feature refines the content of a base program. Both base programs and features may contain various kinds of software artifacts, for example, source code in different languages, models, build scripts, and documentation. We and others have noticed that when composing features, different kinds of software artifacts can be refined in a uniform way, regardless of what they represent. We present gDeep, a core calculus for feature composition, which captures the language independence of FOP; it can be used to compose features containing many different kinds of artifact in a type-safe way. The calculus allows us to gain insight into the principles of FOP and to define general algorithms for feature composition and validation. We provide the formal syntax, operational semantics, and type system of gDeep and outline\u00a0\u2026", "num_citations": "61\n", "authors": ["151"]}
{"title": "How AspectJ is used: an analysis of eleven AspectJ programs\n", "abstract": " While it is well-known that crosscutting concerns occur in many software projects, little is known on how aspect-oriented programming and in particular AspectJ have been used. In this paper, we analyze eleven AspectJ programs by different authors to answer the questions: which mechanisms are used, to what extent, and for what purpose. We found the code of these programs to be on average 86% objectoriented, 12% basic AspectJ mechanisms (introductions and method extensions), and 2% advanced AspectJ mechanisms (homogeneous advice or advanced dynamic advice). There is one class of crosscutting concerns\u2013which is mostly concerned with introductions and method extensions\u2013that matches this result well: collaborations. These results and our discussions with program authors indicate the bulk of coding activities was implementing collaborations. Several studies and researchers suggest that languages explicitly supporting collaborations are better suited than aspectsa la AspectJ for this task.", "num_citations": "61\n", "authors": ["151"]}
{"title": "Distance-based sampling of software configuration spaces\n", "abstract": " Configurable software systems provide a multitude of configuration options to adjust and optimize their functional and non-functional properties. For instance, to find the fastest configuration for a given setting, a brute-force strategy measures the performance of all configurations, which is typically intractable. Addressing this challenge, state-of-the-art strategies rely on machine learning, analyzing only a few configurations (i.e., a sample set) to predict the performance of other configurations. However, to obtain accurate performance predictions, a representative sample set of configurations is required. Addressing this task, different sampling strategies have been proposed, which come with different advantages (e.g., covering the configuration space systematically) and disadvantages (e.g., the need to enumerate all configurations). In our experiments, we found that most sampling strategies do not achieve a good\u00a0\u2026", "num_citations": "51\n", "authors": ["151"]}
{"title": "Research challenges in the tension between features and services\n", "abstract": " We present a feature-based approach, known from software product lines, to the development of service-oriented architectures. We discuss five benefits of such an approach: improvements in modularity, variability, uniformity, specifiability, and typeability. Subsequently, we review preliminary experiences and results, and propose an agenda for further research in this direction.", "num_citations": "50\n", "authors": ["151"]}
{"title": "Towards the development of ubiquitous middleware product lines\n", "abstract": " Ubiquitous computing is a challenge for the design of middleware. The reasons are resource constraints, mobility, heterogeneity, etc., just to name a few. We argue that such middleware has to be tailored to the application scenario as well as to the target platform. Such tailor-made middleware has to be be built from minimal fine-grained components, and the system structure must be highly configurable, as we will explain. We propose to use the well-known mixin layer approach to build the flexible lightweight middleware envisioned. We show that the thoughtful use of mixin layers is promising in this specific domain and allows to deal with issues such as device heterogeneity and resource constraints. To do so, we present the design and implementation of a middleware and three configurations derived from it. Our evaluation criteria are the number of supported features and the memory footprint. The\u00a0\u2026", "num_citations": "49\n", "authors": ["151"]}
{"title": "On the structure of crosscutting concerns: Using aspects or collaborations\n", "abstract": " While it is well known that crosscutting concerns occur in many software projects, little is known about the inherent properties of these concerns nor how aspects (should) deal with them. We present a framework for classifying the structural properties of crosscutting concerns into (1) those that benefit from AOP and (2) those that should be implemented by OOP mechanisms. Further, we propose a set of code metrics to perform this classification. Applying them to a case study is a first to step toward revealing the current practice of AOP.", "num_citations": "45\n", "authors": ["151"]}
{"title": "A comparison of product-based, feature-based, and family-based type checking\n", "abstract": " Analyzing software product lines is difficult, due to their inherent variability. In the past, several strategies for product-line analysis have been proposed, in particular, product-based, feature-based, and family-based strategies. Despite recent attempts to conceptually and empirically compare different strategies, there is no work that empirically compares all of the three strategies in a controlled setting. We close this gap by extending a compiler for feature-oriented programming with support for product-based, feature-based, and family-based type checking. We present and discuss the results of a comparative performance evaluation that we conducted on a set of 12 feature-oriented, Java-based product lines. Most notably, we found that the family-based strategy is superior for all subject product lines: it is substantially faster, it detects all kinds of errors, and provides the most detailed information about them.", "num_citations": "44\n", "authors": ["151"]}
{"title": "Combining feature-oriented and aspect-oriented programming to support software evolution\n", "abstract": " Starting from the advantages of using Feature-Oriented Programming (FOP) and program families to support software evolution, this paper discusses the drawbacks of current FOP techniques. In particular we address the insufficient crosscutting modularity that complicates software evolution. To overcome this tension we propose the integration of concepts of Aspect-Oriented Programming (AOP) into existing FOP solutions. As study object we utilize FeatureC++, a proprietary extension to C++ that supports FOP. After a short introduction to basic language features of FeatureC++, we summarize the problems regarding the crosscutting modularity. In doing so, we point to the strengths of AOP that can help. Thereupon, we introduce three approaches that combine FOP and AOP concepts: Multi Mixins, Aspectual Mixins, and Aspectual Mixin Layers. Furthermore, we discuss their benefits for software evolution.", "num_citations": "43\n", "authors": ["151"]}
{"title": "Evolutionary trends of developer coordination: A network approach\n", "abstract": " Software evolution is a fundamental process that transcends the realm of technical artifacts and permeates the entire organizational structure of a software project. By means of a longitudinal empirical study of 18 large open-source projects, we examine and discuss the evolutionary principles that govern the coordination of developers. By applying a network-analytic approach, we found that the implicit and self-organizing structure of developer coordination is ubiquitously described by non-random organizational principles that defy conventional software-engineering wisdom. In particular, we found that: (a) developers form scale-free networks, in which the majority of coordination requirements arise among an extremely small number of developers, (b) developers tend to accumulate coordination requirements with more and more developers over time, presumably limited by an upper bound, and (c) initially\u00a0\u2026", "num_citations": "42\n", "authors": ["151"]}
{"title": "Refactoring feature modules\n", "abstract": " In feature-oriented programming, a feature is an increment in program functionality and is implemented by a feature module. Programs are generated by composing feature modules. A generated program may be used by other client programs but occasionally must be transformed to match a particular legacy interface before it can be used. We call the mismatch of the interface of a generated program and a client-desired interface an incompatibility. We introduce the notion of refactoring feature modules (RFMs) that extend feature modules with refactorings. We explain how RFMs reduce incompatibilities and facilitate reuse, and report our experiences on five case studies.", "num_citations": "42\n", "authors": ["151"]}
{"title": "Program refactoring using functional aspects\n", "abstract": " A functional aspect is an aspect that has the semantics of a transformation; it is a function that maps a program to an advised program. Functional aspects are composed by function composition. In this paper, we explore functional aspects in the context of aspect-oriented refactoring. We show that refactoring legacy applications using functional aspects is just as flexible and expressive as traditional aspects (functional aspects can be refactored in any order), while having a simpler semantics (aspect composition is just function composition), and causes fewer undesirable interactions between aspects (the number of potential interactions between functional aspects is half the number of potential interactions between traditional aspects). We analyze several aspect-oriented programs of different sizes to support our claims.", "num_citations": "41\n", "authors": ["151"]}
{"title": "An algebra for feature-oriented software development\n", "abstract": " Feature-Oriented Software Development (FOSD) provides a multitude of formalisms, methods, languages, and tools for building variable, customizable, and extensible software. Along different lines of research different ideas of what a feature is have been developed. Although the existing approaches have similar goals, their representations and formalizations have not been integrated so far into a common framework. We present a feature algebra as a foundation of FOSD. The algebra captures the key ideas and provides a common ground for current and future research in this field, in which also alternative options can be explored.", "num_citations": "41\n", "authors": ["151"]}
{"title": "The shape of feature code: an analysis of twenty C-preprocessor-based systems\n", "abstract": " Feature annotations (e.g., code fragments guarded by #ifdef C-preprocessor directives) control code extensions related to features. Feature annotations have long been said to be undesirable. When maintaining features that control many annotations, there is a high risk of ripple effects. Also, excessive use of feature annotations leads to code clutter, hinder program comprehension and harden maintenance. To prevent such problems, developers should monitor the use of feature annotations, for example, by setting acceptable thresholds. Interestingly, little is known about how to extract thresholds in practice, and which values are representative for feature-related metrics. To address this issue, we analyze the statistical distribution of three feature-related metrics collected from a corpus of 20 well-known and long-lived C-preprocessor-based systems from different domains. We consider three metrics\u00a0\u2026", "num_citations": "39\n", "authors": ["151"]}
{"title": "Indicators for merge conflicts in the wild: survey and empirical study\n", "abstract": " While the creation of new branches and forks is easy and fast with modern version-control systems, merging is often time-consuming. Especially when dealing with many branches or forks, a prediction of merge costs based on lightweight indicators would be desirable to help developers recognize problematic merging scenarios before potential conflicts become too severe in the evolution of a complex software project. We analyze the predictive power of several indicators, such as the number, size or scattering degree of commits in each branch, derived either from the version-control system or directly from the source code. Based on a survey of 41 developers, we inferred 7 potential indicators to predict the number of merge conflicts. We tested corresponding hypotheses by studying 163 open-source projects, including 21,488 merge scenarios and comprising 49,449,773 lines of code. A notable (negative\u00a0\u2026", "num_citations": "37\n", "authors": ["151"]}
{"title": "Feature interactions: the next generation (dagstuhl seminar 14281)\n", "abstract": " The feature-interaction problem is a major threat to modularity and impairs compositional development and reasoning. A feature interaction occurs when the behavior of one feature is affected by the presence of another feature; often it cannot be deduced easily from the behaviors of the individual features involved. The feature-interaction problem became a crisis in the telecommunications industry in the late 1980s, and researchers responded with formalisms that enable automatic detection of feature interactions, architectures that avoid classes of interactions, and techniques for resolving interactions at run-time. While this pioneering work was foundational and very successful, it is limited in the sense that it is based on assumptions that hold only for telecommunication systems. In the meantime, different notions of feature interactions have emerged in different communities, including Internet applications, service systems, adaptive systems, automotive systems, software product lines, requirements engineering, and computational biology. So, feature interactions are a much more general concept than investigated in the past in the context of telecommunication systems, but a classification, comparison, and generalization of the multitude of different views is missing. The feature-interaction problem is still of pivotal importance in various industrial applications, and the Dagstuhl seminar\" Feature Interactions: The Next Generation\" gathered researchers and practitioners from different areas of computer science and other disciplines with the goal to compare, discuss, and consolidate their views, experience, and domain-specific solutions to the feature\u00a0\u2026", "num_citations": "36\n", "authors": ["151"]}
{"title": "Family-based performance measurement\n", "abstract": " Most contemporary programs are customizable. They provide many features that give rise to millions of program variants. Determining which feature selection yields an optimal performance is challenging, because of the exponential number of variants. Predicting the performance of a variant based on previous measurements proved successful, but induces a trade-off between the measurement effort and prediction accuracy. We propose the alternative approach of family-based performance measurement, to reduce the number of measurements required for identifying feature interactions and for obtaining accurate predictions. The key idea is to create a variant simulator (by translating compile-time variability to run-time variability) that can simulate the behavior of all program variants. We use it to measure performance of individual methods, trace methods to features, and infer feature interactions based on the call\u00a0\u2026", "num_citations": "34\n", "authors": ["151"]}
{"title": "Introducing Binary Decision Diagrams in the Explicit-State Verification of Java Code\n", "abstract": " One of the big performance problems of software model checking is the state-explosion problem. Various tools exist to tackle this problem. One of such tools is Java Pathfinder (JPF) an explicit-state model checker for Java code that has been used to verify efficiently a number of real applications. We present jpf-bdd, a JPF extension that allows users to annotate Boolean variables in the system under test to be managed using Binary Decision Diagrams (BDDs). Our tool partitions the program states of the system being verified and manages one part using BDDs. It maintains a formula for the values of these state partitions at every point during the verification. This allows us to merge states that would be kept distinct otherwise, thereby reducing the effect of the state-explosion problem. We demonstrate the performance improvement of our extension by means of three example programs including an implementation of the well-known diningphilosophers problem.", "num_citations": "33\n", "authors": ["151"]}
{"title": "Tradeoffs in modeling performance of highly configurable software systems\n", "abstract": " Modeling the performance of a highly configurable software system requires capturing the influences of its configuration options and their interactions on the system\u2019s performance. Performance-influence models quantify these influences, explaining this way the performance behavior of a configurable system as a whole. To be useful in practice, a performance-influence model should have a low prediction error, small model size, and reasonable computation time. Because of the inherent tradeoffs among these properties, optimizing for one property may negatively influence the others. It is unclear, though, to what extent these tradeoffs manifest themselves in practice, that is, whether a large configuration space can be described accurately only with large models and significant resource investment. By means of 10 real-world highly configurable systems from different domains, we have systematically studied\u00a0\u2026", "num_citations": "30\n", "authors": ["151"]}
{"title": "Balancing precision and performance in structured merge\n", "abstract": " Software-merging techniques face the challenge of finding a balance between precision and performance. In practice, developers use unstructured-merge (i.e., line-based) tools, which are fast but imprecise. In academia, many approaches incorporate information on the structure of the artifacts being merged. While this increases precision in conflict detection and resolution, it can induce severe performance penalties. Striving for a proper balance between precision and performance, we propose a structured-merge approach with auto-tuning. In a nutshell, we tune the merge process on-line by switching between unstructured and structured merge, depending on the presence of conflicts. We implemented a corresponding merge tool for Java, called JDime. Our experiments with 50 real-world Java projects, involving 434 merge scenarios with over 51 million lines of code, demonstrate that our approach\u00a0\u2026", "num_citations": "28\n", "authors": ["151"]}
{"title": "Do# ifdefs influence the occurrence of vulnerabilities? an empirical study of the linux kernel\n", "abstract": " Preprocessors support the diversification of software products with# ifdefs, but also require additional effort from developers to maintain and understand variable code. We conjecture that# ifdefs cause developers to produce more vulnerable code because they are required to reason about multiple features simultaneously and maintain complex mental models of dependencies of configurable code.", "num_citations": "27\n", "authors": ["151"]}
{"title": "Feature-oriented software development\n", "abstract": " Feature-oriented software development is a paradigm for the construction, customization, and synthesis of large-scale and variable software systems, focusing on structure, reuse and variation. In this tutorial, we provide a gentle introduction to software product lines, feature oriented programming, virtual separation of concerns, and variability-aware analysis. We provide an overview, show connections between the different lines of research, and highlight possible future research directions.", "num_citations": "27\n", "authors": ["151"]}
{"title": "Language-independent reference checking in software product lines\n", "abstract": " Feature-Oriented Software Development (FOSD) is a paradigm for the development of software product lines. A challenge in FOSD is to guarantee that all software systems of a software product line are correct. Recent work on type checking product lines can provide a guarantee of type correctness without generating all possible systems. We generalize previous results by abstracting from the specifics of particular programming languages. In a first attempt, we present a reference-checking algorithm that performs key tasks of product-line type checking independently of the target programming language. Experiments with two sample product lines written in Java and C are encouraging and give us confidence that this approach is promising.", "num_citations": "27\n", "authors": ["151"]}
{"title": "Visual Support for Understanding Product Lines\n", "abstract": " The C preprocessor is often used in practice to implement variability in software product lines. Using #ifdef statements provokes problems such as obfuscated source code, yet they will still be used in practice at least in the medium-term future. With CIDE, we demonstrate a tool to improve understanding and maintaining code that contains #ifdef statements by visualizing them with colors and providing different views on the code.", "num_citations": "27\n", "authors": ["151"]}
{"title": "Attributed variability models: outside the comfort zone\n", "abstract": " Variability models are often enriched with attributes, such as performance, that encode the influence of features on the respective attribute. In spite of their importance, there are only few attributed variability models available that have attribute values obtained from empirical, real-world observations and that cover interactions between features. But, what does it mean for research and practice when staying in the comfort zone of developing algorithms and tools in a setting where artificial attribute values are used and where interactions are neglected? This is the central question that we want to answer here. To leave the comfort zone, we use a combination of kernel density estimation and a genetic algorithm to rescale a given (real-world) attribute-value profile to a given variability model. To demonstrate the influence and relevance of realistic attribute values and interactions, we present a replication of a widely\u00a0\u2026", "num_citations": "25\n", "authors": ["151"]}
{"title": "On the duality of aspect-oriented and feature-oriented design patterns\n", "abstract": " Design patterns aim at improving reusability and variability of object-oriented software. Despite a notable success, aspect-oriented programming (AOP) has been discussed recently to improve the design pattern implementations. In another line of research it has been noticed that feature-oriented programming (FOP) is related closely to AOP and that FOP suffices in many situations where AOP is commonly used. In this paper we explore the assumed duality between AOP and FOP mechanisms. As a case study we use the aspect-oriented design pattern implementations of Hannemann and Kiczales. We observe that almost all of the 23 aspect-oriented design pattern implementations can be transformed straightforwardly into equivalent feature-oriented design patterns. For further investigations we provide a set of general rules how to transform aspect-oriented programs into feature-oriented programs.", "num_citations": "25\n", "authors": ["151"]}
{"title": "View infinity: a zoomable interface for feature-oriented software development\n", "abstract": " Software product line engineering provides efficient means to develop variable software. To support program comprehension of software product lines (SPLs), we developed View Infinity, a tool that provides seamless and semantic zooming of different abstraction layers of an SPL. First results of a qualitative study with experienced SPL developers are promising and indicate that View Infinity is useful and intuitive to use.", "num_citations": "24\n", "authors": ["151"]}
{"title": "Feature (de) composition in functional programming\n", "abstract": " The separation of concerns is a fundamental principle in software engineering. Crosscutting concerns are concerns that do not align with hierarchical and block decomposition supported by mainstream programming languages. In the past, crosscutting concerns have been studied mainly in the context of object orientation. Feature orientation is a novel programming paradigm that supports the (de)composition of crosscutting concerns in a system with a hierarchical block structure. In two case studies we explore the problem of crosscutting concerns in functional programming and propose two solutions based on feature orientation.", "num_citations": "24\n", "authors": ["151"]}
{"title": "Predicting quality attributes of software product lines using software and network measures and sampling\n", "abstract": " Software product-line engineering aims at developing families of related products that share common assets to provide customers with tailor-made products. Customers are often interested not only in particular functionalities (ie, features), but also in non-functional quality attributes, such as performance, reliability, and footprint. Measuring quality attributes of all products of a product line usually does not scale. In this research-in-progress report, we propose a systematic approach aiming at efficient and scalable prediction of quality attributes of products. To this end, we establish predictors for certain categories of quality attributes (eg, a predictor for high memory consumption) based on software and network measures, and receiver operating characteristic analysis. We use these predictors to guide a sampling process that takes the assets of a product line as input and determines the products that fall into the category\u00a0\u2026", "num_citations": "22\n", "authors": ["151"]}
{"title": "How to compare program comprehension in FOSD empirically-An experience report\n", "abstract": " There are many different implementation approaches to realize the vision of feature-oriented software development, ranging from simple preprocessors, over feature-oriented programming, to sophisticated aspect-oriented mechanisms. Their impact on readability and maintainability (or program comprehension in general) has caused a debate among researchers, but sound empirical results are missing. We report experience from our endeavor to conduct experiments to measure the influence of different implementation mechanisms on program comprehension. We describe how to design such experiments and report from possibilities and pitfalls we encountered. Finally, we present some early results of our first experiment on comparing the CPP tool with the CIDE tool.", "num_citations": "20\n", "authors": ["151"]}
{"title": "Modeling and optimizing MapReduce programs\n", "abstract": " MapReduce frameworks allow programmers to write distributed, data\u2010parallel programs that operate on multisets. These frameworks offer considerable flexibility to support various kinds of programs and data. To understand the essence of the programming model better and to provide a rigorous foundation for optimizations, we present an abstract, functional model of MapReduce along with a number of customization options. We demonstrate that the MapReduce programming model can also represent programs that operate on lists, which differ from multisets in that the order of elements matters. Along with the functional model, we offer a cost model that allows programmers to estimate and compare the performance of MapReduce programs. Based on the cost model, we introduce two transformation rules aiming at performance optimization of MapReduce programs, which also demonstrates the usefulness of our\u00a0\u2026", "num_citations": "18\n", "authors": ["151"]}
{"title": "Experiments on optimizing the performance of stencil codes with spl conqueror\n", "abstract": " A standard technique for numerically solving elliptic partial differential equations on structured grids is to discretize them, and, then, to apply an efficient geometric multi-grid solver. Unfortunately, finding the optimal choice of multi-grid components and parameter settings is challenging and existing auto-tuning techniques fail to explain performance-optimal settings. To improve the state of the art, we explore whether recent work on optimizing configurations of product lines can be applied to the stencil-code domain. In particular, we extend the domain-independent tool SPL Conqueror in an empirical study to predict the performance-optimal configurations of three geometric multi-grid stencil codes: a program using HIPAcc, the evaluation prototype HSMGP, and a program using DUNE. For HIPAcc, we reach an prediction accuracy of 96%, on average, measuring only 21.4% of all configurations; we predict a\u00a0\u2026", "num_citations": "18\n", "authors": ["151"]}
{"title": "Automating energy optimization with features\n", "abstract": " Mobile devices such as cell phones and notebooks rely on battery power supply. For these systems, optimizing the power consumption is important to increase the system's lifetime. However, this is hard to achieve because energy-saving functions often depend on the hardware, and operating systems. The diversity of hardware components and operating systems makes the implementation time consuming and difficult. We propose an approach to automate energy optimization of programs by implementing energy-saving functionality as modular, separate implementation units (eg, feature modules or aspects). These units are bundled as energy features into an energy-optimization feature library. Based on aspect-oriented and feature-oriented programming, we discuss different techniques to compose the source code of a client program and the implementation units of the energy features.", "num_citations": "18\n", "authors": ["151"]}
{"title": "Optimizing non-functional properties of software product lines by means of refactorings\n", "abstract": " Today, software product line engineering concentrates on tailoring the functionality of programs. However, we and others observed an increasing interest in non-functional properties of products. For example, performance, power awareness, maintainability, and resource consumption are important nonfunctional properties in software development. Current product line techniques have the potential to flexibly optimize nonfunctional properties. In this paper, we present our vision of optimizing non-functional properties in software product lines. We show how such an optimization can be achieved using refactorings and present first results of a case study.", "num_citations": "17\n", "authors": ["151"]}
{"title": "An overview of the gDeep calculus\n", "abstract": " The goal of Feature-oriented Programming (FOP) is to modularize software systems in terms of features. A feature is an increment in functionality and refines the content of other features. A software system typically consists of a collection of different kinds of software artifacts, eg source code, build scripts, documentation, design documents, and performance profiles. We and others have noticed a principle of uniformity, which dictates that when composing features, all software artifacts can actually be refined in a uniform way, regardless of what they represent. Previous work did not take advantage of this uniformity; each kind of software artifact used a separate tool for composition, developed from scratch. We present gDEEP, a core calculus for features and feature composition which is language-independent; it can be used to compose features containing any kinds of artifact. This calculus allows us to define general algorithms for feature refinement, composition, and validation. We provide the formal syntax, operational semantics, and type system of gDEEP and explain how different kinds of software artifacts, including Java, Bali, and XML files, can be represented. A prototype tool and three case studies demonstrate the practicality of our approach.", "num_citations": "17\n", "authors": ["151"]}
{"title": "Biology-Inspired Optimizations of Peer-to-Peer Overlay Networks\n", "abstract": " The aim of this article is to examine the relationship of large-scale Peer-to-Peer (P2P) overlay networks and certain biological systems. In particular, we focus on organization mechanisms that are crucial to adjust and optimize the behavior of large-scale P2P systems in the face of a dynamic environment. We propose to adopt concepts and mechanisms of biological systems in order to extend their capabilities to cope with environmental changes, e.g. a highly dynamic network topology. We introduce the notion of organic P2P overlay networks that adopt behavioral and structural characteristics of biological systems. We present a framework that poses as a basis for understanding, investigating, and implementing organic P2P overlay networks. Using a case study, we analyze an organic P2P overlay network, AntCAN, that utilizes ant colony optimization to improve the query processing in the face of varying query\u00a0\u2026", "num_citations": "17\n", "authors": ["151"]}
{"title": "The interplay of sampling and machine learning for software performance prediction\n", "abstract": " Artificial intelligence has gained considerable momentum in software engineering, but there are major challenges that make this domain special. We review recent advances, raise awareness of the distinctiveness of software configuration spaces, and provide practical guidelines for modeling, predicting, and optimizing performance.", "num_citations": "16\n", "authors": ["151"]}
{"title": "Performance\u2010influence models of multigrid methods: A case study on triangular grids\n", "abstract": " Multigrid methods are among the most efficient algorithms for solving discretized partial differential equations. Typically, a multigrid system offers various configuration options to tune performance for different applications and hardware platforms. However, knowing the best performing configuration in advance is difficult, because measuring all multigrid system variants is costly. Instead of direct measurements, we use machine learning to predict the performance of the variants. Selecting a representative set of configurations for learning is nontrivial, although, but key to prediction accuracy. We investigate different sampling strategies to determine the tradeoff between accuracy and measurement effort. In a nutshell, we learn a performance\u2010influence model that captures the influences of configuration options and their interactions on the time to perform a multigrid iteration and relate this to existing domain knowledge\u00a0\u2026", "num_citations": "16\n", "authors": ["151"]}
{"title": "Static type checking of Hadoop MapReduce programs\n", "abstract": " MapReduce is a programming model for the development of Web-scale programs. It is based on concepts from functional programming, namely higher-order functions, which can be strongly typed using parametric polymorphism. Yet this connection is tenuous. For example, in Hadoop, the connection between the two phases of a MapReduce computation is unsafe: there is no static type check of the generic type parameters involved. We provide a static check for Hadoop programs without asking the user to write any more code. To this end, we use strongly typed higher-order functions checked by the standard Java 5 type checker together with the Hadoop program. We also generate automatically the code needed to execute this program with a standard Hadoop implementation.", "num_citations": "16\n", "authors": ["151"]}
{"title": "Semistructured merge in revision control systems\n", "abstract": " Revision control systems are a major means to manage versions and variants of today\u2019s software systems. An ongoing problem in these systems is how to resolve conflicts when merging independently developed revisions. Unstructured revision control systems are purely text-based and solve conflicts based on textual similarity. Structured revision control systems are tailored to specific languages and use language-specific knowledge for conflict resolution. We propose semistructured revision control systems to inherit the strengths of both classes of systems: generality and expressiveness. The idea is to provide structural information of the underlying software artifacts in the form of annotated grammars, which is motivated by recent work on software product lines. This way, a wide variety of languages can be supported and the information provided can assist the resolution of conflicts. We have implemented a preliminary tool and report on our experience with merging Java artifacts. We believe that drawing a connection between revision control systems and product lines has benefits for both fields.", "num_citations": "16\n", "authors": ["151"]}
{"title": "On the Notion of Functional Aspects in Aspect-Oriented Refactoring\n", "abstract": " In this paper, we examine the notion of functional aspects in context of aspect-oriented refactoring. Treating aspects as functions reduces the potential interactions between aspects significantly. We propose a simple mathematical model that incorporates fundamental properties of aspects and their interactions. Using this model, we show that with regard to refactoring functional aspects are as flexible as traditional aspects, but with reduced program complexity.", "num_citations": "16\n", "authors": ["151"]}
{"title": "Performance prediction of multigrid-solver configurations\n", "abstract": " Geometric multigrid solvers are among the most efficient methods for solving partial differential equations. To optimize performance, developers have to select an appropriate combination of algorithms for the hardware and problem at hand. Since a manual configuration of a multigrid solver is tedious and does not scale for a large number of different hardware platforms, we have been developing a code generator that automatically generates a multigrid-solver configuration tailored to a given problem. However, identifying a performance-optimal solver configuration is typically a non-trivial task, because there is a large number of configuration options from which developers can choose. As a solution, we present a machine-learning approach that allows developers to make predictions of the performance of solver configurations, based on quantifying the influence of individual configuration options and\u00a0\u2026", "num_citations": "15\n", "authors": ["151"]}
{"title": "Self-organization in overlay networks\n", "abstract": " Overlay networks are an important kind of P2P infrastructures. The range of applications and requirements is broad. Consequently, our research objective are overlay networks which organize and adapt themselves at runtime. This article describes the current state of our project and gives an overview of the steps envisioned. We briefly show the necessity for self-organization in overlay networks. Based on our experience, we then provide a list of overlay network system parameters relevant for dynamic adjustment. When designing mechanisms for self-adaptation for overlay networks, we have observed implementationlevel interference and semantic-level interference. To deal with these phenomena, the overlay network architecture envisioned (1) must separate functionality for self-organization and system core functionality and (2) should preserve system integrity. To achieve this, we propose to use self-tuning mechanisms with explicit pre-and postconditions and conflict resolution as well as reflection. We discuss alternative implementation techniques and present one concrete approach based on Aspect-Oriented Programming and Mixin Layers. We conclude with first insights into organic overlay networks and emergent behavior.", "num_citations": "15\n", "authors": ["151"]}
{"title": "Characterizing complexity of highly-configurable systems with variational call graphs: Analyzing configuration options interactions complexity in function calls\n", "abstract": " Security has consistently been the focus of attention in many highly-configurable software systems. Several vulnerabilities on widely-used systems, such as the Linux kernel and OpenSSL, are reported every day in the National Vulnerability Database (NVD). The configurability of these systems enables the rapid generation of customized products, but also creates security challenges in the development and maintenance processes. For instance, interactions caused by configurations may create serious security threats and make generated products more susceptible to attacks [6], but the causes of these problems may be harder to detect because they occur only in specific configurations.", "num_citations": "14\n", "authors": ["151"]}
{"title": "The potential of polyhedral optimization: An empirical study\n", "abstract": " Present-day automatic optimization relies on powerful static (i.e., compile-time) analysis and transformation methods. One popular platform for automatic optimization is the polyhedron model. Yet, after several decades of development, there remains a lack of empirical evidence of the model's benefits for real-world software systems. We report on an empirical study in which we analyzed a set of popular software systems, distributed across various application domains. We found that polyhedral analysis at compile time often lacks the information necessary to exploit the potential for optimization of a program's execution. However, when conducted also at run time, polyhedral analysis shows greater relevance for real-world applications. On average, the share of the execution time amenable to polyhedral optimization is increased by a factor of nearly 3. Based on our experimental results, we discuss the merits and\u00a0\u2026", "num_citations": "14\n", "authors": ["151"]}
{"title": "Generating qualifiable avionics software: An experience report (E)\n", "abstract": " We report on our experience with enhancing the data-management component in the avionics software of the NH90 helicopter at Airbus Helicopters. We describe challenges regarding the evolution of avionics software by means of real-world evolution scenarios that arise in industrial practice. A key role plays a legally-binding certification process, called qualification, which is responsible for most of the development effort and cost. To reduce effort and cost, we propose a novel generative approach to develop qualifiable avionics software by combining model-based and product-line technology. Using this approach, we have already generated code that is running on the NH90 helicopter and that is in the process of replacing the current system code. Based on an interview with two professional developers at Airbus and an analysis of the software repository of the NH90, we systematically compare our approach with\u00a0\u2026", "num_citations": "13\n", "authors": ["151"]}
{"title": "Optimizing performance of stencil code with SPL conqueror\n", "abstract": " A standard technique to numerically solve elliptic partial differential equations on structured grids is to discretize them via finite differences and then to apply an efficient geometric multi-grid solver. Unfortunately, finding the optimal choice of multi-grid components and parameters is challenging and platform dependent, especially, in cases where domain knowledge is incomplete. Auto-tuning is a viable alternative, but faces the problem of large configuration spaces and feature interactions. To improve the state of the art, we explore whether recent work on configuration optimization in product lines can be applied to the stencil-code domain. In particular, we extend and use the domain-independent tool SPL Conqueror in a series of experiments to predict the performance-optimal configurations of two geometric multigrid codes: a program using the HIPAcc framework and an evaluation prototype called HSMGP. For HIPAcc, we can predict the performance of all configurations with an accuracy of 98%, on average, when measuring 57.5% of the configurations, and we are able to predict a configuration that is close to the optimal one after measuring only less than 4% of all configurations. For HSMGP, we can predict the performance with an accuracy of 88% when measuring 11% of all configurations.", "num_citations": "13\n", "authors": ["151"]}
{"title": "PolyJIT: polyhedral optimization just in time\n", "abstract": " While polyhedral optimization appeared in mainstream compilers during the past decade, its profitability in scenarios outside its classic domain of linear-algebra programs has remained in question. Recent implementations, such as the LLVM plugin Polly, produce promising speedups, but the restriction to affine loop programs with control flow known at compile time continues to be a limiting factor. PolyJIT combines polyhedral optimization with multi-versioning at run time, at which one has access to knowledge enabling polyhedral optimization, which is not available at compile time. By means of a fully-fledged implementation of a light-weight just-in-time compiler and a series of experiments on a selection of real-world and benchmark programs, we demonstrate that the consideration of run-time knowledge helps in tackling compile-time violations of affinity and, consequently, offers new opportunities of optimization\u00a0\u2026", "num_citations": "12\n", "authors": ["151"]}
{"title": "Configcrusher: Towards white-box performance analysis for configurable systems\n", "abstract": " Stakeholders of configurable systems are often interested in knowing how configuration options influence the performance of a system to facilitate, for example, the debugging and optimization processes of these systems. Several black-box approaches can be used to obtain this information, but they either sample a large number of configurations to make accurate predictions or miss important performance-influencing interactions when sampling few configurations. Furthermore, black-box approaches cannot pinpoint the parts of a system that are responsible for performance differences among configurations. This article proposes ConfigCrusher, a white-box performance analysis that inspects the implementation of a system to guide the performance analysis, exploiting several insights of configurable systems in the process. ConfigCrusher employs a static data-flow analysis to identify how configuration options may\u00a0\u2026", "num_citations": "11\n", "authors": ["151"]}
{"title": "Renaming and shifted code in structured merging: Looking ahead for precision and performance\n", "abstract": " Diffing and merging of source-code artifacts is an essential task when integrating changes in software versions. While state-of-the-art line-based merge tools (e.g., git merge) are fast and independent of the programming language used, they have only a low precision. Recently, it has been shown that the precision of merging can be substantially improved by using a language-aware, structured approach that works on abstract syntax trees. But, precise structured merging is NP hard, especially, when considering the notoriously difficult scenarios of renamings and shifted code. To address these scenarios without compromising scalability, we propose a syntax-aware, heuristic optimization for structured merging that employs a lookahead mechanism during tree matching. The key idea is that renamings and shifted code are not arbitrarily distributed, but their occurrence follows patterns, which we address with a syntax\u00a0\u2026", "num_citations": "11\n", "authors": ["151"]}
{"title": "Pointcuts, advice, refinements, and collaborations: similarities, differences, and synergies\n", "abstract": " Aspect-oriented programming (AOP) is a novel programming paradigm that aims at modularizing complex software. It embraces several mechanisms including (1) pointcuts and advice as well as (2) refinements and collaborations. Though all these mechanisms deal with crosscutting concerns, i.e., a special class of design and implementation problems that challenge traditional programming paradigms, they do so in different ways. In this article we explore their relationship and their impact on modularity, which is an important prerequisite for reliable and maintainable software. Our exploration helps researchers and practitioners to understand their differences and exposes which mechanism is best used for which problem.", "num_citations": "11\n", "authors": ["151"]}
{"title": "Does feature scattering follow power-law distributions? an investigation of five pre-processor-based systems\n", "abstract": " Feature scattering is long said to be an undesirable characteristic in source code. Since scattered features introduce extensions across the code base, their maintenance requires analyzing and changing different locations in code, possibly causing ripple effects. Despite this fact, scattering often occurs in practice, either due to limitations in existing programming languages (eg, imposition of a dominant decomposition) or time-pressure issues. In the latter case, scattering provides a simple way to support new capabilities, avoiding the upfront investment of creating modules and interfaces (when possible). Hence, we argue that scattering is not necessarily bad, provided it is kept within certain limits, or thresholds. Extracting thresholds, however, is not a trivial task. For instance, research shows that some source-code-metric distributions are heavy-tailed, usually following power-law models. In the face of heavy-tailed\u00a0\u2026", "num_citations": "10\n", "authors": ["151"]}
{"title": "Is static analysis able to identify unnecessary source code?\n", "abstract": " Grown software systems often contain code that is not necessary anymore. Such unnecessary code wastes resources during development and maintenance, for example, when preparing code for migration or certification. Running a profiler may reveal code that is not used in production, but it is often time-consuming to obtain representative data in this way. We investigate to what extent a static analysis approach, which is based on code stability and code centrality, is able to identify unnecessary code and whether its recommendations are relevant in practice. To study the feasibility and usefulness of our approach, we conducted a study involving 14 open-source and closed-source software systems. As there is no perfect oracle for unnecessary code, we compared recommendations for unnecessary code with historical cleanups, runtime usage data, and feedback from 25 developers of five software projects. Our\u00a0\u2026", "num_citations": "9\n", "authors": ["151"]}
{"title": "Quantifying structural attributes of system decompositions in 28 feature-oriented software product lines\n", "abstract": " A key idea of feature orientation is to decompose a software product line along the features it provides. Feature decomposition is orthogonal to object-oriented decomposition\u2014it crosscuts the underlying package and class structure. It has been argued often that feature decomposition improves system structure by reducing coupling and by increasing cohesion. However, recent empirical findings suggest that this is not necessarily the case. In this exploratory, observational study, we investigate the decompositions of 28 feature-oriented software product lines into classes, features, and feature-specific class fragments. The product lines under investigation are implemented using the feature-oriented programming language Fuji. In particular, we quantify and compare the internal attributes import coupling and cohesion of the different product-line decompositions in a systematic, reproducible manner. For this\u00a0\u2026", "num_citations": "9\n", "authors": ["151"]}
{"title": "Lifting inter-app data-flow analysis to large app sets\n", "abstract": " Mobile apps process increasing amounts of private data, giving rise to privacy concerns. Such concerns do not only arise from single apps, which might\u2014accidentally or intentionally\u2014leak private information to untrusted parties, but also from multiple apps communicating with each other. Certain combinations of apps can create critical data flows not detectable by analyzing single apps individually. While sophisticated tools exist to analyze data flows inside and across apps, none of these scale to large numbers of apps, given the combinatorial explosion of possible (inter-app) data flows. We present a scalable approach to analyze data flows across Android apps. At the heart of our approach is a graph-based data structure that represents inter-app flows Following ideas from productline analysis, the structure exploits redundancies among flows and thereby prevents the combinatorial explosion. Instead of focusing on specific installations of app sets on mobile devices, we lift traditional data-flow analysis approaches to analyze and represent data flows of all possible combinations of apps. We developed the tool Sifta and applied it to several existing app benchmarks and real-word app sets, demonstrating its scalability while maintaining reasonable accuracy.", "num_citations": "9\n", "authors": ["151"]}
{"title": "Language-independent quantification and weaving for feature composition\n", "abstract": " Based on a general model of feature composition, we present a composition language that enables programmers by means of quantification and weaving to formulate extensions to programs written in different languages. We explore the design space of composition languages that rely on quantification and weaving and discuss our choices. We outline a tool that extends an existing infrastructure for feature composition and discuss results of three initial case studies. We found that, due to its language independence, our approach is less powerful than aspect-oriented languages but still usable for many implementation problems.", "num_citations": "9\n", "authors": ["151"]}
{"title": "Streamlining feature-oriented designs\n", "abstract": " Software development for embedded systems gains momentum but faces many challenges. Especially the constraints of deeply embedded systems, i.e., extreme resource and performance constraints, seem to prohibit the successful application of modern and approved programming and modularization techniques. In this paper we indicate that this objection is not necessarily justified. We propose to use refinement chain optimization to tailor and streamline feature-oriented designs to satisfy the resource constraints of (deeply) embedded systems. By means of a quantitative analysis of a case study we show that our proposal leads to a performance and footprint improvement significant for (deeply) embedded systems.", "num_citations": "9\n", "authors": ["151"]}
{"title": "Piggyback Meta-Data Propagation in Distributed Hash Tables\n", "abstract": " Distributed Hashtables (DHT) are intended to provide Internet-scale data management. By following the peer-to-peer paradigm, DHT consist of independent peers and operate without central coordinators. Consequentially, global knowledge is not available and any information have to be exchanged by local interactions between the peers. Beneath data management operations, a lot of meta-data have to be exchanged between the nodes, eg, status updates, feedback for reputation management or application-specific information. Because of the large scale of the DHT, it would be expensive to disseminate meta-data by peculiar messages. In this article we investigate in a lazy dissemination protocol that piggybacks attachments to messages the peers send out anyhow. We present a software engineering approach based on mixin layers and aspect-oriented programming to cope with the extremely differing application-specific requirements. The applicability of our protocol is confirmed by means of experiments with a CAN implementation.", "num_citations": "9\n", "authors": ["151"]}
{"title": "White-box analysis over machine learning: Modeling performance of configurable systems\n", "abstract": " Performance-influence models can help stakeholders understand how and where configuration options and their interactions influence the performance of a system. With this understanding, stakeholders can debug performance behavior and make deliberate configuration decisions. Current black-box techniques to build such models combine various sampling and learning strategies, resulting in tradeoffs between measurement effort, accuracy, and interpretability. We present Comprex, a white-box approach to build performance-influence models for configurable systems, combining insights of local measurements, dynamic taint analysis to track options in the implementation, compositionality, and compression of the configuration space, without relying on machine learning to extrapolate incomplete samples. Our evaluation on 4 widely-used, open-source projects demonstrates that Comprex builds similarly\u00a0\u2026", "num_citations": "8\n", "authors": ["151"]}
{"title": "ExaStencils: Advanced multigrid solver generation\n", "abstract": " Present-day stencil codes are implemented in general-purpose programming languages, such as Fortran, C, or Java, Python or derivates thereof, and harnesses for parallelism, such as OpenMP, OpenCL or MPI. Project ExaStencils pursued a domain-specific approach with a language, called ExaSlang, that is stratified into four layers of abstraction, the most abstract being the formulation in continuous mathematics and the most concrete a full, automatically generated implementation. At every layer, the corresponding language expresses not only computational directives but also domain knowledge of the problem and platform to be leveraged for optimization. We describe the approach, the software technology", "num_citations": "8\n", "authors": ["151"]}
{"title": "Iterative schedule optimization for parallelization in the polyhedron model\n", "abstract": " The polyhedron model is a powerful model to identify and apply systematically loop transformations that improve data locality (e.g., via tiling) and enable parallelization. In the polyhedron model, a loop transformation is, essentially, represented as an affine function. Well-established algorithms for the discovery of promising transformations are based on performance models. These algorithms have the drawback of not being easily adaptable to the characteristics of a specific program or target hardware. An iterative search for promising loop transformations is more easily adaptable and can help to learn better models. We present an iterative optimization method in the polyhedron model that targets tiling and parallelization. The method enables either a sampling of the search space of legal loop transformations at random or a more directed search via a genetic algorithm. For the latter, we propose a set of novel\u00a0\u2026", "num_citations": "8\n", "authors": ["151"]}
{"title": "Vergleich und Integration von Komposition und Annotation zur Implementierung von Produktlinien\n", "abstract": " Es gibt eine Vielzahl sehr unterschiedlicher Techniken, Sprachen und Werkzeuge zur Entwicklung von Softwareproduktlinien. Trotzdem liegen gemeinsame Mechanismen zu Grunde, die eine Klassifikation in Kompositionsund Annotationsansatz erlauben. W\u00e4hrend der Kompositionsansatz in der Forschung gro\u00dfe Beachtung findet, kommt im industriellen Umfeld haupts\u00e4chlich der Annotationsansatz zur Anwendung. Wir analysieren und vergleichen beide Ans\u00e4tze anhand von drei repr\u00e4sentativen Vertretern und identifizieren anhand von sechs Kriterien individuelle St\u00e4rken und Schw\u00e4chen. Wir stellen fest, dass die jeweiligen St\u00e4rken und Schw\u00e4chen komplement\u00e4r sind. Aus diesem Grund schlagen wir die Integration des Kompositionsund Annotationsansatzes vor, um so die Vorteile beider zu vereinen, dem Entwickler eine breiteres Spektrum an Implementierungsmechanismen zu Verf\u00fcgung zu stellen und die Einf\u00fchrung von Produktlinientechnologie in bestehende Softwareprojekte zu erleichtern.", "num_citations": "8\n", "authors": ["151"]}
{"title": "Generic feature modules: Two-staged program customization\n", "abstract": " With feature-oriented programming (FOP) and generics programmers have proper means for structuring software so that its elements can be reused and extended. This paper addresses the issue whether both approaches are equivalent. While FOP targets at large-scale building blocks and compositional programming, generics provide fine-grained customization at type-level. We contribute an analysis that reveals the individual capabilities of both approaches with respect to program customization. Therefrom, we extract guidelines for programmers in what situations which approach suffices. Furthermore, we present a fully implemented language proposal that integrates FOP and generics in order to combine their strengths. Our approach facilitates two-staged program customization:(1) selecting sets of features;(2) parameterizing features subsequently. This allows a broader spectrum of code reuse to be covered\u2013reflected by proper language level mechanisms. We underpin our proposal by means of a case study.", "num_citations": "8\n", "authors": ["151"]}
{"title": "Implementing Bounded Aspect Quantification in AspectJ\n", "abstract": " The integration of aspects into the methodology of stepwise software development and evolution is still an open issue. This paper focuses on the global quantification mechanism of nowadays aspect-oriented languages that contradicts basic principles of this methodology. One potential solution to this problem is to bound the potentially global effects of aspects to a set of local development steps. We discuss several alternatives to implement such bounded aspect quantification in AspectJ. Afterwards, we describe a concrete approach that relies on meta-data and pointcut restructuring in order to control the quantification of aspects. Finally, we discuss open issues and further work.", "num_citations": "8\n", "authors": ["151"]}
{"title": "On the relation of external and internal feature interactions: A case study\n", "abstract": " Detecting feature interactions is imperative for accurately predicting performance of highly-configurable systems. State-of-the-art performance prediction techniques rely on supervised machine learning for detecting feature interactions, which, in turn, relies on time consuming performance measurements to obtain training data. By providing information about potentially interacting features, we can reduce the number of required performance measurements and make the overall performance prediction process more time efficient. We expect that the information about potentially interacting features can be obtained by statically analyzing the source code of a highly-configurable system, which is computationally cheaper than performing multiple performance measurements. To this end, we conducted a qualitative case study in which we explored the relation between control-flow feature interactions (detected through static program analysis) and performance feature interactions (detected by performance prediction techniques using performance measurements). We found that a relation exists, which can potentially be exploited to predict performance interactions.", "num_citations": "7\n", "authors": ["151"]}
{"title": "On the relation between internal and external feature interactions in feature-oriented product lines: a case study\n", "abstract": " The feature-interaction problem has been explored for many years. Still, we lack sufficient knowledge about the interplay of different kinds of interactions in software product lines. Exploring the relations between different kinds of feature interactions will allow us to learn more about the nature of interactions and their causes. This knowledge can then be applied for improving existing approaches for detecting, managing, and resolving feature interactions. We present a framework for studying relations between different kinds of interactions. Furthermore, we report and discuss the results of a preliminary study in which we examined correlations between internal feature interactions (quantified by a set of software measures) and external feature interactions (represented by product-line-specific type errors). We performed the evaluation on a set of 15 feature-oriented, Java-based product lines. We observed moderate\u00a0\u2026", "num_citations": "7\n", "authors": ["151"]}
{"title": "PLUS: performance learning for uncertainty of software\n", "abstract": " Uncertainty is particularly critical in software performance engineering when it relates to the values of important parameters such as workload, operational profile, and resource demand, because such parameters inevitably affect the overall system performance. Prior work focused on monitoring the performance characteristics of software systems while considering influence of configuration options. The problem of incorporating uncertainty as a first-class concept in the software development process to identify performance issues is still challenging. The PLUS (Performance Learning for Uncertainty of Software) approach aims at addressing these limitations by investigating the specification of a new class of performance models capturing how the different uncertainties underlying a software system affect its performance characteristics. The main goal of PLUS is to answer a fundamental question in the software\u00a0\u2026", "num_citations": "6\n", "authors": ["151"]}
{"title": "Feature-Oriented System Design and Engineering.\n", "abstract": " This is a personal appreciation and snapshot view of Manfred Broy\u2019s contributions to the research area of feature-oriented system design and engineering. We sketch the algebraic approach to the area and relate Broy\u2019s work to it. To give it a concrete context, we compare it with our own work on feature orientation. There are a number of correspondences and some differences: Broy works at a higher level of abstraction, the specification level, we at a level closer to the software structure, the programming level. We put more emphasis on the concept of program similarity than Broy does.", "num_citations": "6\n", "authors": ["151"]}
{"title": "Konfigurierbarkeit f\u00fcr ressourceneffiziente Datenhaltung in eingebetteten Systemen am Beispiel von Berkeley DB.\n", "abstract": " Funktionsumfang und Komplexit\u00e4t von Datenbankmanagementsystemen nehmen fortw\u00e4hrend zu. Die tats\u00e4chlich ben\u00f6tigte Funktionalit\u00e4t wird dabei oft au\u00dfer Acht gelassen und f\u00fcr unterschiedlichste Anwendungsgebiete die gleiche Software ausgeliefert. Im stetig wachsenden Bereich eingebetteter Systeme ist der Ressourcenbedarf von Datenmanagementsystemen von besonderer Bedeutung. Auf Grund der Vielzahl existierender Hardwarearchitekturen f\u00fchrt dies h\u00e4ufig zu Neuentwicklungen. Merkmalsorientierte Programmierung (FOP) unterst\u00fctzt die Entwicklung hoch konfigurierbarer Software und hat das Potential diese Probleme zu l\u00f6sen. Bedenken bez\u00fcglich der Performance verhindern aber oft den Einsatz moderner Softwaretechniken im Bereich des Datenmanagements. In diesem Beitrag zeigen wir, wie hoch konfigurierbare DBMS mit Hilfe von FOP entwickelt werden k\u00f6nnen, ohne dabei Einschr\u00e4nkungen hinsichtlich des Ressourcenbedarfs und der Performance in Kauf nehmen zu m\u00fcssen. Mit der Umsetzung der Konzepte am Beispiel von Berkeley DB und einer umfangreichen Analyse untermauern wir unsere Argumente.", "num_citations": "6\n", "authors": ["151"]}
{"title": "Configurable Binding: How to Exploit Mixins and Design Patterns for Resource Constrained Environments\n", "abstract": " Realtime and embedded systems are subject to strict requirements on performance and resource consumption. However, modern software-engineering methods do not always allow to meet these technical requirements. More specifically, common approaches for the design of flexible, reusable and customizable software lead to increased memory consumption and reduced performance. This article proposes a configurable binding mechanism that addresses this important problem. It adopts features of design patterns and mixins. Our proposal is not obvious\u2013the difficulty is to arrange the structural elements of design patterns and mixins so that the binding is configurable and copes with those requirements specific to realtime and embedded systems. Measurements show that our approach indeed accomplishes this. At the same time, it provides the known virtues of modern software-engineering methods, in particular configurability and reusability. Finally, we say how to apply our approach to other common design and implementation methodologies, eg, program families, frameworks, and aspects.", "num_citations": "6\n", "authors": ["151"]}
{"title": "White-Box Performance-Influence models: A profiling and learning approach\n", "abstract": " Many modern software systems are highly configurable, allowing the user to tune them for performance and more. Current performance modeling approaches aim at finding performance-optimal configurations by building performance models in a black-box manner. While these models provide accurate estimates, they cannot pinpoint causes of observed performance behavior to specific code regions. This does not only hinder system understanding, but it also complicates tracing the influence of configuration options to individual methods.We propose a white-box approach that models configuration-dependent performance behavior at the method level. This allows us to predict the influence of configuration decisions on individual methods, supporting system understanding and performance debugging. The approach consists of two steps: First, we use a coarse-grained profiler and learn performance-influence\u00a0\u2026", "num_citations": "5\n", "authors": ["151"]}
{"title": "A decision tree lifted domain for analyzing program families with numerical features\n", "abstract": " Lifted (family-based) static analysis by abstract interpretation is capable of analyzing all variants of a program family simultaneously, in a single run without generating any of the variants explicitly. The elements of the underlying lifted analysis domain are tuples, which maintain one property per variant. Still, explicit property enumeration in tuples, one by one for all variants, immediately yields combinatorial explosion. This is particularly apparent in the case of program families that, apart from Boolean features, contain also numerical features with large domains, thus giving rise to astronomical configuration spaces. The key for an efficient lifted analysis is a proper handling of variabilityspecific constructs of the language (eg, feature-based runtime tests and# if directives). In this work, we introduce a new symbolic representation of the lifted abstract domain that can efficiently analyze program families with numerical features. This makes sharing between property elements corresponding to different variants explicitly possible. The elements of the new lifted domain are constraint-based decision trees, where decision nodes are labeled with linear constraints defined over numerical features and the leaf nodes belong to an existing single-program analysis domain. To illustrate the potential of this representation, we have implemented an experimental lifted static analyzer, called SPLNum2 Analyzer, for inferring invariants of C programs. An empirical evaluation on BusyBox and on benchmarks from SV-COMP yields promising preliminary results indicating that our decision trees-based approach is effective and outperforms the baseline tuple-based approach.", "num_citations": "5\n", "authors": ["151"]}
{"title": "The impact of structure on software merging: semistructured versus structured merge\n", "abstract": " Merge conflicts often occur when developers concurrently change the same code artifacts. While state of practice unstructured merge tools (e.g Git merge) try to automatically resolve merge conflicts based on textual similarity, semistructured and structured merge tools try to go further by exploiting the syntactic structure and semantics of the artifacts involved. Although there is evidence that semistructured merge has significant advantages over unstructured merge, and that structured merge reports significantly fewer conflicts than unstructured merge, it is unknown how semistructured merge compares with structured merge. To help developers decide which kind of tool to use, we compare semistructured and structured merge in an empirical study by reproducing more than 40,000 merge scenarios from more than 500 projects. In particular, we assess how often the two merge strategies report different results, we\u00a0\u2026", "num_citations": "5\n", "authors": ["151"]}
{"title": "Speeding up iterative polyhedral schedule optimization with surrogate performance models\n", "abstract": " Iterative program optimization is known to be able to adapt more easily to particular programs and target hardware than model-based approaches. An approach is to generate random program transformations and evaluate their profitability by applying them and benchmarking the transformed program on the target hardware. This procedure\u2019s large computational effort impairs its practicality tremendously, though. To address this limitation, we pursue the guidance of a genetic algorithm for program optimization via feedback from surrogate performance models. We train the models on program transformations that were evaluated during previous iterative optimizations. Our representation of programs and program transformations refers to the polyhedron model. The representation is particularly meaningful for an optimization of loop programs that profit a from coarse-grained parallelization for execution on modern\u00a0\u2026", "num_citations": "5\n", "authors": ["151"]}
{"title": "Variability of stencil computations for porous media\n", "abstract": " Many problems formulated in partial differential equations lead to stencil\u2010type structures after applying an appropriate structured discretization. On one hand, exploiting these stencil structures in simulations can lead to massive performance improvements, compared to forming a sparse matrix. On the other hand, the generality of the simulation is restricted, depending on the exact definition of the stencils. In this article, we discuss the variability of stencils in the domain of porous\u2010media applications and present a family of models that grows in complexity. To demonstrate the relation between equation and discretization on the resulting stencil used to simulate the equation, we consider 4 models from the porous media domain. This way, we describe the influence of design decisions made during the discretization on the shape of stencils, to give application engineers' information on the variability they have to consider\u00a0\u2026", "num_citations": "5\n", "authors": ["151"]}
{"title": "On the fulfillment of coordination requirements in open-source software projects: An exploratory study\n", "abstract": " In large-scale open-source software projects, where developers are often distributed across the entire planet, coordination among developers is crucial. To estimate whether a state of socio-technical congruence is achieved, which is associated with software quality and project success, we assess the alignment of collaboration and communication in such software projects in terms of coordination requirements. By means of an empirical study on a substantial set of large-scale open-source software projects\u2014the development histories of all projects sum up to over 180 years\u2014we aim at shedding light on this issue. To this end, to take a more semantic view on this phenomenon in comparison to previous work, we do not only identify coordination requirements arising from files and functions only, but also those arising from features. We found that open-source developers fulfill coordination requirements\u00a0\u2026", "num_citations": "4\n", "authors": ["151"]}
{"title": "Predicting performance of software configurations: There is no silver bullet\n", "abstract": " Many software systems offer configuration options to tailor their functionality and non-functional properties (e.g., performance). Often, users are interested in the (performance-)optimal configuration, but struggle to find it, due to missing information on influences of individual configuration options and their interactions. In the past, various supervised machine-learning techniques have been used to predict the performance of all configurations and to identify the optimal one. In the literature, there is a large number of machine-learning techniques and sampling strategies to select from. It is unclear, though, to what extent they affect prediction accuracy. We have conducted a comparative study regarding the mean prediction accuracy when predicting the performance of all configurations considering 6 machine-learning techniques, 18 sampling strategies, and 6 subject software systems. We found that both the learning technique and the sampling strategy have a strong influence on prediction accuracy. We further observed that some learning techniques (e.g., random forests) outperform other learning techniques (e.g., k-nearest neighbor) in most cases. Moreover, as the prediction accuracy strongly depends on the subject system, there is no combination of a learning technique and sampling strategy that is optimal in all cases, considering the tradeoff between accuracy and measurement overhead, which is in line with the famous no-free-lunch theorem.", "num_citations": "4\n", "authors": ["151"]}
{"title": "Understanding Programmers' Brains with fMRI\n", "abstract": " The human factor plays an important role in software engineering, because humans design, implement, and maintain software. One of the most important cognitive processes is program comprehension, because programmers spend most of their time with understanding source code [3, 4, 5]. However, despite of more than 30 years of research, we still have no clear understanding of the relevant processes during comprehending source code. To gain a deeper understanding of program comprehension, we measured it by using functional magnetic resonance imaging (fMRI)[2] since fMRI has proved successful to study comparatively complex cognitive processes in detail. Our hope is that in the process of understanding developer's cognition, we can create a platform for sharing how these complex processes map onto other studies of cognition, and even incorporate ideas for organizing software into models of cognitive processes.In our study, we designed several short source-code snippets and asked computer-science students to determine the output if the source code would be executed (see Fig. 2 or project's website (tinyurl. com/ProgramComprehensionAndfMRI/) for examples). As control condition, we let participants locate syntax errors that did not require understanding the source code (Fig. 3).", "num_citations": "4\n", "authors": ["151"]}
{"title": "ExaStencils: Advanced Stencil-Code Engineering\u2014First Project Report\u2014\n", "abstract": " Project ExaStencils pursues a radically new approach to stencil-code engineering. Present-day stencil codes are implemented in general-purpose programming languages, such as Fortran, C, or Java, or derivates thereof, and harnesses for parallelism, such as OpenMP, OpenCL or MPI. ExaStencils favors a much more domain-specific approach being the mathematical formulation, the most concrete the optimized target code. At every layer, the corresponding language expresses not only computational directives but also domain knowledge of the problem and platform to be leveraged for optimization. This approach will enable a highly automated code generation at all layers and has been demonstrated successfully before in the US projects FFTW and SPIRAL for certain linear transforms. 1 The Challenges of Exascale Computing The performance of supercomputers is on the way from petascale to exascale.", "num_citations": "4\n", "authors": ["151"]}
{"title": "The potential of polyhedral optimization\n", "abstract": " Present-day automatic optimization relies on powerful static (ie, compile-time) analysis and transformation methods. One popular platform for automatic optimization is the polyhedron model. Yet, after several decades of development, there remains a lack of empirical evidence of the model\u2019s benefits for real-world software systems. We report on an empirical study in which we analyzed a set of popular software systems, distributed across various application domains. We found that polyhedral optimization at compile time often lacks the information necessary to exploit the potential for optimization of a program\u2019s execution. However, when conducted also at run time, polyhedral optimization shows greater relevance for real-world applications. On average, the share of the run time amenable to polyhedral optimization is increased by a factor of nearly 3.", "num_citations": "4\n", "authors": ["151"]}
{"title": "Experience from measuring program comprehension-toward a general framework\n", "abstract": " Program comprehension plays a crucial role during the software-development life cycle: Maintenance programmers spend most of their time with comprehending source code, and maintenance is the main cost factor in software development. Thus, if we can improve program comprehension, we can save considerable amount of time and cost. To improve program comprehension, we have to measure it first. However, program comprehension is a complex, internal cognitive process that we cannot observe directly. Typically, we need to conduct controlled experiments to soundly measure program comprehension. However, empirical research is applied only reluctantly in software engineering. To close this gap, we set out to support researchers in planning and conducting experiments regarding program comprehension. We report our experience with experiments that we conducted and present the resulting framework to support researchers in planning and conducting experiments. Additionally, we discuss the role of teaching for the empirical researchers of tomorrow.", "num_citations": "4\n", "authors": ["151"]}
{"title": "An Overview of Feature Featherweight Java\n", "abstract": " Feature-oriented programming (FOP) is a paradigm that incorporates programming language technology, program generation techniques, and stepwise refinement. In their GPCE\u201907 paper, Thaker et al. suggest the development of a type system for FOP in order to guarantee safe feature composition. We present such a type system along with a calculus for a simple feature-oriented, Java-like language, called Feature Featherweight Java (FFJ). Furthermore, we explore several extensions of FFJ and how they affect type soundness.", "num_citations": "4\n", "authors": ["151"]}
{"title": "Mastering uncertainty in performance estimations of configurable software systems\n", "abstract": " Understanding the influence of configuration options on performance is key for finding optimal system configurations, system understanding, and performance debugging. In prior research, a number of performance-influence modeling approaches have been proposed, which model a configuration option's influence and a configuration's performance as a scalar value. However, these point estimates falsely imply a certainty regarding an option's influence that neglects several sources of uncertainty within the assessment process, such as (1) measurement bias, (2) model representation and learning process, and (3) incomplete data. This leads to the situation that different approaches and even different learning runs assign different scalar performance values to options and interactions among them. The true influence is uncertain, though. There is no way to quantify this uncertainty with state-of-the-art performance\u00a0\u2026", "num_citations": "3\n", "authors": ["151"]}
{"title": "From crosscutting concerns to feature interactions: A tale of misunderstandings and enlightenments (keynote)\n", "abstract": " Both crosscutting concerns and feature interactions are phenomena that may impair modularity. Crosscutting concerns have been in the center of interest in aspect-oriented software development. Much research in this direction\u2014including my own\u2014is aimed at developing mechanisms to avoid and manage code scattering and tangling, with a strong focus on source-code organization. Feature interactions have been studied even earlier. Research on feature interactions always emphasized behavioral aspects that arise when two features interact, regardless of the implementation. Both phenomena are related, but not as closely as one may think. In this talk, I will tell my personal story on how I started my research career with investigating crosscutting concerns and\u2014after a long journey and many misunderstanding and enlightments\u2014arrived at being interested in feature interactions. In some sense this journey is also\u00a0\u2026", "num_citations": "3\n", "authors": ["151"]}
{"title": "Program Sketching Using Lifted Analysis for Numerical Program Families\n", "abstract": " This work presents a novel approach for synthesizing numerical program sketches using lifted (family-based) static program analysis. In particular, our approach leverages a lifted static analysis based on abstract interpretation, which is used for analyzing program families with numerical features. It takes as input the common code base, which encodes all variants of a program family, and produces precise results for all variants in a single analysis run. The elements of the underlying lifted analysis domain are decision trees, in which decision nodes are labeled with linear constraints defined over numerical features and leaf nodes belong to a given single-program analysis domain.", "num_citations": "2\n", "authors": ["151"]}
{"title": "Lifted static analysis of dynamic program families by abstract interpretation\n", "abstract": " Program families (software product lines) are increasingly adopted by industry for building families of related software systems. A program family offers a set of features (configured options) to control the presence and absence of software functionality. Features in program families are often assigned at compile-time, so their values can only be read at run-time. However, today many program families and application domains demand run-time adaptation, reconfiguration, and post-deployment tuning. Dynamic program families (dynamic software product lines) have emerged as an attempt to handle variability at run-time. Features in dynamic program families can be controlled by ordinary program variables, so reads and writes to them may happen at run-time. Recently, a decision tree lifted domain for analyzing traditional program families with numerical features has been proposed, in which decision nodes contain linear constraints defined over numerical features and leaf nodes contain analysis properties defined over program variables. Decision nodes partition the configuration space of possible feature values, while leaf nodes provide analysis information corresponding to each partition of the configuration space. As features are statically assigned at compile-time, decision nodes can be added, modified, and deleted only when analyzing read accesses of features. In this work, we extend the decision tree lifted domain so that it can be used to efficiently analyze dynamic program families with numerical features. Since features can now be changed at run-time, decision nodes can be modified when handling read and write accesses of feature\u00a0\u2026", "num_citations": "2\n", "authors": ["151"]}
{"title": "Generating attributed variability models for transfer learning\n", "abstract": " Modern software systems often provide configuration options for customizing of the system's functional and non-functional properties, such as response time and energy consumption. The valid configurations of a software system are commonly documented in a variability model. Supporting the optimization of a system's non-functional properties, variability models have been extended with attributes that represent the influence of one or multiple options on a property. The concrete values of attributes are typically determined only in a single environment (eg, for a specific software version, a certain workload, and a specific hardware setup) and are applicable only for this context. Changing the environment, attribute values need to be updated. Instead of determining all attributes from scratch with new measurements, recent approaches rely on transfer learning to reduce the effort of obtaining new attribute values\u00a0\u2026", "num_citations": "2\n", "authors": ["151"]}
{"title": "Energy and Performance Evolution of Configurable Systems: Case Studies and Experiments\n", "abstract": " Contemporary software systems are often highly configurable and additionally they change over time (evolve). These changes do not only affect the functionality but also non-functional properties such as performance and energy consumption. While performance has always been important and in the focus of optimizations, recently energy consumption is becoming increasingly more relevant. However, reducing energy consumption while maintaining or even improving performance at the same time is not trivial and requires an understanding of the relation between performance and energy consumption and their behaviour with respect to software evolution. Existing studies in this field have either considered only the performance but not the energy consumption or have compared performance and energy consumption without considering the aspect of evolution. In this thesis, we combine these aspects in an exploratory manner by measuring and evaluating the performance and energy consumption of different releases and configurations of four case studies\u2013HSQLDB, Apache httpd, PostgreSQL and libvpx VP8. For the evaluation, we directly compare performance and energy consumption and also investigate influences of specific configuration options on the performance and energy consumption. Additionally, we consider the correlation between performance and energy consumption. We find that there are changes in performance and energy consumption over the course of time and that changes equally affect performance and energy consumption. We are also able to attribute changes to specific configuration options in some cases\u00a0\u2026", "num_citations": "2\n", "authors": ["151"]}
{"title": "On the relation of control-flow and performance feature interactions: a case study\n", "abstract": " Detecting feature interactions is imperative for accurately predicting performance of highly-configurable systems. State-of-the-art performance prediction techniques rely on supervised machine learning for detecting feature interactions, which, in turn, relies on time-consuming performance measurements to obtain training data. By providing information about potentially interacting features, we can reduce the number of required performance measurements and make the overall performance prediction process more time efficient. We expect that information about potentially interacting features can be obtained by analyzing the source code of a highly-configurable system, which is computationally cheaper than performing multiple performance measurements. To this end, we conducted an in-depth qualitative case study on two real-world systems (mbedTLS and SQLite), in which we explored the relation\u00a0\u2026", "num_citations": "2\n", "authors": ["151"]}
{"title": "Configcrusher: White-box performance analysis for configurable systems\n", "abstract": " In configurable software systems, stakeholders are often interested in knowing how configuration options influence the performance of a system to facilitate, for example, the debugging and optimization processes of these systems. There are several black-box approaches to obtain this information, but they usually require a large number of samples to make accurate predictions, whereas the few existing white-box approaches impose limitations on the systems that they can analyze. This paper proposes ConfigCrusher, a white-box performance analysis that exploits several insights of configurable systems. ConfigCrusher employs a static data-flow analysis to identify how configuration options may influence control-flow decisions and instruments code regions corresponding to these decisions to dynamically analyze the influence of configuration options on the regions\u2019 performance. Our evaluation using 10 real-world configurable systems shows that ConfigCrusher is more efficient at building performance models that are similar to or more accurate than current state-of-the-art black-box and white-box approaches. Overall, this paper showcases the benefits and potential of whitebox performance analyses to outperform black-box approaches and provide additional information for analyzing configurable systems.", "num_citations": "2\n", "authors": ["151"]}
{"title": "The new feature interaction challenge\n", "abstract": " Feature interactions are a major threat to software modularity and impair compositional development and reasoning. A feature interaction occurs when the behavior of one feature is affected by the presence of another feature. Usually, interactions cannot be deduced easily from the behaviors of the individual features involved. The feature-interaction problem became a crisis in the telecommunications industry in the 1980s, when the growing number of features led to uncontrolled software complexity and long development cycles. Since then, the problem has been recognized in many other domains---each with its own manifestations, challenges, and possible solutions.", "num_citations": "2\n", "authors": ["151"]}
{"title": "Do colors improve program comprehension in the# ifdef hell\n", "abstract": " To implement variability in software product lines, practitioners often use preprocessors. However, preprocessors bear threats to program comprehension, which can lead to high maintenance costs. We evaluate whether using colors to highlight variable code instead of the common textual# ifdef directives can improve program comprehension. In a controlled experiment, we show that depending on the task colors can have both positive and negative effects. If variable code should be located, colors can speed up the process by up to 43%. When actually fixing a bug, colors have at best no effect and can even slow down the comprehension process significantly. These empirical insights provide first steps toward designing a next generation of variability-aware source code editors.", "num_citations": "2\n", "authors": ["151"]}
{"title": "Einfluss erweiterter Programmier-Paradigmen auf die Entwicklung eingebetteter DBMS.\n", "abstract": " Die Unterst\u00fctzung der Entwicklung eingebetteter, variabler Systeme durch die Software-Technik ist derzeit problembehaftet. Paradigmen wie die Objekt-Orientierte Programmierung (OOP) erfordern zur Entwicklung angepasster Software stetige Neuentwicklungen. Weiterf\u00fchrende Techniken l\u00f6sen einige der Probleme der OOP. In Vorbereitung des DFG-Projekts FAME werden diese Techniken vorgestellt und analysiert. Generische Programmierung (GP) verbessert die Wiederverwendbarkeit von Software. Aspekt-Orientierte Programmierung (AOP) erh\u00f6ht die Wartbarkeit durch die Modularisierung von Belangen. Feature-Orientierte Programmierung (FOP) verbessert die Erweiterbarkeit. Nachfolgend werden erste Ergebnisse einer systematischen Untersuchung der Paradigmen zur Erstellung konfigurierbarer, performanter DBMS im Bereich der eingebetteten Systeme pr\u00e4sentiert. Untersucht wird insbesondere der Einfluss der verschiedenen Techniken auf w\u00fcnschenswerte Eigenschaften des Zielsystems. Weiterf\u00fchrend wird eine gezielte Kombination von Techniken vorgeschlagen um Vorteile einzelner Techniken zu vereinen.", "num_citations": "2\n", "authors": ["151"]}
{"title": "VEER: Disagreement-Free Multi-objective Configuration\n", "abstract": " Software comes with many configuration options, satisfying varying needs from users. Exploring those options for non-functional requirements can be tedious, time-consuming, and even error-prone (if done manually). Worse, many software systems can be tuned to multiple objectives (e.g., faster response time, fewer memory requirements, decreased network traffic, decreased energy consumption, etc.). Learning how to adjust the system among these multiple objectives is complicated due to the trade-off among objectives; i.e., things that seem useful to achieve one objective could be detrimental to another objective. Consequentially, the optimizer built for one objective may have different (or even opposite) insights on how to locate good solutions from the optimizer built from another objective. In this paper, we define this scenario as the model disagreement problem. One possible solution to this problem is to find a one-dimensional approximation to the N-objective space. In this way, the case is converted to a single-objective optimization, which is naturally confusion-free. This paper demonstrates VEER, a tool that builds such an approximation by combining our dimensionality-reduction heuristic on top of one of the state-of-the-art optimizers, FLASH. VEER can explore very large configuration spaces by evaluating just a small fraction of the total number of configurations (e.g., a space of 81,000 configurations can be explored by 70 samples). The experimental result in this paper demonstrates the feasibility of our approach in terms of the on-par quality of the solution set generated by the optimizer and the resolved model disagreement within the\u00a0\u2026", "num_citations": "1\n", "authors": ["151"]}
{"title": "A decision tree lifted domain for analyzing program families with numerical features (extended version)\n", "abstract": " Lifted (family-based) static analysis by abstract interpretation is capable of analyzing all variants of a program family simultaneously, in a single run without generating any of the variants explicitly. The elements of the underlying lifted analysis domain are tuples, which maintain one property per variant. Still, explicit property enumeration in tuples, one by one for all variants, immediately yields combinatorial explosion. This is particularly apparent in the case of program families that, apart from Boolean features, contain also numerical features with big domains, thus admitting astronomic configuration spaces. The key for an efficient lifted analysis is proper handling of variability-specific constructs of the language (e.g., feature-based runtime tests and #if directives). In this work, we introduce a new symbolic representation of the lifted abstract domain that can efficiently analyze program families with numerical features. This makes sharing between property elements corresponding to different variants explicitly possible. The elements of the new lifted domain are constraint-based decision trees, where decision nodes are labeled with linear constraints defined over numerical features and the leaf nodes belong to an existing single-program analysis domain. To illustrate the potential of this representation, we have implemented an experimental lifted static analyzer, called SPLNUM^2Analyzer, for inferring invariants of C programs. It uses existing numerical domains (e.g., intervals, octagons, polyhedra) from the APRON library as parameters. An empirical evaluation on benchmarks from SV-COMP and BusyBox yields promising preliminary results indicating\u00a0\u2026", "num_citations": "1\n", "authors": ["151"]}
{"title": "Recommending Unnecessary Source Code Based on Static Analysis\n", "abstract": " Grown software systems often contain code that is not necessary anymore. Unnecessary code wastes resources during development and maintenance, for example, when preparing code for migration or certification. Running a profiler may reveal code that is not used in production, but it is often time-consuming to obtain representative data this way. We investigate to what extent a static analysis approach which is based on code stability and code centrality, is able to identify unnecessary code and whether its recommendations are relevant in practice. To study the feasibility and usefulness of our static approach, we conducted a study involving 14 open-source and closed-source software systems. As there is no perfect oracle for unnecessary code, we compared recommendations of our approach with historical cleanup actions, runtime usage data, and feedback from 25 developers of 5 software projects. Our study\u00a0\u2026", "num_citations": "1\n", "authors": ["151"]}
{"title": "Performance Prediction in the Presence of Feature Interactions.\n", "abstract": " 1 Introduction. Customizable programs and program families provide user-selectable features allowing users to tailor the programs to the application scenario. Beside functional requirements, users are often interested in non-functional requirements, such as a binary-size limit, a minimized energy consumption, and a maximum response time. To tailor a program to non-functional requirements, we have to know in advance which feature selection, that is, configuration, affects which non-functional properties. Due to the combinatorial explosion of possible feature selections, a direct measurement of all of them is infeasible.In our work, we aim at predicting a configuration\u2019s non-functional properties for a specific workload based on the user-selected features [SRK+11, SRK+13]. To this end, we quantify the influence of each selected feature on a non-functional property to compute the properties of a specific configuration. Here, we concentrate on performance only. Unfortunately, the accuracy of performance predictions may be low when considering features only in isolation, because many factors influence performance. Usually, a property is program-wide: it emerges from the presence and interplay of multiple features. For example, database performance depends on whether a search index or encryption is used and how both features interplay. If we knew how the combined presence of two features influences performance, we could predict a configuration\u2019s performance more accurately. Two features interact (ie, cause a performance interaction) if their simultaneous presence in a configuration leads to an unexpected performance, whereas their\u00a0\u2026", "num_citations": "1\n", "authors": ["151"]}
{"title": "LJAR: A Model of Refactoring Physically and Virtually Separated Features\n", "abstract": " Physical separation with class refinements and method refinements \u00e0 la AHEAD and virtual separation using annotations \u00e0 la# ifdef or CIDE are two competing groups of implementation approaches for software product lines with complementary advantages. Although both groups have been mainly discussed in isolation, we strive for an integration to leverage the respective advantages. In this paper, we provide the basis for such an integration by providing a model that supports both, physical and virtual separation, and by describing refactorings in both directions. We prove the refactorings complete, such that every virtually separated product line can be automatically transformed into a physically separated one (replacing annotations by refinements) and vice versa. To demonstrate the feasibility of our approach, we have implemented the refactorings in our tool CIDE and conducted four case studies.", "num_citations": "1\n", "authors": ["151"]}
{"title": "Die Rolle von Features und Aspekten in der Softwareentwicklung (The Role of Features and Aspects in Software Development)\n", "abstract": " Feature-orientierte Programmierung (FOP) und Aspekt-orientierte Programmierung (AOP) sind komplement\u00e4re Technologien. Obwohl beide auf die Modularit\u00e4t von so genannten querschneidenden Belangen abzielen, so tun sie dies auf unterschiedliche Art und Weise. Im Rahmen der Arbeit wurde beobachtet, dass FOP und AOP kombiniert werden k\u00f6nnen, um ihre individuellen Schw\u00e4chen zu \u00fcberwinden. Die Arbeit schl\u00e4gt mit Aspekt-basierten Featuremodulen und Aspektverfeinerung zwei Techniken zur Symbiose von FOP und AOP vor. Beide Techniken werden in einer Fallstudie evaluiert und entsprechende Programmierrichtlinien zum Einsatz von FOP und AOP werden abgeleitet. Schlussendlich wird mittels der Analyse von acht AspectJ-Programmen unterschiedlicher Gr\u00f6\u00dfe die Frage beantwortet, wie Implementierungsmechanismen der FOP und der AOP heutzutage Verwendung finden.", "num_citations": "1\n", "authors": ["151"]}
{"title": "Aspect Refinement and Bounded Quantification in Incremental Designs\n", "abstract": " This article investigates aspects in the context of the incremental software development, ie software product lines. Specifically, we propose the integration of aspects into AHEAD, an architectural model for feature-based product line development. We introduce the notion of aspect refinement based on Aspectual Mixin Layers, a novel technique for implementing features. Aspect refinement enables a programmer to evolve aspects over several product line development stages. This is novel since common AOP approaches do not have such an architectural model. We realize the idea of aspect refinement by introducing mixin-based inheritance to aspects. Furthermore, we propose a bounded aspect quantification that reduces the complexity and unpredictability of aspects in incremental software development. Our novel bounding mechanism exploits the natural order of the layered architecture introduced by the concept of aspect refinement. Aspect refinement and bounded aspect quantification improve the incremental development of product lines using AOP techniques.", "num_citations": "1\n", "authors": ["151"]}
{"title": "Using Mixins to Build a Flexible Lightweight Middleware for Ubiquitous Computing\n", "abstract": " Ubiquitous computing is a challenge for the design of middleware. The reasons are resource constraints, mobility, heterogeneity, etc., just to name a few. We argue that such middleware has to be tailored to the application scenario as well as to the target platform. Such tailormade middleware has to be be built from minimal fine-grained components, and the system structure must be highly configurable, as we will explain. We propose to use the well-known mixin layer approach to build the flexible lightweight middleware envisioned. We show that the thoughtful use of mixin layers is promising in this specific domain and allows to deal with issues such as device heterogeneity and resource constraints. To do so, we present the design and implementation of a middleware and three configurations derived from it. Our evaluation criteria are the number of supported features and the memory footprint. The middleware configurations derived perform well in these respects.", "num_citations": "1\n", "authors": ["151"]}
{"title": "Komponenten einer Middleware-Plattform f\u00fcr mobile Informationssysteme.\n", "abstract": " Leistungsfahige mobile Gerate wie z. B. Mobiltelefone, Smartphones oder PDAs (Personal Digital Assistant) sind aus dem taglichen Leben nicht mehr wegzudenken. Moderne Konzepte wie ubiquitous und pervasive computing beschreiben eine neue Qualitat der Verfugbarkeit von Informationen an jedem Ort zu jeder Zeit mit beliebigen Geraten. Bereits jetzt stehen mit GSM/GPRS und WLAN moderne Kommunikationsverfahren zur Verfugung, die den Zugang zu Informationen nahezu fl\u00e4hendeckend ermoglichen. Das leistungsfahigere breitbandige UMTS wird in Zukunft (2004/2005) GSM/GPRSuber kurz oder lang ersetzen. Neben der technischen Moglichkeit zwischen Geraten Informationen auszutauschen, ist jedoch auch eine Integration von Anwendungen und Diensten gerateubergreifend erforderlich. Ein probater Ansatz zu solch einer Integration ist eine Middleware-Plattform, welche alle dafur notigen\u00a0\u2026", "num_citations": "1\n", "authors": ["151"]}