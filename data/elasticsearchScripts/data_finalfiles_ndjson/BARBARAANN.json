{"title": "Guidelines for performing systematic literature reviews in software engineering\n", "abstract": " CiteSeerX \u0393\u00c7\u00f6 Guidelines for performing Systematic Literature Reviews in Software Engineering Documents Authors Tables Log in Sign up MetaCart DMCA Donate CiteSeerX logo Documents: Advanced Search Include Citations Authors: Advanced Search Include Citations Tables: DMCA Guidelines for performing Systematic Literature Reviews in Software Engineering (2007) Cached Download as a PDF Download Links [www.elsevier.com] Save to List Add to Collection Correct Errors Monitor Changes by B. Kitchenham , S Charters Citations: 259 - 10 self Summary Citations Active Bibliography Co-citation Clustered Documents Version History Share Facebook Twitter Reddit Bibsonomy OpenURL Abstract Keyphrases software engineering systematic literature review Powered by: Apache Solr About CiteSeerX Submit and Index Documents Privacy Policy Help Data Source Contact Us Developed at and hosted by The \u0393\u00c7\u00aa", "num_citations": "7478\n", "authors": ["13"]}
{"title": "Procedures for performing systematic reviews\n", "abstract": " The objective of this report is to propose a guideline for systematic reviews appropriate for software engineering researchers, including PhD students. A systematic review is a means of evaluating and interpreting all available research relevant to a particular research question, topic area, or phenomenon of interest. Systematic reviews aim to present a fair evaluation of a research topic by using a trustworthy, rigorous, and auditable methodology.", "num_citations": "5969\n", "authors": ["13"]}
{"title": "Systematic literature reviews in software engineering\u0393\u00c7\u00f4a systematic literature review\n", "abstract": " BackgroundIn 2004 the concept of evidence-based software engineering (EBSE) was introduced at the ICSE04 conference.AimsThis study assesses the impact of systematic literature reviews (SLRs) which are the recommended EBSE method for aggregating evidence.MethodWe used the standard systematic literature review method employing a manual search of 10 journals and 4 conference proceedings.ResultsOf 20 relevant studies, eight addressed research trends rather than technique evaluation. Seven SLRs addressed cost estimation. The quality of SLRs was fair with only three scoring less than 2 out of 4.ConclusionsCurrently, the topic areas covered by SLRs are limited. European researchers, particularly those at the Simula Laboratory appear to be the leading exponents of systematic literature reviews. The series of cost estimation SLRs demonstrate the potential value of EBSE for synthesising evidence\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "2987\n", "authors": ["13"]}
{"title": "Lessons from applying the systematic literature review process within the software engineering domain\n", "abstract": " A consequence of the growing number of empirical studies in software engineering is the need to adopt systematic approaches to assessing and aggregating research outcomes in order to provide a balanced and objective summary of research evidence for a particular topic. The paper reports experiences with applying one such approach, the practice of systematic literature review, to the published studies relevant to topics within the software engineering domain. The systematic literature review process is summarised, a number of reviews being undertaken by the authors and others are described and some lessons about the applicability of this practice to software engineering are extracted.The basic systematic literature review process seems appropriate to software engineering and the preparation and validation of a review protocol in advance of a review activity is especially valuable. The paper highlights\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "2109\n", "authors": ["13"]}
{"title": "Preliminary guidelines for empirical research in software engineering\n", "abstract": " Empirical software engineering research needs research guidelines to improve the research and reporting processes. We propose a preliminary set of research guidelines aimed at stimulating discussion among software researchers. They are based on a review of research guidelines developed for medical researchers and on our own experience in doing and reviewing software engineering research. The guidelines are intended to assist researchers, reviewers, and meta-analysts in designing, conducting, and evaluating empirical studies. Editorial boards of software engineering journals may wish to use our recommendations as a basis for developing guidelines for reviewers and for framing policies for dealing with the design, data collection, and analysis and reporting of empirical studies.", "num_citations": "1847\n", "authors": ["13"]}
{"title": "Evidence-based software engineering\n", "abstract": " Our objective is to describe how software engineering might benefit from an evidence-based approach and to identify the potential difficulties associated with the approach. We compared the organisation and technical infrastructure supporting evidence-based medicine (EBM) with the situation in software engineering. We considered the impact that factors peculiar to software engineering (i.e. the skill factor and the lifecycle factor) would have on our ability to practice evidence-based software engineering (EBSE). EBSE promises a number of benefits by encouraging integration of research results with a view to supporting the needs of many different stakeholder groups. However, we do not currently have the infrastructure needed for widespread adoption of EBSE. The skill factor means software engineering experiments are vulnerable to subject and experimenter bias. The lifecycle factor means it is difficult to\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1498\n", "authors": ["13"]}
{"title": "Software quality: the elusive target [special issues section]\n", "abstract": " If you are a software developer, manager, or maintainer, quality is often on your mind. But what do you really mean by software quality? Is your definition adequate? Is the software you produce better or worse than you would like it to be? We put software quality on trial, examining both the definition and evaluation of our software products and processes.", "num_citations": "911\n", "authors": ["13"]}
{"title": "Systematic literature reviews in software engineering\u0393\u00c7\u00f4a tertiary study\n", "abstract": " ContextIn a previous study, we reported on a systematic literature review (SLR), based on a manual search of 13 journals and conferences undertaken in the period 1st January 2004 to 30th June 2007.ObjectiveThe aim of this on-going research is to provide an annotated catalogue of SLRs available to software engineering researchers and practitioners. This study updates our previous study using a broad automated search.MethodWe performed a broad automated search to find SLRs published in the time period 1st January 2004 to 30th June 2008. We contrast the number, quality and source of these SLRs with SLRs found in the original study.ResultsOur broad search found an additional 35 SLRs corresponding to 33 unique studies. Of these papers, 17 appeared relevant to the undergraduate educational curriculum and 12 appeared of possible interest to practitioners. The number of SLRs being published is\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "879\n", "authors": ["13"]}
{"title": "Does the technology acceptance model predict actual use? A systematic literature review\n", "abstract": " ContextThe technology acceptance model (TAM) was proposed in 1989 as a means of predicting technology usage. However, it is usually validated by using a measure of behavioural intention to use (BI) rather than actual usage.ObjectiveThis review examines the evidence that the TAM predicts actual usage using both subjective and objective measures of actual usage.MethodWe performed a systematic literature review based on a search of six digital libraries, along with vote-counting meta-analysis to analyse the overall results.ResultsThe search identified 79 relevant empirical studies in 73 articles. The results show that BI is likely to be correlated with actual usage. However, the TAM variables perceived ease of use (PEU) and perceived usefulness (PU) are less likely to be correlated with actual usage.ConclusionCare should be taken using the TAM outside the context in which it has been validated.", "num_citations": "877\n", "authors": ["13"]}
{"title": "Case studies for method and tool evaluation\n", "abstract": " Case studies help industry evaluate the benefits of methods and tools and provide a cost-effective way to ensure that process changes provide the desired results. However, unlike formal experiments and surveys, case studies do not have a well-understood theoretical basis. This article provides guidelines for organizing and analyzing case studies so that they produce meaningful results.< >", "num_citations": "815\n", "authors": ["13"]}
{"title": "Towards a framework for software measurement validation\n", "abstract": " In this paper we propose a framework for validating software measurement. We start by defining a measurement structure model that identifies the elementary component of measures and the measurement process, and then consider five other models involved in measurement: unit definition models, instrumentation models, attribute relationship models, measurement protocols and entity population models. We consider a number of measures from the viewpoint of our measurement validation framework and identify a number of shortcomings; in particular we identify a number of problems with the construction of function points. We also compare our view of measurement validation with ideas presented by other researchers and identify a number of areas of disagreement. Finally, we suggest several rules that practitioners and researchers can use to avoid measurement problems, including the use of measurement\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "811\n", "authors": ["13"]}
{"title": "A systematic review of systematic review process research in software engineering\n", "abstract": " ContextMany researchers adopting systematic reviews (SRs) have also published papers discussing problems with the SR methodology and suggestions for improving it. Since guidelines for SRs in software engineering (SE) were last updated in 2007, we believe it is time to investigate whether the guidelines need to be amended in the light of recent research.ObjectiveTo identify, evaluate and synthesize research published by software engineering researchers concerning their experiences of performing SRs and their proposals for improving the SR process.MethodWe undertook a systematic review of papers reporting experiences of undertaking SRs and/or discussing techniques that could be used to improve the SR process. Studies were classified with respect to the stage in the SR process they addressed, whether they related to education or problems faced by novices and whether they proposed the use of\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "606\n", "authors": ["13"]}
{"title": "Using mapping studies as the basis for further research\u0393\u00c7\u00f4a participant-observer case study\n", "abstract": " ContextWe are strong advocates of evidence-based software engineering (EBSE) in general and systematic literature reviews (SLRs) in particular. We believe it is essential that the SLR methodology is used constructively to support software engineering research.ObjectiveThis study aims to assess the value of mapping studies which are a form of SLR that aims to identify and categorise the available research on a broad software engineering topic.MethodWe used a multi-case, participant-observer case study using five examples of studies that were based on preceding mapping studies. We also validated our results by contacting two other researchers who had undertaken studies based on preceding mapping studies and by assessing review comments related to our follow-on studies.ResultsOur original case study identified 11 unique benefits that can accrue from basing research on a preceding mapping study of\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "601\n", "authors": ["13"]}
{"title": "A simulation study of the model evaluation criterion MMRE\n", "abstract": " The mean magnitude of relative error, MMRE, is probably the most widely used evaluation criterion for assessing the performance of competing software prediction models. One purpose of MMRE is to assist us to select the best model. In this paper, we have performed a simulation study demonstrating that MMRE does not always select the best model. Our findings cast some doubt on the conclusions of any study of competing software prediction models that use MMRE as a basis of model comparison. We therefore recommend not using MMRE to evaluate and compare prediction models. At present, we do not have any universal replacement for MMRE. Meanwhile, we therefore recommend using a combination of theoretical justification of the models that are proposed together with other metrics proposed in this paper.", "num_citations": "536\n", "authors": ["13"]}
{"title": "What accuracy statistics really measure [software estimation]\n", "abstract": " Provides the software estimation research community with a better understanding of the meaning of, and relationship between, two statistics that are often used to assess the accuracy of predictive models: the mean magnitude relative error (MMRE) and the number of predictions within 25% of the actual, pred(25). It is demonstrated that MMRE and pred(25) are, respectively, measures of the spread and the kurtosis of the variable z, where z=estimate/actual. Thus, z is considered to be a measure of accuracy, and statistics such as MMRE and pred(25) to be measures of properties of the distribution of z. It is suggested that measures of the central location and skewness of z, as well as measures of spread and kurtosis, are necessary. Furthermore, since the distribution of z is non-normal, non-parametric measures of these properties may be needed. For this reason, box-plots of z are useful alternatives to simple summary\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "520\n", "authors": ["13"]}
{"title": "Principles of survey research: part 5: populations and samples\n", "abstract": " This article is the fifth installment of our series of articles on survey research. In it, we discuss what we mean by a population and a sample and the implications of each for survey research. We provide examples of correct and incorrect sampling techniques used in software engineering surveys.", "num_citations": "465\n", "authors": ["13"]}
{"title": "Personal opinion surveys\n", "abstract": " Although surveys are an extremely common research method, surveybased research is not an easy option. In this chapter, we use examples of three software engineering surveys to illustrate the advantages and pitfalls of using surveys. We discuss the six most important stages in survey-based research: setting the survey\u0393\u00c7\u00d6s objectives; selecting the most appropriate survey design; constructing the survey instrument (concentrating on self-administered questionnaires); assessing the reliability and validity of the survey instrument; administering the instrument; and, finally, analysing the collected data. This chapter provides only an introduction to survey-based research; readers should consult the referenced literature for more detailed advice.", "num_citations": "445\n", "authors": ["13"]}
{"title": "Evidence-based software engineering and systematic reviews\n", "abstract": " In the decade since the idea of adapting the evidence-based paradigm for software engineering was first proposed, it has become a major tool of empirical software engineering. Evidence-Based Software Engineering and Systematic Reviews provides a clear introduction to the use of an evidence-based model for software engineering research and practice.", "num_citations": "431\n", "authors": ["13"]}
{"title": "Cross versus within-company cost estimation studies: A systematic review\n", "abstract": " The objective of this paper is to determine under what circumstances individual organizations would be able to rely on cross-company-based estimation models. We performed a systematic review of studies that compared predictions from cross-company models with predictions from within-company models based on analysis of project data. Ten papers compared cross-company and within-company estimation models; however, only seven presented independent results. Of those seven, three found that cross-company models were not significantly different from within-company models, and four found that cross-company models were significantly worse than within-company models. Experimental procedures used by the studies differed making it impossible to undertake formal meta-analysis of the results. The main trend distinguishing study results was that studies with small within-company data sets (i.e., $20\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "416\n", "authors": ["13"]}
{"title": "Effort estimation using analogy\n", "abstract": " The staff resources or effort required for a software project are notoriously difficult to estimate in advance. To date most work has focused upon algorithmic cost models such as COCOMO and Function Points. These can suffer from the disadvantage of the need to calibrate the model to each individual measurement environment coupled with very variable accuracy levels even after calibration. An alternative approach is to use analogy for estimation. We demonstrate that this method has considerable promise in that we show it to out perform traditional algorithmic methods for six different datasets. A disadvantage of estimation by analogy is that it requires a considerable amount of computation. The paper describes an automated environment known as ANGEL that supports the collection, storage and identification of the most analogous projects in order to estimate the effort for a new project. ANGEL is based upon the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "407\n", "authors": ["13"]}
{"title": "Using Mapping Studies in Software Engineering.\n", "abstract": " Background: A mapping study provides a systematic and objective procedure for identifying the nature and extent of the empirical study data that is available to answer a particular research question. Such studies can also form a useful preliminary step for PhD study.Aim: We set out to assess how effective such studies have been when used for software engineering topics, and to identify the specific challenges that they present.Method: We have conducted an informal review of a number of mapping studies in software engineering, describing their main characteristics and the forms of analysis employed. Results: We examine the experiences and outcomes from six mapping studies, of which four are published. From these we note a recurring theme about the problems of classification and a preponderance of \u0393\u00c7\u00ffgaps\u0393\u00c7\u00d6 in the set of empirical studies.Conclusions: We identify our challenges as improving classification guidelines, encouraging better reporting of primary studies, and argue for identifying some\u0393\u00c7\u00d6empirical grand challenges\u0393\u00c7\u00d6 for software engineering as a focus for the community1.", "num_citations": "404\n", "authors": ["13"]}
{"title": "Principles of survey research: part 1: turning lemons into lemonade\n", "abstract": " Surveys are probably the most commonly-used research method world-wide. Survey work is visible not only because we see many examples of it in software engineering research, but also because we are often asked to participate in surveys in our private capacity, as electors, consumers, or service users. This widespread use of surveys may give us the impression that surveybased research is straightforward, an easy option for researchers to gather important information about products, context, processes, workers and more. In our personal experience with applying and evaluating research methods and their results, we certainly did not expect to encounter major problems with a survey that we planned, to investigate issues associated with technology adoption. This article and subsequent ones in this series describe how wrong we were. We do not want to give the impression that there is any way of turning a bad\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "372\n", "authors": ["13"]}
{"title": "Evaluating software engineering methods and tool part 1: The evaluation context and evaluation methods\n", "abstract": " In the last five issues of SIGSOFT Notes, Shari Lawrence Pfleeger has discussed the use of formal experiments to evaluate software engineering methods and tools [1]. Shari's articles were based on work she performed for the U.K. DESMET project which aimed to develop a methodology for evaluating software engineering methods and tools.The DESMET project identified a number of useful evaluation methods in addition to formal experiments, and Shari asked me to continue this column by describing some of other methods. As a starting point, I will give an overview of the scope of the DESMET methodology in this article and describe the nine different evaluation methods DESMET identified. In the next few articles I will discuss criteria for selecting a specific method in particular circumstances. Later I will present the DESMET guidelines for performing quantitative case studies and feature analysis.", "num_citations": "346\n", "authors": ["13"]}
{"title": "What\u0393\u00c7\u00d6s up with software metrics?\u0393\u00c7\u00f4A preliminary mapping study\n", "abstract": " BackgroundMany papers are published on the topic of software metrics but it is difficult to assess the current status of metrics research.AimThis paper aims to identify trends in influential software metrics papers and assess the possibility of using secondary studies to integrate research results.MethodSearch facilities in the SCOPUS tool were used to identify the most cited papers in the years 2000\u0393\u00c7\u00f42005 inclusive. Less cited papers were also selected from 2005. The selected papers were classified according factors such as to main topic, goal and type (empirical or theoretical or mixed). Papers classified as \u0393\u00c7\u00a3Evaluation studies\u0393\u00c7\u00a5 were assessed to investigate the extent to which results could be synthesized.ResultsCompared with less cited papers, the most cited papers were more frequently journal papers, and empirical validation or data analysis studies. However, there were problems with some empirical validation\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "313\n", "authors": ["13"]}
{"title": "An empirical study of maintenance and development estimation accuracy\n", "abstract": " We analyzed data from 145 maintenance and development projects managed by a single outsourcing company, including effort and duration estimates, effort and duration actuals, and function points counts. The estimates were made as part of the company\u0393\u00c7\u00d6s standard project estimating process that involved producing two or more estimates for each project and selecting one estimate to be the basis of client-agreed budgets. We found that effort estimates chosen as a basis for project budgets were, in general, reasonably good, with 63% of the estimates being within 25% of the actual value, and an average absolute error of 0.26. These estimates were significantly better than regression estimates based on adjusted function points, although the function point models were based on a homogeneous subset of the full data set, and we allowed for the fact that the model parameters changed over time. Furthermore, there\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "290\n", "authors": ["13"]}
{"title": "DESMET: a methodology for evaluating software engineering methods and tools\n", "abstract": " DESMET was a DTI-backed project with the goal of developing and validating a methodology for evaluating software engineering methods and tools. The project identified nine methods of evaluation and a set of criteria to help evaluators select an appropriate method. Detailed guidelines were developed for three important evaluation methods: formal experiments, quantitative case studies and feature analysis evaluations. This article describes the way the DESMET project used the DESMET methodology both to evaluate the methodology itself and to provide direct assistance to the commercial organisations using it.", "num_citations": "276\n", "authors": ["13"]}
{"title": "Principles of survey research part 4: questionnaire evaluation\n", "abstract": " This article discusses how to avoid biased questions in survey instruments, how to motivate people to complete instruments and how to evaluate instruments. In the context of survey evaluation, we discuss how to assess survey reliability i.e. how reproducible a survey's data is and survey validity i.e. how well a survey instrument measures what it sets out to measure.", "num_citations": "262\n", "authors": ["13"]}
{"title": "Principles of survey research: part 3: constructing a survey instrument\n", "abstract": " In this article, we discuss how to construct a questionnaire. We point out the need to use any previous research results to reduce the overheads of survey construction. We identify a number of issues to consider when selecting questions, constructing questions, deciding on the type of question and finalizing the format of the questionnaire.", "num_citations": "257\n", "authors": ["13"]}
{"title": "Principles of survey research part 2: designing a survey\n", "abstract": " This second article of our series looks at the process of designing a survey. The design process begins with reviewing the objectives, examining the target population identified by the objectives, and deciding how best to obtain the information needed to address those objectives. However, we also need to consider factors such as determining the appropriate sample size and ensuring the largest possible response rate.To illustrate our ideas, we use the three surveys described in Part 1 of this series to suggest good and bad practice in software engineering survey research.", "num_citations": "240\n", "authors": ["13"]}
{"title": "Principles of survey research part 6: data analysis\n", "abstract": " This article is the last of our series of articles on survey research. In it, we discuss how to analyze survey data. We provide examples of correct and incorrect analysis techniques used in software engineering surveys.", "num_citations": "236\n", "authors": ["13"]}
{"title": "Towards a constructive quality model. Part 1: Software quality modelling, measurement and prediction\n", "abstract": " This paper considers the approach taken by the ESPRIT-funded REQUEST project to measuring, modelling and predicting software quality. The paper describes previous work on defining software quality and indicates how the work has been used by REQUEST to begin the formulation of a constructive quality model (COQUAMO). The paper concludes that the original goal of the project to produce a predictive quality model was unrealistic, but that a quality system incorporating prediction, analysis and advice is feasible.", "num_citations": "204\n", "authors": ["13"]}
{"title": "Combining empirical results in software engineering\n", "abstract": " In this paper we investigate the techniques used in medical research to combine results from independent empirical studies of a particular phenomenon: meta-analysis and vote-counting.We use an example to illustrate the benefits and limitations of each technique and to indicate the criteria that should be used to guide your choice of technique. Meta-analysis is appropriate for homogeneous studies when raw data or quantitative summary information, e.g. correlation coefficient, are available. It can also be used for heterogeneous studies where the cause of the heterogeneity is due to well-understood partitions in the subject population. In other circumstances, meta-analysis is usually invalid. Although intuitively appealing, vote-counting has a number of serious limitations and should usually be avoided.We suggest that combining study results is unlikely to solve all the problems encountered in empirical software\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "197\n", "authors": ["13"]}
{"title": "Using a protocol template for case study planning\n", "abstract": " In order to undertake a series of case studies aimed at investigating systematic literature reviews, we have developed a case study protocol template. This paper introduces the template and discusses our experiences of using the template and the resulting case study protocol. We suggest that using our template to prepare a protocol and using the protocol for case study planning can improve the rigour of software engineering case studies.", "num_citations": "188\n", "authors": ["13"]}
{"title": "The use and usefulness of the ISO/IEC 9126 quality standard\n", "abstract": " This paper reports an evaluation the utility of ISO/IEC 9126. ISO/IEC 9126 is an international standard intended to ensure the quality of all software-intensive products including safety-critical systems where lives are at risk if software components fail. Our evaluation exercise arose from an experiment that required a quality assessment of outputs of the design process. Although ISO/IEC 9126 is intended to support evaluation of intermediate software products, both the experimental subjects (158 final year computer science and engineering student) and experimenters found the standard was ambiguous in meaning, incomplete with respect to quality characteristics and overlapping with respect to measured properties. We conclude that ISO/IEC 9126 is not suitable for measuring design quality of software products. This casts serious doubts as to the validity of the standard as a whole.", "num_citations": "184\n", "authors": ["13"]}
{"title": "Modeling software measurement data\n", "abstract": " This paper proposes a method for specifying models of software data sets in order to capture the definitions and relationships among software measures. We believe a method of defining software data sets is necessary to ensure that software data are trustworthy. Software companies introducing a measurement program need to establish procedures to collect and store trustworthy measurement data. Without appropriate definitions it is difficult to ensure data values are repeatable and comparable. Software metrics researchers need to maintain collections of software data sets. Such collections allow researchers to assess the generality of software engineering phenomena. Without appropriate safeguards, it is difficult to ensure that data from different sources are analyzed correctly. These issues imply the need for a standard method of specifying software data sets so they are fully documented and can be exchanged\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "183\n", "authors": ["13"]}
{"title": "Status report on software measurement\n", "abstract": " The most successful software measurement programs are ones in which researcher, practitioner, and customer work hand in hand to meet goals and solve problems. But such collaboration is rare. The authors explore the gaps between these groups and point toward ways to bridge them.", "num_citations": "176\n", "authors": ["13"]}
{"title": "DESMET: A method for evaluating software engineering methods and tools\n", "abstract": " This report describes the results of the DESMET project. The DESMET project was a collaborative project part funded by the UK DTI. Its goal was develop and validate a method for evaluating software engineering methods and tools. This report describes the guidelines developed by DESMET for\u0393\u00c7\u00f3 selecting an appropriate evaluation method;\u0393\u00c7\u00f3 performing a feature analysis;\u0393\u00c7\u00f3 performing a quantitative case study. It also describes some of the management and sociological issues that can influence evaluation exercise. Finally, the report describes briefly the attempt the project made to evaluate the DESMET method itself.", "num_citations": "173\n", "authors": ["13"]}
{"title": "Risks and risk mitigation in global software development: A tertiary study\n", "abstract": " ContextThere is extensive interest in global software development (GSD) which has led to a large number of papers reporting on GSD. A number of systematic literature reviews (SLRs) have attempted to aggregate information from individual studies.ObjectiveWe wish to investigate GSD SLR research with a focus on discovering what research has been conducted in the area and to determine if the SLRs furnish appropriate risk and risk mitigation advice to provide guidance to organizations involved with GSD.MethodWe performed a broad automated search to identify GSD SLRs. Data extracted from each study included: (1) authors, their affiliation and publishing venue, (2) SLR quality, (3) research focus, (4) GSD risks, (5) risk mitigation strategies and, (6) for each SLR the number of primary studies reporting each risk and risk mitigation strategy.ResultsWe found a total of 37 papers reporting 24 unique GSD SLR\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "170\n", "authors": ["13"]}
{"title": "Empirical studies of assumptions that underlie software cost-estimation models\n", "abstract": " The paper reviews some of the assumptions built into conventional cost models and identifies whether or not there is empirical evidence to support these assumptions. The results indicate that the assumption that there is a nonlinear relationship between size and effort is not supported, but the assumption of a nonlinear relationship between effort and duration is. Second, the assumption that a large number of subjective productivity adjustment factors is necessary is not supported. In addition, it also appears that a large number of size adjustment factors are unnecessary. Third, the assumption that staff experience and/or staff capability are the most significant cost drivers (after allowing for the effect of size) is not supported by the data available to the MERMAID project, but neither can it be confirmed from analysis of the COCOMO data set. Finally, the assumption that compression of schedule decreases productivity\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "166\n", "authors": ["13"]}
{"title": "Robust statistical methods for empirical software engineering\n", "abstract": " There have been many changes in statistical theory in the past 30 years, including increased evidence that non-robust methods may fail to detect important results. The statistical advice available to software engineering researchers needs to be updated to address these issues. This paper aims both to explain the new results in the area of robust analysis methods and to provide a large-scale worked example of the new methods. We summarise the results of analyses of the Type 1 error efficiency and power of standard parametric and non-parametric statistical tests when applied to non-normal data sets. We identify parametric and non-parametric methods that are robust to non-normality. We present an analysis of a large-scale software engineering experiment to illustrate their use. We illustrate the use of kernel density plots, and parametric and non-parametric methods using four different software\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "163\n", "authors": ["13"]}
{"title": "Further comparison of cross-company and within-company effort estimation models for web applications\n", "abstract": " This paper extends a previous study, using data on 67 Web projects from the Tukutuku database, investigating to what extent a cross-company cost model can be successfully employed to estimate effort for projects that belong to a single company, where no projects from this company were used to build the cross-company model. Our within-company model employed data on 14 Web projects from a single Web company. Our results were similar to those from the previous study, showing that predictions based on the within-company model were significantly more accurate than those based on the cross-company model. We also found that predictions were very poor when the within-company cost model was used to estimate effort for 53 Web projects from different companies. We analysed the data using two techniques, forward stepwise regression and case-based reasoning. We found estimates produced using\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "160\n", "authors": ["13"]}
{"title": "Why comparative effort prediction studies may be invalid\n", "abstract": " Background: Many cost estimation papers are based on finding a\" new\" estimation method, trying out the method on one or two past datasets and\" proving\" that the new method is better than linear regression. Aim: This paper aims to explain why this approach to model comparison is often invalid and to suggest that the PROMISE repository may be making things worse. Method: We identify some of the theoretical problems with studies that compare different estimation models. We review some of the commonly used datasets from the viewpoint of the reliability of the data and the validity of the proposed linear regression models. Discussion points: It is invalid to select one or two datasets to\" prove\" the validity of a new technique because we cannot be sure that, of the many published datasets, those chosen are the only ones that favour the new technique. When new models are compared with regression models\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "159\n", "authors": ["13"]}
{"title": "Empirical evidence about the UML: a systematic literature review\n", "abstract": " The Unified Modeling Language (UML) was created on the basis of expert opinion and has now become accepted as the \u0393\u00c7\u00ffstandard\u0393\u00c7\u00d6 object\u0393\u00c7\u00c9oriented modelling notation. Our objectives were to determine how widely the notations of the UML, and their usefulness, have been studied empirically, and to identify which aspects of it have been studied in most detail. We undertook a mapping study of the literature to identify relevant empirical studies and to classify them in terms of the aspects of the UML that they studied. We then conducted a systematic literature review, covering empirical studies published up to the end of 2008, based on the main categories identified. We identified 49 relevant publications, and report the aggregated results for those categories for which we had enough papers\u0393\u00c7\u00f6 metrics, comprehension, model quality, methods and tools and adoption. Despite indications that a number of problems exist\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "158\n", "authors": ["13"]}
{"title": "Software productivity measurement using multiple size measures\n", "abstract": " Productivity measures based on a simple ratio of product size to project effort assume that size can be determined as a single measure. If there are many possible size measures in a data set and no obvious model for aggregating the measures into a single measure, we propose using the expression AdjustedSize/Effort to measure productivity. AdjustedSize is defined as the most appropriate regression-based effort estimation model, where all the size measures selected for inclusion in the estimation model have a regression parameter significantly different from zero (p<0.05). This productivity measurement method ensures that each project has an expected productivity value of one. Values between zero and one indicate lower than expected productivity, values greater than one indicate higher than expected productivity. We discuss the assumptions underlying this productivity measurement method and present an\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "155\n", "authors": ["13"]}
{"title": "Software metrics: measurement for software process improvement\n", "abstract": " From the Publisher: Software Metrics explains how software measurement can be used to support software process improvement by providing objective methods of characterizing process capability and evaluating the effect of process changes. The author explains what is meant by software measurement and how to decide what to measure; how to use measurement to support different aspects of a process improvement programme; how to set quantitative goals using a pragmatic approach to the Goal-Question-Metric paradigm; how to set up a metrication programme and design a data collection system; and how to analyse the software data collected. Further, the author provides insight into how software measurement can act as a process improvement agent for quality control and software estimating. This book sets out an approach to the formal validation of measures and the theory of statistical data analysis for\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "154\n", "authors": ["13"]}
{"title": "Guidelines for industrially-based multiple case studies in software engineering\n", "abstract": " Without careful methodological guidance, case studies in software engineering are difficult to plan, design and execute. While there are a number of broad guidelines for case study research, there are none that specifically address the needs of a software engineer undertaking multiple case studies in an industrial setting. Through a synthesis of existing best practices in case study research, we provide a set of comprehensive guidelines for conducting multiple case studies in software engineering research. Our guidelines can assist software engineering researchers with all stages of multiple case study research, although in this paper we concentrate on the early phases, such as focusing the case study and detailed plan design. To date, three exploratory research projects found our guidelines very useful. We illustrate our guidelines with examples from one of these projects.", "num_citations": "151\n", "authors": ["13"]}
{"title": "Estimates, uncertainty, and risk\n", "abstract": " The authors discuss the sources of uncertainty and risk, their implications for software organizations, and how risk and uncertainty can be managed. Specifically, they assert that uncertainty and risk cannot be managed effectively at the individual project level. These factors must be considered in an organizational context.", "num_citations": "151\n", "authors": ["13"]}
{"title": "Quality of service approaches in cloud computing: A systematic mapping study\n", "abstract": " Context: Cloud computing is a new computing technology that provides services to consumers and businesses. Due to the increasing use of these services, the quality of service (QoS) of cloud computing has become an important and essential issue since there are many open challenges which need to be addressed related to trust in cloud services. Many research issues have been proposed in QoS approaches in the cloud computing area.Objective: The aim of this study is to survey current research on QoS approaches in cloud computing in order to identify where more emphasis should be placed in both current and future research directions.Method: A systematic mapping study was performed to find the related literature, and 67 articles were selected as primary studies that are classified in relation to the focus, research type and contribution type.Result: The majority of the articles are of the validation research\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "150\n", "authors": ["13"]}
{"title": "The value of mapping studies\u0393\u00c7\u00f4A participant-observer case study\n", "abstract": " Background: We are strong advocates of evidence-based software engineering (EBSE) in general and systematic literature reviews (SLRs) in particular. We believe it is essential that the SLR methodology is being used constructively to support software engineering research. Aim: This study aims to assess the value of mapping studies which are a form of SLR that aims to identify and categorise the available research on a specific topic. Methods: We use a multi-case, participant observer case study using five examples of studies that were based on preceding mapping studies. Results: We identified 13 unique benefits that can accrue from basing research on a preceding mapping study of which only 2 were case specific. We also identified 9 problems associated with using preceding mapping studies of which two were case specific. Conclusions: Mapping studies can save time and effort for researchers and provide baselines to assist new research efforts. However, they must be of high quality in terms of completeness and rigour if they are to be a reliable basis for follow-on research.", "num_citations": "147\n", "authors": ["13"]}
{"title": "Analogy-X: providing statistical inference to analogy-based software cost estimation\n", "abstract": " Data-intensive analogy has been proposed as a means of software cost estimation as an alternative to other data intensive methods such as linear regression. Unfortunately, there are drawbacks to the method. There is no mechanism to assess its appropriateness for a specific dataset. In addition, heuristic algorithms are necessary to select the best set of variables and identify abnormal project cases. We introduce a solution to these problems based upon the use of the Mantel correlation randomization test called Analogy-X. We use the strength of correlation between the distance matrix of project features and the distance matrix of known effort values of the dataset. The method is demonstrated using the Desharnais dataset and two random datasets, showing (1) the use of Mantel's correlation to identify whether analogy is appropriate, (2) a stepwise procedure for feature selection, as well as (3) the use of a leverage\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "147\n", "authors": ["13"]}
{"title": "A procedure for analyzing unbalanced datasets\n", "abstract": " This paper describes a procedure for analyzing unbalanced datasets that include many nominal- and ordinal-scale factors. Such datasets are often found in company datasets used for benchmarking and productivity assessment. The two major problems caused by lack of balance are that the impact of factors can be concealed and that spurious impacts can be observed. These effects are examined with the help of two small artificial datasets. The paper proposes a method of forward pass residual analysis to analyze such datasets. The analysis procedure is demonstrated on the artificial datasets and then applied to the COCOMO dataset. The paper ends with a discussion of the advantages and limitations of the analysis procedure.", "num_citations": "146\n", "authors": ["13"]}
{"title": "An evaluation of some design metrics\n", "abstract": " The paper attempts to evaluate some software design metrics, using data from a communications system. The design metrics investigated were based on the information flow metrics proposed by Henry and Kafura, and the problems they encountered are discussed. The slightly simpler metrics used in this study are described. The ability of the design metrics to identify change-prone, error-prone and complex programs was contrasted with that of simple code metrics. Although one of the design metrics (informational fan-out) was able to identify change-prone, fault-prone and complex programs, code metrics (i.e. lines of code and number of branches) were better. In this context \u0393\u00c7\u00ffbetter\u0393\u00c7\u00d6 means correctly identifying a larger proportion of change-prone, error-prone and/or complex programs, while maintaining a relatively low false identification rate (i.e. incorrectly identifying a program which did not in fact exhibit any\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "140\n", "authors": ["13"]}
{"title": "Evidence relating to Object-Oriented software design: A survey\n", "abstract": " There is little empirical knowledge of the effectiveness of the object-oriented paradigm. To conduct a systematic review of the literature describing empirical studies of this paradigm. We undertook a Mapping Study of the literature. 138 papers have been identified and classified by topic, form of study involved, and source. The majority of empirical studies of OO concentrate on metrics, relatively few consider effectiveness.", "num_citations": "139\n", "authors": ["13"]}
{"title": "Counterpoint: the problem with function points\n", "abstract": " BARBARA KITCHENHAM Keele University he concept of function points goes a long way toward addressing the genuine need for frontend measures of product size. My concern is that specific types of function points\u0393\u00c7\u00f6such as Albrecht\u0393\u00c7\u00d6s version, 1 which is popular in the US, and Symons Mark II version, 2 which is popular in the UK\u0393\u00c7\u00f6are not the straightforward, simple measures some people imagine.Function points have fundamental flaws in their construction that prevent them from being valid measures. 3 This means there is a danger they will behave in unexpected ways, for example asserting that one program is larger than another when it is not. In most cases, if function points are used with caution within a specific organization, you will probably have few problems. But if you want to use them for cross-company benchmarking, as the basis for development or support contracts between different companies, or to\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "139\n", "authors": ["13"]}
{"title": "A method for software quality planning, control, and evaluation\n", "abstract": " Squid is a method and a tool for quality assurance and a control that allows a software development organization to plan and control product quality during development. The Telescience software development project used it to build a remote monitoring and control system based in Antarctica.", "num_citations": "136\n", "authors": ["13"]}
{"title": "Evaluating guidelines for reporting empirical software engineering studies\n", "abstract": " Background                 Several researchers have criticized the standards of performing and reporting empirical studies in software engineering. In order to address this problem, Jedlitschka and Pfahl have produced reporting guidelines for controlled experiments in software engineering. They pointed out that their guidelines needed evaluation. We agree that guidelines need to be evaluated before they can be widely adopted.                                               Aim                 The aim of this paper is to present the method we used to evaluate the guidelines and report the results of our evaluation exercise. We suggest our evaluation process may be of more general use if reporting guidelines for other types of empirical study are developed.                                               Method                 We used a reading method inspired by perspective-based and checklist-based reviews to perform a theoretical evaluation of the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "134\n", "authors": ["13"]}
{"title": "Inter-item correlations among function points\n", "abstract": " The paper reports on an empirical investigation of Albrecht function points.The study suggests that function points are not well-formed metrics because there is a correlation between their constituent elements. It also suggests that (for the dataset under investigation) two of the constituent elements were as good at predicting effort as the raw function point count and that the unweighted counts can be reasonable predictors of effort.< >", "num_citations": "134\n", "authors": ["13"]}
{"title": "How reliable are systematic reviews in empirical software engineering?\n", "abstract": " BACKGROUND-The systematic review is becoming a more commonly employed research instrument in empirical software engineering. Before undue reliance is placed on the outcomes of such reviews it would seem useful to consider the robustness of the approach in this particular research context. OBJECTIVE-The aim of this study is to assess the reliability of systematic reviews as a research instrument. In particular, we wish to investigate the consistency of process and the stability of outcomes. METHOD-We compare the results of two independent reviews undertaken with a common research question. RESULTS-The two reviews find similar answers to the research question, although the means of arriving at those answers vary. CONCLUSIONS-In addressing a well-bounded research question, groups of researchers with similar domain experience can arrive at the same review outcomes, even though they\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "121\n", "authors": ["13"]}
{"title": "Reflections on 10 years of software process simulation modeling: A systematic review\n", "abstract": " Software process simulation modeling (SPSM) has become an increasingly active research area since its introduction in the late 1980s. Particularly during the last ten years the related research community and the number of publications have been growing. The objective of this research is to provide insights about the evolution of SPSM research during the last 10 years. A systematic literature review was proposed with two subsequent stages to achieve this goal. This paper presents the preliminary results of the first stage of the review that is exclusively focusing on a core set of publication sources. More than 200 relevant publications were analyzed in order to find answers to the research questions, including the purposes and scopes of SPSM, application domains, and predominant research issues. From the analysis the following conclusions could be drawn: (1) Categories for classifying software process\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "121\n", "authors": ["13"]}
{"title": "A systematic review of cross-vs. within-company cost estimation studies\n", "abstract": " OBJECTIVE  \u0393\u00c7\u00f4 The objective of this paper is to determine under what circumstances individual organisations would be able to rely on cross-company based estimation models. METHOD  \u0393\u00c7\u00f4 We performed a systematic review of studies that compared predictions from cross-company models with predictions from within-company models based on analysis of project data. RESULTS  \u0393\u00c7\u00f4 Ten papers compared cross-company and within-company estimation models, however, only seven of the papers presented independent results. Of those seven, three found that cross-company models were as good as within-company models, four found cross-company models were significantly worse than within-company models. Experimental procedures used by the studies differed making it impossible to undertake formal meta-analysis of the results. The main trend distinguishing study results was that studies with small single company data sets (i.e. <20 projects) that used leave-one-out cross-validation all found that the within-company model was significantly more accurate than the cross-company model. CONCLUSIONS  \u0393\u00c7\u00f4 The results of this review are inconclusive. It is clear that some organisations would be ill-served by cross-company models whereas others would benefit. Further studies are needed, but they must be independent (i.e. based on different data bases or at least different single company data sets). In addition, experimenters need to standardise their experimental procedures to enable formal meta-analysis.", "num_citations": "119\n", "authors": ["13"]}
{"title": "An investigation of software engineering curricula\n", "abstract": " We adapted a survey instrument developed by Timothy Lethbridge to assess the extent to which the education delivered by four UK universities matches the requirements of the software industry. We propose a survey methodology that we believe addresses the research question more appropriately than the one used by Lethbridge. In particular, we suggest that restricting the scope of the survey to address the question of whether the curricula for a specific university addressed the needs of its own students, allowed us to identify an appropriate target population. However, our own survey suffered from several problems. In particular the questions used in the survey are not ideal, and the response rate was poor.Although the poor response rate reduces the value of our results, our survey appears to confirm several of Lethbridge's observations with respect to the over-emphasis of mathematical topics and the under\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "116\n", "authors": ["13"]}
{"title": "Using card sorts to elicit web page quality attributes\n", "abstract": " The Internet's rapid expansion has given rise to a new set of problems relating to the development of measures for software products such as Web pages. This article identifies Web page design quality attributes and explains how to measure them using card sorts.", "num_citations": "116\n", "authors": ["13"]}
{"title": "Principles of survey research\n", "abstract": " This article is the fifth installment of our series of articles on survey research. In it, we discuss what we mean by a population and a sample and the implications of each for survey research. We provide examples of correct and incorrect sampling techniques used in software engineering surveys.", "num_citations": "112\n", "authors": ["13"]}
{"title": "Coupling measures and change ripples in C++ application software\n", "abstract": " This paper describes an investigation into the effects of class couplings on changes made to a commercial C++ application over a period of 2 1 2 yr. The Chidamber and Kemerer CBO metric is used to measure class couplings within the application and its limitations are identified. Through an in-depth study of the ripple effects of changes to the source code, practical insight into the nature and extent of software couplings is provided.", "num_citations": "109\n", "authors": ["13"]}
{"title": "The SQUID approach to defining a quality model\n", "abstract": " This paper describes an attempt to use the approach developed by the SQUIDproject, which was part of the ESPRIT 3 programme, to define the software quality requirements of the Telescience project. The SQUID project developed its approach to quality modelling in parallel with ongoing feedback from testing that approach on the Telescience project, which was both large and software intensive. As part of this exercise we used the ISO software quality standard ISO 9126. It was an assessment of this and other existing quality models that caused us to re-assess what was meant by a quality model and led to a decomposition of existing \u0393\u00c7\u00ffquality models\u0393\u00c7\u00d6 into a composite model reflecting the different aspects of the model and its mapping onto a specific project or product. We break existing quality models into components which reflect the structure and content of the model. This composite model must then be\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "108\n", "authors": ["13"]}
{"title": "Software project development cost estimation\n", "abstract": " This paper reports the results of an empirical investigation of the relationships between effort expended, time scales, and project size for software project development. The observed relationships were compared with those predicted by Lawrence Putnam's Rayleigh curve model and Barry Boehm's COCOMO model. The results suggested that although the form of the basic empirical relationships were consistent with the cost models, the COCOMO model was a poor estimator of cost for the current data set and the data did not follow the Rayleigh curve suggested by Putnam. However, the results did suggest that it was possible to develop cost models tailored to a particular environment and to improve the precision of the models as they are used during the development cycle by including additional information such as the known effort for the early development phases. The paper finishes by discussing some of the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "105\n", "authors": ["13"]}
{"title": "Refining the systematic literature review process\u0393\u00c7\u00f6two participant-observer case studies\n", "abstract": " Systematic literature reviews (SLRs) are a major tool for supporting evidence-based software engineering. Adapting the procedures involved in such a review to meet the needs of software engineering and its literature remains an ongoing process. As part of this process of refinement, we undertook two case studies which aimed 1) to compare the use of targeted manual searches with broad automated searches and 2) to compare different methods of reaching a consensus on quality. For Case 1, we compared a tertiary study of systematic literature reviews published between January 1, 2004 and June 30, 2007 which used a manual search of selected journals and conferences and a replication of that study based on a broad automated search. We found that broad automated searches find more studies than manual restricted searches, but they may be of poor quality. Researchers undertaking SLRs may be\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "104\n", "authors": ["13"]}
{"title": "The role of replications in empirical software engineering\u0393\u00c7\u00f6a word of warning\n", "abstract": " There are many issues raised by Shull et al. that I would whole-heartedly endorse. For example, replication is a basic component of the scientific method, so it hardly needs to be justified. Furthermore, a full comprehensive report of any empirical study is a requirement of good science. When restrictions on the length of conference or journal paper prevent such reporting, it is an extremely good idea to record all the experimental details somewhere, whether in a \u0393\u00c7\u00a3laboratory package\u0393\u00c7\u00a5 or a technical report. However, is such supporting information essential for subsequent replications? The authors give examples such as insufficient training, or systems not having defects appropriate for the technique, causing replications to wrongly contradict the original experiment. I believe issues such as the criticality of training or the type of relevant defects should be reported in the published study not hidden in ancillary data. It may\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "100\n", "authors": ["13"]}
{"title": "Systematic literature reviews in global software development: a tertiary study\n", "abstract": " Context: There has been an increase in research into global software development (GSD) and in the number of systematic literature reviews (SLRs) addressing this topic. Objective: The aim of this research is to catalogue GSD SLRs in order to identify the topics covered, the active researchers, the publication vehicles, and to assess the quality of the SLRs identified. Method: We performed a broad automated search to find SLRs dealing with GSD. We differentiate between SLR studies and papers reporting those studies. Data relating to each of the following was extracted and synthesized from each study: authors and their affiliation at the time of publication, the journal or conference in which the paper was published, the quality of each study and the main GSD study topic. Results: Twenty-four GSD SLR studies and 37 papers reporting those studies were identified. Major GSD topics covered include: (1\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "99\n", "authors": ["13"]}
{"title": "Software cost models\n", "abstract": " The paper reports the results of a joint research project by ICL and British Telecom aimed at establishing methods of evaluating software cost models. The document considers some of the problems involved in software cost estimation and some of the features that would be required of a reasonable cost model.", "num_citations": "97\n", "authors": ["13"]}
{"title": "An empirical validation of the relationship between the magnitude of relative error and project size\n", "abstract": " Cost estimates are important deliverables of a software project. Consequently, a number of cost prediction models have been proposed and evaluated. The common evaluation criteria have been MMRE, MdMRE and PRED(k). MRE is the basic metric in these evaluation criteria. The implicit rationale of using a relative error measure like MRE, rather than an absolute one, is presumably to have a measure that is independent of project size. We investigate if this implicit claim holds true for several data sets: Albrecht, Kemerer, Finnish, DMR and Accenture-ERP. The results suggest that MRE is not independent of project size. Rather, MRE is larger for small projects than for large projects. A practical consequence is that a project manager predicting a small project may falsely believe in a too low MRE. Vice versa when predicting a large project. For researchers, it is important to know that MMRE is not an appropriate\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "89\n", "authors": ["13"]}
{"title": "The MERMAID approach to software cost estimation\n", "abstract": " Despite the large supply of methods and tools for cost estimation, estimating the costs of a software development project remains a non-trivial activity. Research [Heemstra 1989, Mermaid 1989] has shown that the accuracy of such tools is low. It has also been shown that only a limited group of organizations uses systematic methods for drawing up a cost estimate for project-based software development. For many years, various lines of industry (for example the building industry and the catering industry) have used experience figures when drawing up cost estimates for projects. Cost estimates for software projects often lack such a basis, partly because there are no adequate tools for recording and analysing historical project data. This paper presents an analysis of the problems in the field of software cost models and describes the MERMAID approach to cost estimation. The MERMAID approach makes\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "87\n", "authors": ["13"]}
{"title": "A framework for supporting architecture knowledge and rationale management\n", "abstract": " There is growing recognition of the importance of documenting and managing background knowledge about architecture design decisions. However, there is little guidance on the types of information that form Architecture Design Knowledge (ADK), how to make implicitly described ADK explicit, and how such knowledge can be documented to improve architecture processes. We propose a framework that provides a support mechanism to capture and manage ADK. We analyze different approaches f to capturing tacit and implicit design knowledge describe a process of extracting ADK from patterns and an effective way of documenting it. We also present a data model to characterize architecture design primitives used or generated during architecture design and evaluation. This data model can be tailored to implement repositories of ADK. We complete this chapter with open issues that architecture research must\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "85\n", "authors": ["13"]}
{"title": "Modeling software bidding risks\n", "abstract": " We discuss a method of developing a software bidding model that allows users to visualize the uncertainty involved in pricing decisions and make appropriate bid/no bid decisions. We present a generic bidding model developed using the modeling method. The model elements were identified after a review of bidding research in software and other industries. We describe the method we developed to validate our model and report the main results of our model validation, including the results of applying the model to four bidding scenarios.", "num_citations": "82\n", "authors": ["13"]}
{"title": "Presenting software engineering results using structured abstracts: a randomised experiment\n", "abstract": " When conducting a systematic literature review, researchers usually determine the relevance of primary studies on the basis of the title and abstract. However, experience indicates that the abstracts for many software engineering papers are of too poor a quality to be used for this purpose. A solution adopted in other domains is to employ structured abstracts to improve the quality of information provided. This study consists of a formal experiment to investigate whether structured abstracts are more complete and easier to understand than non-structured abstracts for papers that describe software engineering experiments. We constructed structured versions of the abstracts for a random selection of 25 papers describing software engineering experiments. The 64 participants were each presented with one abstract in its original unstructured form and one in a structured form, and for each one were asked to\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "79\n", "authors": ["13"]}
{"title": "An investigation of analysis techniques for software datasets\n", "abstract": " The goal of the study was to investigate the efficacy of different data analysis techniques for software data. We used simulation to create datasets with a known underlying model and with non-Normal characteristics that are frequently found in software datasets: skewness, unstable variance, and outliers and combinations of these characteristics. We investigated three main statistically based data analysis techniques: residual analysis; multivariate regression; classification and regression trees (CART). In addition to the standard \"least squares\" version of the technique, we also investigated robust and nonparametric versions of the techniques. We found that standard multivariate regression techniques were best if the data only exhibited skewness. However, under more extreme conditions such as severe heteroscedasticity, the nonparametric residual analysis technique performed best. We also found that even when\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "77\n", "authors": ["13"]}
{"title": "Software process simulation modeling: an extended systematic review\n", "abstract": " Software Process Simulation Modeling (SPSM) research has increased in the past two decades, especially since the first ProSim Workshop held in 1998. Our research aims to systematically assess how SPSM has evolved during the past 10 years in particular whether the purposes for SPSM, the simulation paradigms, tools, research topics, and the model scopes and outputs have changed. We performed a systematic literature review of the SPSM research in two subsequent stages, and identified 156 relevant studies in four categories. This paper reports the review process of the second stage and the preliminary results by aggregating studies from the two stages. Although the load of SPSM studies was dominated in ProSim/ICSP community, the outside research presented more diversity in some aspects. We also perceived an immediate need for refining and updating the reasons and the classification\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "74\n", "authors": ["13"]}
{"title": "Systematic review in software engineering: where we are and where we should be going\n", "abstract": " In 2004 Kitchenham et al. first proposed the idea of evidence-based software engineering (EBSE). EBSE requires a systematic and unbiased method of aggregating empirical studies and has encouraged software engineering researches to undertake systematic literature reviews (SLRs) of Software Engineering topics and research questions. As software engineers began to use the SLR technology, they also began to comment on the SLR process itself. Brereton et al (2007) was one of the first papers that commented on issues connected with performing SLRs and many such papers have followed since covering topics such as: The use of SLRs in education; Experiences of novices using SLRs; The adoption of mapping and scoping studies; The repeatability of SLRs; Improving the search and selection processes; Quality assessment of primary studies; Improving aggregation processes.", "num_citations": "72\n", "authors": ["13"]}
{"title": "A quantitative approach to monitoring software development\n", "abstract": " This paper provides a preliminary overview of a procedure for monitoring software development with respect to quality. The monitoring process is based on the extraction, analysis and interpretation of metrics relating to software products and processes. In particular, we are concerned with the problem of interpreting software metrics data in terms that are meaningful to project and quality managers. We present a four-stage model of the interpretation of metrics which comprises the identification of abnormal metric values (i.e. exceptions), possible causes of exceptions, additional information required to distinguish between causes, and possible corrective actions. We envisage that the model could be implemented as an advice system for managers. We discuss the general principles of project monitoring throughout the development life-cycle, and identify metrics that may be collected throughout, from requirements\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "71\n", "authors": ["13"]}
{"title": "Tools to support systematic reviews in software engineering: a feature analysis\n", "abstract": " Background The labour intensive and error prone nature of the systematic review process has led to the development and use of a range of tools to provide automated support.Aim The aim of this research is to evaluate a set of candidate tools that provide support for the overall systematic review process.", "num_citations": "69\n", "authors": ["13"]}
{"title": "The impact of limited search procedures for systematic literature reviews\u0393\u00c7\u00f6A participant-observer case study\n", "abstract": " This study aims to compare the use of targeted manual searches with broad automated searches, and to assess the importance of grey literature and breadth of search on the outcomes of SLRs. We used a participant-observer multi-case embedded case study. Our two cases were a tertiary study of systematic literature reviews published between January 2004 and June 2007 based on a manual search of selected journals and conferences and a replication of that study based on a broad automated search. Broad searches find more papers than restricted searches, but the papers may be of poor quality. Researchers undertaking SLRs may be justified in using targeted manual searches if they intend to omit low quality papers; if publication bias is not an issue; or if they are assessing research trends in research methodologies.", "num_citations": "62\n", "authors": ["13"]}
{"title": "The question of scale economies in software\u0393\u00c7\u00f6why cannot researchers agree?\n", "abstract": " This paper investigates the different research results obtained when different researchers have investigated the issue of economies and diseconomies of scale in software projects. Although researchers have used broadly similar sets of software project data sets, the results of their analyses and the conclusions they have drawn have differed. The paper highlights methodological differences that have lead to the conflicting results and shows how in many cases the differing results can be reconciled. It discusses the application of econometric concepts such as production frontiers and data envelopment analysis (DEA) to software data sets. It concludes that the assumptions underlying DEA may make it unsuitable for most software datasets but stochastic production frontiers may be relevant. It also raises some statistical issues that suggest testing hypothesis about economies and diseconomies of scale may be much\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "61\n", "authors": ["13"]}
{"title": "Teaching evidence-based software engineering to university students\n", "abstract": " Evidence-based software engineering (EBSE) describes a process of identifying, understanding and evaluating findings from research and practice-based experience. This process aims at improving software engineering decisions. For the last three years, EBSE has been taught to university students at Hedmark University College, Rena, Norway. The motivation for the EBSE-course is that it is essential for the students, as future practitioners, to learn how to base important software engineering decisions on the systematic and critical evaluation of the best available evidence. The main purpose of this paper is to inspire and support other universities in their work on developing their own EBSE-courses. For this purpose we report on how our course has been organized and what lessons have been learned. There are currently no studies available on the effects of teaching EBSE and, as far as we know, only we have\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "60\n", "authors": ["13"]}
{"title": "Evaluating logistic regression models to estimate software project outcomes\n", "abstract": " ContextSoftware has been developed since the 1960s but the success rate of software development projects is still low. During the development of software, the probability of success is affected by various practices or aspects. To date, it is not clear which of these aspects are more important in influencing project outcome.ObjectiveIn this research, we identify aspects which could influence project success, build prediction models based on the aspects using data collected from multiple companies, and then test their performance on data from a single organization.MethodA survey-based empirical investigation was used to examine variables and factors that contribute to project outcome. Variables that were highly correlated to project success were selected and the set of variables was reduced to three factors by using principal components analysis. A logistic regression model was built for both the set of variables and\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "59\n", "authors": ["13"]}
{"title": "Tools to support systematic reviews in software engineering: a cross-domain survey using semi-structured interviews\n", "abstract": " Background: A number of software tools are being developed to support systematic reviewers within the software engineering domain. However, at present, we are not sure which aspects of the review process can most usefully be supported by such tools or what characteristics of the tools are most important to reviewers. Aim: The aim of the study is to explore the scope and practice of tool support for systematic reviewers in other disciplines. Method: Researchers with experience of performing systematic reviews in Healthcare and the Social Sciences were surveyed. Qualitative data was collected through semi-structured interviews and data analysis followed an inductive approach. Results: 13 interviews were carried out. 21 software tools categorised into one of seven types were identified. Reference managers were the most commonly mentioned tools. Features considered particularly important by participants\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "57\n", "authors": ["13"]}
{"title": "The educational value of mapping studies of software engineering literature\n", "abstract": " We identify three challenges related to the provenance of the material we use in teaching software engineering. We suggest that these challenges can be addressed by using evidence-based software engineering (EBSE) and its primary tool of systematic literature reviews (SLRs). This paper aims to assess the educational and scientific value of undergraduate and postgraduate students undertaking a specific form of SLR called a mapping study. Using a case study methodology, we asked three postgraduate students and three undergraduates and their supervisor to complete a questionnaire concerning the educational value of mapping studies and any problems they experienced. Students found undertaking a mapping study to be a valuable experience providing both reusable research skills and a good overview of a research topic. Postgraduates found it useful as a starting point for their studies. Undergraduates\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "57\n", "authors": ["13"]}
{"title": "Repeatability of systematic literature reviews\n", "abstract": " Background: One of the anticipated benefits of systematic literature reviews (SLRs) is that they can be conducted in an auditable way to produce repeatable results. Aim: This study aims to identify under what conditions SLRs are likely to be stable, with respect to the primary studies selected, when used in software engineering. The conditions we investigate in this report are when novice researchers undertake searches with a common goal. Method: We undertook a participant-observer multi-case study to investigate the repeatability of systematic literature reviews. The \"cases\" in this study were the early stages, involving identification of relevant literature, of two SLRs of unit testing methods. The SLRs were performed independently by two novice researchers. The SLRs were restricted to the ACM and IEEE digital libraries for the years 1986-2005 so their results could be compared with a published expert literature\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "56\n", "authors": ["13"]}
{"title": "Large-scale software engineering questions\u0393\u00c7\u00f4expert opinion or empirical evidence?\n", "abstract": " A recent report on the state of the UK information technology (IT) industry based most of its findings and recommendations on expert opinion. It is surprising that the report was unable to incorporate more empirical evidence. This paper aims to assess whether it is necessary to base IT industry and academic policy on expert opinion rather than on empirical evidence. Current evidence related to the rate of project failure is identified and the methods used to accumulate that evidence discussed. This shows that the report failed to identify relevant evidence and most evidence related to project failure is based on convenience samples. The status of empirical research in the computing disciplines is reviewed showing that empirical evidence covers a restricted range of subjects and seldom addresses the \u0393\u00c7\u00ffSociety\u0393\u00c7\u00d6 level of analysis. Other more robust designs that would address large-scale IT questions are discussed. We\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "56\n", "authors": ["13"]}
{"title": "A further empirical investigation of the relationship between MRE and project size\n", "abstract": " The mean magnitude of relative error, MMRE, is the de facto standard evaluation criterion to assess the accuracy of software project prediction models. The fundamental metric of MMRE is MRE, a relative residual error. For MMRE to be a meaningful summary statistic, it is a necessary, but not sufficient, condition that MRE and project size are uncorrelated. Except for two previous conference studies done by the same authors, it has never been empirically validated that MRE and project size really are uncorrelated. In this paper, we extend the previous studies using the same data sets as before: Albrecht, Kemerer, Finnish, DMR and Accenture-ERP. Unlike the previous studies, we plot MRE against the predicted effort rather than against the actual effort and, in so doing, we obtain very different results from the previous studies. The results of this study suggest that MRE and project size are uncorrelated, which\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "56\n", "authors": ["13"]}
{"title": "Misleading metrics and unsound analyses\n", "abstract": " The authors demonstrate that the recommendations for analyzing productivity in the appendix to the ISO/IEC 15939 standard are inappropriate. They also show that problems with the ISO/IEC advice can be compounded if software engineers attempt to apply statistical process-control techniques to software productivity metrics. They recommend using small meaningful data sets as the basis for productivity analysis and using effort-estimation models to assess productivity rather than productivity metrics. This article is part of a special focus section of software metrics", "num_citations": "55\n", "authors": ["13"]}
{"title": "Software process simulation modeling: Facts, trends and directions\n", "abstract": " Software process simulation modeling (SPSM) research has increased since the first ProSim workshop held in 1998 and Kellner, Madachy and Raffo (KMR) discussed the \"why, what and how\" of process simulation. This paper aims to assess how SPSM has evolved during the past 10 years in particular whether the reasons for SPSM, the simulation paradigms, tools, problem domains, and model scopes have changed. We performed a systematic literature review of software process simulation papers from the ProSim series publications in the last decade. We identified 96 studies from the sources and included them in this review. The papers were categorized into four major types and data needed to address each research question was extracted. We found a need for refining the reasons and the classification scheme for SPSM introduced by KMR. More emerging SPSM paradigms and model scopes were added to\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "54\n", "authors": ["13"]}
{"title": "Measurement modeling technology\n", "abstract": " Although rigorous measurement has become a necessity in the software industry, many measurement programs fail to deliver real benefit to software managers. The required data is often missing, invalid, or late. But a properly automated measurement system can deliver timely reports that enable proactive management. The measurement modeling technology introduced here supports such automation. It also establishes standard measurements and metrics that organizations can share and combine across projects, departments, and companies.", "num_citations": "54\n", "authors": ["13"]}
{"title": "Design metrics in practice\n", "abstract": " The paper describes a method of software quality control based on the use of software metrics. The method is applied to software design metrics to illustrate how design metrics can be used constructively during the software production process. The various types of design metrics and how they can be used to support module (procedure) quality-control are discussed. This involves adapting conventional quality control methods such as control charts to the realities of software, by using: \u0393\u00c7\u00ffrobust\u0393\u00c7\u00d6 summary statistics to construct ranges of acceptable metric values; scatterplots to detect modules with unusual combinations of metric values; and different types of metrics to help identify the underlying reasons for a module having unacceptable (anomalous) metric values. The approach is illustrated with examples of metrics from a number of existing software products.", "num_citations": "53\n", "authors": ["13"]}
{"title": "How does project size affect cost estimation error? Statistical artifacts and methodological challenges\n", "abstract": " Empirical studies differ in what they report as the underlying relation between project size and percent cost overrun. As a consequence, the studies also differ in their project management recommendations. We show that studies with a project size measure based on the actual cost systematically report an increase in percent cost overrun with increased project size, whereas studies with a project size measure based on the estimated cost report a decrease or no change in percent cost overrun with increased project size. The observed pattern is, we argue, to some extent a statistical artifact caused by imperfect correlation between the estimated and the actual cost. We conclude that the previous observational studies cannot be considered to provide reliable evidence in favor of an underlying project size related cost estimation bias. We discuss the limited, statistically robust evidence and the need for other types of\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "52\n", "authors": ["13"]}
{"title": "Can we evaluate the quality of software engineering experiments?\n", "abstract": " Context: The authors wanted to assess whether the quality of published human-centric software engineering experiments was improving. This required a reliable means of assessing the quality of such experiments. Aims: The aims of the study were to confirm the usability of a quality evaluation checklist, determine how many reviewers were needed per paper that reports an experiment, and specify an appropriate process for evaluating quality. Method: With eight reviewers and four papers describing human-centric software engineering experiments, we used a quality checklist with nine questions. We conducted the study in two parts: the first was based on individual assessments and the second on collaborative evaluations. Results: The inter-rater reliability was poor for individual assessments but much better for joint evaluations. Four reviewers working in two pairs with discussion were more reliable than eight\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "52\n", "authors": ["13"]}
{"title": "Validating software measures\n", "abstract": " There are two distinct notions of validation for software measures. On the one hand there is the informal notion that a measure is only valid if it is useful and practical. Although this view of validation is important, it is almost impossible to test. On the other hand there is a formal notion of validation of measures and it is this view which is addressed here. By considering software measures in the context of measurement theory, and by considering a classification of software entities into products, processes and resources the paper describes what the authors believe are the necessary (but not sufficient) activities required for validation, and show how far this deviates from the \u0393\u00c7\u00ffaccepted\u0393\u00c7\u00d6 view of validation.", "num_citations": "50\n", "authors": ["13"]}
{"title": "A framework for evaluating a software bidding model\n", "abstract": " This paper discusses the issues involved in evaluating a software bidding model. We found it difficult to assess the appropriateness of any model evaluation activities without a baseline or standard against which to assess them. This paper describes our attempt to construct such a baseline. We reviewed evaluation criteria used to assess cost models and an evaluation framework that was intended to assess the quality of requirements models. We developed an extended evaluation framework and an associated evaluation process that will be used to evaluate our bidding model. Furthermore, we suggest the evaluation framework might be suitable for evaluating other models derived from expert-opinion based influence diagrams.", "num_citations": "49\n", "authors": ["13"]}
{"title": "An empirical study of groupware support for distributed software architecture evaluation process\n", "abstract": " Software architecture evaluation is an effective means of addressing quality related issues early in the software development lifecycle. Scenario-based approaches to evaluate architecture usually involve a large number of stakeholders, who need to be collocated for face-to-face evaluation meetings. Collocating a large number of stakeholders is an expensive and time-consuming exercise, which may prove to be a hurdle in the wide-spread adoption of disciplined architectural evaluation practices. Drawing upon the successful introduction of groupware applications to support geographically distributed teams in software inspection, and requirements engineering disciplines, we propose the concept of distributed architectural evaluation using Internet-based collaborative technologies. This paper presents a pilot study used to assess the viability of a larger experiment intended to investigate the feasibility of groupware\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "48\n", "authors": ["13"]}
{"title": "Evaluating guidelines for empirical software engineering studies\n", "abstract": " Background. Several researchers have criticized the standards of performing and reporting empirical studies in software engineering. In order to address this problem, Andreas Jedlitschka and Dietmar Pfahl have produced reporting guidelines for controlled experiments in software engineering. They pointed out that their guidelines needed evaluation. We agree that guidelines need to be evaluated before they can be widely adopted. If guidelines are flawed, they will cause more problems that they solve. Aim. The aim of this paper is to present the method we used to evaluate the guidelines and report the results of our evaluation exercise. We suggest our evaluation process may be of more general use if reporting guidelines for other types of empirical study are developed. Method. We used perspective-based inspections to perform a theoretical evaluation of the guidelines. A separate inspection was performed for\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "47\n", "authors": ["13"]}
{"title": "Length and readability of structured software engineering abstracts\n", "abstract": " Attempts to perform systematic literature reviews have identified a problem with the quality of software engineering abstracts for papers describing empirical studies. Structured abstracts have been found useful for improving the quality of abstracts in many other disciplines. However, there have been no studies of the value of structured abstracts in software engineering. Therefore this paper aims to assess the comparative length and readability of unstructured abstracts and structured versions of the same abstract. Abstracts were obtained from all empirical conference papers from the Evaluation and Assessment in Software Engineering Conference (EASE04 and EASE06) that did not have a structured abstract (23 in total). Two novice researchers created structured versions of the abstracts, which were checked by the papers' authors (or a surrogate). Web tools were used to extract the length in words and readability\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "46\n", "authors": ["13"]}
{"title": "Critical review of quantitative assessment\n", "abstract": " The paper discusses several empirical studies reported in the literature aimed at evaluating the benefits of using software engineering methods and tools. The discussion highlights a number of problems associated with the methodology of the studies. The main problems concerned the difficulty of formulating the hypothesis to be tested, using surrogate measures, defining a control and minimising the effect of personalities. Most of these problems are found in many experimental situations, but the problem associated with the proper definition of a control group seems to be a particular issue for software experiments. The paper concludes with some guidelines for improving the organisation of empirical studies.", "num_citations": "46\n", "authors": ["13"]}
{"title": "The challenge of introducing a new software cost estimation technology into a small software organisation\n", "abstract": " Fostering innovation is the key to survival in today's IT business and is exemplified by introducing new technologies and methods to improve the development processes. We present a follow-up case study of technology transfer in a small software organisation. A new software estimation technique, Web-CoBRA was introduced to a small software company to improve their software estimation process. Web-CoBRA was considerably more accurate than the company's current estimation process. However, despite management being aware of this improvement, the company has not fully adopted the new method. We used interviews and the technology acceptance model (TAM) questionnaire to assess the extent to which Web-CoBRA was used by the company. We found take-up of part of the Web-CoBRA technology but the full technology and the support tools were not used. We identify the reasons for the failure to\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "45\n", "authors": ["13"]}
{"title": "Reporting computing projects through structured abstracts: a quasi-experiment\n", "abstract": " Previous work has demonstrated that the use of structured abstracts can lead to greater completeness and clarity of information, making it easier for researchers to extract information about a study. In academic year 2007/08, Durham University\u0393\u00c7\u00d6s Computer Science Department revised the format of the project report that final year students were required to write, from a \u0393\u00c7\u00fftraditional dissertation\u0393\u00c7\u00d6 format, using a conventional abstract, to that of a 20-page technical paper, together with a structured abstract. This study set out to determine whether inexperienced authors (students writing their final project reports for computing topics) find it easier to produce good abstracts, in terms of completeness and clarity, when using a structured form rather than a conventional form. We performed a controlled quasi-experiment in which a set of \u0393\u00c7\u00ffjudges\u0393\u00c7\u00d6 each assessed one conventional and one structured abstract for its\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "42\n", "authors": ["13"]}
{"title": "Considering quality in the management of software-based development projects\n", "abstract": " Projects in which software is developed are notorious for being late, above budget, and out of specification. Typically, project managers point to a number of technical problems, most of which are specific to software development, as reasons for this. The paper attempts to show, however, that inadequate management and a lack of attention to quality are the main causes of the trouble.", "num_citations": "42\n", "authors": ["13"]}
{"title": "The meaning of quality\n", "abstract": " Quality is a popular word nowadays. No manager, designer, salesman or inspector can consider his or her vocabulary complete unless it contains the word quality, frequently repeated. What is all too often overlooked, although it has become increasingly apparent to those who work principally in the field of quality, is that\" quality\" conjures up quite different ideas in different people. This paper is written against a background of software development, but the fundamentally different views of quality it discusses are quite independent of any specific product considerations.", "num_citations": "42\n", "authors": ["13"]}
{"title": "An evaluation of software structure metrics\n", "abstract": " Evaluates some software design metrics, based on the information flow metrics of S. Henry and D. Kafura (1981, 1984), using data from a communications system. The ability of the design metrics to identify change-prone, error-prone, and complex programs was contrasted with that of simple code metrics. It was found that the design metrics were not as good at identifying change-prone, fault-prone, and complex programs as simple code metrics (ie lines of code and number of branches). It was also observed that the compound metrics, built up from several different basic counts, can obscure underlying effects, and thus make it more difficult to use metrics constructively.<>", "num_citations": "41\n", "authors": ["13"]}
{"title": "Comparing distributed and face-to-face meetings for software architecture evaluation: A controlled experiment\n", "abstract": " Scenario-based methods for evaluating software architecture require a large number of stakeholders to be collocated for evaluation meetings. Collocating stakeholders is often an expensive exercise. To reduce expense, we have proposed a framework for supporting software architecture evaluation process using groupware systems. This paper presents a controlled experiment that we conducted to assess the effectiveness of one of the key activities, developing scenario profiles, of the proposed groupware-supported process of evaluating software architecture. We used a cross-over experiment involving 32 teams of three 3rd and 4th year undergraduate students. We found that the quality of scenario profiles developed by distributed teams using a groupware tool were significantly better than the quality of scenario profiles developed by face-to-face teams (p\u0393\u00c7\u00eb<\u0393\u00c7\u00eb0.001). However, questionnaires indicated\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "40\n", "authors": ["13"]}
{"title": "Investigating the applicability of the evidence-based paradigm to software engineering\n", "abstract": " Context: The success of the evidence-based paradigm in other domains, especially medicine, has raised the question of how this might be employed in software engineering. Objectives: To report the research we are doing to evaluate problems associated with adopting the evidence-based paradigm in software engineering and identifying strategies to address these problems. Method: Currently the experimental paradigms used in a selected set of domains are being examined along with the experimental protocols that they employ. Our aim is to identify those domains that have generally similar characteristics to software engineering and to study the strategies that they employ to overcome the lack of rigorous empirical protocols. We are also undertaking a series of systematic literature reviews to identify the factors that may limit their applicability in the software engineering domain. Conclusions: We have identified\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "40\n", "authors": ["13"]}
{"title": "Evaluating software engineering methods and tools: part 9: quantitative case study methodology\n", "abstract": " This article is the first of three articles describing how to undertake a quantitative case study based on work done as part of the DESMET project [1], [2]. In the context of methods and tool evaluations, case studies are a means of evaluating methods and tools as part of the normal software development activities undertaken by an organisation. The main benefit of such case studies is that they allow the effect of new methods and tools to be assessed in realistic situations. Thus, case studies provide a cost-effective means of ensuring that process changes provide the desired results. However, unlike formal experiments and surveys, case studies do not have a well-understood theoretical basis. This series of articles provides guidelines for organising and analysing case studies so that your investigations of new technologies will produce meaningful results.", "num_citations": "39\n", "authors": ["13"]}
{"title": "Mapping study completeness and reliability-a case study\n", "abstract": " Context: We have been undertaking a series of case studies to investigate the value of mapping (scoping) studies in software engineering. Our previous studies have assessed these using the subjective opinions of researchers. Objective: In order to provide a more objective assessment of value, for this study, we used the results of a systematic mapping study to investigate how well mapping studies identify clusters of related studies and to what extent such clusters are complete. Method: In this participant-observer case study, we undertook a mapping study of unit testing and regression testing empirical studies, which we compared with a previous expert literature review and with six other mapping studies and systematic literature reviews (SLRs) that addressed overlapping topics. Results: Our mapping study found more clusters than the expert literature review although it benefited from the set of studies identified\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "38\n", "authors": ["13"]}
{"title": "Evaluating software engineering methods and tools part 12: evaluating desmet\n", "abstract": " Evaluating software engineering methods and tools part 12: evaluating DESMET: ACM SIGSOFT Software Engineering Notes: Vol 23, No 5 ACM Digital Library Logo ACM Logo Google, Inc. (search) Advanced Search Browse About Sign in Register Advanced Search Journals Magazines Proceedings Books SIGs Conferences People More Search ACM Digital Library SearchSearch Advanced Search ACM SIGSOFT Software Engineering Notes Newsletter Home Latest Issue Archive Authors Affiliations Award Winners More HomeSIGsSIGSOFTACM SIGSOFT Software Engineering NotesVol. , No. Evaluating software engineering methods and tools part 12: evaluating DESMET article Evaluating software engineering methods and tools part 12: evaluating DESMET Share on Author: Barbara Ann Kitchenham profile image Barbara Ann Kitchenham View Profile Authors Info & Affiliations Publication: ACM SIGSOFT Notes:/\u0393\u00c7\u00aa", "num_citations": "38\n", "authors": ["13"]}
{"title": "Lessons learnt undertaking a large-scale systematic literature review\n", "abstract": " We have recently undertaken a large-scale Systematic Literature Review (SLR) of a research question concerning the Technology Acceptance Model (TAM). At the end of the study, we observed some anomalies during the analysis of the extracted data. In our attempts to identify the cause of the anomalies, we found a number of mistakes that had been made during the data extraction process. We discuss each of the mistakes in terms of why they occurred and how they might have been avoided. We suggest a number of ways in which the available guidelines for conducting SLRs should be amended to help avoid such problems occurring in future reviews.", "num_citations": "36\n", "authors": ["13"]}
{"title": "Empirical paradigm\u0393\u00c7\u00f4the role of experiments\n", "abstract": " This article discusses the role of formal experiments in empirical software engineering. I take the view that the role of experiments has been overemphasised. Laboratory experiments are not representative of industrial software engineering tasks, so do not provide us with a reliable assessment of the effect of our techniques and tools. I suggest we need to concentrate a larger proportion of our research effort on industrial quasi-experiments and case studies. Methodologies for these empirical methods are well-understood in the social science and would appear to be appropriate mechanisms for investigating many software engineering research questions. In addition, I believe we need to make the results of empirical software engineering more visible and relevant to practitioners. To influence practitioners I suggest that we need to produce evidence-based text books and evidence-based software engineering\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "36\n", "authors": ["13"]}
{"title": "Using simulated data sets to compare data analysis techniques used for software cost modelling\n", "abstract": " The goals of this study were to compare different data analysis methods and to demonstrate the viability of simulation as a mechanism to allow such comparisons. Simulation was used to create data sets with a known underlying model and with non-Normal characteristics that are frequently found in software data sets: skewness, unstable variance, and outliers and combinations of these characteristics. Three data analysis approaches were investigated: residual analysis; multiple regression; classification and regression trees (CART). In addition to the standard statistical `least squares' version of each method, robust and non-parametric versions of the techniques were also investigated.It was found that standard multiple regression techniques were best if the data only exhibited moderate non-Normality. As might be expected, under more extreme conditions such as severe heteroscedasticity, the non-parametric\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "36\n", "authors": ["13"]}
{"title": "Empirical Software Engineering Issues. Critical Assessment and Future Directions: International Workshop, Dagstuhl Castle, Germany, June 26-30, 2006, Revised Papers\n", "abstract": " Victor R. Basili, Dieter Rombach, and Kurt Schneider Introduction In 1992, a Dagstuhl seminar was held on \u0393\u00c7\u00a3Experimental Software Engineering Issues\u0393\u00c7\u00a5(seminar no. 9238). Its goal was to discuss the state of the art of empirical software engineering (ESE) by assessing past accomplishments, raising open questions, and proposing a future research agenda. Since 1992, the topic of ESE has been adopted more widely by academia as an interesting and promising research topic, and in industrial practice as a necessary infrastructure technology for goal-oriented, sustained process improvement. At the same time, the spectrum of methods applied in ESE has broadened. For example, in 1992, the empirical methods applied in software engineering were basically restricted to quantitative studies (mostly controlled experiments), whereas since then, a range of qualitative methods have been introduced, from observational to ethnographical studies. Thus, the field can be said to have moved from experimental to empirical software engineering. We believe that it is now time to again bring together practitioners and researchers to identify both the progress made since 1992 and the most important challenges for the next five to ten years.", "num_citations": "34\n", "authors": ["13"]}
{"title": "An Evaluation of Quality Checklist Proposals-A participant-observer case study\n", "abstract": " Background:  A recent set of guidelines for software engineering systematic literature reviews (SLRs) includes a list of quality criteria obtained from the literature. The guidelines suggest that the list can be used to construct a tailored set of questions to evaluate the quality of primary studies.  Aim:  This paper aims to evaluate whether the list of quality criteria help researchers construct tailored quality checklists.  Method:  We undertook a participant-observer case study to investigate the list of quality criteria. The \u0393\u00c7\u00a3case\u0393\u00c7\u00a5 in this study was the planning stage of a systematic literature review on unit testing.  Results:  The checklists in our SLR guidelines do not provide sufficient help with the construction of a quality checklist for a specific SLR either for novices or for experienced researchers. However, the checklists are reasonably complete and lead to the use of a common terminology for quality questions selected for a specific systematic literature review.  Conclusions:  The guidelines document should be amended to include a much shorter generic checklist. Researchers might find it useful to adopt a team-based process for quality checklist construction and provide suggestions for answering quality checklist questions.", "num_citations": "33\n", "authors": ["13"]}
{"title": "Experiments with analogy-x for software cost estimation\n", "abstract": " We developed a novel method called Analogy-X to provide statistical inference procedures for analogy- based software effort estimation. Analogy-X is a method to statistically evaluate the relationship between useful project features and target features such as effort to be estimated, which ensures the dataset used is relevant to the prediction problem, and project features are selected based on their statistical contribution to the target variables. We hypothesize that this method can be (1) easily applied to a much larger dataset, and (2) also it can be used for incorporating joint effort and duration estimation into analogy, which was not previously possible with conventional analogy estimation. To test these two hypotheses, we conducted two experiments using different datasets. Our results show that Analogy-X is able to deal with ultra large datasets effectively and provides useful statistics to assess the quality of the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "33\n", "authors": ["13"]}
{"title": "Lessons learnt from the analysis of large-scale corporate databases\n", "abstract": " This paper presents the lessons learnt during the analysis of the corporate databases developed by IBM Global Services (Australia). IBM is rated as CMM level 5. Following CMM level 4 and above practices, IBM designed several software metrics databases with associated data collection and reporting systems to manage its corporate goals. However, IBM quality staff believed the data were not as useful as they had expected. NICTA staff undertook a review of IBM's statistical process control procedures and found problems with the databases mainly due to a lack of links between the different data tables. Such problems might be avoided by using M 3 P variant of the GQM paradigm to define a hierarchy of goals, with project goals at the lowest level, then process goals and corporate goals at the highest level. We propose using ER models to identify problems with existing databases and to design databases once\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "32\n", "authors": ["13"]}
{"title": "Qualitative simulation model for software engineering process\n", "abstract": " Software process simulation models hold out the promise of improving project planning and control. However, quantitative models require a very detailed understanding of the software process. In particular, process knowledge needs to be represented quantitatively which requires extensive, reliable software project data. When such data is lacking, quantitative models must impose severe constraints, restricting the value of the models. In contrast qualitative models are able to cope with imprecise knowledge by reasoning at a more abstract level. This paper illustrates the value and flexibility of qualitative models by developing a model of the software staffing process and comparing it with other quantitative staffing models. We show that the qualitative model provides more insights into the staffing process than the quantitative models because it requires fewer constraints and can thus simulate more behaviors. In\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "32\n", "authors": ["13"]}
{"title": "Measures of programming complexity\n", "abstract": " The increasing cost of software development and maintenance has revealed the need to identify methods that encourage the production of high qual ity software. This in turn has highlighted the need to be able to quantify factors influencing the amount of effort needed to produce such software, such as program complexity.Two approaches to the problem of identifying complexity metrics have attracted interest in America; the theoretical treatment of software science by Halstead of Purdue University and the graph-theoretical concept devel oped by McCabe of the US Department of Defense. This paper reports an attempt to assess the ability of the measures of complexity proposed by these authors to provide objective indicators of the effort involved in soft ware production, when applied to selected subsystems of the ICL operating system VME/B. The proposed metrics were computed for each of the mod ules comprising these subsystems, also counts of the numbers of machinelevel instructions (Primitive Level Instructions, PLI) and measures of the effort involved in bringing the modules to an acceptable standard for field release. It was found that all the complexity metrics were correlated posi tively with the measure of effort, those modules which had proved more difficult having large values for all these metrics. However, neither Halstead\u0393\u00c7\u00d6s nor McCabe\u0393\u00c7\u00d6s metrics offered any substantial improvement over the simple PLI count as predictors of effort.", "num_citations": "32\n", "authors": ["13"]}
{"title": "Trends in the Quality of Human-Centric Software Engineering Experiments--A Quasi-Experiment\n", "abstract": " Context: Several text books and papers published between 2000 and 2002 have attempted to introduce experimental design and statistical methods to software engineers undertaking empirical studies. Objective: This paper investigates whether there has been an increase in the quality of human-centric experimental and quasi-experimental journal papers over the time period 1993 to 2010. Method: Seventy experimental and quasi-experimental papers published in four general software engineering journals in the years 1992-2002 and 2006-2010 were each assessed for quality by three empirical software engineering researchers using two quality assessment methods (a questionnaire-based method and a subjective overall assessment). Regression analysis was used to assess the relationship between paper quality and the year of publication, publication date group (before 2003 and after 2005), source journal\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "31\n", "authors": ["13"]}
{"title": "The case against cross-over designs in software engineering\n", "abstract": " We are often encouraged to follow experimental procedures in undertaking software engineering studies, however we should not do so blindly as often assumptions are made as part of that process that software engineering methods artefacts and processes breach. One such example is the use of crossover designs. We consider the case where there are period by treatment interactions, (i.e where the treatments are non-commutative) and demonstrate the hazards in using a cross-over design in these cases.", "num_citations": "31\n", "authors": ["13"]}
{"title": "The certainty of uncertainty\n", "abstract": " Dept. Computer Science Keele University Staffs ST5 5BG England barbara@ cs. keele. ac. uk+ 44 (0) 1782 583413", "num_citations": "31\n", "authors": ["13"]}
{"title": "The architecture of system quality\n", "abstract": " This paper discusses the problem of understanding and measuring quality in complex systems which include hardware and software. Using general systems concepts, a schematic model of quality is proposed and its use illustrated. The paper concludes by discussion of the problems involved with the use of the schematic model in developing the specific quality models required in the software and systems industry.", "num_citations": "28\n", "authors": ["13"]}
{"title": "The danger of using axioms in software metrics\n", "abstract": " The authors contrast axiom sets used in mathematics with those used in software metrics research. There are striking differences in the way the two types of axiom sets are derived and their intended purpose. In particular, axiom sets in software metrics are primarily used for purposes of definition, whereas in mathematics they are used for logical deduction. In addition, software metrics axiom sets have been developed without a consensus as to valid example measures and without a common understanding of the data to which they apply. The authors conclude that there is a non-negligible risk that inappropriate axiom sets will not be detected which may lead to an incorrect assessment of the validity of some software measures.", "num_citations": "27\n", "authors": ["13"]}
{"title": "The effects of inspections on software quality and productivity\n", "abstract": " Sauf mention contraire ci-dessus, le contenu de cette notice bibliographique peut \u251c\u00actre utilis\u251c\u2310 dans le cadre d\u0393\u00c7\u00d6une licence CC BY 4.0 Inist-CNRS/Unless otherwise stated above, the content of this bibliographic record may be used under a CC BY 4.0 licence by Inist-CNRS/A menos que se haya se\u251c\u2592alado antes, el contenido de este registro bibliogr\u251c\u00edfico puede ser utilizado al amparo de una licencia CC BY 4.0 Inist-CNRS", "num_citations": "27\n", "authors": ["13"]}
{"title": "An evaluation of the business object approach to software development\n", "abstract": " In this paper, we report the result of an evaluation of the use of business objects and business components for developing business application software. This evaluation was a replicated product case study in which a part of an existing product was re-implemented using an UML-based development process. In order to assess the impact of re-use, a second related product was implemented using the new technology. We found that producing software from scratch using UML was less productive during the development lifecycle, but productivity improved when substantial reuse (48%) was achieved. Time to market was not much affected by the new technology but was greatly improved when substantial reuse was achieved. Defect rates appeared substantially lower for the new technology irrespective of reuse levels. The technology also had other benefits including provision of documentation and less reliance on\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "26\n", "authors": ["13"]}
{"title": "Experiences introducing a measurement program\n", "abstract": " Measurement is an integral part of total quality management and process improvement strategies. This paper describes our experiences using the Goal-Question-Metric (GQM) paradigm to help design a company-wide measurement program for Engineering Ingegneria S.p.A., an Italian software house. The introduction of the measurement program was supported by the Commission of the European Communities within the European Software and Systems Initiative (ESSI) as a Process Improvement Experiment (PIE). We found it necessary to supplement GQM into two ways. Firstly, we defined our measures rigorously in terms of entities, attributes, units and counting rules. Secondly, the original GQM plan was subject to an independent review. The most critical problem identified by the review was that the GQM plan identified too many productivity factors for any statistical analysis to handle concurrently. In order to\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "26\n", "authors": ["13"]}
{"title": "System evolution dynamics of VME/B\n", "abstract": " The development of the ICL Operating System VME/B, was investigated over successive system releases in the light of the theory of program evolution dynamics developed by MM Lehman and LA Belady.", "num_citations": "26\n", "authors": ["13"]}
{"title": "Would wider adoption of reproducible research be beneficial for empirical software engineering research?\n", "abstract": " Researchers have identified problems with the validity of software engineering research findings. In particular, it is often impossible to reproduce data analyses, due to lack of raw data, or sufficient summary statistics, or undefined analysis procedures. The aim of this paper is to raise awareness of the problems caused by unreproducible research in software engineering and to discuss the concept of reproducible research (RR) as a mechanism to address these problems. RR is the idea that the outcome of research is both a paper and its computational environment. We report some recent studies that have cast doubts on the reliability of research outcomes in software engineering. Then we discuss the use of RR as a means of addressing these problems. We discuss the use of RR in software engineering research and present the methodology we have used to adopt RR principles. We report a small working example\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "24\n", "authors": ["13"]}
{"title": "The case for knowledge translation\n", "abstract": " Context: For the outcomes of systematic literature reviews to be of use for practitioners, we need to develop models for addressing the needs of Knowledge Translation (KT). Aim: To identify some of the key issues that need to be addressed by a KT process for software engineering (SE) and possible routes for achieving these. Method: We have examined some of the models used in other disciplines, and suggested a possible interpretation for software engineering. Results: We propose a model for achieving KT. Conclusions: Research with industry and commerce is needed to explore how this can be realised.", "num_citations": "23\n", "authors": ["13"]}
{"title": "Optimising project feature weights for analogy-based software cost estimation using the mantel correlation\n", "abstract": " Software cost estimation using analogy is an important area in software engineering research. Previous research has demonstrated that analogy is a viable alternative to other conventional estimation methods in terms of predictive accuracy. One of the important research areas for analogy is how to determine suitable project feature weights. This can be achieved by using an extensive project feature weights search, where the quality measure is optimised. However, this approach suffers similar issues as the brute-force feature selection approach in analogy. We propose a novel method to deal with this issue based upon the use of the Mantel randomisation test. Specifically, we determine project feature weights based on the strength of correlation between the distance matrix of project features and the distance matrix of known effort values of the dataset. We demonstrate the procedure on a specific dataset, showing\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "23\n", "authors": ["13"]}
{"title": "Systematic review of statistical process control: An experience report\n", "abstract": " Background: A systematic review is a rigorous method for assessing and aggregating research results. Unlike an ordinary literature review consisting of an annotated bibliography, a systematic review analyzes existing literature with reference to specific research questions on a topic of interest. Objective: Statistical Process Control (SPC) is a well established technique in manufacturing contexts that only recently has been used in software production. Software production is unlike manufacturing because it is human rather than machine-intensive, and results in the production of single one-off items. It is therefore pertinent to assess how successful SPC is in the context of software production. These considerations have therefore motivated us to define and carry out a systematic review to assess whether SPC is being used effectively and correctly by software practitioners. Method: A protocol has been defined, according to the systematic literature review process, it was revised and refined by the authors. At the current time, the review is being carried out. Results: We report our considerations and preliminary results in defining and carrying out a systematic review on SPC, and how graduate students have been included in the review process of a first set of the papers. Conclusions: Our first results and impressions are positive. Also, involving graduate students has been a successful experience.", "num_citations": "23\n", "authors": ["13"]}
{"title": "An investigation of coupling, reuse and maintenance in a commercial C++ application\n", "abstract": " This paper describes an investigation into the use of coupling complexity metrics to obtain early indications of various properties of a system of C++ classes. The properties of interest are: (i) the potential reusability of a class and (ii) the likelihood that a class will be affected by maintenance changes made to the overall system. The study indicates that coupling metrics can provide useful indications of both reusable classes and of classes that may have a significant influence on the effort expended during system maintenance and testing.", "num_citations": "23\n", "authors": ["13"]}
{"title": "Software quality assurance\n", "abstract": " Quality and quality assurance are difficult to define unambiguously in most industries, and this problem is even worse in the software industry. There is, also, a lack of agreement about the goals and functions of software quality assurance (SQA). SQA seems to have no distinctive role other than subjectively assessing the work of software developers and managers. The paper suggests that SQA should take responsibility for the company software metrics programme, and describes the research from the Alvey and the ESPRIT programmes which provide a useful starting point for QA staff. Not only would this give SQA staff a well-defined role, it would also provide the means to address the real issues which should be resolved to assure product quality, such as providing the means both to measure \u0393\u00c7\u00ffquality\u0393\u00c7\u00d6, and to obtain objective evidence of the costs and benefits of various software development techniques.", "num_citations": "23\n", "authors": ["13"]}
{"title": "Systematic reviews\n", "abstract": " During the past few years, I have become interested in evidence-based software engineering [5]. The evidence-based paradigm began in medicine and has totally revolutionised medical research [7]. It is now being adopted by a number of humancentered disciplines such as nursing, social policy, education etc. Although I have some reservations about whether software engineering can adopt fully the evidencebased paradigm, I am convinced that we should adopt certain aspects of the approach immediately. In particular, I believe we should adopt systematic review in place of ad hoc literature reviews, and recognise that a systematic review is a research method in its own right. Currently, all PhD students need to conduct a literature review as a part of their research but such reviews are seldom performed with the rigour now being required in other disciplines (including Information Systems Research, [6]). A\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "22\n", "authors": ["13"]}
{"title": "Preliminary results of a study of the completeness and clarity of structured abstracts\n", "abstract": " CONTEXT: Systematic literature reviews largely rely upon using the titles and abstracts of primary studies as the basis for determining their relevance. However, our experience indicates that the abstracts for software engineering papers are frequently of such poor quality they cannot be used to determine the relevance of papers. Both medicine and psychology recommend the use of structured abstracts to improve the quality of abstracts. AIM: This study investigates whether structured abstracts are more complete and easier to understand than non-structured abstracts for software engineering papers that describe experiments. METHOD: We constructed structured abstracts for a random selection of 25 papers describing software engineering experiments. The original abstract was assessed for clarity (assessed subjectively on a scale of 1 to 10) and completeness (measured with a questionnaire of 18 items) by the researcher who constructed the structured version. The structured abstract was reviewed for clarity and completeness by another member of the research team. We used a paired \u0393\u00c7\u00fft\u0393\u00c7\u00d6 test to compare the word length, clarity and completeness of the original and structured abstracts. RESULTS: The structured abstracts were significantly longer than the original abstracts (size difference =106.4 words with 95% confidence interval 78.1 to 134.7). However, the structured abstracts had a higher clarity score (clarity difference= 1.47 with 95% confidence interval 0.47 to 2.41) and were more complete (completeness difference=3.39 with 95% confidence intervals 4.76 to 7.56). CONCLUSIONS: The results of this study are consistent with previous\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "20\n", "authors": ["13"]}
{"title": "Evaluating software eng. methods and tools part 10: designing and running a quantitative case study\n", "abstract": " In the last article we considered how to identify the context for a case study and how to define and validate a case study hypothesis. In this article, we continue my discussion of the eight steps involved in a quantitative case study by considering the remaining six steps: selecting the host projects; identifying the method of comparison; minimising the effect of confounding factors, planning the case study, monitoring the case study, analysing the results.", "num_citations": "20\n", "authors": ["13"]}
{"title": "Assessment of a framework for comparing software architecture analysis methods\n", "abstract": " We have developed a framework, FOCSAAM, for comparing software architecture analysis methods. FOCSAAM can help architects and managers to choose a specific method to support architecture analysis process. We have been assessing the suitability of the framework\u0393\u00c7\u00d6s elements in different ways. During the development of FOCSAAM, a theoretical assessment was performed by relating each of its elements to the published literature on quality assurance, process improvement, and software development approaches. Moreover, we have also found that most of the elements of FOCSAAM can also be mapped onto the elements of a well-known framework for comparing information systems development methods, NIMSAD framework. Our goal of this study was to further assess the suitability of different elements of FOCSAAM by using the expert opinion approach. We asked 17 practicing architects with extensive experience to assess the suitability of the elements of FOCSAAM for selecting a particular method to support the software architecture analysis process. The findings of this study provide support for each element of FOCSAAM to be included in forming criteria for comparing software architecture analysis methods.", "num_citations": "19\n", "authors": ["13"]}
{"title": "Semi-quantitative simulation modeling of software engineering process\n", "abstract": " Software process simulation models hold out the promise of improving project planning and control. However, purely quantitative models require a very detailed understanding of the software process, i.e. process knowledge represented quantitatively. When such data is lacking, quantitative models impose severe constraints, restricting the model\u0393\u00c7\u00d6s value. In contrast, qualitative models display all possible behaviors but only in qualitative terms. This paper illustrates the value and flexibility of semi-quantitative modeling by developing a model of the software staffing process and comparing it with other quantitative staffing models. We show that the semi-quantitative model provides more insights into the staffing process and more confidence in the outcomes than the quantitative models by achieving a tradeoff between quantitative and qualitative simulation. In particular, the semi-quantitative simulation produces\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "19\n", "authors": ["13"]}
{"title": "Evaluating software engineering methods and tool\u0393\u00c7\u00f6part 4: the influence of human factors\n", "abstract": " In previous articles, we have described the range of methods available if you want to evaluate a software engineering method/tool and the criteria you need to consider to select a method appropriate to your individual circumstances. In future articles we will describe some guidelines to help you perform quantitative case studies and feature analysis. However, in this article we would like to review some of the human factors issues that can affect an evaluation exercise.", "num_citations": "19\n", "authors": ["13"]}
{"title": "Evidence-based software engineering and systematic literature reviews\n", "abstract": " This keynote addresses the evidence-based paradigm currently being adopted in many practical sciences (e.g., medicine, education, social policy) and discusses whether it is applicable to software engineering. In the presentation, the view is taken that although Evidence-based Software Engineering may be unproven, one aspect of the evidencebased paradigm is hard to ignore, that is: Systematic literature reviews. Systematic literature reviews aim to summarize research studies related to a specific research question in a way that is fair, rigorous, and auditable. The keynote presentation will outline the potential benefit of systematic literature reviews and describe in detail the process of performing such a systematic literature review.", "num_citations": "18\n", "authors": ["13"]}
{"title": "Semi-quantitative modeling for managing software development processes\n", "abstract": " Software process modeling has become an essential technique for managing software development processes. However, purely quantitative process modeling requires a detailed understanding and accurate measurement of software process, which relies on reliable and precise history data. This paper presents a semi-quantitative process modeling approach to model and manage software development processes. It allows for the existence of uncertainty and contingency during software development, and facilitates a manager's qualitative and quantitative estimates and assessments of process progress. We demonstrate its value and flexibility by developing semi-quantitative models of the test-and-fix process of incremental software development. Results conclude that the semi-quantitative process modeling approach can support process or project management activities, including estimating, planning, tracking\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "16\n", "authors": ["13"]}
{"title": "Three empirical studies on the agreement of reviewers about the quality of software engineering experiments\n", "abstract": " ContextDuring systematic literature reviews it is necessary to assess the quality of empirical papers. Current guidelines suggest that two researchers should independently apply a quality checklist and any disagreements must be resolved. However, there is little empirical evidence concerning the effectiveness of these guidelines.AimsThis paper investigates the three techniques that can be used to improve the reliability (i.e. the consensus among reviewers) of quality assessments, specifically, the number of reviewers, the use of a set of evaluation criteria and consultation among reviewers. We undertook a series of studies to investigate these factors.MethodTwo studies involved four research papers and eight reviewers using a quality checklist with nine questions. The first study was based on individual assessments, the second study on joint assessments with a period of inter-rater discussion. A third more formal\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "15\n", "authors": ["13"]}
{"title": "Planning software project success with semi-quantitative reasoning\n", "abstract": " Software process modeling and simulation hold out the promise of improving project planning and control. However, purely quantitative approaches require a very detailed understanding of the software project and process, including reliable and precise project data. Contemporary project management defines the success of project as a box or hyper-cube, rather than the traditional single point, which allows the planning for software project success semi-quantitatively with uncertainty-tolerance. This paper introduces semi-quantitative reasoning into software project planning and develops a practical approach to enhance the confidence of project success under the uncertainty and contingency. We illustrate its value and flexibility by a simplified software process model focusing on staffing issues.", "num_citations": "15\n", "authors": ["13"]}
{"title": "Measurement for software control and assurance\n", "abstract": " For many years software engineers and managers have paid lip service to the need for quantitative methods for software product and process control. However, in practice industry has been slow to adopt such methods. This reluctance to use measurement methods has been excused, with some justification, on the grounds that many proposed metrics and models were of dubious relevance and/or inadequately validated, and methods of data collection were expensive and unreliable.Many research workers and practitioners believe that quantitative approaches are now mature enough to be more widely used. In addition, the emergence of integrated project support environments (IPSEs) provide the framework in which many of the practical difficulties of data collection, storage and analysis can be avoided. The aim of the CSR Conference on Measurement for Software Assurance and Control\u0393\u00c7\u00f6and this record of the proceedings\u0393\u00c7\u00f6are to:", "num_citations": "15\n", "authors": ["13"]}
{"title": "Problems with statistical practice in human-centric software engineering experiments\n", "abstract": " Background Examples of questionable statistical practice, when published in high quality software engineering (SE) journals, may lead to novice researchers adopting incorrect statistical practices.Objective Our goal is to highlight issues contributing to poor statistical practice in human-centric SE experiments.Method We reviewed the statistical analysis practices used in the 13 papers that reported families of human-centric SE experiments and were published in high quality journals.Results Reviewed papers related to 45 experiments and involved a total of 1303 human participants. We searched for issues that were related to questionable statistical practice that were found in more than one paper. We observed three types of bad practice: incorrect use of terminology, incorrect analysis of repeated measures designs, and post-hoc power testing. We also found two analysis practices (ie, multiple testing and pre-testing\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "14\n", "authors": ["13"]}
{"title": "Toward trustworthy software process models: an exploratory study on transformable process modeling\n", "abstract": " Software process modeling and simulation have become effective tools for support of software process management and improvement over the past two decades. They have recently been integrated into the Trustworthy Process Management Framework (TPMF) as the infrastructural components to facilitate the delivery of trustworthy software products. This paper proposes the concept of Trustworthy Software Process Models as inputs to TPMF and introduces transformable process modeling for supporting effective and productive development of trustworthy process models. Furthermore, this paper undertakes an exploratory study on process model transformation by investigating and comparing process modeling semantics between quantitative (e.g., System Dynamics, SD) and qualitative forms of modeling and simulation. By following the model transformation scheme, a quantitative continuous (SD) software\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "14\n", "authors": ["13"]}
{"title": "Effect sizes and their variance for AB/BA crossover design studies\n", "abstract": " Vegas et al. IEEE Trans Softw Eng 42(2):120:135 (2016) raised concerns about the use of AB/BA crossover designs in empirical software engineering studies. This paper addresses issues related to calculating standardized effect sizes and their variances that were not addressed by the Vegas et al.\u0393\u00c7\u00d6s paper. In a repeated measures design such as an AB/BA crossover design each participant uses each method. There are two major implication of this that have not been discussed in the software engineering literature. Firstly, there are potentially two different standardized mean difference effect sizes that can be calculated, depending on whether the mean difference is standardized by the pooled within groups variance or the within-participants variance. Secondly, as for any estimated parameters and also for the purposes of undertaking meta-analysis, it is necessary to calculate the variance of the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "13\n", "authors": ["13"]}
{"title": "Lessons from conducting a distributed quasi-experiment\n", "abstract": " Context: Due to the lack of suitably skilled participants, software engineering experiments often lack the statistical power needed to detect the levels of effect that may be encountered. Aim: To investigate whether this can be remedied by running an experiment across multiple sites, organised as a single study rather than as a set of replications. Method: We performed a `trial' of the idea using a topic (structured abstracts) that some of us had studied previously and which required no participant training. We used five sites, each with 16 participants. Results: We were able to demonstrate the benefits of increased statistical power (and of structured abstracts). We report on our experiences with designing and conducting the study and identify some key lessons about how future studies of this form might be organised. Conclusions: The distributed model offers a flexible, robust form that is capable of delivering better\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "13\n", "authors": ["13"]}
{"title": "Experiences of using an evaluation framework\n", "abstract": " This paper reports two trials of an evaluation framework intended to evaluate novel software applications. The evaluation framework was originally developed to evaluate a risk-based software bidding model, and our first trial of using the framework was our evaluation of the bidding model. We found that the framework worked well as a validation framework but needed to be extended before it would be appropriate for evaluation. Subsequently, we compared our framework with a recently completed evaluation of a software tool undertaken as part of the Framework V CLARiFi project. In this case, we did not use the framework to guide the evaluation; we used the framework to see whether it would identify any weaknesses in the actual evaluation process. Activities recommended by the framework were not undertaken in the order suggested by the evaluation process and we found problems relating to that oversight\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "13\n", "authors": ["13"]}
{"title": "Validation, verification, and testing: diversity rules\n", "abstract": " Many software project managers try to decide whether to enhance reliability by performing detailed inspections or by doing execution-based testing using operational profiles. The authors regard this as a false choice. Operational-profile-based testing is an important method, but it is not a simple, cost-effective panacea. Instead, they suggest a better approach: a diverse validation, verification, and testing strategy that includes inspections and execution-based testing. Such an approach addresses the more appropriate question of what selection of W and T techniques should a project employ to achieve the functionality and quality that the product requires?.", "num_citations": "13\n", "authors": ["13"]}
{"title": "Software metrics and integrated project support environments\n", "abstract": " This paper considers how the information needs of managers may be helped by the development of integrated project support environments (IPSEs). In particular the paper concentrates on the issue of obtaining quantitative information about a software product and its development process using software metrics. The nature and use of software metrics and the problems of metrics data collection are discussed in terms of the authors' experience of an already existing metrics data collection system. The paper then considers the way in which metrics collection and analysis can be incorporated into IPSEs and the requirements that metrics systems place on the design of IPSEs.", "num_citations": "13\n", "authors": ["13"]}
{"title": "Realising evidence-based software engineering a report from the workshop held at ICSE 2005\n", "abstract": " Context: The workshop was held to explore the potential for adapting the ideas of evidence-based practices as used in medicine and other disciplines for use in software engineering. Objectives: To devise ways of developing suitable evidence-based practices and procedures, especially the use of structured literature reviews, and introducing these into software engineering research and practice. Method: Three sessions were dedicated to a mix of presentations based on position papers and interactive discussion, while the fourth focused upon the key issues as decided by the participants. Results: An initial scoping of the major issues, identification of useful parallels, and some plans for future development of an evidence-based software engineering community. Conclusions: While there are substantial challenges to introducing evidence-based practices, there are useful experiences to be drawn from a variety of\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "12\n", "authors": ["13"]}
{"title": "Comments on: evaluating alternative software production functions\n", "abstract": " Software development projects are notorious for cost overruns and schedule delays. While dozens of software cost models have been proposed, few of them seem to have any degree of consistent accuracy. One major factor contributing to this persistent and widespread problem is an inadequate understanding of the real behavior of software development processes. We believe that software development could be studied as an economic production process and that established economic theories and methods could be used to develop and validate software production and cost models. We present the results of evaluating four alternative software production models using the P-test, a statistical procedure developed specifically for testing the truth of a hypothesis in the presence of alternatives in econometric studies. We found that the truth of the widely used Cobb-Douglas type of software production and cost\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "12\n", "authors": ["13"]}
{"title": "Evaluation and assessment in software engineering\n", "abstract": " Software engineers find that experiments are difficult to perform. Furthermore, no experiment can be considered flawless no matter how well conducted. In this Special Issue the authors have addressed this problem as well as emphasising the need to undertake more empirical studies with discussing practical and methodological issues associated with evaluation.", "num_citations": "12\n", "authors": ["13"]}
{"title": "Research and practice: software design methods and tools\n", "abstract": " Publisher SummaryThis chapter focuses on research and practice associated with software design methods and tools. Programming is practised at different levels by different people. Almost anyone can write a program and get it to work but there are dramatic individual differences between programmers. The term software is a very broad term encompassing the executable machine code, the source code in a high-level language, the design documents, the test and management documents, and the user manuals for operating and maintaining the code. It refers, therefore, to the whole of the nonphysical component of a computing system. Most commercial and industrial software systems are such that several different people are involved in their development. The development takes place in an organization that has socio-economic goals and structures. It is a social as well as a technical and commercial exercise\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "12\n", "authors": ["13"]}
{"title": "Robust statistical methods: why, what and how: keynote\n", "abstract": " This keynote discusses the need for more robust statistical methods. For visualizing data I suggest using Kernel density plots rather than box plots. For parametric analysis, I propose more robust measures of central location such as trimmed means, which can support reliable tests of the differences between the central location of two or more samples. In addition, I also recommend non-parametric effect sizes such as Cliff's \u256c\u2524 and Brunner and Munzel's p-hat that avoid some of the problems with rank-based non-parametric methods.", "num_citations": "11\n", "authors": ["13"]}
{"title": "Reproducible research\u0393\u00c7\u00f4what, why and how\n", "abstract": " The minimum criterion for valid research and data analyses in software engineering and other disciplines is reproducibility. The aim of this paper is to explain the concept of Reproducible Research (RR), embracing reporting modern data analyses in a reproducible manner, its importance, and how researchers and data analysts can use current technology to adopt RR and increase reproducibility of their data analysis tasks.", "num_citations": "11\n", "authors": ["13"]}
{"title": "Cross-domain investigation of empirical practices\n", "abstract": " The authors are seeking the best ways to employ evidence-based practices in software engineering research and practice so that the outcomes can inform practice and policy-making. The objective of this study is to investigate how other academic disciplines use evidence-based practices in order to help assess the guidelines that the authors have developed for conducting systematic literature reviews in software engineering. They undertook two studies to investigate how other domains used evidence-based practices. One used a questionnaire that was administered to a set of experts, and this was then followed up with a study that used semi-structured interviews to gain a deeper understanding. As a result, the authors have identified how a number of disciplines that experience similar empirical constraints to those that apply to software engineering employ and rank different forms of empirical data. In conclusion\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "11\n", "authors": ["13"]}
{"title": "Objectivity in research: Challenges from the evidence-based paradigm\n", "abstract": " For other domains that have adopted the evidence-based paradigm, the impact has included research outcomes having greater influence in terms of informing and influencing practitioners and policy-makers. We examine how evidence-based practices are being adapted for use in software engineering and discuss how decision-making in our own discipline can be liberated from over-reliance on expert judgement. To support our arguments we discuss some outcomes from recent studies and present an example in which performing a systematic literature review demonstrates the unreliability of depending only upon the outcomes of individual studies. Finally, we identify six challenges that need to be addressed in order to provide software engineers with standards and practices that are underpinned by evidence.", "num_citations": "11\n", "authors": ["13"]}
{"title": "Meta-analysis for families of experiments in software engineering: a systematic review and reproducibility and validity assessment\n", "abstract": " Context                 Previous studies have raised concerns about the analysis and meta-analysis of crossover experiments and we were aware of several families of experiments that used crossover designs and meta-analysis.                                               Objective                 To identify families of experiments that used meta-analysis, to investigate their methods for effect size construction and aggregation, and to assess the reproducibility and validity of their results.                                               Method                 We performed a systematic review (SR) of papers reporting families of experiments in high quality software engineering journals, that attempted to apply meta-analysis. We attempted to reproduce the reported meta-analysis results using the descriptive statistics and also investigated the validity of the meta-analysis process.                                               Results                 Out of 13 identified primary studies\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "10\n", "authors": ["13"]}
{"title": "Problems adopting metrics from other disciplines\n", "abstract": " In this paper, we argue that metrics validation approaches used in software engineering are problematic. In particular, theoretical validation is not rigorous enough to detect invalid metrics and empirical validation has no mechanism for making any final decisions about the validity of metrics. In addition, we argue that cohesion and information-theoretic metrics are problematic if they are based on mathematical graphs which do not consider program semantics. We conclude that we should not adopt metrics from other disciplines if we cannot validate them properly. We propose the use of the representation condition as a means to demonstrate metrics that are not valid. We also believe that design metrics must make sense to software designers or, even if they are valid, they will not be used.", "num_citations": "10\n", "authors": ["13"]}
{"title": "Distributed versus face-to-face meetings for architecture evalution: a controlled experiment\n", "abstract": " Scenario-based methods for evaluating software architecture require a large number of stakeholders to be collocated for evaluation sessions. Collocating stakeholders is often an expensive exercise. To reduce expense, we have proposed a framework for supporting software architecture evaluation process using groupware systems. This paper presents a controlled experiment that we conducted to assess the effectiveness of scenario profile construction using distributed meetings. We used a cross-over experiment involving 32 teams of three 3rd and 4th year undergraduate students. We found that the quality of scenarios produced by distributed teams using a groupware tool were significantly better than the quality of scenarios produced by face-to-face teams (p< 0.001). However, questionnaires indicated that most participants preferred the face-toface arrangement (82%) and 60% thought the distributed meetings\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "10\n", "authors": ["13"]}
{"title": "Realising evidence-based software engineering\n", "abstract": " This paper provides an introduction to the papers for the Workshop on Realising Evidence-Based Software Engineering.", "num_citations": "10\n", "authors": ["13"]}
{"title": "Program history records: a system of software data collection and analysis\n", "abstract": " This paper describes a semi-automatic system of software data collection and analysis which has been in use in VME production projects since March 1981. The paper outlines the nature of the system itself and indicates the way information about the software development task has been used in practice. It concludes by indicating some of the problems that have been encountered in attempting to use the system.", "num_citations": "10\n", "authors": ["13"]}
{"title": "Risk mitigation advice for global software development from systematic literature reviews\n", "abstract": " Objective: We wish to identify what risk mitigation advice is provided to organizations in order to increase the likelihood of GSD project success.Method: We extend an earlier tertiary systematic review (SLR) in GSD to extract data from the SLR studies identified in order to discover: 1) what advice is provided to practitioners to increase the chances of project success, 2) what is the strength of the advice, and 3) to whom is the advice addressed. Results: We found 428 advice items that we categorized under four major headings: outsourcing rationale, software development, human resources, and project management. Most of the suggestions were related to agile methods (16%) and communication and collaboration (13%). Conclusions: The majority of our SLRs focussed on mapping the research rather than providing guidance to industry. Suggestions are mainly for vendor project managers and vendor practitioners. There is low empirical support for most of the advice provided.", "num_citations": "9\n", "authors": ["13"]}
{"title": "Experimenter induced distortions in empirical software engineering\n", "abstract": " This paper discusses distortions and biases that occur when people perform experiments and other empirical studies using other people as subjects. There has been considerable research into these issues by researchers in the field of psychology and sociology but little discussion of the problem this issue poses for empirical software engineering. We explain the impact the distortions and bias have on experiments. We, then, report the approaches taken to specify and control distortions and bias in psychology and sociology, and discuss the extent to which the problems and proposed solutions apply to software engineering studies. We conclude that software engineering researchers should consider these issues when designing and reporting empirical studies.", "num_citations": "9\n", "authors": ["13"]}
{"title": "Tool features to support systematic reviews in software engineering\u0393\u00c7\u00f4a cross domain study\n", "abstract": " Context: Previously, the authors had developed and evaluated a framework to evaluate systematic review (SR) lifecycle tools.Goal: The goal of this study was to use the experiences of researchers in other domains to further evaluate and refine the evaluation framework.Method: The authors investigated the opinions of researchers with experience of systematic reviews in the healthcare and social sciences domains. They used semi-structured interviews to elicit their experiences of systematic reviews and SR support tools.Results: Study participants found broadly the same problems as software engineering (SE) researchers with the SR process. They agreed with the tool features included in the evaluation framework. Furthermore, although there were some differences, the majority of the importance assessments were very close.Conclusions: In the context of SRs, the experiences of researchers in other domains can be useful to software engineering researchers. The evaluation framework for SR lifecycle tools appeared quite robust.", "num_citations": "8\n", "authors": ["13"]}
{"title": "Corrections to effect size variances for continuous outcomes of cross-over clinical trials\n", "abstract": " We would like to make some corrections to the formulas presented in the 2002 Statistics in Medicine article by Curtin et al.[1]. That article presented formulas for the variances of standardized weighted mean difference of an AB/BA cross-over trial that would be comparable both with parallel designs and cross-over designs. There are three main issues in the Curtin et al.\u0393\u00c7\u00d6s paper [1] that we address in this communication. Firstly, the paper proposes a standardized effect size for cross-over trials that is inconsistent with the standardized effect sizes used for other repeated measures designs such as the pretest-posttest studies used in educational studies, see [2] and [3]. Secondly, the change to the standardized effect size for cross-over studies necessitates a change to variance of the standardized effect size. Thirdly, the variance of the standardized effect size comparable with parallel trials was not based on the distribution of a valid t-variable, so includes some errors. We follow Curtin et al.\u0393\u00c7\u00d6s approach and base our revised variance equations on the moments of the non-central t-distribution, replacing the t-variable with a variable based on the effect size.", "num_citations": "8\n", "authors": ["13"]}
{"title": "Evidence-based Global Software Engineering Risks Extracted from Systematic Literature Reviews\n", "abstract": " Global software development (GSD) projects are often largescale, tend to be complex, and have an increased risk of failure. An understanding of common GSD risks can help project managers control the risks and thus increase the likelihood of project success. We extend an earlier tertiary GSD systematic literature review (SLR) to extract data from the SLR studies identified to discover, 1) what are the most common risks described in the SLRs, 2) what empirical evidence is there for the risks, and 3) who is responsible for controlling the risk. The 123 risks we found were categorized under four major themes 1) GSD outsourcing rationale, 2) software development, 3) human resources, and 4) project management. Most risks (32%) were related to project management. The risk with the most empirical support was a high level strategy risk. Mapping the research rather than providing industry guidance was the focus of the majority of the GSD SLRs.", "num_citations": "8\n", "authors": ["13"]}
{"title": "Qualitative vs. quantitative software process simulation modeling: Conversion and comparison\n", "abstract": " Software process simulation modeling (SPSM) research has increased in the past two decades. However, most of these models are quantitative, which require detailed understanding and accurate measurement. As the continuous work to our previous studies in qualitative modeling of software process, this paper aims to investigate the structure equivalence and model conversion between quantitative and qualitative process modeling, and to compare the characteristics and performance of these two approaches by modeling and simulating a software evolution process. Following the model conversion scheme, the reference quantitative (SD) model and the corresponding qualitative model become comparable. The results present their different capabilities and interesting perspectives, and further the potential use of qualitative modeling in software process research.", "num_citations": "8\n", "authors": ["13"]}
{"title": "2nd internationalworkshop on realising evidence-based software engineering (rebse-2): Overview and introduction\n", "abstract": " The REBSE international workshops are concerned with exploring the adaptation and use of the evidence-based paradigm in software engineering research and practice, through a mix of presentations and discussion. Here, we provide some background about evidence-based software engineering and its current state.", "num_citations": "8\n", "authors": ["13"]}
{"title": "Quality requirements specification and evaluation\n", "abstract": " This paper describes a method of specifying the non-functional requirements for a software product in unambiguous and measurable terms. This method has been implemented within a prototype Quality Management System, developed as part of the work of the Alvey Test Specification and Quality Management project. The paper also describes an experimental approach to evaluating such a requirements specification, based on the relationships between various quality factors and the software engineering techniques which may be used to support them.", "num_citations": "8\n", "authors": ["13"]}
{"title": "Is evidence based software engineering mature enough for practice & policy?\n", "abstract": " Since their use was first proposed in 2004, evidencebased practices, and particularly systematic literature reviews, are becoming widely used in empirical software engineering. We describe the key concepts of evidence-based software engineering (EBSE) and its use of systematic literature reviews (SLRs) and mapping studies. We then report on the extent to which evidence-based studies are changing our ideas, and the extent to which they have so far addressed major software engineering themes and topics. We discuss how far this can already be used to inform policy and practice, and what further developments could usefully reinforce this role.", "num_citations": "7\n", "authors": ["13"]}
{"title": "Lessons from a cross-domain investigation of empirical practices\n", "abstract": " Context: We are seeking the best ways to employ evidence-based practices in software engineering research and practice. Objectives: To help assess our guidelines for conducting systematic literature reviews we have investigated how other academic disciplines use evidence-based practices. Method: This involved performing two studies, one using a questionnaire with a set of experts, and a second using semi-structured interviews. Results: We have identified how disciplines with similar empirical constraints to software engineering place weight upon different forms of empirical data. Conclusions: We describe both the resulting changes to our systematic literature review guidelines and some issues this raises for empirical software engineering.", "num_citations": "7\n", "authors": ["13"]}
{"title": "The impact of group size on software architecture evaluation: a controlled experiment\n", "abstract": " An important element in scenario-based architecture evaluation is the development of scenario profiles by stakeholders working in groups. In practice groups can vary in size from 2 to 20 people. Currently, there is no empirical evidence about the impact of group size on the scenario development activity. Our experimental goal was to investigate the impact of group size on the quality of scenario profiles developed by different sizes of groups. We had 165 subjects, who were randomly assigned to 10 groups of size 3, 13 groups of size 5, and 10 groups of size 7. Participants were asked to develop scenario profiles. After the experiment each participant completed a questionnaire aimed at identifying their opinion of the group activity. The average quality score for group scenario profiles for 3 person groups was 362.4, for groups of 5 person groups was 534.23 and for 7 person groups was. 444.5. The quality of scenario\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "7\n", "authors": ["13"]}
{"title": "An exploratory study of groupware support for distributed software architecture evaluation process\n", "abstract": " Software architecture evaluation is an effective means of addressing quality related issues quite early in the software development lifecycle. Scenario-based approaches to evaluate architecture usually involve a large number of stakeholders, who need to be collocated for evaluation sessions. Collocating a large number of stakeholders is an expensive and time-consuming exercise, which may prove to be a hurdle in the wide-spread adoption of architectural evaluation practices. Drawing upon the successful introduction of groupware applications to support geographically distributed teams in software inspection, and requirements engineering disciplines, we propose the concept of distributed architectural evaluation using Internet-based collaborative technologies. This paper illustrates the methodology of a pilot study to assess the viability of a larger experiment intended to investigate the feasibility of groupware\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "7\n", "authors": ["13"]}
{"title": "Never mind the metrics what about the numbers!\n", "abstract": " Many of the other papers in this volume consider the issues of formality with respect to measurement theory. This paper takes the view that it is equally important to have a formalism for dealing with the values obtained from the measurement process as it is to have a formalism for generating the values. This paper argues that statistics offers the appropriate formalism for dealing with such values because the models we use to interpret measurements are inherently stochastic not deterministic.               In this paper, I examine the mistakes that the software engineering community makes when it ignores the proper formulation and testing of non-deterministic models using examples from the domain of software cost estimation. The empirical work described in this paper is part of the ESPRIT MERMAID project. MERMAID is a collaborative project aimed at developing and automating improved methods of cost\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "7\n", "authors": ["13"]}
{"title": "Protocol for systematic review of within-and cross-company estimation models\n", "abstract": " 1. BackgroundEarly studies of cost estimation models (see for example, Kitchenham and Taylor, 1984, or Kemerer, 1987) suggested that general purpose models such as COCOMO (Boehm, 1981) and SLIM (Putnam, 1978) needed to be calibrated to specific companies before they could be used effectively. Taking this result further and following the suggestions made by DeMarco (1982), Kok et al.(1990) suggested that cost estimation models should be developed only from within-company data. However, the problem with company-specific estimation models is that it presupposes that companies are able to collect sufficient data to construct such models.In 1999, Maxwell et al. took a new look at the issue by analysing a multi-company benchmarking database and comparing the accuracy of a within-company model with the accuracy of a cross company model. They found the within-company model to be more accurate than the cross-company model for the specific company. In the same year, Briand and his co-workers published a report suggesting that cross-company models could be as accurate as within-company models (Briand et al., 1999). The following year, he confirmed his result on a different data set (Briand et al., 2000). Two years later Wieczorek and Ruhe (2002) confirmed the same trend using the same database employed by Briand et al.(1999). These results seemed to contradict the results of the earlier studies and pave the way for improved estimation methods for companies who did not have their own project data. However, other researchers found less encouraging results. Jeffery and his co-workers undertook two studies, both\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "6\n", "authors": ["13"]}
{"title": "Interpretation problems related to the use of regression models to decide on economy of scale in software development\n", "abstract": " Many research studies report an economy of scale in software development, i.e., an increase in productivity with increasing project size. Several software practitioners seem, on the other hand, to believe in a diseconomy of scale, i.e., a decrease in productivity with increasing project size. In this paper we argue that violations of essential regression model assumptions in the research studies to a large extent may explain this disagreement. Particularly illustrating is the finding that the use of the production function (Size\u252c\u00e1=\u252c\u00e1a\u252c\u2556Effortb), instead of the factor input model (Effort\u252c\u00e1=\u252c\u00e1a\u252c\u2556Sizeb), would most likely have led to the opposite result, i.e., a tendency towards reporting diseconomy of scale in the research studies. We conclude that there are good reasons to warn against the use of regression analysis parameters to investigate economies of scale and to look for other analysis methods when studying economy of scale in\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "6\n", "authors": ["13"]}
{"title": "Management metrics\n", "abstract": " The purpose of this paper is to discuss the ways in which metrics might be utilized to assist the management of software development. The paper will discuss the types of metrics useful to managers and will give examples of the ways in which these metrics are used within ICL. It will finish with a description of the practical problems involved in data collection.The majority of the results in this paper are based on measure-ments of the VME Operating System and the production group who maintain and enhance it. VME is ICL's general purpose operating system for the 2900 series computers. Work on VME started in the late 1960s and incorporated such features as top down design, structured programming and implementation in a high level language (a varient of Algol 68). In addition, the development process has been controlled since the start of development by the CADES system (McGuffin et al. 1980). CADES is a database system which records the design of VME, maintains interface, data and mode information, and provides configuration control features.", "num_citations": "6\n", "authors": ["13"]}
{"title": "A framework for adopting software process simulation in cmmi organizations\n", "abstract": " The Capability Maturity Model Integration (CMMI) has become very influential as a basis for software process improvement. It is accepted that process maturity is associated with better project performance and organizational performance. Software process simulation is being applied to the management of software projects, product life cycles, and organizations. This paper argues that the successful adoption of one particular simulation paradigm to a large extent depends on an organization\u0393\u00c7\u00d6s capability maturity. We investigate four typical simulation paradigms and map them to their appropriate CMMI maturity levels. We believe that an understanding of these relationships helps researchers and practitioners in implementing and institutionalizing process simulation in software organizations.", "num_citations": "5\n", "authors": ["13"]}
{"title": "Estimating Project Outcomes\n", "abstract": " In spite of many years of research, many software projects still fail. As the goal of this paper is to contribute to ongoing research into the identification of factors that affect project success and failure, ie project outcome, we undertook a correlation study of project variables and project outcome. Our data consisted of three data sets. We developed two logistic models for the combined data selected by stepwise regression: one based on three factors and the other based on four of the raw variables. The models were compared for predictive accuracy. Twelve variables significantly associated with project success and failure, in all data sets, were identified. We found that logistic models did not predict failure well, unless the cut-off probability for classifying a project as successful corresponded to the proportion of successful projects in the data set. Only a limited number of the 12 key variables predicted project success and failure. A much larger set of variables appear not to impact project outcome.", "num_citations": "5\n", "authors": ["13"]}
{"title": "Assessing the value of architectural information extracted from patterns for architecting\n", "abstract": " Background: We have developed an approach to identifying and capturing architecturally significant information from patterns (ASIP), which can be used to improve architecture design and evaluation.Goal: Our goal was to evaluate whether the use of the ASIP provides more effective support in understanding or designing software architecture composed of the software design patterns which are the source of the ASIP compared with the original design pattern documentation.Experimental design: Our subjects were 20 experienced software engineers who had returned to University for a post graduate course. All participants were taking a course in software architecture. The participants were randomly assigned to two groups of equal size. Both groups performed two tasks: understanding the use of J2EE design pattern in a given architecture based on the quality requirements the architecture was supported to satisfy\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "5\n", "authors": ["13"]}
{"title": "Evolutive Maintenance: Observing Object-Oriented Software Decay\n", "abstract": " While the maintenance refers to the activities that happen in any time after the implementation of a new software development project, the software evolution is defined by the exam of the systems characteristics dynamic behavior and how they change along the time.The Laws of Software Evolution (LSE) describe how a system behaves throughout their successive versions (LEHMAN, 1980). Works found in the literature makes reference to software evolution experimental studies just considering the legacy systems\u0393\u00c7\u00d6 source code (KEMERER & SLAUGHTER, 1999)(SCACHI, 2003). Besides, LEHMAN & RAMIL (2002) have been pointing out the need for evolution studies regarding object-oriented systems and in other software development process phases (LEHMAN & RAMIL, 2003). Due to these characteristics, decay causes\u0393\u00c7\u00d6 study throughout object-oriented development processes becomes relevant, providing us a better understanding of how this type of software evolves.", "num_citations": "5\n", "authors": ["13"]}
{"title": "A preliminary risk-based software bidding model\n", "abstract": " This document outlines a preliminary risk-based software bidding model. It is based on the requirements for a bidding model identified by a review of bidding practices in software and other industries. The model is presented as a generic contributing-factor model that can be specialized by determining the distribution of input variables. The modelling method is explained by reference to a terrorist risk model.", "num_citations": "5\n", "authors": ["13"]}
{"title": "Evaluating software engineering methods and tools, part 11: analysing quantitative case studies\n", "abstract": " This article considers the issue of analysing and reporting case study results. It considers the problem of identifying an orgauisation profile for host project selection and the approaches you can take to analysing the three different case study designs: companybaseline designs, within-project component comparison designs, sister project designs. However, analysing and reporting the results of a case study is not the end of the evaluation exercise. The purpose of an evaluation exercise is to allow someone to make an informed decision about adoption of a new technology. Thus, you will need to ensure that not onlydo you analyse your data appropriately but that you present your conclusions in manner that will allow the recipients of your report to make their decisions effectively. So although this article is full of statistical terminology, remember to keep your report to your case study sponsor as straightforward and\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "5\n", "authors": ["13"]}
{"title": "Protocol for extending an existing tertiary study of systematic literature reviews in software engineering\n", "abstract": " This document is a protocol for extending a completed tertiary study that investigated the adoption of evidence-based software engineering and, in particular, the use of systematic literature reviews to aggregate software engineering primary studies. The original study was based on manual search of 13 journals and conference proceedings. This protocol defines a plan to extend the original tertiary study to include additional primary studies found by an electronic search of multiple digital libraries.", "num_citations": "4\n", "authors": ["13"]}
{"title": "EPIC Case Study 2\u0393\u00c7\u00f4Extension of a Tertiary Study\n", "abstract": " CiteSeerX \u0393\u00c7\u00f6 EPIC Case Study 2 \u0393\u00c7\u00f4 Extension of a Tertiary Study Documents Authors Tables Log in Sign up MetaCart DMCA Donate CiteSeerX logo Documents: Advanced Search Include Citations Authors: Advanced Search Include Citations Tables: DMCA EPIC Case Study 2 \u0393\u00c7\u00f4 Extension of a Tertiary Study (2008) Cached Download as a PDF Download Links [community.dur.ac.uk] [community.dur.ac.uk] Save to List Add to Collection Correct Errors Monitor Changes by Barbara Kitchenham , O. Pearl Brereton , Mark Turner Citations: 1 - 1 self Summary Citations Active Bibliography Co-citation Clustered Documents Version History Share Facebook Twitter Reddit Bibsonomy OpenURL Abstract and Keyphrases epic case study tertiary study Powered by: Apache Solr About CiteSeerX Submit and Index Documents Privacy Policy Help Data Source Contact Us Developed at and hosted by The College of Information and \u252c\u2310 \u0393\u00c7\u00aa", "num_citations": "4\n", "authors": ["13"]}
{"title": "Achieving software project success: a semi-quantitative approach\n", "abstract": " Software process modeling and simulation hold out the promise of improving project planning and control. However, purely quantitative approaches require a very detailed understanding of the software project and process, including reliable and precise project data. Contemporary project management defines the success of project as a cube, rather than the traditional single point, which allows the management of software project semi-quantitatively with uncertainty-tolerance. This paper introduces semi-quantitative simulation into software project planning and control, and develops a practical approach to enhance the confidence of project success under uncertainty and contingency. We illustrate its value and flexibility by an example implementation with a simplified software process model.", "num_citations": "4\n", "authors": ["13"]}
{"title": "The current state of evidence-based software engineering\n", "abstract": " Background\u0393\u00f9\u00ef At ICSE04\u0393\u00f9\u00c5 Kitchenham, Dyb\u251c\u00d1, and J\u251c\u2555rgensen, proposed adopting Evidence-Based Software Engineering (EBSE)\u0393\u00f9\u00c5 Followed by papers at Metrics05 and in IEEE software", "num_citations": "4\n", "authors": ["13"]}
{"title": "Web Productivity Measurement and Benchmarking\n", "abstract": " Project managers use software productivity measures to assess software development efficiency. Productivity is commonly measured as the ratio of output to input. Within the context of software development, output is often assumed to be product size and input to be effort. However, Web applications are often characterised using several different size measures and there is no standard model for aggregating those measures into a single size measure. This makes it difficult to measure Web application productivity.           In this chapter, we present a productivity measurement method, which allows for the use of different size measures. An advantage of the method is that it has a built-in interpretation scale. It ensures that each project has an expected productivity value of one. Values between zero and one indicate lower than expected productivity; values greater than one indicate higher than expected\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "4\n", "authors": ["13"]}
{"title": "\u2500\u2591 Kitchenham, 2004 Procedures for Performing Systematic Reviews\n", "abstract": " CiteSeerX \u0393\u00c7\u00f6 \u2500\u2591 Kitchenham, 2004 Procedures for Performing Systematic Reviews Documents Authors Tables Log in Sign up MetaCart DMCA Donate CiteSeerX logo Documents: Advanced Search Include Citations Authors: Advanced Search Include Citations Tables: DMCA \u2500\u2591 Kitchenham, 2004 Procedures for Performing Systematic Reviews (2004) Cached Download as a PDF Download Links [www.idi.ntnu.no] [tests-zingarelli.googlecode.com] [www.inf.ufsc.br] [www.eecis.udel.edu] [people.ucalgary.ca] Save to List Add to Collection Correct Errors Monitor Changes by Barbara Kitchenham Summary Citations Active Bibliography Co-citation Clustered Documents Version History Share Facebook Twitter Reddit Bibsonomy OpenURL Abstract Keyphrases systematic review Powered by: Apache Solr About CiteSeerX Submit and Index Documents Privacy Policy Help Data Source Contact Us Developed at and hosted by -\u0393\u00c7\u00aa", "num_citations": "4\n", "authors": ["13"]}
{"title": "Editorial special issue: empirical software engineering\n", "abstract": " One of the paradoxes of software engineering is that, although it extensively employs concepts and practices that are drawn from experience and observation (for example, Information Hiding, Design Patterns, etc.), we rarely possess any empirical validation of these ideas that could link theory and concepts to observed practices. Although the last decade has seen a growth in interest in adopting empirical techniques for use in software engineering, these are still far from being incorporated into mainstream practice, even for academic research projects. Also, while a few areas have been subjected to quite extensive empirical study (most notably that of Inspection Techniques), many key concepts and practices have had little attention (for example, the CMM \u0393\u00c7\u00ffquality\u0393\u00c7\u00d6culture; the use of design patterns; the usability of UML).The nature of software, and hence of software development, and in particular the complexity that\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "4\n", "authors": ["13"]}
{"title": "Can Software Bidding Practices be Improved?\u0393\u00c7\u00f6Bidding and Portfolio Management Practices in Software and Other Industries\n", "abstract": " We report the results of a literature survey that reviewed bidding processes and portfolio management processes in a variety of different industries including the make-to-order engineering industry, the pharmaceutical industry, the finance industry, insurance industry, the oil industry, shipbuilding, and construction. As a starting point we reviewed the bidding process and portfolio management procedures used in the software industry. The goal of the wider survey was to identify whether any of the models or methods used in other industries to improve the bidding process and manage portfolios could be used to assist bidding and portfolio management in the custom-built software industry. We found some industries in which bidding and pricing are the most significant issues and others where portfolio management is the most important issue. Only the oil industry seemed equally concerned about both issues. Each industry developed models and methods based on its own specific circumstances, so we found no methods or models that can be used \u0393\u00c7\u00a3as-is\u0393\u00c7\u00a5 for software projects. However, we found useful general approaches and strategies in all industries. From the viewpoint of bidding and pricing practices, software practices could benefit by developing pricing models that include consideration of delivery dates, the probability of a successful bid, and the impact of other commitments (actual and prospective). We should also consider the strategic and practical implications of \u0393\u00c7\u00a3over-booking\u0393\u00c7\u00a5 ie bidding for more work than can be done in case some bids fail. From the viewpoint of portfolio management, we can see from the oil industry that it is possible to\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "4\n", "authors": ["13"]}
{"title": "Experiences introducing a measurement program\n", "abstract": " Experiences introducing a measurement program \u251c\u00f9 Close The Infona portal uses cookies, ie strings of text saved by a browser on the user's device. The portal can access those files and use them to remember the user's data, such as their chosen settings (screen view, interface language, etc.), or their login data. By using the Infona portal the user accepts automatic saving and using this information for portal operation purposes. More information on the subject can be found in the Privacy Policy and Terms of Service. By closing this window the user confirms that they have read the information on cookie usage, and they accept the privacy policy and the way cookies are used by the portal. You can change the cookie settings in your browser. I accept Polski English Login or register account remember me Password recovery INFONA - science communication portal INFONA Search advanced search Browse series books . \u0393\u00c7\u00aa", "num_citations": "4\n", "authors": ["13"]}
{"title": "Metrics and measurement\n", "abstract": " 30 Metrics and measurement Barbara A Kitchenham Software Metrics Consultant National Computing Centre, Manchester Contents 30.1 30.2 30.3 30.4 30.5 30.6 30.7 30.8 30.9 Introduction 30/3 Metrics and measurement systems 30/3 Nature and use of software metrics 30/3 Practical constraints 30/4 A pragmatic approach to metrics 30/5 Metrics for project control 30/6 An example of metrics analysis and interpretation 30/7 Metrics selection 30/7 Conclusions 30/8 30.10 Appendix: metrics descriptions 30/8 30.11 References 30/11 30.12 Acknowledgements 30/11 Nature and use of software metrics 30/3 30.1 Introduction This chapter describes software metrics from the viewpoint of their contribution to project monitoring and control. This means that a number of areas where measurement is used in software production and support are not considered. In particular, readers requiring information about reliability\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "4\n", "authors": ["13"]}
{"title": "Why reproducible research is beneficial for security research\n", "abstract": " Researchers in software engineering and other disciplines have identified problems with the validity of published research findings. Similar problems occur in security and privacy research. In any science, reproducibility (understood as ability to access the existing data set, to use existing tools with the study-\u0393\u00c7\u00c9prescribed defaults and settings, and to apply the study-\u0393\u00c7\u00c9prescribed processes to double-\u0393\u00c7\u00c9check the results and the study-\u0393\u00c7\u00c9prescribed processes and settings) is a minimum criterion for valid research. This paper explains the concept of Reproducible Research (RR), its importance, and how researchers can use current technology to adopt RR to security and privacy. It concludes that RR does not cure all the ills of current security and privacy research practice. But it offers a start toward good scientific practice and more effective methods and tools.", "num_citations": "3\n", "authors": ["13"]}
{"title": "A quality checklist for technology-centred testing studies\n", "abstract": " Background:  One aspect of undertaking a systematic literature review is to perform a quality evaluation of primary studies. Most quality checklists adopted from medicine, psychology and social studies assume that the experimental unit in an experiment is a human being. However, in empirical studies in software engineering the experimental unit may be a technology, an application or an algorithm.  Aim:  This paper presents a checklist we are developing to evaluate the quality of empirical technology-centred testing studies.  Discussion points:  The checklist was developed by considering entities used in technology-centred testing studies and the validation problems associated with them. The planned validation process includes face validation, usability and reliability assessment and external validation. As yet the external validation has not been performed.  Conclusions:  The checklists appear to be usable and after some experience applying them they appear to give consistent results. However, their validation is as yet incomplete. The method of developing and evaluating the checklist may be of use to other researchers requiring a means of assessing the quality of technology-centred software engineering studies.", "num_citations": "3\n", "authors": ["13"]}
{"title": "Introduction to special section on Evaluation and Assessment in Software Engineering EASE06\n", "abstract": " Editorial: Introduction to special section on Evaluation and Assessment in Software Engineering EASE06: Journal of Systems and Software: Vol 80, No 9 ACM Digital Library home ACM home Google, Inc. (search) Advanced Search Browse About Sign in Register Advanced Search Journals Magazines Proceedings Books SIGs Conferences People More Search ACM Digital Library SearchSearch Advanced Search Journal of Systems and Software Periodical Home Latest Issue Archive Authors Affiliations Award Winners More HomeBrowse by TitlePeriodicalsJournal of Systems and SoftwareVol. , No. Editorial: Introduction to special section on Evaluation and Assessment in Software Engineering EASE06 article Editorial: Introduction to special section on Evaluation and Assessment in Software Engineering EASE06 Share on Authors: Barbara Kitchenham School of Computing and Mathematics, University of Keele, , . .: \u0393\u00c7\u00aa", "num_citations": "3\n", "authors": ["13"]}
{"title": "Software engineering for large software systems\n", "abstract": " These proceedings include tutorials and papers presented at the Sixth CSR Confer ence on the topic of Large Software Systems. The aim of the Conference was to identify solutions to the problems of developing and maintaining large software systems, based on approaches which are currently being undertaken by software practitioners. These proceedings are intended to make these solutions more widely available to the software industry. The papers from software practitioners describe:\u0393\u00c7\u00f3 important working systems, highlighting their problems and successes;\u0393\u00c7\u00f3 techniques for large system development and maintenance, including project management, quality management, incremental delivery, system security, in dependent V & V, and reverse engineering. In addition, academic and industrial researchers discuss the practical impact of current research in formal methods, object-oriented design and advanced environ ments. The keynote paper is provided by Professor Brian Warboys of ICL and the University of Manchester, who masterminded the development of the ICL VME Operating System, and the production of the first database-driven software en gineering environment (CADES). The proceedings commence with reports of the two tutorial sessions which preceded the conference:\u0393\u00c7\u00f3 Professor Keith Bennett of the Centre for Software Maintenance at Durham University on Software Maintenance;\u0393\u00c7\u00f3 Professor John McDermid of the University of York on Systems Engineering Environments for High Integrity Systems. The remaining papers deal with reports on existing systems (starting with Professor Warboys' keynote paper), approaches to large\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "3\n", "authors": ["13"]}
{"title": "What we can learn from systematic reviews\n", "abstract": " As a strong advocate of evidence-based software engineering ([Dyb\u251c\u00d1 et al. 2005],[Kitchenham et al. 2004]), I am also a strong advocate of systematic reviews (SRs)([Kitchenham 2004],[Kitchenham and Charters 2007]). These are sometimes referred to as systematic literature reviews in software engineering to avoid confusions with inspection methods (ie, methods for reading and reviewing software engineering documents or code). We cannot have evidence-based software engineering without a sound methodology for aggregating evidence from different empirical studies. SRs provide that methodology.SRs have been in widespread use in other disciplines for decades. Each SR is launched by a researcher to investigate all available evidence that supports or refutes a particular \u0393\u00c7\u00a3topic of interest,\u0393\u00c7\u00a5 which in software engineering typically involves asking about the effect of a method or process. A researcher conducting an SR selects empirical studies that are relevant to the particular research question, assesses the validity of each one, and then determines the trend shown by those studies. Thus, SRs aim to find, assess, and aggregate all relevant evidence about some topic of interest in a fair, repeatable, and auditable manner.", "num_citations": "2\n", "authors": ["13"]}
{"title": "Results from Case Study 1\u0393\u00c7\u00f4Quality Checklists\n", "abstract": " This paper reports the results of one research question addressed by a case study undertaken by the EPIC project to investigate the use of Systematic Literature Reviews in software Engineering. The specific research question addressed in this report is:", "num_citations": "2\n", "authors": ["13"]}
{"title": "Protocol for a Tertiary study of Systematic Literature Reviews and Evidence-based Guidelines in IT and Software Engineering\n", "abstract": " BackgroundAt ICSE04, Kitchenham et al.(2004) Suggested software engineering researchers should adopt \u0393\u00c7\u00a3Evidence-based Software Engineering\u0393\u00c7\u00a5(EBSE). EBSE aims to apply an evidence-based approach to software engineering research and practice. The ICSE paper was followed-up by a paper at Metrics05 (J\u251c\u2555rgensen et al., 2005) and an article in IEEE Software (Dyb\u251c\u00d1 et al., 2005).", "num_citations": "2\n", "authors": ["13"]}
{"title": "Evaluation and Assessment in Software Engineering (EASE 05)\n", "abstract": " Editorial: Evaluation and Assessment in Software Engineering (EASE 05): Information and Software Technology: Vol 48, No 5 ACM Digital Library home ACM home Google, Inc. (search) Advanced Search Browse About Sign in Register Advanced Search Journals Magazines Proceedings Books SIGs Conferences People More Search ACM Digital Library SearchSearch Advanced Search Information and Software Technology Periodical Home Latest Issue Archive Authors Affiliations Award Winners More HomeBrowse by TitlePeriodicalsInformation and Software TechnologyVol. , No. Editorial: Evaluation and Assessment in Software Engineering (EASE 05) article Editorial: Evaluation and Assessment in Software Engineering (EASE 05) Share on Author: Barbara Ann Kitchenham profile image Barbara Ann Kitchenham Department of Computer Science, Keele University, Keele Village, Stoke-on-Trent, Staffordshire ST5 -\u0393\u00c7\u00aa", "num_citations": "2\n", "authors": ["13"]}
{"title": "The value of architecturally significant information extracted from patterns for architecture evaluation: a controlled experiment\n", "abstract": " We have developed an approach to identify and capture architecturally significant information from patterns (ASIP), which can be used to improve architecture design and evaluation. Our experimental goal was to evaluate whether the use of the ASIP improves the quality of scenarios developed to evaluate software architecture. Out of 24 subjects 21 were experienced software engineers who had returned to University for a postgraduate studies and remaining 3 were fourth year undergraduate students. All participants were taking a course in software architecture. The participants were randomly assigned to two groups of equal size. Both groups developed scenarios for architecture evaluation. One group (treatment group) was given ASIP information the other (control group) was not. The outcome variable was the quality of the scenarios produced by each participant working individually. The treatment group\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "2\n", "authors": ["13"]}
{"title": "Case studies for method and tool evaluation\n", "abstract": " Barbara Kitchenham and Lesley Pickard, National Computing Centre Shari Lawrence Pfleeger, City University \u2564\u00e4 Case studies help industry evaluate the benefits of methods and tools and provide a cost-effective way to ensure that process changes provide the desired results. However, unlike formal experiments and surveys, case studies do not have a well-understood theoretical basis. This article provides guidelines for organizing and analyzing case studies so that they produce meaningful results. ou have read about a new technique or tool in IEEE Software or elsewhere, and you are considering its use on your project. If it worked for someone else, how do you know it will work for you? The last decade has seen explosive growth in the number of software-engineering methods and tools, each one offering to improve some characteristic of software, its development, or its maintenance. With an increasing\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "2\n", "authors": ["13"]}
{"title": "An introduction to software metrics\n", "abstract": " This session will discuss the why software measurement is important. It will consider the way in which software measures are used in practice to support project management, and quality management.Next the session will define software measurement in terms of measurable attributes of software entities. It will describe the framework for measures defined by Kitchenham et al.(1995) and some of the implications of the framework including:", "num_citations": "2\n", "authors": ["13"]}
{"title": "Measuring to manage\n", "abstract": " This paper discusses ways in which software metrics can be used to assist software project management. The approach to project management and control described in this paper arose from work undertaken as part of the Alvey Test Specification and Quality Management Project (TSQM) and the ESPRIT REQUEST project. In this paper the term'software metrics' is used to mean measures (in terms of amounts or counts) related to software products and the process of software production and support.. In this context, the'software products' from which software metrics may be derived should be taken to include all the intermediate products, such as design documents, specifications, code listings, test reports, etc., which are produced during software development and maintenance, not just the final software product.This fairly loose definition reflects the fact that the term software metrics is used as a general tag to cover\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "2\n", "authors": ["13"]}
{"title": "Controlling software projects\n", "abstract": " Attempts to control software development often cause difficulties. Conventional engineering management techniques must be adapted to cope with the problems of software", "num_citations": "2\n", "authors": ["13"]}
{"title": "Supplementary materials for the paper \u0393\u00c7\u00a3Meta-analysis for families of experiments in software engineering: a systematic review and reproducibility and validity assessment\u0393\u00c7\u00a5\n", "abstract": " The first author (BAK) applied the inclusion and exclusion criteria to the identified candidate primary studies. The third author (PB) checked the application of the inclusion/exclusion criteria to each candidate primary study. The single disagreement during the search and selection process was resolved by discussion.", "num_citations": "1\n", "authors": ["13"]}
{"title": "employing the evidence-based paradigm for technology-related decision making\n", "abstract": " We report our experiences with adapting the systematic review procedures to help consolidate software engineering knowledge, seen as a step towards being able to employ evidence-based practices to assist with decision making in IT. We describe the different studies performed, and illustrate our procedures through a fuller description of one of our reviews. Our work has demonstrated the value of a study protocol and identified a number of problems. We conclude that it is practical to employ systematic literature reviews in software engineering, but that some cultural changes are required, particularly for reporting of empirical studies.", "num_citations": "1\n", "authors": ["13"]}
{"title": "Software process simulation over decade: Trends discovery from a systematic review\n", "abstract": " Software Process Simulation (SPS) research has increased since 1998 when the first ProSim Workshop was held. This paper aims to discover how SPS has evolved during the past 10 years based on the preliminary results from the systematic literature review of SPS publications from 1998 to 2007. Trends over the period showed that interest in continuous modelling was decreasing and interest in micro-processes was increasing. Hybrid models were based primarily on system dynamics and discrete event simulation and were all implemented by vertical integration.", "num_citations": "1\n", "authors": ["13"]}
{"title": "11th International Conference on Evaluation and Assessment in Software Engineering (EASE)-Index\n", "abstract": " The international conference on Evaluation and Assessment in Software Engineering (EASE) provides a forum for empirical researchers to present their latest research, as well as to discuss issues related to evaluation and empirical studies. We welcome papers that discuss important evaluation methods including both quantitative and qualitative methods. We are particularly fortunate this year that in addition to papers describing laboratory experiments, surveys and a case study, we have papers that describe the use of methods as diverse as systematic literature reviews, grounded theory and constructive research. The international conference on Evaluation and Assessment in Software Engineering (EASE) provides a forum for empirical researchers to present their latest research, as well as to discuss issues related to evaluation and empirical studies. We welcome papers that discuss important evaluation methods\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["13"]}
{"title": "Synthesising research results\n", "abstract": " I am currently working on the Evidence-Based Software Engineering (EBSE) project (EP/C51839X/1) awarded by the UK Engineering and Physical Sciences Research Council. We are investigating how readily the concept of evidence-based practice can be adapted to Software Engineering. Evidence-based practice relies on research synthesis, aggregating and analysing relevant \u0393\u00c7\u00ffprimary studies\u0393\u00c7\u00d6 by using the methodology of systematic literature reviews. Much of the original impetus came from medical research and emphasised synthesis of high-quality quantitative experiments (i.e. double-blind randomised controlled trials) using statistical metaanalysis.", "num_citations": "1\n", "authors": ["13"]}
{"title": "Assessing the value of Architectural Information Extracted from Patterns for Architecting\n", "abstract": " Background:  We have developed an approach to identifying and capturing architecturally significant information from patterns (ASIP), which can be used to improve architecture design and evaluation. Goal:  Our goal was to evaluate whether the use of the ASIP provides more effective support in understanding or designing software architecture composed of the software design patterns which are the source of the ASIP compared with the original design pattern documentation. Experimental design:  Our subjects were 20 experienced software engineers who had returned to University for a post graduate course. All participants were taking a course in software architecture. The participants were randomly assigned to two groups of equal size. Both groups performed two tasks: understanding the use of J2EE design pattern in a given architecture based on the quality requirements the architecture was supported to satisfy, and designing software architecture to satisfy a given set of quality requirements using J2EE design patterns. For the first task, one group (treatment group) was given ASIP information the other (control group) was given the standard J2EE pattern documentation. For the second task, treatment group became the control group and vice versa and the type of support information was kept constant. The outcome variables were the number of correctly identified design patterns. The participants also completed a post-experiment questionnaire. Result:  The average score for the first task for the treatment group was 23.90 and for the control group was 13.80. The difference between the groups was significant using Mann-Whiney test (p=0\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["13"]}
{"title": "Erratum to\" An empirical study of maintenance and development estimation accuracy\"\n", "abstract": " Erratum to \"An empirical study of maintenance and development estimation accuracy\" | Journal of Systems and Software ACM Digital Library home ACM home Google, Inc. (search) Advanced Search Browse About Sign in Register Advanced Search Journals Magazines Proceedings Books SIGs Conferences People More Search ACM Digital Library SearchSearch Advanced Search Journal of Systems and Software Periodical Home Latest Issue Archive Authors Affiliations Award Winners More HomeBrowse by TitlePeriodicalsJournal of Systems and SoftwareVol. , No. Erratum to \"An empirical study of maintenance and development estimation accuracy\" article Erratum to \"An empirical study of maintenance and development estimation accuracy\" Share on Authors: Barbara Ann Kitchenham profile image Barbara Kitchenham Department of Computer Science, Keele University, Keele, Staffs ST5 5BG, Staffordshire, UK , /\u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["13"]}
{"title": "A Process for Evaluating a Software Bidding Model\n", "abstract": " This report discusses the issues involved in evaluating a software bidding model. We found it difficult to assess the appropriateness of any model evaluation activities without a baseline or standard against which to assess them. This paper describes our attempt to construct such a baseline. We reviewed evaluation criteria used to assess cost models and an evaluation framework that was intended to assess the quality of requirements models. We developed an extended evaluation framework that will be used to evaluating our bidding model. Furthermore, we suggest the evaluation framework might be suitable for evaluating other models derived from expert opinion based influence diagrams.We use a simple process model to relate the evaluation framework to the model building process. The process model indicates the order in which different evaluation activities are performed and the role responsible for performing them. It also illustrates the difference between evaluating a generic bidding model and evaluating a specialized bidding model.", "num_citations": "1\n", "authors": ["13"]}
{"title": "Automating software quality modelling, measurement and assessment\n", "abstract": " This paper describes the SQUID approach to modelling, measuring, and assessing software quality. SQUID is an ESPRIT III project that has developed a method and an experimental toolset to support and automate these activities. The toolset assists in quality specification, quality planning, quality control and quality evaluation. More specifically, it provides the means to establish targets for the product quality requirements and evaluate their feasibility (quality specification). Then, the toolset supports the identification of the internal software product and process attributes that must be controlled during the development process to fulfil the project quality requirements (quality planning and control). Finally, the toolset helps to assess the fulfilment of the project quality requirements (quality evaluation).", "num_citations": "1\n", "authors": ["13"]}
{"title": "Comments on \u0393\u00c7\u00ffConsidering quality in the management of software-based development projects\u0393\u00c7\u00d6\n", "abstract": " I agree with the author that many of the difficulties associated with software project management can be solved by better adherence to standard project management principles. However, I think that there are critical features of software production that render conventional project management approaches insufficient for software projects.In this context I regard conventional project management techniques as those appropriate to dealing with long-linked technology 1, ie, serially independent production in which sequential activities are coordinated by means of a plan. Such techniques are sufficient for dealing with manufacturing processes but are not necessarily sufficient to handle software production, which is a design process.", "num_citations": "1\n", "authors": ["13"]}
{"title": "Software reliability and metrics\n", "abstract": " I first started to work in the field of software metrics in 1980. At that time few people in Europe had heard of software metrics. Most metrics work was being done in the United States and much of it concentrated on producing complicated mathematical formulas with elaborate names based on code. In addition, many academics considered metrics unscientific, and most commercial software producers considered metrics a waste of time and effort. As one of the people who was pigheaded enough to disagree with the general assessment of metrics, I have been pleased with the progress that has been made over the past decade which is well illustrated by the papers in this issue. First, as a result of initiatives such as ESPRIT there is now a substantial research community in Europe. This issue includes four papers describing work which was partially funded by ESPRIT, as well as papers funded by national European\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["13"]}
{"title": "The architecture of an automated quality management system\n", "abstract": " Sauf mention contraire ci-dessus, le contenu de cette notice bibliographique peut \u251c\u00actre utilis\u251c\u2310 dans le cadre d\u0393\u00c7\u00d6une licence CC BY 4.0 Inist-CNRS/Unless otherwise stated above, the content of this bibliographic record may be used under a CC BY 4.0 licence by Inist-CNRS/A menos que se haya se\u251c\u2592alado antes, el contenido de este registro bibliogr\u251c\u00edfico puede ser utilizado al amparo de una licencia CC BY 4.0 Inist-CNRS", "num_citations": "1\n", "authors": ["13"]}