{"title": "An examination of the effects of requirements changes on software maintenance releases\n", "abstract": " Requirements are the foundation of the software release process. They provide the basis for estimating costs and schedules, as well as developing design and testing specifications. When requirements have been agreed on by both clients and maintenance management, then adding to, deleting from, or modifying those existing requirements during the execution of the software maintenance process impacts the maintenance cost, schedule, and quality of the resulting product. The basic problem is not the changing in itself, but rather the inadequate approaches for dealing with changes in a way that minimizes and communicates the impact to all stakeholders. Using data collected from one organization on 44 software releases spanning seven products, this paper presents two quantitative techniques for dealing with requirements change in a maintenance environment. First, exploratory data analysis helps one to\u00a0\u2026", "num_citations": "121\n", "authors": ["1218"]}
{"title": "Using metrics in management decision making\n", "abstract": " The amount of code in NASA systems has continued to grow over the past 30 years. This growth brings with it the increased risk of system failure caused by software. Thus, managing the risks inherent in software development and maintenance is becoming a highly visible and important field. The metrics effort within NASA's Mission Operations Directorate has helped managers and engineers better understand their processes and products. The toolkit helps ensure consistent data collection across projects and increases the number and types of analysis options available to project personnel. The decisions made on the basis of metrics analysis have helped project engineers make decisions about project and mission readiness by removing the inherent optimism of \"engineering judgment\".< >", "num_citations": "96\n", "authors": ["1218"]}
{"title": "Measurements for managing software maintenance.\n", "abstract": " Software maintenance is central to the mission of many organizations. Thus, it is natural for managers to characterize and measure those aspects of products and processes that seem to affect the cost, schedule, quality and functionality of software maintenance delivery. This paper answers basic questions about software maintenance for a single organization and discusses some of the decisions made based on the answers. Attributes of both the software maintenance process and the resulting product were measured to direct management and engineering attention toward improvement areas, track the improvement over time, and help make choices among alternatives.", "num_citations": "62\n", "authors": ["1218"]}
{"title": "An examination of the effects of requirements changes on software releases\n", "abstract": " Requirements are the foundation of the software release process. They provide the basis to develop budgets, schedules, and design and testing specifications. Changing requirements during a software release process impacts the cost, schedule, and quality of the product that results. We have collected data on 40 software releases in our environment to understand the source, magnitude, and effects of changing requirements on software maintenance releases. The benefits received include better management of releases and improved customer communications.During release planning, requirements analysis, design, and test reviews, new priorities are established, and changes to the release content are requested in the form of change requests being added or deleted from the release. This requirements volatility makes it difficult to develop dependable release schedules and budgets. B. Curtis, H. Krasner, and N. Iscoe conclude that accurate problem domain knowledge is critical to the success of a project, and requirements volatility causes major difficulties during development [10]. Although these conclusions confirm most people\u2019s intuitions concerning requirements volatility, they are not precise enough to help managers take effective action on their projects. M. Lubars, C. Potts, and C. Richter went further by interviewing 23 project teams and recommending organizational solutions rather than technological solutions to the requirements analysis issue [11]. In no case did they find a coherent relationship between requirements analysis and project planning. This article therefore has two major goals: first, to present an organization\u2019s data\u00a0\u2026", "num_citations": "44\n", "authors": ["1218"]}
{"title": "Classifying server behavior and predicting impact of modernization actions\n", "abstract": " Today the decision of when to modernize which elements of the server HW/SW stack is often done manually based on simple business rules. In this paper we alleviate this problem by supporting the decision process with an automated approach based on incident tickets and server attributes data. As a first step we identify and rank servers with problematic behavior as candidates for modernization using a random forest classifier. Second, this predictive model is used to evaluate the impact of different modernization actions and suggest the most effective ones. We show that our chosen model yields high quality predictions and outperforms traditional linear regression models on a large set of real data.", "num_citations": "33\n", "authors": ["1218"]}
{"title": "Dependability evaluation of integrated hardware/software systems\n", "abstract": " A methodology for predicting the dependability (reliability and availability) of an integrated realtime hardware/software system using a semi-Markov process is described. This methodology was used to evaluate the reliability of the shuttle mission simulators at the NASA Johnson Space Center.", "num_citations": "31\n", "authors": ["1218"]}
{"title": "A software metric set for program maintenance management\n", "abstract": " Managers at the National Aeronautics and Space Administration's Mission Operations Directorate at the Johnson Space Center wanted to increase their insight into the cost, schedule, and quality of the software-intensive systems maintained for the space shuttle programs. To support this objective, we defined and implemented a software metrics set that contains 13 metrics related to corrective and adaptive maintenance actions. Management support and tools were necessary for effective implementation. The start-up cost was low because much of the data were already collected by the projects. Management's ability to assess and predict the state of software maintenance processes has increased. Managers have taken actions to improve software quality and reduce software maintenance costs based on analysis of the metrics data.", "num_citations": "28\n", "authors": ["1218"]}
{"title": "Modeling a complex global service delivery system\n", "abstract": " Enterprises and IT service providers are increasingly challenged with improving the quality of service while reducing the cost of service delivery. Effectively balancing dynamic customer workload, strict service level constraints, and diverse service personnel skills challenges the most experienced management teams. In this paper we describe a modeling framework for analyzing complex service delivery systems. The interaction among various key factors are included in the model to allow decision-making around staffing skill levels, scheduling, and service level constraints in system design. We demonstrate the applicability of the proposed approach in a large IT services delivery environment.", "num_citations": "26\n", "authors": ["1218"]}
{"title": "Impact of HW and OS type and currency on server availability derived from problem ticket analysis\n", "abstract": " Technology refresh is an important component in data center management. The goal of this paper is to assess the impact of HW and OS currency on server availability based on a large set of incident tickets and server attributes data collected from several different IT environments. In order to achieve this we first identify the server failure incidents using a machine learning method for automatic ticket classification. Then we conduct the data analysis to inspect the impact of HW and OS type along with their currency on the rates of server failures. This can further be used to derive guidelines to support the technology refresh decisions in the data centers.", "num_citations": "19\n", "authors": ["1218"]}
{"title": "Software maintenance management strategies: observations from the field\n", "abstract": " There is much literature describing software maintenance process models, but few comparative studies on the approaches used by managers in the field. This paper describes three software maintenance management strategies currently implemented by six organizations. The strategies are compared on the attributes of performance, economics, efficiency and end\u2010product quality. The paper defines operational measurements for each attribute and describes the data collected over the past two years. The three strategies investigated were labelled the fixed staff/variable schedule, variable staff/fixed schedule and the variable staff/variable schedule. Each strategy has attributes that make it appealing for implementation by a software maintenance project manager. The fixed staff/variable schedule strategy had the fastest priority change response time, the variable staff/fixed schedule strategy had the highest\u00a0\u2026", "num_citations": "19\n", "authors": ["1218"]}
{"title": "Analysis of labor efforts and their impact factors to solve server incidents in datacenters\n", "abstract": " A company's IT infrastructure delivers the basic hardware, networking, operating system, and middleware support to the business' applications. IT service providers perform incident and problem resolution, as well as user administration and change implementation required to maintain the availability and service provided for the business. As a result, they become increasingly challenged with delivering better, faster, and cheaper services to their customers. With the variety of incident tickets reported on a daily basis, understanding where and how much effort is spent to resolve them is critical. Moreover, analyzing the effort data identifies opportunities for self-service and automation, as well as what modernization strategies businesses should implement to reduce incident volumes and, by association, labor effort. In this paper, we conduct a large scale study on the incident and server factors that affect technician effort\u00a0\u2026", "num_citations": "18\n", "authors": ["1218"]}
{"title": "A survey of software reliability measurement tools\n", "abstract": " The results are given of a survey of software reliability measurement tools performed for the American Institute of Aeronautics and Astronautics (AIAA) Space Based Observation Systems (SBOS) Committee on Standards (COS). This survey is part of an effort to develop a standard for software reliability measurement. Surveys were completed on four tools. This information is presented, along with partial information on three other known tools. The need for critical evaluation and review of these tools is stressed and a brief description of planned work in this area is presented.<>", "num_citations": "15\n", "authors": ["1218"]}
{"title": "Evaluation of software testing metrics for NASA's Mission Control Center\n", "abstract": " Software metrics are used to evaluate the software developmentprocess and the quality of the resultingproduct. We used five metrics during the testing phase of the Shuttle Mission Control Center (MCC) Upgrade (MCCU) at the National Aeronautics and Space Administration's (NASA) Johnson Space Center. All but one metric provided useful information. Based on our experience we recommend using metrics during the test phase of software development and propose additional candidate metrics for further study.", "num_citations": "14\n", "authors": ["1218"]}
{"title": "System recommendations based on incident analysis\n", "abstract": " In one embodiment, a computer-implemented method includes obtaining incident data related to a plurality of servers, including a first server. Configuration data is obtained for each of the plurality of servers. The configuration data includes information about a set of one or more configuration items of the first server. A predictive modeler is trained to predict incident characteristics, based at least in part on the incident data and the configuration data. A modification is selected to the set of configuration items of the first server. Predicted incident characteristics of the first server are simulated, by a computer processor, based on the selected modifications. It is recommended that the selected modifications be made to the first server if predetermined criteria are met by the simulation.", "num_citations": "13\n", "authors": ["1218"]}
{"title": "On the adoption and impact of predictive analytics for server incident reduction\n", "abstract": " The Predictive Analytics for Server Incident Reduction (PASIR) solution developed at IBM has been broadly deployed to 130 IT environments since the beginning of 2014. The infrastructures of these IT environments, pertaining to various industries around the world, are serviced by IBM support groups. More specifically, incidents occurring on servers, including the descriptions of the problems, are reported into a ticket management system. These tickets are then resolved by the assigned support teams, which record in the system the resolution steps taken. PASIR, first classifies the incident tickets of an IT environment to identify high-impact incidents describing server unavailability and performance degradation issues by using ticket descriptions and resolutions. Second, the occurrence of these high-impact tickets is correlated with server properties and utilization measures to identify troubled server configurations\u00a0\u2026", "num_citations": "8\n", "authors": ["1218"]}
{"title": "Measurement and the eSourcing Capability Model for Service Providers v2\n", "abstract": " The eSourcing Capability Model for Service Providers (eSCM-SP) is a \u201cbest practices\u201d capability model. It provides guidance to service providers on improving their capability across the sourcing lifecycle, and it provides clients with an objective means of evaluating the capability of service providers. Measurement is fundamental to effective service management, business process outsourcing (BPO), and organizational improvement. This report describes the importance of aligning an organization\u2019s measurements with its business objectives. It provides guidance on measurement principles and defines four measurement categories that span the Practices of the eSCM-SP. Examples and lessons learned are used to illustrate the principles and guidance.", "num_citations": "8\n", "authors": ["1218"]}
{"title": "A survey instrument for understanding the complexity of software maintenance\n", "abstract": " Software maintenance is a multi\u2010faceted, multidimensional effort that, upon inspection, is larger and more complex than it first seems. At least three components contribute to the complexity of the software maintenance effort: (1) the code and documentation being produced, (2) the process used to manage the maintenance, and (3) the maintenance and target computer system environments. This paper describes the development and use of a simple spreadsheet tool designed to measure the complexity of a software maintenance release. The tool can be used by software maintenance managers to direct their attention at the cost drivers associated with a software maintenance release, to aid in the re\u2010engineering decision, and to keep track of the degradation or improvement associated with a software system due to maintenance actions or process improvements. The approach has been successful in identifying\u00a0\u2026", "num_citations": "8\n", "authors": ["1218"]}
{"title": "File origin determination\n", "abstract": " A file validation method and system is provided. The method includes retrieving from an authoritative source system, an artifact file. Identification information identifying a requesting user of the artifact file is recorded and associated metadata and a modified artifact file comprising the metadata combined with the artifact file are generated. An encryption key including a first portion and a second portion is generated and the first portion is stored within a central key store database. An encrypted package comprising the modified artifact file and the second portion of the key is generated.", "num_citations": "7\n", "authors": ["1218"]}
{"title": "Computing service level risk\n", "abstract": " Statistical process control, performance distribution identification, and a simulation model based on, for example, Monte Carlo simulation, are used to calculate the risk of various service levels. A recommended service level is determined, the service level being one that is estimated to have an appropriate risk for both the outsourcing supplier and the customer.", "num_citations": "7\n", "authors": ["1218"]}
{"title": "Predicting service request breaches\n", "abstract": " An approach for prioritizing work requests to resolve incidents in an information technology (IT) infrastructure is presented. Historical data of work requests to resolve incidents in the IT infrastructure is divided into first and second data sets. A first set of data fields of work requests in the first data set is used to generate incident concept (s). The incident concept (s) are combined with a second set of data fields of the work requests in the first data set to form a set of predictive variables. Utilizing the predictive variables, a statistical model is generated for predicting whether or not work requests will be resolved in accordance with a service level target. The statistical model is validated using the second data set. The statistical model is deployed to the IT infrastructure.", "num_citations": "7\n", "authors": ["1218"]}
{"title": "Measurement to manage software maintenence\n", "abstract": " Software maintenance is central to the mission of many organizations. Thus, it is natural for managers to characterize and measure those aspects of products and processes that seem to affect cost, schedule, quality, and functionality of a software maintenance delivery. This article answers basic questions about software maintenance for a single organization and discusses some of the decisions made based on the answers. These examples form a useful basis for other software maintenance managers to make better decisions in the day\u2010to\u2010day interactions that occur on their programs.", "num_citations": "6\n", "authors": ["1218"]}
{"title": "Software reliability tools\n", "abstract": " It is the commonality of these and other features that leads to the development of special-purpose SRE measurement tools. This appendix provides a summary description of several of these tools. It does not consider early prediction tools, as they are generally less mature and not as readily available to the software engineering community.Section A. 2 discusses the relative merits of using an SRE tool rather than a general-purpose application like a spreadsheet or statistical package for conducting an SRE analysis. Section A. 3 lists criteria for consideration when deciding which tool to purchase or use for a particular SRE project. These criteria should be kept in mind while reading Secs. A. 4 through A. 9, which summarize some widely used tools with an example execution of each tool. Finally, Sec. A. 10 provides additional details on the features of the highlighted tools as well as", "num_citations": "6\n", "authors": ["1218"]}
{"title": "High integrity software for nuclear power plants: Candidate guidelines, technical basis and research needs. Executive summary: Volume 1\n", "abstract": " Prepared by S. Seth, W. Bail, D. Cleaves, H. Cohen, D. Hybertson, C. Schaefer, G. Stark, A. la, B. Ulery", "num_citations": "6\n", "authors": ["1218"]}
{"title": "A micro and macro based examination of the effects of requirements changes on aerospace software maintenance\n", "abstract": " Requirements are the foundation of the software release process. They provide the basis for developing budgets, schedules, and design and testing specifications. Changing requirements during a software release process impacts the cost, schedule, and quality of the resulting product. We have collected data on 20 software releases in our environment to understand the source, magnitude, and effects of changing requirements on software maintenance releases. The benefits received include better management of releases and improved customer communications.", "num_citations": "5\n", "authors": ["1218"]}
{"title": "Standards-software reliability handbook: achieving reliable software\n", "abstract": " The results to date of a three-year research effort on software reliability engineering sponsored by the American Institute of Astronautics and Aeronautics (AIAA) are discussed. After a standard for recommended practice for software reliability estimation was approved by the AIAA, a blue-ribbon panel of industry experts was convened to consider the future of reliable software and software technology in space systems. The panel's work towards a software reliability engineering database and research in software reliability tools and software reliability models are discussed. The software reliability recommended practice, its intended audience, and its users are reviewed.< >", "num_citations": "5\n", "authors": ["1218"]}
{"title": "COACH: Cognitive analytics for change\n", "abstract": " This paper presents our initial efforts towards building a cognitive analytics framework for change management. We propose a novel predictive algorithm for change risk calculation based on historical change failures, server failures, change triggered incidents as well as expert user input. Our predictive algorithm provides significant improvement over traditional risk assessments in proactively capturing problematic changes when tested with real client account data.", "num_citations": "4\n", "authors": ["1218"]}
{"title": "Feedback based model validation and service delivery optimization using multiple models\n", "abstract": " An approach for validating a model is presented. Data from a system being modeled is collected. First and second models of the system are constructed from the collected data. Based on the first model, a first determination of an aspect of the system is determined. Based on the second model, a second determination of the aspect of the system is determined. A variation between the first and second determinations is determined. An input for resolving the variation is received and in response, a model of the system that reduces the variation is derived.", "num_citations": "4\n", "authors": ["1218"]}
{"title": "Determining a risk level for server health check processing\n", "abstract": " Approaches are provided for determining a risk level for server health check processing. An approach includes creating statistical process control analyzes for each of one or more servers on a network by calculating performance parameters for each of one or more servers based on historical values for one or more key process indicators of server health or vulnerability. The approach further includes collecting new values for each of the one or more key process indicators, detecting significant changes of the mean or variation in each of the one or more key process indicators, and determining a risk level for each of the one or more servers based on a number of the significant changes of the mean or variation. The approach further includes providing a health check recommendation for each of the one or more servers based on the risk level determined for each of the one or more servers.", "num_citations": "3\n", "authors": ["1218"]}
{"title": "Technologies for improving the dependability of software-intensive systems: Review of NASA experience\n", "abstract": " Software is central to the mission of many NASA systems. It is also a major source of failure. Fortunately, techniques such as best current practice, complexity measurement, fault tolerance, formal methods, and software reliability exist to mitigate the risk of system failure because of software. Unfortunately, these techniques have become disciplines of their own with very little integration, resulting in a lack of support from project managers. NASA SR&QA should develop a strategy to integrate the techniques into the agency reliability plan.< >", "num_citations": "3\n", "authors": ["1218"]}
{"title": "Deriving optimal actions from a random forest model\n", "abstract": " Training a random forest model to relate settings of a network security device to undesirable behavior of the network security device is provided. A determination of a corresponding set of settings associated with each region of lowest incident probability is made using a random forest. The plurality of identified desired settings are presented as options for changing the network security device from the as-is settings to the identified desired settings. A choice is received from the plurality of options. The choice informs the random forest model. The random forest model ranks for a new problematic network security device the plurality of options for changing the new problematic network security device from as-is settings to desired settings by aggregating an identified cost of individual configuration changes, thereby identifying a most cost-effective setting for the network security device to achieve a desired output of the\u00a0\u2026", "num_citations": "2\n", "authors": ["1218"]}
{"title": "Categorization of document content based on entity relationships\n", "abstract": " A document of written content may be obtained. The document may be a candidate for inclusion in a corpus. A first entity associated with the document may be identified. A first discrete entity associated with the first entity may be identified. The relationship associated with the first entity and the first discrete entity may be analyzed. Based on the analyzing, a likelihood that the document contains content that would be detrimental for inclusion in the corpus may be determined.", "num_citations": "2\n", "authors": ["1218"]}
{"title": "Assisting database management\n", "abstract": " In an approach to assisting database management, a computer generates one or more combinations of values of one or more database configuration parameters. The computer associates each of the one or more generated combinations of values with an incident probability. The computer defines relationships between the one or more generated combinations and the associated incident probabilities. The computer stores the defined relationships into an object representable as a multi-dimensional matrix, whose dimensions correspond to a plurality of database configuration parameters used to generate the combinations of values. The computer traverses the object to identify a path in the matrix. The computer returns the identified path for enabling subsequent interpretation thereof as a rule for passing from a first database configuration, corresponding to the first one of the one or more generated combinations, to\u00a0\u2026", "num_citations": "2\n", "authors": ["1218"]}
{"title": "Detection of operator likelihood of deviation from planned route\n", "abstract": " Embodiments of the present invention provide detection of operator likelihood of deviation from a planned route. In embodiments, a route is planned on an electronic navigation system. Vehicle and/or operator parameters are monitored, and a likelihood of deviation from the route is detected. Upon detecting a likely upcoming deviation, an alert is provided to the user so that corrective action can be taken before missing a waypoint of the route.", "num_citations": "2\n", "authors": ["1218"]}
{"title": "Return on investment from a software measurement program\n", "abstract": " Return on Investment from a Software Measurement Program Page 1 MITRE Return on Investment from a Software Measurement Program George E. Stark 21 July 1997 Page 2 MITRE Background Organization Became Responsible for Software Maintenance in Nov. 1994 \u2013 Reorganized Software Maintenance Process \u2022 From: Several Level-of-Effort Programs \u2022 To: Single Organization tasked by Release \u2013 Conducted SEI System Acquisition Maturity Model (SAMM) Assessment \u2022 Improve Planning & Estimation \u2022 Improve Configuration Management \u2022 Improve Release Execution Page 3 MITRE MWSSS Software Measurement Strategy Understand Use Transfer Iterate Know our software business (process & product) How do we do business today? (eg, OIs and languages used, % time in test, CSCI size) What are our product characteristics (eg, cost, complexity, reliability) Determine Improvements to our software \u2026", "num_citations": "2\n", "authors": ["1218"]}
{"title": "Monitoring software reliability in the shuttle mission simulator\n", "abstract": " The Shuttle Mission Training Facility (SMTF) is used to train astronauts and ground controllers for Space Shuttle missions. The complex contains three simulators driven by real-time software on large mainframe host computers and supported by as many as twelve super-mini computers. The Shuttle Mission Simulators are complex systems with a large number of hardware components and many diverse pieces of software running simultaneously and interacting with each other. The real-time executable code consists of approximately 365,000 lines of FORTRAN source in 956 modules and 275,000 lines of ASSEMBLER code in 686 modules.", "num_citations": "2\n", "authors": ["1218"]}
{"title": "Cognitive prediction of problematic servers in unknown server group\n", "abstract": " A set of profile parameters to characterize an unknown group of servers is computed. A set of known groups of servers is selected from a historical repository of known group of servers. A subset of known group is selected such that each known group in the subset has a corresponding similarity distance that is within a threshold similarity distance from the unknown group. A decision tree is constructed corresponding to a known group in the subset, by cognitively analyzing a usage of the set of profile parameters of the unknown group in the known group. Using the decision tree a number of problematic servers is predicted in the unknown group. When the predicted number of problematic servers does not exceed a threshold number, a post-prediction action is caused to occur on the unknown group, which causes a reduction in an actual number of problematic servers in the unknown group.", "num_citations": "1\n", "authors": ["1218"]}
{"title": "Predictive modeling of risk for services in a computing environment\n", "abstract": " A method includes obtaining, by one or more processor, data related to a service level agreement for a service from a provider, wherein the service includes providing, to a client, network resources from a shared pool of network resources in a computing environment. The one or more processor utilizes the data to generate input parameters and these input parameters include target parameters for the service level agreement, penalty parameters for the service level agreement, and statistical process control parameters. The one or more processor analyzes the input parameters to model risk associated with probability of failure of the service from the provider in a given computing environment and generates and based on the risk model, allocating, by the one or more processor, a portion of the network resources.", "num_citations": "1\n", "authors": ["1218"]}
{"title": "Benchmarking performance of a service organization\n", "abstract": " A method and associated systems for benchmarking performance of a service organization. A processor collects recorded performance data that identifies how much time members of randomly selected service teams of the service organization required to perform tasks associated with sub-activities of an activity of interest. The data is sorted by sub-activity and data associated with certain biasing types of tasks may be discarded. For each sub-activity for which enough valid data exists, a sub-activity benchmark is identified as a function of a median of task-performance times associated with that sub-activity. This sub-activity benchmark may be used to derive statistical functions that characterize performance of another service team when performing tasks related to the same sub-activity. Such characterizations may be aggregated and exported to a scorecard report that identifies overperforming and underperforming\u00a0\u2026", "num_citations": "1\n", "authors": ["1218"]}
{"title": "3 Sponsor\u2019s Note 28 BackTalk\n", "abstract": " What do you do if you want to create an estimate and you have 100 candidate variables to use in your estimating model? This is also a common question for CMMI\u00ae high maturity organizations that need to create process performance models. According to SEI, process performance models are:\u201cA description of relationships among attributes of a process and its work products that is developed from historical processperformance data and calibrated using collected process and product or service measures from the project and that are used to predict results by following a process.\u201d High maturity organizations typically use process performance models for operational purposes such as project monitoring, project planning, and to identify and evaluate improvement opportunities. They typically are used to predict many output variables including defects, test effectiveness, cost schedule and duration, requirements volatility, customer satisfaction, and work product size. 1", "num_citations": "1\n", "authors": ["1218"]}
{"title": "Software reliability for flight crew training simulators\n", "abstract": " Flight crew simulator failures are costly and may have an impact on the timing or efficiency of a mission; thus, reliability is one of the most important issues facing simulator developers today. The reliability of a simulator is the probability that a training session of length t can be completed without a failure. This paper defines simulator failure and then identifies and compares the three sources of simulator failure: hardware, software, and human, focusing on the cost of software failure. The paper next describes a model for software reliability measurement and proposes a method for establishing a software reliability objective. Data from the NASA Shuttle Mission Training Facility illustrate the technique. Finally, the paper examines the implications of using the method on the software testing successes.", "num_citations": "1\n", "authors": ["1218"]}
{"title": "Software reliability for flight crew training simulators\n", "abstract": " Flight crew simulator failures are costly and may impact the timing or efficiency of a mission; thus, reliability is one of the most important issues facing simulator developers today. The reliability of a simulator is the probability that a training session of length can be completed without a failure. This paper defines simulator failure and then identifies and compares the three sources of simulator failure: hardware, software, and human, focusing on the cost of software failure. The paper next describes a model for software reliability measurement and proposes a method for establishing a software reliability object. Data from the NASA Shuttle Mission Training Facility illustrate the technique. Finally, the paper examines the implications of using the method on the software testing successes. (Author)", "num_citations": "1\n", "authors": ["1218"]}