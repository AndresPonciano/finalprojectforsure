{"title": "Seattle: a platform for educational cloud computing\n", "abstract": " Cloud computing is rapidly increasing in popularity. Companies such as RedHat, Microsoft, Amazon, Google, and IBM are increasingly funding cloud computing infrastructure and research, making it important for students to gain the necessary skills to work with cloud-based resources. This paper presents a free, educational research platform called Seattle that is community-driven, a common denominator for diverse platform types, and is broadly deployed.", "num_citations": "219\n", "authors": ["678"]}
{"title": "{CHAINIAC}: Proactive software-update transparency via collectively signed skipchains and verified builds\n", "abstract": " Software-update mechanisms are critical to the security of modern systems, but their typically centralized design presents a lucrative and frequently attacked target. In this work, we propose CHAINIAC, a decentralized software-update framework that eliminates single points of failure, enforces transparency, and provides efficient verifiability of integrity and authenticity for software-release processes. Independent witness servers collectively verify conformance of software updates to release policies, build verifiers validate the source-to-binary correspondence, and a tamper-proof release log stores collectively signed updates, thus ensuring that no release is accepted by clients before being widely disclosed and validated. The release log embodies a skipchain, a novel data structure, enabling arbitrarily out-of-date clients to efficiently validate updates and signing keys. Evaluation of our CHAINIAC prototype on reproducible Debian packages shows that the automated update process takes the average of 5 minutes per release for individual packages, and only 20 seconds for the aggregate timeline. We further evaluate the framework using real-world data from the PyPI package repository and show that it offers clients security comparable to verifying every single update themselves while consuming only one-fifth of the bandwidth and having a minimal computational overhead.", "num_citations": "114\n", "authors": ["678"]}
{"title": "Survivable key compromise in software update systems\n", "abstract": " Today's software update systems have little or no defense against key compromise. As a result, key compromises have put millions of software update clients at risk. Here we identify three classes of information whose authenticity and integrity are critical for secure software updates. Analyzing existing software update systems with our framework, we find their ability to communicate this information securely in the event of a key compromise to be weak or nonexistent. We also find that the security problems in current software update systems are compounded by inadequate trust revocation mechanisms. We identify core security principles that allow software update systems to survive key compromise. Using these ideas, we design and implement TUF, a software update framework that increases resilience to key compromise.", "num_citations": "93\n", "authors": ["678"]}
{"title": "Understanding password database compromises\n", "abstract": " Despite continuing advances in cyber security, website incursions, in which password databases are compromised, occur for high profile sites dozens of times each year. Dumps of recently stolen credentials appear on a regular basis at websites like pastebin. com and pastie. com, as do stories concerning significant breaches. As a result of these observations, we chose to examine this phenomenon.A study was undertaken to research information posted on the web concerning recent, high profile website intrusions, wherein user login credentials and other data were compromised. We searched for the party responsible for the incursion, the attack mechanism utilized, the format in which the login data was stored, and the location of any password dumps pilfered from the site. News stories from trade related journals, press releases from the victim company, hacker sites, and blogs from individuals and companies engaged in security analysis were, in particular, searched in order to find related information. A total of thirty four breaches were researched. It should be noted that some dumps, previously published, no longer exist. This is due to either the affected parties taking action against the site posting them, expiration of the allowed posting period, or removal by the original poster. An effort was made to locate copies of these files, sometimes to no avail. In those cases, details concerning the contents of the dumps were collected from published reports about them.", "num_citations": "83\n", "authors": ["678"]}
{"title": "A look in the mirror: Attacks on package managers\n", "abstract": " This work studies the security of ten popular package managers. These package managers use different security mechanisms that provide varying levels of usability and resilience to attack. We find that, despite their existing security mechanisms, all of these package managers have vulnerabilities that can be exploited by a man-in-the-middle or a malicious mirror. While all current package managers suffer from vulnerabilities, their security is also positively or negatively impacted by the distribution's security practices. Weaknesses in package managers are more easily exploited when distributions use third-party mirrors as official mirrors. We were successful in using false credentials to obtain an official mirror on all five of the distributions we attempted. We also found that some security mechanisms that control where a client obtains metadata and packages from may actually decrease security. We analyze current\u00a0\u2026", "num_citations": "83\n", "authors": ["678"]}
{"title": "\" On the internet, nobody knows you're a dog\" a twitter case study of anonymity in social networks\n", "abstract": " Twitter does not impose a Real-Name policy for usernames, giving users the freedom to choose how they want to be identified. This results in some users being Identifiable (disclosing their full name) and some being Anonymous (disclosing neither their first nor last name).", "num_citations": "76\n", "authors": ["678"]}
{"title": "It's the psychology stupid: how heuristics explain software vulnerabilities and how priming can illuminate developer's blind spots\n", "abstract": " Despite the security community's emphasis on the importance of building secure software, the number of new vulnerabilities found in our systems is increasing. In addition, vulnerabilities that have been studied for years are still commonly reported in vulnerability databases. This paper investigates a new hypothesis that software vulnerabilities are blind spots in developer's heuristic-based decision-making processes. Heuristics are simple computational models to solve problems without considering all the information available. They are an adaptive response to our short working memory because they require less cognitive effort. Our hypothesis is that as software vulnerabilities represent corner cases that exercise unusual information flows, they tend to be left out from the repertoire of heuristics used by developers during their programming tasks.", "num_citations": "62\n", "authors": ["678"]}
{"title": "Retaining sandbox containment despite bugs in privileged memory-safe code\n", "abstract": " Flaws in the standard libraries of secure sandboxes represent a major security threat to billions of devices worldwide. The standard libraries are hard to secure because they frequently need to perform low-level operations that are forbidden in untrusted application code. Existing designs have a single, large trusted computing base that contains security checks at the boundaries between trusted and untrusted code. Unfortunately, flaws in the standard library often allow an attacker to escape the security protections of the sandbox.", "num_citations": "61\n", "authors": ["678"]}
{"title": "A first look at vehicle data collection via smartphone sensors\n", "abstract": " Smartphones serve as a technical interface to the outside world. These devices have embedded, on-board sensors (such as accelerometers, WiFi, and GPSes) that can provide valuable information for investigating users' needs and behavioral patterns. Similarly, computers that are embedded in vehicles are capable of collecting valuable sensor data that can be accessed by smartphones through the use of On-Board Diagnostics (OBD) sensors. This paper describes a prototype of a mobile computing platform that provides access to vehicles' sensors by using smartphones and tablets, without compromising these devices' security. Data such as speed, engine RPM, fuel consumption, GPS locations, etc. are collected from moving vehicles by using a WiFi On-Board Diagnostics (OBD) sensor, and then backhauled to a remote server for both real-time and offline analysis. We describe the design and implementation\u00a0\u2026", "num_citations": "47\n", "authors": ["678"]}
{"title": "Selectively taming background android apps to improve battery lifetime\n", "abstract": " Background activities on mobile devices can cause significant battery drain with little visibility or recourse to the user. They can range from useful but sometimes overly aggressive tasks, such as polling for messages or updates from sensors and online services, to outright bugs that cause resources to be held unnecessarily. In this paper we instrument the Android OS to characterize background activities that prevent the device from sleeping. We present T AMER, an OS mechanism that interposes on events and signals that cause task wakeups, and allows for their detailed monitoring, filtering, and rate-limiting. We demonstrate how T AMER can help reduce battery drain in scenarios involving popular Android apps with background tasks. We also show how T AMER can mitigate the effects of well-known energy bugs while maintaining most of the apps\u2019 functionality. Finally, we elaborate on how developers and users can devise their own application-control policies for T AMER to maximize battery lifetime.", "num_citations": "46\n", "authors": ["678"]}
{"title": "{API} Blindspots: Why Experienced Developers Write Vulnerable Code\n", "abstract": " Despite the best efforts of the security community, security vulnerabilities in software are still prevalent, with new vulnerabilities reported daily and older ones stubbornly repeating themselves. One potential source of these vulnerabilities is shortcomings in the used language and library APIs. Developers tend to trust APIs, but can misunderstand or misuse them, introducing vulnerabilities. We call the causes of such misuse blindspots. In this paper, we study API blindspots from the developers' perspective to:(1) determine the extent to which developers can detect API blindspots in code and (2) examine the extent to which developer characteristics (ie, perception of code correctness, familiarity with code, confidence, professional experience, cognitive function, and personality) affect this capability. We conducted a study with 109 developers from four countries solving programming puzzles that involve Java APIs known to contain blindspots. We find that (1) The presence of blindspots correlated negatively with the developers' accuracy in answering implicit security questions and the developers' ability to identify potential security concerns in the code. This effect was more pronounced for I/O-related APIs and for puzzles with higher cyclomatic complexity.(2) Higher cognitive functioning and more programming experience did not predict better ability to detect API blindspots.(3) Developers exhibiting greater openness as a personality trait were more likely to detect API blindspots. This study has the potential to advance API security in (1) design, implementation, and testing of new APIs;(2) addressing blindspots in legacy APIs;(3) development of novel\u00a0\u2026", "num_citations": "42\n", "authors": ["678"]}
{"title": "Netcheck: Network diagnoses from blackbox traces\n", "abstract": " This paper introduces NetCheck, a tool designed to diagnose network problems in large and complex applications. NetCheck relies on blackbox tracing mechanisms, such as strace, to automatically collect sequences of network system call invocations generated by the application hosts. NetCheck performs its diagnosis by (1) totally ordering the distributed set of input traces, and by (2) utilizing a network model to identify points in the totally ordered execution where the traces deviated from expected network semantics.", "num_citations": "39\n", "authors": ["678"]}
{"title": "Uptane: Securing software updates for automobiles\n", "abstract": " Software update systems for automobiles can deliver significant benefits, but, if not implemented carefully, they could potentially incur serious security vulnerabilities. Previous solutions for securing software updates consider standard attacks and deploy widely understood security mechanisms, such as digital signatures for the software updates, and hardware security modules (HSM) to sign software updates. However, no existing solution considers more advanced security objectives, such as resilience against a repository compromise, or freeze attacks to the vehicle\u2019s update mechanism, or a compromise at a supplier\u2019s site. Solutions developed for the PC world do not generalize to automobiles for two reasons: first, they do not solve problems that are unique to the automotive industry (eg, that there are many different types of computers to be updated on a vehicle), and second, they do not address security attacks that can cause a vehicle to fail (eg a man-in-themiddle attack without compromising any signing key) or that can cause a vehicle to become unsafe. In this paper, we present Uptane, the first software update framework for automobiles that counters a comprehensive array of security attacks, and is resilient to partial compromises. Uptane adds strategic features to the state-of-the-art software update framework, TUF, in order to address automotivespecific vulnerabilities and limitations. Uptane is flexible and easy to adopt, and its design details were developed together with the main automotive industry stakeholders in the USA.", "num_citations": "35\n", "authors": ["678"]}
{"title": "Stork: Package Management for Distributed VM Environments.\n", "abstract": " In virtual machine environments each application is often run in its own virtual machine (VM), isolating it from other applications running on the same physical machine. Contention for memory, disk space, and network bandwidth among virtual machines, coupled with an inability to share due to the isolation virtual machines provide, leads to heavy resource utilization. Additionally, VMs increase management overhead as each is essentially a separate system.", "num_citations": "35\n", "authors": ["678"]}
{"title": "Proper: Privileged operations in a virtualised system environment\n", "abstract": " Virtualised systems have experienced a resurgence in popularity in recent years, particularly in supporting a large number of independent services on a single host. This paper describes our work designing and implementing Proper, a service running on the PlanetLab system, that allows other services to perform privileged operations in a safe, controlled manner. We describe how implementing such a system in a traditional UNIX environment is non-trivial, and discuss the practical use of Proper.", "num_citations": "35\n", "authors": ["678"]}
{"title": "Rhizoma: a runtime for self-deploying, self-managing overlays\n", "abstract": " The trend towards cloud and utility computing infrastructures raises challenges not only for application development, but also for management: diverse resources, changing resource availability, and differing application requirements create a complex optimization problem. Most existing cloud applications are managed externally, and this separation can lead to increased response time to failures, and slower or less appropriate adaptation to resource availability and pricing changes.               In this paper, we explore a different approach more akin to P2P systems: we closely couple a decentralized management runtime (\u201cRhizoma\u201d) with the application itself. The application expresses its resource requirements to the runtime as a constrained optimization problem. Rhizoma then fuses multiple real-time sources of resource availability data, from which it decides to acquire or release resources (such as virtual\u00a0\u2026", "num_citations": "34\n", "authors": ["678"]}
{"title": "Understanding misunderstandings in source code\n", "abstract": " Humans often mistake the meaning of source code, and so misjudge a program's true behavior. These mistakes can be caused by extremely small, isolated patterns in code, which can lead to significant runtime errors. These patterns are used in large, popular software projects and even recommended in style guides. To identify code patterns that may confuse programmers we extracted a preliminary set ofatoms of confusion'from known confusing code. We show empirically in an experiment with 73 participants that these code patterns can lead to a significantly increased rate of misunderstanding versus equivalent code without the patterns. We then go on to take larger confusing programs and measure (in an experiment with 43 participants) the impact, in terms of programmer confusion, of removing these confusing patterns. All of our instruments, analysis code, and data are publicly available online for replication\u00a0\u2026", "num_citations": "33\n", "authors": ["678"]}
{"title": "Diplomat: Using delegations to protect community repositories\n", "abstract": " Community repositories, such as Docker Hub, PyPI, and RubyGems, are bustling marketplaces that distribute software. Even though these repositories use common software signing techniques (eg, GPG and TLS), attackers can still publish malicious packages after a server compromise. This is mainly because a community repository must have immediate access to signing keys in order to certify the large number of new projects that are registered each day.", "num_citations": "30\n", "authors": ["678"]}
{"title": "Measuring the fitness of fitness trackers\n", "abstract": " Data collected by fitness trackers could play an important role in improving the health and well-being of the individuals who wear them. Many insurance companies even offer monetary rewards to participants who meet certain steps or calorie goals. However, in order for it to be useful, the collected data must be accurate and also reflect real-world performance. While previous studies have compared step counts data in controlled laboratory environments for limited periods of time, few studies have been done to measure performance over longer periods of time, while the subject does real-world activities. There are also few direct comparisons of a range of health indicators on different fitness tracking devices. In this study, we compared step counts, calories burned, and miles travelled data collected by three pairs of fitness trackers over a 14-day time period in free-living conditions. Our work indicates that the number\u00a0\u2026", "num_citations": "28\n", "authors": ["678"]}
{"title": "Blursense: Dynamic fine-grained access control for smartphone privacy\n", "abstract": " For many people, smartphones serve as a technical interface to the modern world. These smart devices have embedded on-board sensors, such as accelerometers, gyroscopes, GPS sensors, and cameras, which can be used to develop new mobile applications. However, the sensors also pose privacy risks to users. This work describes BlurSense, a tool that provides secure and customizable access to all of the sensors on smartphones, tablets, and similar end user devices. The current access control to the smartphone resources, such as sensor data, is static and coarse-grained. BlurSense is a dynamic, fine-grained, flexible access control mechanism, acting as a line of defense that allows users to define and add privacy filters. As a result, the user can expose filtered sensor data to untrusted apps, and researchers can collect data in a way that safeguards users' privacy.", "num_citations": "28\n", "authors": ["678"]}
{"title": "San ferm\u0131n: aggregating large data sets using a binomial swap forest\n", "abstract": " San Ferm\u0131n is a system for aggregating large amounts of data from the nodes of large-scale distributed systems. Each San Ferm\u0131n node individually computes the aggregated result by swapping data with other nodes to dynamically create its own binomial tree. Nodes that fall behind abort their trees, thereby reducing overhead. Having each node create its own binomial tree makes San Ferm\u0131n highly resilient to failures and ensures that the internal nodes of the tree have high capacity, thereby reducing completion time.Compared to existing solutions, San Ferm\u0131n handles large aggregations better, has higher completeness when nodes fail, computes the result faster, and has better scalability. We analyze the completion time, completeness, and overhead of San Ferm\u0131n versus existing solutions using analytical models, simulation, and experimentation with a prototype built on peer-to-peer system deployed on PlanetLab. Our evaluation shows that San Ferm\u0131n is scalable both in the number of nodes and in the aggregated data size. San Ferm\u0131n aggregates large amounts of data significantly faster than existing solutions: compared to SDIMS, an existing aggregation system, San Ferm\u0131n computes a 1MB result from 100 PlanetLab nodes in 61\u201376% of the time and from 2-6 times as many nodes. Even if 10% of the nodes fail during aggregation, San Ferm\u0131n still includes the data from 97% of the nodes in the result and does so faster than the underlying peer-to-peer system recovers from failures.", "num_citations": "28\n", "authors": ["678"]}
{"title": "Package management security\n", "abstract": " Package management is the task of determining which packages should be installed on a host and then downloading and installing those packages. This paper examines the popular package managers APT and YUM and presents nine feasible attacks on them. There are attacks that install malicious packages, deny users package updates, or cause the host to crash. This work identifies three rules of package management security: don\u2019t trust the repository, the trusted entity with the most information should be the one who signs, and don\u2019t install untrusted packages. The violation of these rules leads to the described vulnerabilities. Unfortunately, many of the flaws are architectural in nature, so repair requires more than patches to APT and YUM.While the rules of package management security argue that the design of existing package managers is insufficient, they do not prescribe how to provide security. This led to the development of three design principles for building a secure package manager: selective trust delegation, customized repository views, and explicitly treating the repository as untrusted. These principles were used to construct a package manager Stork which is not vulnerable to the attacks identified for YUM and APT. Stork has been in use for four years and has managed over half a million clients.", "num_citations": "27\n", "authors": ["678"]}
{"title": "Future Internet bandwidth trends: An investigation on current and future disruptive technologies\n", "abstract": " Abstract/Executive SummaryNew technologies sometimes result in disruptive changes to the existing infrastructure. Without adequate foresight, industry, academia, and government can be caught flat-footed. In this work, we focus on the trends surrounding home Internet bandwidth\u2014the bandwidth required by end user applications at home. As building and managing last mile network infrastructure incurs substantial cost, the foresight of such trends is necessary to plan upgrades.Using a bottom-up approach, we look at four potentially disruptive technologies, including millimeter wave wireless (mm-wave), the Internet of Things (IoT), Fog Computing, and Software Defined Networking (SDN). We examine use cases proposed by academia and industry, delve into the bandwidth requirements for proposed applications, and use this data to forecast future traffic demands for typical home users. Our projections show that bandwidth changes at end user devices will most likely be driven by two of the above technologies: millimeter wave wireless and Fog Computing. These technologies not only change the peak bandwidth, but also have noticeable secondary effects on bandwidth such as increasing upload bandwidth use, improving flash crowd tolerance, and increasing off-peak demand. While IoT and SDN are important, innovative technologies, they will not drastically alter the bandwidth usage patterns of ordinary users at home. We hope that the data and recommendations from this study can help business leaders and policy makers get an early jump on emerging technologies before they begin to shape the economy and society.", "num_citations": "26\n", "authors": ["678"]}
{"title": "Lock-in-pop: Securing privileged operating system kernels by keeping on the beaten path\n", "abstract": " Virtual machines (VMs) that try to isolate untrusted code are widely used in practice. However, it is often possible to trigger zero-day flaws in the host Operating System (OS) from inside of such virtualized systems. In this paper, we propose a new security metric showing strong correlation between \u201cpopular paths\u201d and kernel vulnerabilities. We verify that the OS kernel paths accessed by popular applications in everyday use contain significantly fewer security bugs than less-used paths. We then demonstrate that this observation is useful in practice by building a prototype system which locks an application into using only popular OS kernel paths. By doing so, we demonstrate that we can prevent the triggering of zero-day kernel bugs significantly better than three other competing approaches, and argue that this is a practical approach to secure system design.", "num_citations": "25\n", "authors": ["678"]}
{"title": "PolyPasswordHasher: protecting passwords in the event of a password file disclosure\n", "abstract": " Over the years, we have witnessed various password-hash database breaches that have affected small and large companies, with a diversity of users and budgets. The industry standard, salted hashing (and even key stretching), has proven to be insufficient protection against attackers who now have access to clusters of GPU-powered password crackers. Although there are various proposals for better securing password storage, most do not offer the same adoption model (software-only, server-side) as salted hashing, which may impede adoption.In this paper, we present PolyPasswordHasher, a softwareonly, server-side password storage mechanism that requires minimal additional work for the server, but exponentially increases the attacker\u2019s effort. PolyPasswordHasher uses a threshold cryptosystem to interrelate stored password data so that passwords cannot be individually cracked. Our analysis shows that Poly-PasswordHasher is memory and storage efficient, hard to crack, and easy to implement. In many realistic scenarios, cracking a PolyPasswordHasher-enabled database would be infeasible even for an adversary with millions of computers.", "num_citations": "22\n", "authors": ["678"]}
{"title": "Uptane: Security and customizability of software updates for vehicles\n", "abstract": " In this article we discuss Uptane, the first, to our knowledge, compromise-resilient software update security system designed specifically for vehicles. It is designed to make obtaining all the pieces required to control a vehicle extremely difficult for attackers.", "num_citations": "20\n", "authors": ["678"]}
{"title": "Tomato a virtual research environment for large scale distributed systems research\n", "abstract": " Networks and distributed systems are an important field of research. To enable experimental research in this field we propose a new tool ToMaTo (Topology Management Tool) which was developed to support research projects within the BMBF funded project G-Lab. It should support researchers from various branches of science who investigate distributed systems by providing a virtual environment for their research.Using various virtualization technologies, ToMaTo is able to provide realistic components that can run realworld software as well as lightweight components that can be used to analyze algorithms at large scale.This paper describes how an additional virtualization technology from the Seattle testbed has been added to ToMaTo to allow even larger experiments with distributed algorithms. Moreover the paper describes some concrete experiments that are carried out with ToMaTo.", "num_citations": "20\n", "authors": ["678"]}
{"title": "Experience with Seattle: A Community Platform for Research and Education\n", "abstract": " Hands-on experience is a critical part of research and education. Today's distributed testbeds fulfill that need for many students studying networking, distributed systems, cloud computing, security, operating systems, and similar topics. In this work, we discuss one such testbed, Seattle. Seattle is an open research and educational testbed that utilizes computational resources provided by end users on their existing devices. Unlike most other platforms, resources are not dedicated to the platform which allows a greater degree of network diversity and realism at the cost of programmability. Seattle is designed to preserve user security and to minimally impact application performance. We describe the architectural design of Seattle, and summarize our experiences with Seattle over the past few years as both researchers and educators. We have found that Seattle is very easy to adopt due to cross-platform support, and is\u00a0\u2026", "num_citations": "19\n", "authors": ["678"]}
{"title": "Dependable Self-Hosting Distributed Systems Using Constraints.\n", "abstract": " We describe a technique for writing distributed applications which manage themselves over one or more utility computing infrastructures: by dynamically acquiring new computational resources, deploying themselves on these resources, and releasing others when no longer required. Unlike prior work, such management functionality is closely integrated with the application, allowing greater freedom in application-specific policies and faster response to failures and other changes in the environment without requiring any external management system. We address the programming complexity of this approach by applying constraint logic programming, and describe Rhizoma, an experimental runtime to explore these ideas. We present early experience of deploying self-hosting applications on PlanetLab using Rhizoma.", "num_citations": "19\n", "authors": ["678"]}
{"title": "User anonymity on twitter\n", "abstract": " A novel machine-based classifier system leverages Twitter user anonymity patterns and their correlation to content sensitivity to automatically identify accounts that tweet sensitive content. Anonymity's role in society and the nuances and complexity of content sensitivity are confirmed.", "num_citations": "17\n", "authors": ["678"]}
{"title": "On omitting commits and committing omissions: Preventing git metadata tampering that (re) introduces software vulnerabilities\n", "abstract": " Metadata manipulation attacks represent a new threat class directed against Version Control Systems, such as the popular Git. This type of attack provides inconsistent views of a repository state to different developers, and deceives them into performing unintended operations with often negative consequences. These include omitting security patches, merging untested code into a production branch, and even inadvertently installing software containing known vulnerabilities. To make matters worse, the attacks are subtle by nature and leave no trace after being executed.", "num_citations": "17\n", "authors": ["678"]}
{"title": "Avoiding Theoretical Optimality to Efficiently and Privately Retrieve Security Updates\n", "abstract": " This work demonstrates the feasibility of building a PIR system with performance similar to non-PIR systems in real situations. Prior Chor PIR systems have chosen block sizes that are theoretically optimized to minimize communication. This (ironically) reduces the throughput of the resulting system by roughly 50x. We constructed a Chor PIR system called upPIR that is efficient by choosing block sizes that are theoretically suboptimal (from a communications standpoint), but fast and efficient in practice. For example, an upPIR mirror running on a threeyear- old desktop provides security updates from Ubuntu 10.04 (1.4 GB of data) fast enough to saturate a T3 link. Measurements run using mirrors distributed around the Internet demonstrate that a client can download software updates with upPIR about as quickly as with FTP.", "num_citations": "17\n", "authors": ["678"]}
{"title": "Prevalence of confusing code in software projects: atoms of confusion in the wild\n", "abstract": " Prior work has shown that extremely small code patterns, such as the conditional operator and implicit type conversion, can cause considerable misunderstanding in programmers. Until now, the real world impact of these patterns-known as' atoms of confusion'-was only speculative. This work uses a corpus of 14 of the most popular and influential open source C and C++ projects to measure the prevalence and significance of these small confusing patterns. Our results show that the 15 known types of confusing micro patterns occur millions of times in programs like the Linux kernel and GCC, appearing on average once every 23 lines. We show there is a strong correlation between these confusing patterns and bug-fix commits as well as a tendency for confusing patterns to be commented. We also explore patterns at the project level showing the rate of security vulnerabilities is higher in projects with more atoms\u00a0\u2026", "num_citations": "16\n", "authors": ["678"]}
{"title": "Teaching the security mindset with reference monitors\n", "abstract": " One of the central skills in computer security is reasoning about how programs fail. As a result, computer security necessarily involves thinking about the corner cases that arise when software executes. An unfortunate side effect of this is that computer security assignments typically necessitate deep understanding of a topic, such as how the stack is laid out in memory or how web applications interact with databases. This work presents a series of assignments that require very little background knowledge from students, yet provide them with the ability to reason about failures in programs. In this set of assignments, students implement two very simple programs in a high-level language (Python). Students first implement a reference monitor that tries to uphold a security property within a sandbox. For the second portion, the students are provided each others' reference monitors and then write attack code to try to bypass\u00a0\u2026", "num_citations": "14\n", "authors": ["678"]}
{"title": "Simultaneous graph embedding with bends and circular arcs\n", "abstract": " A simultaneous embedding of two vertex-labeled planar graphs on n vertices is possible if there exists a labeled point set of size n such that each of the graphs can be realized on that point set without crossings. We demonstrate how to simultaneously embed a path and an n-level planar graph and how to use radial embeddings for curvilinear simultaneous embeddings of a path and an outerplanar graph. We also show how to use star-shaped levels to find 2-bends per path edge simultaneous embeddings of a path and an outerplanar graph. All embedding algorithms run in O (n) time.", "num_citations": "13\n", "authors": ["678"]}
{"title": "Why It Is Hard to Build a Long-Running Service on PlanetLab.\n", "abstract": " PlanetLab was conceived as both an experimental testbed and a platform for long-running services. It has been quite successful at the former, less so at the latter. In this paper we examine why. The crux of the problem is that there are few incentives for researchers to develop long-running services. Research prototypes fulfill publishing requirements, whereas long-running services do not. Several groups have tried to deploy research services, long-running services that are useful, but also novel enough to be published. These services have been generally unsuccessful. In this paper we discuss the difficulties in developing a research service, our experiences in developing a research service called Stork, and offer suggestions on how to increase the incentives for researchers to develop research services.", "num_citations": "13\n", "authors": ["678"]}
{"title": "Can the security mindset make students better testers?\n", "abstract": " Writing secure code requires a programmer to think both as a defender and an attacker. One can draw a parallel between this model of thinking and techniques used in test-driven development, where students learn by thinking about how to effectively test their code and anticipate possible bugs. In this study, we analyzed the quality of both attack and defense code that students wrote for an assignment given in an introductory security class of 75 (both graduate and senior undergraduate levels) at NYU. We made several observations regarding students' behaviors and the quality of both their defensive and offensive code. We saw that student defensive programs (ie, assignments) are highly unique and that their attack programs (ie, test cases) are also relatively unique. In addition, we examined how student behaviors in writing defense programs correlated with their attack program's effectiveness. We found evidence\u00a0\u2026", "num_citations": "11\n", "authors": ["678"]}
{"title": "EdgeNet: A global cloud that spreads by local action\n", "abstract": " EdgeNet has been informed by the advances of cloud computing and the successes of such distributed systems as PlanetLab, GENI, G-Lab, SAVI, and V-Node: a large number of small points-of-presence, designed for the deployment of highly distributed experiments and applications. EdgeNet differs from its predecessors in two significant areas: first, it is a software-only infrastructure, where each worker node is designed to run part-or full-time on existing hardware at the local site; and, second, it uses modern, industry-standard software both as the node agent and the control framework. The first innovation permits rapid and unlimited scaling: whereas GENI and PlanetLab required the installation and maintenance of dedicated hardware at each site, EdgeNet requires only a software download, and a node can be added to the EdgeNet infrastructure in 15 minutes. The second offers performance, maintenance, and\u00a0\u2026", "num_citations": "10\n", "authors": ["678"]}
{"title": "Design of activity recognition systems with wearable sensors\n", "abstract": " Wearable sensors are widely utilized in human activity monitoring and recognition systems. Not only do these sensors come in different form factors but the software that comes bundled with them also varies from device to device and is constantly evolving. Also, multiple types of sensors on these devices are used to recognize human activity. Owing to the flexible form factor, these devices can also be mounted on a plethora of different positions on the human body. With all the aforementioned variables, it becomes imperative that the quality of data provided by wearable sensors needs to be evaluated. This paper describes an empirical study resulting in evaluating the accuracy of human activity recognition by wearable sensors based on the type of sensor, the physical mounting position of the sensor on the human body, their type of activity being monitored and the type of device being used. The paper further delves\u00a0\u2026", "num_citations": "10\n", "authors": ["678"]}
{"title": "Mining anonymity: identifying sensitive accounts on Twitter\n", "abstract": " We explore the feasibility of automatically finding accounts that publish sensitive content on Twitter. One natural approach to this problem is to first create a list of sensitive keywords, and then identify Twitter accounts that use these words in their tweets. But such an approach may overlook sensitive accounts that are not covered by the subjective choice of keywords. In this paper, we instead explore finding sensitive accounts by examining the percentage of anonymous and identifiable followers the accounts have. This approach is motivated by an earlier study showing that sensitive accounts typically have a large percentage of anonymous followers and a small percentage of identifiable followers. To this end, we first considered the problem of automatically determining if a Twitter account is anonymous or identifiable. We find that simple techniques, such as checking for name-list membership, perform poorly. We designed a machine learning classifier that classifies accounts as anonymous or identifiable. We then classified an account as sensitive based on the percentages of anonymous and identifiable followers the account has. We applied our approach to approximately 100,000 accounts with 404 million active followers. The approach uncovered accounts that were sensitive for a diverse number of reasons. These accounts span across varied themes, including those that are not commonly proposed as sensitive or those that relate to socially stigmatized topics. To validate our approach, we applied Latent Dirichlet Allocation (LDA) topic analysis to the tweets in the detected sensitive and non-sensitive accounts. LDA showed that the sensitive and\u00a0\u2026", "num_citations": "10\n", "authors": ["678"]}
{"title": "Mercury: Bandwidth-effective prevention of rollback attacks against community repositories\n", "abstract": " A popular community repository such as Docker Hub, PyPI, or RubyGems distributes tens of thousands of software projects to millions of users. The large number of projects and users make these repositories attractive targets for exploitation. After a repository compromise, a malicious party can launch a number of attacks on unsuspecting users, including rollback attacks that revert projects to obsolete and vulnerable versions. Unfortunately, due to the rapid rate at which packages are updated, existing techniques that protect against rollback attacks would cause each user to download 2\u20133 times the size of an average package in metadata each month, making them impractical to deploy.", "num_citations": "10\n", "authors": ["678"]}
{"title": "A fast multi-server, multi-block private information retrieval protocol\n", "abstract": " Private Information Retrieval (PIR) allows users to retrieve information from a database without revealing the content of these queries to anyone. The traditional information-theoretic PIR schemes utilize multiple servers to download single data block, thus incur high communication overhead and high computation burden. In this paper, we develop an Information- theoretic multi-block PIR scheme that significantly reduce the client communication and computation overheads by downloading multiple data blocks at a time. The design of k-safe binary matrices insures the information will not be revealed even if up to k servers collude. Our scheme has much lower overhead than the classic PIR schemes. The implementation of fast XOR operations benefits both servers and clients in reducing coding and decoding time. Our work demonstrates that multi-block PIR scheme can be optimized to simultaneously achieve low\u00a0\u2026", "num_citations": "10\n", "authors": ["678"]}
{"title": "Net-\u03c7: unified data-centric internet services\n", "abstract": " Databases and networks currently have different service models. Database services are data-centric in that users typically describe the content of data and the system finds and returns matching data. However, traditional Internet services are server-centric in that users have to know the location of data (eg, a URL) in order to retrieve it. We envision a future in which Internet services are data-centric. Users specify their interests and publishers describe their data. Based on the matching between user interests and data contents, users can pull data from publishers, and publishers can push data to interested users.", "num_citations": "10\n", "authors": ["678"]}
{"title": "MicroCash: Practical Concurrent Processing of Micropayments\n", "abstract": " Micropayments have a large number of potential applications. However, processing these small payments individually can be expensive, with transaction fees often exceeding the payment value itself. By aggregating the small transactions into a few larger ones, and using cryptocurrencies, today\u2019s decentralized probabilistic micropayment schemes can reduce these fees. Unfortunately, existing solutions force micropayments to be issued sequentially, thus to support fast issuance rates a customer needs a large number of escrows, which bloats the blockchain. Moreover, these schemes incur a large computation and bandwidth overhead, limiting their applicability in large-scale systems. In this paper, we propose MicroCash, the first decentralized probabilistic framework that supports concurrent micropayments. MicroCash introduces a novel escrow setup that enables a customer to concurrently issue payment tickets at a fast\u00a0\u2026", "num_citations": "9\n", "authors": ["678"]}
{"title": "Trust evaluation in mobile devices: An empirical study\n", "abstract": " Mobile devices today, such as smartphones and tablets, have become both more complex and diverse. This paper presents a framework to evaluate the trustworthiness of the individual components in a mobile system, as well as the entire system. The major components are applications, devices and networks of devices. Given this diversity and multiple levels of a mobile system, we develop a hierarchical trust evaluation methodology, which enables the combination of trust metrics and allows us to verify the trust metric for each component based on the trust metrics for others. The paper first demonstrates this idea for individual applications and Android-based smartphones. The methodology involves two stages: initial trust evaluation and trust verification. In the first stage, an expert rule system is used to produce trust metrics at the lowest level of the hierarchy. In the second stage, the trust metrics are verified by\u00a0\u2026", "num_citations": "9\n", "authors": ["678"]}
{"title": "Vulnerabilities as blind spots in developer's heuristic-based decision-making processes\n", "abstract": " The security community spares no effort in emphasizing security awareness and the importance of building secure software. However, the number of new vulnerabilities found in today's systems is still increasing. Furthermore, old and well-studied vulnerability types such as buffer overflows and SQL injections, are still repeatedly reported in vulnerability databases. Historically, the common response has been to blame the developers for their lack of security education. This paper discusses a new hypothesis to explain this problem by introducing a new security paradigm where software vulnerabilities are viewed as developers' blind spots in their decision making. We argue that such a flawed mental process is heuristic-based, where humans solve problems without considering all the information available, just like taking shortcuts. This paper's thesis is that security thinking tends to be left out by developers during\u00a0\u2026", "num_citations": "9\n", "authors": ["678"]}
{"title": "Collaboration with diamondtouch\n", "abstract": " We study the performance of collaborative spatial/visual tasks under different input configurations. The configurations used are a traditional mouse-monitor, a shared-monitor with multiple-mice, and a multi-user input device (DiamondTouch). Our experiments indicate that there is a significant variation in performance for the different configurations with pairs of users, while there is no such variation with individual users. The traditional configuration is not well-suited for collaborative tasks, and even after augmenting it to a shared monitor with multiple-mice it is still significantly inferior to the multi-user input device.", "num_citations": "9\n", "authors": ["678"]}
{"title": "Sensorium-A Generic Sensor Framework.\n", "abstract": " This contribution describes Sensorium, our framework for accessing sensor values on computing devices and making them available to other applications. Meanwhile, it allows users control the exposure of privacy-related data. Our goal is to bring the sensing capabilities of modern devices to a broader range of reseachers and experimenters via an open source framework. We also present a real application making use of Sensorium\u2019s virtues: For our web service Open3GMap, we crowd-source radio reception quality measurements in 3G networks. We combine the data into an open geo-information system.", "num_citations": "8\n", "authors": ["678"]}
{"title": "Sensibility testbed: Automated IRB policy enforcement in mobile research apps\n", "abstract": " Due to their omnipresence, mobile devices such as smartphones could be tremendously valuable to researchers. However, since research projects can extract data about device owners that could be personal or sensitive, there are substantial privacy concerns. Currently, the only regulation to protect user privacy for research projects is through Institutional Review Boards (IRBs) from researchers' institutions. However, there is no guarantee that researchers will follow the IRB protocol. Even worse, researchers without security expertise might build apps that are vulnerable to attacks.", "num_citations": "7\n", "authors": ["678"]}
{"title": "Sensibility testbed: An internet-wide cloud platform for programmable exploration of mobile devices\n", "abstract": " Smartphones contain sensors that provide information about the user of the device. Studying smartphones, therefore, provides an invaluable window into the behavioral patterns of users. In this work, we describe an open infrastructure that provides research communities with principled access to mobile devices and their sensors. Our platform, the Sensibility Testbed, is a free, community-driven platform for mobile devices. It provides secure data access to user-owned mobile devices while preserving user privacy. The unified programmable interface to sensors on heterogeneous devices in the Sensibility Testbed enables research scientists to design and deploy experiments that run across large numbers of devices, for example to measure the coverage and performance of cellular and WiFi networks. The Sensibility Testbed makes the sensing capabilities of smartphones more accessible to a broad range of\u00a0\u2026", "num_citations": "7\n", "authors": ["678"]}
{"title": "Et (smart) phone home!\n", "abstract": " Most home users are not able to troubleshoot advanced network issues themselves. Hours on the phone with an ISP's customer representative is a common way to solve this problem. With the advent of mobile devices with both Wi-Fi and cellular radios, troubleshooters at the ISP have a new back-door into a malfunctioning residential network. However, placing full trust in an ISP is a poor choice for a home user. In this paper we present Extra Technician (ET), a system designed to provide ISPs and others with an environment to troubleshoot home networking in a remote, safe and flexible manner.", "num_citations": "7\n", "authors": ["678"]}
{"title": "in-toto: Providing farm-to-table guarantees for bits and bytes\n", "abstract": " The software development process is quite complex and involves a number of independent actors. Developers check source code into a version control system, the code is compiled into software at a build farm, and CI/CD systems run multiple tests to ensure the software\u2019s quality among a myriad of other operations. Finally, the software is packaged for distribution into a delivered product, to be consumed by end users. An attacker that is able to compromise any single step in the process can maliciously modify the software and harm any of the software\u2019s users.", "num_citations": "6\n", "authors": ["678"]}
{"title": "Hands-on internet with seattle and computers from across the globe\n", "abstract": " The Internet Connectivity module is a short assignment covering distributed computing and networking. The Internet Connectivity module is part of the curriculum created for the Northwest Distributed Computer Science Department and is built upon the Seattle distributed computing platform. In this paper, we describe the module and illustrate how Seattle facilitates networking projects and experiments that use computers/resources from across the globe. In addition, we describe how the Internet Connectivity module was used in two courses, provide some comments on students' reactions to the project, and conclude with suggestions for faculty considering how to use this module in their future courses.", "num_citations": "6\n", "authors": ["678"]}
{"title": "Detecting latent cross-platform api violations\n", "abstract": " Many APIs enable cross-platform system development by abstracting over the details of a platform, allowing application developers to write one implementation that will run on a wide variety of platforms. Unfortunately, subtle differences in the behavior of the underlying platforms make cross-platform behavior difficult to achieve. As a result, applications using these APIs can be plagued by bugs difficult to observe before deployment. These portability bugs can be particularly difficult to diagnose and fix because they arise from the API implementation, the operating system, or hardware, rather than application code. This paper describes CheckAPI, a technique for detecting violations of cross-platform portability. CheckAPI compares an application's interactions with the API implementation to its interactions with a partial specification-based API implementation, and does so efficiently enough to be used in real production\u00a0\u2026", "num_citations": "5\n", "authors": ["678"]}
{"title": "Privacy-preserving experimentation with sensibility testbed\n", "abstract": " Recent privacy breaches and security break-ins of mobile systems have raised concerns about using mobile devices like smartphones and tablets [1]. As a result, many users are aware that running apps on their smartphones can increase privacy risks. On the other hand, the data from the enormous number of smartphones, if used properly, can be of tremendous value to the research community. Is there a way to safely do research on these devices without rendering them vulnerable? We explain about how our project may help both researchers and volunteers.", "num_citations": "5\n", "authors": ["678"]}
{"title": "Fence: protecting device availability with uniform resource control\n", "abstract": " Applications such as software updaters or a run-away web app, even if low priority, can cause performance degradation, loss of battery life, or other issues that reduce a computing device\u2019s availability. The core problem is that OS resource control mechanisms unevenly apply uncoordinated policies across different resources. This paper shows how handling resources\u2013eg, CPU, memory, sockets, and bandwidth\u2013in coordination, through a unifying abstraction, can be both simpler and more effective. We abstract resources along two dimensions of fungibility and renewability, to enable resource-agnostic algorithms to provide resource limits for a diverse set of applications.", "num_citations": "5\n", "authors": ["678"]}
{"title": "Towards a representive testbed: Harnessing volunteers for networks research\n", "abstract": " A steady rise in home systems has been seen over the past few years. As more systems are designed and deployed, an appropriate testbed is required to test these systems. Several systems exist, such as PlanetLab, that currently provide a networking testbed allowing researchers and developers to test and measure various applications. However in the long run such testbeds will be unable to keep up and meet all the demands of many of the large scale modern day peer-to-peer systems. We outline the various challenges and essentials of a networking testbed and we provide an alternate networking testbed that is driven by resources that are voluntarily contributed. We talk about the various advantages and disadvantages of the Seattle system, an open source peer-topeer computing testbed that has the potential to meet these demands. The testbed is composed of sandboxed resources that are donated by volunteers. Seattle has been deployed for about three years and supports many researchers who are interested in a networking testbed. The testbed consists of over 4100 nodes and is constantly growing. Seattle looks to grow and meet the demands of networking testbeds as they are made.", "num_citations": "5\n", "authors": ["678"]}
{"title": "Stork: Secure package management for VM environments\n", "abstract": " Package managers are a common tool for installing, removing, and updating software on modern computer systems. Unfortunately existing package managers have two major problems. First, inadequate security leads to vulnerability to attack. There are nine feasible attacks against modern package managers, many of which are enabled by flaws in the underlying security architecture. Second, in Virtual Machine (VM) environments such as Xen, VMWare, and VServers, different VMs on the same physical machine are treated as separate systems by package managers leading to redundant package downloads and installations.", "num_citations": "5\n", "authors": ["678"]}
{"title": "Commit signatures for centralized version control systems\n", "abstract": " Version Control Systems (VCS-es) play a major role in the software development life cycle, yet historically their security has been relatively underdeveloped compared to their importance. Recent history has shown that source code repositories represent appealing attack targets. Attacks that violate the integrity of repository data can impact negatively millions of users. Some VCS-es, such as Git, employ commit signatures as a mechanism to provide developers with cryptographic protections for the code they contribute to a repository. However, an entire class of other VCS-es, including the well-known Apache Subversion (SVN), lacks such protections.               We design the first commit signing mechanism for centralized version control systems, which supports features such as working with a subset of the repository and allowing clients to work on disjoint sets of files without having to retrieve each other\u2019s\u00a0\u2026", "num_citations": "4\n", "authors": ["678"]}
{"title": "Practical fog computing with seattle\n", "abstract": " In this paper we present Seattle, a practical and publicly accessible fog computing platform with a deployment history going back to 2009. Seattle's cross-platform portable sandbox implementation tackles the widely-recognized issue of node heterogeneity. Its componentized architecture supports a number of approaches to operating a Seattle-based fog system, from isolated, standalone and peer-to-peer operations, to full-fledged provisioning by a dedicated operator, or federations of many operators. Seattle's components and interfaces are designed for compatibility and reuse, and may be aligned with existing trust boundaries between different stakeholders. Seattle comprises implementations of all components discussed in this paper. Its free, open-source software stack has been used for teaching and research; outside groups have used existing Seattle components and constructed new components with\u00a0\u2026", "num_citations": "4\n", "authors": ["678"]}
{"title": "Teaching networking and distributed systems with seattle: tutorial presentation\n", "abstract": " In this tutorial we describe an educational testbed called Seattle [1] that an instructor can use to enhance a course on networks or distributed systems. We give an overview of the Seattle educational testbed and describe assignments and resources available to educators planning to use Seattle in the classroom. Finally, we discuss the applicability of Seattle to other types of distributed systems paradigms such as cloud computing, peer-to-peer networking, and ubiquitous computing. The purpose of the Seattle testbed is to provide instructors with resources on computers all around the world. This allows students to learn about real world network behavior as well as topics like cloud computing and peer-to-peer computing. Seattle is free and is comprised of resources donated from universities. Today, Seattle consists of resources on about 1000 computers at over 100 universities on 6 continents.", "num_citations": "4\n", "authors": ["678"]}
{"title": "Teaching networking and distributed systems with Seattle\n", "abstract": " In this tutorial we describe an educational testbed called Seattle [1] that an instructor can use to enhance a course on networks or distributed systems. We give an overview of the Seattle educational testbed and describe assignments and resources available to educators planning to use Seattle in the classroom. Finally, we discuss the applicability of Seattle to other types of distributed systems paradigms such as cloud computing, peer-to-peer networking, and ubiquitous computing.The purpose of the Seattle testbed is to provide instructors with resources on computers all around the world. This allows students to learn about real world network behavior as well as topics like cloud computing and peer-to-peer computing. Seattle is free and is comprised of resources donated from universities. Today, Seattle consists of resources on about 1000 computers at over 100 universities on 6 continents.", "num_citations": "4\n", "authors": ["678"]}
{"title": "Package managers still vulnerable: how to project your systems\n", "abstract": " Serious vulnerabilities have been discovered in all Linux package managers. Although most major package managers and distributors have begun addressing these issues, some of the most popular are not fully protecting this users. We'll look at kown vulnerabilities in package managers, what is being done to fix them, and how you can protect your system even if you're using a vulnerable package manager.", "num_citations": "4\n", "authors": ["678"]}
{"title": "Dscats: Animating data structures for cs 2 and cs 3 courses\n", "abstract": " A new data structure animation tool called DsCats is available for classroom use. This tool supports educator presentations, student experimentation, and programming assignments. It implements a user-centered approach supporting a wide range of detail levels, the ability to jump to any point in the animation, and on the fly variations in the data structure during animations. The tool is written in Java with modularity and expandability in mind.", "num_citations": "4\n", "authors": ["678"]}
{"title": "EdgeNet: A Multi-Tenant and Multi-Provider Edge Cloud\n", "abstract": " EdgeNet is a public Kubernetes cluster dedicated to network and distributed systems research, supporting experiments that are deployed concurrently by independent groups. Its nodes are hosted by multiple institutions around the world. It represents a departure from the classic Kubernetes model, where the nodes that are available to a single tenant reside in a small number of well-interconnected data centers. The free open-source EdgeNet code extends Kubernetes to the edge, making three key contributions: multi-tenancy, geographical deployments, and single-command node installation. We show that establishing a public Kubernetes cluster over the internet, with multiple tenants and multiple hosting providers is viable. Preliminary results also indicate that the EdgeNet testbed that we run provides a satisfactory environment to run a variety of experiments with minimal network overhead.", "num_citations": "3\n", "authors": ["678"]}
{"title": "CAPnet: a defense against cache accounting attacks on content distribution networks\n", "abstract": " Peer-assisted content distribution networks (CDNs)have emerged to improve performance and reduce deployment costs of traditional, infrastructure-based content delivery networks. This is done by employing peer-to-peer data transfers to supplement the resources of the network infrastructure. However, these hybrid systems are vulnerable to accounting attacks in which the peers, or caches, collude with clients in order to report that content was transferred when it was not. This is a particular issue in systems that incentivize cache participation, because malicious caches may collect rewards from the content publishers operating the CDN without doing any useful work. In this paper, we introduce CAPnet, the first technique that lets untrusted caches join a peer-assisted CDN while providing a bound on the effectiveness of accounting attacks. At its heart is a lightweight cache accountability puzzle that clients must\u00a0\u2026", "num_citations": "3\n", "authors": ["678"]}
{"title": "ABC: a cryptocurrency-focused threat modeling framework\n", "abstract": " Cryptocurrencies are an emerging economic force, but there are concerns about their security. This is due, in part, to complex collusion cases and new threat vectors, that could be missed by conventional security assessment strategies. To address these issues, we propose ABC, an Asset-Based Cryptocurrency-focused threat modeling framework capable of identifying such risks. ABC's key innovation is the use of collusion matrices. A collusion matrix forces a threat model to cover a large space of threat cases while simultaneously manages this process to prevent it from being overly complex. We demonstrate that ABC is effective by presenting real-world use cases and by conducting a user study. The user study showed that around 71% of those who used ABC were able to identify financial security threats, as compared to only 13% of participants who used the popular framework STRIDE.", "num_citations": "3\n", "authors": ["678"]}
{"title": "Finding sensitive accounts on Twitter: an automated approach based on follower anonymity\n", "abstract": " We explore the feasibility of automatically finding accounts that publish sensitive content on Twitter, by examining the percentage of anonymous and identifiable followers the accounts have. We first designed a machine learning classifier to automatically determine if a Twitter account is anonymous or identifiable. We then classified an account as potentially sensitive based on the percentages of anonymous and identifiable followers the account has. We applied our approach to approximately 100,000 accounts with 404 million active followers. The approach uncovered accounts that were sensitive for a diverse number of reasons.", "num_citations": "3\n", "authors": ["678"]}
{"title": "Tsumiki: A meta-platform for building your own testbed\n", "abstract": " Network testbeds are essential research tools that have been responsible for valuable network measurements and major advances in distributed systems research. However, no single testbed can satisfy the requirements of every research project, prompting continual efforts to develop new testbeds. The common practice is to re-implement functionality anew for each testbed. This work introduces a set of ready-to-use software components and interfaces called Tsumiki to help researchers to rapidly prototype custom networked testbeds without substantial effort. We derive Tsumiki's design using a set of component and interface design principles, and demonstrate that Tsumiki can be used to implement new, diverse, and useful testbeds. We detail a few such testbeds: a testbed composed of Android devices, a testbed that uses Docker for sandboxing, and a testbed that shares computation and storage resources\u00a0\u2026", "num_citations": "2\n", "authors": ["678"]}
{"title": "Model-based Testing Without a Model: Assessing Portability in the Seattle Testbed.\n", "abstract": " Despite widespread OS, network, and hardware heterogeneity, there has been a lack of research into quantifying and improving portability of a programming environment. We have constructed a distributed testbed called Seattle built on a platform-independent programming API that is implemented on different operating systems and architectures. Our goal is to show that applications written to our API will be portable. In this work, we use an instrumented version of the programming environment for testing purposes. The instrumentation allows us to gather traces of actual program behavior from a running implementation. These traces can be used across different versions of the implementation exactly as if they were test cases generated offline from a model program, so we can commence testing using model based testing tools, without constructing a model program.Such offline testing is only effective in scenarios where traces are expected to be reproducible (deterministic). Where reproducibility is not expected, for instance due to nondeterminism in the network environment, we must resort to on-the-fly testing, which does require a model program. To validate this model program, we can use the recorded traces of actual behavior. Validating with captured traces should provide greater coverage than we could achieve by validating only with traces constructed a priori.", "num_citations": "2\n", "authors": ["678"]}
{"title": "le-git-imate: Towards verifiable web-based Git repositories\n", "abstract": " Web-based Git hosting services such as GitHub and GitLab are popular choices to manage and interact with Git repositories. However, they lack an important security feature-the ability to sign Git commits. Users instruct the server to perform repository operations on their behalf and have to trust that the server will execute their requests faithfully. Such trust may be unwarranted though because a malicious or a compromised server may execute the requested actions in an incorrect manner, leading to a different state of the repository than what the user intended.", "num_citations": "1\n", "authors": ["678"]}
{"title": "Securing Software Updates for Automotives Using Uptane\n", "abstract": " The automotive industry has traditionally relied upon proprietary strategies developed behind closed doors. However, experience in the software security community suggests that open development processes can find flaws before they can be exploited. We introduce Uptane, a secure system for updating software on automobiles that follows the open door strategy. It was jointly developed with the University of Michigan Transportation Research Institute (UMTRI), and the Southwest Research Institute (SWRI), with input from the automotive industry as well as government regulators. We are now looking for academics and security researchers to attempt to break our system before black-hat hackers do it in the real world\u2014with possibly fatal consequences.", "num_citations": "1\n", "authors": ["678"]}
{"title": "Teaching security using hands-on exercises in 2015\n", "abstract": " We see teaching cybersecurity through hands-on, interactive exercises as a way to engage students. Some of the exercises that we have seen require significant preparation on the part of the instructor. Having a community makes it easier to share exercises, knowing what works and what problems students and instructors have encountered. The purpose of this BOF is to bring together instructors who have developed hands-on exercises, those who have used them and those who would like to. We recognize that few CS programs can afford new required courses, so we will discuss ways to integrate security-related exercises into existing ones. This could include networking, OS, computer architecture, programming languages, software engineering, algorithms and programming. The questions we will ask are,\" What exercises have you tried? What are your experiences? What are you looking for?\"", "num_citations": "1\n", "authors": ["678"]}
{"title": "Teaching security using hands-on exercises\n", "abstract": " We see teaching information security through hands-on, interactive exercises as a way to engage students. Some of the exercises that we have tried require significant preparation on the part of the instructor. Having a community makes it easier to share exercises, knowing what works and what problems students and instructors have encountered. The purpose of this BOF is to bring together instructors who have used hands-on exercises and those who would like to. We recognize that few CS programs can afford new required courses, so we would be discussing ways to integrate security-related exercises into existing ones. This could include networking, OS, computer architecture, programming languages, software engineering and algorithms. The questions we will ask are,\" What exercises, if any, have you tried\" What are your experiences? What are you looking for?", "num_citations": "1\n", "authors": ["678"]}
{"title": "Lind: Challenges turning virtual composition into reality\n", "abstract": " Security is a constant sore spot in application development. Applications now need structural support for better isolation and security on a domain specific basis to stave off the multitude of modern security vulnerabilities. Currently, application developers have been relying upon cumbersome workarounds to address these issues. We propose the design and initial implementation details for Lind, a highly flexible composition infrastructure that can be well-integrated with modern application development processes and extends traditional mechanisms like virtualization and software fault isolation in a way that can be tailored according to an application\u2019s need. Lind does this by providing the structures and services needed to build a virtual component model. Since compositions of virtual components are different than current software systems, building and using virtual component models provides a new set of software engineering challenges in composition and system construction. As a possible solution to many modern security problems, it is important to understand how virtual component models can be evaluated, to further both the user\u2019s understanding of them, and future research in this area. This paper proposes a design and implementation strategy for components that run in isolation. Then the paper presents an evaluation of the efficacy of this approach in terms of performance, isolation, security and composition provides insight into the possible advantages and disadvantages of a virtual component model.", "num_citations": "1\n", "authors": ["678"]}
{"title": "TransCloud: a distributed environment based on dynamic networking\n", "abstract": " GEC 10 Demonstrations TransCloud: A Distributed Environment Based On Dynamic Networking Page 1 Sponsored by the National Science Foundation GEC 10 Demonstrations TransCloud: A Distributed Environment Based On Dynamic Networking Rick McGeer, HP Labs Joe Mambretti, Northwestern Paul M\u00fcller , TU Kaiserslautern Chris Matthews, Chris Pearson, Yvonne Coady, Victoria Jim Chen, Fei Yeh, Northwestern Andy Bavier, PlanetWorks Marco Yuen, Princeton Jessica Blaine, Alvin Au Young, HP Labs Alex Snoeren, UC San Diego March 16, 2010 http://www.icair.org http://www.geni.net Page 2 Sponsored by the National Science Foundation 2 November 3, 2010 Introduction \u2013 TransCloud \u2022 TransCloud = A Cloud Where Services Migrate, Anytime, Anywhere In a World Where Distance Is Eliminated \u2013 Joint Project Between GENICloud, iGENI, et al \u2013 GENICloud Provides Seamless Interoperation of Cloud \u2026", "num_citations": "1\n", "authors": ["678"]}
{"title": "NanoXen: Better Systems Through Rigorous Containment and Active Modeling\n", "abstract": " Modern software design has less writing large programs and more orchestrating the actions of prewritten library elements. These elements, generally known as components, are stateful software elements which can operate and interact in unexpected ways. Most errors in large systems result from unanticipated behavior from components, or unexpected interaction between components. In this paper, we argue that two principal innovations permit the rapid construction of far more robust and reliable software systems: rigorous containment, to control interactions, and active modeling with dynamic model checking, to rapidly detect unexpected behavior. We outline a small set of requirements which will produce such a system, NanoXen, of virtual components, the component analog to virtual machines.", "num_citations": "1\n", "authors": ["678"]}
{"title": "Centralized Package Management using Stork\n", "abstract": " MANAGING THE SOFTWARE INSTALLED on multiple systems can be one of the duller aspects of system administration. One has to deal with varied sets of packages, each replicated on numerous machines, and bring up new systems, with the complication of those that are almost like the others, but not quite. In many cases, great amounts of time could be saved and more than a few mistakes avoided by using tools specifically created to make this job easier.", "num_citations": "1\n", "authors": ["678"]}
{"title": "A Resource Allocation Framework for Global Service-Oriented Networks\n", "abstract": " We study the problem of allocating resources on global networks where there is no central administrative control. We describe a framework that abstractly describes a number of components that are necessary in an auction system to provide users with a secure trading environment. We propose solutions for specific issues relating to auction granularity, cost/value effective bidding, bids on large resource sets, currency control, and computationally effective auction resolution. We then describe the application of our framework to PlanetLab and how the components would be implemented on this system.", "num_citations": "1\n", "authors": ["678"]}