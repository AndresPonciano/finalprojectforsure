{"title": "Foundational proof certificates in first-order logic\n", "abstract": " It is the exception that provers share and trust each others proofs. One reason for this is that different provers structure their proof evidence in remarkably different ways, including, for example, proof scripts, resolution refutations, tableaux, Herbrand expansions, natural deductions, etc. In this paper, we propose an approach to foundational proof certificates as a means of flexibly presenting proof evidence so that a relatively simple and universal proof checker can check that a certificate does, indeed, elaborate to a formal proof. While we shall limit ourselves to first-order logic in this paper, we shall not limit ourselves in many other ways. Our framework for defining and checking proof certificates will work with classical and intuitionistic logics and with proof structures as diverse as resolution refutations, matings, and natural deduction.", "num_citations": "41\n", "authors": ["1431"]}
{"title": "A semantic framework for proof evidence\n", "abstract": " Theorem provers produce evidence of proof in many different formats, such as proof scripts, natural deductions, resolution refutations, Herbrand expansions, and equational rewritings. In implemented provers, numerous variants of such formats are actually used: consider, for example, such variants of or restrictions to resolution refutations as binary resolution, hyper-resolution, ordered-resolution, paramodulation, etc. We propose the foundational proof certificates (FPC) framework for defining the semantics of a broad range of proof evidence. This framework allows both producers of proof certificates and the checkers of those certificates to have a clear formal definition of the semantics of a wide variety of proof evidence. Employing the FPC framework will allow one to separate a proof from its provenance and to allow anyone to construct their own proof checker for a given style of proof evidence. The\u00a0\u2026", "num_citations": "30\n", "authors": ["1431"]}
{"title": "The proof certifier checkers\n", "abstract": " Different theorem provers work within different formalisms and paradigms, and therefore produce various incompatible proof objects. Currently there is a big effort to establish foundational proof certificates (fpc), which would serve as a common \u201cspecification language\u201d for all these formats. Such framework enables the uniform checking of proof objects from many different theorem provers while relying on a small and trusted kernel to do so. Checkers is an implementation of a proof checker using foundational proof certificates. By trusting a small kernel based on (focused) sequent calculus on the one hand and by supporting fpc specifications in a prolog-like language on the other hand, it can be used for checking proofs of a wide range of theorem provers. The focus of this paper is on the output of equational resolution theorem provers and for this end, we specify the paramodulation rule. We describe the\u00a0\u2026", "num_citations": "22\n", "authors": ["1431"]}
{"title": "Real behavior of floating point numbers\n", "abstract": " We present an efficient constraint programming (CP) approach to the SMTLIB theory of quantifier-free floating-point arithmetic (QF FP). We rely on dense interreduction between many domain representations to greatly reduce the search space. We compare our tool to current state-of-the-art SMT solvers and show that it is consistently better on large problems involving non-linear arithmetic operations (for which bit-blasting techniques tend to scale badly). Our results emphasize the importance of the conservation of the high-level structure of the original problems.", "num_citations": "19\n", "authors": ["1431"]}
{"title": "Translating between implicit and explicit versions of proof\n", "abstract": " The Foundational Proof Certificate (FPC) framework can be used to define the semantics of a wide range of proof evidence. For example, such definitions exist for a number of textbook proof systems as well as for the proof evidence output from some existing theorem proving systems. An important decision in designing a proof certificate format is the choice of how many details are to be placed within certificates. Formats with fewer details are smaller and easier for theorem provers to output but they require more sophistication from checkers since checking will involve some proof reconstruction. Conversely, certificate formats containing many details are larger but are checkable by less sophisticated checkers. Since the FPC framework is based on well-established proof theory principles, proof certificates can be manipulated in meaningful ways. In this paper, we illustrate how it is possible to automate moving\u00a0\u2026", "num_citations": "15\n", "authors": ["1431"]}
{"title": "Sharpening constraint programming approaches for bit-vector theory\n", "abstract": " We address the challenge of developing efficient Constraint Programming-based approaches for solving formulas over the quantifier-free fragment of the theory of bitvectors (BV), which is of paramount importance in software verification. We propose CP(BV), a highly efficient BV resolution technique built on carefully chosen anterior results sharpened with key original features such as thorough domain combination or dedicated labeling. Extensive experimental evaluations demonstrate that CP(BV) is much more efficient than previous similar attempts from the CP community, that it is indeed able to solve the majority of the standard verification benchmarks for bitvectors, and that it already complements the standard SMT approaches on several crucial (and industry-relevant) aspects, notably in terms of scalability w.r.t.\u00a0bit-width, theory combination or intricate mix of non-linear arithmetic and bitwise operators\u00a0\u2026", "num_citations": "12\n", "authors": ["1431"]}
{"title": "Checking foundational proof certificates for first-order logic\n", "abstract": " We present the design philosophy of a proof checker based on a notion of foundational proof certificates. At the heart of this design is a semantics of proof evidence that arises from recent advances in the theory of proofs for classical and intuitionistic logic. That semantics is then performed by a (higher-order) logic program: successful performance means that a formal proof of a theorem has been found. We describe how the lambda Prolog programming language provides several features that help guarantee such a soundness claim. Some of these features (such as strong typing, abstract datatypes, and higher-order programming) were features of the ML programming language when it was first proposed as a proof checker for LCF. Other features of lambda Prolog (such as support for bindings, substitution, and backtracking search) turn out to be equally important for describing and checking the proof evidence encoded in proof certificates. Since trusting our proof checker requires trusting a programming language implementation, we discuss various avenues for enhancing one's trust of such a checker.", "num_citations": "12\n", "authors": ["1431"]}
{"title": "Proof certificates for equality reasoning\n", "abstract": " The kinds of inference rules and decision procedures that one writes for proofs involving equality and rewriting are rather different from proofs that one might write in first-order logic using, say, sequent calculus or natural deduction. For example, equational logic proofs are often chains of replacements or applications of oriented rewriting and normal forms. In contrast, proofs involving logical connectives are trees of introduction and elimination rules. We shall illustrate here how it is possible to check various equality-based proof systems with a programmable proof checker (the kernel checker) for first-order logic. Our proof checker's design is based on the implementation of focused proof search and on making calls to (user-supplied) clerks and experts predicates that are tied to the two phases found in focused proofs. It is the specification of these clerks and experts that provide a formal definition of the structure of\u00a0\u2026", "num_citations": "9\n", "authors": ["1431"]}
{"title": "Classical polarizations yield double-negation translations\n", "abstract": " Double-negation translations map formulas to formulas in such a way that if a formula is a classical theorem then its translation is an intuitionistic theorem. We shall go beyond just examining provability by looking at correspondences between inference rules in classical proofs and in intuitionistic proofs of translated formulas. In order to make this comparison interesting and precise, we will examine focused versions of proofs in classical and intuitionistic logics using the LKF and LJF proof systems. We shall show that for a number of known double-negation translations, one can get essentially identical (focused) intuitionistic proofs as (focused) classical proofs. Thus the choice of a common double-negation translation is really the same choice as a polarization of classical logic (of which there are many).", "num_citations": "5\n", "authors": ["1431"]}
{"title": "Certification of First-order proofs in classical and intuitionistic logics\n", "abstract": " The field of automated reasoning contains a plethora of methods and tools, each with its own language and its community, often evolving separately. These tools express their proofs, or some proof evidence, in different formats such as resolution refutations, proof scripts, natural deductions, expansion trees, equational rewritings and many others. The disparity in formats reduces communication and trust between the different communities. Related efforts were deployed to fill the gaps in communication including libraries and languages bridging two or more tools. This thesis proposes a novel approach at filling this gap for first-order classical and intuitionistic logics. Rather than translating proofs written in various languages to proofs written in one chosen language, this thesis introduces a framework for describing the semantics of a wide range of proof evidence languages through a relational specification, called Foundational Proof Certification (FPC). The description of the semantics of a language can then be appended to any proof evidence written in that language, forming a proof certificate, which allows a small kernel checker to verify them independently from the tools that created them. The use of semantics description for one language rather than proof translation from one language to another relieves one from the need to radically change the notion of proof. Proof evidence, unlike complete proof, does not have to contain all details. Using proof reconstruction, a kernel checker can rebuild missing parts of the proof, allowing for compression and gain in storage space. Other desired aspects such as non-determinism, parallelism and flexibility in\u00a0\u2026", "num_citations": "4\n", "authors": ["1431"]}
{"title": "CAMUS: A Framework to Build Formal Specifications for Deep Perception Systems Using Simulators\n", "abstract": " The topic of provable deep neural network robustness has raised considerable interest in recent years. Most research has focused on adversarial robustness, which studies the robustness of perceptive models in the neighbourhood of particular samples. However, other works have proved global properties of smaller neural networks. Yet, formally verifying perception remains uncharted. This is due notably to the lack of relevant properties to verify, as the distribution of possible inputs cannot be formally specified. We propose to take advantage of the simulators often used either to train machine learning models or to check them with statistical tests, a growing trend in industry. Our formulation allows us to formally express and verify safety properties on perception units, covering all cases that could ever be generated by the simulator, to the difference of statistical tests which cover only seen examples. Along with this theoretical formulation , we provide a tool to translate deep learning models into standard logical formulae. As a proof of concept, we train a toy example mimicking an autonomous car perceptive unit, and we formally verify that it will never fail to capture the relevant information in the provided inputs.", "num_citations": "3\n", "authors": ["1431"]}
{"title": "A semantics for proof evidence\n", "abstract": " The approach: Provide a framework for defining the semantics of \u201cproof evidence\u201d. A proof certificate is a particular piece of proof evidence following a given semantic definition. Have two trusted kernels (one each for classical and intuitionistic logics) which check that a given certificate can be elaborated into an actual proof.", "num_citations": "2\n", "authors": ["1431"]}
{"title": "DISCO Verification: Division of Input Space into COnvex polytopes for neural network verification\n", "abstract": " The impressive results of modern neural networks partly come from their non linear behaviour. Unfortunately, this property makes it very difficult to apply formal verification tools, even if we restrict ourselves to networks with a piecewise linear structure. However, such networks yields subregions that are linear and thus simpler to analyse independently. In this paper, we propose a method to simplify the verification problem by operating a partitionning into multiple linear subproblems. To evaluate the feasibility of such an approach, we perform an empirical analysis of neural networks to estimate the number of linear regions, and compare them to the bounds currently known. We also present the impact of a technique aiming at reducing the number of linear regions during training.", "num_citations": "1\n", "authors": ["1431"]}
{"title": "CDCL-inspired Word-level Learning for Bit-vector Constraint Solving\n", "abstract": " The theory of quantifier-free bit-vectors (QF_BV) is of paramount importance in software verification. The standard approach for satisfiability checking reduces the bit-vector problem to a Boolean problem, leveraging the powerful SAT solving techniques and their conflict-driven clause learning (CDCL) mechanisms. Yet, this bit-level approach loses the structure of the initial bit-vector problem. We propose a conflict-driven, word-level, combinable constraints learning for the theory of quantifier-free bit-vectors. This work paves the way to truly word-level decision procedures for bit-vectors, taking full advantage of word-level propagations recently designed in CP and SMT communities.", "num_citations": "1\n", "authors": ["1431"]}
{"title": "The exp-log normal form of types and canonical terms for lambda calculus with sums\n", "abstract": " In the presence of sum types, the eta-long beta-normal form of terms of lambda calculus is not canonical. Natural deduction systems for intuitionistic logic (with disjunction) suffer the same defect, thanks to the Curry-Howard correspondence. This canonicity problem has been open in Proof Theory since the 1960s, while it has been addressed in Computer Science, since the 1990s, by a number of authors using decision procedures: instead of deriving a notion of syntactic canonical normal form, one gives a procedure based on program analysis to decide when any two terms of the lambda calculus with sum types are essentially the same one. In this paper, we show the canonicity problem is difficult because it is too specialized: rather then picking a canonical representative out of a class of beta-eta-equal terms of a given type, one should do so for the enlarged class of terms that are of a type isomorphic to the given one. We isolate a type normal form, ENF, generalizing the usual disjunctive normal form to handle exponentials, and we show that the eta-long beta-normal form of terms at ENF type is canonical, when the eta axiom for sums is expressed via evaluation contexts. By coercing terms from a given type to its isomorphic ENF type, our technique gives unique canonical representatives for examples that had previously been handled using program analysis.", "num_citations": "1\n", "authors": ["1431"]}