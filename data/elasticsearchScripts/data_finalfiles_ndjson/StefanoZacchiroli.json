{"title": "User interaction with the Matita proof assistant\n", "abstract": " Matita is a new, document-centric, tactic-based interactive theorem prover. This paper focuses on some of the distinctive features of the user interaction with Matita, characterized mostly by the organization of the library as a searchable knowledge base, the emphasis on a high-quality notational rendering, and the complex interplay between syntax, presentation, and semantics.", "num_citations": "100\n", "authors": ["918"]}
{"title": "Strong dependencies between software components\n", "abstract": " Component-based systems often describe context requirements in terms of explicit inter-component dependencies. Studying large instances of such systems - such as free and open source software (FOSS) distributions - in terms of declared dependencies between packages is appealing. It is however also misleading when the language to express dependencies is as expressive as Boolean formulae, which is often the case. In such settings, a more appropriate notion of component dependency exists: strong dependency. This paper introduces such notion as a first step towards modeling semantic, rather then syntactic, inter-component relationships. Furthermore, a notion of component sensitivity is derived from strong dependencies, with applications to quality assurance and to the evaluation of upgrade risks. An empirical study of strong dependencies and sensitivity is presented, in the context of one of the largest\u00a0\u2026", "num_citations": "77\n", "authors": ["918"]}
{"title": "Software heritage: Why and how to preserve software source code\n", "abstract": " Software is now a key component present in all aspects of our society. Its preservation has attracted growing attention over the past years within the digital preservation community. We claim that source code\u2014the only representation of software that contains human readable knowledge\u2014is a precious digital object that needs special handling: it must be a first class citizen in the preservation landscape and we need to take action immediately, given the increasingly more frequent incidents that result in permanent losses of source code collections. In this paper we present Software Heritage, an ambitious initiative to collect, preserve, and share the entire corpus of publicly accessible software source code. We discuss the archival goals of the project, its use cases and role as a participant in the broader digital preservation ecosystem, and detail its key design decisions. We also report on the project road map and the current status of the Software Heritage archive that, as of early 2017, has collected more than 3 billion unique source code files and 700 million commits coming from more than 50 million software development projects.", "num_citations": "76\n", "authors": ["918"]}
{"title": "A content based mathematical search engine: Whelp\n", "abstract": " The prototype of a content based search engine for mathematical knowledge supporting a small set of queries requiring matching and/or typing operations is described. The prototype \u2014 called Whelp \u2014 exploits a metadata approach for indexing the information that looks far more flexible than traditional indexing techniques for structured expressions like substitution, discrimination, or context trees. The prototype has been instantiated to the standard library of the Coq proof assistant extended with many user contributions.", "num_citations": "74\n", "authors": ["918"]}
{"title": "Package upgrades in FOSS distributions: Details and challenges\n", "abstract": " The upgrade problems faced by Free and Open Source Software distributions have characteristics not easily found elsewhere. We describe the structure of packages and their role in the upgrade process. We show that state of the art package managers have shortcomings inhibiting their ability to cope with frequent upgrade failures. We survey current counter-measures to such failures, argue that they are not satisfactory, and sketch alternative solutions.", "num_citations": "70\n", "authors": ["918"]}
{"title": "Building the universal archive of source code\n", "abstract": " A global collaborative project for the benefit of all.", "num_citations": "60\n", "authors": ["918"]}
{"title": "Dependency solving: a separate concern in component evolution management\n", "abstract": " Maintenance of component-based software platforms often has to face rapid evolution of software components. Component dependencies, conflicts, and package managers with dependency solving capabilities are the key ingredients of prevalent software maintenance technologies that have been proposed to keep software installations synchronized with evolving component repositories. We review state-of-the-art package managers and their ability to keep up with evolution at the current growth rate of popular component-based platforms, and conclude that their dependency solving abilities are not up to the task.We show that the complexity of the underlying upgrade planning problem is NP-complete even for seemingly simple component models, and argue that the principal source of complexity lies in multiple available versions of components. We then discuss the need of expressive languages for user\u00a0\u2026", "num_citations": "53\n", "authors": ["918"]}
{"title": "Crafting a proof assistant\n", "abstract": " Proof assistants are complex applications whose development has never been properly systematized or documented. This work is a contribution in this direction, based on our experience with the development of Matita: a new interactive theorem prover based\u2014as Coq\u2014on the Calculus of Inductive Constructions (CIC). In particular, we analyze its architecture focusing on the dependencies of its components, how they implement the main functionalities, and their degree of reusability.             The work is a first attempt to provide a ground for a more direct comparison between different systems and to highlight the common functionalities, not only in view of reusability but also to encourage a more systematic comparison of different softwares and architectural solutions.", "num_citations": "51\n", "authors": ["918"]}
{"title": "Automated synthesis and deployment of cloud applications\n", "abstract": " Complex networked applications are assembled by connecting software components distributed across multiple machines. Building and deploying such systems is a challenging problem which requires a significant amount of expertise: the system architect must ensure that all component dependencies are satisfied, avoid conflicting components, and add the right amount of component replicas to account for quality of service and fault-tolerance. In a cloud environment, one also needs to minimize the virtual resources provisioned upfront, to reduce the cost of operation. Once the full architecture is designed, it is necessary to correctly orchestrate the deployment phase, to ensure all components are started and connected in the right order.", "num_citations": "49\n", "authors": ["918"]}
{"title": "Supporting software evolution in component-based FOSS systems\n", "abstract": " FOSS (Free and Open Source Software) systems present interesting challenges in system evolution. On one hand, most FOSS systems are based on very fine-grained units of software deployment\u2013called packages\u2013which promote system evolution; on the other hand, FOSS systems are among the largest software systems known and require sophisticated static and dynamic conditions to be verified, in order to successfully deploy upgrades on users\u2019 machines. The slightest error in one of these conditions can turn a routine upgrade into a system administrator\u2019s nightmare.In this paper we introduce a model-based approach to support the upgrade of FOSS systems. The approach promotes the simulation of upgrades to predict failures before affecting the real system. Both fine-grained static aspects (e.g.\u00a0configuration incoherences) and dynamic aspects (e.g.\u00a0the execution of configuration scripts) are taken into account\u00a0\u2026", "num_citations": "47\n", "authors": ["918"]}
{"title": "Towards a formal component model for the cloud\n", "abstract": " We consider the problem of deploying and (re)configuring resources in a \u201ccloud\u201d setting, where interconnected software components and services can be deployed on clusters of heterogeneous (virtual) machines that can be created and connected on-the-fly. We introduce the Aeolus component model to capture similar scenarii from realistic cloud deployments, and instrument automated planning of day-to-day activities such as software upgrade planning, service deployment, elastic scaling, etc. We formalize the model and characterize the feasibility and complexity of configuration achievability in Aeolus.", "num_citations": "46\n", "authors": ["918"]}
{"title": "The ultimate debian database: Consolidating bazaar metadata for quality assurance and data mining\n", "abstract": " FLOSS distributions like RedHat and Ubuntu require a lot more complex infrastructures than most other FLOSS projects. In the case of community-driven distributions like Debian, the development of such an infrastructure is often not very organized, leading to new data sources being added in an impromptu manner while hackers set up new services that gain acceptance in the community. Mixing and matching data is then harder than should be, albeit being badly needed for Quality Assurance and data mining. Massive refactoring and integration is not a viable solution either, due to the constraints imposed by the bazaar development model. This paper presents the Ultimate Debian Database (UDD), which is the countermeasure adopted by the Debian project to the above \u00bfdata hell\u00bf. UDD gathers data from various data sources into a single, central SQL database, turning Quality Assurance needs that could not be\u00a0\u2026", "num_citations": "45\n", "authors": ["918"]}
{"title": "Common upgradeability description format (CUDF) 2.0\n", "abstract": " The solver competition which will be organized by Mancoosi relies on the standardized format for describing package upgrade scenarios. This document describes the Common Upgradeability Description Format (CUDF), the document format used to encode upgrade scenarios, abstracting over distribution-specific details. Solvers taking part in the competition will be fed with input in CUDF format.", "num_citations": "43\n", "authors": ["918"]}
{"title": "MPM: a modular package manager\n", "abstract": " Software distributions in the FOSS world rely on so-called package managers for the installation and removal of packages on target machines. State-of-the-art package managers are monolithic in architecture, and each of them is hard-wired to an ad-hoc dependency solver implementing a customized heuristics. In this paper we propose a modular architecture allowing for pluggable dependency solvers and backends. We argue that this is the path that leads to the next generation of package managers that will deliver better results, accept more expressive input languages, and can be easily adaptable to new platforms. We present a working prototype---called MPM---which has been implemented following the design advocated in this paper.", "num_citations": "36\n", "authors": ["918"]}
{"title": "Constrained wiki: an oxymoron?\n", "abstract": " In this paper we propose a new wiki concept---light constraints---designed to encode community best practices and domain-specific requirements, and to assist in their application. While the idea of constraining user editing of wiki content seems to inherently contradict\" The Wiki Way\", it is well-known that communities of users involved in wiki sites have the habit of establishing best authoring practices. For domain-specific wiki systems which process wiki content, it is often useful to enforce some well-formedness conditions on specific page contents. This paper describes a general framework to think about the interaction of wiki system with constraints, and presents a generic architecture which can be easily incorporated into existing wiki systems to exploit the capabilities enabled by light constraints.", "num_citations": "36\n", "authors": ["918"]}
{"title": "Content cloaking: preserving privacy with google docs and other web applications\n", "abstract": " Web office suites such as Google Docs offer unparalleled collaboration experiences in terms of low software requirements, ease of use, data ubiquity, and availability. When the data holder (Google, Microsoft, etc.) is not perceived as trusted though, those benefits are considered at stake with important privacy requirements. Content cloaking is a lightweight, cryptographic, client-side solution to protect content from data holders while using web office suites and other\" Web 2.0\", AJAX-based, collaborative applications.", "num_citations": "31\n", "authors": ["918"]}
{"title": "Towards the unification of formats for overlapping markup\n", "abstract": " Overlapping markup refers to the issue of how to represent data structures more expressive than trees\u2014for example direct acyclic graphs\u2014using markup (meta-) languages which have been designed with trees in mind\u2014for example XML. In this paper we observe that the state of the art in overlapping markup is far from being the widespread and consistent stack of standards and technologies readily available for XML and develop a roadmap for closing the gap.             In particular we present in the paper the design and implementation of what we believe to be the first needed step, namely: a syntactic conversion framework among the plethora of overlapping markup serialization formats. The algorithms needed to perform the various conversions are presented in pseudo-code, they are meant to be used as blueprints for researchers and practitioners which need to write batch translation programs from one\u00a0\u2026", "num_citations": "31\n", "authors": ["918"]}
{"title": "Wiki content templating\n", "abstract": " Wiki content templating enables reuse of content structures among wiki pages. In this paper we present a thorough study of this widespread feature, showing how its two state of the art models (functional and creational templating) are sub-optimal. We then propose a third, better, model called lightly constrained (LC) templating and show its implementation in the Moin wiki engine. We also show how LC templating implementations are the appropriate technologies to push forward semantically rich web pages on the lines of (lowercase) semantic web and microformats.", "num_citations": "30\n", "authors": ["918"]}
{"title": "Tinycals: step by step tacticals\n", "abstract": " Most of the state-of-the-art proof assistants are based on procedural proof languages, scripts, and rely on LCF tacticals as the primary tool for tactics composition. In this paper we discuss how these ingredients do not interact well with user interfaces based on the same interaction paradigm of Proof General (the de facto standard in this field), identifying in the coarse-grainedness of tactical evaluation the key problem.We propose Tinycals as an alternative to a subset of LCF tacticals, showing that the user does not experience the same problem if tacticals are evaluated in a more fine-grained manner. We present the formal operational semantics of tinycals as well as their implementation in the Matita proof assistant.", "num_citations": "26\n", "authors": ["918"]}
{"title": "The Software Heritage graph dataset: public software development under one roof\n", "abstract": " Software Heritage is the largest existing public archive of software source code and accompanying development history: it currently spans more than five billion unique source code files and one billion unique commits, coming from more than 80 million software projects. This paper introduces the Software Heritage graph dataset: a fully-deduplicated Merkle DAG representation of the Software Heritage archive. The dataset links together file content identifiers, source code directories, Version Control System (VCS) commits tracking evolution over time, up to the full states of VCS repositories as observed by Software Heritage during periodic crawls. The dataset's contents come from major development forges (including GitHub and GitLab), FOSS distributions (e.g., Debian), and language-specific package managers (e.g., PyPI). Crawling information is also included, providing timestamps about when and where all\u00a0\u2026", "num_citations": "22\n", "authors": ["918"]}
{"title": "Identifiers for digital objects: the case of software source code preservation\n", "abstract": " In the very broad scope addressed by digital preservation initiatives, a special place belongs to the scientific and technical artifacts that we need to properly archive to enable scientific reproducibility. For these artifacts we need identifiers that are not only unique and persistent, but also support integrity in an intrinsic way. They must provide strong guarantees that the object denoted by a given identifier will always be the same, without relying on third parties and external administrative processes. In this article, we report on our quest for this identifiers for digital objects (IDOs), whose properties are different from, and complementary to, those of the various digital identifiers of objects (DIOs) that are in widespread use today. We argue that both kinds of identifiers are needed and present the framework for intrinsic persistent identifiers that we have adopted in Software Heritage for preserving billions of software artifacts.", "num_citations": "22\n", "authors": ["918"]}
{"title": "A modular package manager architecture\n", "abstract": " ContextThe success of modern software distributions in the Free and Open Source world can be explained, among other factors, by the availability of a large collection of software packages and the possibility to easily install and remove those components using state-of-the-art package managers. However, package managers are often built using a monolithic architecture and hard-wired and ad-hoc dependency solvers implementing some customized heuristics.ObjectiveWe aim at laying the foundation for improving on existing package managers. Package managers should be complete, that is find a solution whenever there exists one, and allow the user to specify complex criteria that define how to pick the best solution according to the user\u2019s preferences.MethodIn this paper we propose a modular architecture relying on precise interface formalisms that allows the system administrator to choose from a variety of\u00a0\u2026", "num_citations": "22\n", "authors": ["918"]}
{"title": "Searching mathematics on the web: state of the art and future developments\n", "abstract": " A huge amount of mathematical knowledge is nowadays available on the World Wide Web. Many different solutions and technologies for searching that knowledge have been developed as well. We present the state of the art of searching mathematics on the web, giving some insight on future developments in this area. 1.", "num_citations": "22\n", "authors": ["918"]}
{"title": "Feature diagrams as package dependencies\n", "abstract": " FOSS (Free and Open Source Software) distributions use dependencies and package managers to maintain huge collections of packages and their installations; recent research have led to efficient and complete configuration tools and techniques, based on state of the art solvers, that are being adopted in industry. We show how to encode a significant subset of Free Feature Diagrams as interdependent packages, enabling to reuse package tools and research results into software product lines.", "num_citations": "20\n", "authors": ["918"]}
{"title": "Solving package dependencies: from EDOS to Mancoosi\n", "abstract": " Mancoosi (Managing the Complexity of the Open Source Infrastructure) is an ongoing research project funded by the European Union for addressing some of the challenges related to the \"upgrade problem\" of interdependent software components of which Debian packages are prototypical examples. Mancoosi is the natural continuation of the EDOS project which has already contributed tools for distribution-wide quality assurance in Debian and other GNU/Linux distributions. The consortium behind the project consists of several European public and private research institutions as well as some commercial GNU/Linux distributions from Europe and South America. Debian is represented by a small group of Debian Developers who are working in the ranks of the involved universities to drive and integrate back achievements into Debian. This paper presents relevant results from EDOS in dependency management and gives an overview of the Mancoosi project and its objectives, with a particular focus on the prospective benefits for Debian.", "num_citations": "20\n", "authors": ["918"]}
{"title": "Efficient ambiguous parsing of mathematical formulae\n", "abstract": " Mathematical notation has the characteristic of being ambiguous: operators can be overloaded and information that can be deduced is often omitted. Mathematicians are used to this ambiguity and can easily disambiguate a formula making use of the context and of their ability to find the right interpretation.             Software applications that have to deal with formulae usually avoid these issues by fixing an unambiguous input notation. This solution is annoying for mathematicians because of the resulting tricky syntaxes and becomes a show stopper to the simultaneous adoption of tools characterized by different input languages.             In this paper we present an efficient algorithm suitable for ambiguous parsing of mathematical formulae. The only requirement of the algorithm is the existence of a \u201cvalidity\u201d predicate over abstract syntax trees of incomplete formulae with placeholders. This requirement can be\u00a0\u2026", "num_citations": "20\n", "authors": ["918"]}
{"title": "Mining component repositories for installability issues\n", "abstract": " Component repositories play an increasingly relevant role in software life-cycle management, from software distribution to end-user, to deployment and upgrade management. Software components shipped via such repositories are equipped with rich metadata that describe their relationship (e.g., Dependencies and conflicts) with other components. In this practice paper we show how to use a tool, distcheck, that uses component metadata to identify all the components in a repository that cannot be installed (e.g., Due to unsatisfiable dependencies), provides detailed information to help developers understanding the cause of the problem, and fix it in the repository. We report about detailed analyses of several repositories: the Debian distribution, the OPAM package collection, and Drupal modules. In each case, distcheck is able to efficiently identify not installable components and provide valuable explanations of the\u00a0\u2026", "num_citations": "18\n", "authors": ["918"]}
{"title": "Learning from the future of component repositories\n", "abstract": " An important aspect of the quality assurance of large component repositories is to ensure the logical coherence of component metadata, and to this end one needs to identify incoherences as early as possible. Some relevant classes of problems can be formulated in term of properties of the future repositories into which the current repository may evolve. However, checking such properties on all possible future repositories requires a way to construct a finite representation of the infinite set of all potential futures. A class of properties for which this can be done is presented in this work.We illustrate the practical usefulness of the approach with two quality assurance applications: (i) establishing the amount of \u201cforced upgrades\u201d induced by introducing new versions of existing components in a repository, and (ii) identifying outdated components that are currently not installable and need to be upgraded in order to become\u00a0\u2026", "num_citations": "18\n", "authors": ["918"]}
{"title": "Debsources: Live and historical views on macro-level software evolution\n", "abstract": " Context. Software evolution has been an active field of research in recent years, but studies on macro-level software evolution---ie, on the evolution of large software collections over many years---are scarce, despite the increasing popularity of intermediate vendors as a way to deliver software to final users.Goal. We want to ease the study of both day-by-day and long-term Free and Open Source Software (FOSS) evolution trends at the macro-level, focusing on the Debian distribution as a proxy of relevant FOSS projects.Method. We have built Debsources, a software platform to gather, search, and publish on the Web all the source code of Debian and measures about it. We have set up a public Debsources instance at http://sources. debian. net, integrated it into the Debian infrastructure to receive live updates of new package releases, and written plugins to compute popular source code metrics. We have injected all\u00a0\u2026", "num_citations": "17\n", "authors": ["918"]}
{"title": "From notation to semantics: There and back again\n", "abstract": " Mathematical notation is a structured, open, and ambiguous language. In order to support mathematical notation in MKM applications one must necessarily take into account presentational as well as semantic aspects. The former are required to create a familiar, comfortable, and usable interface to interact with. The latter are necessary in order to process the information meaningfully.               In this paper we investigate a framework for dealing with mathematical notation in a meaningful, extensible way, and we show an effective instantiation of its architecture to the field of interactive theorem proving. The framework builds upon well-known concepts and widely-used technologies and it can be easily adopted by other MKM applications.", "num_citations": "17\n", "authors": ["918"]}
{"title": "Towards maintainer script modernization in FOSS distributions\n", "abstract": " Free and Open Source Software (FOSS) distributions are complex software systems, made of thousands packages that evolve rapidly, independently, and without centralized coordination. During packages upgrades, corner case failures can be encountered and are hard to deal with, especially when they are due to misbehaving maintainer scripts: executable code snippets used to finalize package configuration.", "num_citations": "16\n", "authors": ["918"]}
{"title": "Referencing source code artifacts: a separate concern in software citation\n", "abstract": " Among the entities involved in software citation, software source code requires special attention due to the role it plays in ensuring scientific reproducibility. To reference source code, we need identifiers that are not only unique and persistent, but also support integrity checking intrinsically. Suitable identifiers must guarantee that denoted objects will always stay the same, without relying on external third parties and administrative processes. We analyze the role of identifiers for digital objects, whose properties are different from, and complementary to, those of the various digital identifiers of objects that are today popular building blocks of software and data citation toolchains. We argue that both kinds of identifiers are needed and detail the syntax, semantics, and practical implementation of the persistent identifiers adopted by the Software Heritage project to reference billions of software source code artifacts such as\u00a0\u2026", "num_citations": "15\n", "authors": ["918"]}
{"title": "\u2018Open source has won and lost the war\u2019: Legitimising commercial\u2013communal hybridisation in a FOSS project\n", "abstract": " Information technology (IT) firms are paying developers in Free and Open Source Software (FOSS) projects, leading to the emergence of hybrid forms of work. In order to understand how the firm\u2013project hybridisation process occurs, we present the results of an online survey of participants in the Debian project, as well as interviews with Debian Developers. We find that the intermingling of the commercial logic of the firm and the communal logic of the project requires rhetorical legitimation. We analyse the discourses used to legitimise firm\u2013project cooperation as well as the organisational mechanisms which facilitate this cooperation. A first phase of legitimation, based on firm adoption of open licenses and developer self-fulfilment, aims to erase the commercial/communal divide. A second more recent phase seeks to professionalise work relations inside the project and, in doing so, challenges the social order which\u00a0\u2026", "num_citations": "12\n", "authors": ["918"]}
{"title": "Optimal provisioning in the cloud\n", "abstract": " Complex distributed systems are classically assembled by deploying several existing software components to multiple servers. Building such systems is a challenging problem that requires a significant amount of problem solving as one must i) ensure that all inter-component dependencies are satisfied; ii) ensure that no conflicting components are deployed on the same machine; and iii) take into account replication and distribution to account for quality of service, or possible failure of some services. We propose a tool, Zephyrus, that automates to a great extent assembling complex distributed systems. Given i) a high level specification of the desired system architecture, ii) the set of available components and their requirements) and iii) the current state of the system, Zephyrus is able to generate a formal representation of the desired system, to place the components in an optimal manner on the available machines, and to interconnect them as needed.", "num_citations": "12\n", "authors": ["918"]}
{"title": "Spacetime characterization of real-time collaborative editing\n", "abstract": " Real-Time Collaborative Editing (RTCE) is a popular way of instrumenting cooperative work on documents, in particular on the Web. Little is known in the literature yet about RTCE usage patterns in the real world. In this paper we study how a popular RTCE editor (Etherpad) is used in the wild, digging into the edit histories of a large collection of documents (about 14 000 pads), retrieved from one of the most popular public instances of the platform, hosted by the Wikimedia Foundation. The pad analysis is supported by a novel conceptual model that allows to label edit operations as \"collaborative\" or not depending on their distance-in edit position (space), edit time, or spacetime (both)-from edits made by other authors. The model is applied to classify all edits from the pad corpus. Classification results are further used to characterize the collaboration behavior of pad authors. Findings show that: 1) about half of the\u00a0\u2026", "num_citations": "10\n", "authors": ["918"]}
{"title": "Expressing advanced user preferences in component installation\n", "abstract": " State of the art component-based software collections-such as FOSS distributions-are made of up to dozens of thousands components, with complex inter-dependencies and conflicts. Given a particular installation of such a system, each request to alter the set of installed components has potentially (too) many satisfying answers.", "num_citations": "10\n", "authors": ["918"]}
{"title": "Where are your manners? Sharing best community practices in the Web 2.0\n", "abstract": " The Web 2.0 fosters the creation of communities by offering users a wide array of social software tools. While the success of these tools is based on their ability to support different interaction patterns among users by imposing as few limitations as possible, the communities they support are not free of rules (just think about the posting rules in a community forum or the editing rules in a thematic wiki).", "num_citations": "10\n", "authors": ["918"]}
{"title": "A model driven approach to upgrade package-based software systems\n", "abstract": " Complex software systems are often based on the abstraction of package, brought to popularity by Free and Open Source Software (FOSS) distributions. While helpful as an encapsulation layer, packages do not solve all problems of deployment, and more generally of management, of large software collections. In particular upgrades, which can affect several packages at once due to inter-package dependencies, often fail and do not hold good transactional properties. This paper shows how to apply model driven techniques to describe and manage software upgrades of FOSS distributions. It is discussed how to model static and dynamic aspects of package upgrades\u2014the latter being the more challenging to deal with\u2014in order to be able to predict common causes of upgrade failures and undo residual effects of failed or undesired upgrades.", "num_citations": "9\n", "authors": ["918"]}
{"title": "Spurious disambiguation error detection\n", "abstract": " The disambiguation approach to the input of formulae enables the user to type correct formulae in a terse syntax close to the usual ambiguous mathematical notation. When it comes to incorrect formulae we want to present only errors related to the interpretation meant by the user, hiding errors related to other interpretations (spurious errors).               We propose a heuristic to recognize spurious errors, which has been integrated with the disambiguation algorithm of [6].", "num_citations": "7\n", "authors": ["918"]}
{"title": "User interaction widgets for interactive theorem proving\n", "abstract": " Matita (that means pencil in Italian) is a new interactive theorem prover under development at the University of Bologna. When compared with state-of-the-art proof assistants, Matita presents both traditional and innovative aspects. The underlying calculus of the system, namely the Calculus of (Co)Inductive Constructions (CIC for short), is well-known and is used as the basis of another mainstream proof assistant\u2014Coq\u2014with which Matita is to some extent compatible. In the same spirit of several other systems, proof authoring is conducted by the user as a goal directed proof search, using a script for storing textual commands for the system. In the tradition of LCF, the proof language of Matita is procedural and relies on tactic and tacticals to proceed toward proof completion. The interaction paradigm offered to the user is based on the script management technique at the basis of the popularity of the Proof General generic interface for interactive theorem provers: while editing a script the user can move forth the execution point to deliver commands to the system, or back to retract (or \u201cundo\u201d) past commands. Matita has been developed from scratch in the past 8 years by several members of the Helm research group, this thesis author is one of such members. Matita is now a full-fledged proof assistant with a library of about 1.000 concepts. Several innovative solutions spun-off from this development effort. This thesis is about the design and implementation of some of those solutions, in particular those relevant for the topic of user interaction with theorem provers, and of which this thesis author was a major contributor. Joint work with other members of the\u00a0\u2026", "num_citations": "7\n", "authors": ["918"]}
{"title": "Spurious disambiguation errors and how to get rid of them\n", "abstract": " The disambiguation approach to the input of formulae enables users of mathematical assistants to type correct formulae in a terse syntax close to the usual ambiguous mathematical notation. When it comes to incorrect formulae however, far too many typing errors are generated; among them we want to present only errors related to the formula interpretation meant by the user, hiding errors related to other interpretations.               We study disambiguation errors and how to classify them into the spurious and genuine error classes. To this end we give a general presentation of the classes of disambiguation algorithms and efficient disambiguation algorithms. We also quantitatively assess the quality of the presented error classification criteria benchmarking them in the setting of a formal development of constructive algebra.", "num_citations": "6\n", "authors": ["918"]}
{"title": "The Debsources dataset: Two decades of Debian source code metadata\n", "abstract": " We present the Debsources Dataset: distribution metadata and source code metrics spanning two decades of Free and Open Source Software (FOSS) history, seen through the lens of the Debian distribution. Debsources is a software platform used to gather, search, and publish on the Web the full source code of the Debian operating system, as well as measures about it. A notable public instance of Debsources is available at http://sources.debian.net, it includes both current and historical releases of Debian. Plugins to compute popular source code metrics (lines of code, defined symbols, disk usage) and other derived data (e.g., Checksums) have been written, integrated, and run on all the source code available on sources.debian.net. The Debsources Dataset is a PostgreSQL database dump of sources.debian.net metadata, as of February 10th, 2015. The dataset contains both Debian-specific metadata -- e.g\u00a0\u2026", "num_citations": "5\n", "authors": ["918"]}
{"title": "Constrained wiki: The WikiWay to validating content\n", "abstract": " The \u201cWikiWay\u201d is the open editing philosophy of wikis meant to foster open collaboration and continuous improvement of their content. Just like other online communities, wikis often introduce and enforce conventions, constraints, and rules for their content, but do so in a considerably softer way, expecting authors to deliver content that satisfies the conventions and the constraints, or, failing that, having volunteers of the community, the WikiGnomes, fix others' content accordingly. Constrained wikis is our generic framework for wikis to implement validators of community-specific constraints and conventions that preserve the WikiWay and their open collaboration features. To this end, specific requirements need to be observed by validators and a specific software architecture can be used for their implementation, that is, as independent functions (implemented as internal modules or external services) used in a\u00a0\u2026", "num_citations": "5\n", "authors": ["918"]}
{"title": "Debian: 18 years of free software, do-ocracy, and democracy\n", "abstract": " Debian: 17 years of Free Software, ``do-ocracy'', and democracy Page 1 Debian: 17 years of Free Software, \u201cdo-ocracy\u201d, and democracy Stefano Zacchiroli Debian Project Leader 21 March 2011 Software Liberty Association of Taiwan (SLAT) Taipei, Taiwan Stefano Zacchiroli (Debian) Debian: do-ocracy and democracy Taipei, Taiwan 1 / 43 Page 2 Outline 1 What is Debian? History A system, a project, a community 2 What\u2019s so special about Debian? 3 More in-depth Commitments Decision making Processes 4 Derivatives 5 Contribute to Debian Stefano Zacchiroli (Debian) Debian: do-ocracy and democracy Taipei, Taiwan 2 / 43 Page 3 Prelude \u2014 the notion of \u201cdistribution\u201d distributions are meant to ease software management key notion: the abstraction of package offer coherent collections of software killer application: package managers Stefano Zacchiroli (Debian) Debian: do-ocracy and democracy Taipei, Taiwan \u2026", "num_citations": "5\n", "authors": ["918"]}
{"title": "Enforcing Type-Safe Linking using Inter-Package Relationships.\n", "abstract": " Strongly-typed languages rely on link-time checks to ensure that type safety is not violated at the borders of compilation units. Such checks entail very fine-grained dependencies among compilation units, which are at odds with the implicit assumption of backward compatibility that is relied upon by common library packaging techniques adopted by FOSS (Free and", "num_citations": "5\n", "authors": ["918"]}
{"title": "Mapping Open Source Capitalism: The Firm-Volunteer Project Co-Production Network and its Media Representation\n", "abstract": " Free and Open source software (FOSS) produced by volunteers in self-governed projects is being used in", "num_citations": "4\n", "authors": ["918"]}
{"title": "The creation of a new type of scientific deposit: Software\n", "abstract": " Software has become an indissociable support of technical and scientific knowledge. The preservation of this universal body of knowledge has become as essential as preserving research articles and data sets. Software preservation is a pillar of reproducibility.", "num_citations": "4\n", "authors": ["918"]}
{"title": "Preliminary report on the influence of capital in an ethical-modular project: Quantitative data from the 2016 Debian survey\n", "abstract": " The benefits of private ownership mainly flow to owners and shareholders in the shape of financial profits, efficient and reliable operations, and the control of a docile workforce. In addition, markets enable transactions to be recorded, and different types of speculation to occur, which engender more profits for owners. The benefits of ethical-modular organisations (EMOs) are socialized: there are no restrictions on who profits. A central characteristic of EMOs is that participants relinquish exclusive property rights over the resource they have created. In order to investigate the relationship between paid development time and contribution to Debian and the question of how corporate interests influence the direction of the project, we created a survey using the online tool LimeSurvey. Questions focused on motivation for contribution, employer and professional interest in the Debian project, and other professional concerns. Additionally, we gathered demographic information, including age, country of residence, education level, and status within the project. The survey was approved by the University of Canberra Human Research Ethics Committee.", "num_citations": "4\n", "authors": ["918"]}
{"title": "Gender differences in public code contributions: a 50-year perspective\n", "abstract": " We study the gender of commits authors over 120 million projects and a period of 50 years. Commits by female authors remain low overall but are growing steadily, providing hope of a more gender-balanced future for collaborative software development.", "num_citations": "3\n", "authors": ["918"]}
{"title": "Software provenance tracking at the scale of public source code\n", "abstract": " We study the possibilities to track provenance of software source code artifacts within the largest publicly accessible corpus of publicly available source code, the Software Heritage archive, with over 4 billions unique source code files and 1 billion commits capturing their development histories across 50 million software projects. We perform a systematic and generic estimate of the replication factor across the different layers of this corpus, analysing how much the same artifacts (e.g., SLOC, files or commits) appear in different contexts (e.g., files, commits or source code repositories). We observe a combinatorial explosion in the number of identical source code files across different commits. To discuss the implication of these findings, we benchmark different data models for capturing software provenance information at this scale, and we identify a viable solution, based on the properties of isochrone subgraphs, that is\u00a0\u2026", "num_citations": "3\n", "authors": ["918"]}
{"title": "Ultra-large-scale repository analysis via graph compression\n", "abstract": " We consider the problem of mining the development history-as captured by modern version control systems-of ultra-large-scale software archives (e.g., tens of millions software repositories corresponding). We show that graph compression techniques can be applied to the problem, dramatically reducing the hardware resources needed to mine similarly-sized corpus. As a concrete use case we compress the full Software Heritage archive, consisting of 5 billion unique source code files and 1 billion unique commits, harvested from more than 80 million software projects-encompassing a full mirror of GitHub. The resulting compressed graph fits in less than 100 GB of RAM, corresponding to a hardware cost of less than 300 U.S. dollars. We show that the compressed in-memory representation of the full corpus can be accessed with excellent performances, with edge lookup times close to memory random access. As a\u00a0\u2026", "num_citations": "3\n", "authors": ["918"]}
{"title": "Growth and duplication of public source code over time: Provenance tracking at scale\n", "abstract": " We study the evolution of the largest known corpus of publicly available source code, i.e., the Software Heritage archive (4B unique source code files, 1B commits capturing their development histories across 50M software projects). On such corpus we quantify the growth rate of original, never-seen-before source code files and commits. We find the growth rates to be exponential over a period of more than 40 years.We then estimate the multiplication factor, i.e., how much the same artifacts (e.g., files or commits) appear in different contexts (e.g., commits or source code distribution places). We observe a combinatorial explosion in the multiplication of identical source code files across different commits.We discuss the implication of these findings for the problem of tracking the provenance of source code artifacts (e.g., where and when a given source code file or commit has been observed in the wild) for the entire body of publicly available source code. To that end we benchmark different data models for capturing software provenance information at this scale and growth rate. We identify a viable solution that is deployable on commodity hardware and appears to be maintainable for the foreseeable future.", "num_citations": "3\n", "authors": ["918"]}
{"title": "Towards a Model Driven Approach to Upgrade Complex Software Systems.\n", "abstract": " Complex software systems are more and more based on the abstraction of package, brought to popularity by Free and Open Source Software (FOSS) distributions. While helpful as an encapsulation layer, packages do not solve all problems of deployment and management of large software collections. In particular upgrades, which often affect several packages at once due to inter-package dependencies, often fail and do not hold good transactional properties. This paper shows how to apply model driven techniques to describe and manage software upgrades of FOSS distributions. It is discussed how to model static and dynamic aspects of package upgrades-the latter being the most challenging aspect to deal with-in order to be able to predict common causes of upgrade failures and undo residual effects of failed or undesired upgrades.", "num_citations": "3\n", "authors": ["918"]}
{"title": "A generative approach to the implementation of language bindings for the document object model\n", "abstract": " The availability of a C implementation of the Document Object Model (DOM) offers the interesting opportunity of generating bindings for different programming languages automatically. Because of the DOM bias towards Java-like languages, a C implementation that fakes objects, inheritance, polymorphism, exceptions and uses reference-counting introduces a gap between the API specification and its actual implementation that the bindings should try to close. In this paper we overview the generative approach in this particular context and apply it for the generation of C++ and OCaml bindings.", "num_citations": "3\n", "authors": ["918"]}
{"title": "Brokers and Web-services for automatic deduction: a case study\n", "abstract": " We present a planning broker and several Web-Services for automatic deduction. Each Web-Service implements one of the tactics usually available in interactive proof-assistants. When the broker is submitted a\" proof status\"(an incomplete proof tree and a focus on an open goal) it dispatches the proof to the Web-Services, collects the successful results, and send them back to the client as\" hints\" as soon as they are available.", "num_citations": "3\n", "authors": ["918"]}
{"title": "The coproduction of open source software by volunteers and big tech firms\n", "abstract": " The DCPC is an international think tank established in 2021. It advocates for initiatives seeking to protect and expand the digital commons and use the digital commons to accelerate the transition to a more ecologically sustainable and fair society. It does so by publishing public reports on the digital commons and related topics, by co-producing how-to guides to assist in the creation of commonly held resources, and by making submissions and recommendations to government. More information at www. dcpc. info", "num_citations": "2\n", "authors": ["918"]}
{"title": "Reproducible Builds: Increasing the Integrity of Software Supply Chains\n", "abstract": " Although it is possible to increase confidence in Free and Open Source Software (FOSS) by reviewing its source code, trusting code is not the same as trusting its executable counterparts. These are typically built and distributed by third-party vendors, with severe security consequences if their supply chains are compromised. In this paper, we present reproducible builds, an approach that can determine whether generated binaries correspond with their original source code. We first define the problem, and then provide insight into the challenges of making real-world software build in a \u201creproducible\u201d manner\u2014this is, when every build generates bit-for-bit identical results. Through the experience of the Reproducible Builds project making the Debian Linux distribution reproducible, we also describe the affinity between reproducibility and quality assurance (QA).", "num_citations": "2\n", "authors": ["918"]}
{"title": "Firm discourses and digital infrastructure projects\n", "abstract": " Free and open source software (aka FOSS or \u2018digital infrastructure\u2019) is now fully integrated into commercial ecosystems. IT firms invest in FOSS in order (a) to share with other firms development costs;(b) to help attract prospective employees in a competitive job market where hiring skilled IT professionals is challenging and (c) to shape the governance and technical orientation of projects: firm employees participating in leading in FOSS projects may help IT firms create digital infrastructure more suited to the firmware they develop atop this infrastructure. How does the world of FOSS volunteers connect to the world of commercial ecosystems? Are firms developing policies in relation to open source communities, requesting projects conform to certain technical or behavioral standards, for example? To what extent are these strategies successful? To answer, we present a qualitative analysis of firm discourses collected during three open source conferences. We then analyze the email discussion lists of Linux and Firefox and search for the occurrence of key firm discourse terms in order to ascertain in what way these discourses are being used by FOSS developers. Our in-depth analysis of firm discourses and exploratory analysis of project discussions around these terms show that the FOSS world encompasses a diversity of industrial outlooks. They also highlight the evolution of the role of foundations: whilst foundations used to protect projects from firm interference, some have now wholly been placed in the service of firm efforts to standardize project work, particularly around the key issue of security.", "num_citations": "2\n", "authors": ["918"]}
{"title": "Streaming validation of schemata: the lazy typing discipline\n", "abstract": " Assertions, identity constraints, and conditional type assignments are (planned) features of XML Schema which rely on XPath evaluation to various ends. The allowed XPath subset exploitable in those features is trimmed down for streamability concerns partly understandable (the apparent wish to avoid buffering to determine the evaluation of an expression) and partly artificial.In this paper we dissect the XPath language in subsets with varying streamability characteristics. We also identify the larger subset which is compatible with the typing discipline we believe underlies some of the choices currently present in the XML Schema specifications. We describe such a discipline as imposing that the type of an element has to be decided when its start tag is encountered and its validity has to be when its end tag is.", "num_citations": "2\n", "authors": ["918"]}
{"title": "Templating wiki content for fun and profit\n", "abstract": " Content templating enables reuse of content structures between wiki pages. Such a feature is implemented in several mainstream wiki engine. Systematic study of its conceptual models and comparison of the available implementations are unfortunately missing in the wiki literature. In this paper we aim to fill this gap first analyzing template-related user needs, and then reviewing existing approaches at content templating.Our investigation shows that two models emerge\u2014functional and creational templating\u2014and that both have weakness failing to properly fit in \u201cThe Wiki Way\u201d. As a solution, we propose the adoption of creational templates enriched with light constraints, showing that such a solution has a low implementative footprint in state-of-the-art wiki engines, and that it has a synergy with semantic wikis.", "num_citations": "2\n", "authors": ["918"]}
{"title": "Content-Based Textual File Type Detection at Scale\n", "abstract": " Programming language detection is a common need in the analysis of large source code bases. It is supported by a number of existing tools that rely on several features, and most notably file extensions, to determine file types. We consider the problem of accurately detecting the type of files commonly found in software code bases, based solely on textual file content. Doing so is helpful to classify source code that lack file extensions (eg, code snippets posted on the Web or executable scripts), to avoid misclassifying source code that has been recorded with wrong or uncommon file extensions, and also shed some light on the intrinsic recognizability of source code files. We propose a simple model that (a) use a language-agnostic word tokenizer for textual files,(b) group tokens in 1-/2-grams,(c) build feature vectors based on N-gram frequencies, and (d) use a simple fully connected neural network as classifier. As\u00a0\u2026", "num_citations": "1\n", "authors": ["918"]}
{"title": "Forking without clicking: On how to identify software repository forks\n", "abstract": " The notion of software\" fork\" has been shifting over time from the (negative) phenomenon of community disagreements that result in the creation of separate development lines and ultimately software products, to the (positive) practice of using distributed version control system (VCS) repositories to collaboratively improve a single product without stepping on each others toes. In both cases the VCS repositories participating in a fork share parts of a common development history.", "num_citations": "1\n", "authors": ["918"]}
{"title": "Determining the intrinsic structure of public software development history\n", "abstract": " Background. Collaborative software development has produced a wealth of version control system (VCS) data that can now be analyzed in full. Little is known about the intrinsic structure of the entire corpus of publicly available VCS as an interconnected graph. Understanding its structure is needed to determine the best approach to analyze it in full and to avoid methodological pitfalls when doing so.Objective. We intend to determine the most salient network topology properties of public software development history as captured by VCS. We will explore: degree distributions, determining whether they are scale-free or not; distribution of connect component sizes; distribution of shortest path lengths.Method. We will use Software Heritage---which is the largest corpus of public VCS data---compress it using webgraph compression techniques, and analyze it in-memory using classic graph algorithms. Analyses will be\u00a0\u2026", "num_citations": "1\n", "authors": ["918"]}
{"title": "Towards Universal Software Evolution Analysis.\n", "abstract": " Software evolution studies have mostly focused on individual software products, generally developed as Free/Open Source Software (FOSS) projects, and more sparingly on software collections like component and package ecosystems. We argue in this paper that the next step in this organic scale expansion is universal software evolution analysis, ie, the study of software evolution at the scale of the whole body of publicly available software.We consider the case of Software Heritage, the largest existing archive of publicly available software source code artifacts (more than 5 B unique files archived and 1 B commits, coming from more than 80 M software projects). We propose research requirements that would allow to leverage the Software Heritage archive to study universal software evolution. We discuss the challenges that need to be overcome to address such requirements and outline a research roadmap to do so.", "num_citations": "1\n", "authors": ["918"]}
{"title": "Using preferences to tame your package manager\n", "abstract": " Determining whether some components can be installed on a system is a complex problem: not only it is NP-complete in the worst case, but there can also be exponentially many solutions to it. Ordinary package managers use ad-hoc heuristics to solve this installation problem and choose a particular solution, making extremely difficult to change or sidestep these heuristics when the result is not the one we expect. When software repositories become complex enough, one gets vastly superior results by delegating dependency handling to a specialised solver, and use optimisation functions (or preferences) to control the class of solutions that are found. The opam package manager relies on the CUDF pivot format, which allows OCaml users that have a CUDF-compliant solver on their machine to reap the benefits of preferences-based dependency resolution. Thanks to the solver farm provided by Irill, these benefits are now extended to the OCaml community at large. In this talk we will present the preferences language and explain how to use it.", "num_citations": "1\n", "authors": ["918"]}
{"title": "Formal aspects of free and open source software components\n", "abstract": " Free and Open Source Software (FOSS) distributions are popular solutions to deploy and maintain software on server, desktop, and mobile computing equipment. The typical deployment method in the FOSS setting relies on software distributions as vendors, packages as independently deployable components, and package managers as upgrade tools. We review research results from the past decade that apply formal methods to the study of inter-component relationships in the FOSS context. We discuss how those results are being used to attack both issues faced by users, such as dealing with upgrade failures on target machines, and issues important to distributions such as quality assurance processes for repositories containing tens of thousands, rapidly evolving software packages.", "num_citations": "1\n", "authors": ["918"]}
{"title": "Wiki semantics via Wiki templating\n", "abstract": " A foreseeable incarnation of Web 3.0 could inherit machine understandability from the Semantic Web and collaborative editing from Web 2.0 applications. We review the research and development trends, which are getting, today, Web nearer to such an incarnation. We present semantic wikis, microformats, and the so-called \u201clowercase semantic web\u201d; they are the main approaches at closing the technological gap between content authors and Semantic Web technologies. We discuss a too often neglected aspect of the associated technologies, namely how much they adhere to the wiki philosophy of open editing: is there an intrinsic incompatibility between semantic rich content and unconstrained editing? We argue that the answer to this question can be \u201cno,\u201d provided that a few, yet relevant, shortcomings of current Web technologies will be fixed soon.", "num_citations": "1\n", "authors": ["918"]}
{"title": "Enforcing OCaml link-time compatibility using Debian dependencies\n", "abstract": " To ensure type-safety, OCaml has strict link-time rules: involved objects are not allowed to change between (separate) compilation time and linking time. This does not cope well with the implicit assumption of future compatibility relied upon in general library packaging in F/OSS distributions such as Debian.We discuss how to enforce OCaml link-time type safety using Debian inter-package relationships (ie, dependencies). In doing so, we take into account Debian maintainability problems such as support for binNMUs (binary non-maintainer uploads) and preservation of human-readable dependency strings.", "num_citations": "1\n", "authors": ["918"]}
{"title": "Stream processing of XML documents made easy with LALR (1) parser generators\n", "abstract": " Because of their fully annotated structure, XML documents are normally believed to require a straightforward parsing phase. However, the standard APIs for accessing their content (the Document Object Model and the Simple API for XML) provide a programming interface that is very low-level and is thus inadequate for the recognition of any structure that is not isomorphic to its XML encoding. Even when the document undergoes validation, its unmarshalling into application-specific data using these APIs requires poorly maintainable, tedious-to-write, and possibly inefficient code. We describe a technique for the simultaneous parsing, validation, and unmarshalling of XML documents that combines a stream-oriented XML parser with a LALR (1) parser in order to guarantee efficient stream processing, expressive validation capabilities, and the possibility to associate user-provided actions with specific patterns occurring in the source documents.1. padovani@ sti. uniurb. it, Information Science and Technology Institute, University of Urbino 2. zacchiro@ cs. unibo. it, Department of Computer Science, University of Bologna", "num_citations": "1\n", "authors": ["918"]}
{"title": "Co-constraint validation in a streaming context\n", "abstract": " In many use cases applications are bound to be run consuming only a limited amount of memory. When they need to validate large XML documents, they have to adopt streaming validation, which does not rely on an in-memory representation of the whole input document. In order to validate an XML document, different kinds of constraints need to be verified. Co-constraints\u2014which relate the content of elements to the presence and values of other attributes or elements\u2014are one such kind of constraints. In this paper we propose an approach to the problem of validating in a streaming fashion an XML document against a schema also specifying coconstraints. We describe how the streaming evaluation of co-constraints influences the output of the validation process. Our proposal makes use of the validation language SchemaPath [11], a light extension to XML Schema [14], adding conditional type assignment for the support of coconstraints. The paper is based on the description of our streaming SchemaPath validator [10].", "num_citations": "1\n", "authors": ["918"]}