{"title": "Cyberinfrastructures: Bridging the Divide between Scientific Research and Software Engineering\n", "abstract": " Cyberinfrastructures--scientific research environments that span multiple institutions--require careful crafting and collaboration among domain specialists and software engineers, but several factors impede this collaboration. Recognizing five rules of thumb can help facilitate successful cyberinfrastructure creation and accelerate science in the disciplines they support.", "num_citations": "230\n", "authors": ["645"]}
{"title": "Measure it? manage it? ignore it? software practitioners and technical debt\n", "abstract": " The technical debt metaphor is widely used to encapsulate numerous software quality problems. The metaphor is attractive to practitioners as it communicates to both technical and nontechnical audiences that if quality problems are not addressed, things may get worse. However, it is unclear whether there are practices that move this metaphor beyond a mere communication mechanism. Existing studies of technical debt have largely focused on code metrics and small surveys of developers. In this paper, we report on our survey of 1,831 participants, primarily software engineers and architects working in long-lived, software-intensive projects from three large organizations, and follow-up interviews of seven software engineers. We analyzed our data using both nonparametric statistics and qualitative text analysis. We found that architectural decisions are the most important source of technical debt. Furthermore\u00a0\u2026", "num_citations": "215\n", "authors": ["645"]}
{"title": "Issues in co-operative software engineering using globally distributed teams\n", "abstract": " The use of geographically separated software development groups is proposed as a method for enabling 24-hour software development, or software shift work. The advantages of such an approach are explained, and potential organizational models for such virtual teams are described. Virtual team co-operation, information requirements and communication channels are explored. Activities in the software development life-cycle in which virtual teams can be advantageously utilized are explained, and examples of the successful use of virtual teams are cited. Finally a measure of the effectiveness of virtual teams known as the distributed working overhead is defined, which will enable project managers to clearly see the benefits and associated costs of employing virtual teams on a project.", "num_citations": "184\n", "authors": ["645"]}
{"title": "Data-intensive computing in the 21st century\n", "abstract": " The deluge of data that future applications must process\u2014in domains ranging from science to business informatics\u2014creates a compelling argument for substantially increased R&D targeted at discovering scalable hardware and software solutions for data-intensive problems.", "num_citations": "180\n", "authors": ["645"]}
{"title": "The changing paradigm of data-intensive computing\n", "abstract": " Through the development of new classes of software, algorithms, and hardware, data-intensive applications provide timely and meaningful analytical results in response to exponentially growing data complexity and associated analysis requirements.", "num_citations": "174\n", "authors": ["645"]}
{"title": "Tropical forest seed\n", "abstract": " The role and perception of forests in tropical areas has changed drastically during the last half century. Natural forests, as resources for forest products, are dwindling. Sustainable management of natural forests faces many diffic-ties in practice, although progress has been made. However, rural people in tropical countries often experience that the forests, which were previously the buffer for agriculture and an important resource, are becoming more and more inaccessible. Remaining forests are to a large extent protected, degraded or so far away from settlement that in practice they are beyond reach. The majority of the world\u2019s forest products in the future will come from m-made plantations and cultivated trees. The term \u2018plantation\u2019, usually referring to traditional block plantings of industrial species, is acquiring a wider me-ing which includes, for example, smaller woodlots, shelterbelts and various types of agroforestry. Forest seeds are in this context of utmost importance. Not only are seeds the most commonly used propagation material, they are also the carriers of the genetic material from one generation to the next. Forest seed handling is thus an integrated part of selection, management, devel-ment and conservation of forest genetic resources in a larger context. With this in mind, and considering how self-evident the matter of seed quality is in agriculture, one can wonder how little attention has been and is given to f-est germplasm in many afforestation and plantation programmes.", "num_citations": "120\n", "authors": ["645"]}
{"title": "Design-level performance prediction of component-based applications\n", "abstract": " Server-side component technologies such as Enterprise JavaBeans (EJBs), .NET, and CORBA are commonly used in enterprise applications that have requirements for high performance and scalability. When designing such applications, architects must select suitable component technology platform and application architecture to provide the required performance. This is challenging as no methods or tools exist to predict application performance without building a significant prototype version for subsequent benchmarking. In this paper, we present an approach to predict the performance of component-based server-side applications during the design phase of software development. The approach constructs a quantitative performance model for a proposed application. The model requires inputs from an application-independent performance profile of the underlying component technology platform, and a design\u00a0\u2026", "num_citations": "119\n", "authors": ["645"]}
{"title": "Performance prediction of component-based applications\n", "abstract": " One of the major problems in building large-scale enterprise systems is anticipating the performance of the eventual solution before it has been built. The fundamental software engineering problem becomes more difficult when the systems are built on component technology. This paper investigates the feasibility of providing a practical solution to this problem. An empirical approach is proposed to determine the performance characteristics of component-based applications by benchmarking and profiling. Based on observation, a model is constructed to act as a performance predictor for a class of applications based on the specific component technology. The performance model derived from empirical measures is necessary to make the problem tractable and the results relevant. A case study applies the performance model to an application prototype implemented by two component infrastructures: CORBA and J2EE.", "num_citations": "118\n", "authors": ["645"]}
{"title": "Performance evaluation of nosql databases: A case study\n", "abstract": " The choice of a particular NoSQL database imposes a specific distributed software architecture and data model, and is a major determinant of the overall system throughput. NoSQL database performance is in turn strongly influenced by how well the data model and query capabilities fit the application use cases, and so system-specific testing and characterization is required. This paper presents a method and the results of a study that selected among three NoSQL databases for a large, distributed healthcare organization. While the method and study considered consistency, availability, and partition tolerance (CAP) tradeoffs, and other quality attributes that influence the selection decision, this paper reports on the performance evaluation method and results. In our testing, a typical workload and configuration produced throughput that varied from 225 to 3200 operations per second between database products, while\u00a0\u2026", "num_citations": "114\n", "authors": ["645"]}
{"title": "Distribution, data, deployment: Software architecture convergence in big data systems\n", "abstract": " Big data applications are pushing the limits of software engineering on multiple horizons. Successful solutions span the design of the data, distribution, and deployment architectures. The body of software architecture knowledge must evolve to capture this advanced design knowledge for big data systems. This article is a first step on this path. Our research is proceeding in two complementary directions. First, we're expanding our collection of architecture tactics and encoding them in an environment that supports navigation between quality attributes and tactics, making crosscutting concerns for design choices explicit. Second, we're linking tactics to design solutions based on specific big data technologies, enabling architects to rapidly relate a particular technology's capabilities to a specific set of tactics.", "num_citations": "87\n", "authors": ["645"]}
{"title": "CAmkES: A component model for secure microkernel-based embedded systems\n", "abstract": " Component-based software engineering promises to provide structure and reusability to embedded-systems software. At the same time, microkernel-based operating systems are being used to increase the reliability and trustworthiness of embedded systems. Since the microkernel approach to designing systems is partially based on the componentisation of system services, component-based software engineering is a particularly attractive approach to developing microkernel-based systems. While a number of widely used component architectures already exist, they are generally targeted at enterprise computing rather than embedded systems. Due to the unique characteristics of embedded systems, a component architecture for embedded systems must have low overhead, be able to address relevant non-functional issues, and be flexible to accommodate application specific requirements. In this paper we\u00a0\u2026", "num_citations": "81\n", "authors": ["645"]}
{"title": "Behavior and performance of message-oriented middleware systems\n", "abstract": " The middleware technology used as the foundation of Internet-enabled enterprise systems is becoming increasingly complex. In addition, the various technologies offer a number of standard architectures that can be used by designers as templates to build applications. However, there is little concrete understanding in the software industry on the strengths and weaknesses of competing technologies, and the different trade-offs that various component architectures impose. The SACT Group at CSIRO has qualitatively and quantitatively evaluated a number of commercially available Message-Oriented Middleware (MOM) systems. This paper focuses on the results obtained from the performance evaluation of the IBM's MQSeries V5.2. It presents an overview of the technology, and discusses the metric used in this study for performance measurement The test results related to the sustainable performance of the system\u00a0\u2026", "num_citations": "79\n", "authors": ["645"]}
{"title": "Rigorous evaluation of COTS middleware technology\n", "abstract": " The adoption of commercial off-the-shelf middleware products across the software industry has gathered significant momentum. While COTS middleware products demonstrably solve many problems, their adoption and use are by no means straightforward. Competition among products that apparently offer identical services complicates the COTS middleware selection process, especially when competing products provide different implementations of standards-based technologies, such as CORBA and Java 2 Enterprise Edition. The article discusses the Middleware Technology Evaluation project, which represents a significant attempt to provide rigorously derived, in-depth evaluations of technology for use by middleware product adopters.", "num_citations": "76\n", "authors": ["645"]}
{"title": "A high-performance hybrid computing approach to massive contingency analysis in the power grid\n", "abstract": " Operating the electrical power grid to prevent power black-outs is a complex task. An important aspect of this is contingency analysis, which involves understanding and mitigating potential failures in power grid elements such as transmission lines. When taking into account the potential for multiple simultaneous failures (known as the N-x contingency problem), contingency analysis becomes a massively computational task. In this paper we describe a novel hybrid computational approach to contingency analysis. This approach exploits the unique graph processing performance of the Cray XMT in conjunction with a conventional massively parallel compute cluster to identify likely simultaneous failures that could cause widespread cascading power failures that have massive economic and social impact on society. The approach has the potential to provide the first practical and scalable solution to the N-x\u00a0\u2026", "num_citations": "60\n", "authors": ["645"]}
{"title": "Performance evaluation for parallel systems: A survey\n", "abstract": " Performance is often a key factor in determining the success of a parallel software system. Performance evaluation techniques can be classified into three categories: measurement, analytical modeling, and simulation. Each of them has several types. For example, measurement has software, hardware, and hybrid; simulation has discrete event, trace/execution driven, Monte Carlo; and analytical modeling has queueing network, Petri net, etc.. This paper systematically reviews various techniques, and surveys work done in each category. Also addressed and discussed are other issues related to performance evaluation. These issues include how to select metrics and proper techniques that are well suited for the particular development stage, how to construct a good model, and how to perform workload characterization. We also present fundamental laws and scalability analysis techniques. While many techniques discussed are common in both sequential and parallel system performance evaluation, our focus is on the parallel systems.", "num_citations": "60\n", "authors": ["645"]}
{"title": "Accelerating COTS middleware acquisition: the i-Mate process\n", "abstract": " COTS middleware speeds e-business application deployment but can be difficult to select. The i-Mate tool provides a proven, structured software engineering process for COTS middleware acquisition. Using i-Mate in six major projects for a wide range of organizations led to highly visible, accountable, and ultimately reliable selections of COTS middleware products in greatly compressed time scales. This significantly reduced the risks associated with inappropriate product selections and made these projects more likely to succeed.", "num_citations": "53\n", "authors": ["645"]}
{"title": "Performance prediction of J2EE applications using messaging protocols\n", "abstract": " Predicting the performance of component-based applications is difficult due to the complexity of the underlying component technology. This problem is exacerbated when a messaging protocol is introduced to create a loosely coupled software architecture. Messaging uses asynchronous communication, and must address quality of service issues such as message persistence and flow control. In this paper, we present an approach to predicting the performance of Java 2 Enterprise Edition (J2EE) applications using messaging services. The prediction is done during application design, without access to the application implementation. This is achieved by modeling the interactions among J2EE and messaging components using queuing network models, calibrating the performance model with architecture attributes associated with these components, and populating the model parameters using a lightweight\u00a0\u2026", "num_citations": "51\n", "authors": ["645"]}
{"title": "Designing a test suite for empirically-based middleware performance prediction\n", "abstract": " One of the major problems in building large-scale enterprise systems is anticipating the performance of the eventual solution before it has been built. This problem is especially germane to modern Internet-based e-business applications, where failure to provide high performance and scalability can lead to application and business failure. The fundamental software engineering problem is compounded by many factors, including application diversity, architectural trade-offs and options, COTS component integration requirements, and differences in performance of various software and hardware infrastructures. In the ForeSight project, a practical solution to this problem, based on empirical testing is being investigated. The approach constructs useful models that act as predictors of the performance and the effects of architectural trade-offs for component-based systems such as CORBA, COM+ and J2EE. This paper focuses on describing the issues involved in designing and executing a test suite that is efficient to characterize the behavior and performance profile of a J2EE application server product. The aims of the test suite are described, along with its design and some illustrative empirical results to show it's effectiveness...", "num_citations": "50\n", "authors": ["645"]}
{"title": "Evaluating the performance of EJB components\n", "abstract": " As part of the Middleware Technology Evaluation (MTE) project, we conducted several experiments to explore the performance implications of two common application architectures supported by J2EE's enterprise JavaBean (EJB) component technology. One architecture promises simpler engineering and maintenance of the resulting component collection. For applications that require high performance and scalability, however, the alternative architecture might offer a better solution. Such knowledge is crucial to software architects, who must make initial design decisions early in a project, before extensive engineering has begun. An examination of two EJB-based architectures reveals differences that can significantly affect the performance and scalability of applications built on them.", "num_citations": "48\n", "authors": ["645"]}
{"title": "Software component quality assessment in practice: successes and practical impediments\n", "abstract": " This paper describes the authors' experiences of initiating and sustaining a project at CSIRO aimed at accelerating the successful adoption of COTS middleware technologies in large business and scientific information systems. The projects aims are described, along with example outcomes and an assessment of what is needed for wide-scale software component quality assessments to succeed.", "num_citations": "48\n", "authors": ["645"]}
{"title": "Architectures and technologies for enterprise application integration\n", "abstract": " Architects are faced with the problem of building enterprise scale information systems, with streamlined, automated internal business processes and Web-enabled business functions, all across multiple legacy applications. The underlying architectures for such systems are embodied in a range of diverse products known as enterprise application integration (EAI) technologies. In this paper, we highlight some of the major problems, approaches and issues in designing EAI architectures and selecting appropriate supporting technology. The tutorial presents a range of the common architectural patterns frequently used for EAI applications. It also explains service-oriented architectures as the current best practice architectural framework for EAI. It then describes the state-or-the-art in EAI technologies that support these architectural styles, and discusses some of the key design trade-offs involved when selecting an\u00a0\u2026", "num_citations": "47\n", "authors": ["645"]}
{"title": "Performance prediction of COTS component-based enterprise applications\n", "abstract": " One of the major problems in building large-scale enterprise systems is anticipating the performance of the eventual solution before it has been built. This problem is especially germane to modern Internet-based e-business applications, where failure to provide high performance and scalability can lead to application and business failure. The fundamental software engineering problem is compounded by many factors, including application diversity, architectural trade-offs and options, COTS component integration requirements, and differences in performance of various software and hardware infrastructures. This paper investigates the feasibility of providing a novel and practical solution to this problem. The approach as demonstrated, constructs useful models that act as predictors of the performance for component-based systems hosted by middleware infrastructures such as CORBA, COM+ and J2EE.", "num_citations": "47\n", "authors": ["645"]}
{"title": "Large-scale data challenges in future power grids\n", "abstract": " This paper describes technical challenges in supporting large-scale real-time data analysis for future power grid systems and discusses various design options to address these challenges. Even though the existing U.S. power grid has served the nation remarkably well over the last 120 years, big changes are in the horizon. The widespread deployment of renewable generation, smart grid controls, energy storage, plug-in hybrids, and new conducting materials will require fundamental changes in the operational concepts and principal components. The whole system becomes highly dynamic and needs constant adjustments based on real time data. Even though millions of sensors such as phase measurement units (PMUs) and smart meters are being widely deployed, a data layer that can support this amount of data in real time is needed. Unlike the data fabric in cloud services, the data layer for smart grids must\u00a0\u2026", "num_citations": "42\n", "authors": ["645"]}
{"title": "Enabling software shift work with groupware: a case study\n", "abstract": " Describes a software development trial to evaluate the use of a groupware support environment for widely geographically separated software development teams. A four-person dislocated team, working in (simulated) disjoint time zones, was assigned a development task to carry out over a two-week period. Due to the time and location displacement, the developers were denied almost all opportunities for synchronous communications and had to rely on support from a prototype software engineering support system developed in Lotus Notes for interactions and coordination. In addition, the tasks in the trial were allocated such that productivity gains could be experienced through positive exploitation of time-zone differences, effectively giving around-the-clock working. This paper describes the design and organisation of the trial, reports on the progress of the trial, and presents both quantitative and qualitative results\u00a0\u2026", "num_citations": "42\n", "authors": ["645"]}
{"title": "Toward agile architecture: Insights from 15 years of ATAM data\n", "abstract": " Agile teams strive to balance short-term feature development with longer-term quality concerns. These evolutionary approaches often hit a \"complexity wall\"' from the cumulative effects of unplanned changes, resulting in unreliable, poorly performing software. So, the agile community is refocusing on approaches to address architectural concerns. Researchers analyzed quality attribute concerns from 15 years of Architecture Trade-Off Analysis Method data, gathered from 31 projects. Modifiability was the dominant concern across all project types; performance, availability, and interoperability also received considerable attention. For IT projects, a relatively new quality-deployability-emerged as a key concern. The study results provide insights for agile teams allocating architecture-related tasks to iterations. For example, teams can use these results to create checklists for release planning or retrospectives to help\u00a0\u2026", "num_citations": "37\n", "authors": ["645"]}
{"title": "Predicting the performance of middleware-based applications at the design level\n", "abstract": " In this paper, we present an approach to predict the performance of middleware-based applications at the design level. We develop a quantitative performance model for a proposed system design. The inputs needed to produce this performance prediction are a state diagram showing the main waiting and resource usage aspects of the proposed system architecture, and measurements taken on the middleware infrastructure using a simple benchmark application which is much cheaper to implement than the full system. The performance model allows the system designer to evaluate the architecture and implementation approaches in term of their ability to achieve required performance. We show our method in action using a J2EE application, Stock-Online, and validate the predictions by implementing the design and measuring its performance. The modeling approach is applicable to applications built on common\u00a0\u2026", "num_citations": "37\n", "authors": ["645"]}
{"title": "Object-based modeling of parallel programs\n", "abstract": " The Parse Project has been investigating software development issues covering a range of parallel applications. Parse itself is an object-based design methodology that incorporates design management strategies based on data and function encapsulation, hierarchical decomposition, and staged refinement. Parse represents parallel software designs with a graphical notation called process graphs. After capturing a design's important structural features, this notation systematically derives the design's skeletal dynamic properties. For this, it can use either a behavioral specification language or formal methods such as CSP (Communicating Sequential Processes) or Petri nets. Parse builds on existing parallel software design techniques that are based on dataflow and object-oriented approaches. Parse attempts to capture precise object interactions and synchronization in an abstract, architecture-independent\u00a0\u2026", "num_citations": "37\n", "authors": ["645"]}
{"title": "Architecture knowledge for evaluating scalable databases\n", "abstract": " Designing massively scalable, highly available big data systems is an immense challenge for software architects. Big data applications require distributed systems design principles to create scalable solutions, and the selection and adoption of open source and commercial technologies that can provide the required quality attributes. In big data systems, the data management layer presents unique engineering problems, arising from the proliferation of new data models and distributed technologies for building scalable, available data stores. Architects must consequently compare candidate database technology features and select platforms that can satisfy application quality and cost requirements. In practice, the inevitable absence of up-to-date, reliable technology evaluation sources makes this comparison exercise a highly exploratory, unstructured task. To address these problems, we have created a detailed\u00a0\u2026", "num_citations": "34\n", "authors": ["645"]}
{"title": "Velo: A knowledge-management framework for modeling and simulation\n", "abstract": " Velo is a reusable, domain-independent knowledge-management infrastructure for modeling and simulation. Velo leverages, integrates, and extends Web-based open source collaborative and data-management technologies to create a scalable and flexible core platform tailored to specific scientific domains. As the examples here describe, Velo has been used in both the carbon sequestration and climate modeling domains.", "num_citations": "33\n", "authors": ["645"]}
{"title": "Next generation application integration: challenges and new approaches\n", "abstract": " Integrating multiple heterogeneous data sources into applications is a time-consuming, costly and error-prone engineering task. Relatively mature technologies exist that make integration tractable from an engineering perspective. These technologies, however, have many limitations, and hence present opportunities for breakthrough research. This paper briefly describes some of these limitations, and enumerates a subset of the general open research problems. It then describes the Data Concierge research project and prototype that is attempting to provide solutions to some of these problems.", "num_citations": "33\n", "authors": ["645"]}
{"title": "Scalable real time data management for smart grid\n", "abstract": " This paper presents GridMW, a scalable and reliable data middleware layer for smart grids. Smart grids promise to improve the efficiency of power grid systems and reduce green house emissions through incorporating power generation from renewable sources and shaping demands to match the supply. As a result, power grid will become much more dynamic and require constant adjustments, which requires analysis and decision making applications to improve the efficiency and reliability of smart grid systems. However, these applications rely on large amounts of data gathered from power generation, transmission, and consumption. To this end, millions of sensors, including phasor measurement units (PMU) and smart meters, are being deployed across the smart grid system. Existing data middleware does not have the capability to collect, store, retrieve, and deliver the enormous amount of data from these\u00a0\u2026", "num_citations": "32\n", "authors": ["645"]}
{"title": "Model driven benchmark generation for web services\n", "abstract": " Web services solutions are being increasingly adopted in enterprise systems. However, ensuring the quality of service of Web services applications remains a costly and complicated performance engineering task. Some of the new challenges include limited controls over consumers of a service, unforeseeable operational scenarios and vastly different XML payloads. These challenges make existing manual performance analysis and benchmarking methods difficult to use effectively. This paper describes an approach for generating customized benchmark suites for Web services applications from a software architecture description following a Model Driven Architecture (MDA) approach. We have provided a performance-tailored version of the UML 2.0 Testing Profile so architects can model a flexible and reusable load testing architecture, including test data, in a standards compatible way. We extended our\u00a0\u2026", "num_citations": "32\n", "authors": ["645"]}
{"title": "Software engineering for big data systems\n", "abstract": " Software engineering for big data systems is complex and faces challenges including pervasive distribution, write-heavy workloads, variable request loads, computation-intensive analytics, and high availability. The articles in this theme issue examine several facets of this complicated puzzle. The Web extra at https://youtu.be/YKBGf9EOBUo is an audio recording of Davide Falessi speaking with Ayse Basar Bener and Audris Mockus about the authors, articles, and discussions that went into the IEEE Software March/April 2016 theme issue on software engineering for big data systems.", "num_citations": "31\n", "authors": ["645"]}
{"title": "The medici integration framework: A platform for high performance data streaming applications\n", "abstract": " Building high performance analytical applications for data streams generated from sensors is a challenging software engineering problem. Such applications typically comprise a complex pipeline of processing components that capture, transform and analyze the incoming data stream. In addition, applications must provide high throughput, be scalable and easily modifiable so that new analytical components can be added with minimum effort. In this paper we describe the MeDICi integration framework (MIF), which is a middleware platform we have created to address these challenges. The MIF extends an open source messaging platform with a component-based API for integrating components into analytical pipelines. We describe the features and capabilities of the MIF, and show how it has been used to build a production analytical application for detecting cyber security attacks. The application was composed\u00a0\u2026", "num_citations": "30\n", "authors": ["645"]}
{"title": "An extensible and lightweight architecture for adaptive server applications\n", "abstract": " Server applications augmented with behavioral adaptation logic can react to environmental changes, creating self\u2010managing server applications with improved quality of service at runtime. However, developing adaptive server applications is challenging due to the complexity of the underlying server technologies and highly dynamic application environments. This paper presents an architecture framework, the Adaptive Server Framework (ASF), to facilitate the development of adaptive behavior for legacy server applications. ASF provides a clear separation between the implementation of adaptive behavior and the business logic of the server application. This means a server application can be extended with programmable adaptive features through the definition and implementation of control components defined in ASF. Furthermore, ASF is a lightweight architecture in that it incurs low CPU overhead and memory\u00a0\u2026", "num_citations": "29\n", "authors": ["645"]}
{"title": "Software engineering for parallel systems\n", "abstract": " Current approaches to software engineering practice for parallel systems are reviewed. The parallel software designer has not only to address the issues involved in the characterization of the application domain and the underlying hardware platform, but, in many instances, the production of portable, scalable software is desirable. In order to accommodate these requirements, a number of specific techniques and tools have been proposed, and these are discussed in this review in the framework of the parallel software life-cycle. The paper outlines the role of formal methods in the practical production of parallel software, but its main focus is the emergence of development methodologies and environments. These include CASE tools and run-time support systems, as well as the use of methods taken from experience of conventional software development. Because of the particular emphasis on performance of\u00a0\u2026", "num_citations": "27\n", "authors": ["645"]}
{"title": "Enterprise Transaction Processing Systems: Putting the Cobra Ots, Encina++ and Orbixotm to Work\n", "abstract": " From the Book: Preface The recent merging of distributed object technology and transaction processing monitors has created a new class of technology known as Object Transaction Monitors (OTMs). OTMs typically contain a comprehensive set of features that make it possible to build enterprise-scale, high performance transaction processing systems. As more systems based on OTMs are built, there's a need for software professionals new to the area to acquire an understanding of the concepts and features of the available OTM technologies. They also need to appreciate the important issues that drive a project's architecture, detailed design and programming. This book attempts to fill this niche, to educate people in the complexities of OTM technologies. The approach taken is necessarily a practical one. It attempts to distill several years of the author's consulting experience gained working with OTM technologies\u00a0\u2026", "num_citations": "26\n", "authors": ["645"]}
{"title": "A high-performance workflow system for subsurface simulation\n", "abstract": " The U.S. Department of Energy (DOE) recently invested in developing a numerical modeling toolset called ASCEM (Advanced Simulation Capability for Environmental Management) to support modeling analyses at legacy waste sites. This investment includes the development of an open-source user environment called Akuna that manages subsurface simulation workflows. Core toolsets accessible through the Akuna user interface include model setup, grid generation, sensitivity analysis, model calibration, and uncertainty quantification. Additional toolsets are used to manage simulation data and visualize results. This new workflow technology is demonstrated by streamlining model setup, calibration, and uncertainty analysis using high performance computation for the BC Cribs Site, a legacy waste area at the Hanford Site in Washington State. For technetium-99 transport, the uncertainty assessment for potential\u00a0\u2026", "num_citations": "25\n", "authors": ["645"]}
{"title": "Architecting in the face of uncertainty: an experience report\n", "abstract": " Understanding an application's functional and non-functional requirements is normally seen as essential for developing a robust product suited to client needs. This paper describes our experiences in a project that, by necessity, commenced well before concrete client requirements could be known. After a first version of the application was successfully released, emerging requirements forced an evolution of the application architecture. The key reasons for this are explained, along with the architectural strategies and software engineering practices that were adopted. The resulting application architecture is highly flexible, modifiable and scalable, and therefore should provide a solid foundation for the duration of the application's lifetime.", "num_citations": "25\n", "authors": ["645"]}
{"title": "Exploring architecture options for a federated, cloud-based system biology knowledgebase\n", "abstract": " Systems biology is characterized by a large community of scientists who use a wide variety of fragmented and competing data sets and computational tools of all scales to support their research. In order to provide a more coherent computational environment for systems biology, we are working as part of the Department of Energy Systems Biology Knowledgebase (Kbase) project to define a federated cloud-based system architecture. The Kbase will eventually host massive amounts of biological data, provide high performance and scalable computational resources, and support a large user community with tools and services to enable them to utilize the Kbase resources. In this paper, we describe the results of our investigations into the design of a workflow infrastructure suitable for use in the Kbase. The approach utilizes standards-based workflow description and open source integration technologies, and\u00a0\u2026", "num_citations": "24\n", "authors": ["645"]}
{"title": "Runtime performance challenges in big data systems\n", "abstract": " Big data systems are becoming pervasive. They are distributed systems that include redundant processing nodes, replicated storage, and frequently execute on a shared'cloud'infrastructure. For these systems, design-time predictions are insufficient to assure runtime performance in production. This is due to the scale of the deployed system, the continually evolving workloads, and the unpredictable quality of service of the shared infrastructure. Consequently, a solution for addressing performance requirements needs sophisticated runtime observability and measurement. Observability gives real-time insights into a system's health and status, both at the system and application level, and provides historical data repositories for forensic analysis, capacity planning, and predictive analytics. Due to the scale and heterogeneity of big data systems, significant challenges exist in the design, customization and operations of\u00a0\u2026", "num_citations": "23\n", "authors": ["645"]}
{"title": "Implementing adaptive performance management in server applications\n", "abstract": " Performance and scalability are critical quality attributes for server applications in Internet-facing business systems. These applications operate in dynamic environments with rapidly fluctuating user loads and resource levels, and unpredictable system faults- Adaptive (autonomic) systems research aims to augment such server applications with intelligent control logic that can detect and react to sudden environmental changes. However, developing this adaptive logic is complex in itself. In addition, executing the adaptive logic consumes processing resources, and hence may (paradoxically) adversely effect application performance. In this paper we describe an approach for developing high-performance adaptive server applications and the supporting technology. The Adaptive Server Framework (ASF) is built on standard middleware services, and can be used to augment legacy systems with adaptive behavior\u00a0\u2026", "num_citations": "23\n", "authors": ["645"]}
{"title": "Evaluating agent architectures: cougaar, aglets and AAA\n", "abstract": " Research and development organizations are constantly evaluating new technologies in order to implement the next generation of advanced applications. At Pacific Northwest National Laboratory, agent technologies are perceived as an approach that can provide a competitive advantage in the construction of highly sophisticated software systems in a range of application areas. To determine the sophistication, utility, performance, and other critical aspects of such systems, a project was instigated to evaluate three candidate agent toolkits. This paper reports on the outcomes of this evaluation, the knowledge accumulated from carrying out this project, and provides insights into the capabilities of the agent technologies evaluated.", "num_citations": "23\n", "authors": ["645"]}
{"title": "Software quality attributes\n", "abstract": " Much of a software architect\u2019s life is spent designing software systems to meet a set of quality attribute requirements. General software quality attributes include scalability, security, performance and reliability. These are often informally called an application\u2019s \u201c-ilities\u201d (though of course some, like performance, don\u2019t quite fit this lexical specification).", "num_citations": "22\n", "authors": ["645"]}
{"title": "Agent-based software development methodologies\n", "abstract": " In this White Paper, produced as a result of discussions at the OOPSLA 2002 Workshop on Agent-Oriented Methodologies, we outline the current state of play of agent-oriented methodologies, how they might be integrated into an underlying, metamodel-based framework, and what the research community needs to do to make their products acceptable to industry. We conclude with an invitation to the community.", "num_citations": "22\n", "authors": ["645"]}
{"title": "Components in the Pipeline\n", "abstract": " State-of-the-art scientific instruments and simulations routinely produce massive datasets requiring intensive processing to disclose key features of the artifact or model under study. Scientists commonly call these data-processing pipelines, which are structured according to the pipe and-filter architecture pattern. 1  Different stages typically communicate using files; each stage is an executable program that performs the processing needed at that point in the pipeline.The MeDICi (Middleware for Data-Intensive Computing) Integration Framework supports constructing complex software pipelines from distributed heterogeneous components and controlling qualities of service to meet performance, reliability and communication requirements.", "num_citations": "21\n", "authors": ["645"]}
{"title": "Evaluating the scalability of enterprise javabeans technology\n", "abstract": " One of the major problems in building large-scale distributed systems is to anticipate the performance of the eventual solution before it has been built. This problem is especially germane to Internet-based e-business applications, where failure to provide high performance and scalability can lead to application and business failure. The fundamental software engineering problem is compounded by many factors, including individual application diversity, software architecture trade-offs, COTS component integration requirements, and differences in performance of various software and hardware infrastructures. We describe the results of an empirical investigation into the scalability of a widely used distributed component technology, Enterprise JavaBeans (EJB). A benchmark application is developed and tested to measure the performance of a system as both the client load and component infrastructure are scaled up\u00a0\u2026", "num_citations": "21\n", "authors": ["645"]}
{"title": "Parallel program design using high\u2010level Petri nets\n", "abstract": " Petri nets are proposed as a general\u2010purpose design and modelling tool for parallel programs. The advantages of Petri nets for this purpose are discussed, and a solution to the Dining Philosophers problem is developed using simple Place\u2010Transition nets. The limitations of Place\u2010Transition nets are described, and the Dining Philosophers problem is used to illustrate how Coloured Petri nets can overcome these limitations. A more complex example of a Coloured Petri net is then given, and it is shown how a collection of processes in the Occam programming language can be developed directly from the properties of the net. Another Petri net model of a simple process farm is given, and a solution is developed in Parallel C: this further highlights the suitability of Petri nets as a design tool for parallel programs.", "num_citations": "21\n", "authors": ["645"]}
{"title": "Distribution+ persistence= global virtual memory\n", "abstract": " The Distributed Systems Group at the University of New South Wales is constructing a distributed operating system based on global virtual memory (GVM). The system combines local and remote storage into a single large virtual address space. This provides a uniform method for naming and accessing objects regardless of their location, removes the distinction between persistent and transient data, and simplifies the migration of data and processes. The GVM system uses conventional computing nodes connected to specialised network interfaces. A fault-tolerant migration and replication protocol keeps the system operational and consistent in case of network errors or node crashes. Password capabilities are used to control access to the GVM.< >", "num_citations": "21\n", "authors": ["645"]}
{"title": "An extensible, lightweight architecture for adaptive J2EE applications\n", "abstract": " Server applications with adaptive behaviors can adapt their functionality in response to environmental changes, and significantly reduce the on-going costs of system deployment and administration. However, developing adaptive server applications is challenging due to the complexity of server technologies and highly dynamic application environments. This paper presents an architecture framework, known as the Adaptive Server Framework (ASF). ASF provides a clear separation between the implementation of adaptive behaviors and the server application business logic. This means a server application can be cost effectively extended with programmable adaptive features through the definition and implementation of control components defined in ASF. Furthermore, ASF is a lightweight architecture in that it incurs low CPU overhead and memory usage. We demonstrate the effectiveness of ASF through a case\u00a0\u2026", "num_citations": "20\n", "authors": ["645"]}
{"title": "An efficient, scalable content-based messaging system\n", "abstract": " Large-scale information processing environments must rapidly search through massive streams of raw data to locate useful information. These data streams contain textual and numeric data items, and may be highly structured or mostly freeform text. This project aims to create a high performance and scalable engine for locating relevant content in data streams. Based on the J2EE Java Messaging Service (JMS), the content-based messaging (CBM) engine provides highly efficient message formatting and filtering. This paper describes the design of the CBM engine, and presents empirical results that compare the performance with a standard JMS to demonstrate the performance improvements that are achieved.", "num_citations": "20\n", "authors": ["645"]}
{"title": "Data-intensive computing: architectures, algorithms, and applications\n", "abstract": " The world is awash with digital data from social networks, blogs, business, science and engineering. Data-intensive computing facilitates understanding of complex problems that must process massive amounts of data. Through the development of new classes of software, algorithms and hardware, data-intensive applications can provide timely and meaningful analytical results in response to exponentially growing data complexity and associated analysis requirements. This emerging area brings many challenges that are different from traditional high-performance computing. This reference for computing professionals and researchers describes the dimensions of the field, the key challenges, the state of the art and the characteristics of likely approaches that future data-intensive problems will require. Chapters cover general principles and methods for designing such systems and for managing and analyzing the big data sets of today that live in the cloud and describe example applications in bioinformatics and cybersecurity that illustrate these principles in practice.", "num_citations": "19\n", "authors": ["645"]}
{"title": "The rigorous evaluation of Enterprise Java Bean technology\n", "abstract": " The middleware technology used as the foundation of Internet-enabled enterprise information systems is becoming increasingly complex. In addition, the various technologies offer a number of standard component architectures that can be used by designers as templates to build applications. However, there is little understanding in the software industry on the strengths and weaknesses of competing technologies, and the different trade-offs that various component architectures impose. The paper describes the approach being taken in CSIRO's Middleware Technology Evaluation (MTE) project to attempt to alleviate some of these problems. Specifically, the results from some experiments in using different Enterprise Java Bean application architectures are presented, which clearly show how the different architectures scale and perform under various client loads.", "num_citations": "19\n", "authors": ["645"]}
{"title": "A performance prototyping approach to designing concurrent software architectures\n", "abstract": " A prototyping approach to evaluating the performance of parallel and distributed software architectures is proposed. The approach utilises conventional object-oriented modelling to describe the system requirements. Alternative architectural approaches are then derived and expressed in the PARSE process graph notation, and performance prototyping and formal validation tools allow various aspects of the proposed solution to be rapidly explored. This paper describes work-in-progress on the HL language and prototyping environment which enables designers to express architectural structures in a simple language, and provides an execution environment which allows the potential performance of the architecture to be investigated. The paper gives an overview of the language and tools, and illustrates its application with an example.", "num_citations": "19\n", "authors": ["645"]}
{"title": "An extensible, scalable architecture for managing bioinformatics data and analyses\n", "abstract": " Systems biology research demands the availability of tools and technologies that span a comprehensive range of computational capabilities, including data management, transfer, processing, integration, and interpretation. To address these needs, we have created the bioinformatics resource manager (BRM), a scalable, flexible, and easy to use tool for biologists to undertake complex analyses. This paper describes the underlying software architecture of the BRM that integrates multiple commodity platforms to provide a highly extensible and scalable software infrastructure for bioinformatics. The architecture integrates a J2EE 3-tier application with an archival experimental data management system, the GAGGLE framework for desktop tool integration, and the MeDICi integration framework for high-throughput data analysis workflows. This architecture facilitates a systems biology software solution that enables the\u00a0\u2026", "num_citations": "18\n", "authors": ["645"]}
{"title": "Distributing the software process\n", "abstract": " This paper sees software development as made up of two important parts. One is the technical work in producing software. The other is the team management and coordination in large and complex systems. Support tools for software development have in general concentrated on providing technical support to improve task productivity. These have included CASE tools as well as various editors, debugging tools and so on. Trends to geographically distributed software development call for providing team support to coordinate activities across distances. This paper describes methods for providing such support using groupware. It outlines different support strategies for different team structures and describes an experimental system for supporting mission oriented teams and some results of using this experimental system.", "num_citations": "18\n", "authors": ["645"]}
{"title": "Exploring performance models of hadoop applications on cloud architecture\n", "abstract": " Hadoop is an open source implementation of the MapReduce programming model, and provides the runtime infrastructure for map and reduce functions programmed in individual applications. Commercial clouds such as Amazon Elastic MapReduce provides the Hadoop architecture with IaaS support. In this architecture, the map and reduce functions are major determinants of end-to-end application latency, along with the framework components responsible for data access and exchange. In this paper, we aim to explore modeling methods that capture the performance characteristic and the semantics of a Hadoop architecture. We present our early results for modeling the performance of a Hadoop application given the design of map and reduce functions using Layered Queueing Network (LQN). We build two different LQN models to represent the data parallel computing of these functions and calibrate both\u00a0\u2026", "num_citations": "17\n", "authors": ["645"]}
{"title": "MEMS: a method for evaluating middleware architectures\n", "abstract": " Middleware architectures play a crucial role in determining the overall quality of many distributed applications. Systematic evaluation methods for middleware architectures are therefore important to thoroughly assess the impact of design decisions on quality goals. This paper presents MEMS, a scenario-based evaluation approach. MEMS provides a principled way of evaluating middleware architectures by leveraging generic qualitative and quantitative evaluation techniques such as prototyping, testing, rating, and analysis. It measures middleware architectures by rating multiple quality attributes, and the outputs aid the determination of the suitability of alternative middleware architectures to meet an application\u2019s quality goals. MEMS also benefits middleware development by uncovering potential problems at early stage, making it cheaper and quicker to fix design problems. The paper describes a case\u00a0\u2026", "num_citations": "17\n", "authors": ["645"]}
{"title": "An empirical evaluation of architectural alternatives for j2ee and web services\n", "abstract": " Component-based technologies such as J2EE and .NET have been widely adopted to develop Web-based applications. With the emerging use of Web services and service-based architectures, such component technologies provide the necessary back-end components to execute application business logic, supporting a variety of architectural alternatives. The combination of component technologies with Web services further extends the set of architecture choices, with each providing different levels of performance. It is therefore important for an architect to understand these performance implications in the early stages of the architecture design. In this paper, we measure and analyze the performance of four alternative architectures using J2EE and Web services. The purpose is to investigate the effects on performance, identify both hardware and software bottlenecks, and analyze the architectural design trade-offs.", "num_citations": "17\n", "authors": ["645"]}
{"title": "Collaborative tools and processes to support software engineering shift work\n", "abstract": " This paper describes the construction of and early experiences with a software engineering support environment for projects using globally distributed teams. The goals of the project are twofold. Firstly, it aims to construct a pragmatic solution to the problems experienced by widely geographically dispersed groups which collaborate on software development projects. Secondly, it aims to experiment with processes which facilitate software shift work through exploitation of time differences between collaborating groups. The construction of the support environment, known as GWSE (Global Working in Software Engineering) system, is presented, including its architecture and integration with existing workflow, document and project management tools. The use of the GWSE system in a trial development project is then described, including an initial quantitative analysis of the collaboration overheads experienced.", "num_citations": "17\n", "authors": ["645"]}
{"title": "Accuracy of Performance Prediction for EJB applications: A statistical analysis\n", "abstract": " A challenging software engineering problem is the design and implementation of component-based (CB) applications that can meet specified performance requirements. Our PPCB approach has been developed to facilitate performance prediction of CB applications built using black-box component infrastructures such as J2EE. Such deployment scenarios are problematic for traditional performance modeling approaches, which typically focus on modeling application component performance and neglect the complex influence of the specific component technology that hosts the application. In this paper, an overview of the PPCB modeling approach is given. Example results from predicting the performance of a J2EE application are presented. These results are then statistically analyzed to quantify the uncertainty in the predicted results. The contribution of the paper is the presentation of concrete measures\u00a0\u2026", "num_citations": "16\n", "authors": ["645"]}
{"title": "Parallel software engineering with PARSE\n", "abstract": " The aims of the PARSE methodology are described, and the process graph design notation is summarised. Process graphs are a new graphical notation for describing systems comprising a collection of parallel processes in a language- and architecture-independent fashion. Further, process graph designs can be mechanically transformed into Petri nets to give a more detailed, executable design specification. Some simple process graphs and their corresponding Petri nets are described in order to demonstrate this transformation process. A more extensive example then illustrates the initial stages of the design process in practice.< >", "num_citations": "16\n", "authors": ["645"]}
{"title": "The architecture of an event correlation service for adaptive middleware-based applications\n", "abstract": " Loosely coupled component communication driven by events is a key mechanism for building middleware-based applications that must achieve reliable qualities of service in an adaptive manner. In such a system, events that encapsulate state snapshots of a running system are generated by monitoring components. Hence, an event correlation service is necessary for correlating monitored events from multiple sources. The requirements for the event correlation raise two challenges: to seamlessly integrate event correlation services with other services and applications; and to provide reliable event management with minimal delay. This paper describes our experience in the design and implementation of an event correlation service. The design encompasses an event correlator and an event proxy that are integrated with an architecture for adaptive middleware components. The implementation utilizes the common\u00a0\u2026", "num_citations": "15\n", "authors": ["645"]}
{"title": "An architects guide to enterprise application integration with j2ee and. net\n", "abstract": " Architects are faced with the problem of building enterprise scale information systems, with streamlined, automated internal business processes and web-enabled business functions, all across multiple legacy applications. The underlying architectures for such systems are embodied in a range of diverse products known as Enterprise Application Integration (EAI) technologies. In this tutorial, we highlight some of the major problems, approaches and issues in designing EAI architectures and selecting appropriate supporting technology. An architect's perspective on designing large-scale integrated applications is taken, and we discuss requirements elicitation, architecture patterns, EAI technology and features, and risk mitigation. J2EE and .NET technologies are used to illustrate the capabilities of state-or-the-art integration technologies.", "num_citations": "15\n", "authors": ["645"]}
{"title": "Evaluating the sustained performance of COTS\u2010based messaging systems\n", "abstract": " Messaging systems, which include message brokers built on top of message\u2010oriented middleware, have been used as middleware components in many enterprise application integration projects. There are many COTS\u2010based messaging systems on the market, but there is little concrete understanding in the software industry on the performance of these different technologies. The authors have carried out a scenario\u2010based evaluation of three leading messaging systems to provide insight into performance issues. The evaluation process includes a study of the sustained performance of the system under load. The result of this study is used to derive a generic metric for quantifying a messaging system's performance. The paper describes a synthetic transactional scenario, which is used for load tests and performance measurement. The results from executing this test scenario with three messaging systems are then\u00a0\u2026", "num_citations": "15\n", "authors": ["645"]}
{"title": "Streamlining the acquisition process for large-scale COTS middleware components\n", "abstract": " CSIRO\u2019s i-MATE process is an established approach to help IT organizations in the acquisition of large-scale COTS middleware components. It aims to minimize technical risk by matching detailed application and infrastructure requirements to the capabilities of COTS middleware products. This paper describes a case study on the use of i-MATE in a project that required the selection of appropriate components in a compressed timeline and from a broad range of candidate COTS technologies. The steps and tools in i-MATE are briefly explained, along with the characteristics of COTS middleware components that make them a unique challenge in terms of acquisition and adoption. The case study project is then outlined, and the key business and technical issues explained. Finally, we describe and evaluate the enhancements made to the i-MATE approach to successfully respond to the challenges encountered.", "num_citations": "14\n", "authors": ["645"]}
{"title": "Toward real time data analysis for smart grids\n", "abstract": " This paper describes the architecture and design of a novel system for supporting large-scale real-time data analysis for future power grid systems. The widespread deployment of renewable generation, smart grid controls, energy storage, plug-in hybrids, and new conducting materials will require fundamental changes in the operational concepts and principal components of the grid. As a result, the whole system becomes highly dynamic and requires constant adjusting based on real time data. Even though millions of sensors such as phase measurement units (PMU) and smart meters are being widely deployed, a data layer that can analyze this amount of data in real time is needed. Unlike the data fabric in other cloud services, the data layer for smart grids has some unique design requirements. First, this layer must provide real time guarantees. Second, this layer must be scalable to allow a large number of\u00a0\u2026", "num_citations": "13\n", "authors": ["645"]}
{"title": "Velo: riding the knowledge management wave for simulation and modeling\n", "abstract": " Modern scientific enterprises are inherently knowledge-intensive. In general, scientific studies in domains such as geosciences, climate, and biology require the acquisition and manipulation of large amounts of experimental and field data in order to create inputs for large-scale computational simulations. The results of these simulations must then be analyzed, leading to refinements of inputs and models and additional simulations. Further, these results must be managed and archived to provide justifications for regulatory decisions and publications that are based on these models. In this paper we introduce our Velo framework that is designed as a reusable, domain independent knowledge management infrastructure for modeling and simulation. Velo leverages, integrates, and extends open source collaborative and content management technologies to create a scalable and flexible core platform that can be\u00a0\u2026", "num_citations": "13\n", "authors": ["645"]}
{"title": "GridOPTICS (TM): A design for plug-and-play smart grid software architecture\n", "abstract": " As the smart grid becomes reality, software architectures for integrating legacy systems with new innovative approaches for grid management are needed. These architectures must exhibit flexibility, extensibility, interoperability and scalability. In this position paper, we describe our preliminary work to design such an architecture, known as GridOPTICS, that will enable the deployment and integration of new software tools in smart grid operations. Our preliminary design is based upon use cases from PNNL's Future Power Grid Initiative, which is developing a collection of advanced software technologies for smart grid management and control. We describe the motivations for GridOPTICS, and the preliminary design that we are currently prototyping for several distinct use cases.", "num_citations": "12\n", "authors": ["645"]}
{"title": "Kepler+ MeDICi service-oriented scientific workflow applications\n", "abstract": " Scientific applications are often structured as workflows that execute a series of interdependent, distributed software modules to analyze large data sets. The order of execution of the tasks in a workflow is commonly controlled by complex scripts, which over time become difficult to maintain and evolve. In this paper, we describe how we have integrated the Kepler scientific workflow platform with the MeDICi Integration Framework, which has been specifically designed to provide a standards-based, lightweight and flexible integration platform. The MeDICi technology provides a scalable, component-based architecture that efficiently handles integration with heterogeneous, distributed software systems. This paper describes the MeDICi Integration Framework and the mechanisms we used to integrate MeDICi components with Kepler workflow actors. We illustrate this solution with a workflow application for an\u00a0\u2026", "num_citations": "12\n", "authors": ["645"]}
{"title": "Engineering high quality parallel software using PARSE\n", "abstract": " The PARSE design methodology provides a hierarchical, object-based approach to the development of high quality, reliable parallel software systems. A system design is initially structured into a collection of concurrently executing objects which communicate via message-passing. A graphical notation known as process graphs is then used to capture the structural and important dynamic properties of the system. Process graph designs can then be semi-mechanically transformed into complete Petri nets to give a detailed, executable and formally verifiable design specification.", "num_citations": "12\n", "authors": ["645"]}
{"title": "Application-Specific Evaluation of No SQL Databases\n", "abstract": " The selection of a particular NoSQL database for use in a big data system imposes a specific distributed software architecture and data model, making the technology selection difficult to defer and expensive to change. This paper reports on the selection of a NoSQL database for use in an Electronic Healthcare Record system being developed by a large healthcare provider. We performed application-specific prototyping and measurement to identify NoSQL products that fit data model and query use cases, and meet performance requirements. We found that database throughput varied by a factor of 10, read operation latency varied by a factor of 5, and write latency by a factor of 4 (with the highest throughput product delivering the highest latency). We also found that the throughput for workloads using strong consistency was 10-25% lower than workloads using eventual consistency. We conclude by reflecting on\u00a0\u2026", "num_citations": "11\n", "authors": ["645"]}
{"title": "Design assistant for NoSQL technology selection\n", "abstract": " NoSQL databases create tight coupling between data model, deployment topology, and application architecture, and so this technology selection must be one of the earliest architecture decisions. The NoSQL technology landscape is large and evolving rapidly, so architects need efficient and trusted design assistance to explore the solution space. Our solution was to create a queryable knowledge base, populated with curated information, which is rendered dynamically as content grows and changes. We built the knowledge base using Semantic MediaWiki, implementing a novel knowledge model that enables reasoning from quality attributes to architecture patterns and tactics to features implemented in specific NoSQL products. We also provide tabular and graphical visualizations to support both systematic and ad hoc exploration. Our contributions to the field of architecture design assistants are the knowledge\u00a0\u2026", "num_citations": "11\n", "authors": ["645"]}
{"title": "GridOPTICS (TM) A Novel software framework for integrating power grid data storage, management and analysis\n", "abstract": " This paper describes the architecture and design of GridOPTICS TM , a novel software framework for integrating a collection of software tools developed by NPNNL's Future Power Grid Initiative (FPGI) into a coherent, powerful operations and planning tool for the power grid of the future. GridOPTICS TM  enables plug-and-play of various analysis, modeling and visualization software tools to improve the efficiency and reliability of power grid. To bridge the data access for different control purposes, GridOPTICS TM  provides a scalable, lightweight event processing layer that hides the complexity of data collection, storage, delivery and management. A significant challenge is the requirement to access large amount of data in real time. We address this challenge though a scalable system architecture that balances system performance and ease of integration. The initial prototype of GridOPTICS TM  was demonstrated\u00a0\u2026", "num_citations": "11\n", "authors": ["645"]}
{"title": "A testbed for deploying distributed state estimation in power grid\n", "abstract": " With the increasing demand, scale, and data information of power systems, fast distributed applications are becoming more important in power system operation and control. This paper proposes a testbed for evaluating power system distributed applications, considering data exchange among distributed areas. A high-performance computing (HPC) version of distributed state estimation is implemented and used as an example distributed application. The IEEE 118-bus system is used to deploy the parallel distributed state estimation, and the MeDICi middleware is used for data communication. The performance of the testbed demonstrates its capability to evaluate parallel distributed state estimation by leveraging the HPC paradigm. This testbed can also be applied to evaluate other distributed applications.", "num_citations": "11\n", "authors": ["645"]}
{"title": "An autonomic middleware solution for coordinating multiple qos controls\n", "abstract": " Adaptive self-managing applications can adapt their behavior through components that monitor the application behavior and provide feedback controls. This paper outlines an autonomic approach to coordinate multiple controls for managing service quality using executable control models. In this approach, controls are modeled as process models. Moreover, controls with cross-cutting concerns are provisioned by a dedicated process model. The flexibility of this approach allows composing new controls from existing control components. The coordination of their dependencies is performed within a unified architecture framework for modeling, deploying and executing these models. We integrate the process modeling and execution techniques into a middleware architecture to deliver such features. To demonstrate the practical utilization of this approach, we employ it to manage fail-over and over-loading\u00a0\u2026", "num_citations": "11\n", "authors": ["645"]}
{"title": "J2EE technology performance evaluation methodology\n", "abstract": " Internet-enabled enterprise information systems using middleware technology are becoming increasingly popular. These technologies are becoming more and more complex. However, there is little understanding in the software industry on the strengths and weaknesses of competing technologies. This paper describes the approach being taken in CSIRO's Middleware Technology Evaluation (MTE) project to attempt to alleviate some of these problems. In evaluating application server performance, we have used a simple benchmark application in preference to the more complete benchmarks, such as ECperf\u2122 or RUBiS. This benchmark has the dual advantage of supplying unambiguous results, focussed on a particular area of J2EE and the flexibility to test different J2EE programming idioms.Performance evaluation requires care in the choice of methodology. We have been able to improve performance by more than an order of magnitude by adjusting the tuning parameters available in some application servers. We have followed the methodology reported here consistently to produce such effects.", "num_citations": "11\n", "authors": ["645"]}
{"title": "The PARSE project\n", "abstract": " Within the PARSE project, issues relating to the development of parallel and distributed software are being researched. These include analysis and design techniques, verification of system behaviour, performance evaluation and tool support. This paper reviews the work undertaken in the project, indicates future directions for research and provides a bibliography of key publications.", "num_citations": "11\n", "authors": ["645"]}
{"title": "Experiments in curation: Towards machine-assisted construction of software architecture knowledge bases\n", "abstract": " Software architects inhabit a complex, rapidly evolving technological landscape. An ever growing collection of competing architecturally significant technologies, ranging from distributed databases to middleware and cloud platforms, makes rigorously comparing alternatives and selecting appropriate solutions a daunting engineering task. To address this problem, we envisage an ecosystem of curated, automatically updated knowledge bases that enable straightforward and streamlined technical comparisons of related products. These knowledge bases would emulate engineering handbooks that are commonly found in other engineering disciplines. As a first step towards this vision, we have built a curated knowledge base for comparing distributed databases based on a semantically defined feature taxonomy. We report in this paper on the initial results of using supervised machine learning to assist with\u00a0\u2026", "num_citations": "10\n", "authors": ["645"]}
{"title": "Model-driven observability for big data storage\n", "abstract": " The scale, heterogeneity, and pace of evolution of the storage components in big data systems makes it impractical to manually insert monitoring code for observability metric collection and aggregation. In this paper we present an architecture that automates these metric collection processes, using a model-driven approach to configure a distributed runtime observability framework. We describe and evaluate an implementation of the architecture that collects and aggregates metrics for a big data system using heterogeneous NoSQL data stores. Our scalability tests demonstrate that the implementation can monitor 20 different metrics from 10,000 database nodes with a sampling interval of 20 seconds. Below this interval, we lose metrics due to the sustained write load required in the metrics database. This indicates that observability at scale must be able to support very high write loads in a metrics collection database.", "num_citations": "10\n", "authors": ["645"]}
{"title": "Method and system for managing power grid data\n", "abstract": " A system and method of managing time-series data for smart grids is disclosed. Data is collected from a plurality of sensors. An index is modified for a newly created block. A one disk operation per read or write is performed. The one disk operation per read includes accessing and looking up the index to locate the data without movement of an arm of the disk, and obtaining the data. The one disk operation per write includes searching the disk for free space, calculating an offset, modifying the index, and writing the data contiguously into a block of the disk the index points to.", "num_citations": "10\n", "authors": ["645"]}
{"title": "Visual data analysis as an integral part of environmental management\n", "abstract": " The U.S. Department of Energy's (DOE) Office of Environmental Management (DOE/EM) currently supports an effort to understand and predict the fate of nuclear contaminants and their transport in natural and engineered systems. Geologists, hydrologists, physicists and computer scientists are working together to create models of existing nuclear waste sites, to simulate their behavior and to extrapolate it into the future. We use visualization as an integral part in each step of this process. In the first step, visualization is used to verify model setup and to estimate critical parameters. High-performance computing simulations of contaminant transport produces massive amounts of data, which is then analyzed using visualization software specifically designed for parallel processing of large amounts of structured and unstructured data. Finally, simulation results are validated by comparing simulation results to measured\u00a0\u2026", "num_citations": "10\n", "authors": ["645"]}
{"title": "Software architecture challenges for data intensive computing\n", "abstract": " Data intensive computing is concerned with creating scalable solutions for capturing, analyzing, managing and understanding multi-terabyte and petabyte data volumes. Such data volumes exist in a diverse range of application domains, including scientific research, bio-informatics, cyber security, social computing and commerce. Innovative hardware and software technologies to address these problems must scale to meet these ballooning data volumes and simultaneously reduce the time needed to provide effective data analysis. This paper describes some of the software architecture challenges that must be addressed when building data intensive applications and supporting infrastructures. These revolve around requirements for adaptive resource utilization and management, flexible integration, robustness and scalable data management.", "num_citations": "10\n", "authors": ["645"]}
{"title": "A flexible, high performance service-oriented architecture for detecting cyber attacks\n", "abstract": " A high percentage of false positives remains a problem in current network security detection systems. With the growing reliance of industry on computer networks, and the growing variety of attacks that can be directed towards a computer network, it is clear that detection systems must be improved in order to tackle this growing problem. To help minimise the problem of false positives, this paper describes a method and apparatus for security alert analysis that is based on two technologies: (i) event correlation and (ii) a truth maintenance system. This work was undertaken in the context of practical network security management in a large outsourced management service provider in the Asia-Pacific region.", "num_citations": "10\n", "authors": ["645"]}
{"title": "Software Engineering for Parallel and Distributed Systems: Callenges and Opportunities\n", "abstract": " Design Identifying solution-domain parallelism Intercomponent communications Component mobility Component synchronization Shared component access Creating or destroying processes and threads Distributed exception-handling techniques Design validation", "num_citations": "10\n", "authors": ["645"]}
{"title": "Groupware support tools for collaborative software engineering\n", "abstract": " This paper describes a software engineering support environment to facilitate software development and testing using globally distributed teams. The construction of this support environment, known as the GWSE (Global Working in Software Engineering) system, has been based upon Lotus Notes, which has provided a flexible and extensible data repository. Software configuration management and project management facilities are incorporated into the GWSE system by integrating existing tools with the Notes repository. The architecture and design of the GWSE system is presented, and a brief account of its use in a project is given.", "num_citations": "10\n", "authors": ["645"]}
{"title": "A parallel approach to high-speed protocol processing\n", "abstract": " A rapid increase in the transmission bandwidth of optical networks has created a bottleneck in protocol processing at the end systems. This has resulted in the inability of applications and network protocols to exploit the full bandwidth of a high-speed network. This paper presents a parallel architecture that is designed to support high-speed protocol processing. The advent of the T9000 transputer and C104 router technology has provided a platform that is suitable for the construction of a highly parallel and scalable protocol processing architecture based on packet and functional parallelism. A simulation of the architecture has been implemented and has demonstrated the advantage of exploiting a parallel architecture for protocol processing.", "num_citations": "10\n", "authors": ["645"]}
{"title": "Interfacing transputer links to external devices\n", "abstract": " Two approaches to interfacing transputers to external circuitry are considered. The design and construction of both a serial and a parallel interface for transputers is described. These enable transputers to communicate with peripherals via the standard Inmos transputer links. A Motorola MC68701 8-bit microcomputer, serving as a programmable I/O controller, is incorporated into both interfaces. This solution lends a high degree of flexibility into the interface design, and can be used with any transputer board which has spare links available at edge connectors. Finally, two control applications are briefly described which utilize the interfaces.", "num_citations": "10\n", "authors": ["645"]}
{"title": "Exploring cloud computing for large-scale scientific applications\n", "abstract": " This paper explores cloud computing for large-scale data intensive scientific applications. Cloud computing is attractive because it provides hardware and software resources on-demand, which relieves the burden of acquiring and maintaining a huge amount of resources that may be used only once by a scientific application. However, unlike typical commercial applications that often just requires a moderate amount of ordinary resources, large-scale scientific applications often need to process enormous amount of data in the terabyte or even petabyte range and require special high performance hardware with low latency connections to complete computation in a reasonable amount of time. To address these challenges, we build an infrastructure that can dynamically select high performance computing hardware across institutions and dynamically adapt the computation to the selected resources to achieve high\u00a0\u2026", "num_citations": "9\n", "authors": ["645"]}
{"title": "Build less code deliver more science: An experience report on composing scientific environments using component-based and commodity software platforms\n", "abstract": " Modern scientific software is daunting in its diversity and complexity. From massively parallel simulations running on the world's largest supercomputers, to visualizations and user support environments that manage ever growing complex data collections, the challenges for software engineers are plentiful. While high performance simulators are necessarily specialized codes to maximize performance on specific supercomputer architectures, we argue the vast majority of supporting infrastructure, data management and analysis tools can leverage commodity open source and component-based technologies. This approach can significantly drive down the effort and costs of building complex, collaborative scientific user environments, as well as increase their reliability and extensibility. In this paper we describe our experiences in creating an initial user environment for scientists involved in modeling the detailed effects\u00a0\u2026", "num_citations": "9\n", "authors": ["645"]}
{"title": "Services+ components= data intensive scientific workflow applications with medici\n", "abstract": " Scientific applications are often structured as workflows that execute a series of distributed software modules to analyze large data sets. Such workflows are typically constructed using general-purpose scripting languages to coordinate the execution of the various modules and to exchange data sets between them. While such scripts provide a cost-effective approach for simple workflows, as the workflow structure becomes complex and evolves, the scripts quickly become complex and difficult to modify. This makes them a major barrier to easily and quickly deploying new algorithms and exploiting new, scalable hardware platforms. In this paper, we describe the MeDICi Workflow technology that is specifically designed to reduce the complexity of workflow application development, and to efficiently handle data intensive workflow applications. MeDICi integrates standard component-based and service-based\u00a0\u2026", "num_citations": "9\n", "authors": ["645"]}
{"title": "An architecture for dynamic data source integration\n", "abstract": " Integrating multiple heterogeneous data sources in to applications is a time-consuming, costly and error-prone engineering task. Relatively mature technologies exist that make integration tractable from an engineering perspective. These technologies however have many limitations, and hence present opportunities for breakthrough research. This paper briefly describes some of these limitations. It then provides an overview of the Data Concierge research project and prototype that is attempting to provide solutions to some of these limitations. The paper focuses on the core architecture and mechanisms in the Data Concierge for dynamically attaching to a previously unidentified source of information. The generic API supported by the Data Concierge is described, along with the architecture and prototype tools for describing the meta-data necessary to facilitate dynamic integration. In addition, we describe the\u00a0\u2026", "num_citations": "9\n", "authors": ["645"]}
{"title": "The devil is in the detail: A comparison of CORBA object transaction services\n", "abstract": " The CORBA Object Transaction Service (OTS) is a key component in many enterprise information systems built using distributed object technology. The Object Management Group (OMG) defines the OTS through a set of standard interfaces and services that a CORBA-compliant OTS must adhere to. Product vendors then implement a realization of the OTS in their CORBA technology. This level of standardization in theory allows application designers to architect their systems in a product-neutral manner, using a standard set of interfaces that all the various OTS implementations support. This paper examines three concrete OTS products from different vendors. The underlying approaches and architectures are compared, and their strengths and weaknesses analyzed. The analysis clearly shows that major differences exist in the behavior of the different products, and these have profound effects on application\u00a0\u2026", "num_citations": "9\n", "authors": ["645"]}
{"title": "Towards A Methodology for 24 Hour Software Production Using Globally Separated Development Teams\n", "abstract": " The potential advantages of utilising software development teams which reside in different time zones are discussed. The experiences of an experimental study which attempted to share a software project between teams in Australia and India are then related. The results of this study show that, co-operative software development on a global scale is feasible, productivity gains are achievable and potentially enhanced product quality is an important by-product.", "num_citations": "9\n", "authors": ["645"]}
{"title": "A collaborative extensible user environment for simulation and knowledge management\n", "abstract": " In scientific simulation, scientists use measured data to create numerical models, execute simulations and analyze results from advanced simulators executing on high performance computing platforms. This process usually requires a team of scientists collaborating on data collection, model creation and analysis, and on authorship of publications and data. This paper shows that scientific teams can benefit from a user environment called Akuna that permits subsurface scientists in disparate locations to collaborate on numerical modeling and analysis projects. The Akuna application is built on the Velo software platform that provides a desktop client environment for conducting and analyzing simulations, a knowledge management server for project and data management, annotation, collaboration, job execution, and event-based communication, and a Tool Integration Framework for connecting any type of software\u00a0\u2026", "num_citations": "8\n", "authors": ["645"]}
{"title": "Parallel architecture support for high-speed protocol processing\n", "abstract": " A rapid increase in the transmission bandwidth of optical networks has created a bottleneck in protocol processing at the host systems. This paper presents a high-performance transport protocol, HTPNET, that is designed to exploit the evolving characteristics of high-speed networks. Importantly, the highly parallel architecture of HTPNET provides a suitable platform for the parallel implementation of presentation processing, which incurs high computation overheads. A parallel architecture based on the T9000 transputer and C104 router technology has been designed to support high-speed protocol processing. A simulation of the architecture has been implemented and has demonstrated the advantage of exploiting a parallel architecture for protocol processing.", "num_citations": "8\n", "authors": ["645"]}
{"title": "Reliable parallel software construction using PARSE\n", "abstract": " The PARSE design methodology provides a hierarchical, object\u2010based approach to the development of parallel software systems. A system design is initally structured into a collection of concurrently executing objects which communicate via message\u2010passing. A graphical notation known as the process graph is then used to capture the structural and important dynamic properties of the system. Process graph designs can then be semi\u2010mechanically transformed into complete Petri nets to give a detailed, executable and potentially verifiable design specification. From a complete design, translation rules for target programming languages are defined to enable the implementation to proceed in a systematic manner. The paper describes the steps in PARSE methodology and process graph notation, and illustrates the application of PARSE from design specification to program code using a network protocol example.", "num_citations": "8\n", "authors": ["645"]}
{"title": "Advanced Simulation Capability for Environmental Management (ASCEM) Phase II Demonstration\n", "abstract": " In 2009, the National Academies of Science (NAS) reviewed and validated the US Department of Energy Office of Environmental Management (EM) Technology Program in its publication, Advice on the Department of Energy\u2019s Cleanup Technology Roadmap: Gaps and Bridges. The NAS report outlined prioritization needs for the Groundwater and Soil Remediation Roadmap, concluded that contaminant behavior in the subsurface is poorly understood, and recommended further research in this area as a high priority. To address this NAS concern, the EM Office of Site Restoration began supporting the development of the Advanced Simulation Capability for Environmental Management (ASCEM). ASCEM is a state-of-the-art scientific approach that uses an integration of toolsets for understanding and predicting contaminant fate and transport in natural and engineered systems. The ASCEM modeling toolset is modular and open source. It is divided into three thrust areas: Multi-Process High Performance Computing (HPC), Platform and Integrated Toolsets, and Site Applications. The ASCEM toolsets will facilitate integrated approaches to modeling and site characterization that enable robust and standardized assessments of performance and risk for EM cleanup and closure activities. During fiscal year 2012, the ASCEM project continued to make significant progress in capabilities development. Capability development occurred in both the Platform and more\u00bb", "num_citations": "7\n", "authors": ["645"]}
{"title": "Performance Evaluation: Metrics, Models and Benchmarks: SPEC International Performance Evaluation Workshop, SIPEW 2008, Darmstadt, Germany, June 27-28, 2008, Proceedings\n", "abstract": " This book constitutes the refereed proceedings of the SPEC International Performance Evaluation Workshop, SIPEW 2008, held in Darmstadt, Germany, in June 2008. The 17 revised full papers presented together with 3 keynote talks were carefully reviewed and selected out of 39 submissions for inclusion in the book. The papers are organized in topical sections on models for software performance engineering; benchmarks and workload characterization; Web services and service-oriented architectures; power and performance; and profiling, monitoring and optimization.", "num_citations": "7\n", "authors": ["645"]}
{"title": "A high-performance event service for hpc applications\n", "abstract": " Event services based on publish-subscribe architectures are well established components of distributed computing applications. Recently, an event service has been proposed as part of the common component architecture (CCA) for high-performance computing applications. In this paper we describe our experiences investigating implementation options for the CCA event service that exploit interprocess communications mechanisms commonly used on HPC platforms. The aim of our work is to create an event service that supports the well-known software engineering advantages of publish-subscribe architectures, and provides performance levels approaching those achievable using more primitive message-passing mechanisms such as MPI.", "num_citations": "7\n", "authors": ["645"]}
{"title": "Dynamic adapter generation for data integration middleware\n", "abstract": " Relatively mature integration technologies are available that make application integration tractable from an engineering perspective. However, integrating multiple heterogeneous data sources into applications is still a time-consuming, costly and error-prone engineering task, because existing integration technologies are based on static integration architectures. Such architectures tightly couple a data source to the data integration infrastructure. This paper describes the architecture and evolving prototype implementation of the Data Concierge, which provides a dynamic solution to integrate heterogeneous data sources. The core architecture and mechanisms in the Data Concierge can be used for dynamically attaching to a previously unidentified source of information, without the need for a pre-existing adapter component. In this paper, an ontology based API description mechanism supported by the Data\u00a0\u2026", "num_citations": "7\n", "authors": ["645"]}
{"title": "A predictive performance model to evaluate the contention cost in application servers\n", "abstract": " In multi-tier enterprise systems, application servers are key components that implement business logic and provide application services. To support a large number of simultaneous accesses from clients over the Internet and intranet, most application servers use replication and multi-threading to handle concurrent requests. While multiple processes and multiple threads enhance the processing bandwidth of servers, they also increase the contention for resources in application servers. The paper investigates this issue empirically based on a middleware benchmark. A cost model is proposed to estimate the overall performance of application servers, including the contention overhead. This model is then used to determine the optimal degree of the concurrency of application servers for a specific client load. A case study based on CORBA is presented to validate our model and demonstrate its application.", "num_citations": "7\n", "authors": ["645"]}
{"title": "Formal validation of a high performance error control protocol using SPIN\n", "abstract": " This paper presents the specification and validation of a high performance error control protocol. A formal specification model of the protocol was described using the PROMELA language. Formal analysis of the protocol model was validated using the SPIN validation tool. The uncovering of several subtle properties of the protocol has demonstrated the advantage of employing formal validation methods in designing distributed systems.", "num_citations": "7\n", "authors": ["645"]}
{"title": "Simulating microprocessor systems using occam and a network of transputers\n", "abstract": " The simulation at component level of microprocessor systems provides a precise technique for evaluating the design of a system with regards to its requirements specification. The paper describes the use of occam to simulate individual microprocessor components, and presents a method of modelling the behaviour of microprocessor bus systems. The possibility of utilising transputers to provide real-time simulation is discussed.< >", "num_citations": "7\n", "authors": ["645"]}
{"title": "Reveal: An extensible reduced-order model builder for simulation and modeling\n", "abstract": " Many science domains need to build computationally efficient and accurate representations of high fidelity, computationally expensive simulations known as reduced-order models (ROMs). This article presents the design and implementation of the Reveal toolset, a ROM builder that generates ROMs based on science- and engineering-domain-specific simulations executed on high-performance computing (HPC) platforms. The toolset encompasses a range of sampling and regression methods for ROM generation, automatically quantifies ROM accuracy, and supports an iterative approach to improve ROM accuracy. Reveal is designed to be extensible for any simulator that has published input and output formats. It also defines programmatic interfaces to include new sampling and regression techniques so users can mix and match mathematical techniques best suited to their model characteristics. The article\u00a0\u2026", "num_citations": "6\n", "authors": ["645"]}
{"title": "Advanced simulation capability for environmental management-current status and phase II demonstration results-13161\n", "abstract": " The US Department of Energy (USDOE) Office of Environmental Management (EM), Office of Soil and Groundwater, is supporting development of the Advanced Simulation Capability for Environmental Management (ASCEM). ASCEM is a state-of-the-art scientific tool and approach for understanding and predicting contaminant fate and transport in natural and engineered systems. The modular and open source high-performance computing tool facilitates integrated approaches to modeling and site characterization that enable robust and standardized assessments of performance and risk for EM cleanup and closure activities.The ASCEM project continues to make significant progress in development of computer software capabilities with an emphasis on integration of capabilities in FY12. Capability development is occurring for both the Platform and Integrated Toolsets and High-Performance Computing (HPC) Multiprocess Simulator. The Platform capabilities provide the user interface and tools for end-to-end model development, starting with definition of the conceptual model, management of data for model input, model calibration and uncertainty analysis, and processing of model output, including visualization. The HPC capabilities target increased functionality of process model representations, toolsets for interaction with Platform, and verification and model confidence testing.", "num_citations": "6\n", "authors": ["645"]}
{"title": "Modeling and analysis of middleware design for streaming power grid applications\n", "abstract": " High quality, high throughput sensor devices in the power distribution network are driving an increase in the volume and the rate of data streams available to monitor and control the power grid. Middleware support is essential to coordinate data streams with distributed power models and adapt to situations with data communication failures and errors in the sensor measurements. One challenge in designing this middleware support is scalability. In particular, the number of sensor devices and their intercommunications is a significant factor in determining temporal and functional properties of power models such as distributed state estimation. In this paper, we present our experience modeling the entire data flow from sensor devices to distributed state estimators using middleware. This model helps to analyze the middleware's behavior and its scalability in coordinating data streams.", "num_citations": "6\n", "authors": ["645"]}
{"title": "Advanced simulation capability for environmental management (ASCEM): an overview of initial results\n", "abstract": " The US Department Energy (DOE) Office of Environmental Management (EM) determined that uniform application of advanced modeling in the subsurface could potentially help reduce the cost and risk associated with its environmental cleanup mission. In response to this determination, the EM Office of Technology Innovation and Development (OTID), Groundwater and Soil Remediation (GW&S) began the program Advanced Simulation Capability for Environmental Management (ASCEM). ASCEM is a state-of-the-art scientific tool and approach for integrating data and scientific understanding to enable prediction of contaminant fate and transport in natural and engineered systems. This initiative supports the reduction of uncertainties and risks associated with EM's environmental cleanup and closure programs by better understanding and quantifying the subsurface flow and contaminant transport behavior in\u00a0\u2026", "num_citations": "6\n", "authors": ["645"]}
{"title": "Software Architecture\n", "abstract": " Software Architecture - NASA/ADS Now on home page ads icon ads Enable full ADS view NASA/ADS Software Architecture Babar, Muhammad Ali ; Gorton, Ian Abstract Publication: Lecture Notes in Computer Science Pub Date: 2010 DOI: 10.1007/978-3-642-15114-9 Bibcode: 2010LNCS......B Keywords: Computer Science; Software Engineering; Logics and Meanings of Programs; Computer Communication Networks; Management of Computing and Information Systems; Information Systems Applications (incl.Internet); Programming Languages; Compilers; Interpreters full text sources Publisher | \u00a9 The SAO/NASA Astrophysics Data System adshelp[at]cfa.harvard.edu The ADS is operated by the Smithsonian Astrophysical Observatory under NASA Cooperative Agreement NNX16AC86A NASA logo Smithsonian logo Resources About ADS ADS Help What's New Careers@ADS Social @adsabs ADS Blog Project \u2026", "num_citations": "6\n", "authors": ["645"]}
{"title": "PARSE-DAT: An Integrated Environment for the Design and Analysis of Dynamic Software Architectures.\n", "abstract": " Robust distributed software infrastructures such as DCE and CORBA are becoming widely used to aid in building complex distributed systems. However, the engineering of distributed software is a difficult task since there are many concurrency and correctness issues that need to be considered. PARSE-DAT(PARallel Software Engineering\u2013Design Analysis Tool) is an integrated environment that enables the design and analysis of dynamic software architectures. Architects construct software architectures using a set of well-defined graphical notation called Dynamic PARSE Process Graph Notation (Dynamic PARSE-PGN) in the graph editing environment (PARSE-DT). These software designs can then be translated into the corresponding \u03c0-Calculus model, and subsequently analysed for structural deadlock in the analysis/verification environment (PARSE-AT). This paper firstly presents the Dynamic PARSE design\u00a0\u2026", "num_citations": "6\n", "authors": ["645"]}
{"title": "Workflow support for change management and concurrency\n", "abstract": " This paper describes the development of a coordination support system for distributed software development teams within environments of change and pressure for improved delivery times. It concentrates on using concurrent engineering methods to improve delivery and shows how concurrent processes can be supported using coordination support tools. It then speculates on extensions needed for the cycle to support change and how these extensions can be supported by coordination tools.", "num_citations": "6\n", "authors": ["645"]}
{"title": "Special issue on software engineering for parallel systems\n", "abstract": " Many software applications require the use of explicit parallel programming techniques in order to meet their specification. Parallelism is needed to exploit the processing power of multiprocessor systems in order to achieve high performance, to provide fault-tolerance and reliability in safety-critical and real-time systems, and to deal with physically distributed computing resources. While the range of existing software and hardware technology that can be employed in parallel systems development is massive, a set of underlying problems concerned solely with the use of parallelism can be identified. Briefly these include:\u2022 identification of problem-domain and solution-domain parallelism;\u2022 incorporation of parallel activities in specification and design;\u2022 architectural influences on design and implementation, including use of virtual machines;\u2022 correctness and testing of parallel systems;\u2022 performance prediction\u00a0\u2026", "num_citations": "6\n", "authors": ["645"]}
{"title": "Designing parallel database programs using PARSE\n", "abstract": " The problems of constructing parallel programs are considered and the aims of the PARSE parallel software engineering methodology are introduced. PARSE is a multi-stage methodology that covers logical and physical design, design verification, and implementation strategy. The first stage in the methodology involves the use of process graphs which provide a language and architecture independent notation for partitioning the problem into processes and specifying the communication relationships between them. This paper gives an outline of the PARSE methodology and describes the process graph notation. The use of the notation is illustrated with an example of a design for a parallel database system.< >", "num_citations": "6\n", "authors": ["645"]}
{"title": "Hyperscalability\u2013The Changing Face of Software Architecture\n", "abstract": " The scale of contemporary Internet-based systems, along with their rate of growth, is daunting. Data repositories are growing at phenomenal rates, and new data centers hosting tens of thousands of machines are being built all over the world to store, process, and analyze this data. Societal change driven by these systems has been immense in the last decade, and the rate of innovation is only set to grow. We have truly entered the era of big data. This chapter is about engineering systems at hyperscale \u2013 these must rapidly grow throughput while constraining costs and resources. It describes the characteristics of hyperscale systems, and some of the core principles that are necessary to ensure hyperscalability. These principles are illustrated by state-of-the-art approaches and technologies that are used to engineering hyperscalable systems.", "num_citations": "5\n", "authors": ["645"]}
{"title": "Insights from 15 Years of ATAM Data: Towards Agile Architecture\n", "abstract": " Agile teams strive to balance short term feature development with longer term quality concerns. These evolutionary approaches often hit a\" complexity wall\" from the cumulative effects of unplanned changes, resulting in unreliable, poorly performing software. Consequently, there is renewed focus on approaches to address architectural concerns within the Agile community. We present an analysis of quality attribute concerns from 15 years of Architecture Trade-off Analysis Method data, gathered from 31 projects. We found that modifiability is the dominant concern across all project types; additionally there was considerable focus on performance, availability, and interoperability. For information technology projects, a relatively new quality\u2014deployability\u2014has emerged as a key concern. Our results provide insights for Agile teams allocating architecture-related tasks to iterations. For example they can use these results to create checklists for release planning or retrospectives to help assess whether a given quality should be addressed to support future needs.", "num_citations": "5\n", "authors": ["645"]}
{"title": "Geolens: Enabling interactive visual analytics over large-scale, multidimensional geospatial datasets\n", "abstract": " With the rapid increase of scientific data volumes, interactive tools that enable effective visual representation for scientists are needed. This is critical when scientists are manipulating voluminous datasets and especially when they need to explore datasets interactively to develop their hypotheses. In this paper, we present an interactive visual analytics framework, GeoLens. GeoLens provides fast and expressive interactions with voluminous geo-spatial datasets. We provide an expressive visual query evaluation scheme to support advanced interactive visual analytics technique, such as brushing and linking. To achieve this, we designed and developed the Geohash based image tile generation algorithm that automatically adjusts the range of data to access based on the minimum acceptable size of the image tile. In addition, we have also designed an autonomous histogram generation algorithm that generates\u00a0\u2026", "num_citations": "5\n", "authors": ["645"]}
{"title": "Using AI to model quality attribute tradeoffs\n", "abstract": " Many AI techniques have been applied to goal-oriented requirements engineering. However, such techniques have focused mostly on the intellectual challenge and ignored the engineering challenge of RE at scale. We discuss some of these existing approaches. We then introduce some early work that aims to add contextual quality attribute information to leverage the power of AI techniques and tools with real-world engineering. We believe this will address some of the acquisition and context problems that have plagued AI in RE.", "num_citations": "5\n", "authors": ["645"]}
{"title": "Designing the cloud-based DOE systems biology knowledgebase\n", "abstract": " Systems Biology research, even more than many other scientific domains, is becoming increasingly data-intensive. Not only have advances in experimental and computational technologies lead to an exponential increase in scientific data volumes and their complexity, but increasingly such databases are providing the basis for new scientific discoveries. To engage effectively with these community resources, integrated analyses, synthesis and simulation software is needed, supported by scientific workflows. In order to provide a more collaborative, community driven research environment for this heterogeneous setting, the Department of Energy (DOE) has decided to develop a federated, cloud based cyber infrastructure the Systems Biology Knowledgebase (Kbase). In this context the Pacific Northwest National Laboratory (PNNL) has been defining and testing the basic federated cloud-based system architecture\u00a0\u2026", "num_citations": "5\n", "authors": ["645"]}
{"title": "Analyzing Web Logs to Detect User-Visible Failures.\n", "abstract": " Web applications suffer from poor reliability. Practitioners commonly rely on fast failure detection to recover their applications quickly to reduce the effects of the failures on other users. In this paper, we present a technique for detecting user-visible failures by analyzing Web logs. Our technique applies a first-order Markov model to infer anomalous browsing behavior discovered in Web logs as indicators that users have encountered failures. We implemented our technique in a tool called REBA (REcursive Byesian Analysis of Web Logs). We evaluated our technique using REBA applied to the Web site of NASA. The results demonstrate that our technique can detect user-visible failures with reasonable cost.", "num_citations": "5\n", "authors": ["645"]}
{"title": "GS3: A Knowledge Management Architecture for Collaborative Geologic Sequestration Modeling\n", "abstract": " Modern scientific enterprises are inherently knowledge-intensive. In general, scientific studies in domains such as geoscience, chemistry, physics and biology require the acquisition and manipulation of large amounts of experimental and field data in order to create inputs for large-scale computational simulations. The results of these simulations must then be analyzed, leading to refinements of inputs and models and further simulations. In this paper we describe our efforts in creating a knowledge management platform to support collaborative, wide-scale studies in the area of geologic sequestration modeling. The platform, known as GS3 (Geologic Sequestration Software Suite), exploits and integrates off-the-shelf software components including semantic wikis, content management systems and open source middleware to create the core architecture. We then extend the wiki environment to support the capture of\u00a0\u2026", "num_citations": "5\n", "authors": ["645"]}
{"title": "Soya: a programming model and runtime environment for component composition using SSDL\n", "abstract": " The SOAP Service Description Language (SSDL) is a SOAP-centric language for describing Web Service contracts. SSDL focuses on message abstraction as the building block for creating service-oriented applications and provides an extensible range of protocol frameworks that can be used to describe and formally model component composition based on Web Service interactions. Given its novel approach, implementing support for SSDL contracts presents interesting challenges to middleware developers. At one end of the spectrum, programming abstractions that support message-oriented designs need to be created. At the other end, new functionality and semantics must be added to existing SOAP engines. In this paper we explain how component developers can create message-oriented Web Service interfaces with contemporary tool support (specifically the Windows Communication Foundation\u00a0\u2026", "num_citations": "5\n", "authors": ["645"]}
{"title": "Analyzing the scalability of transactional CORBA applications\n", "abstract": " The middleware technology used as the foundation of Internet-enabled enterprise information systems is becoming increasingly complex. In addition, the various technologies offer a number of standard component architectures that can be used by designers as templates to build applications. However, there is little understanding in the software industry on the strengths and weaknesses of competing technologies, and the different trade-offs that various component architectures impose. This paper describes the approach being taken in CSIRO's Middleware Technology Evaluation (MTE) project to attempt to alleviate some of these problems. Specifically, this paper focuses on the scalability of transactional CORBA applications using the Visibroker Integrated Transaction Service v1.2. It discusses the methodology that has been developed, the application and environment used, and presents some performance\u00a0\u2026", "num_citations": "5\n", "authors": ["645"]}
{"title": "Understanding the role of constraints on architecturally significant requirements\n", "abstract": " A key constraint on software development is reliance on tools, which we define as COTS products, software services, languages, frameworks and platforms. These tools may have significant architectural impacts that are not obvious when the requirements are elicited, tools selected, and architecture sketched out. In this paper, we report on a case study we conducted to identify architecturally significant requirements (ASRs) that were impacted by tool selection. We identified ASRs in an existing health IT project, CONNECT, and also identified the constraints on the project that were tool-related. We produce a mapping showing how the architectural risks identified in the initial architectural analysis were impacted by the tool choices made. We produce metrics showing how much time has been consumed when implementing ASRs that involve working around/with these constraints and the risks associated with them.", "num_citations": "4\n", "authors": ["645"]}
{"title": "Advancing software architecture modeling for large scale heterogeneous systems\n", "abstract": " In this paper we describe how incorporating technology-specific modeling at the architecture level can help reduce risks and produce better designs for large, heterogeneous software applications. We draw an analogy with established modeling approaches in scientific domains, using groundwater modeling as an example, to help illustrate gaps in current software architecture modeling approaches. We then describe the advances in modeling, analysis and tooling that are required to bring sophisticated modeling and development methods within reach of software architects.", "num_citations": "4\n", "authors": ["645"]}
{"title": "Hardware-accelerated components for hybrid computing systems\n", "abstract": " We present a study on the use of component technology for encapsulating platform-specific hardware-accelerated algorithms on hybrid HPC systems. Our research shows that component technology can have significant benefits from a software engineering point-of-view to increase encapsulation, portability and reduce or eliminate platform dependence for hardware-accelerated algorithms.", "num_citations": "4\n", "authors": ["645"]}
{"title": "Software Architecture in Game Development\n", "abstract": " Summary form only given. Video games have now existed in various forms for over 30 years, and have evolved from humble beginnings into remarkably complex software projects. The ever present emphasis on an immersive audio/visual experience has put game developers in the position of being on the bleeding edge of exploring the performance of modern consumer hardware. This talk will discuss the elements that make up a contemporary video game, the software processes that are involved in development, key challenges, and look at some important design patterns that form the architectural basis.", "num_citations": "4\n", "authors": ["645"]}
{"title": "Architecture knowledge management: concepts, technologies, challenges\n", "abstract": " Summary form only given. In this tutorial, we highlight the benefits and challenges in capturing and managing software architecture knowledge for supporting an architecture-centric software development process. We discuss various approaches to characterize software architecture knowledge based on the requirements of a particular domain. We describe various concepts and approaches to manage the software architecture knowledge from both management and technical perspectives. We also demonstrate the utility of captured architecture knowledge to support software architecture activities with a case study covering the use of architecture knowledge management techniques and tools in an industrial project. The specific architecture knowledge management technology used during the tutorial is BRedB, a software tool developed in National ICT Australia. Finally, the ways in which Wikis can be used to\u00a0\u2026", "num_citations": "4\n", "authors": ["645"]}
{"title": "Design level performance modeling of component-based applications\n", "abstract": " In this paper, we present an approach to predict the performance of component-based applications in the design phase of development. We can build a quantitative performance model for a proposed system design. The inputs needed to produce this performance prediction are a state diagram showing the main waiting and resource usage aspects of the proposed system architecture, and measurements taken on the middleware infrastructure using a simple benchmark application which is much cheaper to implement than the full system. The performance model allows the system designer to make decisions between alternative architectures and implementation approaches, in terms of their scalability, and ability to achieve required service levels. We show our method in action using a J2EE application, Stock-Online, and validate these predictions by implementing the design and measuring its performance. The modeling approach is applicable to applications built on common middleware technologies such as CORBA, J2EE and COM+/.NET.", "num_citations": "4\n", "authors": ["645"]}
{"title": "Evaluating Object Transactional Monitors with OrbixOTM.\n", "abstract": " As more and more object-oriented transactional processing monitors are being developed, users in industries such as banking and telecommunications need systematic and critical evaluations of the strengths and weaknesses of these products. This paper presents the Middleware Evaluation Project (MEP) which aims to provide an impartial evaluation based on rigorously derived tests and benchmarks. The evaluation framework based on TPC's benchmark C is firstly presented followed by discussions on the set of evaluation criteria. Preliminary results on the OTM product OrbixOTM are also given.", "num_citations": "4\n", "authors": ["645"]}
{"title": "An MS in CS for non-CS Majors: Moving to Increase Diversity of Thought and Demographics in CS\n", "abstract": " We have created, piloted and are growing the Align program, a Master of Science in Computer Science (MS in CS) for post-secondary graduates who did not major in CS. Our goal is to create a pathway to CS for all students, with particular attention to women and underrepresented minorities. Indeed, women represent 57% and underrepresented minorities represent 25% of all bachelor's recipients in the US, but only 19.5% and 12.6% of CS graduates, respectively. If we can fill this opportunity gap, we will satisfy a major economic need and address an issue of social equity and inclusion. In this paper, we present our\" Bridge''curriculum, which is a two-semester preparation for students to then join the traditional MS in CS students in master's-level classes. We describe co-curricular activities designed to help students succeed in the program. We present our empirical findings around enrollment, demographics\u00a0\u2026", "num_citations": "3\n", "authors": ["645"]}
{"title": "Model driven performance simulation of cloud provisioned Hadoop MapReduce applications\n", "abstract": " Hadoop is a widely adopted open source implementation of MapReduce. A Hadoop cluster can be fully provisioned by a Cloud service provider to provide elasticity in computational resource allocation. Understanding the performance characteristics of a Hadoop job can help achieve an optimal balance between resource usage (cost) and job latency on a cloud-based cluster. This paper presents a method that estimates the performance of a MapReduce application in a Cloud provisioned Hadoop cluster. We develop a model-driven approach that models a cloud provided independent Hadoop MapReduce model and customizes it for a specific Cloud deployment. These models are further transformed into a simulation model that produces estimations of end-to-end job latency. We explore this method in the design space of MapReduce applications to estimate the performance for different sizes of input data. Our\u00a0\u2026", "num_citations": "3\n", "authors": ["645"]}
{"title": "Quality Attribute-Guided Evaluation of NoSQL Databases: A Case Study\n", "abstract": " For software developers, the selection of a particular NoSQL technology imposes a specific distributed software architecture and data model, making the technology selection difficult to defer. NoSQL database technologies provide high levels of performance, scalability, and availability by simplifying data models and supporting horizontal scaling and data replication. Each NoSQL product embodies a particular set of consistency, availability, and partition tolerance CAP tradeoffs, along with a data model that reduces the conceptual mismatch between data access and data storage models. This means technology selection must be done early, often with limited information about specific application requirements, and the decision must balance speed with precision, as the NoSQL solution space is large and evolving rapidly. In this paper we present the method and results of a study to compare the architecturally-relevant characteristics of three NoSQL databases for use in a large, distributed healthcare organization. We reflect on some of the fundamental difficulties of performing detailed technical evaluations of NoSQL databases specifically, and big data systems in general, that have become apparent during our study.Descriptors:", "num_citations": "3\n", "authors": ["645"]}
{"title": "Nosql data store technologies\n", "abstract": " The Interagency Program Office has been exploring a number of options for interoperation between the electronic health record systems used by DoD and the Veterans Health Administration. These have included replacing both systems with a single new Integrated Electronic Health Record iEHR system and federating the information contained in existing systems, among other approaches. In addition, DoD has been exploring caching approaches to improve delivery of electronic health record applications over network links with low quality of service. The Military Health Systems Joint Program Committee funded the Army Telemedicine and Advanced Technology Research Center TATRC to work with the SEI to investigate the use of emerging NoSQL database technology to achieve the data storage capabilities needed for these systems. The SEI conducted a stakeholder workshop with MHS stakeholders to identify architecture drivers and quality attribute requirements for these applications. These requirements were then used to create technology evaluation criteria. The SEI then worked with developers from the TATRC Advanced Concepts Team to conduct a series of technology experiments to assess the suitability of several NoSQL products against the evaluation criteria. One NoSQL product was selected for evaluation from each of the four NoSQL categories Document Store MongoDB, Column Family Store Cassandra, Key-Value Store Riak, and Graph Store Neo4J. Each product was installed in a server cluster in the SEI Virtual Private Cloud, and performance measurements were made for each using the YCSB Yahoo Cloud Serving\u00a0\u2026", "num_citations": "3\n", "authors": ["645"]}
{"title": "Nationwide buildings energy research enabled through an integrated data intensive\n", "abstract": " Modern workflow systems can enable scientists to run ensemble simulations at unprecedented scales and levels of complexity, allowing them to study system sizes previously impossible to achieve. However as a result of these new capabilities the science teams suddenly also face unprecedented data volumes that they are unable to analyze with their existing tools and methodologies in a timely fashion. In this paper we describe the ongoing development work to create an integrated data intensive scientific workflow and analysis environment that offers researchers the ability to easily create and execute complex simulation studies and provides them with different scalable methods to analyze the resulting data volumes. The capabilities of the new environment are demonstrated on a use case that focuses on building energy modeling. As part of the PNNL research initiative PRIMA (Platform for Regional\u00a0\u2026", "num_citations": "3\n", "authors": ["645"]}
{"title": "Architecture-Based Adaptivity Support for Service Oriented Scientific Workflows\n", "abstract": " Adaptivity is the ability of a program to change its behavior automatically according to its context. Programs over multiple scientific workflows and analytic domains have similar needs of adaptivity to handle data intensive computing. These include dynamically selecting analytical models or processes according to data sets at runtime, handling exceptions to make long running workflows reliable, and reducing large volumes of data to a suitable form for visualization. Architecture support to reuse these adaptive techniques across different scientific domains helps to enhance the workflows' efficiency, extensibility and reliability. In this paper, a service oriented architecture framework is presented to compose adaptive scientific workflows. This framework has a core of service components and mechanisms that eases the interoperation between disparate workflows and programs that encapsulate the adaptive control logic\u00a0\u2026", "num_citations": "3\n", "authors": ["645"]}
{"title": "Dimension reduction for streaming data\n", "abstract": " With sensors becoming ubiquitous, there is an increasing interest in mining the data from these sensors as the data are being collected. This analysis of streaming data, or data streams, is presenting new challenges to analysis algorithms. The size of the data can be massive, especially when the sensors number in the thousands and the data are sampled at a high frequency. The data can be non-stationary, with statistics that vary over time. Real-time analysis is often required, either to avoid untoward incidents or to understand an interesting phenomenon better. These factors make the analysis of streaming data, whether from sensors or other sources, very data-and compute-intensive. One possible approach to making this analysis tractable is to identify the important data streams to focus on them. This chapter describes the different ways in which this can be done, given that what makes a stream important varies from problem to problem and can often change with time in a single problem. The following illustrate these techniques by applying them to data from a real problem and discuss the challenges faced in this emerging \ufb01eld of streaming data analysis.This chapter is organized as follows: \ufb01rst, I de\ufb01ne what is meant by streaming data and use examples from practical problems to discuss the challenges in the analysis of these data. Next, I describe the two main approaches used to handle the streaming nature of the data\u2014the sliding window approach and the forgetting factor approach. I discuss how these can be incorporated into commonly-used dimension reduction methods such as identi\ufb01cation of correlated features, random projections\u00a0\u2026", "num_citations": "3\n", "authors": ["645"]}
{"title": "A service oriented architecture for exploring high performance distributed power models\n", "abstract": " Power grids are increasingly incorporating high quality, high throughput sensor devices inside power distribution networks. These devices are driving an unprecedented increase in the volume and rate of available information. The real-time requirements for handling this data are beyond the capacity of conventional power models running in central utilities. Hence, we are exploring distributed power models deployed at the regional scale. The connection of these models for a larger geographic region is supported by a distributed system architecture. This architecture is built in a service oriented style, whereby distributed power models running on high performance clusters are exposed as services. Each service is semantically annotated and therefore can be discovered through a service catalog and composed into workflows. The overall architecture has been implemented as an integrated workflow\u00a0\u2026", "num_citations": "3\n", "authors": ["645"]}
{"title": "Data-intensive computing: A challenge for the 21st century\n", "abstract": " In our world of rapid technological change, occasionally it is instructive to contemplate how much has altered in the last few years. Remembering life without the ability to view the World Wide Web (WWW) through browser windows will be dif\ufb01cult, if not impossible, for less \u201cmature\u201d readers. Is it only seven years since YouTube \ufb01rst appeared, a Web site that is now ingrained in many facets of modern life? How did we survive without Facebook all those (actually, about \ufb01ve) years ago?In 2010, various estimates put the amount of data stored by consumers and businesses around the world in the vicinity of 13 exabytes, with a growth rate of 20 to 25 percent per annum. That is a lot of data. No wonder IBM is pursuing building a IZO-petabyte storage array. 1 Obviously there is going to be a market for such devices in the future. As data volumes of all types\u2014from video and photos to text documents and binary \ufb01les for science\u2014continue to grow in number and resolution, it is clear that we have genuinely entered the realm of data-intensive computing, or as it is often now referred to, big data. 2 Interestingly, the term \u201cdata-intensive computing\u201d was actually coined by the scienti\ufb01c community. Traditionally, scienti\ufb01c codes have been starved of suf\ufb01cient compute cycles, a paucity that has driven the creation of ever larger and faster high-performance computing machines, typically known as supercomputers. The Top 500 Web site3 shows the latest benchmark results that characterize the fastest supercomputers on the planet. While this fascination with compute performance continues, scienti\ufb01c computing has been gradually coming to terms with the challenges\u00a0\u2026", "num_citations": "3\n", "authors": ["645"]}
{"title": "Akuna-Integrated toolsets supporting advanced Subsurface flow and transport simulations for environmental management\n", "abstract": " A next generation open source subsurface simulator and user environment for environmental management is being developed through a collaborative effort across Department of Energy National Laboratories. The flow and transport simulator, Amanzi, will be capable of modeling complex subsurface environments and processes using both unstructured and adaptive meshes at very fine spatial resolutions that require supercomputing-scale resources. The user environment, Akuna, provides users with a range of tools to manage environmental and simulator data sets, create models, manage and share simulation data, and visualize results. Underlying the user interface are core toolsets that provide algorithms for sensitivity analysis, parameter estimation, and uncertainty quantification. Akuna is open-source, cross platform software that is initially being demonstrated on the Hanford BC Cribs remediation site. In this paper, we describe the emerging capabilities of Akuna and illustrate how these are being applied to the BC Cribs site.", "num_citations": "3\n", "authors": ["645"]}
{"title": "Implementing high performance remote method invocation in cca\n", "abstract": " We report our effort in engineering a high performance remote method invocation (RMI) mechanism for the Common Component Architecture (CCA). This mechanism provides a highly efficient and easy-to-use mechanism for distributed computing in CCA, enabling CCA applications to effectively leverage parallel systems to accelerate computations. This work is built on the previous work of Babel RMI. Babel is a high performance language interoperability tool that is used in CCA for scientific application writers to share, reuse, and compose applications from software components written in different programming languages. Babel provides a transparent and flexible RMI framework for distributed computing. However, the existing Babel RMI implementation is built on top of TCP and does not provide the level of performance required to distribute fine-grained tasks. We observed that the main reason the TCP based\u00a0\u2026", "num_citations": "3\n", "authors": ["645"]}
{"title": "Scalable data middleware for smart grids\n", "abstract": " Smart grids promise to improve the efficiency of power grid systems and reduce green house emissions through incorporating power generation from renewable sources and shaping demands to match the supply. Renewable sources include solar or wind. Power generation from these sources is affected by weather factors that can be highly fluctuating. To ensure these energy sources can be utilized efficiently, smart grid systems often shape demand through incentive to match the supply. As a result, the whole system becomes highly dynamic and requires constant adjusting. How to adjust the system can have a great impact on the efficiency and reliability of power grid systems, which offer many opportunities for innovation. In the previous work by us and other researchers [2, 4, 5, 7, 10, 11], we have identified and developed several applications can be used to optimize power grid operations.However, these applications rely on the precise estimation of the state of power grid systems. To enable precise estimate of power grid, enormous amount of data from millions of sensors from power grid systems must be used. Moreover, the relevant data must be delivered to applications within real time constraints. Even though millions of sensors such as phase measurement units (PMU) and smart meters are being widely deployed over the Internet, these does not exist a software system can collect, store, retrieve, and deliver these amount of data in real time.", "num_citations": "3\n", "authors": ["645"]}
{"title": "Documenting a Software Architecture\n", "abstract": " Architecture documentation is often a thorny issue in IT projects. It\u2019s common for there to be little or no documentation covering the architecture in many projects. Sometimes, if there is some, it\u2019s out-of-date, inappropriate and basically not very useful. At the other extreme there are projects that have masses of architecture related information captured in various documents and design tools. Sometimes this is invaluable, but at times it\u2019s out-of-date, inappropriate and not very useful!", "num_citations": "3\n", "authors": ["645"]}
{"title": "Integration of a text search engine with a Java messaging service\n", "abstract": " Large-scale information processing applications must rapidly search through high volume streams of structured and unstructured textual data to locate useful information. Content-based messaging systems (CBMSs) provide a powerful technology platform for building such stream handling systems. CBMSs make it possible to efficiently execute queries on messages in streams to extract those that contain content of interest. In this paper, we describe efforts to augment an experimental CBMS with the ability to perform efficient free-text search operations. The design of the CBMS platform, based upon a Java Messaging Service, is described, and an empirical evaluation is presented to demonstrate the performance implications of a range of queries varying in complexity.", "num_citations": "3\n", "authors": ["645"]}
{"title": "Designing distributed object systems with PARSE.\n", "abstract": " With the emergence of distributed object technology, such as CORBA, IBM's SOM/DSOM, Microsoft's COMIOLE, and Java, distributed object technology is becoming more commonplace. However, most object-oriented design methods do not focus on producing high performance, concurrent, distributed and verifiable object-oriented systems. The PARSE software design method provides a hierarchical, object-based approach to the development of parallel and distributed software systems. In PARSE, deadlock avoidance in the software development process can be carried out via the explicit use of design heuristics involving client-server modelling. A graphical design notation known as process graphs is firstly used to capture the structural and dynamic properties of the required distributed software system. Then, the process graph designs can be analyzed and transformed into clientserver behaviour graphs, for the purpose of design verification. This paper briefly presents the PARSE design notation. Next, the design and construction of a cooperative graph editor utilizing the PARSE methodology will be presented. Finally, experience of an implementation involving the Common Object Request Broker Architecture (CORBA) will be discussed. an cooperative work environments, distributed databases, and distributed real-time control applications are emerging. However, most object-oriented design techniques are somewhat inadequate for distributed system design (2), despite the fact that the distributed software engineer faces various complex issues relating to the partitioning and distribution of software components. Although middleware\u00a0\u2026", "num_citations": "3\n", "authors": ["645"]}
{"title": "Modelling dynamic distributed system structures in PARSE\n", "abstract": " With the increasing availability of parallel and networked computers, applications based on distributed systems are gradually becoming commonplace. It is therefore important for software professionals to practice sound parallel software engineering methods. PARSE (Parallel Software Engineering) is an object-based software engineering methodology to facilitate the design of reliable and reusable parallel systems. Systems designed using PARSE are represented using a graphical notation known as process graphs, which enable process structures and their precise interactions to be hierarchically constructed. However, the current PARSE graphical notation cannot adequately handle the design of concurrent systems which incorporate dynamic features. This paper describes extensions to the present PARSE process graph notations to support the design of well-engineered distributed systems. Example designs\u00a0\u2026", "num_citations": "3\n", "authors": ["645"]}
{"title": "Designing distributed multimedia systems using PARSE\n", "abstract": " With recent vast improvements in computer hardware, in particular, the processing capacity of multimedia database servers, and high performance of networks, distributed multimedia applications are becoming a reality. This paper presents an object-based approach to the design of distributed multimedia software. In particular, the PARSE methodology for designing parallel and distributed systems is employed. Justification of the object-based approach is given, and an overview of the PARSE process graph notation is presented. A case-study of a video-on-demand application is then presented, and a mapping from the design to an implementation based on Windows NT is described.", "num_citations": "3\n", "authors": ["645"]}
{"title": "HTPNET: a high performance transport protocol\n", "abstract": " The quantum increase in transmission speed of optical fibre networks has created a bottleneck in the transport protocol processing at host systems. In this paper, we present a high performance transport protocol system, HTPNET, that is designed to overcome this protocol processing bottleneck. HTPNET is based on a highly parallel architecture and is designed to exploit the evolving characteristics of high-speed networks. HTPNET uses an out-of-band signalling system based upon transmitter-paced periodic exchanges of state information between end systems. This mechanism exhibits several attractive properties which have been demonstrated to perform efficiently in a simulated high-speed environment with high bandwidth-delay-product. A prototype implementation of HTPNET has been constructed from a network of T800 transputers. The results obtained from this implementation are presented: these\u00a0\u2026", "num_citations": "3\n", "authors": ["645"]}
{"title": "Evaluating major controls on basinal stratigraphy, Pine Valley, Nevada: Implications for syntectonic deposition: Discussion and reply\n", "abstract": " Basin filling is generally controlled by subsidence, uplift and geology of source region, intrabasinal structure, and climate. Because these factors combine to act on sedimentation, it is very difficult to discriminate the effect and relative magnitude of individual factors. Recently Gordon and Heller (1993) suggested that subsidence has exerted a predominant role on lithofacies distribution in the Pine Valley Basin, and that there is a strong inverse relationship between the subsidence rate and the distance of gravel transport into the basin:(1) Alluvial gravels fed from the hanging wall\" have prograded far out toward the basin axis\"(p. 54), whereas gravels from the footwall\" are severely limited in extent\"(p. 54).(2) Uneven subsidence along the basin axis has resulted in differential progradation of footwall gravels. Gravels over the basement ridge (slow subsidence) have prograded farther than those over subbasin (rapid\u00a0\u2026", "num_citations": "3\n", "authors": ["645"]}
{"title": "Scalability and Cost Evaluation of Incremental Data Processing Using Amazon's Hadoop Service.\n", "abstract": " Based on the MapReduce model and Hadoop Distributed File System (HDFS), Hadoop enables the distributed processing of large data sets across clusters with scalability and fault tolerance. Many data-intensive applications involve continuous and incremental updates of data. Understanding the scalability and cost of a Hadoop platform to handle small and independent updates of data sets sheds light on the design of scalable and cost-effective data-intensive applications. In this chapter, we introduce a motivating movie recommendation application implemented in the MapReduce model and deployed on Amazon Elastic MapReduce (EMR), a Hadoop", "num_citations": "2\n", "authors": ["645"]}
{"title": "Quality attribute-guided evaluation of NoSQL databases: An experience report\n", "abstract": " NoSQL is a family of database technologies that promises to provide unprecedented levels of performance, scalability and availability by simplifying data models and supporting horizontal scaling and data replication. Each NoSQL product embodies a particular set of consistency, availability and partition-tolerance CAP tradeoffs, along with a data model that reduces the conceptual mismatch between data access and data storage models. For software developers, the choice of a NoSQL technology imposes a specific distributed software architecture and data model, making the technology selection one that is difficult to defer in a software project. This means technology selection must be done early, often with limited information about specific application requirements, and the decision must balance speed with precision as the NoSQL solution space is large and evolving rapidly. In this paper we present the method and results of a study we performed to compare the characteristics of 3 NoSQL databases for use in healthcare. We describe the aims of the study, the experimental method, and the outcomes of both quantitative and qualitative comparisons of MongoDB, Cassandra and Riak. We conclude by reflecting on some of the fundamental difficulties of performing detailed technical evaluations of NoSQL databases specifically, and big data systems in general, that have become apparent during our study.Descriptors:", "num_citations": "2\n", "authors": ["645"]}
{"title": "Modeling uncertainty for middleware-based streaming power grid applications\n", "abstract": " The power grid is incorporating high throughput sensor devices into power distribution networks. The future power grid needs to guarantee accuracy and responsiveness of applications that consume data from multiple sensor streams. The end-to-end performance and overall scalability of cyber-physical energy applications depend on the middleware's ability to handle multi-source sensor data, which exhibits uncertain behavior under highly variable numbers of sensors and middleware topologies. In this paper, we present a parametric approach to model middleware uncertainty and to analyze its effect on distributed power applications. The models encapsulate the entire data flow paths from sensor devices, through network and middleware components to the power application nodes that utilize sensor data streams. Using the Ptolemy II framework for modeling and simulation, we generate Monte Carlo samples of\u00a0\u2026", "num_citations": "2\n", "authors": ["645"]}
{"title": "Cyber dumpster diving: Creating new software systems for less\n", "abstract": " This is the first article we're fortunate to have from the SATURN 2012 conference. This issue highlights a compelling story of crisis, larceny, and, of course, Fortran-the first programming language I learned, which I did by teaching it to undergrad engineering students. Because I never saw those engineers after they left my class, it's heartening to see that others like them learned some good lessons and developed useful insights. I'm sure you will enjoy reading this architectural tale.", "num_citations": "2\n", "authors": ["645"]}
{"title": "Cloudflow introduction\n", "abstract": " Cloud computing is gaining tremendous momentum in both academia and industry, more and more people are migrating their data and applications into the Cloud. We have observed wide adoption of the MapReduce computing model and the open source Hadoop system for large scale distributed data processing, and a variety of ad hoc mashup techniques that weave together Web applications. However, these are just first steps towards managing complex task and data dependencies in the Cloud, as there are more challenging issues such as large parameter space exploration, data partitioning and distribution, scheduling and optimization, smart reruns, and provenance tracking associated with workflow execution.Cloud needs structured and mature workflow technologies to handle such issues, and vice versa, as Cloud offers unprecedented scalability to workflow systems, and could potentially change the way we perceive and conduct research and experiments. The scale and complexity of the science and data analytics problems that can be handled can be greatly increased on the Cloud, and the on-demand nature of resource allocation on the Cloud will also help improve resource utilization and user experience.", "num_citations": "2\n", "authors": ["645"]}
{"title": "Scientific workflows composition and deployment on SOA frameworks\n", "abstract": " Scientific workflows normally consist of multiple applications acquiring and transforming data, running data intensive analyses and visualizing the results for scientific discovery. In this paper, we present our application of Service-Oriented Architecture (SOA) to compose and deploy systems biology workflows. In developing this application, our solution uses MeDICi a middleware framework built on SOA platforms as an integration layer. We discuss our experience and lessons learnt about this solution that are generally applicable to scientific workflows in other domains.", "num_citations": "2\n", "authors": ["645"]}
{"title": "Applications in data-intensive computing\n", "abstract": " The total quantity of digital information in the world is growing at an alarming rate. Scientists and engineers are contributing heavily to this data \u201ctsunami\u201d by gathering data using computing and instrumentation at incredible rates. As data volumes and complexity grow, it is increasingly arduous to extract valuable information from the data and derive knowledge from that data. Addressing these demands of ever-growing data volumes and complexity requires game-changing advances in software, hardware, and algorithms. Solution technologies also must scale to handle the increased data collection and processing rates and simultaneously accelerate timely and effective analysis results. This need for ever faster data processing and manipulation as well as algorithms that scale to high-volume data sets have given birth to a new paradigm or discipline known as \u201cdata-intensive computing.\u201d In this chapter, we define\u00a0\u2026", "num_citations": "2\n", "authors": ["645"]}
{"title": "An architecture for real time data acquisition and online signal processing for high throughput tandem mass spectrometry\n", "abstract": " Independent, greedy collection of data events using simple heuristics results in massive over-sampling of the prominent data features in large-scale studies over what should be achievable through \u00bfintelligent\", online acquisition of such data. As a result, data generated are more aptly described as a collection of a large number of small experiments rather than a true large-scale experiment. Nevertheless, achieving \u00bfintelligent\u00bf, online control requires tight interplay between state-of-the-art, data-intensive computing infrastructure developments and analytical algorithms. In this paper, we propose a Software Architecture for Mass spectrometry-based Proteomics coupled with Liquid chromatography Experiments (SAMPLE) to develop an \u00bfintelligent\u00bf online control and analysis system to significantly enhance the information content from each sensor (in this case, a mass spectrometer). Using online analysis of data\u00a0\u2026", "num_citations": "2\n", "authors": ["645"]}
{"title": "Design and implementation of a high\u2010performance CCA event service\n", "abstract": " Event services based on publish\u2013subscribe architectures are well\u2010established components of distributed computing applications. Recently, an event service has been proposed as part of the common component architecture (CCA) for high\u2010performance computing (HPC) applications. In this paper we describe our implementation, experimental evaluation, and initial experience with a high\u2010performance CCA event service that exploits efficient communications mechanisms commonly used on HPC platforms. We describe the CCA event service model and briefly discuss the possible implementation strategies of the model. We then present the design and implementation of the event service using the aggregate remote memory copy interface as an underlying communication layer for this mechanism. Two alternative implementations are presented and evaluated on a Cray XD\u20101 platform. The performance results\u00a0\u2026", "num_citations": "2\n", "authors": ["645"]}
{"title": "It Takes glue to Tango: MeDICi integration framework creates data-intensive computing pipeline\n", "abstract": " Biologists increasingly rely on high-performance computing (HPC) platforms to rapidly process the tsunami of data generated by high throughput genome and metagenome sequencing technology and high-throughput proteomics. Unfortunately, the platforms that produce the massive data sets rarely work smoothly with the interactive analysis and visualization programs used in bioinformatics. This makes it difficult for researchers to exploit the computational power of HPC platforms to speed scientific discovery. At the Department of Energy\u2019s Pacific Northwest National Laboratory in Richland, Wash., researchers are creating computing environments for biologists that seamlessly integrate collections of data and computational resources. These advantages enable users to rapidly analyze high-throughput data. A major goal is to shield the biologist from the complexity of interacting with multiple dissimilar databases and running tasks on HPC platforms and computational clusters. One of those environments the MeDICi Integration Framework is now available for free download. Short for Middleware for Data-Intensive Computing, MeDICi makes it easy to integrate separate codes into complex applications that operate as a data analysis pipeline.", "num_citations": "2\n", "authors": ["645"]}
{"title": "Research initiatives for plug-and-play scientific computing\n", "abstract": " This paper introduces three component technology initiatives within the SciDAC Center for Technology for Advanced Scientific Component Software (TASCS) that address ever-increasing productivity challenges in creating, managing, and applying simulation software to scientific discovery. By leveraging the Common Component Architecture (CCA), a new component standard for high-performance scientific computing, these initiatives tackle difficulties at different but related levels in the development of component-based scientific software:(1) deploying applications on massively parallel and heterogeneous architectures,(2) investigating new approaches to the runtime enforcement of behavioral semantics, and (3) developing tools to facilitate dynamic composition, substitution, and reconfiguration of component implementations and parameters, so that application scientists can explore tradeoffs among factors such\u00a0\u2026", "num_citations": "2\n", "authors": ["645"]}
{"title": "A configurable event correlation architecture for adaptive J2EE applications\n", "abstract": " Distributed applications that adapt as their environment changes are developed from self-managing, self-configuring and self-optimising behaviours. This requires constant monitoring of the state of the environment, and analysing multiple sources of events. Event correlation is the process of correlating monitored events from multiple sources for further analysis. It is essential that event correlation supports reliable event management with minimal delay. This paper describes the design and implementation of an event correlation architecture for adaptive J2EE applications. The architecture supports flexible configuration of event correlation in terms of the reliability and performance. This is especially useful in situations when multiple sources of events have different level of requirements for reliability and performance. We evaluate the performance overhead of this event correlation architecture and demonstrate its\u00a0\u2026", "num_citations": "2\n", "authors": ["645"]}
{"title": "Dimensions of Usability: Cougaar, Aglets, and Adaptive Agent Architecture (AAA)\n", "abstract": " Research and development organizations are constantly evaluating new technologies in order to implement the next generation of advanced applications. At the Pacific Northwest National Laboratory, agent technologies are perceived as an approach that can provide a competitive advantage in the construction of highly sophisticated software systems in a range of application areas. An important factor in selecting a successful agent architecture is the level of support it provides the developer in respect to developer support, examples of use, integration into current workflow and community support. Without such assistance, the developer must invest more effort into learning instead of applying the technology. Like many other applied research organizations, our staff are not dedicated to a single project and must acquire new skills as required, underlining the importance of being able to quickly become proficient. A project was instigated to evaluate three candidate agent toolkits across the dimensions of support they provide. This paper reports on the outcomes of this evaluation and provides insights into the agent technologies evaluated.", "num_citations": "2\n", "authors": ["645"]}
{"title": "A distributed multi-user role-based model integration framework\n", "abstract": " Integrated computational modelling can be very useful in making quick, yet informed decisions related to environmental issues including Brownfield assessments. Unfortunately, the process of creating meaningful information using this methodology is fraught with difficulties, particularly when multiple computational models are required. Common problems include the inability to seamlessly transfer information between models, the difficulty of incorporating new models and integrating heterogeneous data sources, executing large numbers of model runs in a reasonable time frame, and adequately capturing pedigree information that describes the specific computational steps and data required to reproduce results. While current model integration frameworks have successfully addressed some of these problems, none have addressed", "num_citations": "2\n", "authors": ["645"]}
{"title": "A methodology for predicting the performance of component based systems\n", "abstract": " The poster describes an evolving methodology to predict the performance of component-based systems. The methodology describes the steps to understand and model various architectural properties and their effects on performance. It aims to help designers gain deep insights into the component and infrastructure behavior and leverage this knowledge in their designs.", "num_citations": "2\n", "authors": ["645"]}
{"title": "Evaluating Enterprise Java Bean Technology\n", "abstract": " This paper describe CSIRO's Middleware Technology Evaluation (MTE) project, and the approach being taken to investigating the various features and behavior of EJB technologies. As an example of the work in the MTE project, the architecture and main features of BEA's WebLogic Server product are presented, and results obtained from experimenting with the product are analyzed.", "num_citations": "2\n", "authors": ["645"]}
{"title": "High-Performance Services A La Carte\n", "abstract": " Our research group has designed and implemented a programming environment that can be used to test some of the ideas and goals mentioned above. The system is based on Alwan (http://www. ifi. unibas. ch/PP/alwan), a coordination language for the development of parallel-program skeletons at a high level of abstraction (as compared with messagepassing libraries). In addition to the wellknown sequential-procedure concept, Alwan offers a parallel-procedure concept, the Topology construct. Topologies process different local data in an SPMD (single-program, multiple-data) or SPDD (single-program, distributeddata) manner. The language comprises constructs for parallelism-oriented tasks such as synchronization, data distribution, and parallel I/O. Sequential-calculation routines written in different languages (currently, C and Fortran 77) can be interfaced to Alwan. As a result, Alwan program control switches\u00a0\u2026", "num_citations": "2\n", "authors": ["645"]}
{"title": "Software engineering techniques and tools for high performance parallel systems\n", "abstract": " This paper considers the current state of software engineering for parallel systems. A review of existing approaches and techniques identifies inadequacies. Recent work on design, verification and automated support is outlined. The next generation of embedded and distributed technologies will compound the problems through increased demand and diversity. This paper discusses the implications for the progression of current techniques into new methods for future software engineering of parallel systems.< >", "num_citations": "2\n", "authors": ["645"]}
{"title": "Putting software development on the information highway: going beyond email\n", "abstract": " Hawryszkiewycz, I; GORTON, I; Fing, L. Putting software development on the information highway: going beyond email. American Programmer. 1995; 8 (8): 8-14.< a href=\" http://hdl. handle. net/102.100. 100/231793? index= 1\" target=\" _blank\"> http://hdl. handle. net/102.100. 100/231793? index= 1", "num_citations": "2\n", "authors": ["645"]}
{"title": "Design Issues on Error Control Mechanisms for High-speed Networks\n", "abstract": " Audiovisual material available from this site has been copied and communicated to you under a Screenrights licence pursuant to Section 113P of the Copyright Act 1968 solely for the educational purposes of your institution. No other use is authorised. For more information please contact Screenrights at [email protected] or at www. screenrights. org.", "num_citations": "2\n", "authors": ["645"]}
{"title": "Software Architecture for Big Data Systems\n", "abstract": " I've written a book in 2006, Essential Software Architecture, published by Springer-Verlag. It sold well and has had several excellent reviews in Dr Dobbs and ACM's QUEUE Magazine. A 2nd Edition was published in 2011. I also co-edited'Data Intensive Systems' which was published by Cambridge University Press in 2012. I've also published 34 refereed journal and 100 refereed international conference and workshop papers, with an h-index of 28.", "num_citations": "1\n", "authors": ["645"]}
{"title": "ADVANCED SIMULATION CAPABILITY FOR ENVIRONMENTAL MANAGEMENT-CURRENT STATUS AND PHASE II DEMONSTRATION RESULTS\n", "abstract": " The US Department of Energy (USDOE) Office of Environmental Management (EM), Office of Soil and Groundwater, is supporting development of the Advanced Simulation Capability for Environmental Management (ASCEM). ASCEM is a state-of-the-art scientific tool and approach for understanding and predicting contaminant fate and transport in natural and engineered systems. The modular and open source high-performance computing tool facilitates integrated approaches to modeling and site characterization that enable robust and standardized assessments of performance and risk for EM cleanup and closure activities. The ASCEM project continues to make significant progress in development of computer software capabilities with an emphasis on integration of capabilities in FY12. Capability development is occurring for both the Platform and Integrated Toolsets and High-Performance Computing (HPC) Multiprocess Simulator. The Platform capabilities provide the user interface and tools for end-to-end model development, starting with definition of the conceptual model, management of data for model input, model calibration and uncertainty analysis, and processing of model output, including visualization. The HPC capabilities target increased functionality of process model representations, toolsets for interaction with Platform, and verification and model confidence testing. The Platform and HPC capabilities are being tested and evaluated for EM applications in a set of demonstrations as part more\u00bb", "num_citations": "1\n", "authors": ["645"]}
{"title": "Implementations of a Flexible Framework for Managing Geologic Sequestration Modeling Projects\n", "abstract": " Numerical simulation is a standard practice used to support designing, operating, and monitoring CO2 injection projects. Although a variety of computational tools have been developed that support the numerical simulation process, many are single-purpose or platform specific and have a prescribed workflow that may or may not be suitable for a particular project. We are developing an open-source, flexible framework named Velo that provides a knowledge management infrastructure and tools to support modeling and simulation for various types of projects in a number of scientific domains. The Geologic Sequestration Software Suite (GS3) is a version of this framework with features and tools specifically tailored for geologic sequestration studies. Because of its general nature, GS3 is being employed in a variety of ways on projects with differing goals. GS3 is being used to support the Sim-SEQ international model\u00a0\u2026", "num_citations": "1\n", "authors": ["645"]}
{"title": "Amanzi and Akuna: Two new community codes for subsurface contaminant flow and transport\n", "abstract": " The Advanced Simulation Capability for Environmental Management (ASCEM) program is developing a modular and extensible open-source set of tools for understanding the fate and transport of contaminants in natural and engineered systems. These tools not only support a fundamental shift toward standardized assessments of performance and risk for the Department of Energy Office of Environmental Management (DOE-EM) cleanup and closure decisions, but establish a modern high-quality code base for a growing interdisciplinary community. Specifically, ASCEM is leveraging advances and expertise from applied mathematics, computer and computational sciences, and the geosciences, in this new development. A toolset named Akuna will provide capabilities for data management, visualization, conceptual model development, uncertainty quantification, parameter estimation, risk analysis, and decision\u00a0\u2026", "num_citations": "1\n", "authors": ["645"]}
{"title": "Designing a Distributed Systems Architecture Testbed for Real-Time Power Grid Systems\n", "abstract": " Power engineers who are striving to improve real-time attribute of power grid applications are ill equipped with software engineering methods and tools that allow them to rigorously evaluate their designs, taken into account data communication, geographic locations, and high performance computing capacity. This paper presents a technical approach to designing a testbed for embedding real-time monitoring and computation functionalities into the power grid system. The approach focuses on integrating the parallel computational models with the data management infrastructure for near-real time power grid state estimation. We study and summarize various forces and requirements that drive the design decisions in the distributed systems architecture. Given the continental scale of the power grid, it is important for the testbed to be extensible and scalable within a complex topology of physical entities, controlled by an overlaid network of power utilities and regulatory balancing authorities. This paper outlines the technical steps, and software toolkits to develop this testbed.", "num_citations": "1\n", "authors": ["645"]}
{"title": "Towards Composing Data Aware Systems Biology Workflows on Cloud Platforms: A MeDICi-Based Approach\n", "abstract": " Cloud computing is being increasingly adopted for deploying systems biology scientific workflows. Scientists developing these workflows use a wide variety of fragmented and competing data sets and computational tools of all scales to support their research. To this end, the synergy of client side workflow tools with cloud platforms is a promising approach to share and reuse data and workflows. In such systems, the location of data and computation is essential consideration in terms of quality of service for composing a scientific workflow across remote cloud platforms. In this paper, we describe a cloud-based workflow for genome annotation processing that is underpinned by MeDICi -- a middleware designed for data intensive scientific applications. The workflow implementation incorporates an execution layer for exploiting data locality that routes the workflow requests to the processing steps that are colocated with\u00a0\u2026", "num_citations": "1\n", "authors": ["645"]}
{"title": "An Adaptive Middleware Framework for Optimal Scheduling on Large Scale Compute Clusters\n", "abstract": " In production multi-user high-performance (HPC) batch computing environments, wait times for scheduled jobs are highly dynamic. For scientific users, the primary measure of efficiency is wall clock time-to-solution. In high throughput applications, such as many kinds of biological analysis, the computational work to be done can be flexibly scheduled taking a longer time on a small number of processors or a shorter time on a large number of processors. Therefore the capability to choose a platform at run-time based on both processing capabilities and availability (lowest wait time) would be attractive. The goal of our work was to create an adaptive interface to HPC systems that dynamically reschedules high-throughput calculations in response to fluctuating load, optimizing for time-to-solution. This was done by implementing middleware functionality to (1) monitor the resource load on a given compute cluster, (2\u00a0\u2026", "num_citations": "1\n", "authors": ["645"]}
{"title": "Advanced Simulation Capability for Environmental Management\n", "abstract": " Advanced Simulation Capability for Environmental Management (ASCEM) is a state-of-the-art scientific tool and approach for understanding and predicting contaminant fate and transport in natural and engineered systems. ASCEM\u2019s modular and open source, high-performance computing toolsets will facilitate integrated approaches to modeling and site characterization that enable robust and standardized assessments of performance and risk for US Department of Energy (DOE) Environmental Management (EM) cleanup and closure activities. Use of ASCEM will help EM better estimate cleanup time and costs and reduce uncertainties and risks. We highlight the results of a recent set of simulations using both an unstructured approach and a structured mesh approach that have been incorporated into a new high-performance computing, multiprocess simulator, AMANZI. The calculations modeled variably saturated flow, advection of nonreactive species, and reactive transport of 17 different chemical species. Data management, visualization, and uncertainty quantification capabilities that are part of an integrated platform called AKUNA were also developed to analyze the simulation results. These calculations were performed in a demonstration at the DOE EM Savannah River Site F-Area.", "num_citations": "1\n", "authors": ["645"]}
{"title": "An Introduction to Middleware Architectures and Technologies\n", "abstract": " I\u2019m not really a great enthusiast for drawing strong analogies between the role of a software architect and that of a traditional building architect. There are similarities, but also lots of profound differences. But let\u2019s ignore those differences for a second, in order to illustrate the role of middleware in software architecture.", "num_citations": "1\n", "authors": ["645"]}
{"title": "Advanced Simulation Capability for Environmental Management (ASCEM)\n", "abstract": " The United States Department Energy (DOE) Office of Environmental Management (EM) determined that uniform application of advanced modeling in the subsurface could help reduce the cost and risks associated with its environmental cleanup mission. In response to this determination, the EM Office of Technology Innovation and Development (OTID), Groundwater and Soil Remediation (GW&S) began the program Advanced Simulation Capability for Environmental Management (ASCEM). ASCEM is a state-of-the-art scientific tool and approach for integrating data and scientific understanding to enable prediction of contaminant fate and transport in natural and engineered systems. This initiative supports the reduction of uncertainties and risks associated with EM\u2019s environmental cleanup and closure programs through better understanding and quantifying the subsurface flow and contaminant transport behavior in\u00a0\u2026", "num_citations": "1\n", "authors": ["645"]}
{"title": "Geologic Sequestration Software Suite (GS3): A collaborative approach to the management of geological GHG storage projects\n", "abstract": " Geologic storage projects associated with large anthropogenic sources of greenhouse gases (GHG) will have lifecycles that may easily span a century, involve several numerical simulation cycles, and have distinct modeling teams. The process used for numerical simulation of the fate of GHG in the subsurface follows a generally consistent sequence of steps that often are replicated by scientists and engineers around the world. Site data is gathered, assembled, interpreted, and assimilated into conceptualizations of a solid-earth model; assumptions are made about the processes to be modeled; a computational domain is specified and spatially discretized; driving forces and initial conditions are defined; the conceptual models, computational domain, and driving forces are translated into input files; simulations are executed; and results are analyzed. Then, during and after the GHG injection, a continuous monitoring\u00a0\u2026", "num_citations": "1\n", "authors": ["645"]}
{"title": "Engineering high performance service-oriented pipeline applications with MeDICi\n", "abstract": " The pipeline software architecture pattern is commonly used in many application domains to structure a software system. A pipeline comprises a sequence of processing steps that progressively transform data to some desired outputs. As pipeline-based systems are required to handle increasingly large volumes of data and provide high throughput services, simple scripting-based technologies that have traditionally been used for constructing pipelines do not scale. In this paper we describe the MeDICI Integration Framework (MIF), which is specifically designed for building flexible, efficient and scalable pipelines that exploit distributed services as elements of the pipeline. We explain the core runtime and development infrastructures that MIF provides, and demonstrate how MIF has been used in two complex applications to improve performance and modifiability.", "num_citations": "1\n", "authors": ["645"]}
{"title": "A New Initiative for Developing Advanced Simulation Capabilities for Environmental Management (ASCEM)-10470\n", "abstract": " The United States Department of Energy (DOE), Office of Environmental Management (EM), in collaboration with other DOE offices, is leading a multi-institution, multidisciplinary team of geoscientists, material scientists, and computational scientists from Los Alamos, Lawrence Berkeley, Pacific Northwest, Oak Ridge, and Savannah River National Laboratories to launch a new modeling initiative for Advanced Simulation Capability for Environmental Management (ASCEM). ASCEM is a-state-of-the-art scientific tool and approach for understanding and predicting contaminant fate and transport in natural and engineered systems. This modular and open source high performance computing tool will facilitate integrated approaches to modeling and site characterization that enable robust and standardized assessments of performance and risk for EM cleanup and closure activities. The ASCEM program is aimed at addressing critical EM program needs to better understand and quantify the subsurface flow and contaminant transport behavior in complex geological systems and the long-term performance of engineered components including cementitous materials in nuclear waste disposal facilities, in order to reduce uncertainties and risks associated with DOE EM\u2019s environmental cleanup and closure programs. Building upon national capabilities developed from decades of R&D in subsurface geosciences, modeling and simulation, and environmental remediation, the ASCEM initiative will develop an integrated, highperformance, open-source computer modeling system for multiphase, multicomponent, multiscale subsurface flow and contaminant\u00a0\u2026", "num_citations": "1\n", "authors": ["645"]}
{"title": "Software Architecture\n", "abstract": " Welcome to the European Conference on Software Architecture (ECSA), which is the premier European software engineering conference. ECSA provides researchers and practitioners with a platform to present and discuss the most recent, innovative, and significant findings and experiences in the field of software architecture research and practice. The fourth edition of ECSA was built upon a history of a successful series of European workshops on software architecture held from 2004 through 2006 and a series of European software architecture conferences from 2007 through 2009. The last ECSA was merged with the 8th Working IEEE/IFIP Conference on Software Architecture (WICSA).Apart from the traditional technical program consisting of keynote talks, a main research track, and a poster session, the scope of the ECSA 2010 was broadened to incorporate other tracks such as an industry track, doctoral\u00a0\u2026", "num_citations": "1\n", "authors": ["645"]}
{"title": "XML does real programmers a service\n", "abstract": " Having ingeniously morphed their external personae to become almost mainstream, the future looks bright for real programmers.", "num_citations": "1\n", "authors": ["645"]}
{"title": "Evaluating Enterprise Integration Middleware Technologies\n", "abstract": " Summary form only given. Architects are faced with the problem of building enterprise scale information systems, with streamlined, automated internal business processes and Web-enabled business functions, all across multiple legacy applications. The underlying architectures such systems are embodied in a range of diverse products known as 'enterprise integration' technologies. In this tutorial, we highlight some of the major problems, approaches and issues in designing enterprise integration architectures and selecting appropriate supporting technology. An architect's perspective on designing large-scale integrated applications is taken, and we discuss requirements elicitation, architecture patterns, enterprise integration technology features and risk mitigation. Specific technology platforms covered include messaging, message brokers, application servers, business process management and Web services and\u00a0\u2026", "num_citations": "1\n", "authors": ["645"]}
{"title": "A guide to middleware architectures and technologies\n", "abstract": " I\u2019m not really a great enthusiast for drawing strong analogies between the role of a software architect and that of a traditional building architect. There are similarities, but also lots of profound differences. 14 But let\u2019s ignore those differences for a second, in order to illustrate the role of middleware in software architecture.When an architect designs a building, they create drawings, essentially a design that shows, from various angles, the structure and geometric properties of the building. This design is based on the building\u2019s requirements, such as the available space, function (office, church, shopping center, home), desired aesthetic and functional qualities and budget. These drawings are an abstract representation of the intended concrete (sic) artifact. There\u2019s obviously an awful lot of design effort still required to turn the architectural drawings into something that people can actually start to build. There\u2019s detailed\u00a0\u2026", "num_citations": "1\n", "authors": ["645"]}
{"title": "Model driven capacity planning: methods and tools\n", "abstract": " Capacity planning is the process of predicting when future load levels will saturate the system and of determining the most cost-effective way of delaying system saturation as much as possible [1].Performance, as one of the most important metrics for capacity planning, is a critical factor for assessing the quality of a system for a particular purpose. The success of many enterprise applications relies upon scalable, high performance infrastructures to handle business transactions and provide access to core back-end systems. While J2EE provides such an infrastructure solution for enterprise applications, it does not help in understanding the resulting performance of an application before it has been built and tested. Apart from the underlying hardware and software environment, the performance of a deployed J2EE application depends on a combination of the following basic factors [4]:\u2022 the behavior of its application\u00a0\u2026", "num_citations": "1\n", "authors": ["645"]}
{"title": "Engineering Distributed Object Systems: A Case of D\u00e9j\u00e0 Vu?\n", "abstract": " Recent advances in network technologies and the performance of workstations has led to the widespread use of distributed systems in industry and commerce. Reduced costs of the components have accelerated the introduction of large scale network systems. This has been matched by increased user demands for advanced information access and handling. In manufacturing applications, integration between control and information systems is becoming possible; financial and retail institutions are turning to distributed systems to support sophisticated information storage and analysis tasks, and the advent of multi-media technologies has increased uptake of distributed systems for commercial, leisure and educational purposes. Access to internet services is now accepted as a necessary facility for the many aspects of modern living. Distributed systems provide the foundation for a wide range of applications, and it is clear that the demand for better, faster and smarter systems will continue from a growing body of individual and corporate users. New network technologies will emerge to meet these market demands [1]. The range of platforms and propriety software involved in many distributed systems is considerable. In order to facilitate the development of these heterogeneous systems, there has been a rapid uptake of distributed object technology. This provides for easy interoperability between different applications and is thus is a key feature in the construction of advanced distributed systems. Its importance can be judged by the speed with which system developers have embraced this form of middleware support. However the rapid expansion in\u00a0\u2026", "num_citations": "1\n", "authors": ["645"]}
{"title": "Platforms for cooperative software development\n", "abstract": " CSIRO Research Publications Repository - Platforms for cooperative software development Skip to Main Content CSIRO Home Research Publications Repository Contact Us Help CSIRO Login Username Password Search Publications Advanced Search Browse Publications Selected Records () Platforms for cooperative software development Description Select | OK Print Author: Hawryszkiewycz, I; GORTON, I Date of Publication: 1996 Publication Type: Journal Article Journal Title: American programmer Volume: August 1996 Pages: 12-17 Keywords: Mathematical and Information Sciences Identifier: procite:3bdfa22f-077e-42f2-8b73-ef75888ab14d Attribution Statement: Hawryszkiewycz, I; GORTON, I. Platforms for cooperative software development. American programmer. 1996; August 1996:12-17. http://hdl.handle.net/102.100.100/229491?index=1 PermaLink http://hdl.handle.net/102.100./229491?index=1 \u2026", "num_citations": "1\n", "authors": ["645"]}
{"title": "Real programmers do use Delphi\n", "abstract": " The author describes a career-journey that began with the maintenance of obsolete, poorly structured programs that came with almost no documentation. But he survived and moved on to work with and observe \"real\" programming practices in several companies. He describes how the \"real\" programming profession evolved and expanded into the mainstream of the software industry. E. Post's article \"Real Programmers Don't Use Pascal\" (Datamation, 1983) sent the overriding message that, despite the efforts of that day's quiche-eating programmers, real programming took real talent and, if done correctly, led to fun, wealth, and job security. Borland recently released a new, object-oriented version of Pascal called Delphi. It's far removed from the simplistic language Niklaus Wirth conceived; it's a pleasure to use, and it commands good contracting rates.< >", "num_citations": "1\n", "authors": ["645"]}
{"title": "HTPNET: A New Transport Protocol for High-speed Networks\n", "abstract": " The quantum increase in transmission speed of optical fibre networks has created a bottleneck in transport protocol processing at host systems. In this paper we present a new transport protocol system, HTPNET, that is designed to overcome this protocol processing bottleneck. HTPNET is based on a highly parallel architecture and is designed to exploit the evolving characteristics of high-speed networks. HTPNET uses an out-of-band signalling system based upon transmitter-paced periodic exchanges of state information between end systems. This mechanism exhibits several attractive properties which have been demonstrated to perform efficiently in a high-speed environment with high-bandwidth-delay-product. A prototype implementation of HTPNET has been constructed from a network of T800 transputers. The results obtained from this implementation are presented: these demonstrate the advantages of exploiting a parallel architecture for protocol processing.", "num_citations": "1\n", "authors": ["645"]}
{"title": "HTPNET: a high-speed transport protocol system for networks\n", "abstract": " This paper presents a new transport protocol system, HTPNET, that is designed to overcome the protocol processing bottleneck in high-speed networks. HTPNET is based on a highly parallel architecture and is designed to exploit the evolving characteristics of high-speed networks. A prototype implementation of HTPNET has been constructed from a network of T800 transputers. The results obtained from this implementation are presented: these demonstrate the advantages of exploiting a parallel architecture for protocol processing.", "num_citations": "1\n", "authors": ["645"]}