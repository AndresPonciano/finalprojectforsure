{"title": "Detecting fault modules applying feature selection to classifiers\n", "abstract": " At present, automated data collection tools allow us to collect large amounts of information, not without associated problems. This paper, we apply feature selection to several software engineering databases selecting attributes with the final aim that project managers can have a better global vision of the data they manage. In this paper, we make use of attribute selection techniques in different datasets publicly available (PROMISE repository), and different data mining algorithms for classification to defect faulty modules. The results show that in general, smaller datasets with less attributes maintain or improve the prediction capability with less attributes than the original datasets.", "num_citations": "88\n", "authors": ["571"]}
{"title": "Comparaci\u00f3n de diferentes algoritmos de clustering en la estimaci\u00f3n de coste en el desarrollo de software\n", "abstract": " Los modelos de estimaci\u00f3n de coste software que obtienen una \u00fanica relaci\u00f3n matem\u00e1tica entre el esfuerzo y alg\u00fan otro atributo caracter\u00edstico de los proyectos software, proporcionan buenos resultados cuando la base de datos de proyectos, a partir de la que mediante m\u00e9todos de regresi\u00f3n se obtiene la relaci\u00f3n anteriormente mencionada, est\u00e1 formada por proyectos homog\u00e9neos. Sin embargo, para bases de datos de proyectos procedentes de muy diversas fuentes, tales como la base de proyectos de ISBSG formada por miles de proyectos heterog\u00e9neos, el utilizar una \u00fanica relaci\u00f3n matem\u00e1tica para representar a todos estos proyectos, no ofrece tan buenos resultados como si los proyectos fuesen homog\u00e9neos. En este trabajo se plantea, como mejora del proceso de estimaci\u00f3n, segmentar la base de datos ISBSG en diferentes grupos de proyectos mediante la utilizaci\u00f3n de tres algoritmos de agrupamiento diferentes: COBWEB, EM, y k-means, de manera que para cada uno de estos grupos (formados por proyectos homog\u00e9neos entre s\u00ed) se obtenga una relaci\u00f3n matem\u00e1tica diferente. La segmentaci\u00f3n llevada a cabo por estos algoritmos mejora la estimaci\u00f3n con respecto al modelo que utiliza la base de datos sin segmentar. Por otra parte si se comparan entre s\u00ed los resultados obtenidos al aplicar cada uno de ellos, se observa que el algoritmo que presenta un mejor comportamiento es EM debido a su naturaleza probabilista.", "num_citations": "70\n", "authors": ["571"]}
{"title": "Searching for rules to detect defective modules: A subgroup discovery approach\n", "abstract": " Data mining methods in software engineering are becoming increasingly important as they can support several aspects of the software development life-cycle such as quality. In this work, we present a data mining approach to induce rules extracted from static software metrics characterising fault-prone modules. Due to the special characteristics of the defect prediction data (imbalanced, inconsistency, redundancy) not all classification algorithms are capable of dealing with this task conveniently. To deal with these problems, Subgroup Discovery (SD) algorithms can be used to find groups of statistically different data given a property of interest. We propose EDER-SD (Evolutionary Decision Rules for Subgroup Discovery), a SD algorithm based on evolutionary computation that induces rules describing only fault-prone modules. The rules are a well-known model representation that can be easily understood and\u00a0\u2026", "num_citations": "60\n", "authors": ["571"]}
{"title": "Finding defective modules from highly unbalanced datasets\n", "abstract": " Many software engineering datasets are highly unbalanced, ie, the number of instances of a one class outnumber the number of instances of the other class. In this work, we analyse two balancing techniques with two common classification algorithms using five open public datasets from the PROMISE repository in order to find defective modules. The results show that although balancing techniques may not improve the percentage of correctly classified instances, they do improve the AUC measure, ie, they classify better those instances that belong to the minority class from the minority class.", "num_citations": "56\n", "authors": ["571"]}
{"title": "Distributed ReliefF based Feature Selection in Spark\n", "abstract": " Feature selection (FS) is a key research area in the machine learning and data mining fields; removing irrelevant and redundant features usually helps to reduce the effort required to process a dataset while maintaining or even improving the processing algorithm\u2019s accuracy. However, traditional algorithms designed for executing on a single machine lack scalability to deal with the increasing amount of data that have become available in the current Big Data era. ReliefF is one of the most important algorithms successfully implemented in many FS applications. In this paper, we present a completely redesigned distributed version of the popular ReliefF algorithm based on the novel Spark cluster computing model that we have called DiReliefF. The effectiveness of our proposal is tested on four publicly available datasets, all of them with a large number of instances and two of them with also a large number of\u00a0\u2026", "num_citations": "47\n", "authors": ["571"]}
{"title": "Comparing Bayesian inference and case-based reasoning as support techniques in the diagnosis of Acute Bacterial Meningitis\n", "abstract": " The amount of information available for physicians has dramatically increased in the recent past. In contrast, the specialist\u2019s ability to understand, synthesize and take into account such information is severely constrained by the short time available for the appointments. Therefore, systems reusing available knowledge and implementing reasoning processes become critical to support the tasks of the doctors. As a number of different techniques for building such systems are available, contrasting their effectiveness becomes a major concern. This is especially important in the case of infectious diseases that can be lethal within hours such as the Acute Bacterial Meningitis (ABM) for which implementing and contrasting different techniques allows for an increased reliability and speed in supporting the process of diagnosis. This work focuses on the construction of diagnosis support tools for ABM, reporting a comparative\u00a0\u2026", "num_citations": "46\n", "authors": ["571"]}
{"title": "The impact of readability on the usefulness of online product reviews: a case study on an online bookstore\n", "abstract": " Online product reviews is an important advantage for consumers of experience goods in online marketplaces and act as a useful source of information during the purchase of a good. Furthermore in some online marketplaces consumers have the opportunity to evaluate how helpful a review was by using a binary evaluation interface provided by the online marketplace. This results to the usefulness score of a review which is calculated as a fraction of helpful votes over the total votes that this review has received. Our early results indicate that the usefulness score of a particular review is affected in a significant way by the qualitative characteristics of the review as measured by readability tests applied to a large dataset of reviews collected from the UK section of the popular online marketplace Amazon.", "num_citations": "45\n", "authors": ["571"]}
{"title": "Attribute selection in software engineering datasets for detecting fault modules\n", "abstract": " Decision making has been traditionally based on managers experience. At present, there is a number of software engineering (SE) repositories, and furthermore, automated data collection tools allow managers to collect large amounts of information, not without associated problems. On the one hand, such a large amount of information can overload project managers. On the other hand, problems found in generic project databases, where the data is collected from different organizations, is the large disparity of its instances. In this paper, we characterize several software engineering databases selecting attributes with the final aim that project managers can have a better global vision of the data they manage. In this paper, we make use of different data mining algorithms to select attributes from the different datasets publicly available (PROMISE repository), and then, use different classifiers to defect faulty modules\u00a0\u2026", "num_citations": "45\n", "authors": ["571"]}
{"title": "Empirical findings on ontology metrics\n", "abstract": " Ontologies are becoming the preferred way of representing, dealing and reasoning with large volumes of information in several domains. In consequence, the creation, evaluation and maintenance of ontologies has become an engineering process that needs to be managed and measured using sound and reliable methods. As part of any ontology engineering or revision process, metrics can play a role helping in identifying possible problems or incorrect use of ontology elements, along with providing a kind of quality assessment that complements reviews requiring expert inspection. However, in spite of the fact that there are a number of ontology metric proposals described in the literature, there is a lack of empirical studies that provide a basis for their interpretation. This paper reports a systematic exploration of existing ontology metrics from a large set of ontologies extracted from the Swoogle search engine. The\u00a0\u2026", "num_citations": "44\n", "authors": ["571"]}
{"title": "Defining software process model constraints with rules using OWL and SWRL\n", "abstract": " The Software & Systems Process Engineering meta-model (SPEM) allows the modelling of software processes using OMG (Object Management Group) standards such as the MOF (Meta-Object Facility) and UML (Unified Modelling Language) making it possible to represent software processes using tools compliant with UML. Process definition encompasses both the static and dynamic structure of roles, tasks and work products together with imposed constraints on those elements. However, the latter requires support for constraint enforcement that is not always directly available in SPEM. Such constraint-checking behaviour could be used to detect possible mismatches between process definitions and the actual processes being carried out in the course of a project. This paper approaches the modelling of such constraints using the SWRL (Semantic Web Rule Language), which is a W3C recommendation. To do so\u00a0\u2026", "num_citations": "41\n", "authors": ["571"]}
{"title": "An empirical study of process-related attributes in segmented software cost-estimation relationships\n", "abstract": " Parametric software effort estimation models consisting on a single mathematical relationship suffer from poor adjustment and predictive characteristics in cases in which the historical database considered contains data coming from projects of a heterogeneous nature. The segmentation of the input domain according to clusters obtained from the database of historical projects serves as a tool for more realistic models that use several local estimation relationships. Nonetheless, it may be hypothesized that using clustering algorithms without previous consideration of the influence of well-known project attributes misses the opportunity to obtain more realistic segments. In this paper, we describe the results of an empirical study using the ISBSG-8 database and the EM clustering algorithm that studies the influence of the consideration of two process-related attributes as drivers of the clustering process: the use of\u00a0\u2026", "num_citations": "41\n", "authors": ["571"]}
{"title": "The evaluation of a palliative care programme for people suffering from life\u2010limiting diseases\n", "abstract": " Aims and objectives To report on the effectiveness of an eight\u2010week palliative care programme in Hong Kong.   Background A recent survey reported that the quality of palliative care services in Hong Kong ranked the 20th among 40 countries and it is far behind other Asian countries. There are disagreement and inadequate communication in clinical decision\u2010making among patients, families and healthcare professionals, and that the nurses lack sufficient knowledge and skills in providing palliative care and advance care planning.   Design A pretest post\u2010test design and semi\u2010structured interviews were adopted.   Methods A total of 108 home care patients with life\u2010limiting disease and their family caregivers in Hong Kong were recruited to complete a set of questionnaire including The McGill Quality of Life Questionnaire for Hong Kong Chinese and the Family Satisfaction Scale before and after they attended an\u00a0\u2026", "num_citations": "34\n", "authors": ["571"]}
{"title": "Software project effort estimation based on multiple parametric models generated through data clustering\n", "abstract": " Parametric software effort estimation models usually consists of only a single mathematical relationship. With the advent of software repositories containing data from heterogeneous projects, these types of models suffer from poor adjustment and predictive accuracy. One possible way to alleviate this problem is the use of a set of mathematical equations obtained through dividing of the historical project datasets according to different parameters into subdatasets called partitions. In turn, partitions are divided into clusters that serve as a tool for more accurate models. In this paper, we describe the process, tool and results of such approach through a case study using a publicly available repository, ISBSG. Results suggest the adequacy of the technique as an extension of existing single-expression models without making the estimation process much more complex that uses a single estimation model. A tool to\u00a0\u2026", "num_citations": "27\n", "authors": ["571"]}
{"title": "The evaluation of ontological representation of the SWEBOK as a revision tool\n", "abstract": " The SWEBOK represents an important milestone in reaching a broad agreement on the contents of the Software Engineering discipline. Formal ontologies thus become a tool to represent such agreement in a logicsbased framework for a number of applications. In this paper, the use of common ontological criteria in the< Onto-SWEBOK> project is described as a useful assessment tool. The use of such concepts and a disciplined approach to representing terms and relations has resulted in a tentative structured revision procedure for SWEBOK material that could be used as a technique for improving or restructuring definitions. The main elements of the technique are described, along with illustrative examples of its potential application as a revision tool.", "num_citations": "26\n", "authors": ["571"]}
{"title": "Ontologies of engineering knowledge: general structure and the case of Software Engineering\n", "abstract": " Engineering knowledge is a specific kind of knowledge that is oriented to the production of particular classes of artifacts, is typically related to disciplined design methods, and takes place in tool-intensive contexts. As a consequence, representing engineering knowledge requires the elaboration of complex models that combine functional and structural representations of the resulting artifacts with process and methodological knowledge. The different categories used in the engineering domain vary in their status and in the way they should be manipulated when building applications that support engineering processes. These categories include artifacts, activities, methods and models. This paper surveys existing models of engineering knowledge and discusses an upper ontology that abstracts the categories that crosscut different engineering domains. Such an upper model can be reused for particular engineering\u00a0\u2026", "num_citations": "23\n", "authors": ["571"]}
{"title": "An\u00e1lisis de la satisfacci\u00f3n de cliente mediante el uso de cuestionarios con preguntas abiertas\n", "abstract": " En este trabajo se analiza, c\u00f3mo el uso de cuestionarios de pre guntas abiertas permite a las peque\u00f1as y medianas empresas, mej orar la evaluaci\u00f3n del grado de satisfacci\u00f3n de clientes seg\u00fan la norma ISO 9001. Al obtener mayor informaci\u00f3n que con los cuestionari os de preguntas cerradas, se eliminan las limitaciones de estos \u00faltim os. Para conseguir este objetivo, se han analizado las pregunta s abiertas mediante su estudio sem\u00e1ntico, obteniendo previamente la ra\u00edz de cada palabra y eliminando las que no aportan informaci\u00f3n, det ectando la tendencia positiva y negativa de cada una de las respuestas. Este estudio prueba que el uso de cuestionarios de preguntas a biertas, facilita cumplir con la norma ISO 9001 y permite su comparaci\u00f3n con los datos del sistema de gesti\u00f3n de las relaciones con los clientes (CRM\u2013del ingles Customer Relationship Management). Adem\u00e1s abre nuevas l\u00edneas de investigaci\u00f3n de la sem\u00e1ntica en los sistemas de calidad y de marketing.", "num_citations": "21\n", "authors": ["571"]}
{"title": "Exploring affiliation network models as a collaborative filtering mechanism in e-learning\n", "abstract": " The online interaction of learners and tutors in activities with concrete objectives provides a valuable source of data that can be analyzed for different purposes. One of these purposes is the use of the information extracted from that interaction to aid tutors and learners in decision making about either the configuration of further learning activities or the filtering of learning resources. This article explores the use of an affiliation network model for such kind of purposes. Concretely, the use of techniques such as blockmodeling\u00a0\u2013\u00a0a technique used to derive meaningful patterns of relationships in the network\u00a0\u2013\u00a0and the analysis of m-slices\u00a0\u2013\u00a0a technique helpful to study cohesion in relationships\u00a0\u2013\u00a0are explored as tools to decide on the configuration of topics and/or learner groups. In particular, the results of the case study show that such techniques can be used to (i) filter participants for rearranging groups; (ii) rearrange topics\u00a0\u2026", "num_citations": "20\n", "authors": ["571"]}
{"title": "ON-SMMILE: Ontology Network-based Student Model for MultIple Learning Environments\n", "abstract": " Currently, many educational researchers focus on the extraction of information about the learning progress to properly assist students. We present ON-SMMILE, a student-centered and flexible student model which is represented as an ontology network combining information related to (i) students and their knowledge state, (ii) assessments that rely on rubrics and different types of objectives, (iii) units of learning and (iv) information resources previously employed as support for the student model in intelligent virtual environment for training/instruction and here extended. The aim of this work is to design and build methodologically, throughout ontological engineering, the ON-SMMILE model to be used as support of future works closely linked to supervision of student's learning as competence-based recommender system. For this purpose, our model is designed as a set of ontological resources that have been\u00a0\u2026", "num_citations": "19\n", "authors": ["571"]}
{"title": "GTC control system: an overview\n", "abstract": " The GTC project is in charge of the construction of an optical/IR 10-meter class telescope at the Observatorio del Roque de los Muchachos in Canary Islands. The GTC control system (GCS) will be responsible for the management and operation of the telescope, including its instrumentation. Its conceptual design has been completed in summer 1997. The continuous and rapid development of hardware, software and communications technology has permitted a greater complexity in control systems. In their turn, the development of active and adaptive optics, the new methods of optimizing available observing time and the continuous development programs to maintain telescope competitiveness present new challenges in the design of control systems. During its life-cycles, the GCS will be subject to continuous changes brought about by different factors, such as the advent of new technologies, the evolution of the\u00a0\u2026", "num_citations": "17\n", "authors": ["571"]}
{"title": "Latitudinal and longitudinal process diversity\n", "abstract": " Software processes vary across organizations and over time. Managing this process diversity is a delicate balancing act between creative, healthy diversity and chaos. In this paper, we examine a particular aspect of this issue, namely some relationships between diversity in software processes, software evolution and the quality of software products and processes. Our main contribution is to distinguish between two broad kinds of process diversity, which we call latitudinal and longitudinal process diversity. To illustrate the differences between these two, we examine the case of a medium\u2010sized system (50\u2009000 lines of C++ code) which has undergone major changes during its lifetime of 10 years. The software was originally developed by an individual academic using a research\u2010oriented process to develop a standalone proof\u2010of\u2010concept system. In a current multi\u2010team project, involving three industrial and three\u00a0\u2026", "num_citations": "16\n", "authors": ["571"]}
{"title": "Multi-Objective Testing Resource Allocation under Uncertainty\n", "abstract": " Testing resource allocation is the problem of planning the assignment of resources to testing activities of software components so as to achieve a target goal under given constraints. Existing methods build on software reliability growth models (SRGMs), aiming at maximizing reliability given time/cost constraints, or at minimizing cost given quality/time constraints. We formulate it as a multiobjective debug-aware and robust optimization problem under uncertainty of data, advancing the state-of-the-art in the following ways. Multiobjective optimization produces a set of solutions, allowing to evaluate alternative tradeoffs among reliability, cost, and release time. Debug awareness relaxes the traditional assumptions of SRGMs-in particular the very unrealistic immediate repair of detected faults-and incorporates the bug assignment activity. Robustness provides solutions valid in spite of a degree of uncertainty on input\u00a0\u2026", "num_citations": "15\n", "authors": ["571"]}
{"title": "Finding Defective Software Modules by Means of Data Mining Techniques\n", "abstract": " The characterization of defective modules in software engineering remains a challenge. In this work, we use data mining techniques to search for rules that indicate modules with a high probability of being defective. Using datasets from the PROMISE repository 1, we first applied feature selection to work only with those attributes from the datasets capable of predicting defective modules. Then, a genetic algorithm search for rules characterising subgroups with a high probability of being defective. This algorithm overcomes the problem of unbalanced datasets where the number of non-defective samples in the dataset highly outnumbers the defective ones.", "num_citations": "15\n", "authors": ["571"]}
{"title": "Segmentation of software engineering datasets using the m5 algorithm\n", "abstract": " This paper reports an empirical study that uses clustering techniques to derive segmented models from software engineering repositories, focusing on the improvement of the accuracy of estimates. In particular, we used two datasets obtained from the International Software Benchmarking Standards Group (ISBSG) repository and created clusters using the M5 algorithm. Each cluster is associated with a linear model. We then compare the accuracy of the estimates so generated with the classical multivariate linear regression and least median squares. Results show that there is an improvement in the accuracy of the results when using clustering. Furthermore, these techniques can help us to understand the datasets better; such techniques provide some advantages to project managers while keeping the estimation process within reasonable complexity.", "num_citations": "14\n", "authors": ["571"]}
{"title": "Redes bayesianas en la ingenier\u00eda del software\n", "abstract": " Muchas de las actividades en la ingenier\u00eda del software, como por ejemplo, la estimaci\u00f3n de costes o esfuerzo, evaluaci\u00f3n de riesgos o fiabilidad tratan con valores inciertos o probabil\u00edsticas. Por tanto, diversas t\u00e9cnicas estad\u00edsticas y la teor\u00eda de la probabilidad han sido aplicadas a la ingenier\u00eda del software desde sus inicios. M\u00e1s recientemente, modelos gr\u00e1ficos, que combinan probabilidad y teor\u00eda de grafos, est\u00e1n siendo aplicados a problemas de la ingenier\u00eda del software donde la incertidumbre est\u00e1 presente. En este cap\u00edtulo, se proporciona una visi\u00f3n general de las redes Bayesianas, sus fundamentos en la teor\u00eda de la probabilidad, la noci\u00f3n de propagaci\u00f3n y su construcci\u00f3n, incluyendo t\u00e9cnicas de miner\u00eda de datos. Adem\u00e1s, se describen diferentes extensiones de las redes Bayesianas, as\u00ed c\u00f3mo su aplicaci\u00f3n a la ingenier\u00eda del software y comparaci\u00f3n con otras t\u00e9cnicas.", "num_citations": "13\n", "authors": ["571"]}
{"title": "Overview of XBRL technologies for decision making in Accounting Information Systems\n", "abstract": " XBRL (eXtensible Business Reporting Language) is a language for the electronic communication of business and financial data based on XML (eXtensible Markup Language). Compared with paper based or other previous adhoc EDI (Electronic Data Interchange) technologies, XBRL provides major benefits in the preparation, analysis and communication of business information. Those benefits include cost savings, greater efficiency, accuracy and reliability to all activities involved in supplying or using financial data. This paper provides an overview of XBRL technologies and how it is applied to decision making in several financial areas. It also covers some possible extensions with the semantic Web and Web services as future challenges.", "num_citations": "12\n", "authors": ["571"]}
{"title": "A two-stage zone regression method for global characterization of a project database\n", "abstract": " One of the problems found in generic project databases, where the data is collected from different organizations, is the large disparity of its instances. In this chapter, we characterize the database selecting both attributes and instances so that project managers can have a better global vision of the data they manage. To achieve that, we first make use of data mining algorithms to create clusters. From each cluster, instances are selected to obtain a final subset of the database. The result of the process is a smaller database which maintains the prediction capability and has a lower number of instances and attributes than the original, yet allow us to produce better predictions.", "num_citations": "12\n", "authors": ["571"]}
{"title": "Maintenance of object oriented systems through re-engineering: A case study\n", "abstract": " Unregulated evolution of software often leads to software ageing which not only makes the product difficult to maintain but also breaks the consistency between design and implementation. In such a case, it may become necessary to re-engineer the software so that it becomes maintainable again. In this paper we present the case study of the reengineering of the People Tracking subsystem of a surveillance system written in C++. We discuss the problems, the challenges and the approaches taken, and we show how the re-engineered product is now better maintainable. We also discuss the generation of the relevant artefacts - from requirement document through to design document.", "num_citations": "12\n", "authors": ["571"]}
{"title": "Exploring structural prestige in learning object repositories: Some insights from examining references in MERLOT\n", "abstract": " Several existing learning object repositories provide mechanisms for users to arrange personal collections with their selection of resources or to provide reviews and ratings for other's resources, creating a kind of community dynamics. The resulting information can be used to build structural prestige models for the creators of the resources. This paper reports preliminary explorations on relational models that could be used to develop metrics of quality and prestige for learning object authors. Concretely, social network analysis tools are used to analyze the overall community structure of a dataset obtained from the MERLOT repository. Networks extracted from the indirect reference between users through references in personal collections and reviews are examined with regards to the position of relevant community members.", "num_citations": "11\n", "authors": ["571"]}
{"title": "Defining spem 2 process constraints with semantic rules using swrl\n", "abstract": " The Software & Systems Process Engineering meta-model (SPEM 2) allows the modelling of software process using OMG\u2019s MOF meta-model and UML profiles, thus being compliant with UML tools. Process definition encompasses both the static structure of activities, roles, tasks and work products and the constraints on those elements. The latter require support for constraint enforcement that is not directly available in SPEM 2. Such constraint-checking behaviour could be used to detect mismatches between process definitions and the actual processes being carried out in the course of a project. This paper approaches the modelling of such constraints using the SWRL (Semantic Web Rule Language) W3C recommendation, using an underlying OWL representation of SPEM 2 models that can be directly derived from their XMI serialization.", "num_citations": "10\n", "authors": ["571"]}
{"title": "Generation of management rules through system dynamics and evolutionary computation\n", "abstract": " Decision making has been traditionally based on a managers experience. This paper, however, discusses how a software project simulator based on System Dynamics and Evolutionary Computation can be combined to obtain management rules. The purpose is to provide accurate decision rules to help project managers to make decisions at any time in the software development life cycle. To do so, a database from which management rules are generated is obtained using a software project simulator based on system dynamics. We then find approximate optimal management rules using an evolutionary algorithm which implements a novel method for encoding the individuals, i.e., management rules to be searched by the algorithm. The resulting management rules of our method are also compared with the ones obtained by another algorithm called C4.5. Results show that our evolutionary approach\u00a0\u2026", "num_citations": "10\n", "authors": ["571"]}
{"title": "Defining a legal risk management strategy: process, legal risk and lifecycle\n", "abstract": " All systems during their lifecycle, no matter how simple, will generate legal implications that need to be managed. The potential cost of an inadequate management of legal aspects can even imply the failure of the project. As a consequence, legal risk management should not only be a major activity of the development lifecycle, but it needs to be performed by qualified personnel following well-defined procedures and standards. However, current software process improvement models do not properly include processes for legal audits and more concretely legal risks management for each phase of the software development lifecycle. Neither in industry related to manage legal risks of software projects is possible to find well-defined and standardised projects. This lack of standardised process means that legal risks are handled reactively instead of proactively. This work presents a process for managing legal\u00a0\u2026", "num_citations": "9\n", "authors": ["571"]}
{"title": "An ontology-based and model-driven approach for designing IT service management systems\n", "abstract": " Currently, few projects applying a Model-Driven Engineering (MDE) approach start from high-level requirements models defined exclusively in terms of domain knowledge and business logic. Ontology Engineering (OE) aims to formalize and make explicit the knowledge related to a particular domain. In this vein, this paper presents a modeling approach, formalized in ontological terms, for defining high-level requirements models of software systems that provide support for the implementation of Information Technology Service Management Systems (ITSMSs). This approach allows for:(1) formalizing the knowledge associated to the ITSM processes contained in an ITSMS;(2) modeling the semantics of the activities associated to these processes in terms of workflows;(3) automatically generating the high-level requirements models of the workflow-based software systems needed to support (part of) the ITSM\u00a0\u2026", "num_citations": "8\n", "authors": ["571"]}
{"title": "Exploring ontology metrics in the biomedical domain\n", "abstract": " Ontologies are gaining popularity in many domains as a way of representing, dealing and reasoning with large volumes of information, and they are starting to play a major role in computational science applications, e.g. in biomedical studies. The creation and maintenance of ontologies has become an engineering process that needs to be managed and measured. Therefore, as part of any ontology engineering processes, metrics can play a role helping in identifying possible problems or incorrect use of ontology elements. However, empirical studies that measure existing ontologies and set standard values are required for metrics to be effectively used. This paper reports a preliminary exploration of ontology metrics from a large set of ontologies extracted from the Open Biomedical Ontologies (OBO) repository. In this paper, we also present an ontology metrics tool, called Ontometrics, used to collect the set of\u00a0\u2026", "num_citations": "8\n", "authors": ["571"]}
{"title": "Assertions in object oriented software maintenance: analysis and case study\n", "abstract": " Assertions had their origin in program verification. For the systems developed in industry, construction of assertions and their use in showing program correctness is a near-impossible task. However, they can be used to show that some key properties are satisfied during program execution. We first present a survey of the special roles that assertions can play in object oriented software construction. We then analyse such assertions by relating them to the case study of an automatic surveillance system. In particular, we address the following two issues: What types of assertions can be used most effectively in the context of object oriented software? How can you discover them and where should they be placed? During maintenance, both the design and the software are continuously changed. These changes can mean that the original assertions, if present, are no longer valid for the new software. Can we automatically\u00a0\u2026", "num_citations": "8\n", "authors": ["571"]}
{"title": "Applying rules to an ontology for project management\n", "abstract": " There is an increasing interest in using ontologies in the area of project and process management and obtain knowledge from ontologies by reasoning. Different approaches are being used for representing knowledge in this field. Based upon previous works on project representations we have developed a basic ontology and we apply rules on it. We used recommended OWL and SWRL languages for defining the ontologies and rules, respectively. Current article shows an example of different types of rules that can be applied on our specific ontology. In this way, we see how further knowledge can be derived and, thus, decision-making for managing projects can be improved.", "num_citations": "7\n", "authors": ["571"]}
{"title": "Evaluation of case based reasoning for clinical decision support systems applied to acute meningitis diagnose\n", "abstract": " This work presents a research about the applicability of Case Based Reasoning to Clinical Decision Support Systems (CDSS), particularly applied to the diagnosis of the disease known as Acute Bacterial Meningitis.           In the last few years, the amount of information available to the medical doctor, who usually finds himself in the situation of making a diagnosis of one or more diseases, has dramatically increased. However, the specialist\u2019s ability to understand, synthesize and take advantage of such information in the alwayslittle time during the medical act remains to be developed.           Many contributions have been made by the computer sciences, especially those by Artificial intelligence, in order to solve these problems. This work focuses on the diagnose of the Acute Bacterial Meningitis, and carries out a comparative assessment of the quality of a Clinical Decision Support System made through Case\u00a0\u2026", "num_citations": "6\n", "authors": ["571"]}
{"title": "SMOTE-I: mejora del algoritmo SMOTE para balanceo de clases minoritarias\n", "abstract": " Las t\u00e9cnicas de miner\u00eda de datos est\u00e1n encaminadas a desarrollar algoritmos que sean capaces de tratar y analizar datos de forma autom\u00e1tica con objeto de extraer de cualquier tipo de informaci\u00f3n subyacente en dichos datos. El problema del desbalanceo de los datos consiste en la predominancia de ciertos valores en los datos y la escasez o ausencia de otros que dificulta o impide la extracci\u00f3n de informaci\u00f3n. En este trabajo se presenta un nuevo algoritmo basado en SMOTE y llamado SMOTE-I que mejora al original con la la base de datos analizada.", "num_citations": "6\n", "authors": ["571"]}
{"title": "Searching for rules to find defective modules in unbalanced data sets\n", "abstract": " The characterisation of defective modules in software engineering remains a challenge. In this work, we use data mining techniques to search for rules that indicate modules with a high probability of being defective. Using data sets from the PROMISE repository, we first applied feature selection (attribute selection) to work only with those attributes from the data sets capable of predicting defective modules. With the reduced data set, a genetic algorithm is used to search for rules characterising modules with a high probability of being defective. This algorithm overcomes the problem of unbalanced data sets where the number of non-defective samples in the data set highly outnumbers the defective ones.", "num_citations": "5\n", "authors": ["571"]}
{"title": "SLA: A legal assurance process model for software engineering management\n", "abstract": " The legal assurance activities and measures are a key element for the viability of information systems projects because nowadays there can arise legal risks in some cases, which can be a serious threat for project commercial and financial success. In spite of this, there does not exist in the main evaluation and improvement processes models a process of legal assurance that systematizes and orders the activities and measures precisely by to manage such legal risks. On the other hand, the professional practice does not generally incorporate standardized processes in order to discipline the legal assurance activities and measures. This circumstance can generate the appearance of deficits in the project's legal security. This work proposes to consider the legal assurance activities and measures as a process to implement more in the evaluation and improvement processes models, with the objective to provide a\u00a0\u2026", "num_citations": "5\n", "authors": ["571"]}
{"title": "Making predictions on new data using Weka\n", "abstract": " First, the file with cases to predict needs to have the same structure that the file used to learn the model. The difference is that the value of the class attribute is \u201c?\u201d for all instances (question marks represent missing values in Weka). For example assuming that we have learnt a decision tree using the diabetes datasets included weka, the following file will be used to predict the 5 cases included in the arff file:@ relation pima_diabetes@ attribute'preg'real@ attribute'plas' real@ attribute'pres' real@ attribute'skin'real@ attribute'insu'real@ attribute'mass' real@ attribute'pedi'real@ attribute'age'real@ attribute'class'{tested_negative, tested_positive}@ data", "num_citations": "5\n", "authors": ["571"]}
{"title": "Triaxial Accelerometer Located on the Wrist for Elderly People's Fall Detection\n", "abstract": " The loss of motor function in the elderly makes this population group prone to accidental falls. Actually, falls are one of the most notable concerns in elder care. Not surprisingly, there are several technical solutions to detect falls, however, none of them has achieved great acceptance. The popularization of smartwatches provides a promising tool to address this problem. In this work, we present a solution that applies machine learning techniques to process the output of a smartwatch accelerometer, being able to detect a fall event with high accuracy. To this end, we simulated the two most common types of falls in elders, gathering acceleration data from the wrist, then applied that data to train two classifiers. The results show high accuracy and robust classifiers able to detect falls.", "num_citations": "4\n", "authors": ["571"]}
{"title": "Epistemological and ontological representation in software engineering\n", "abstract": " This paper provides an overview of how empirical research can be a valid approach to improve epistemological foundations and ontological representations in Software Engineering (SE). Despite of all the research done in SE, most of the results have not been yet been stated as laws, theories, hypothesis or conjectures, i.e., from an epistemological point of view. This paper explores such facts and advocates that the use of empirical methods can help to improve this situation. Furthermore, it is also imperative for SE experiments to be planned and executed properly in order to be valid epistemologically. Finally, this paper presents some epistemological and ontological results obtained from empirical research in SE.", "num_citations": "4\n", "authors": ["571"]}
{"title": "Personal metadata for cataloguing microcontent\u2013linking to large ontologies?\n", "abstract": " Microlearning objects can be related by means of freely created annotations or tagging. Although such solution can help with filtering and searching, it is not enough for formalizing microlearning towards the semantic Web approach. In this paper, we propose the use of upper ontologies and more concretely, OpenCyc, to provide the required formal semantics needed by the semantic Web.", "num_citations": "4\n", "authors": ["571"]}
{"title": "The open source software vs. proprietary software debate and its impact on technological innovation\n", "abstract": " Technological innovation\u2013in the field of information technology in general and in the field of software specifically\u2013is emerging as a key factor in the development and general well-being of advanced industrial societies. The competitiveness of economic agents is in direct proportion to their capacity for innovation, and the wealth and level of development of nations is the result of the innovative capacity of each country.One factor which will have a decisive impact on the structure and evolution of technological innovation policies and, more specifically, on software industry, is the final outcome of the OSS (Open Source Software) vs. proprietary software dilemma. This will depend on the dynamics of the market itself and, to a great extent, on the policies and regulations established by the national and supranational bodies working in this field.", "num_citations": "4\n", "authors": ["571"]}
{"title": "Modelos Segmentados de estimaci\u00f3n del esfuerzo de desarrollo del software: un caso de estudio con la base de datos ISBSG\n", "abstract": " Parametric sofware effort estimation models use historical project databases to adjust the parameters of the required effort function. The use of databases with data coming from heterogeneous sources often entail that the resulting models are subject to excessively high mean errors, due to the fact that data are widely diverging in magnitude. In order to improve this situation, the segmentation of the input domain has been proposed, so that a regression model with different parameters is obtained for each segment. In this paper, the use of well-known clustering algorithms in a recursive way is described as a technique to obtain segmented models of the kind mentioned. Concretely, the ISBSG database and the EM algorithm are use as a demonstration of the results that can be obtained through that technique.Resumen: Los modelos param\u00e9tricos de estimaci\u00f3n del esfuerzo de desarrollo de software utilizan bases de datos de proyectos pasados para ajustar la funci\u00f3n del esfuerzo requerido. El uso de bases de datos cuyas fuentes son heterog\u00e9neas hace que los modelos obtenidos mediante regresi\u00f3n a menudo tengan errores medios excesivamente altos, debido a que los datos son muy diferentes entre s\u00ed en cuanto a su magnitud. Para mejorar esta situaci\u00f3n se ha propuesto el dividir el dominio de las entradas en varios segmentos, de modo que se obtenga un modelo de regresi\u00f3n con diferentes par\u00e1metros para cada uno de ellos. En este art\u00edculo se describe c\u00f3mo se pueden utilizar algoritmos conocidos de agrupamiento de manera recursiva para obtener ese tipo de modelo segmentado. Concretamente, se utiliza la base de datos ISBSG y el\u00a0\u2026", "num_citations": "4\n", "authors": ["571"]}
{"title": "Using Simulation and the NSGA-II Evolutionary Multi-Objective Algorithm in the Design of a Compact Dual-band Equatorial Helix Antenna\n", "abstract": " The design of a Compact Dual-band Equatorial helix antenna is presented. These antennas are used for Telemetry, Tracking, and Control (TTC) of satellites from the terrain base station. A simulation-optimization process is presented, a simulation tool named MONURBS is linked with a well-known multi-objective algorithm (NSGA-II) in order to design and optimize the parameters of the antenna. The size of the antenna that fulfills radiation patterns needed for the communication are obtained using simulation together with a multi-objective algorithm. In this work, a comparison with previous designs and the antenna prototype are be presented showing that this approach can achive solutions expediting the process.", "num_citations": "3\n", "authors": ["571"]}
{"title": "Defining the semantics of IT service management models using OWL and SWRL\n", "abstract": " Service management is a set of specialized organizational capabilities that provide value to customers in the form of services. Many organizations are aware of the need to adopt best practices in order to create an effective IT Service Management (ITSM) for enabling Business and IT integration. However, the reuse and interchange of service models is still quite limited in the area of IT service support due to the problems in connecting with natural language. In this context, this paper presents the ITIL-based Service Management Model aimed at capturing ITSM best practices by means of a formal ontology-based business DSL (Domain-Specific Language). We show how this DSL can be formally represented adopting the Web Ontology Language (OWL) and the Semantic Web Rule Language (SWRL). This ontology will precisely define the semantics associated to IT service management models, enabling different tools to interchange them without ambiguities. These models will be defined just in terms of the business logic, without any architectural or platform-specific consideration. That is, according to the OMG's four-layered architecture, the proposed model could be placed at a CIM level.", "num_citations": "3\n", "authors": ["571"]}
{"title": "Using genetic algorithms to generate estimation models\n", "abstract": " 1 36.48 [2562] 2 28.05 [2081, 2238] 3 23.94 [1235, 1280, 1408] 4 20.57 [1206, 1250, 1373, 1667] 5 19.66 [342, 535, 1039, 2041, 2474] 6 18.55 [339, 963, 1238, 1594, 1781, 2185] 7 16.75 [507, 1260, 1395, 1462, 2047, 2090, 2359] 8 14.71 [356, 688, 1180, 1303, 1655, 1734, 2014, 2091] 9 13.23 [321, 630, 863, 890, 1014, 1881, 2004, 2160, 2663] 10 13.43 [229, 461, 705, 919, 962, 1218, 1389, 1666, 1893, 2484]", "num_citations": "2\n", "authors": ["571"]}
{"title": "Design of a TTC Antenna Using Simulation and Multiobjective Evolutionary Algorithms\n", "abstract": " The design of a Compact Dual-band Equatorial helix antenna using Computational Electromagnetic Methods together with multiobjective optimization algorithms is presented. These antennas are used for Telemetry, Tracking, and Control of satellites from the terrain base station. In order to optimize the parameters an antenna, a simulation-optimization process is shown along a real case study. The parameters of the antenna that fulfills the radiation patterns needed for the communication are obtained using a simulation tool called MONURBS together with two well-known multiobjective algorithms: NSGA-II and SPEA-2. In this paper, a comparison with previous designs and the antenna prototype is presented, showing that this approach can obtain multiple valid solutions and accelerate the design process.", "num_citations": "1\n", "authors": ["571"]}
{"title": "Predicci\u00f3n de m\u00f3dulos defectuosos como un problema de optimizaci\u00f3n multiobjetivo\n", "abstract": " La dificultad de aplicar t\u00e9cnicas de an\u00e1lisis de datos al problema de la calidad del software radica principalmente en dos razones: la ausencia de datos generalistas y de herramientas espec\u0131ficas. En este trabajo exponemos los primeros pasos de una iniciativa para paliar estos inconvenientes. Con respecto al primero, hemos trabajado con dos conjuntos de datos p\u00fablicos que han sido tratados de forma conjunta para poder lograr modelos m\u00e1s generales. Para el segundo prop\u00f3sito se ha aplicado un algoritmo multiobjetivo que mediante reglas cuantitativas establezca cu\u00e1les son los l\u0131mites emp\u0131ricos de los atributos que miden la complejidad a partir de los cuales la probabilidad de error aumenta significativamente e incluso la posibilidad de medir ese aumento.", "num_citations": "1\n", "authors": ["571"]}
{"title": "Ontology-Based CMS News Authoring Environment\n", "abstract": " This paper describes the specification, modelling and design of an ontology-based news authoring environment for the Semantic Web, that takes into account the construction and use of an ontology of the Zika disease. CMSs are being adapted in order to receive semantic features, such as automatic generations of keywords, semantic annotation and tagging, content reviewing etc. We present here the infrastructure designed to foster research on semantic CMSs as well as semantic web technologies that can be integrated into an ontology-based news authoring environment.", "num_citations": "1\n", "authors": ["571"]}
{"title": "Analysis of customer satisfaction using surveys with open questions\n", "abstract": " In this paper the use of open-ended questionnaires to improve the evaluation of customer satisfaction according to ISO 9001 in small and medium-sized enterprises is analyzed. By obtaining more information in comparison to the closed questions questionnaire some limitations coming from the second one are removed. The open-ended questionnaire is analyzed by applying a semantic study to obtain the root of each word and remove the word that is not relevant for the information needs of the organization. This way the positive or negative trend for each response is identified. This study proofs that the use of open-ended questionnaires facilitates the fulfilment of the ISO 9001 standard. It allows the comparison between the data coming from the Customer Relationship Management System (CRM) and the data obtained through the questionnaire. Furthermore it opens new areas of research based in the use of semantic analysis in quality systems and marketing.", "num_citations": "1\n", "authors": ["571"]}
{"title": "Devising instruction from empirical findings on student errors: A case in usability engineering education\n", "abstract": " Problem-based learning relies on the use of problems as the fundamental activity driving the learning process, focusing on the application of knowledge to realistic settings. Problems requiring students the design or evaluation of artifacts are a fundamental ingredient of engineering education in diverse fields. In those settings, the effectiveness of instructional design critically relies on the quality of the problems used, which should emphasize the aspects that students usually find difficult to master, so that relevant domain knowledge is exercised during learning activity. The analysis of the errors in student\u2019s solutions to problem assignments can be used as an empirical source of information for the instructional design of problem collections. In this approach, problem design is driven by findings on the kind and frequency of errors. This paper reports the use of such an approach in the domain of heuristic usability evaluation in the context of an introductory Human Computer Interaction course, using the 3C3R model as a framework. The method for data elaboration and the resulting approach to devising problems can be transferred to other domains in which similar high-level design analysis is required.", "num_citations": "1\n", "authors": ["571"]}
{"title": "Exploring Cohesion, Flexibility, Communication Overhead and Distribution for Web Services Interfaces in Computational Science\n", "abstract": " Computational science studies often rely on the availability of large datasets through the Web. Web services (WS) provide a convenient way for making those datasets available, since they rely on a standard and widely available technology. However, there are many ways to devise a Web service interface for a given dataset, and the resulting interfaces vary in their properties related to cohesion, distribution, flexibility and communication overhead, among other parameters. This paper explores these attributes and provides some directions on how they can be measured. Concretely, a well-known cohesion metric is explored as a way to characterize the possible kinds of Web service interface designs. This is discussed for a concrete distributed context of service-oriented architectures.", "num_citations": "1\n", "authors": ["571"]}
{"title": "Panorama de la Computaci\u00f3n Ubicua\n", "abstract": " Panorama de la Computaci\u00f3n Ubicua - Dialnet Ir al contenido Dialnet Buscar Revistas Tesis Congresos Ayuda Panorama de la Computaci\u00f3n Ubicua Autores: Jos\u00e9 Antonio Guti\u00e9rrez Mesa, Daniel Rodr\u00edguez Garc\u00eda, Mitiadis D. Lytras Localizaci\u00f3n: Nov\u00e1tica: Revista de la Asociaci\u00f3n de T\u00e9cnicos de Inform\u00e1tica, ISSN 0211-2124, N\u00ba. , 2005 (Ejemplar dedicado a: Omnipresencia computacional), p\u00e1gs. 4-7 Idioma: espa\u00f1ol Enlaces Texto completo (pdf) Fundaci\u00f3n Dialnet Acceso de usuarios registrados Imagen de identificaci\u00f3n Identificarse \u00bfOlvid\u00f3 su contrase\u00f1a? \u00bfEs nuevo? Reg\u00edstrese Ventajas de registrarse Dialnet Plus M\u00e1s informaci\u00f3n sobre Dialnet Plus Opciones de compartir Facebook Twitter Opciones de entorno Sugerencia / Errata \u00a9 2001-2021 Fundaci\u00f3n Dialnet \u00b7 Todos los derechos reservados Dialnet Plus Accesibilidad Aviso Legal Coordinado por: Fundaci\u00f3n Dialnet Inicio Buscar Revistas Tesis Co\u2026", "num_citations": "1\n", "authors": ["571"]}
{"title": "Bayesian Networks and Classifiers in Project Management\n", "abstract": " Bayesian Networks are becoming increasingly popular within the Software Engineering research community as an effective method of analysing collected data. This paper deals with the creation and the use of Bayesian networks and Bayesian classifiers in project management. We illustrate this process with an example in the context of software estimation that uses the Maxwell\u2019s dataset [17](it is a subset of the Finnish dataset\u2013STTF\u2013). We highlight some of the difficulties and challenges of using Bayesian networks and Bayesian classifiers. We discuss how the Bayesian approach can be used as a viable technique in Software Engineering in general and for project management in particular; and also the challenges and the open issues.", "num_citations": "1\n", "authors": ["571"]}
{"title": "Descubrimiento de Subgrupos para predecir m\u00f3dulos defectuosos\n", "abstract": " La aplicaci\u00f3n de m\u00e9todos de Miner\u00eda de Datos a la Ingenier\u00eda del Software tiene una importancia creciente en distintos aspectos del ciclo de vida del software. En este trabajo presentamos una metodolog\u00eda para inducir reglas que nos permitan establecer cu\u00e1les son las m\u00e9tricas y los umbrales que caracterizan la aparici\u00f3n de m\u00f3dulos con fallos. Abordamos el problema a partir de modelos de Descubrimiento de Subgrupos (DS) que nos permite buscar patrones s\u00f3lo para un tipo de dato con alguna propiedad de inter\u00e9s, en nuestro caso, m\u00f3dulos con errores.", "num_citations": "1\n", "authors": ["571"]}