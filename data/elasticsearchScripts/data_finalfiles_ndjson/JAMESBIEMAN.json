{"title": "Cohesion and reuse in an object-oriented system\n", "abstract": " We define and apply two new measures of object-oriented class cohesion to a reasonably large C++ system. We find that most of the classes are quite cohesive, but that the classes that are reused more frequently via inheritance exhibit clearly lower cohesion.", "num_citations": "675\n", "authors": ["308"]}
{"title": "Measuring functional cohesion\n", "abstract": " We examine the functional cohesion of procedures using a data slice abstraction. Our analysis identifies the data tokens that lie on more than one slice as the \"glue\" that binds separate components together. Cohesion is measured in terms of the relative number of glue tokens, tokens that lie on more than one data slice, and super-glue tokens, tokens that lie on all data slices in a procedure, and the adhesiveness of the tokens. The intuition and measurement scale factors are demonstrated through a set of abstract transformations.< >", "num_citations": "382\n", "authors": ["308"]}
{"title": "Rapid prototyping: lessons learned\n", "abstract": " Opinions on rapid prototyping as a practical development tool vary widely, with conventional wisdom seeing it more as a research topic than a workable method. The authors counter this notion with results from 39 case studies, most of which have used this approach successfully.< >", "num_citations": "228\n", "authors": ["308"]}
{"title": "Towards the systematic testing of aspect-oriented programs\n", "abstract": " The code that provides solutions to key software requirements, such as security and fault-tolerance, tends to be spread throughout (or cross-cut) the program modules that implement the \u201cprimary functionality\u201d of a software system. Aspect-oriented programming is an emerging programming paradigm that supports implementing such cross-cutting requirements into named program units called \u201caspects\u201d. To construct a system as an aspect-oriented program (AOP), one develops code for primary functionality in traditional modules and code for cross-cutting functionality in aspect modules. Compiling and running an AOP requires that the aspect code be \u201cwoven\u201d into the code. Although aspect-oriented programming supports the separation of concerns into named program units, explicit and implicit dependencies of both aspects and traditional modules will result in systems with new testing challenges, which include new sources for program faults. This paper introduces a candidate fault model, along with associated testing criteria, for AOPs based on interactions that are unique to AOPs. The paper also identifies key issues relevant to the systematic testing of AOPs.", "num_citations": "220\n", "authors": ["308"]}
{"title": "The FreeBSD project: a replication case study of open source development\n", "abstract": " Case studies can help to validate claims that open source software development produces higher quality software at lower cost than traditional commercial development. One problem inherent in case studies are external validity - we do not know whether or not results from one case study apply to another development project. We gain or lose confidence in case study results when similar case studies are conducted on other projects. This case study of the FreeBSD project, a long-lived open source project, provides further understanding of open source development. The paper details a method for mining repositories and querying project participants to retrieve key process information. The FreeBSD development process is fairly well-defined with proscribed methods for determining developer responsibilities, dealing with enhancements and defects, and managing releases. Compared to the Apache project\u00a0\u2026", "num_citations": "196\n", "authors": ["308"]}
{"title": "Measuring Design\u2013level Cohesion\n", "abstract": " Cohesion was first introduced as a software attribute that, when measured, could be used to predict properties of implementations that would be created from a given design. Unfortunately, cohesion, as originally defined, could not be objectively assessed, while more recently developed objective cohesion measures depend on code-level information. We show that association-based and slice-based approaches can be used to measure cohesion using only design-level information. An analytical and empirical analysis shows that the design-level measures correspond closely with code-level cohesion measures. They can be used as predictors of or surrogates for the code-level measures. The design-level cohesion measures are formally defined, have been implemented, and can support software design, maintenance and restructuring.", "num_citations": "180\n", "authors": ["308"]}
{"title": "Competencies of exceptional and nonexceptional software engineers\n", "abstract": " The attributes of individual software engineers are perhaps the most important factors determining the success of software development. Our goal is to identify the professional competencies that are most essential. In particular, we seek to identify the attributes that differentiate exceptional and nonexceptional software engineers. Phase 1 of our research is a qualitative study designed to identify competencies to be used in the quantitative analysis performed in phase 2. In phase 1, we conduct an in-depth review of 10 exceptional and 10 nonexceptional software engineers working for a major computing firm. We use biographical data and Myers-Briggs Type Indicator test results to characterize our sample. We conduct Critical Incident Interviews focusing on the subjects' experience in software and identify 38 essential competencies of software engineers. Phase 2 of this study is a survey of 129 software engineers\u00a0\u2026", "num_citations": "170\n", "authors": ["308"]}
{"title": "Design patterns and change proneness: An examination of five evolving systems\n", "abstract": " Design patterns are recognized, named solutions to common design problems. The use of the most commonly referenced design patterns should promote adaptable and reusable program code. When a system evolves, changes to code involving a design pattern should, in theory, consist of creating new concrete classes that are extensions or subclasses of previously existing classes. Changes should not, in theory, involve direct modifications to the classes in prior versions that play roles in a design patterns. We studied five systems, three proprietary systems and two open source systems, to identify the observable effects of the use of design patterns in early versions on changes that occur as the systems evolve. In four of the five systems, pattern classes are more rather than less change prone. Pattern classes in one of the systems were less change prone. These results held up after normalizing for the effect of\u00a0\u2026", "num_citations": "168\n", "authors": ["308"]}
{"title": "A philosophy for software measurement\n", "abstract": " We as a group\u2014called the Grubstake Group\u2014are convinced that software measures are essential for \u201ccontrolling\u201d software. Thus, we are dedicated to producing an environment in which software measures can be confidently used by software managers and programmers. However, we are also convinced that such an environment can only be created if there exists a formal and rigorous foundation for software measurement. This foundation will not have to be understood by the users of the software measures, but it will have to be understood by those who define, validate, and provide tool support for the measures. It is this foundation that we are introducing in this article.", "num_citations": "150\n", "authors": ["308"]}
{"title": "Testing scientific software: A systematic literature review\n", "abstract": " ContextScientific software plays an important role in critical decision making, for example making weather predictions based on climate models, and computation of evidence for research publications. Recently, scientists have had to retract publications due to errors caused by software faults. Systematic testing can identify such faults in code.ObjectiveThis study aims to identify specific challenges, proposed solutions, and unsolved problems faced when testing scientific software.MethodWe conducted a systematic literature survey to identify and analyze relevant literature. We identified 62 studies that provided relevant information about testing scientific software.ResultsWe found that challenges faced when testing scientific software fall into two main categories: (1) testing challenges that occur due to characteristics of scientific software such as oracle problems and (2) testing challenges that occur due to cultural\u00a0\u2026", "num_citations": "130\n", "authors": ["308"]}
{"title": "A mathematical perspective for software measure research\n", "abstract": " We identify and analyse basic principles which necessarily underlie software measures research. In the prevailing paradigm for the validation of software measures, there is a fundamental assumption that the sets of measured documents are ordered and that measures should report these orders. We describe mathematically the nature of such orders. Consideration of these orders suggests a hierarchy of software document measures, a methodology for developing new measures and a general approach to the analytical evaluation of measures. We also point out the importance of units for any type of measurement and stress the perils of equating document structure complexity and psychological complexity.", "num_citations": "123\n", "authors": ["308"]}
{"title": "Understanding change-proneness in OO software through visualization\n", "abstract": " During software evolution, adaptive, and corrective maintenance are common reasons for changes. Often such changes cluster around key components. It is therefore important to analyze the frequency of changes to individual classes, but, more importantly, to also identify and show related changes in multiple classes. Frequent changes in clusters of classes may be due to their importance, due to the underlying architecture or due to chronic problems. Knowing where those change-prone clusters are can help focus attention, identify targets for re-engineering and thus provide product-based information to steer maintenance processes. This paper describes a method to identify and visualize classes and class interactions that are the most change-prone. The method was applied to a commercial embedded, real-time software system. It is object-oriented software that was developed using design patterns.", "num_citations": "118\n", "authors": ["308"]}
{"title": "Mutation of Java objects\n", "abstract": " Fault insertion based techniques have been used for measuring test adequacy and testability of programs. Mutation analysis inserts faults into a program with the goal of creating mutation-adequate test sets that distinguish the mutant from the original program. Software testability is measured by calculating the probability that a program will fail on the next test input coming from a predefined input distribution, given that the software includes a fault. Inserted faults must represent plausible errors. It is relatively easy to apply standard transformations to mutate scalar values such as integers, floats, and character data, because their semantics are well understood. Mutating objects that are instances of user defined types is more difficult. There is no obvious way to modify such objects in a manner consistent with realistic faults, without writing custom mutation methods for each object class. We propose a new object\u00a0\u2026", "num_citations": "92\n", "authors": ["308"]}
{"title": "Coupling of design patterns: Common practices and their benefits\n", "abstract": " Object-oriented (OO) design patterns define collections of interconnected classes that serve a particular purpose. A design pattern is a structural unit in a system built out of patterns, not unlike the way a function is a structural unit in a procedural program or a class is a structural unit in an OO system designed without patterns. When designers treat patterns as structural units, they become concerned with issues such as coupling and cohesion at a new level of abstraction. We examine the notion of pattern coupling to classify how designs may include coupled patterns. We find many examples of coupled patterns; this coupling may be \"tight\" or \"loose\", and provides both benefits and costs. We qualitatively assess the goodness of pattern coupling in terms of effects on maintainability, factorability, and reusability when patterns are coupled in various ways.", "num_citations": "92\n", "authors": ["308"]}
{"title": "Predicting metamorphic relations for testing scientific software: a machine learning approach using graph kernels\n", "abstract": " Comprehensive, automated software testing requires an oracle to check whether the output produced by a test case matches the expected behaviour of the programme. But the challenges in creating suitable oracles limit the ability to perform automated testing in some programmes, and especially in scientific software. Metamorphic testing is a method for automating the testing process for programmes without test oracles. This technique operates by checking whether the programme behaves according to properties called metamorphic relations. A metamorphic relation describes the change in output when the input is changed in a prescribed way. Unfortunately, finding the metamorphic relations satisfied by a programme or function remains a labour\u2010intensive task, which is generally performed by a domain expert or a programmer. In this work, we propose a machine learning approach for predicting metamorphic\u00a0\u2026", "num_citations": "84\n", "authors": ["308"]}
{"title": "OO Design patterns, design structure, and program changes: an industrial case study\n", "abstract": " A primary expected benefit of object-oriented (OO) methods is the creation of software systems that are easier to adapt and maintain. OO design patterns are especially geared to improve adaptability, since patterns generally increase the complexity of an initial design in order to ease future enhancements. For design patterns to really provide benefit, they must reduce the cost of future adaptation. The evidence of improvements in adaptability through the use of design patterns and other design structures consists primarily of intuitive arguments and examples. There is little empirical evidence to support claims of improved flexibility of these preferred structures. In this case study, we analyze 39 versions of an evolving industrial OO software system to see if there is a relationship between patterns, other design attributes, and the number of changes. We found a strong relationship between class size and the number of\u00a0\u2026", "num_citations": "84\n", "authors": ["308"]}
{"title": "Deriving measures of software reuse in object oriented systems\n", "abstract": " The analysis and measurement of current levels of software reuse are necessary to monitor improvements. This paper provides a framework for the derivation of measures of software reuse and introduces several definitions, attributes, and abstractions of potentially measurable reuse properties. The framework is applied to the problem of measuring reuse in object oriented systems which support \u201cleveraged\u201d reuse through inheritance. I describe the importance of the perspective of the observer when analyzing, measuring, and profiling reuse. Three perspectives are examined: the server perspective, the client perspective, and the system perspective. Candidate reuse metrics are proposed from each perspective.", "num_citations": "82\n", "authors": ["308"]}
{"title": "Open source software development: a case study of FreeBSD\n", "abstract": " A common claim is that open source software development produces higher quality software at lower cost than traditional commercial development To validate such claims, researchers have conducted case studies of \"successful\" open source development projects. This case study of the FreeBSD project provides further understanding of open source development. The FreeBSD development process is fairly well-defined with proscribed methods for determining developer responsibilities, dealing with enhancements and defects, and for managing releases. Compared to the Apache project, FreeBSD uses a smaller set of core developers that implement a smaller portion of the system, and uses a more well-defined testing process. FreeBSD and Apache have a similar ratio of core developers to (1) people involved in adapting and debugging the system, and (2) people who report problems. Both systems have similar\u00a0\u2026", "num_citations": "79\n", "authors": ["308"]}
{"title": "Using machine learning techniques to detect metamorphic relations for programs without test oracles\n", "abstract": " Much software lacks test oracles, which limits automated testing. Metamorphic testing is one proposed method for automating the testing process for programs without test oracles. Unfortunately, finding the appropriate metamorphic relations required for use in metamorphic testing remains a labor intensive task, which is generally performed by a domain expert or a programmer. In this work we present a novel approach for automatically predicting metamorphic relations using machine learning techniques. Our approach uses a set of features developed using the control flow graph of a function for predicting likely metamorphic relations. We show the effectiveness of our method using a set of real world functions often used in scientific applications.", "num_citations": "76\n", "authors": ["308"]}
{"title": "An empirical evaluation (and specification) of the all-du-paths testing criterion\n", "abstract": " The all-du-paths structural testing criterion is one of the most discriminating of the data-flow testing criteria. Unfortunately, in the worst case, the criterion requires an intractable number of test cases. In a case study of an industrial software system, we find that the worst-case scenario is rare. Eighty percent of the subroutines require ten or fewer test cases. Only one subroutine out of 143 requires an intractable number of tests. However, the number of required test cases becomes tractable when using the all-uses criterion. This paper includes a formal specification of both the all-du-paths criterion and the software tools used to estimate a minimal number of test cases necessary to meet the criterion.", "num_citations": "71\n", "authors": ["308"]}
{"title": "The effectiveness of automated static analysis tools for fault detection and refactoring prediction\n", "abstract": " Many automated static analysis (ASA) tools have been developed in recent years for detecting software anomalies. The aim of these tools is to help developers to eliminate software defects at early stages and produce more reliable software at a lower cost. Determining the effectiveness of ASA tools requires empirical evaluation. This study evaluates coding concerns reported by three ASA tools on two open source software (OSS) projects with respect to two types of modifications performed in the studied software CVS repositories: corrections of faults that caused failures, and refactoring modifications. The results show that fewer than 3% of the detected faults correspond to the coding concerns reported by the ASA tools. ASA tools were more effective in identifying refactoring modifications and corresponded to about 71% of them. More than 96% of the coding concerns were false positives that do not relate to any\u00a0\u2026", "num_citations": "69\n", "authors": ["308"]}
{"title": "Reuse through inheritance: A quantitative study of C++ software\n", "abstract": " According to proponents of object-oriented programming, inheritance is an excellent way to organize abstraction and a superb tool for reuse. Yet, few quantitative studies of the actual use of inheritance have been conducted. Quantitative studies are necessary to evaluate the actual usefulness of structures such as inheritance. We characterize the use of inheritance in 19 existing C++ software systems containing 2,744 classes. We measure the class depth in the inheritance hierarchies, and the number of child and parent classes in the software. We find that inheritance is used far less frequently than expected.", "num_citations": "66\n", "authors": ["308"]}
{"title": "Using fault injection to increase software test coverage\n", "abstract": " During testing, it is nearly impossible to run all statements or branches of a program. It is especially difficult to test the code used to respond to exceptional conditions. This untested code, often the error recovery code, will tend to be an error prone part of a system. We show that test coverage can be increased through an \"assertion violation\" technique for injecting software faults during execution. Using our prototype tool, Visual C-Patrol (VCP), we were able to substantially increase test branch coverage in four software systems studied.", "num_citations": "63\n", "authors": ["308"]}
{"title": "Developing measures of class cohesion for object-oriented software\n", "abstract": " Cohesion refers to the relatedness of module components and is a well-understood concept in the procedural paradigm. In the object-oriented paradigm, a concept of class cohesion appears to be necessary. In this paper, we compare two di erent approaches to measuring class cohesion.", "num_citations": "59\n", "authors": ["308"]}
{"title": "Candidate reuse metrics for object oriented and Ada software\n", "abstract": " The measurement of levels of software reuse is necessary to monitor improvements in software reuse. The paper introduces a set of measurable reuse attributes appropriate to object oriented systems, and proposes a suite of metrics which quantify these attributes. Metrics suitable for the object based language Ada are identified and a prototype measurement tool design is proposed.< >", "num_citations": "57\n", "authors": ["308"]}
{"title": "A quantitative framework for software restructuring\n", "abstract": " Many existing software systems can benefit from restructuring to reduce maintenance cost and improve reusability. Yet, intuition\u2010based, ad hoc restructuring can be difficult and expensive, and can even make software structure worse. We introduce a quantitative framework for software restructuring. In the framework, restructuring decisions are guided by visualized design information and objective criteria. The design information can be extracted directly from code to restructure existing or legacy software. Criteria for comparing alternative design structures include measures of design\u2010level cohesion and coupling. Restructuring is accomplished through a series of decomposition and composition operations which increase the cohesion and/or decrease the coupling of individual system components. An example and a case study demonstrate the framework. The framework ensures that restructuring results in\u00a0\u2026", "num_citations": "56\n", "authors": ["308"]}
{"title": "Program slices as an abstraction for cohesion measurement\n", "abstract": " The basis for measuring many attributes in the physical world, such as size and mass, is fairly obvious when compared to the measurement of software attributes. Software has a very complex structure, and this makes it difficult to define meaningful measures that actually quantify attributes of interest. Program slices provide an abstraction that can be used to define important software attributes that can serve as a basis for measurement. We have successfully used program slices to define objective, meaningful, and valid measures of cohesion. Previously, cohesion was viewed as an attribute that could not be objectively measured; cohesion assessment relied on subjective evaluations. In this paper we review the original slice-based cohesion measures defined to measure functional cohesion in the procedural paradigm as well as the derivative work aimed at measuring cohesion in other paradigms and situations. By\u00a0\u2026", "num_citations": "56\n", "authors": ["308"]}
{"title": "Aspect-oriented refactoring of legacy applications: An evaluation\n", "abstract": " The primary claimed benefits of aspect-oriented programming (AOP) are that it improves the understandability and maintainability of software applications by modularizing crosscutting concerns. Before there is widespread adoption of AOP, developers need further evidence of the actual benefits as well as costs. Applying AOP techniques to refactor legacy applications is one way to evaluate costs and benefits. We replace crosscutting concerns with aspects in three industrial applications to examine the effects on qualities that affect the maintainability of the applications. We study several revisions of each application, identifying crosscutting concerns in the initial revision and also crosscutting concerns that are added in later revisions. Aspect-oriented refactoring reduced code size and improved both change locality and concern diffusion. Costs include the effort required for application refactoring and aspect creation\u00a0\u2026", "num_citations": "44\n", "authors": ["308"]}
{"title": "Techniques for testing scientific programs without an oracle\n", "abstract": " The existence of an oracle is often assumed in software testing. But in many situations, especially for scientific programs, oracles do not exist or they are too hard to implement. This paper examines three techniques that are used to test programs without oracles: (1) Metamorphic testing, (2) Run-time Assertions and (3) Developing test oracles using machine learning. We examine these methods in terms of their (1) fault finding ability, (2) automation, and (3) required domain knowledge. Several case studies apply these three techniques to effectively test scientific programs that do not have oracles. Certain techniques have reported a better fault finding ability than the others when testing specific programs. Finally, there is potential to increase the level of automation of these techniques, thereby reducing the required level of domain knowledge. Techniques that can potentially be automated include (1) detection of likely\u00a0\u2026", "num_citations": "42\n", "authors": ["308"]}
{"title": "Using Alloy and UML/OCL to Specify Run-Time Configuration Management: A Case Study.\n", "abstract": " There are many different ways to specify the requirements of complex software systems, and the optimal methods often vary according to the problem domain. We apply and compare two languages, UML/OCL and Alloy, to specify a problem in one domain, the run-time configuration management of a loosely coupled distributed system, to determine which is more appropriate for this domain. The specific problem that we specify in the case study involves the run-time configuration management of an Asynchronous Transfer Mode / Internet Protocol (ATM/IP) Network Monitoring System. Neither Alloy nor UML/OCL supports the specification of key temporal aspects of the problem. This paper addresses the representation of requirements specification; continuing research will compare the usefulness of the specifications for modeling and design purposes.", "num_citations": "42\n", "authors": ["308"]}
{"title": "Effects of Software Changes on Module Cohesion\n", "abstract": " We use program slices to model module cohesion. For our purposes, a slice is a projection of program text that includes only the data tokens relevant to one output. We define six cohesion metrics in terms of these slices, and evaluate the effects of classes of module changes on these metrics. We find that the effects on cohesion metrics are notably more predictable when the changes result from adding code rather than from moving code. In general, the effects that soft-ware changes have on the cohesion metrics match our intuition.", "num_citations": "36\n", "authors": ["308"]}
{"title": "Cell phone-based system (Chaak) for surveillance of immatures of dengue virus mosquito vectors\n", "abstract": " Capture of surveillance data on mobile devices and rapid transfer of such data from these devices into an electronic database or data management and decision support systems promote timely data analyses and public health response during disease outbreaks. Mobile data capture is used increasingly for malaria surveillance and holds great promise for surveillance of other neglected tropical diseases. We focused on mosquito-borne dengue, with the primary aims of: 1) developing and field-testing a cell phone-based system (called Chaak) for capture of data relating to the surveillance of the mosquito immature stages, and 2) assessing, in the dengue endemic setting of M\u00e9rida, M\u00e9xico, the cost-effectiveness of this new technology versus paper-based data collection. Chaak includes a desktop component, where a manager selects premises to be surveyed for mosquito immatures, and a cell phone component\u00a0\u2026", "num_citations": "35\n", "authors": ["308"]}
{"title": "A Technique for Mutation of Java Objects\n", "abstract": " Mutation analysis inserts faults into a program to create test sets that distinguish the mutant from the original program. Inserted faults must represent plausible errors. Standard transformations can mutate scalar values such as integers, floats, and character data. Mutating objects is an open problem, because object semantics are defined by the programmer and can vary widely. We develop mutation operators and support tools that can mutate Java library items that are heavily used in commercial software. Our mutation engine can support reusable libraries of mutation components to inject faults into objects that instantiate items from these common Java libraries. Our technique should be effective for evaluating real-world software testing suites.", "num_citations": "34\n", "authors": ["308"]}
{"title": "Fault localization for automated program repair: effectiveness, performance, repair correctness\n", "abstract": " Automated program repair (APR) tools apply fault localization (FL) techniques to identify the locations of likely faults to be repaired. The effectiveness, performance, and repair correctness of APR depends in part on the FL method used. If FL does not identify the location of a fault, the application of an APR tool will not be effective\u2014it will fail to repair the fault. If FL assigns the actual faulty statement a low priority for repair, APR performance will be reduced by increasing the time required to find a potential repair. In addition, the correctness of a generated repair will be decreased since APR will modify fault-free statements that are assigned a higher priority for repair than an actual faulty statement. We conducted a controlled experiment to evaluate the impact of ten FL techniques on APR effectiveness, performance, and repair correctness using a brute force APR tool applied to faulty versions of the Siemens\u00a0\u2026", "num_citations": "33\n", "authors": ["308"]}
{"title": "Estimating the number of test cases required to satisfy the all-du-paths testing criterion\n", "abstract": " The all-du-paths software testing criterion is the most discriminating of the data flow testing criteria of Rapps and Weyuker. Unfortunately, in the worst case, the criterion requires an exponential number of test cases. To investigate the practicality of the criterion, we develop tools to count the number of complete program paths necessary to satisfy the criterion. This count is an estimate of the number of test cases required. In a case study of an industrial software system, we find that in eighty percent of the subroutines the all-du-paths criterion is satisfied by testing ten or fewer complete paths. Only one subroutine out of 143 requires an exponential number of test cases.", "num_citations": "33\n", "authors": ["308"]}
{"title": "Improving software testability with assertion insertion\n", "abstract": " Executable assertions can be inserted into a program to find software faults. Unfortunately, the process of designing and embedding these assertions can be expensive and time consuming. We have developed the C-Patrol tool to reduce the overhead of using assertions in C programs. C-Patrol allows a developer to reference a set of previously defined assertions, written in virtual C, bind assertion parameters, and direct the placement of the assertions by a pre-processor.", "num_citations": "32\n", "authors": ["308"]}
{"title": "Reported effects of rapid prototyping on industrial software quality\n", "abstract": " Empirical data are required to determine the effect of rapid prototyping on software quality. We examine 34 published and unpublished case studies of the use of rapid prototyping in \u2018real-world\u2019 software development. We identify common observations, unique events, and opinions. We develop guidelines to help software developers use rapid prototyping to maximize product quality and avoid common pitfalls.", "num_citations": "32\n", "authors": ["308"]}
{"title": "Using design abstractions to visualize, quantify, and restructure software\n", "abstract": " During design or maintenance, software developers often use intuition, rather than an objective set of criteria, to determine or recapture the design structure of a software system. A decision process based on intuition alone can miss alternative design options that are easier to implement, test, maintain, and reuse. The concept of design-level cohesion can provide both visual and quantitative guidance for comparing alternative software designs. The visual support can supplement human intuition; an ordinal design-level cohesion measure provides objective criteria for comparing alternative design structures. The process for visualizing and quantifying design-level cohesion can be readily automated and can be used to re-engineer software.", "num_citations": "31\n", "authors": ["308"]}
{"title": "A standard representation of imperative language programs for data collection and software measures specification\n", "abstract": " Software measures and software tools are often defined in terms of a particular, limited programming language. For example, a number of software measures are defined only for structured programs. Several approaches to program testing and debugging are defined using a specific simple language. As a result, implementing tools and measures so that they can be applied to \u201creal\u201d programs in \u201creal\u201d programming languages is difficult. Further, independent evaluation and comparison of measures and tools is difficult. We propose a standard representation of imperative language programs that is independent of the syntax of any particular programming language, and that supports the definition of a wide range of tools and measures. Additionally, the standard representation masks the actual program semantics. Thus the standard representation provides a vehicle by which large volumes of industrial software can be\u00a0\u2026", "num_citations": "29\n", "authors": ["308"]}
{"title": "Challenges of aspect-oriented technology\n", "abstract": " Aspect-oriented technology is a new programming paradigm that is receiving considerable attention from research and practitioner communities alike. It deals with those concerns that crosscut the modularity of traditional programming mechanisms, and its objectives include a reduction in the amount of code written and higher cohesion. As with any new technology, aspect-oriented technology has both benefits and costs. In this position paper, we explore these costs in terms of their impact on software engineering. We seek to understand both the strengths and limitations of this new technology, and to raise awareness of the potential negative side effects of its use.", "num_citations": "27\n", "authors": ["308"]}
{"title": "Using Design Cohesion to Visualize, Quantify, and Restructure Software.\n", "abstract": " During design or maintenance, software developers often use intuition, rather than an objective set of criteria, to determine or recapture the design structure of a software system. A decision process based on intuition alone can miss alternative design options that are easier to implement, test, maintain, and reuse. The concept of design-level cohesion can provide both visual and quantitative guidance for comparing alternative software designs. The visual support can supplement human intuition; an ordinal design-level cohesion measure provides objective criteria for comparing alternative design structures. The process for visualizing and quantifying design-level cohesion can be readily automated and can be used to re-engineer software.", "num_citations": "27\n", "authors": ["308"]}
{"title": "3\u2013D Visualization of Software Structure\n", "abstract": " A common and frustrating problem in software engineering is the introduction of new faults as a side-effect of software maintenance. An understanding of all of the relationships that exist between modified software and the rest of a system can limit the introduction of new faults. For large systems, these relationships can be numerous and subtle. The relationships can be especially complex in object-oriented systems that include inheritance and dynamic binding. Software visualization can potentially ease both impact analysis and general program understanding. Software visualization can facilitate program understanding by graphically displaying important software features. However, despite recent success in developing useful and intuitive graphical representations for certain aspects of software, current software visualization systems are limited by their lack of scalability\u2014the ability to visualize both small- and\u00a0\u2026", "num_citations": "26\n", "authors": ["308"]}
{"title": "Coping with Java programming stress\n", "abstract": " Despite Java attributes (memory management, strong type checking, and built-in support for exception handling) that promote reliable, bug-free software, some features contribute to, rather than alleviate, programmer stress because they create obscure places for bugs to hide. The authors have identified seven features that can lead to particularly resistant bugs. Their goal is not to indict Java-they are strong supporters, and their own organizations have adopted Java as their primary programming language. Rather, they want programmers to better understand Java's weaknesses and know how to cope with them. Being aware of these design weaknesses (Java's false sense of protection, constructor confusion, finalizer methods, subclass substitution, container limitations, final parameters, and initialization diffusion), programmers can make sure that Java's design flaws don't make implementation more difficult than it\u00a0\u2026", "num_citations": "25\n", "authors": ["308"]}
{"title": "Identifying Essential Competencies of Software Engineers.\n", "abstract": " The knowledge and skills of software engineers are perhaps the most important factors in determining the success of software development. Thus, we seek to identify the professional competencies that are most essential. In the rst phase of our research, we use the Critical Incident Interview technique to identify essential competencies. The Critical Incident Interview technique is a rigorous method for determining critical job requirements from structured interviews with workers. We use this technique in an in-depth review of 20 professional software engineers employed by a major computer rm. Our review includes an evaluation of biographical and Critical Incidence Interview data for 10 exceptional and 10 non-exceptional subjects. We also analyze competencies identi ed by software managers. We identify 38 essential competencies of software engineers. Di erences between exceptional and non-exceptional subjects were not expected in this rst phase of our research. We studied exceptional and non-exceptional engineers to ensure that all competencies are uncovered. Subject areas: software engineering, large software development, software teams, knowledge and skills of software engineers, software productivity, software psychology", "num_citations": "23\n", "authors": ["308"]}
{"title": "An assessment of the quality of automated program operator repair\n", "abstract": " Automated program repair (APR) techniques fix faults by repeatedly modifying suspicious code until a program passes a set of test cases. Although generating a repair is the goal of APR, a repair can have negative consequences. The quality of a repair is reduced when the repair introduces new faults and/or degrades maintainability by adding irrelevant but functionally benign code. We used two APR approaches to repair faulty binary operators: (1) find a repair in existing code by applying a genetic algorithm to replace suspicious code with other existing code as done by GenProg, and (2) mutate suspicious operators within a genetic algorithm. Mutating operators was clearly more effective in repairing faulty operators than using existing code for a repair. We also evaluated the approaches in terms of two potential negative effects: (1) the introduction of new faults and (2) a reduction of program maintainability. We\u00a0\u2026", "num_citations": "22\n", "authors": ["308"]}
{"title": "Measurement of language-supported reuse in object-oriented and object-based software\n", "abstract": " A major benefit of object-oriented software development is the support for reuse provided by object-oriented and object-based languages. Yet, measures and measurement tools that quantify such language-supported reuse have been lacking. Comprehensive reuse measures, particularly for reuse with modifications, are necessary to evaluate the status of reuse in an organization and to monitor improvements. We develop a set of measurable reuse attributes appropriate to object-oriented and object-based systems and a suite of measures that quantify these attributes. One of our major objectives is to measure reuse in software written in the object-based language Ada. A set of suitable primitive reuse measures are expressed in Ada Reuse Tables. These tables support the flexible use of primitive measures in programs with nested packages and subprograms, and Ada generic packages. We designed and\u00a0\u2026", "num_citations": "21\n", "authors": ["308"]}
{"title": "Designing for Software Testability Using Automated Oracles.\n", "abstract": " Software testing often requires massive numbers of test cases that must be manually inspected for correctness. This paper demonstrates the use of software test oracles\" to automate the process of checking the correctness of program output. The Prosper system, implemented by the authors, can be used to de ne test oracles and monitor the runtime behavior of software. An e ective method to design software for testability must include the concurrent development of test oracles.", "num_citations": "21\n", "authors": ["308"]}
{"title": "An analysis of software structure using a generalized program graph\n", "abstract": " Analyses of GPG representations show the interconnection between control flow constructs and data dependencies. Using simple transformations, a GPG can be converted into a control flow graph or a data dependency graph and can therefore be used to study control and data dependency issues in isolation. Because the GPG includes both the control and dependency information in one abstraction, the GPG can be used as an implementation model for the development of measurement and analysis tools for empirical research.In order to study the interface between control structure and data dependencies, we use a generalized program graph (GPG). The GPG incorporates features of both the CFG and the DDG. The GPG representation is essentially equivalent to the string form of a program. However, because of its graph structure, a GPG is better suited for structural analysis than the original string form.", "num_citations": "21\n", "authors": ["308"]}
{"title": "Syntactic fault patterns in oo programs\n", "abstract": " Although program faults are widely studied, there are many aspects of faults that we still do not understand, particularly about OO software. In addition to the simple fact that one important goal during testing is to cause failures and thereby detect faults, a full understanding of the characteristics of faults is crucial to several research areas. The power that inheritance and polymorphism brings to the expressiveness of programming languages also brings a number of new anomalies and fault types. In prior work we presented a fault model for the appearance and realization of OO faults that are specific to the use of inheritance and polymorphism. Many of these faults cannot appear unless certain syntactic patterns are used. The patterns are based on language constructs, such as overriding methods that directly define inherited state variables and non-inherited methods that call inherited methods. If one of these syntactic\u00a0\u2026", "num_citations": "20\n", "authors": ["308"]}
{"title": "Migrating legacy software systems to CORBA based distributed environments through an automatic wrapper generation technique\n", "abstract": " One of the strategies for migrating legacy systems to distributed object-oriented environments is wrapping. Wrapping is a method of encapsulation that provides wellknown interfaces for accessing legacy systems. The advantage of wrapping is that legacy systems become part of the new generation of applications without discarding the value of the legacy applications. There, however, are many styles for interfacing with legacy systems. Application developers who want to migrate legacy systems to new environments and to use them have the burden of understanding and implementing various interfacing techniques. To solve this problem, we construct the extensible wrapping template classes for various interfacing styles and present an automatic wrapper generation method based on them.", "num_citations": "20\n", "authors": ["308"]}
{"title": "Design-level cohesion measures: derivation, comparison, and applications\n", "abstract": " Cohesion was first developed to predict properties of implementations created from a given design. Unfortunately, cohesion, as originally defined, could not be objectively assessed, while more recently developed objective cohesion measures depend on code level information. We show that association based and slice based approaches can be used to measure cohesion using only design level information. Our design level cohesion measures are formally defined, can be readily implemented, and can support software design, maintenance, and restructuring.", "num_citations": "19\n", "authors": ["308"]}
{"title": "Generating input data structures for automated program testing\n", "abstract": " Automatic test data generation usually concerns identifying input values that cause a selected path to execute. If a given path involves pointers, then input values may be represented in terms of two\u2010dimensional dynamic data structures such as lists or trees. Thus, it is very important to identify the shape of the input data structure describing how many nodes are required and how nodes are connected each other. The approach presented in this paper makes use of the points\u2010to information for each statement in the selected path for the shape generation. It also converts each statement into a static single assignment (SSA) form without pointer dereferences. The SSA form serves as a system of constraints to be solved to yield input values for non\u2010pointer types. An empirical evaluation shows that shape generation can be achieved in linear time in terms of the number of pointer dereference operations. Copyright \u00a9\u00a0\u2026", "num_citations": "17\n", "authors": ["308"]}
{"title": "Mesa: Automatic generation of lookup table optimizations\n", "abstract": " Scientific programmers strive constantly to meet performance demands. Tuning is often done manually, despite the significant development time and effort required. One example is lookup table (LUT) optimization, a technique that is generally applied by hand due to a lack of methodology and tools. LUT methods reduce execution time by replacing computations with memory accesses to precomputed tables of results. LUT optimizations improve performance when the memory access is faster than the original computation, and the level of reuse is sufficient to amortize LUT initialization. Current practice requires programmers to inspect program source to identify candidate expressions, then develop specific LUT code for each optimization. Measurement of LUT accuracy is usually ad hoc, and the interaction with multicore parallelization has not been explored.", "num_citations": "16\n", "authors": ["308"]}
{"title": "Metric development for object-oriented software\n", "abstract": " A programming paradigm of great interest and activity is objectoriented programming and design. The reasons to develop and use software metrics for software implemented in imperative programming languages also apply to software developed using objectoriented programming languages. Perhaps metrics are even more important in the object-oriented paradigm, because, in many ways, object-oriented programming can be even more complex than imperative programming.A foundation like measurement theory is critical for the development of metrics for object-oriented programs. Much of the work done in developing metrics for imperative programs was ad hoc. Users and researchers have tended to look at particular problems or needs and have come up with specialized solutions. The obvious di culty is that these problems have for the most part been approached in an ad hoc manner without any theory or foundation to give guidance in the development of the solutions. Thus, there is no reliable way to understand how metrics for imperative programs can be adapted {if they can be {for object-oriented programs, and there is no real experience except for ad hoc methods that can be used to develop metrics for a new programming paradigm such as object-orientation. Thus, the work presented in this chapter is important for its own sake and as a way of gaining", "num_citations": "16\n", "authors": ["308"]}
{"title": "Finding code on the world wide web: A preliminary investigation\n", "abstract": " To find out what kind of design structures programmers really use, we need to examine a wide variety of programs. Unfortunately, most program source code is proprietary and is unavailable for analysis. The World Wide Web (Web) potentially can provide a rich source of programs for study. The freely available code on the Web, if in sufficient quality and quantity, can provide a window into software design as it is practiced today. In a preliminary study of source code availability on the Web, we estimate that 4% of URLs contain object-oriented source code, and 9% of URLs contain executable code: either binary or class files. This represents an enormous resource for program analysis. We can, with some risk of inaccuracy, conservatively project our sampling results to the entire Web. Our estimate is that the Web contains at least 3.4 million files containing either Java, C++, or Perl source code, 20.3 million files\u00a0\u2026", "num_citations": "14\n", "authors": ["308"]}
{"title": "A test driven approach for aspectualizing legacy software using mock systems\n", "abstract": " Aspect-based refactoring, called aspectualization, involves moving program code that implements cross-cutting concerns into aspects. Such refactoring can improve the maintainability of legacy systems. Long compilation and weave times, and the lack of an appropriate testing methodology are two challenges to the aspectualization of large legacy systems. We propose an iterative test driven approach for creating and introducing aspects. The approach uses mock systems that enable aspect developers to quickly experiment with different pointcuts and advice, and reduce the compile and weave times. The approach also uses weave analysis, regression testing, and code coverage analysis to test the aspects. We developed several tools for unit and integration testing. We demonstrate the test driven approach in the context of large industrial C++ systems, and we provide guidelines for mock system creation.", "num_citations": "13\n", "authors": ["308"]}
{"title": "Predicting fault detection effectiveness\n", "abstract": " Regression methods are used to model software fault detection effectiveness in terms of several product and testing process measures. The relative importance of these product/process measures for predicting fault detection effectiveness is assessed for a specific data set. A substantial family of models is considered, specifically, the family of quadratic response surface models with two way interaction. Model selection is based on \"leave one out at a time\" cross validation using the predicted residual sum of squares (PRESS) criterion. Prediction intervals for fault detection effectiveness are used to generate prediction intervals for the number of residual faults conditioned on the observed number of discovered faults. High levels of assurance about measures like fault detection effectiveness (residual faults) require more than just high (low) predicted values, they also require that the prediction intervals have high lower\u00a0\u2026", "num_citations": "12\n", "authors": ["308"]}
{"title": "Fundamental issues in software measurement\n", "abstract": " This chapter builds on the previous one. In the previous chapter the emphasis is on measurement theory, and software measurement is discussed as a type of measurement. In this chapter the emphasis is on software measurement, and ideas from measurement theory are used to guide and direct our development of software measurement ideas and methods.", "num_citations": "12\n", "authors": ["308"]}
{"title": "Tool support for software lookup table optimization\n", "abstract": " A number of scientific applications are performance-limited by expressions that repeatedly call costly elementary functions. Lookup table (LUT) optimization accelerates the evaluation of such functions by reusing previously computed results. LUT methods can speed up applications that tolerate an approximation of function results, thereby achieving a high level of fuzzy reuse. One problem with LUT optimization is the difficulty of controlling the tradeoff between performance and accuracy. The current practice of manual LUT optimization adds programming effort by requiring extensive experimentation to make this tradeoff, and such hand tuning can obfuscate algorithms.", "num_citations": "11\n", "authors": ["308"]}
{"title": "Software design quality: Style and substance\n", "abstract": " Many software development texts, references, tools, and authorities provide advise on good software design and programming styles. Unfortunately, most of the evidence to support the value of this advise consists of intuition and anecdotes. We are working to objectively determine the value of recommended style guides on large-scale real world software systems we are studying both proprietary commercial and open source systems. Our work involves determining whether or not style recommendations are followed, and how these styles affect external quality factors such as faultand change-proneness, and maintainability. Early results indicate that style guidelines are often violated. In addition, we have found that, in contrast with common claims, one design recommendation| the use of design patterns| can lead to more change prone, rather than less change prone classes.", "num_citations": "11\n", "authors": ["308"]}
{"title": "Testing during refactoring: Adding aspects to legacy systems\n", "abstract": " Moving program code that implements cross-cutting concerns into aspects can improve the maintainability of legacy systems. This kind of refactoring, called aspectualization, can also introduce faults into a system. A test driven approach can identify these faults during the refactoring process so that they can be removed. We perform systematic testing as we aspectualize commercial VLSI CAD applications. The process of refactoring these applications revealed the kinds of faults that can arise during aspectualization, and helped us to develop techniques to reduce their occurrences", "num_citations": "10\n", "authors": ["308"]}
{"title": "Prosper: A language for specification by prototyping\n", "abstract": " The PROSPER functional specification language supports a \u201cspecification by prototyping\u201d paradigm and relies on a unique and powerful type specification facility. Executable polymorphic specifications can be built from a small set of primitives. Types and functions are treated as values and can be the arguments and results of functions. Flexible parameterized type expressions are used to specify polymorphic functions and abstract data types. Abstract model specifications can be defined so that the invariants of a data structure can be part of the type, and only objects that meet the invariant are correctly typed.", "num_citations": "10\n", "authors": ["308"]}
{"title": "Measuring software data dependency complexity\n", "abstract": " Although much research has been directed towards measuring the complexity of the programmer/program interface, most of this work has centered on the measurement of flow of control complexity. Experiments, as reported in the literature, have demonstrated that programmers seem to work backwards when debugging, examining only instructions that affect the variables in error. The reported results demonstrate the importance of data dependencies and indicate that a measure of data dependency complexity would be a useful tool for the development of reliable and maintainable software.", "num_citations": "10\n", "authors": ["308"]}
{"title": "Using Cell Phones for Mosquito Vector Surveillance and Control.\n", "abstract": " Novel, low-cost approaches to improving prevention and control of vector-borne diseases, such as mosquito-borne dengue and malaria, are needed in resource-constrained environments. The Chaak application supports the use of cell phones for field capture and rapid transfer of mosquito vector surveillance data to a central database. The cell phones exploit existing communication infrastructure, introduce near real-time monitoring, and provide rapid feedback to field data collectors. Dengue is a mostly an urban disease, thus occurring in environments that often have good cell phone coverage. Cell phones eliminate the need for physical data communication. A preliminary evaluation shows that the use of cell phones can lower labor costs, data collection time, and transcription errors.", "num_citations": "9\n", "authors": ["308"]}
{"title": "Adding formal specifications to a proven V&V process for system-critical flight software\n", "abstract": " The process used to validate, verify, and test flight avionics control systems has produced software that is highly reliable. However, ever greater demands for reliability require new automated tools to improve existing processes. We used the Anna formal specification language and supporting tool set to develop a Test Range Oracle Tool (TROT) to automate the testing of equation execution. Our approach fits within the existing testing process and can increase the level of test coverage without increasing testing costs. The TROT approach introduces the use of formal specification languages and supporting tools to an existing industry program. This approach is being evaluated for expansion into other test support areas.", "num_citations": "8\n", "authors": ["308"]}
{"title": "Is Anyone Listening?\n", "abstract": " Software development disasters continue to make the headlines. Generally, the most highly publicized fiascos are in systems developed to support government functions. Such failures can harm the public, and the failures of systems developed with taxpayer money cannot be as readily hidden from public view. What I find appalling is that many if not most of the disaster could have been prevented if good software assurance practices had been applied. The most recent software disaster to really irk me affects people close to my home. The dysfunctional system is called the Colorado Benefits Management System (CBMS), a system intended to realize the \u201cdream of improving access to public assistance and medical benefits for clients\u201d(Colorado Benefits Management System). This system is supposed to help those in society with the greatest need: the poor, ill, and disabled. According to the Colorado Department of\u00a0\u2026", "num_citations": "7\n", "authors": ["308"]}
{"title": "The Illusive Nature of Quality\n", "abstract": " Quality... you know what it is, yet you don\u2019t know what it is. But that\u2019s selfcontradictory. But some things are better than others, that is, they have more quality. But when you try to say what the quality is, apart from the things that have it, it all goes poof!... But for all practical purposes, it does exist.... Why else would people pay fortunes for some things and throw others in the trash pile?(Pirsig, 1974, pp. 163\u2013164)Years ago, when I was studying the relationship between software metrics and measurement theory, my colleagues and I discovered that developing a single measure that encompasses many of the quality \u201cilities\u201d\u2014reliability, maintainability, understandability, testability, fault-tolerance etc, which often conflict, would only produce a measure that could not distinguish between most programs. At that time, the book Zen and the Art of Motorcycle Maintenance by Robert Pirsig seemed relevant. When I read Jeff Voas\u2019s\u00a0\u2026", "num_citations": "7\n", "authors": ["308"]}
{"title": "Measuring software reuse in object oriented systems and ada software\n", "abstract": " The measurement of levels of software reuse is necessary to monitor improvements in software reuse. We develop a set of measurable reuse attributes appropriate to object oriented systems and a suite of metrics quantifying these attributes. Metrics suitable for the object based language Ada are identified. To measure the proposed reuse metrics a prototype Ada Reuse Metric Analyzer (ARMA) is implemented. The usefulness of ARMA is illustrated by measuring primitive reuse attributes for an Ada software system. Limitations of the tool and possible future directions are also discussed.", "num_citations": "7\n", "authors": ["308"]}
{"title": "The impact of search algorithms in automated program repair\n", "abstract": " Automated program repair (APR) techniques locate and fix faults automatically. In order to fix faults, APR applies a set of program modification operators (PMOs) to modify faulty programs. A potential repair is found when APR applies a PMO that fixes a fault. A brute-force search algorithm applies all PMOs in a predefined order until a potential repair is found. Brute-force can guarantee a fix but lowers APR performance, especially when it uses many PMOs. Stochastic search algorithms, such as a genetic algorithm, efficiently search the modifications space for a PMO that fixes a fault. In this paper, we conduct a comprehensive evaluation of the impact on APR effectiveness, APR performance, and the quality of potential repairs of three stochastic search algorithms:(1) a genetic algorithm (GA), (2) a genetic algorithm without a crossover operator (GAWoCross), and (3) a random search (RS). Our evaluation using 41\u00a0\u2026", "num_citations": "6\n", "authors": ["308"]}
{"title": "Optimizing expression selection for lookup table program transformation\n", "abstract": " Scientific programmers can speed up function evaluation by precomputing and storing function results in lookup table (LUTs), thereby replacing costly evaluation code with an inexpensive memory access. A code transform that replaces computation with LUT code can improve performance, however, accuracy is reduced because of error inherent in reconstructing values from LUT data. LUT transforms are commonly used to approximate expensive elementary functions. The current practice is for software developers to (1) manually identify expressions that can benefit from a LUT transform, (2) modify the code by hand to implement the LUT transform, and (3) run experiments to determine if the resulting error is within application requirements. This approach reduces productivity, obfuscates code, and limits programmer control over accuracy and performance. We propose source code analysis and program\u00a0\u2026", "num_citations": "6\n", "authors": ["308"]}
{"title": "Inheritance tree shapes and reuse\n", "abstract": " The shapes of forests of inheritance trees can affect the amount of code reuse in an object-oriented system. Designers can benefit from knowing how structuring decisions affect reuse, so that they can make more optimal decisions. We show that a set of objective measures can classify forests of inheritance trees into a set of five shape classes. These shape classes determine bounds on reuse measures based on the notion of code savings. The reuse measures impart an ordering on the shape classes that demonstrates that some shapes have more capacity to support reuse through inheritance. An initial empirical study shows that the application of the measures and demonstrates that real inheritance forests can be objectively and automatically classified into one of the five shape classes.", "num_citations": "6\n", "authors": ["308"]}
{"title": "Software Metrics: A Rigorous & Practical Approach\n", "abstract": " A book review is presented of Software Metrics: A Rigorous & Practical Approach, Second Edition by Norman E. Fenton and Shari Lawrence Pfleeger.", "num_citations": "6\n", "authors": ["308"]}
{"title": "Using formal specifications as test oracles for system-critical software\n", "abstract": " The process used to validate, verify, and test flight avionics control systems has produced software that is highly reliable. However, ever greater demands for reliability require new automated tools to improve existing processes. We used the Anna (Annotated Ada) formal specification language and supporting tool set to develop a Test Range Oracle Tool (TROT) to automate the testing of equation execution. Our approach fits within the existing testing process, automates perviously manual analysis, and can increase the level of test coverage. The TROT approach also introduces the use of formal specification languages and supporting tools to an existing industry program. This approach supported production tests and is being expanded into other test support areas.", "num_citations": "6\n", "authors": ["308"]}
{"title": "MUT-APR: MUTation-based automated program repair research tool\n", "abstract": " Automated program repair (APR) techniques introduced to fix faults through the insertion of new code or modifying existing ones until the program under test passes a given set of test cases called repair tests. MUTation based Automated Program Repair (MUT-APR) is a prototype tool built on the top of GenProg to fix binary operator faults in C programs. MUT-APR is a configurable mutation-based APR tool that varies the APR mechanisms and components to find the best combination for a problem under study. The implementation of MUT-APR components is discussed in this paper as well as how the tool can be used to repair faults. Limitations of the existing framework are also discussed.", "num_citations": "5\n", "authors": ["308"]}
{"title": "Free/open source software, silver bullets, and mythical months\n", "abstract": " The notion of free/open source software (F/OSS) development is intriguing. In theory, such software is developed by volunteers who see the software as fulfilling their own needs. Developers work on what they want, anyone can have a copy of the source code and contribute towards improving the system by reporting and/or fixing bugs, and developing needed new functionality. The code is high quality because of the large number of people involved in reporting and fixing errors, and adding functionality. The F/OSS process should satisfy user and developer needs. In contrast, the list of software disasters produced through \u201ctraditional\u201d commercial development is staggering. You can easily find a long list of (the most frightening) software failures by searching for \u201csoftware disasters\u201d using your favorite search engine. Another indicator of problems with traditional development is the high proportion of software\u00a0\u2026", "num_citations": "5\n", "authors": ["308"]}
{"title": "What makes a software failure a page-one story?\n", "abstract": " The stories that reach page one are those that feed our imaginations. To the software quality community these stories have one common lesson. Professional software assurance activities could have identified most of these faults before the systems failed. In most cases, either testing or analysis should have discovered the problems. In some cases, particularly the Sony music CD problem, an outside consultant could have identified the severe risk of this, now clearly insane, copy protection scheme. We in the software quality research and practice community know that number and severity of software failures can be reduced. Independent assurance activities could have found most of these reported problems before they became press stories. As computer systems become more and more interwoven with aspects of human life, publicity of software failures will also increase. The need for software assurance research\u00a0\u2026", "num_citations": "5\n", "authors": ["308"]}
{"title": "Design pattern coupling, change proneness, and change coupling: A pilot study\n", "abstract": " A design pattern realization consists of a cluster of classes that work together to solve a particular problem using a well known, named solution. Developers may build systems out of several pattern realizations, and these pattern realizations may be interconnected, or, in other words, coupled. Coupled pattern realizations may represent a reasonable solution to software design problems, however the coupling can introduce dependencies that increase faultproneness and lower adaptability. We identify mechanisms that can couple pattern realizations, and evaluate the relative tightness of the connections. An examination of pattern coupling in five systems provides initial evidence that pattern coupling is common. In addition, we find initial evidence that classes in pattern realizations that are coupled via associations are (1) more change prone and (2) exhibit higher change coupling\u2014classes that are modified together in response to one required change\u2014than those in pattern realizations that are coupled by other mechanisms", "num_citations": "5\n", "authors": ["308"]}
{"title": "Moving from philosophy to practice in software measurement\n", "abstract": " We, the Grubstake Group, are committed to creating an environment in which software metrics can be used confidently, effectively, and in full knowledge of their limitations. We responded to a lack of rigor in software measurement, and concentrated our efforts towards the development of a suitable theory and foundation. We explained our concerns and beliefs for the development of the software metrics field in our paper entitled A Philosophy for Software Measurement.                We realize, however, that effective software measurement requires more than a philosophy. Thus, in this paper we continue the work needed so that effective software metrics can be routinely designed, specified and used. The design, specification and use of a good software metric should not be an unusual event.               In addition to discussing some foundational issues, we examine metric specifications, the use of statistics in the\u00a0\u2026", "num_citations": "5\n", "authors": ["308"]}
{"title": "Risks to software quality\n", "abstract": " As a software engineering researcher and teacher, I frequently hear comments about how awful our software is. Such comments come from both computer science and engineering academics, and practitioners. We all hear complaints about failures of commonly used applications such as browsers, word processors, presentation software, etc.From my experiences collaborating with industry, I know that many, if not most, software systems are developed in a very ad hoc manner. We professors know many students who are convinced that our lectures on software development techniques are irrelevant. Thus, I have ample reason to believe that most software is likely to be of low quality.", "num_citations": "4\n", "authors": ["308"]}
{"title": "Coding concerns: do they matter\n", "abstract": " In the heyday of C programming it was the requirement of many programming shops to check code for anomalies and potential bugs with a program called lint [4, 6]. As the quality of compilers improved, many of the lint functions were added to compilers as\u2019 warnings\u2019. But many programmers soon became tired of all the compiler warnings and turned most of them off. Today compilers come with just a few warnings turned on. A programmer wishes may set the flags to have the compiler provide additional warnings. As a result, many programs are compiled with the flags set to only a few of the warnings that the original lint programs provided.", "num_citations": "4\n", "authors": ["308"]}
{"title": "Using fault injection to test software recovery code\n", "abstract": " The software code used to respond to exceptional conditions is often the most error prone part of a system. Such code is designed to recover from very unusual events, and as a result it is di cult to nd input data to test this part of a program. One possible technique for testing software recovery code is to inject faults in a system during execution. The injected faults can force the recovery code to run so that it can be tested.We developed techniques to support the injection of faults in software for the purpose of testing error recovery code. We have developed the requirements for tools to support the fault injection techniques, and implemented a prototype tool, Visual C-Patrol VCP, to use the techniques. VCP was built as an extension to the C-Patrol code insertion tool that was developed during our FY93 and FY94 CASI Project with Storage Technology and Micro-Motion. We tested the technique on software from both academic and commercial sources. Using VCP, we were able to increase testing coverage, indicated by program branches covered, in all of the software studied. Test coverage increased by 10.7 in the one fault tolerant system studied.", "num_citations": "4\n", "authors": ["308"]}
{"title": "Using algebraic specifications to find sequencing defects\n", "abstract": " One class of program defect results from illegal sequences of otherwise legal operations in software implementations. Expressions that specify the correct sequences can be written in the Cecil sequencing constraint language. Programs can then be checked at compile time by the Cesar analysis system. Explicit statement of sequencing constraints, however, is not a common activity when specifying software even when using formal specification methods. In this paper, we describe methods to derive constraints on program execution sequences from algebraic specifications. We provide heuristic methods for generating these constraints from the specifications and generalize the methods into automatable rules. Using these generated constraints, we can then detect sequencing defects in software before dynamic testing begins.", "num_citations": "4\n", "authors": ["308"]}
{"title": "Automated metamorphic testing of scientific software\n", "abstract": " Scientific programs present many challenges for testing that do not typically appear in application software. These challenges make it difficult to conduct systematic testing on scientific programs. Due to a lack of systematic testing, subtle errors, errors that produce incorrect outputs without crashing the program, can remain undetected.ABSTRACT", "num_citations": "3\n", "authors": ["308"]}
{"title": "Election software exposed\n", "abstract": " Software quality problems are most exposed to the general public when failures occur in applications that are essential to social or economic processes. The problems are especially noticed when the processes are watched carefully by the news media. During the 2008 election process in the USA, media attention has focused on every election-related event, comment, nuance, and rumor. Problems in software that supports elections are most certainly noticed.Each year software systems play a greater role in conducting elections. These systems are used to tally votes and track voters. They tend to be distributed and operate in real time, and their correct performance is critical. Failures of all kinds are reported as front page news, and rumors of failures spread widely. Election software demonstrates to the public the quality, or lack of quality of our software systems. And because democratic institutions now depend on\u00a0\u2026", "num_citations": "3\n", "authors": ["308"]}
{"title": "The software process model\n", "abstract": " The authors define a software process model (SPM). The SPM is a model of evolution of the full set of documents produced in a software project. It is a general model for software development using any development approach, providing a framework for measuring, analyzing, and understanding the software development process. The authors use the model to characterize formally research in software measures and metrics. Data obtained from two software projects are presented in the SPM format.<>", "num_citations": "3\n", "authors": ["308"]}
{"title": "Emerging models to improve storage management techniques in cloud computing environments\n", "abstract": " Services based on cloud computing technologies have become increasingly widespread and pervasive. In recent years the number of cloud provider has increased and there is a growing need to differentiate themselves in the market with more advanced services. The most common services concern cloud platforms provide storage on demand and ensure customer capacity and flexibility through innovative models. Therefore, the technologies for the management and storage of data is undergoing major changes. New methodologies such as Software-Defined Storage and Storage Orchestration are crucial to better address the design and development of cloud architectures in last generation data center. In this paper has conducted an analysis of the methods and software tools for storage management that promise to be challenge in the short term as they improve the technological aspects of the storage systems.", "num_citations": "2\n", "authors": ["308"]}
{"title": "An optimization\u2010based approach to lookup table program transformations\n", "abstract": " Scientific programmers can improve the performance of function evaluation by precomputing and storing results in a lookup table (LUT), thereby replacing costly evaluation code with an inexpensive memory access. A code transformation that replaces computation with LUT code can improve performance; however, accuracy is reduced because of error inherent in reconstructing values from LUT data. LUTs are commonly used to approximate expensive elementary functions. The current practice is for software developers to: (i) manually identify expressions that can benefit from an LUT; (ii) modify the code by hand to implement the LUT transformation; and (iii) run experiments to determine if the resulting error is within application requirements. This approach reduces productivity, obfuscates code, and limits programmer control over accuracy and performance. We propose source code analysis and program\u00a0\u2026", "num_citations": "2\n", "authors": ["308"]}
{"title": "Evaluating the separation of algorithm and implementation within existing programming models\n", "abstract": " Implementation details and performance tuning obfuscate the algorithms in programs. In the SAIMI project (Separating Algorithm and Implementation via Programming Model Injection), we are investigating ways to use simpler, more restricted programming models to orthogonally express important subcomputations and express implementation details as transformations on the subcomputations. In this initial phase of the project, we are evaluating separation in existing programming models especially with respect to look-up table, grid-based, sparse computation, and task graph programming models. In this paper, we illustrate the general approach of the SAIMI project with a source-to-source compilation tool called Mesa that automates most aspects of applying look-up transformations in scientific codes; we use the CGPOP miniapp as an example geoscience code to discuss the coupling between the discretization grid implementation details and the communication routines that support the simulation algorithms; and we evaluate the ability of existing programming languages such as Cilk, TBB, CnC, and OpenMP to represent general, dynamic task graphs that arise in sparse computations. We conclude with future directions for developing orthogonal implementation abstractions and building on the orthogonal capabilities in existing programming languages.", "num_citations": "2\n", "authors": ["308"]}
{"title": "Aspect-oriented technology and software quality\n", "abstract": " Aspect-oriented technology is a new programming paradigm that is receiving considerable attention from both the research and practitioner communities. Aspect-orientation involves software development concerns that crosscut the modularity of traditional programming mechanisms. Among the claimed benefits of this technology is a reduction in the amount of code written and higher cohesion. As with any new technology, aspect-oriented technology has both benefits and costs (Alexander, 2003). Here we examine these costs in terms of their impact on software engineering. We seek to understand both the strengths and limitations of this new technology. However, here we aim to raise awareness of the potential negative side effects of its use.", "num_citations": "2\n", "authors": ["308"]}
{"title": "The role of prognostication in software design\n", "abstract": " Software design decisions are based in large part on expectations about how a system will evolve. Both the quality of an evolving system and the ease of adapting it depend on the early design. The only hope for making informed design decisions leading to systems that remain high-quality and adaptable is to improve the ability of designers to prognosticate. Rather than use a crystal ball, comprehensive studies of how existing systems have evolved in the past can provide solid evidence into the connection between early design decisions and the evolving adaptability and quality of software systems.", "num_citations": "2\n", "authors": ["308"]}
{"title": "A Tool for Estimating Software Testing Requirements\n", "abstract": " We describe a prototype software tool that estimates the number of test cases required to apply particular testing strategies to program subroutines. The tool was used to evaluate the practicability of data ow testing of a commercial text analysis system which runs on microcomputers. Our system was developed from formal speci cations and is implemented in Prolog. It is easily adaptable and can be used to evaluate proposed testing techniques, estimate the resources required to test a software system, and identify hard-to-test subroutines. The testing e ort estimating tool is one component of a software analysis research environment.", "num_citations": "2\n", "authors": ["308"]}
{"title": "Fault Localization for Automated Program Repair: Effectiveness and Performance\n", "abstract": " Automated program repair (APR) tools apply fault localization (FL) techniques to identify the locations of likely faults to be repaired. The effectiveness and performance of APR depends in part on the FL method used. If FL does not identify the location of a fault, the application of an APR tool will not be effective\u2014it will fail to repair the fault. If FL assigns a faulty location a low priority for repair, the performance of APR will be reduced, increasing the time required to find a repair. In this paper, we evaluate the impact of five FL techniques (Jaccard, Optimal, Ochiai, Tarantula, and the GenProg Weighting Scheme) on the effectiveness and performance of a brute force APR tool when applied to faulty versions of the Siemens Suite and two other large programs: space and sed. All FL techniques were effective in identifying all faults except Optimal which failed to identify faulty locations in two faulty versions of the space subject program. We obtained the best APR performance when Optimal was used. However, Ochiai\u2019s performance was noteworthy since it always assigned faulty statements at an equal or higher priority for repair than other FL techniques with acceptable performance.", "num_citations": "1\n", "authors": ["308"]}
{"title": "An optimization-based approach to LUT program transformations\n", "abstract": " Scientific programmers can speed up function evaluation by precomputing and storing function results in lookup table (LUTs), thereby replacing costly evaluation code with an inexpensive memory access. A code transform that replaces computation with LUT code can improve performance, however, accuracy is reduced because of error inherent in reconstructing values from LUT data. LUT transforms are commonly used to approximate expensive elementary functions. The current practice is for software developers to (1) manually identify expressions that can benefit from a LUT transform,(2) modify the code by hand to implement the LUT transform, and (3) run experiments to determine if the resulting error is within application requirements. This approach reduces productivity, obfuscates code, and limits programmer control over accuracy and performance. We propose source code analysis and program transformation to substantially automate the application of LUT transforms. Our approach uses a novel optimization algorithm that selects Pareto Optimal sets of expressions that benefit most from LUT transformation, based on error and performance estimates. We demonstrate our methodology with the Mesa tool, which achieves speedups of 1.4-6.9\u00d7 on scientific codes while managing introduced error. Our tool makes the programmer more productive and improves the chances of finding an effective solution. Copyright\u00a9 2013 John Wiley & Sons, Ltd.", "num_citations": "1\n", "authors": ["308"]}
{"title": "Why good software goes bad\n", "abstract": " In preparing several courses for my department\u2019s new on-line master degree program, I have been using a courseware software system that supports the delivery of academic courses over the worldwide web. This system was adopted by my university and is used by faculty in a wide-range of departments: courses in the humanities, physics, veterinary medicine, engineering, and, of course, computer science. The system users\u2019 computer backgrounds range from na\u00efve to very savvy. Faculty and support staff members use an email list to post problems and solutions to using the system. Users\u2019 problems can be very basic, from how to use basic browser and editing features, to more sophisticated problems related to the interface between browsers and the system. Some of the problems are due to the awkwardness of the system interface; other problems are caused by the system design. Users can solve many of the\u00a0\u2026", "num_citations": "1\n", "authors": ["308"]}
{"title": "Do Design Patterns Create More Maintainable Systems?\n", "abstract": " gn patterns. Thus, maintenance programmers should have to expend less e# ort to modify these systems. Do design patterns really improve the maintainabilityofasystem? We are trying to answer this question byinvestigating the evolution of software systems that make use of design patterns. The goal is to determine the e# ect of particular design structures on future versions of systems. We are now in middle of a study of 40 versions of a commercial software system consisting of 230 classes and more than 32,000 lines of C++ code. One objective of this research is to demonstrate the bene# ts of the design patterns in this system. The operational hypothesis representing this objective is the following: Hypothesis: Classes that play roles in design patterns require fewer changes than non-pattern classes as the software evolves. Research supported by a grant from the Colorado Advanced Software Institute# CASI#. CASI is sponsored in part by the Colorado Commission on Higher Education,", "num_citations": "1\n", "authors": ["308"]}
{"title": "Automated Test Data Generation Using a Relational Approach\n", "abstract": " In general, test data generation techniques require an entire program path for automated test data generation. This paper presents a new way for generating test data automatically even without specifying a program path completely. The proposed method reduces the burden of selecting a program path and also makes it easy to generate test data according to various test adequacy criteria. For the ends, this paper presents a framework for transforming a program under test into Alloy which is the first-order relational logic and then producing test data via Alloy analyzer. This paper illustrates the proposed method through simple, but illustrative examples.", "num_citations": "1\n", "authors": ["308"]}
{"title": "An Empirical Evaluation (and Specification) of the All-du-paths Testing Criterion: Extended Version\n", "abstract": " The all-du-paths structural testing criterion is one of the most discriminating of the data ow testing criteria. Unfortunately, in the worst case, the criterion requires an intractable number of test cases. In a case study of an industrial software system, we nd that the worst case scenario is rare. Eighty percent of the subroutines require ten or fewer test cases. Only one subroutine out of 143 requires an intractable number of tests. However, the number of required test cases becomes tractable when using the all-uses criterion. This paper includes a formal speci cation of both the all-du-paths criterion and the software tools used to estimate a minimal number of test cases necessary to meet the criterion.", "num_citations": "1\n", "authors": ["308"]}
{"title": "Interlingua: A multilanguage business information center\n", "abstract": " The InterLingua information processing system is proposed as a practical approach to providing analysis and access to large sets of multilanguage publications pertinent to a given industry and a particular world region. An existing natural language text analysis system (NLTAS) provides the base technology. The NLTAS facilitates construction of knowledge bases of the ideas that occur in verbatim text. The system is based on characterizing expressions of ideas as patterns of words. The NLTAS is language independent for all languages that have a finite set of characters. These knowledge bases can then be used to (i) partition large volumes of text into sets of text on narrower topics,(ii) generate intelligent indices to large sets of multilanguage documents without the high cost of verbatim translation,(iii) provide the basis for machine-aided verbatim translation services, and (iv) serve as the statistical basis for the\u00a0\u2026", "num_citations": "1\n", "authors": ["308"]}