{"title": "What consistency does your key-value store actually provide?\n", "abstract": " Many key-value stores have recently been proposed as platforms for always-on, globally-distributed, Internetscale applications. To meet their needs, these stores often sacrifice consistency for availability. Yet, few tools exist that can verify the consistency actually provided by a key-value store, and quantify the violations if any. How can a user check if a storage system meets its promise of consistency? If a system only promises eventual consistency, how bad is it really? In this paper, we present efficient algorithms that help answer these questions. By analyzing the trace of interactions between the client machines and a key-value store, the algorithms can report whether the trace is safe, regular, or atomic, and if not, how many violations there are in the trace. We run these algorithms on traces of our eventually consistent keyvalue store called Pahoehoe and find few or no violations, thus showing that it often behaves like a strongly consistent system during our tests.", "num_citations": "130\n", "authors": ["471"]}
{"title": "MediaBench II video: Expediting the next generation of video systems research\n", "abstract": " The first step towards the design of video processors and systems is to achieve an understanding of the major applications, including not only the theory, but also the workload characteristics of the many image and video compression standards. Introduced in 1997, the MediaBench benchmark suite provided the first set of full application-level benchmarks for multimedia, and has consequently enabled significant research in computer architecture and compiler research for media systems. To expedite the next generation of multimedia systems research, we are developing the MediaBench II benchmark suite, incorporating benchmarks from the latest multimedia technologies, and providing both a single composite benchmark suite (MB 2 comp) as well as separate sub-suites for each area of multimedia. For video, MediaBench II Video (MB 2 video) includes both the popular mainstream video compression standards\u00a0\u2026", "num_citations": "92\n", "authors": ["471"]}
{"title": "Efficiency matters!\n", "abstract": " Current data intensive scalable computing (DISC) systems, although scalable, achieve embarrassingly low rates of processing per node. We feel that current DISC systems have repeated a mistake of old high-performance systems: focusing on scalability without considering efficiency. This poor efficiency comes with issues in reliability, energy, and cost. As the gap between theoretical performance and what is actually achieved has become glaringly large, we feel there is a pressing need to rethink the design of future data intensive computing and carefully consider the direction of future research.", "num_citations": "86\n", "authors": ["471"]}
{"title": "In-memory performance for big data\n", "abstract": " When a working set fits into memory, the overhead imposed by the buffer pool renders traditional databases non-competitive with in-memory designs that sacrifice the benefits of a buffer pool. However, despite the large memory available with modern hardware, data skew, shifting workloads, and complex mixed workloads make it difficult to guarantee that a working set will fit in memory. Hence, some recent work has focused on enabling in-memory databases to protect performance when the working data set almost fits in memory. Contrary to those prior efforts, we enable buffer pool designs to match in-memory performance while supporting the\" big data\" workloads that continue to require secondary storage, thus providing the best of both worlds. We introduce here a novel buffer pool design that adapts pointer swizzling for references between system objects (as opposed to application objects), and uses it to practically eliminate buffer pool overheads for memoryresident data. Our implementation and experimental evaluation demonstrate that we achieve graceful performance degradation when the working set grows to exceed the buffer pool size, and graceful improvement when the working set shrinks towards and below the memory and buffer pool sizes.", "num_citations": "61\n", "authors": ["471"]}
{"title": "Disks Are Like Snowflakes: No Two Are Alike.\n", "abstract": " Gone are the days of homogeneous sets of disks. Even disks of a given batch, of the same make and model, will have significantly different bandwidths. This paper describes the disk technology trends responsible for the now-inherent heterogeneity of multi-disk systems and disk-based clusters, provides measurements quantifying it, and discusses its implications for system designers.", "num_citations": "57\n", "authors": ["471"]}
{"title": "Mediabench II video: expediting the next generation of video systems research\n", "abstract": " The first step towards the design of video processors and video systems is to achieve an accurate understanding of the major video applications, including not only the fundamentals of the many video compression standards, but also the workload characteristics of those applications. Introduced in 1997, the MediaBench benchmark suite provided the first set of full application-level benchmarks for studying video processing characteristics, and has consequently enabled significant research in computer architecture and compiler research for multimedia systems. To expedite the next generation of systems research, the MediaBench Consortium is developing the MediaBench II benchmark suite, incorporating benchmarks from the latest multimedia technologies, and providing both a single composite benchmark suite as well as separate benchmark suites for each area of multimedia. In the area of video, MediaBench II\u00a0\u2026", "num_citations": "55\n", "authors": ["471"]}
{"title": "Multi-geography cloud storage\n", "abstract": " A multi-geography cloud storage system includes a first data center, with a first key-lookup server to access a first lookup table; and a first fragment server to store data or meta data associated with keys; and a second data center, with a second key-lookup server to access a second lookup table; and a second fragment server to store data associated with the keys; and a storage device to store a redundancy specification.", "num_citations": "44\n", "authors": ["471"]}
{"title": "Efficient eventual consistency in Pahoehoe, an erasure-coded key-blob archive\n", "abstract": " Cloud computing demands cheap, always-on, and reliable storage. We describe Pahoehoe, a key-value cloud storage system we designed to store large objects cost-effectively with high availability. Pahoehoe stores objects across multiple data centers and provides eventual consistency so to be available during network partitions. Pahoehoe uses erasure codes to store objects with high reliability at low cost. Its use of erasure codes distinguishes Pahoehoe from other cloud storage systems, and presents a challenge for efficiently providing eventual consistency. We describe Pahoehoe's put, get, and convergence protocols-convergence being the decentralized protocol that ensures eventual consistency. We use simulated executions of Pahoehoe to evaluate the efficiency of convergence, in terms of message count and message bytes sent, for failure-free and expected failure scenarios (e.g., partitions and server\u00a0\u2026", "num_citations": "28\n", "authors": ["471"]}
{"title": "Prioritizing recovery in a storage system implementing raid\n", "abstract": " A method for determining priority of recovery for a RAID implementation includes detecting a first failure of the RAID implementation; detecting a second failure of the RAID implementation; assigning a first priority to the first failure and a second priority to the second failure; and setting the priority of the recovery based on the first priority and the second priority.", "num_citations": "24\n", "authors": ["471"]}
{"title": "Applying performance models to understand data-intensive computing efficiency\n", "abstract": " New programming frameworks for scale-out parallel analysis, such as MapReduce and Hadoop, have become a cornerstone for exploiting large datasets. However, there has been little analysis of how these systems perform relative to the capabilities of the hardware on which they run. This paper describes a simple analytical model that predicts the optimal performance of a parallel dataflow system. The model exposes the inefficiency of popular scale-out systems, which take 3-13x longer to complete jobs than the hardware should allow, even in well-tuned systems used to achieve record-breaking benchmark results. To validate the sanity of our model, we present small-scale experiments with Hadoop and a simplified dataflow processing tool called Parallel DataSeries. Parallel DataSeries achieves performance close to the analytic optimal, showing that the model is realistic and that large improvements in the efficiency of parallel analytics are possible.Descriptors:", "num_citations": "22\n", "authors": ["471"]}
{"title": "Efficient tracing and performance analysis for large distributed systems\n", "abstract": " Distributed systems are notoriously difficult to implement and debug. One important tool for understanding the behavior of distributed systems is tracing. Unfortunately, effective tracing for modern distributed systems faces several challenges. First, many interesting behaviors in distributed systems only occur rarely, or at full production scale. Hence we need tracing mechanisms which impose minimal overhead, in order to allow always-on tracing of production instances. Second, for high-speed systems, messages can be delivered in significantly less time than the error of traditional time synchronization techniques such as network time protocol (NTP), necessitating time adjustment techniques with much higher precision. Third, distributed systems today may generate millions of events per second systemwide, resulting in traces consisting of billions of events. Such large traces can overwhelm existing trace analysis\u00a0\u2026", "num_citations": "22\n", "authors": ["471"]}
{"title": "Applying syntactic similarity algorithms for enterprise information management\n", "abstract": " For implementing content management solutions and enabling new applications associated with data retention, regulatory compliance, and litigation issues, enterprises need to develop advanced analytics to uncover relationships among the documents, eg, content similarity, provenance, and clustering. In this paper, we evaluate the performance of four syntactic similarity algorithms. Three algorithms are based on Broder's\" shingling\" technique while the fourth algorithm employs a more recent approach,\" content-based chunking\". For our experiments, we use a specially designed corpus of documents that includes a set of\" similar\" documents with a controlled number of modifications. Our performance study reveals that the similarity metric of all four algorithms is highly sensitive to settings of the algorithms' parameters: sliding window size and fingerprint sampling frequency. We identify a useful range of these\u00a0\u2026", "num_citations": "21\n", "authors": ["471"]}
{"title": "Remapping for memory wear leveling\n", "abstract": " A method and a corresponding apparatus provide for remapping for wear leveling of a memory. The method is implemented as logic and includes the steps of receiving a memory operation, the memory operation including a logical memory address; dividing the logical address into a logical block address portion, a logical line address portion, and a logical subline address portion; translating the logical block address portion into a physical block address; selecting a line remap key; applying the line remap key to the logical line address portion to produce a physical line address; producing a physical subline address portion; and combining the physical block, line, and subline address portions to produce a physical address for the memory operation.", "num_citations": "19\n", "authors": ["471"]}
{"title": "Sparkle: Optimizing spark for large memory machines and analytics\n", "abstract": " Spark is an in-memory analytics platform that targets commodity server environments today. It relies on the Hadoop Distributed File System (HDFS) to persist intermediate checkpoint states and final processing results. In Spark, immutable data are used for storing data updates in each iteration, making it inefficient for long running, iterative workloads. A non-deterministic garbage collector further worsens this problem. Sparkle is a library that optimizes memory usage in Spark. It exploits large shared memory to achieve better data shuffling and intermediate storage. Sparkle replaces the current TCP/IP-based shuffle with a shared memory approach and proposes an off-heap memory store for efficient updates. We performed a series of experiments on scale-out clusters and scale-up machines. The optimized shuffle engine leveraging shared memory provides 1.3x to 6x faster performance relative to Vanilla Spark. The off-heap memory store along with the shared-memory shuffle engine provides more than 20x performance increase on a probabilistic graph processing workload that uses a large-scale real-world hyperlink graph. While Sparkle benefits at most from running on large memory machines, it also achieves 1.6x to 5x performance improvements over scale out cluster with equivalent hardware setting.", "num_citations": "18\n", "authors": ["471"]}
{"title": "Computer system with cooperative cache\n", "abstract": " A server receives information that identifies which chunks are stored in local caches at client computers and receives a request to evict a chunk from a local cache of a first one of the client computers. The server determines whether the chunk stored at the local cache of the first one of the client computers is globally oldest among the chunks stored in the local caches at the client computers, and authorizes the first one of the client computers to evict the chunk when the chunk is the globally oldest among the chunks stored in the local caches at the client computers.", "num_citations": "18\n", "authors": ["471"]}
{"title": "Identifying a location containing invalid data in a storage media\n", "abstract": " A system includes storage media and control logic coupled to the storage media, where the control logic is configured to receive a write request and determine whether the write request specifies writing a predetermined pattern to a particular location of the storage media. In response to determining that the write request specifies writing the predetermined pattern to the particular location, the control logic is configured to identify with an indicator that the particular location contains invalid data.", "num_citations": "17\n", "authors": ["471"]}
{"title": "Controlled lock violation\n", "abstract": " In databases with a large buffer pool, a transaction may run in less time than it takes to log the transaction's commit record on stable storage. Such cases motivate a technique called early lock release: immediately after appending its commit record to the log buffer in memory, a transaction may release its locks. Thus, it cuts overall lock duration to a fraction and reduces lock contention accordingly.", "num_citations": "17\n", "authors": ["471"]}
{"title": "Trade-offs in protecting storage: a meta-data comparison of cryptographic, backup/versioning, immutable/tamper-proof, and redundant storage solutions\n", "abstract": " Modern storage systems are responsible for increasing amounts of data and the value of the data itself is growing in importance. Several primary storage system solutions have emerged for the protection of data: (1) secure storage through cryptography, (2) backup and versioning systems, (3) immutable and tamper-proof storage, and (4) redundant storage. Using results from published studies, we compare these four solutions against different requirements highlighting trade-offs in performance, space, attack resistance, and cost. We also present a case study of applying these solutions based on design work at NCSA. Lastly, we conclude that while different storage protection solutions may be appropriate for different requirements, some general conclusions can be made about current state-of-the-art storage protection solutions as well as directions for future research.", "num_citations": "16\n", "authors": ["471"]}
{"title": "Storage of data objects based on a time of creation\n", "abstract": " Techniques for storage of data objects based on a time of creation are disclosed. A computing device may receive a request to store a data object and, in response, identify a particular storage location that maintains data for the interval of time including a time of creation of the data object.", "num_citations": "13\n", "authors": ["471"]}
{"title": "Tamper-resistant storage techniques for multimedia systems\n", "abstract": " Tamper-resistant storage techniques provide varying degrees of authenticity and integrity for data. This paper surveys five implemented tamper-resistant storage systems that use encryption, cryptographic hashes, digital signatures and error-correction primitives to provide varying levels of data protection. Five key evaluation points for such systems are: (1) authenticity guarantees, (2) integrity guarantees, (3) confidentiality guarantees, (4) performance overhead attributed to security, and (5) scalability concerns. Immutable storage techniques can enhance tamper-resistant techniques. Digital watermarking is not appropriate for tamper-resistance implemented in the storage system rather than at the application level.", "num_citations": "13\n", "authors": ["471"]}
{"title": "Memory module controller supporting extended writes\n", "abstract": " Example methods and apparatus disclose supporting extended writes to a memory. An example method disclosed herein includes storing recovery information associated with a write request in a memory without processor intervention, the recovery information to facilitate redoing or undoing a write requested by the write request in the event that the write is interrupted, the write request received from a processor and comprising a destination address and new data; and if the write is not interrupted, writing the new data to the destination address in the memory without processor intervention", "num_citations": "12\n", "authors": ["471"]}
{"title": "File system management and balancing\n", "abstract": " A method for inserting a file in a search tree (B-tree) implemented on a file system, includes:(a) in response to a current node being a root node or an internal node, determining a child node;(b) repeating (a) until a leaf node is detected;(c) in response to a number of leaf nodes at a level of the leaf node exceeding an upper limit of files or a number of child nodes of a current node exceeding an upper limit of sub-directories, balancing the level of the leaf node or child nodes; and (d) inserting the file at the level of the leaf node.", "num_citations": "11\n", "authors": ["471"]}
{"title": "Lewis the Graduate Student: An Entry in the AAAI Robot Challenge.\n", "abstract": " In this paper, we describe Lewis the Graduate Student, Washington University\u2019s entry in the AAAI Mobile Robot Competition Challenge Event. Lewis successfully completed a modified subset of the Challenge tasks, with a minimum of human intervention. We describe the architecture of our system, and how each sub-task was achieved. We also offer some thoughts on the performance of the system, and highlight our plans for future work.", "num_citations": "10\n", "authors": ["471"]}
{"title": "Systems and methods for controlling access to a shared data structure with reader-writer locks using multiple sub-locks\n", "abstract": " A computer system for controlling access to a shared data structure includes a shared memory coupled to first and second processing units that stores a multi-lock to control access to a shared data structure. The multi-lock includes a first sub-lock associated with the first processing unit and a second sub-lock associated with the second processing unit The system also includes a data access control engine to receive a request to read from the data structure from the first processing unit and, as a result, determine whether a privately modifiable copy the first sub-lock exists in a first cache dedicated to the first processing unit, acquire a read portion of the first sub-lock and not communicate the acquisition across a coherence bus if a privately modifiable copy of the first sub-lock exists in the first cache, and if a privately modifiable copy of the first sub-lock does not exist in the first cache, load the first sub-lock into the first\u00a0\u2026", "num_citations": "9\n", "authors": ["471"]}
{"title": "Who Moved My Data? A Backup Tracking System for Dynamic Workstation Environments.\n", "abstract": " Periodic data backup is a system administration requirement that has changed as wireless machines have altered the fundamental structure of networks. These changes necessitate a complete rethinking of modern network backup strategies. The approaches of the 1980\u2019s and 1990\u2019s are no longer sufficient and must be updated. In addition to standard backup programs from vendors, specialized system administration tools are often needed. This paper examines one backup system and the major software components used to implement it. NCSA has developed a Backup Tracking System (BTS) 1 to perform backup operations based on knowledge of the network and when each machine was last successfully backed up. BTS can chronologically list all computers: from those currently attached to the network through those that have ever been attached over the life of the BTS program. BTS also provides information about all backup operations including the time of last attempt, success state, amount backed up, etc. The BTS database also contains the date of the last successful backup for each machine and whether it has at least one VIP user (to be given preferred status during backups) or all non-VIP users.", "num_citations": "9\n", "authors": ["471"]}
{"title": "Efficient failure recovery in a distributed data storage system\n", "abstract": " A method is provided for efficiently recovering information in a distributed storage system where a list of values that should be stored on a storage device is maintained. A first convergence round is scheduled to be performed on the list of values to bring each value to an At Maximum Redundancy (AMR) state. A second convergence round is scheduled to be performed on the list by selecting a wait time interval from a predefined range of wait time intervals between starts of convergence rounds.", "num_citations": "7\n", "authors": ["471"]}
{"title": "The techniques and challenges of immutable storage with applications in multimedia\n", "abstract": " Security of storage and archival systems has become a basic necessity in recent years. Due to the increased vulnerability of the existing systems and the need to comply with government regulations, different methods have been explored to attain a secure storage system. One of the primary problems to ensuring the integrity of storage systems is to make sure a file cannot be changed without proper authorization. Immutable storage is storage whose content cannot be changed once it has been written. For example, it is apparent that critical system files and other important documents should never be changed and thus stored as immutable. In multimedia systems, immutability provides proper archival of indices as well as content. In this paper we present a survey of existing techniques for immutability in file systems.", "num_citations": "7\n", "authors": ["471"]}
{"title": "Grouping chunks of data into a compression region\n", "abstract": " Examples disclosed herein relate to grouping chunks of data into a compression region. Examples relate to a chunk container comprising a first plurality of chunks of data in a plurality of first compression regions, and include grouping a second plurality of the chunks into a second compression region, and compressing the chunks of the second compression region relative to each other.", "num_citations": "5\n", "authors": ["471"]}
{"title": "Applying simple performance models to understand inefficiencies in data-intensive computing\n", "abstract": " New programming frameworks for scale-out parallel analysis, such as MapReduce and Hadoop, have become a cornerstone for exploiting large datasets. However, there has been little analysis of how these systems perform relative to the capabilities of the hardware on which they run. This paper describes a simple analytical model that predicts the theoretic ideal performance of a parallel dataflow system. The model exposes the inefficiency of popular scale-out systems, which take 3\u201313\u00d7 longer to complete jobs than the hardware should allow, even in well-tuned systems used to achieve record-breaking benchmark results. Using a simplified dataflow processing tool called Parallel DataSeries, we show that the model\u2019s ideal can be approached (ie, that it is not wildly optimistic), coming within 10\u201314% of the model\u2019s prediction. Moreover, guided by the model, we present analysis of inefficiencies which exposes issues in both the disk and networking subsystems that will be faced by any DISC system built atop standard OS and networking services.Acknowledgements: We thank the members and companies of the PDL Consortium (including APC, EMC, Facebook, Google, Hewlett-Packard Labs, Hitachi, IBM, Intel, LSI, Microsoft Research, NEC Laboratories, NetApp, Oracle, Riverbed, Samsung, Seagate, STEC, Symantec, VMWare, and Yahoo! Labs) for their interest, insights, feedback, and support. This research was sponsored in part by an HP Innovation Research Award and by CyLab at Carnegie Mellon University under grant DAAD19\u201302\u20131\u20130389 from the Army Research Office.", "num_citations": "4\n", "authors": ["471"]}
{"title": "Reducing backup bandwidth by remembering downloads\n", "abstract": " Systems and methods of reducing backup bandwidth by remembering downloads to a computing device. An example method may include remembering information for a download to a computing device. The method may also include backing up the computing device to a different system. The information remembered for the download is used to provide a backup of the computing device without copying some of the downloaded data present on the computing device from the computing device.", "num_citations": "3\n", "authors": ["471"]}
{"title": "Systems and methods for fine granularity memory sparing\n", "abstract": " Systems and methods for fine-grained sparing in non-volatile memories art disclosed. A system may include a memory having a plurality of blocks, a plurality of tags and a plurality of spared lines, wherein each of the tags corresponds to one of the plurality of spared lines, and table having a plurality of machine addresses, wherein each machine address corresponds to a sparing area for each of the blocks of the plurality of blocks. Methods of operation a fine-grained sparing system are also disclosed.", "num_citations": "3\n", "authors": ["471"]}
{"title": "Mapping long names in a filesystem\n", "abstract": " Mapping long names in a filesystem is disclosed. An example method includes hashing a long file name, and storing a file with the hashed file name. Another example method includes splitting a long file name into at least two parts, and encoding the at least two parts of the long file name as directory structures in the filesystem.", "num_citations": "3\n", "authors": ["471"]}
{"title": "Reading by user-level processes\n", "abstract": " Examples described herein relate to a memory structure by a user-level process. In an example, a method includes mapping in a read mode, by a kernel, a memory structure and a lock associated with a portion of the memory structure into an address space of a user-level process based on the user-level process being untrusted. The user-level process reads the portion of the memory structure outside of the kernel and determines a state of the lock after the reading of the portion. A write to the portion during the reading of the portion is detected based on the state of the lock.", "num_citations": "2\n", "authors": ["471"]}
{"title": "Persistent Regions that Survive NVM Media Failure\n", "abstract": " We consider a pool of non-volatile memory shared by thousands of compute nodes. When a single computing node or memory unit fails, the remaining nodes and memory units may stay available, making it theoretically possible for long-running data processing jobs to survive media failure. In practice, however, customers face an allor-nothing choice, and often choose to avoid the overhead of full hardware or software protection from media failure and instead limit the sizes of their data processing jobs to those that are likely to complete without failure. Our solution (TxHPC) enables staged data processing applications to survive media failure without requiring full-fledged hardware or software RAID. We have built an as-yet-unoptimized proof-of-concept of our solution and describe its design and implementation, as well as some preliminary performance results.", "num_citations": "2\n", "authors": ["471"]}
{"title": "Latch-free concurrent searching\n", "abstract": " Systems and methods associated with latch-free searching are disclosed. One example method includes receiving a key identifying data to be retrieved from a tree-based data structure. The method also includes performing a concurrent, latch-free search of the tree-based data structure until a leaf node is reached. The method also includes validating the leaf node. The method also includes retreading a portion of the search if the leaf node fails validation.", "num_citations": "1\n", "authors": ["471"]}
{"title": "Hierarchical virtual file systems for accessing data sets\n", "abstract": " Examples disclosed herein relate to a hierarchical file system. The hierarchical file system may include a first and a second virtual file referencing a stored data set. The first virtual file may include a set of first keys of a first level of specificity, with each key of the set of first keys including a record locator. The second virtual file may include a set of second keys referencing the data set and of a second level of specificity. The set of first keys within the first virtual file is searched in response to a query for data of the data set. A key from the set of second keys is accessed via the record locator from a key from the set of first keys where the data of the data set was not identified by the set of first keys.", "num_citations": "1\n", "authors": ["471"]}
{"title": "Structuring page images in a memory\n", "abstract": " Approaches for structuring a plurality of page images in-memory are described in various examples of the present disclosure. In one example, a unique page identifier provided within a reference page image is identified. The unique page identifier is associated with a target page image stored in-memory. Once identified, the page identifier associated with the target page image is replaced with a location specific identifier of the target page image, wherein the location specific identifier is based on an in-memory location of the target page image.", "num_citations": "1\n", "authors": ["471"]}
{"title": "Understanding Inefficiencies in Data-Intensive Computing\n", "abstract": " New programming frameworks for scale-out parallel analysis, such as MapReduce and Hadoop, have become a cornerstone for exploiting large datasets. However, there has been little analysis of how such systems perform relative to the capabilities of the hardware on which they run. This paper describes a simple model of IO resource consumption that predicts the ideal lowerbound runtime of a parallel dataflow on a particular set of hardware. Comparing actual system performance to the models ideal prediction exposes the inefficiency of a scale-out system. Using a simplified dataflow processing tool called Parallel DataSeries we show that the models ideal can be approached ie, that it is not wildly optimistic, but that a gap of up to 20 remains for workloads using up to 45 nodes. Guided by the model, we analyze inefficiencies exposed in both the disk and networking subsystems--issues that will be faced by any DISC system built atop popular commodity hardware and OSs.Descriptors:", "num_citations": "1\n", "authors": ["471"]}
{"title": "Applying idealized lower-bound runtime models to understand inefficiencies in data-intensive computing\n", "abstract": " \u201cData-intensive scalable computing\u201d(DISC) refers to a rapidly growing style of computing characterized by its reliance on large and expanding datasets [3]. Driven by the desire and capability to extract insight from such datasets, DISC is quickly emerging as a major activity of many organizations. Map-reduce style programming frameworks such as MapReduce [4] and Hadoop [1] support DISC activities by providing abstractions and frameworks to more easily scale data-parallel computations over commodity machines. In the pursuit of scale, popular map-reduce frameworks neglect efficiency as an important metric. Anecdotal experiences indicate that they neither achieve balance nor full goodput of hardware resources, effectively wasting a large fraction of the computers over which jobs are scaled. If these inefficiencies are real, the same work could be completed at much lower costs. An ideal run would provide\u00a0\u2026", "num_citations": "1\n", "authors": ["471"]}
{"title": "Designing and implementing malicious processors\n", "abstract": " It may be possible for attackers to modify integrated circuits (ICs) to insert covert, malicious circuitry into manufactured components; a recent Department of Defense report1 identifies several trends that contribute to this threat. First, it is infeasible economically for government-based IC suppliers to produce technology that matches the performance of commercial suppliers. These high-performance ICs provide a tactical advantage making them an indispensable resource. Second, commercial suppliers are moving more design, manufacturing, and testing of ICs to a geographically diverse set of countries in an effort to cut costs, making it infeasible to secure these steps in the IC life cycle. Together, these trends lead to an \u201cenormous and increasing\u201d opportunity for attack.Motivated attackers will subvert the IC supply chain if doing so provides sufficient value. Since modifying an IC is an expensive attack, it is doubtful that\u00a0\u2026", "num_citations": "1\n", "authors": ["471"]}
{"title": "MediaBench II Video\n", "abstract": " \u25cf In future video systems, we can expect:1) users will continue to demand higher quality video at lower prices 2) research in information theory will continue to push the envelope in maximizing video compression3) designers of video processors and video systems will continue to face the challenge of providing the greatest video capabilities for the lowest dollar", "num_citations": "1\n", "authors": ["471"]}