{"title": "The FAIR Guiding Principles for scientific data management and stewardship\n", "abstract": " There is an urgent need to improve the infrastructure supporting the reuse of scholarly data. A diverse set of stakeholders\u2014representing academia, industry, funding agencies, and scholarly publishers\u2014have come together to design and jointly endorse a concise and measureable set of principles that we refer to as the FAIR Data Principles. The intent is that these may act as a guideline for those wishing to enhance the reusability of their data holdings. Distinct from peer initiatives that focus on the human scholar, the FAIR Principles put specific emphasis on enhancing the ability of machines to automatically find and use the data, in addition to supporting its reuse by individuals. This Comment is the first formal publication of the FAIR Principles, and includes the rationale behind them, and some exemplar implementations in the community.", "num_citations": "5697\n", "authors": ["1168"]}
{"title": "Taverna: a tool for building and running workflows of services\n", "abstract": " Taverna is an application that eases the use and integration of the growing number of molecular biology tools and databases available on the web, especially web services. It allows bioinformaticians to construct workflows or pipelines of services to perform a range of different analyses, such as sequence analysis and genome annotation. These high-level workflows can integrate many different resources into a single analysis. Taverna is available freely under the terms of the GNU Lesser General Public License (LGPL) from http://taverna.sourceforge.net/ .", "num_citations": "1269\n", "authors": ["1168"]}
{"title": "Investigating semantic similarity measures across the Gene Ontology: the relationship between sequence and annotation\n", "abstract": " Motivation: Many bioinformatics data resources not only hold data in the form of sequences, but also as annotation. In the majority of cases, annotation is written as scientific natural language: this is suitable for humans, but not particularly useful for machine processing. Ontologies offer a mechanism by which knowledge can be represented in a form capable of such processing. In this paper we investigate the use of ontological annotation to measure the similarities in knowledge content or\u2018 semantic similarity\u2019 between entries in a data resource. These allow a bioinformatician to perform a similarity measure over annotation in an analogous manner to those performed over sequences.  A measure of semantic similarity for the knowledge component of bioinformatics resources should afford a biologist a new tool in their repetoire of analyses.         Results: We present the results from experiments that investigate the\u00a0\u2026", "num_citations": "1068\n", "authors": ["1168"]}
{"title": "Taverna: lessons in creating a workflow environment for the life sciences\n", "abstract": " Life sciences research is based on individuals, often with diverse skills, assembled into research groups. These groups use their specialist expertise to address scientific problems. The in silico experiments undertaken by these research groups can be represented as workflows involving the co\u2010ordinated use of analysis programs and information repositories that may be globally distributed. With regards to Grid computing, the requirements relate to the sharing of analysis and information resources rather than sharing computational power. The myGrid project has developed the Taverna Workbench for the composition and execution of workflows for the life sciences community. This experience paper describes lessons learnt during the development of Taverna. A common theme is the importance of understanding how workflows fit into the scientists' experimental context. The lessons reflect an evolving\u00a0\u2026", "num_citations": "993\n", "authors": ["1168"]}
{"title": "The Taverna workflow suite: designing and executing workflows of Web Services on the desktop, web or in the cloud\n", "abstract": " The Taverna workflow tool suite (http://www.taverna.org.uk) is designed to combine distributed Web Services and/or local tools into complex analysis pipelines. These pipelines can be executed on local desktop machines or through larger infrastructure (such as supercomputers, Grids or cloud environments), using the Taverna Server. In bioinformatics, Taverna workflows are typically used in the areas of high-throughput omics analyses (for example, proteomics or transcriptomics), or for evidence gathering methods involving text mining or data mining. Through Taverna, scientists have access to several thousand different tools and resources that are freely available from a large range of life science institutions. Once constructed, the workflows are reusable, executable bioinformatics protocols that can be shared, reused and repurposed. A repository of public workflows is available at http://www.myexperiment.org\u00a0\u2026", "num_citations": "852\n", "authors": ["1168"]}
{"title": "Examining the challenges of scientific workflows\n", "abstract": " Workflows have emerged as a paradigm for representing and managing complex distributed computations and are used to accelerate the pace of scientific progress. A recent National Science Foundation workshop brought together domain, computer, and social scientists to discuss requirements of future scientific applications and the challenges they present to current workflow technologies.", "num_citations": "791\n", "authors": ["1168"]}
{"title": "OilEd: a reason-able ontology editor for the semantic web\n", "abstract": " Ontologies will play a pivotal r\u00f4le in the \u201cSemantic Web\u201d, where they will provide a source of precisely defined terms that can be communicated across people and applications. OilEd, is an ontology editor that has an easy to use frame interface, yet at the same time allows users to exploit the full power of an expressive web ontology language (OIL). OilEd uses reasoning to support ontology design, facilitating the development of ontologies that are both more detailed and more accurate.", "num_citations": "716\n", "authors": ["1168"]}
{"title": "The design and realisation of the myExperiment Virtual Research Environment for social sharing of workflows\n", "abstract": " In this paper we suggest that the full scientific potential of workflows will be achieved through mechanisms for sharing and collaboration, empowering scientists to spread their experimental protocols and to benefit from those of others. To facilitate this process we have designed and built the Experiment m y Virtual Research Environment for collaboration and sharing of workflows and experiments. In contrast to systems which simply make workflows available, Experiment m y provides mechanisms to support the sharing of workflows within and across multiple communities. It achieves this by adopting a social web approach which is tailored to the particular needs of the scientist. We present the motivation, design and realisation of Experiment m y.", "num_citations": "586\n", "authors": ["1168"]}
{"title": "myGrid: personalised bioinformatics on the information grid\n", "abstract": " Motivation: The myGrid project aims to exploit Grid technology, with an emphasis on the Information Grid, and provide  middleware layers that make it appropriate for the needs of bioinformatics. myGrid is building high level services for data and application  integration such as resource discovery, workflow enactment and distributed  query processing. Additional services are provided to support the scientific  method and best practice found at the bench but often neglected at the  workstation, notably provenance management, change notification and  personalisation.         Results: We give an overview of these services and their metadata.  In particular, semantically rich metadata expressed using ontologies  necessary to discover, select and compose services into dynamic workflows.         Availability: Software is available on request from the authors  and information from http://www.mygrid.org.uk         Contact\u00a0\u2026", "num_citations": "556\n", "authors": ["1168"]}
{"title": "Ontology-based knowledge representation for bioinformatics\n", "abstract": " Much of biology works by applying prior knowledge (\u2018what is known\u2019) to an unknown entity,           rather than the application of a set of axioms that will elicit knowledge. In addition,           the complex biological data stored in bioinformatics databases often require the addition           of knowledge to specify and constrain the values held in that database. One way of           capturing knowledge within bioinformatics applications and databases is the use of           ontologies. An ontology is the concrete form of a conceptualisation of a community's           knowledge of a domainThis paper aims to introduce the reader to the use of ontologies within bioinformatics. A           description of the type of knowledge held in an ontology will be given. the paper will be           illustrated throughout with examples taken from bioinformatics and molecular biology, and           a survey of current biological ontologies will be presented. From this it\u00a0\u2026", "num_citations": "537\n", "authors": ["1168"]}
{"title": "Why linked data is not enough for scientists\n", "abstract": " Scientific data represents a significant portion of the linked open data cloud and scientists stand to benefit from the data fusion capability this will afford. Publishing linked data into the cloud, however, does not ensure the required reusability. Publishing has requirements of provenance, quality, credit, attribution and methods to provide the reproducibility that enables validation of results. In this paper we make the case for a scientific data publication model on top of linked data and introduce the notion of Research Objects as first class citizens for sharing and publishing.", "num_citations": "457\n", "authors": ["1168"]}
{"title": "The GRAIL concept modelling language for medical terminology\n", "abstract": " The GALEN representation and integration language (GRAIL) has been developed to support effective clinical user interfaces and extensible re-usable models of medical terminology. It has been used successfully to develop the prototype GALEN common reference (CORE) model for medical terminology and for a series of projects in clinical user interfaces within the GALEN and PEN&PAD projects. GRAIL is a description logic or frame language with novel features to support part-whole and other transitive relations and to support the GALEN modelling style aimed at re-use and application independence. GRAIL began as an experimental language. However, it has clarified many requirements for an effective knowledge representation language for clinical concepts. It still has numerous limitations despite its practical successes. The GRAIL experience is expected to form the basis for future languages which meet\u00a0\u2026", "num_citations": "424\n", "authors": ["1168"]}
{"title": "Toward interoperable bioscience data\n", "abstract": " To make full use of research data, the bioscience community needs to adopt technologies and reward mechanisms that support interoperability and promote the growth of an open'data commoning'culture. Here we describe the prerequisites for data commoning and present an established and growing ecosystem of solutions using the shared'Investigation-Study-Assay'framework to support that vision.", "num_citations": "399\n", "authors": ["1168"]}
{"title": "TAMBIS: transparent access to multiple bioinformatics information sources\n", "abstract": " Summary: TAMBIS (Transparent Access to Multiple  Bioinformatics Information Sources) is an application that allows  biologists to ask rich and complex questions over a range of  bioinformatics resources. It is based on a model of the knowledge of  the concepts and their relationships in molecular biology and  bioinformatics.         Availability: TAMBIS is available as an applet from http://img.cs.man.ac.uk/tambis         Supplementary   Information: A full user manual, tutorial and videos can be found at   http://img.cs.man.ac.uk/tambis.         Contact: tambis@cs.man.ac.uk", "num_citations": "384\n", "authors": ["1168"]}
{"title": "State of the nation in data integration for bioinformatics\n", "abstract": " Data integration is a perennial issue in bioinformatics, with many systems being developed and many technologies offered as a panacea for its resolution. The fact that it is still a problem indicates a persistence of underlying issues. Progress has been made, but we should ask \u201cwhat lessons have been learnt?\u201d, and \u201cwhat still needs to be done?\u201d Semantic Web and Web 2.0 technologies are the latest to find traction within bioinformatics data integration. Now we can ask whether the Semantic Web, mashups, or their combination, have the potential to help.This paper is based on the opening invited talk by Carole Goble given at the Health Care and Life Sciences Data Integration for the Semantic Web Workshop collocated with WWW2007. The paper expands on that talk. We attempt to place some perspective on past efforts, highlight the reasons for success and failure, and indicate some pointers to the future.", "num_citations": "373\n", "authors": ["1168"]}
{"title": "An ontology for bioinformatics applications.\n", "abstract": " MOTIVATION: An ontology of biological terminology provides a model of biological concepts that can be used to form a semantic framework for many data storage, retrieval and analysis tasks. Such a semantic framework could be used to underpin a range of important bioinformatics tasks, such as the querying of heterogeneous bioinformatics sources or the systematic annotation of experimental results. RESULTS: This paper provides an overview of an ontology [the Transparent Access to Multiple Biological Information Sources (TAMBIS) ontology or TaO] that describes a wide range of bioinformatics concepts. The present paper describes the mechanisms used for delivering the ontology and discusses the ontology's design and organization, which are crucial for maintaining the coherence of a large collection of concepts and their relationships. AVAILABILITY: The TAMBIS system, which uses a subset of the\u00a0\u2026", "num_citations": "350\n", "authors": ["1168"]}
{"title": "myExperiment: a repository and social network for the sharing of bioinformatics workflows\n", "abstract": " myExperiment (http://www.myexperiment.org) is an online research environment that supports the social sharing of bioinformatics workflows. These workflows are procedures consisting of a series of computational tasks using web services, which may be performed on data from its retrieval, integration and analysis, to the visualization of the results. As a public repository of workflows, myExperiment allows anybody to discover those that are relevant to their research, which can then be reused and repurposed to their specific requirements. Conversely, developers can submit their workflows to myExperiment and enable them to be shared in a secure manner. Since its release in 2007, myExperiment currently has over 3500 registered users and contains more than 1000 workflows. The social aspect to the sharing of these workflows is facilitated by registered users forming virtual communities bound together by a\u00a0\u2026", "num_citations": "348\n", "authors": ["1168"]}
{"title": "Open PHACTS: semantic interoperability for drug discovery\n", "abstract": " Open PHACTS is a public\u2013private partnership between academia, publishers, small and medium sized enterprises and pharmaceutical companies. The goal of the project is to deliver and sustain an \u2018open pharmacological space\u2019 using and enhancing state-of-the-art semantic web standards and technologies. It is focused on practical and robust applications to solve specific questions in drug discovery research. OPS is intended to facilitate improvements in drug discovery in academia and industry and to support open innovation and in-house non-public drug discovery research. This paper lays out the challenges and how the Open PHACTS project is hoping to address these challenges technically and socially.", "num_citations": "331\n", "authors": ["1168"]}
{"title": "Semantic similarity measures as tools for exploring the gene ontology\n", "abstract": " Many bioinformatics resources hold data in the form of sequences. Often this sequence data is associated with a large amount of annotation. In many cases this data has been hard to model, and has been represented as scientific natural language, which is not readily computationally amenable. The development of the Gene Ontology provides us with a more accessible representation of some of this data. However it is not clear how this data can best be searched, or queried. Recently we have adapted information content based measures for use with the Gene Ontology (GO). In this paper we present detailed investigation of the properties of these measures, and examine various properties of GO, which may have implications for its future design.", "num_citations": "309\n", "authors": ["1168"]}
{"title": "TAMBIS: Transparent access to multiple bioinformatics information sources.\n", "abstract": " The TAMBIS project aims to provide transparent access to disparate biological databases and analysis tools, enabling users to utilize a wide range of resources with the minimum of effort. A prototype system has been developed that includes a knowledge base of biological terminology (the biological Concept Model), a model of the underlying data sources (the Source Model) and a \u2018knowledge-driven\u2019user interface. Biological concepts are captured in the knowledge base using a description logic called GRAIL. The Concept Model provides the user with the concepts necessary to construct a wide range of multiple-source queries, and the user interface provides a flexible means of constructing and manipulating those queries. The Source Model provides a description of the underlying sources and mappings between terms used in the sources and terms in the biological Concept Model. The Concept Model and Source Model provide a level of indirection that shields the user from source details, providing a high level of source transparency. Source independent, declarative queries formed from terms in the Concept Model are transformed into a set of source dependent, executable procedures. Query formulation, translation and execution is demonstrated using a working example.", "num_citations": "296\n", "authors": ["1168"]}
{"title": "Conceptual linking: ontology-based open hypermedia\n", "abstract": " This paper describes the attempts of the COHSE project to define and deploy a Conceptual Open Hypermedia Service. Consisting of\u2022 an ontological reasoning service which is used to represent a sophisticated conceptual model of document terms and their relationships;\u2022 a Web-based open hypermedia link service that can offer a range of different link-providing facilities in a scalable and non-intrusive fashion; and integrated to form a conceptual hypermedia system to enable documents to be linked via metadata describing their contents and hence to improve the consistency and breadth of linking of WWW documents at retrieval time (as readers browse the documents) and authoring time (as authors create the documents).", "num_citations": "282\n", "authors": ["1168"]}
{"title": "The first provenance challenge\n", "abstract": " The first Provenance Challenge was set up in order to provide a forum for the community to understand the capabilities of different provenance systems and the expressiveness of their provenance representations. To this end, a functional magnetic resonance imaging workflow was defined, which participants had to either simulate or run in order to produce some provenance representation, from which a set of identified queries had to be implemented and executed. Sixteen teams responded to the challenge, and submitted their inputs. In this paper, we present the challenge workflow and queries, and summarize the participants' contributions. Copyright \u00a9 2007 John Wiley & Sons, Ltd.", "num_citations": "279\n", "authors": ["1168"]}
{"title": "BioCatalogue: a universal catalogue of web services for the life sciences\n", "abstract": " The use of Web Services to enable programmatic access to on-line bioinformatics is becoming increasingly important in the Life Sciences. However, their number, distribution and the variable quality of their documentation can make their discovery and subsequent use difficult. A Web Services registry with information on available services will help to bring together service providers and their users. The BioCatalogue (http://www.biocatalogue.org/) provides a common interface for registering, browsing and annotating Web Services to the Life Science community. Services in the BioCatalogue can be described and searched in multiple ways based upon their technical types, bioinformatics categories, user tags, service providers or data inputs and outputs. They are also subject to constant monitoring, allowing the identification of service problems and changes and the filtering-out of unavailable or unreliable\u00a0\u2026", "num_citations": "276\n", "authors": ["1168"]}
{"title": "Transparent access to multiple bioinformatics information sources\n", "abstract": " This paper describes the Transparent Access to Multiple Bioinformatics Information Sources project, known as TAMBIS, in which a domain ontology for molecular biology and bioinformatics is used in a retrieval-based information integration system for biologists. The ontology, represented using a description logic and managed by a terminology server, is used both to drive a visual query interface and as a global schema against which complex intersource queries are expressed. These source-independent declarative queries are then rewritten into collections of ordered source-dependent queries for execution by a middleware layer. In bioinformatics, the majority of data sources are not databases but tools with limited accessible interfaces. The ontology helps manage the interoperation between these resources. The paper emphasizes the central role that is played by the ontology in the system. The project\u00a0\u2026", "num_citations": "258\n", "authors": ["1168"]}
{"title": "A suite of DAML+ OIL ontologies to describe bioinformatics web services and data\n", "abstract": " The growing quantity and distribution of bioinformatics resources means that finding and utilizing them requires a great deal of expert knowledge, especially as many resources need to be tied together into a workflow to accomplish a useful goal. We want to formally capture at least some of this knowledge within a virtual workbench and middleware framework to assist a wider range of biologists in utilizing these resources. Different activities require different representations of knowledge. Finding or substituting a service within a workflow is often best supported by a classification. Marshalling and configuring services is best accomplished using a formal description. Both representations are highly interdependent and maintaining consistency between the two by hand is difficult. We report on a description logic approach using the web ontology language DAML+OIL that uses property based service descriptions. The\u00a0\u2026", "num_citations": "248\n", "authors": ["1168"]}
{"title": "Taverna, reloaded\n", "abstract": " The Taverna workflow management system is an open source project with a history of widespread adoption within multiple experimental science communities, and a long-term ambition of effectively supporting the evolving need of those communities for complex, data-intensive, service-based experimental pipelines. This short paper describes how the recently overhauled technical architecture of Taverna addresses issues of efficiency, scalability, and extensibility, and presents performance results based on a collection of synthetic workflows, as well as a concrete case study involving a production workflow in the area of cancer research.", "num_citations": "241\n", "authors": ["1168"]}
{"title": "Using semantic web technologies for representing e-science provenance\n", "abstract": " Life science researchers increasingly rely on the web as a primary source of data, forcing them to apply the same rigor to its use as to an experiment in the laboratory. The ${}^{\\mbox{\\scriptsize my}}$Grid project is developing the use of workflows to explicitly capture web-based procedures, and provenance to describe how and why results were produced. Experience within ${}^{\\mbox{\\scriptsize my}}$Grid has shown that this provenance metadata is formed from a complex web of heterogenous resources that impact on the production of a result. Therefore we have explored the use of Semantic Web technologies such as RDF, and ontologies to support its representation and used existing initiatives such as Jena and LSID, to generate and store such material. The effective presentation of complex RDF graphs is challenging. Haystack has been used to provide multiple views of provenance metadata that can\u00a0\u2026", "num_citations": "232\n", "authors": ["1168"]}
{"title": "Research objects: Towards exchange and reuse of digital knowledge\n", "abstract": " What will researchers be publishing in the future? Whilst there is little question that the Web will be the publication platform, as scholars move away from paper towards digital content, there is a need for mechanisms that support the production of self-contained units of knowledge and facilitate the publication, sharing and reuse of such entities. In this paper we discuss the notion of research objects, semantically rich aggregations of resources, that can possess some scientific intent or support some research objective. We present a number of principles that we expect such objects and their associated services to follow.", "num_citations": "204\n", "authors": ["1168"]}
{"title": "Provenance of e-science experiments-experience from bioinformatics\n", "abstract": " Like experiments performed at a laboratory bench, the data associated with an e-Science experiment are of reduced value if other scientists are not able to identify the origin, or provenance, of those data. Provenance information is essential if experiments are to be validated and verified by others, or even by those who originally performed them. In this article, we give an overview of our initial work on the provenance of bioinformatics e-Science experiments within myGrid. We use two kinds of provenance: the derivation path of information and annotation. We show how this kind of provenance can be delivered within the myGrid demonstrator WorkBench and we explore how the resulting Webs of experimental data holdings can be mined for useful information and presentations for the e-Scientist.", "num_citations": "196\n", "authors": ["1168"]}
{"title": "Learning domain ontologies for web service descriptions: an experiment in bioinformatics\n", "abstract": " The reasoning tasks that can be performed with semantic web service descriptions depend on the quality of the domain ontologies used to create these descriptions. However, building such domain ontologies is a time consuming and difficult task. We describe an automatic extraction method that learns domain ontologies for web service descriptions from textual documentations attached to web services. We conducted our experiments in the field of bioinformatics by learning an ontology from the documentation of the web services used in my Grid, a project that supports biology experiments on the Grid. Based on the evaluation of the extracted ontology in the context of the project, we conclude that the proposed extraction method is a helpful tool to support the process of building domain ontologies for web service descriptions.", "num_citations": "186\n", "authors": ["1168"]}
{"title": "A classification of tasks in bioinformatics\n", "abstract": " Motivation: This paper reports on a survey of bioinformatics   tasks currently undertaken by working biologists. The aim was to   find the range of tasks that need to be supported and the components   needed to do this in a general query system. This enabled a set of   evaluation criteria to be used to assess both the biology and   mechanical nature of general query systems.         Results: A classification of the biological content of the tasks   gathered offers a checklist for those tasks (and their   specialisations) that should be offered in a general bioinformatics   query system. This semantic analysis was contrasted with a syntactic   analysis that revealed the small number of components required to   describe all bioinformatics questions. Both the range of biological   tasks and syntactic task components can be seen to provide a set of   bioinformatics requirements for general query systems. These   requirements were\u00a0\u2026", "num_citations": "185\n", "authors": ["1168"]}
{"title": "A methodology to migrate the gene ontology to a description logic environment using DAML+ OIL\n", "abstract": " The Gene Ontology Next Generation Project (GONG) is developing a staged methodology to evolve the current representation of the Gene Ontology into DAML+OIL in order to take advantage of the richer formal expressiveness and the reasoning capabilities of the underlying description logic. Each stage provides a step level increase in formal explicit semantic content with a view to supporting validation, extension and multiple classification of the Gene Ontology. The paper introduces DAML+OIL and demonstrates the activity within each stage of the methodology and the functionality gained.", "num_citations": "178\n", "authors": ["1168"]}
{"title": "Mining Taverna's semantic web of provenance\n", "abstract": " Taverna is a workflow workbench developed as part of the UK's myGrid project. Taverna's provenance model captures both internal provenance locally generated in Taverna and external provenance gathered from third\u2010party data providers. This model also supports overlaying secondary provenance over the primary logs and lineage. This design is motivated by the particular properties of bioinformatics data and services used in Taverna. A Semantic Web of provenance, Ouzo, is built to combine the above different provenance by means of semantic annotations. This paper shows how Ouzo can be mined by a provenance usage component, Provenance Query and Answer (ProQA). ProQA supports provenance retrievals as well as provenance abstraction, aggregation, and semantic reasoning. ProQA is implemented as a suite APIs which can be deployed as provenance services to compose system provenance\u00a0\u2026", "num_citations": "168\n", "authors": ["1168"]}
{"title": "The ontology inference layer OIL\n", "abstract": " Currently computers are changing from single isolated devices to entry points into a worldwide network of information exchange and business transactions. Therefore, support in the exchange of data, information, and knowledge is becoming the key issue in computer technology today. Ontologies provide a shared and common understanding of a domain that can be communicated between people and across application systems. Ontologies will play a major role in supporting information exchange processes in various areas. A prerequisite for such a role is the development of a joint standard for specifying and exchanging ontologies. This paper deals with precisely this necessity. We will present OIL which is a proposal for such a standard. It is based on existing proposals such as OKBC, XOL and RDF, and enriches them with necessary features for expressing rich ontologies. The paper presents the motivation, underlying rationale, modeling primitives, syntax, semantics, and tool environment of OIL. With OIL, we want to make a proposal that initiates a discussion leading to a useful and well defined consensus amongst a large community which could use such an approach.", "num_citations": "164\n", "authors": ["1168"]}
{"title": "A framework for modelling the electronic medical record\n", "abstract": " This paper presents a model for an electronic medical record which satisfies the requirements for a faithful and structured record of patient care set out in a previous paper in this series. The model underlies the PEN & PAD clinical workstation, and it provides for a permanent, completely attributable record of patient care and the process of medical decision making. The model separates the record into two levels: direct observations of the patient and meta-statements about the use of observations in decision making and the clinical dialogue. The model is presented in terms of \u201cdescriptions\u201d formulated in the Structured Meta Knowledge (SMK) formalism, but many of its features are more general than the specific implementation. The use of electronic medical records based on the model for decision support and the analysis of aggregated data are discussed along with potential use of the model in distributed\u00a0\u2026", "num_citations": "163\n", "authors": ["1168"]}
{"title": "Generalized linear modelling in geomorphology\n", "abstract": " Generalized linear modelling (GLM) is a statistical technique used to model the relation between a response variable and a set of explanatory variables. GLM is similar to the well known multiple regression. However, GLM is a powerful technique for exploratory data analysis with many advantages over more traditional techniques. For example, GLM allows the incorporation of categorical as well as continuous response and explanatory variables in the analysis. In this paper, GLM is explained and two examples of the application of the technique in geomorphology are given. The first example involves glacier surging and the second involves landslide susceptibility. The examples demonstrate the relevance of GLM to many common problems in geomorphology. Copyright \u00a9 1998 John Wiley & Sons, Ltd.", "num_citations": "162\n", "authors": ["1168"]}
{"title": "Workflow-Centric Research Objects: A First Class Citizen in the Scholarly Discourse.\n", "abstract": " A workflow-centric research object bundles a workflow, the provenance of the results obtained by its enactment, other digital objects that are relevant for the experiment (papers, datasets, etc.), and annotations that semantically describe all these objects. In this paper, we propose a model to specify workflow-centric research objects, and show how the model can be grounded using semantic technologies and existing vocabularies, in particular the Object Reuse and Exchange (ORE) model and the Annotation Ontology (AO). We describe the life-cycle of a research object, which resembles the life-cycle of a scientific experiment.", "num_citations": "161\n", "authors": ["1168"]}
{"title": "myExperiment: social networking for workflow-using e-scientists\n", "abstract": " We present the Taverna workflow workbench and argue that scientific workflow environments need a rich ecosystem of tools that support the scientists. experimental lifecycle. Workflows are scientific objects in their own right, to be exchanged and reused. myExperiment is a new initiative to create a social networking environment for workflow workers. We present the motivation for myExperiment and sketch the proposed capabilities and challenges. We argue that actively engaging with a scientist's needs, fears and reward incentives is crucial for success.", "num_citations": "160\n", "authors": ["1168"]}
{"title": "Exploring Williams\u2013Beuren syndrome using myGrid\n", "abstract": " Motivation:           In silico experiments necessitate the virtual organization of people, data, tools and machines. The scientific process also necessitates an awareness of the experience base, both of personal data as well as the wider context of work. The management of all these data and the co-ordination of resources to manage such virtual organizations and the data surrounding them needs significant computational infra-structure support.                    Results: In this paper, we show that myGrid, middleware for the Semantic Grid, enables biologists to perform and manage in silico experiments, then explore and exploit the results of their experiments. We demonstrate myGrid in the context of a series of bioinformatics experiments focused on a 1.5 Mb region on chromosome 7 which is deleted in Williams\u2013Beuren syndrome (WBS). Due to the highly repetitive nature of sequence flanking/in the WBS critical\u00a0\u2026", "num_citations": "160\n", "authors": ["1168"]}
{"title": "Teallach: a model-based user interface development environment for object databases\n", "abstract": " Model-based user interface development environments show promise for improving the productivity of user interface developers, and possibly for improving the quality of developed interfaces. While model-based techniques have previously been applied to the area of database interfaces, they have not been specifically targeted at the important area of object database applications. Such applications make use of models that are semantically richer than their relational counterparts in terms of both data structures and application functionality. In general, model-based techniques have not addressed how the information referenced in such applications is manifested within the described models, and is utilised within the generated interface itself. This lack of experience with such systems has led to many model-based projects providing minimal support for certain features that are essential to such data intensive\u00a0\u2026", "num_citations": "160\n", "authors": ["1168"]}
{"title": "Software design for empowering scientists\n", "abstract": " Scientific research is increasingly digital. Some activities, such as data analysis, search, and simulation, can be accelerated by letting scientists write workflows and scripts that automate routine activities. These capture pieces of the scientific method that scientists can share. The Taverna Workbench, a widely deployed scientific-workflow-management system, together with the myExperiment social Web site for sharing scientific experiments, follow six principles of designing software for adoption by scientists and six principles of user engagement.", "num_citations": "150\n", "authors": ["1168"]}
{"title": "Towards a knowledge-based approach to semantic service composition\n", "abstract": " The successful application of Grid and Web Service technologies to real-world problems, such as e-Science [1], requires not only the development of a common vocabulary and meta-data framework as the basis for inter-agent communication and service integration but also the access and use of a rich repository of domain-specific knowledge for problem solving. Both requirements are met by the respective outcomes of ontological and knowledge engineering initiatives. In this paper we discuss a novel, knowledge-based approach to resource synthesis (service composition), which draws on the functionality of semantic web services to represent and expose available resources. The approach we use exploits domain knowledge to guide the service composition process and provide advice on service selection and instantiation. The approach has been implemented in a prototype workflow construction\u00a0\u2026", "num_citations": "149\n", "authors": ["1168"]}
{"title": "Feta: A light-weight architecture for user oriented semantic service discovery\n", "abstract": " Semantic Web Services offer the possibility of highly flexible web service architectures, where new services can be quickly discovered, orchestrated and composed into workflows. Most existing work has, however, focused on complex service descriptions for automated composition. In this paper, we describe the requirements from the bioinformatics domain which demand technically simpler descriptions, involving the user community at all levels. We describe our data model and light-weight semantic discovery architecture. We explain how this fits in the larger architecture of the                    my                 Grid project, which overall enables interoperability and composition across, disparate, autonomous, third-party services. Our contention is that such light-weight service discovery provides a good fit for user requirements of bioinformatics and possibly other domains.", "num_citations": "148\n", "authors": ["1168"]}
{"title": "Conceptual modelling of genomic information\n", "abstract": " Motivation: Genome sequencing projects are making available   complete records of the genetic make-up of organisms. These core   data sets are themselves complex, and present challenges to those   who seek to store, analyse and present the information. However, in   addition to the sequence data, high throughput experiments are   making available distinctive new data sets on protein interactions,   the phenotypic consequences of gene deletions, and on the   transcriptome, proteome, and metabolome. The effective description   and management of such data is of considerable importance to   bioinformatics in the post-genomic era. The provision of clear and   intuitive models of complex information is surprisingly challenging,   and this paper presents conceptual models for a range of important   emerging information resources in bioinformatics. It is hoped that   these can be of benefit to bioinformaticians as\u00a0\u2026", "num_citations": "148\n", "authors": ["1168"]}
{"title": "Using a suite of ontologies for preserving workflow-centric research objects\n", "abstract": " Scientific workflows are a popular mechanism for specifying and automating data-driven in silico experiments. A significant aspect of their value lies in their potential to be reused. Once shared, workflows become useful building blocks that can be combined or modified for developing new experiments. However, previous studies have shown that storing workflow specifications alone is not sufficient to ensure that they can be successfully reused, without being able to understand what the workflows aim to achieve or to re-enact them. To gain an understanding of the workflow, and how it may be used and repurposed for their needs, scientists require access to additional resources such as annotations describing the workflow, datasets used and produced by the workflow, and provenance traces recording workflow executions.In this article, we present a novel approach to the preservation of scientific workflows through\u00a0\u2026", "num_citations": "146\n", "authors": ["1168"]}
{"title": "An overview of S-OGSA: A reference semantic grid architecture\n", "abstract": " The Grid's vision, of sharing diverse resources in a flexible, coordinated and secure manner through dynamic formation and disbanding of virtual communities, strongly depends on metadata. Currently, Grid metadata is generated and used in an ad hoc fashion, much of it buried in the Grid middleware's code libraries and database schemas. This ad hoc expression and use of metadata causes chronic dependency on human intervention during the operation of Grid machinery, leading to systems which are brittle when faced with frequent syntactic changes in resource coordination and sharing protocols.The Semantic Grid is an extension of the Grid in which rich resource metadata is exposed and handled explicitly, and shared and managed via Grid protocols. The layering of an explicit semantic infrastructure over the Grid Infrastructure potentially leads to increased interoperability and greater flexibility.In recent years\u00a0\u2026", "num_citations": "143\n", "authors": ["1168"]}
{"title": "Position statement: Musings on provenance, workflow and (semantic web) annotations for bioinformatics\n", "abstract": " Lineage: line, line of descent, descent, bloodline, pedigree, ancestry, parentage, stock, filiation, derivation (the descendants of an individual, the kinship relation between an individual and their progenitors, inherited properties shared with others of your bloodline)History: the aggregate of past events, the continuum of events occurring in succession, account, chronicle, story, a body of knowledge, all that is remembered of the past preserved in writing.", "num_citations": "141\n", "authors": ["1168"]}
{"title": "Learning domain ontologies for semantic web service descriptions\n", "abstract": " High quality domain ontologies are essential for successful employment of semantic Web services. However, their acquisition is difficult and costly, thus hampering the development of this field. In this paper we report on the first stage of research that aims to develop (semi-)automatic ontology learning tools in the context of Web services that can support domain experts in the ontology building task. The goal of this first stage was to get a better understanding of the problem at hand and to determine which techniques might be feasible to use. To this end, we developed a framework for (semi-)automatic ontology learning from textual sources attached to Web services. The framework exploits the fact that these sources are expressed in a specific sublanguage, making them amenable to automatic analysis. We implement two methods in this framework, which differ in the complexity of the employed linguistic analysis. We\u00a0\u2026", "num_citations": "140\n", "authors": ["1168"]}
{"title": "A short study on the success of the Gene Ontology\n", "abstract": " While most ontologies have been used only by the groups who created them and for their initially defined purposes, the Gene Ontology (GO), an evolving structured controlled vocabulary of nearly 16,000 terms in the domain of biological functionality, has been widely used for annotation of biological-database entries and in biomedical research. As a set of learned lessons offered to other ontology developers, we list and briefly discuss the characteristics of GO that we believe are most responsible for its success: community involvement; clear goals; limited scope; simple, intuitive structure; continuous evolution; active curation; and early use.", "num_citations": "140\n", "authors": ["1168"]}
{"title": "Taverna/myGrid: Aligning a Workflow System with the Life Sciences Community\n", "abstract": " Bioinformatics is a discipline that uses computational and mathematical techniques to store, manage, and analyze biological data in order to answer biological questions. Bioinformatics has over 850 databases [154] and numerous tools that work over those databases and local data to produce even more data themselves. In order to perform an analysis, a bioinformatician uses one or more of these resources to gather, filter, and transform data to answer a question. Thus, bioinformatics is an in silico science.", "num_citations": "138\n", "authors": ["1168"]}
{"title": "Accessibility: a web engineering approach\n", "abstract": " Currently, the vast majority of web sites do not support accessibility for visually impaired users. Usually, these users have to rely on screen readers: applications that sequentially read the content of a web page in audio. Unfortunately, screen readers are not able to detect the meaning of the different page objects, and thus the implicit semantic knowledge conveyed in the presentation of the page is lost. One approach described in literature to tackle this problem, is the Dante approach, which allows semantic annotation of web pages to provide screen readers with extra (semantic) knowledge to better facilitate the audio presentation of a web page. Until now, such annotations were done manually, and failed for dynamic pages. In this paper, we combine the Dante approach with a web design method, WSDM, to fully automate the generation of the semantic annotation for visually impaired users. To do so, the semantic\u00a0\u2026", "num_citations": "134\n", "authors": ["1168"]}
{"title": "Applying semantic web services to bioinformatics: Experiences gained, lessons learnt\n", "abstract": " We have seen an increasing amount of interest in the application of Semantic Web technologies to Web services. The aim is to support automated discovery and composition of the services allowing seamless and transparent interoperability. In this paper we discuss three projects that are applying such technologies to bioinformatics: ${}^{\\scriptsize my}$Grid, MOBY-Services and Semantic-MOBY. Through an examination of the differences and similarities between the solutions produced, we highlight some of the practical difficulties in developing Semantic Web services and suggest that the experiences with these projects have implications for the development of Semantic Web services as a whole.", "num_citations": "134\n", "authors": ["1168"]}
{"title": "The grid: an application of the semantic web\n", "abstract": " The Grid is an emerging platform to support on-demand \"virtual organisations\" for coordinated resource sharing and problem solving on a global scale. The application thrust is large-scale scientific endeavour, and the scale and complexity of scientific data presents challenges for databases. The Grid is beginning to exploit technologies developed for Web Services and to realise its potential it also stands to benefit from Semantic Web technologies; conversely, the Grid and its scientific users provide application pull which will benefit the Semantic Web.", "num_citations": "134\n", "authors": ["1168"]}
{"title": "The travails of visually impaired web travellers\n", "abstract": " This paper proposes the inclusion of travel and mobility in the usability metrics of web design. Hypertext design and usability has traditionally concentrated upon navigation and/or orientation. The notion of travel extends navigation and orientation to include environment, mobility and the purpose of the travel task. The presence of travel aids are important for all users, but particularly so for those with a visual impairment. This paper presents the ground work for including travel into web design and usability metrics by presenting a framework for identifying travel objects and registering them as either cues to aid travel or obstacles that hinder travel for visually impaired users. The aim is to maximise cues and minimise obstacles to give high mobility as measured by the mobility index. This framework is based upon a model of real world travel by both sighted and visually impaired people, where travel objects are used for\u00a0\u2026", "num_citations": "130\n", "authors": ["1168"]}
{"title": "Taverna workflows: Syntax and semantics\n", "abstract": " This paper presents the formal syntax and the operational semantics of Taverna, a workflow management system with a large user base among the e-Science community. Such formal foundation, which has so far been lacking, opens the way to the translation between Taverna workflows and other process models. In particular, the ability to automatically compile a simple domain-specific process description into Taverna facilitates its adoption by e-scientists who are not expert workflow developers. We demonstrate this potential through a practical use case.", "num_citations": "129\n", "authors": ["1168"]}
{"title": "Semantically linking and browsing provenance logs for e-science\n", "abstract": " e-Science experiments are those performed using computer-based resources such as database searches, simulations or other applications. Like their laboratory based counterparts, the data associated with an e-Science experiment are of reduced value if other scientists are not able to identify the origin, or provenance, of those data. Provenance is the term given to metadata about experiment processes, the derivation paths of data, and the sources and quality of experimental components, which includes the scientists themselves, related literature, etc. Consequently provenance metadata are valuable resources for e-Scientists to repeat experiments, track versions of data and experiment runs, verify experiment results, and as a source of experimental insight. One specific kind of in silico experiment is a workflow. In this paper we describe how we can assemble a Semantic Web of workflow provenance logs\u00a0\u2026", "num_citations": "127\n", "authors": ["1168"]}
{"title": "Common motifs in scientific workflows: An empirical analysis\n", "abstract": " Workflow technology continues to play an important role as a means for specifying and enacting computational experiments in modern science. Reusing and re-purposing workflows allow scientists to do new experiments faster, since the workflows capture useful expertise from others. As workflow libraries grow, scientists face the challenge of finding workflows appropriate for their task, understanding what each workflow does, and reusing relevant portions of a given workflow. We believe that workflows would be easier to understand and reuse if high-level views (abstractions) of their activities were available in workflow libraries. As a first step towards obtaining these abstractions, we report in this paper on the results of a manual analysis performed over a set of real-world scientific workflows from Taverna, Wings, Galaxy and Vistrails. Our analysis has resulted in a set of scientific workflow motifs that outline (i) the\u00a0\u2026", "num_citations": "123\n", "authors": ["1168"]}
{"title": "Scientific Process Automation and Workflow Management.\n", "abstract": " Scientific discoveries in the natural sciences are increasingly data driven and computationally intensive, providing unprecedented data analysis and scientific simulation opportunities. To accelerate scientific discovery through advanced computing and information technology, various research programs have been launched in recent years, for example, the SciDAC program by the Department of Energy1 and the Cyberinfrastructure initiative by the National Science Foundation, 2 both in the United States. In the UK, the term e-Science3 was coined to describe computationally and data-intensive science, and a large e-Science research program was started there in 2000. With the new opportunities for scientists also come new challenges, for example, managing the enormous amounts of data generated4 and the increasingly sophisticated but also more complex computing environments provided by cluster computers and distributed grid environments. Scientific workflows aim to address many of these challenges.In general terms, a scientific workflow is a formal description of a process for accomplishing a scientific objective, usually expressed in terms of tasks and their dependencies. 5\u20137 Scientific workflows can be used during several different phases of a larger science process, that is, the cycle of hypothesis formation, experiment design, execution, and data analysis. 8, 86 Scientific workflows can include steps for the acquisition, integration, reduction, analysis, visualization, and publication (for example, in a shared database) of scientific data. Similar to more conventional business workflows, 9 scientific workflows are composed of individual tasks\u00a0\u2026", "num_citations": "123\n", "authors": ["1168"]}
{"title": "Automating experiments using semantic data in a bioinformatics grid\n", "abstract": " The transition from laboratory science to in silico e-science has facilitated a paradigmatic shift in the way we conduct modern science. We can use computationally based analytical models to simulate and investigate scientific questions such as those posed by high-energy physics and bioinformatics, yielding high-quality results and discoveries at an unprecedented rate. However, while experimental media have changed, the scientific methodologies and processes we choose for conducting experiments are still relevant. As in the lab environment, experimental methodology requires samples to undergo several processing stages. The staging of operations is what constitutes the in silico experimental process. The use of workflows formalizes earlier ad hoc approaches for representing experimental methodology. We can represent the stages of in silico experiments formally as a set of services to invoke.", "num_citations": "123\n", "authors": ["1168"]}
{"title": "Query processing in the TAMBIS bioinformatics source integration system\n", "abstract": " Conducting bioinformatic analyses involves biologists in expressing requests over a range of highly heterogeneous information sources and software tools. Such activities are laborious, and require detailed knowledge of the data structures and call interfaces of the different sources. The TAMBIS (Transparent Access to Multiple Bioinformatics Information Sources) project seeks to make the diversity in data structures, call interfaces and locations of bioinformatics sources transparent to users. In TAMBIS, queries are expressed in terms of an ontology implemented using a description logic, and queries over the ontology are rewritten to a middleware level for execution over the diverse sources. The paper describes query processing in TAMBIS, focusing in particular on the way source-independent concepts in the ontology are related to source-dependent middleware calls, and describing how the planner identifies\u00a0\u2026", "num_citations": "123\n", "authors": ["1168"]}
{"title": "Why workflows break\u2014Understanding and combating decay in Taverna workflows\n", "abstract": " Workflows provide a popular means for preserving scientific methods by explicitly encoding their process. However, some of them are subject to a decay in their ability to be re-executed or reproduce the same results over time, largely due to the volatility of the resources required for workflow executions. This paper provides an analysis of the root causes of workflow decay based on an empirical study of a collection of Taverna workflows from the myExperiment repository. Although our analysis was based on a specific type of workflow, the outcomes and methodology should be applicable to workflows from other systems, at least those whose executions also rely largely on accessing third-party resources. Based on our understanding about decay we recommend a minimal set of auxiliary resources to be preserved together with the workflows as an aggregation object and provide a software tool for end-users to create\u00a0\u2026", "num_citations": "122\n", "authors": ["1168"]}
{"title": "Seven bottlenecks to workflow reuse and repurposing\n", "abstract": " To date on-line processes (i.e. workflows) built in e-Science have been the result of collaborative team efforts. As more of these workflows are built, scientists start sharing and reusing stand-alone compositions of services, or workflow fragments. They repurpose an existing workflow or workflow fragment by finding one that is close enough to be the basis of a new workflow for a different purpose, and making small changes to it. Such a \u201cworkflow by example\u201d approach complements the popular view in the Semantic Web Services literature that on-line processes are constructed automatically from scratch, and could help bootstrap the Web of Science. Based on a comparison of e-Science middleware projects, this paper identifies seven bottlenecks to scalable reuse and repurposing. We include some thoughts on the applicability of using OWL for two bottlenecks: workflow fragment discovery and the ranking of\u00a0\u2026", "num_citations": "121\n", "authors": ["1168"]}
{"title": "The myGrid ontology: bioinformatics service discovery\n", "abstract": " myGrid  supports in silico experiments in the life sciences, enabling the design and enactment of workflows as well as providing components to assist service discovery, data and metadata management. The myGrid  ontology is one component in a larger semantic discovery framework for the identification of the highly distributed and heterogeneous bioinformatics services in the public domain. From an initial model of formal OWL-DL semantics throughout, we now adopt a spectrum of expressivity and reasoning for different tasks in service annotation and discovery. Here, we discuss the development and use of the myGrid  ontology and our experiences in semantic service discovery.", "num_citations": "119\n", "authors": ["1168"]}
{"title": "Nano-Publication in the e-science era\n", "abstract": " The rate of data production in the Life Sciences has now reached such proportions that to consider it irresponsible to fund data generation without proper concomitant funding and infrastructure for storing, analyzing and exchanging the information and knowledge contained in, and extracted from, those data, is not an exaggerated position any longer. The chasm between data production and data handling has become so wide, that many data go unnoticed or at least run the risk of relative obscurity, fail to reveal the information contained in the data set or remains inaccessible due to ambiguity, or financial or legal toll-barriers. As a result, inconsistency, ambiguity and redundancy of data and information on the Web are becoming impediments to the performance of comprehensive information extraction and analysis. This paper attempts a stepwise explanation of the use of richly annotated RDF-statements as carriers of unambiguous, meta-analyzed information in the form of traceable nano-publications.", "num_citations": "115\n", "authors": ["1168"]}
{"title": "Designing the myexperiment virtual research environment for the social sharing of workflows\n", "abstract": " Many scientific workflow systems have been developed and are serving to benefit science. In this paper we look beyond individual systems to consider the use of workflows within scientific practice, and we argue that the tremendous scientific potential of workflows will be achieved through mechanisms for sharing and collaboration - empowering the scientist to spread their experimental protocols and to benefit from the protocols of others. We discuss issues in workflow sharing, propose a set of design principles for collaborative e-Science software, and illustrate these principles in action through the design of the  my Experiment Virtual Research Environment for collaboration and sharing of experiments.", "num_citations": "115\n", "authors": ["1168"]}
{"title": "Micropublications: a semantic model for claims, evidence, arguments and annotations in biomedical communications\n", "abstract": " Scientific publications are documentary representations of defeasible arguments, supported by data and repeatable methods. They are the essential mediating artifacts in the ecosystem of scientific communications. The institutional \u201cgoal\u201d of science is publishing results. The linear document publication format, dating from 1665, has survived transition to the Web. Intractable publication volumes; the difficulty of verifying evidence; and observed problems in evidence and citation chains suggest a need for a web-friendly and machine-tractable model of scientific publications. This model should support: digital summarization, evidence examination, challenge, verification and remix, and incremental adoption. Such a model must be capable of expressing a broad spectrum of representational complexity, ranging from minimal to maximal forms. The micropublications semantic model of scientific argument and evidence provides these features. Micropublications support natural language statements; data; methods and materials specifications; discussion and commentary; challenge and disagreement; as well as allowing many kinds of statement formalization. The minimal form of a micropublication is a statement with its attribution. The maximal form is a statement with its complete supporting argument, consisting of all relevant evidence, interpretations, discussion and challenges brought forward in support of or opposition to it. Micropublications may be formalized and serialized in multiple ways, including in RDF. They may be added to publications as stand-off metadata. An OWL 2 vocabulary for micropublications is available at                      http://purl.org\u00a0\u2026", "num_citations": "114\n", "authors": ["1168"]}
{"title": "The FAIR Guiding Principles for scientific data management and stewardship. Sci Data 3: 160018\n", "abstract": " Sometimes, experts in one field use terms not shared by others, or there may be disagreement about which vocabulary is best suited for a specific aspect of research. As a result, it can be difficult to integrate and analyze datasets, which can hinder scientific collaboration and discovery.Two scientists discuss scientific data while looking at data on screen Finding, integrating, and analyzing research information requires community agreement on what terms to use to describe the data and what they mean in a specific context.(Photo courtesy of Gorodenkoff/Shutterstock. com)", "num_citations": "112\n", "authors": ["1168"]}
{"title": "Annotating, linking and browsing provenance logs for e-science\n", "abstract": " Like experiments performed at a laboratory bench, the results of an e-science in silico experiment are of limited value if other scientists are not able to identify the origin, or provenance, of those results. For e-Science, we need more systematic provenance logs across a range of e-Science activities and disciplines as well as a more informed understanding of the information in these provenance data. Semantic Web technology, which enables data to be linked and defined in a way for more effective discovery, integration and cooperation across computers and people, provides an appropriate solution for our current requirement. In this paper we show how we used the COHSE conceptual open hypermedia system to build a dynamically generated hypertext of web of provenance documents arising from the myGrid project based on associated concepts and reasoning over the ontology.", "num_citations": "111\n", "authors": ["1168"]}
{"title": "Semantic matching of grid resource descriptions\n", "abstract": " The ability to describe the Grid resources needed by applications is essential for developing seamless access to resources on the Grid. We consider the problem of resource description in the context of a resource broker being developed in the Grid Interoperability Project (GRIP) which is able to broker for resources described by several Grid middleware systems, GT2, GT3 and Unicore. We consider it necessary to utilise a semantic matching of these resource descriptions, firstly because there is currently no common standard, but more fundamentally because we wish to make the Grid transparent at the application level. We show how the semantic approach to resource description facilitates both these aims and present the GRIP broker as a working prototype of this approach.", "num_citations": "105\n", "authors": ["1168"]}
{"title": "Scientific data management: challenges, technology, and deployment\n", "abstract": " Dealing with the volume, complexity, and diversity of data currently being generated by scientific experiments and simulations often causes scientists to waste productive time. Scientific Data Management: Challenges, Technology, and Deployment describes cutting-edge technologies and solutions for managing and analyzing vast amounts of data, helping", "num_citations": "104\n", "authors": ["1168"]}
{"title": "Conceptual open hypermedia= the semantic web?\n", "abstract": " The Semantic Web is still a web, a collection of linked nodes. Navigation of links is currently, and will remain for humans if not machines, a key mechanism for exploring the space. The Semantic Web is viewed by many as a knowledge base, a database or an indexed and searchable document collection; in the work discussed here we view it as a hypertext. The aim of the COHSE project is to research into methods to improve significantly the quality, consistency and breadth of linking of Web documents at retrieval time (as readers browse the documents) and authoring time (as authors create the documents). The objective is link creation rather than resource discovery; in contrast, many existing projects are concerned primarily with the discovery of resources (reading), rather than the construction of hypertexts (authoring). The project plans to produce a COHSE (Conceptual Open Hypermedia ServicE) by integrating an ontological reasoning service with a Web-based Open Hypermedia link service. This will form a Conceptual Hypermedia system enabling documents to be linked via metadata describing their contents. The bringing together of Open Hypermedia and Ontology services can be seen as one particular implementation of the Semantic Web. Here we briefly present open and conceptual hypermedia, and introduce the architecture being employed within the COHSE project, and the prototype COHSE platform we have developed. We present the questions that we now plan to address that surround the Semantic Web when viewed from the perspective of a hypertext for people.", "num_citations": "102\n", "authors": ["1168"]}
{"title": "An automated Design-Build-Test-Learn pipeline for enhanced microbial production of fine chemicals\n", "abstract": " The microbial production of fine chemicals provides a promising biosustainable manufacturing solution that has led to the successful production of a growing catalog of natural products and high-value chemicals. However, development at industrial levels has been hindered by the large resource investments required. Here we present an integrated Design\u2013Build-Test\u2013Learn (DBTL) pipeline for the discovery and optimization of biosynthetic pathways, which is designed to be compound agnostic and automated throughout. We initially applied the pipeline for the production of the flavonoid (2S)-pinocembrin in Escherichia coli, to demonstrate rapid iterative DBTL cycling with automation at every stage. In this case, application of two DBTL cycles successfully established a production pathway improved by 500-fold, with competitive titers up to 88 mg L\u2212 1. The further application of the pipeline to optimize an alkaloids\u00a0\u2026", "num_citations": "101\n", "authors": ["1168"]}
{"title": "myExperiment: Defining the social virtual research environment\n", "abstract": " The myExperiment virtual research environment supports the sharing of research objects used by scientists, such as scientific workflows. For researchers it is both a social infrastructure that encourages sharing and a platform for conducting research, through familiar user interfaces. For developers it provides an open, extensible and participative environment. We describe the design, implementation and deployment of myExperiment and suggest that its four capabilities - research objects, social model, open environment and actioning research - are necessary characteristics of an effective Virtual Research Environment for e-research and open science.", "num_citations": "98\n", "authors": ["1168"]}
{"title": "RightField: embedding ontology annotation in spreadsheets\n", "abstract": " Motivation: In the Life Sciences, guidelines, checklists and ontologies describing what metadata is required for the interpretation and reuse of experimental data are emerging. Data producers, however, may have little experience in the use of such standards and require tools to support this form of data annotation.Results: RightField is an open source application that provides a mechanism for embedding ontology annotation support for Life Science data in Excel spreadsheets. Individual cells, columns or rows can be restricted to particular ranges of allowed classes or instances from chosen ontologies. The RightField-enabled spreadsheet presents selected ontology terms to the users as a simple drop-down list, enabling scientists to consistently annotate their data. The result is \u2018semantic annotation by stealth\u2019, with an annotation process that is less error-prone, more efficient, and more consistent with community\u00a0\u2026", "num_citations": "97\n", "authors": ["1168"]}
{"title": "Evaluating DANTE: Semantic transcoding for visually disabled users\n", "abstract": " The importance of the World Wide Web for information dissemination is indisputable. However, the dominance of visual design on the Web leaves visually disabled people at a disadvantage. Although assistive technologies, such as screen readers, usually provide basic access to information, the richness of the Web experience is still often lost. In particular, traversing the Web becomes a complicated task since the richness of visual objects presented to their sighted counterparts are neither appropriate nor accessible to visually disabled users. To address this problem, we have proposed an approach called Dante in which Web pages are annotated with semantic information to make their traversal properties explicit. Dante supports usage of different annotation techniques and as a proof-of-concept in this article, pages are annotated manually which when transcoded become rich. We first introduce Dante and then\u00a0\u2026", "num_citations": "97\n", "authors": ["1168"]}
{"title": "Building a bioinformatics ontology using OIL\n", "abstract": " This paper describes the initial stages of building an ontology of bioinformatics and molecular biology. The conceptualization is encoded using the ontology inference layer (OIL), a knowledge representation language that combines the modeling style of frame-based systems with the expressiveness and reasoning power of description logics (DLs). This paper is the second of a pair in this special issue. The first described the core of the OIL language and the need to use ontologies to deliver semantic bioinformatics resources. In this paper, the early stages of building an ontology component of a bioinformatics resource querying application are described. This ontology (TaO) holds the information about molecular biology represented in bioinformatics resources and the bioinformatics tasks performed over these resources. It, therefore, represents the metadata of the resources the application can query. It also\u00a0\u2026", "num_citations": "93\n", "authors": ["1168"]}
{"title": "SEEK: a systems biology data and model management platform\n", "abstract": " Systems biology research typically involves the integration and analysis of heterogeneous data types in order to model and predict biological processes. Researchers therefore require tools and resources to facilitate the sharing and integration of data, and for linking of data to systems biology models. There are a large number of public repositories for storing biological data of a particular type, for example transcriptomics or proteomics, and there are several model repositories. However, this silo-type storage of data and models is not conducive to systems biology investigations. Interdependencies between multiple omics datasets and between datasets and models are essential. Researchers require an environment that will allow the management and sharing of heterogeneous data and models in the context of the experiments which created them. The SEEK is a suite of tools to support the management, sharing and exploration of data and models in systems biology. The SEEK platform provides an access-controlled, web-based environment for scientists to share and exchange data and models for day-to-day collaboration and for public dissemination. A plug-in architecture allows the linking of experiments, their protocols, data, models and results in a configurable system that is available 'off the shelf'. Tools to run model simulations, plot experimental data and assist with data annotation and standardisation combine to produce a collection of resources that support analysis as well as sharing. Underlying semantic web resources additionally extract and serve SEEK metadata in RDF (Resource Description Format). SEEK RDF enables rich semantic\u00a0\u2026", "num_citations": "92\n", "authors": ["1168"]}
{"title": "DAML+ OIL is not Enough.\n", "abstract": " As is well recognised within the Semantic Web community, ontologies will play a crucial part in the delivery of the Semantic Web, facilitating the sharing of information between communities, both of people and software agents.In order to support this use of ontologies, a number of representational formats have been proposed, including RDF Schema (RDF (S))[RDF], the Ontology Interchange Language (OIL)[OIL] and the Darpa Agent Markup Language (DAML)[DAM]. These last two have been brought together to form DAML+ OIL, a language now being proposed as a W3C standard for ontological and metadata representation.", "num_citations": "92\n", "authors": ["1168"]}
{"title": "Multi-scale science: supporting emerging practice with semantically derived provenance\n", "abstract": " Scientific progress is becoming increasingly dependent of our ability to study phenomena at multiple scales and from multiple perspectives. The ability to recontextualize third party data within the semantic and syntactic framework of a given research project is increasingly seen as a primary barrier in multi-scale science. Within the Collaboratory for Multiscale Chemical Science (CMCS) project, we are developing a general-purpose informatics-based approach that emphasizes''on-demand''metadata creation, configurable data translations, and semantic mapping to support the rapidly increasing and continually evolving requirements for managing data, metadata, and data relationships in such projects. A concrete example of this approach is the design of the CMCS provenance subsystem. The concept of provenance varies across communities, and multiple independent applications contribute to and use provenance. In CMCS, we have developed generic tools for viewing provenance relationships and for using them to, for example, scope notifications and searches. These tools rely on a configurable concept of provenance defined in terms of other relationships. The result is a very flexible mechanism capable of tracking data provenance across many disciplines and supporting multiple uses of provenance information.", "num_citations": "91\n", "authors": ["1168"]}
{"title": "Janus: From Workflows to Semantic Provenance and Linked Open Data\n", "abstract": " Data provenance graphs are form of metadata that can be used to establish a variety of properties of data products that undergo sequences of transformations, typically specified as workflows. Their usefulness for answering user provenance queries is limited, however, unless the graphs are enhanced with domain-specific annotations. In this paper we propose a model and architecture for semantic, domain-aware provenance, and demonstrate its usefulness in answering typical user queries. Furthermore, we discuss the additional benefits and the technical implications of publishing provenance graphs as a form of Linked Data. A prototype implementation of the model is available for data produced by the Taverna workflow system.", "num_citations": "90\n", "authors": ["1168"]}
{"title": "Treating shimantic web syndrome with ontologies\n", "abstract": " This paper describes shimantic web syndrome, the use of \u201cshims\u201d to align or mediate mismatching third party Web Services that have closely related, but incompatible, inputs and outputs. The syndrome is illustrated using services from my Grid bioinformatics analyses. An ontology driven treatment for managing this syndrome through semiautomated service discovery and invocation is outlined. This treatment is likely to be applicable to other domains.", "num_citations": "89\n", "authors": ["1168"]}
{"title": "Identifiers for the 21st century: How to design, provision, and reuse persistent identifiers to maximize utility and impact of life science data\n", "abstract": " In many disciplines, data are highly decentralized across thousands of online databases (repositories, registries, and knowledgebases). Wringing value from such databases depends on the discipline of data science and on the humble bricks and mortar that make integration possible; identifiers are a core component of this integration infrastructure. Drawing on our experience and on work by other groups, we outline 10 lessons we have learned about the identifier qualities and best practices that facilitate large-scale data integration. Specifically, we propose actions that identifier practitioners (database providers) should take in the design, provision and reuse of identifiers. We also outline the important considerations for those referencing identifiers in various circumstances, including by authors and data generators. While the importance and relevance of each lesson will vary by context, there is a need for increased awareness about how to avoid and manage common identifier problems, especially those related to persistence and web-accessibility/resolvability. We focus strongly on web-based identifiers in the life sciences; however, the principles are broadly relevant to other disciplines.", "num_citations": "88\n", "authors": ["1168"]}
{"title": "Heterogeneous composition of models of computation\n", "abstract": " A model of computation (MoC) is a formal abstraction of execution in a computer. There is a need for composing diverse MoCs in e-science. Kepler, which is based on Ptolemy II, is a scientific workflow environment that allows for MoC composition. This paper explains how MoCs are combined in Kepler and Ptolemy II and analyzes which combinations of MoCs are currently possible and useful. It demonstrates the approach by combining MoCs involving dataflow and finite state machines. The resulting classification should be relevant to other workflow environments wishing to combine multiple MoCs (available at http://ptolemy.org/heterogeneousMoCs).", "num_citations": "88\n", "authors": ["1168"]}
{"title": "Data lineage model for Taverna workflows with lightweight annotation requirements\n", "abstract": " The provenance, or lineage, of a workflow data product can be reconstructed by keeping a complete trace of workflow execution. This lineage information, however, is likely to be both imprecise, because of the black-box nature of the services that compose the workflow, and noisy, because of the many trivial data transformations that obscure the intended purpose of the workflow. In this paper we argue that these shortcomings can be alleviated by introducing a small set of optional lightweight annotations to the workflow, in a principled way. We begin by presenting a baseline, annotation-free lineage model for the Taverna workflow system, and then show how the proposed annotations improve the results of fundamental lineage queries.", "num_citations": "88\n", "authors": ["1168"]}
{"title": "PAV ontology: provenance, authoring and versioning\n", "abstract": " Provenance is a critical ingredient for establishing trust of published scientific content. This is true whether we are considering a data set, a computational workflow, a peer-reviewed publication or a simple scientific claim with supportive evidence. Existing vocabularies such as Dublin Core Terms (DC Terms) and the W3C Provenance Ontology (PROV-O) are domain-independent and general-purpose and they allow and encourage for extensions to cover more specific needs. In particular, to track authoring and versioning information of web resources, PROV-O provides a basic methodology but not any specific classes and properties for identifying or distinguishing between the various roles assumed by agents manipulating digital artifacts, such as author, contributor and curator. We present the Provenance, Authoring and Versioning ontology (PAV, namespace                      http://purl.org/pav/                                        ): a\u00a0\u2026", "num_citations": "87\n", "authors": ["1168"]}
{"title": "Towards annotation using DAML+ OIL\n", "abstract": " CiNii \u8ad6\u6587 - Towards annotation using DAML+OIL CiNii \u56fd\u7acb\u60c5\u5831\u5b66\u7814\u7a76\u6240 \u5b66\u8853\u60c5\u5831\u30ca\u30d3\u30b2\u30fc\u30bf [\u30b5\u30a4\u30cb\u30a3] \u65e5\u672c\u306e\u8ad6\u6587\u3092\u3055\u304c\u3059 \u5927\u5b66\u56f3\u66f8\u9928\u306e\u672c\u3092\u3055\u304c\u3059 \u65e5\u672c\u306e\u535a\u58eb\u8ad6\u6587\u3092\u3055\u304c\u3059 \u65b0\u898f\u767b\u9332 \u30ed\u30b0\u30a4\u30f3 English \u691c\u7d22 \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u9589\u3058\u308b \u30bf\u30a4\u30c8\u30eb \u8457\u8005\u540d \u8457\u8005ID \u8457\u8005\u6240\u5c5e \u520a\u884c\u7269\u540d ISSN \u5dfb\u53f7\u30da\u30fc\u30b8 \u51fa\u7248\u8005 \u53c2\u8003\u6587\u732e \u51fa\u7248\u5e74 \u5e74\u304b\u3089 \u5e74\u307e\u3067 \u691c\u7d22 \u691c\u7d22 \u691c\u7d22 CiNii\u7a93\u53e3\u696d\u52d9\u306e\u518d\u958b\u306b\u3064\u3044\u3066 Towards annotation using DAML+OIL BECHHOFER S. \u88ab\u5f15\u7528\u6587\u732e: 1\u4ef6 \u8457\u8005 BECHHOFER S. \u53ce\u9332\u520a\u884c\u7269 Proc. Workshop on Semantic Markup and Annotation at 1st International Conference on Knowledge Capture (K-CAP 2001), Victoria, BC, Canada Proc. Workshop on Semantic Markup and Annotation at 1st International Conference on Knowledge Capture (K-CAP 2001), Victoria, BC, Canada, 2001 \u88ab\u5f15\u7528\u6587\u732e: 1\u4ef6\u4e2d 1-1\u4ef6\u3092 \u8868\u793a 1 \u30de\u30fc\u30ad\u30f3\u30b0\u3092\u7528\u3044\u305f \u30bd\u30fc\u30b7\u30e3\u30eb\u30bf\u30ae\u30f3\u30b0\u306e\u6709\u52b9\u6027\u306b\u95a2\u3059\u308b\u691c\u8a3c \u677e\u5ca1 \u6709\u5e0c , \u5742\u672c \u7adc\u57fa , \u4f0a\u85e4\u798e\u5ba3 , \u5927\u5411\u4e00\u8f1d , \u6b66\u7530 \u82f1\u660e \u2026", "num_citations": "86\n", "authors": ["1168"]}
{"title": "Automatic annotation of web services based on workflow definitions\n", "abstract": " Semantic annotations of web services can support the effective and efficient discovery of services, and guide their composition into workflows. At present, however, the practical utility of such annotations is limited by the small number of service annotations available for general use. Manual annotation of services is a time consuming and thus expensive task, so some means are required by which services can be automatically (or semi-automatically) annotated. In this paper, we show how information can be inferred about the semantics of operation parameters based on their connections to other (annotated) operation parameters within tried-and-tested workflows. Because the data links in the workflows do not necessarily contain every possible connection of compatible parameters, we can infer only constraints on the semantics of parameters. We show that despite their imprecise nature these so-called loose\u00a0\u2026", "num_citations": "85\n", "authors": ["1168"]}
{"title": "The myGrid project: services, architecture and demonstrator\n", "abstract": " Abstract myGrid is an e-Science research project developing open source high-level middleware to support in silico experiments in biology. In silico experiments use databases and computational analysis rather than laboratory investigations to test hypothesis. This paper provides an overview of services the myGrid project is developing, and the architecture in which they fit. Registries provide information about available data and computational services, while remote legacy bioinformatics applications are wrapped using a consistent distributed analysis framework Soaplab. As in conventional science, experimental method is as important as final results. myGrid formalises these methods as workflow or query specifications and provides service based middleware components to enact them. e-Science for the individual often has a narrow focus and so personalisation forms a key theme in myGrid service design. Information repositories, service registries and change notification systems are all being developed to provide personalised views of resources. myGrid components make extensive use of metadata to support this need for personalisation and the project is pioneering the use of semantic web technology, to manage annotation, ontologies and semantic discovery. The ultimate goal of myGrid is to supply this collection of services as a toolkit to build end applications. To demonstrate this concept the project is building its own application (the myGrid workBench).", "num_citations": "84\n", "authors": ["1168"]}
{"title": "The semantics of semantic annotation\n", "abstract": " Semantic metadata will playa significant role in the provision of the Semantic Web. Agents will need metadata that describes the content of resources in order to perform operations, such as retrieval, over those resources. In addition, if rich semantic metadata is supplied, those agents can then employ reasoning over the metadata, enhancing their processing power. Key to this approach is the provision of annotations, both through automatic and human means. The semantics of these annotations, however, in terms of the mechanisms through which they are interpreted and presented to the user, are sometimes unclear. In this paper, we identifya number of candidate interpretations of annotation, and discuss the impact these interpretations mayha ve on Semantic Web applications.", "num_citations": "82\n", "authors": ["1168"]}
{"title": "Four simple recommendations to encourage best practices in research software\n", "abstract": " Scientific research relies on computer software, yet software is not always developed following practices that ensure its quality and sustainability. This manuscript does not aim to propose new software development best practices, but rather to provide simple recommendations that encourage the adoption of existing best practices. Software development best practices promote better quality software, and better quality software improves the reproducibility and reusability of research. These recommendations are designed around Open Source values, and provide practical suggestions that contribute to making research software and its source code more discoverable, reusable and transparent. This manuscript is aimed at developers, but also at organisations, projects, journals and funders that can increase the quality and sustainability of research software by encouraging the adoption of these recommendations.", "num_citations": "81\n", "authors": ["1168"]}
{"title": "The semantic grid: Myth busting and bridge building\n", "abstract": " The Semantic Grid is an extension of the current Grid in which information and services are given well-defined meaning, better enabling computers and people to work in cooperation. The full richness of the Grid ambition depends upon realizing the Semantic Grid, but it is still, to many, a mysterious hybrid of the Semantic Web and the Grid, both of which are subject to myths and misunderstandings. In this paper we explain the changing landscape of the Grid, and describe the bridge-building and myth busting needed to achieve the Semantic Grid.", "num_citations": "81\n", "authors": ["1168"]}
{"title": "API-centric linked data integration: The open PHACTS discovery platform case study\n", "abstract": " Data integration is a key challenge faced in pharmacology where there are numerous heterogeneous databases spanning multiple domains (e.g.\u00a0chemistry and biology). To address this challenge, the Open PHACTS consortium has developed the Open PHACTS Discovery Platform that leverages Linked Data to provide integrated access to pharmacology databases. Between its launch in April 2013 and March 2014, the platform has been accessed over 13.5\u00a0million times and has multiple applications that integrate with it. In this work, we discuss how Application Programming Interfaces can extend the classical Linked Data Application Architecture to facilitate data integration. Additionally, we show how the Open PHACTS Discovery Platform implements this extended architecture.", "num_citations": "80\n", "authors": ["1168"]}
{"title": "Better software, better research\n", "abstract": " Modern scientific research isn't possible without software. However, its vital role is often overlooked by funders, universities, assessment committees, and even the research community itself. This is a serious issue that needs urgent attention. This article raises a number of points concerning quality, code review, and openness; development practices and training in scientific computing; career recognition of research software engineers; and sustainability models for funding scientific software. We must get software recognized to be the first-class experimental scientific instrument that it is and get \"better software for better research.\"", "num_citations": "80\n", "authors": ["1168"]}
{"title": "The Software Sustainability Institute: Changing Research Software Attitudes and Practices.\n", "abstract": " Software is critical to research. A recent survey showed that 84 percent of researchers view developing software as\" important or very important for their own research.\"[1] Despite this exalted position, little emphasis is placed on developing good software, which meets the same rigorous specifications that researchers expect of their other tools. Many researchers are yet to be convinced of the importance of developing wellengineered software. Although we might disagree with this viewpoint, it's an understandable one, because the research community provides little if any reward for producing such software. A lack of reward leads researchers to choose quick fixes over a more considered, maintainable approach to development. The effect is to burden much research software with an intractable technical debt (where work is needed to correctly engineer the quick fixes that have been implemented). Researchers who are keen to develop or incorporate software into their research also face obstacles. It's difficult to gain the necessary development skills, because training in software engineering is difficult to obtain, and it's difficult to employ already trained developers on an academic project. In 2010, the Software Sustainability Institute was founded by the UK's Engineering and Physical Sciences Research Council (the largest of the UK government's seven research funding bodies).", "num_citations": "80\n", "authors": ["1168"]}
{"title": "Workflow discovery: the problem, a case study from e-science and a graph-based solution\n", "abstract": " Much has been written on the promise of Web service discovery and (semi-) automated composition. In this discussion, the value to practitioners of discovering and reusing existing service compositions, captured in workflows, is mostly ignored. This paper presents one solution to workflow discovery. Through a survey with 21 scientists and developers from the myGrid workflow environment, workflow discovery requirements are elicited. Through a user experiment with 13 scientists, an attempt is made to build a gold standard for workflow ranking. Through the design and implementation of a workflow discovery tool, a mechanism for ranking workflow fragments is provided based on graph sub-isomorphism matching. The tool evaluation, drawing on a corpus of 89 public workflows from bioinformatics and the results of the user experiment, finds that the average human ranking can largely be reproduced", "num_citations": "80\n", "authors": ["1168"]}
{"title": "Screen readers cannot see\n", "abstract": " Travelling upon the Web is difficult for visually impaired users since the Web pages are designed for visual interaction [6]. Visually impaired users usually use screen readers to access the Web in audio. However, unlike sighted users, screen readers cannot see the implicit structural and navigational knowledge encoded within the visual presentation of Web pages. Therefore, in a visually impaired user\u2019s environment, objects that support travel are missing or inaccessible. Our approach to remedy this is to annotate pages with an ontology, the Travel Ontology, that aims to encapsulate rich structural and navigational knowledge about these objects. We use Semantic Web technologies to make such knowledge explicit and computationally accessible. Our semi-automated tool, Dante identifies travel objects on Web pages, annotates them appropriately with the Travel Ontology and uses this to transform the\u00a0\u2026", "num_citations": "80\n", "authors": ["1168"]}
{"title": "Thesaurus construction through knowledge representation\n", "abstract": " Semantic metadata describing subject content plays a vital role in supporting indexing and retrieval in Digital Libraries. Mechanisms used to deliver this metadata include keyword collections, thesauri and classifications. Constructing a large thesaurus, however, is a difficult process which can be facilitated through the application of knowledge representation techniques developed for managing and reasoning about concepts. We describe such a scheme \u2013 a Description Logic (DL) \u2013 and show through an example how a DL can play a part in the classification construction process, aiding in the production of coherent hierarchies and ensuring that the relationships represented in a thesaurus are sensible.", "num_citations": "80\n", "authors": ["1168"]}
{"title": "Kaleidoquery: A visual query language for object databases\n", "abstract": " In this paper we describe Kaleidoquery, a visual query language for object databases with the same expressive power as OQL. We will describe the design philosophy behind the filter flow nature of Kaleidoquery and present each of the language's constructs, giving examples and relating them to OQL. The Kaleidoquery language is described independent of any implementation details, but a brief description of a 3D interface currently under construction for Kaleidoquery is presented. The queries in this implementation of the language are translated into OQL and then passed to the object database O 2 for evaluation.", "num_citations": "78\n", "authors": ["1168"]}
{"title": "Applying linked data approaches to pharmacology: Architectural decisions and implementation\n", "abstract": " The discovery of new medicines requires pharmacologists to interact with a number of information sources ranging from tabular data to scientific papers, and other specialized formats. In this application report, we describe a linked data platform for integrating multiple pharmacology datasets that form the basis for several drug discovery applications. The functionality offered by the platform has been drawn from a collection of prioritised drug discovery business questions created as part of the Open PHACTS project, a collaboration of research institutions and major pharmaceutical companies. We describe the architecture of the platform focusing on seven design decisions that drove its development with the aim of informing others developing similar software in this or other domains. The utility of the platform is demonstrated by the variety of drug discovery applications being built to access the integrated data.", "num_citations": "75\n", "authors": ["1168"]}
{"title": "Guest editors\u2019 introduction to the special section on scientific workflows\n", "abstract": " Business-oriented workflows have been studied since the 70\u2019s under various names (office automation, workflow management, business process management) and by different communities, including the database community. Much basic and applied research has been conducted over the years, eg theoretical studies of workflow languages and models (based on Petri-nets or process calculi), their properties, transactional behavior, etc.Recently, and largely unnoticed by the database community, scientific workflows have gained momentum due to their central role in e-Science and cyberinfrastructure applications, ie, where scientists need to \u201cglue\u201d together data management, analysis, simulation, and visualization services over often voluminous and (structurally and semantically) complex, distributed scientific data and services. While sharing commonalities with their business workflow relatives, scientific workflows often pose different challenges. For example, scientific workflows are typically data-centric, dataflow-oriented \u201canalysis pipelines\u201d(as opposed to taskcentric and control-flow oriented business workflows) and can be very computationally expensive (often requiring parallel and/or Grid computing capabilities). Another characteristic is that scientific workflows are often more metadata and annotation-intensive, since repurposing of a scientific data product in another scientist\u2019s study requires detailed (and preferably machine-processable) context and data provenance information. Finally, scientists typically are rather individualistic and are more likely to create their own \u201cknowledge discovery workflows\u201d, whereas in business, users are\u00a0\u2026", "num_citations": "75\n", "authors": ["1168"]}
{"title": "Data curation+ process curation= data integration+ science\n", "abstract": " In bioinformatics, we are familiar with the idea of curated data as a prerequisite for data integration. We neglect, often to our cost, the curation and cataloguing of the processes that we use to integrate and analyse our data. Programmatic access to services, for data and processes, means that compositions of services can be made that represent the in silico experiments or processes that bioinformaticians perform. Data integration through workflows depends on being able to know what services exist and where to find those services. The large number of services and the operations they perform, their arbitrary naming and lack of documentation, however, mean that they can be difficult to use. The workflows themselves are composite processes that could be pooled and reused but only if they too can be found and understood. Thus appropriate curation, including semantic mark-up, would enable processes to be\u00a0\u2026", "num_citations": "73\n", "authors": ["1168"]}
{"title": "Towards open science: the myExperiment approach\n", "abstract": " By making research content more reusable, and providing a social infrastructure that facilitates sharing, the human aspects of the scholarly knowledge cycle may be accelerated and \u2018time\u2010to\u2010discovery\u2019 reduced. We propose that the key to this is the sharing of methods and processes. We present myExperiment, a social web site for discovering, sharing and curating Scientific Workflows and experiment plans, and describe how myExperiment facilitates the management and sharing of research workflows, supports a social model for content curation tailored to the researcher and community, and supports Open Science by exposing content and functionality to the users' tools and applications. Based on this, we introduce the notion of the Research Object\u2014the work objects that are built, transformed and published in the course of scientific experiments\u2014and suggest that by encapsulating methods with results we can\u00a0\u2026", "num_citations": "72\n", "authors": ["1168"]}
{"title": "Knowledge based information integration systems\n", "abstract": " Information integration systems provide facilities that support access to heterogeneous information sources in a way that isolates users from differences in the formats, locations and facilities of those sources. A number of systems have been proposed that exploit knowledge based techniques to assist with information integration, but it is not always obvious how proposals differ from each other in their scope, in the quality of integration afforded, or in the cost of exploitation. This paper presents a framework for the comparison of proposals for information integration systems, and applies the framework to a range of representative proposals. It is shown that proposals differ greatly in all of the criteria stated and that the selection of an approach is thus highly dependent on the requirements of specific applications.", "num_citations": "72\n", "authors": ["1168"]}
{"title": "Towards FAIR principles for research software\n", "abstract": " The FAIR Guiding Principles, published in 2016, aim to improve the findability, accessibility, interoperability and reusability of digital research objects for both humans and machines. Until now the FAIR principles have been mostly applied to research data. The ideas behind these principles are, however, also directly relevant to research software. Hence there is a distinct need to explore how the FAIR principles can be applied to software. In this work, we aim to summarize the current status of the debate around FAIR and software, as basis for the development of community-agreed principles for FAIR research software in the future. We discuss what makes software different from data with regard to the application of the FAIR principles, and which desired characteristics of research software go beyond FAIR. Then we present an analysis of where the existing principles can directly be applied to software, where they\u00a0\u2026", "num_citations": "71\n", "authors": ["1168"]}
{"title": "FAIR principles: interpretations and implementation considerations\n", "abstract": " The FAIR principles have been widely cited, endorsed and adopted by a broad range  of stakeholders since their publication in 2016. By intention, the 15 FAIR  guiding principles do not dictate specific technological implementations, but  provide guidance for improving Findability, Accessibility,  Interoperability and Reusability of digital resources. This has likely  contributed to the broad adoption of the FAIR principles, because individual  stakeholder communities can implement their own FAIR solutions. However, it has  also resulted in inconsistent interpretations that carry the risk of leading to  incompatible implementations. Thus, while the FAIR principles are formulated on  a high level and may be interpreted and implemented in different ways, for true  interoperability we need to support convergence in implementation choices that  are widely accessible and (re)-usable. We introduce the concept of FAIR\u00a0\u2026", "num_citations": "71\n", "authors": ["1168"]}
{"title": "FAIRDOMHub: a repository and collaboration environment for sharing systems biology research\n", "abstract": " The FAIRDOMHub is a repository for publishing FAIR (Findable, Accessible, Interoperable and Reusable) Data, Operating procedures and Models (https://fairdomhub.org/) for the Systems Biology community. It is a web-accessible repository for storing and sharing systems biology research assets. It enables researchers to organize, share and publish data, models and protocols, interlink them in the context of the systems biology investigations that produced them, and to interrogate them via API interfaces. By using the FAIRDOMHub, researchers can achieve more effective exchange with geographically distributed collaborators during projects, ensure results are sustained and preserved and generate reproducible publications that adhere to the FAIR guiding principles of data stewardship.", "num_citations": "69\n", "authors": ["1168"]}
{"title": "The open provenance model (v1. 01)\n", "abstract": " In this paper, we introduce the Open Provenance Model, a model for provenance that is designed to meet the following requirements:(1) To allow provenance information to be exchanged between systems, by means of a compatibility layer based on a shared provenance model.(2) To allow developers to build and share tools that operate on such a provenance model.(3) To define the model in a precise, technology-agnostic manner.(4) To support a digital representation of provenance for any\\thing\", whether produced by computer systems or not.(5) To define a core set of rules that identify the valid inferences that can be made on provenance graphs.", "num_citations": "68\n", "authors": ["1168"]}
{"title": "Ontologies in bioinformatics\n", "abstract": " Molecular biology offers a large, complex and volatile domain that tests knowledge representation techniques to the limit of their fidelity, precision, expressivity and adaptability. The discipline of molecular biology and bioinformatics relies greatly on the use of community knowledge, rather than laws and axioms, to further understanding, and knowledge generation. This knowledge has traditionally been kept as natural language. Given the exponential growth of already large quantities of data and associated knowledge, this is an unsustainable form of representation. This knowledge needs to be stored in a computationally amenable form and ontologies offer a mechanism for creating a shared understanding of a community for both humans and computers. Ontologies have been built and used for many domains and this chapter explores their role within bioinformatics. Structured classifications have a long\u00a0\u2026", "num_citations": "68\n", "authors": ["1168"]}
{"title": "The Manchester multimedia information system\n", "abstract": " A Multimedia Information System (MMIS) is a repository for all types of electronically representable data (O\u2019Docherty et al., 1990). Conventional databases provide a large set of operations for retrieval of simple data types. The simplest way of extending this to multimedia objects is to store and retrieve on the basis of a few manually entered associated attributes or links. The true potential of multimedia databases is realised when a rich set of operations is provided to allow transparent manipulation of data objects of all media. This can best be achieved through content retrieval, based on the automatic interpretation of medium objects. Automatic content retrieval avoids the problems of inconsistency, subjectivity and the labour-intensiveness of manual entry. MMISs with content retrieval will have wide application in industry, medicine, education and the military. The Multimedia Group at Manchester University\u00a0\u2026", "num_citations": "65\n", "authors": ["1168"]}
{"title": "Quality, trust, and utility of scientific data on the web: Towards a joint model\n", "abstract": " In science, quality is paramount. As scientists increasingly look to the Web to share and discover scientific data, there is a growing need to support the scientist in assessing the quality of that data. However, quality is an ambiguous and overloaded term. In order to support the scientific user in discovering useful data we have systematically examined the nature of\" quality\" by exploiting three, prevalent properties of scientific data sets:(1) that data quality is commonly defined objectively;(2) the provenance and lineage in its production has a well understood role; and (3)\" fitness-for-use\" is a definition of utility rather than quality or trust, where the quality and trust-worthiness of the data and the entities that produced that data inform its utility. Our study is presented in two stages. First we review existing information quality dimensions and detail an assessment-oriented classification. We introduce definitions for quality, trust\u00a0\u2026", "num_citations": "63\n", "authors": ["1168"]}
{"title": "Towards BioDBcore: a community-defined information specification for biological databases\n", "abstract": " The present article proposes the adoption of a community-defined, uniform, generic description of the core attributes of biological databases, BioDBCore. The goals of these attributes are to provide a general overview of the database landscape, to encourage consistency and interoperability between resources; and to promote the use of semantic and syntactic standards. BioDBCore will make it easier for users to evaluate the scope and relevance of available resources. This new resource will increase the collective impact of the information present in biological databases.", "num_citations": "63\n", "authors": ["1168"]}
{"title": "Recycling workflows and services through discovery and reuse\n", "abstract": " Scientific workflows are becoming a valuable tool for scientists to capture and automate e\u2010Science procedures. Their success brings the opportunity to publish, share, reuse and re\u2010purpose this explicitly captured knowledge. Within the ^my Grid project, we have identified key resources that can be shared including complete workflows, fragments of workflows and constituent services. We have examined the alternative ways that these resources can be described by their authors (and subsequent users) and developed a unified descriptive model to support their later discovery. By basing this model on existing standards, we have been able to extend existing Web service and Semantic Web service infrastructure whilst still supporting the specific needs of the e\u2010Scientist. The ^my Grid components enable a workflow life\u2010cycle that extends beyond execution to include the discovery of previous relevant designs, the\u00a0\u2026", "num_citations": "63\n", "authors": ["1168"]}
{"title": "A foundation for tool based mobility support for visually impaired web users\n", "abstract": " Users make journeys through the Web. Web travel encompasses the tasks of orientation and navigation, the environment and the purpose of the journey. The ease of travel, its mobility, varies from page to page and site to site. For visually impaired users, in particular, mobility is reduced; the objects that support travel are inaccessible or missing altogether. Web development tools need to include support to increase mobility. We present a framework for finding and classifying travel objects within Web pages. The evaluation carried out has shown that this framework supports a systematic and consistent method for assessing travel upon the Web. We propose that such a framework can provide the foundation for a semi-automated tool for the support of travel upon the Web.", "num_citations": "63\n", "authors": ["1168"]}
{"title": "Composing different models of computation in Kepler and Ptolemy II\n", "abstract": " A model of computation (MoC) is a formal abstraction of execution in a computer. There is a need for composing MoCs in e-science. Kepler, which is based on Ptolemy II, is a scientific workflow environment that allows for MoC composition. This paper explains how MoCs are combined in Kepler and Ptolemy II and analyzes which combinations of MoCs are currently possible and useful. It demonstrates the approach by combining MoCs involving dataflow and finite state machines. The resulting classification should be relevant to other workflow environments wishing to combine multiple MoCs.", "num_citations": "62\n", "authors": ["1168"]}
{"title": "23 CHAPTER Enhancing Services and Applications with Knowledge and Semantics\n", "abstract": " What do knowledge and the Grid have to do with each other? The virtual organizations that define Grid systems (Chapter 4) are formed to solve problems, and prob-lem solving is ultimately about the use and generation of knowledge. Knowledge is used to interpret existing information; for prediction; to change the way that scien-tific research or business is done; and ultimately for the pursuit, creation, and dissemination of further knowledge. Scientists use knowledge to steer instruments or experiments; businesses use knowledge to link data together in new insightful ways. The collaborative problem-solving environments that exploit and generate domain knowledge urgently need the sophisticated resource-sharing modalities supported by Grid technologies (281). Thus, a first connection between knowledge and the Grid concerns what we call \u201cknowledge on the Grid,\u201d or \u201cknowledge for Grid applications.\u201d A second connection is equally important. Knowledge is crucial for the flexible and dynamic middleware embodied by the Open Grid Service Architecture (OGSA, Chapter 17). The dynamic discovery, formation, and disbanding of ad hoc virtual organizations of (third-party) resources require that Grid middleware be able to use and process knowledge about the availability of services; their purpose; the way they can be combined and configured or substituted; and how they are discovered, are invoked, and evolve. Knowledge is found in protocols (eg, policy or provisioning) and in service descriptions such as the service data elements of OGSA services. The classification of computational and data resources, performance metrics, job control\u00a0\u2026", "num_citations": "62\n", "authors": ["1168"]}
{"title": "Multimedia Support and Authoring in Microcosm: An Extended Model\n", "abstract": " This paper discusses the issues involved in putting the media into a hypermedia system. The main argument of the paper is that to-date media representation and underlying link strategies have been too closely tied together in the move from hypertext to hypermedia. We argue that it is necessary to separate the issues of media from link structure, and we present a model which solves some of the problems of genuine media integration in a hypermedia system. At the same time the model provides support for the creation of links between data of different media types in a conceptually meaningful way. The paper describes the design of Microcosm++, an object oriented extensible service-based architecture for building consistent integrated hypermedia systems. It is based on the Microcosm hypermedia system which was developed at Southampton. The current implementation of Microcosm++ demonstrates the flexibility of object-based services for making hypermedia more viable in a working environment. The approach described reduces authoring effort significantly while at the same time increasing the integrity of the link structures and providing a unified model for media integration.", "num_citations": "62\n", "authors": ["1168"]}
{"title": "Using provenance to manage knowledge of in silico experiments\n", "abstract": " This article offers a briefing in one of the knowledge management issues of in silico experimentation in bioinformatics. Recording of the provenance of an experiment\u2014what was done; where, how and why, etc. is an important aspect of scientific best practice that should be extended to in silico experimentation. We will do this in the context of eScience which has been part of the move of bioinformatics towards an industrial setting. Despite the computational nature of bioinformatics, these analyses are scientific and thus necessitate their own versions of typical scientific rigour. Just as recording who, what, why, when, where and how of an experiment is central to the scientific process in laboratory science, so it should be in silico science. The generation and recording of these aspects, or provenance, of an experiment are necessary knowledge management goals if we are to introduce scientific rigour into routine\u00a0\u2026", "num_citations": "61\n", "authors": ["1168"]}
{"title": "On the use of agents in a bioinformatics grid\n", "abstract": " My Grid is an e-Science Grid project that aims to help biologists and bioinformaticians to perform workflow-based in silico experiments, and help them to automate the management of such workflows through personalisation, notification of change and publication of experiments. In this paper, we describe the architecture of my Grid and how it will be used by the scientist. We then show how my Grid can benefit from agents technologies. We have identified three key uses of agent technologies in my Grid: user agents, able to customize and personalise data, agent communication languages offering a generic and portable communication medium, and negotiation allowing multiple distributed entities to reach service level agreements.", "num_citations": "61\n", "authors": ["1168"]}
{"title": "A formal semantics for the Taverna 2 workflow model\n", "abstract": " This paper presents a formal semantics for the Taverna 2 scientific workflow system. Taverna 2 is a successor to Taverna, an open-source workflow system broadly adopted within the e-science community worldwide. The new version improves upon the existing model in two main ways: (i) by adding support for data pipelining, which in turns enables input streams of indefinite length to be processed efficiently; and (ii) by providing new extensibility points that make it possible to add new operators to the workflow model. Consistent with previous work by some of the authors, we use trace semantics to describe the effect of workflow computations, and we show how they can be used to describe the new features in the Taverna 2 model.", "num_citations": "60\n", "authors": ["1168"]}
{"title": "Performing statistical analyses on quantitative data in Taverna workflows: an example using R and maxdBrowse to identify differentially-expressed genes from microarray data\n", "abstract": " There has been a dramatic increase in the amount of quantitative data derived from the measurement of changes at different levels of biological complexity during the post-genomic era. However, there are a number of issues associated with the use of computational tools employed for the analysis of such data. For example, computational tools such as R and MATLAB require prior knowledge of their programming languages in order to implement statistical analyses on data. Combining two or more tools in an analysis may also be problematic since data may have to be manually copied and pasted between separate user interfaces for each tool. Furthermore, this transfer of data may require a reconciliation step in order for there to be interoperability between computational tools. Developments in the Taverna workflow system have enabled pipelines to be constructed and enacted for generic and ad hoc analyses of\u00a0\u2026", "num_citations": "59\n", "authors": ["1168"]}
{"title": "Knowledge integration: In silico experiments in bioinformatics\n", "abstract": " Publisher SummaryBiologists, aided by bioinformaticians, have become knowledge workers, intelligently weaving together the information available to the community, linking and correlating it meaningfully, and generating even more information. Many bio-Grid projects focus on the sharing of computational resources, large scale data movement and replication for simulations, remote instrumentation steerage, high-throughput sequence analysis, or image processing, as in the Biomedical Informatics Research Network (BIRN) project. However, much of bioinformatics involves a scientific process with relatively modest computational needs but significant semantic and data complexity. The myGrid project is building high-level services for integrating applications and data resources, concentrating on dynamic resource discovery, workflow specification and dynamic enactment, and distributed query processing. These\u00a0\u2026", "num_citations": "59\n", "authors": ["1168"]}
{"title": "A Description Logic Based Schema for the Classification of Medical Data.\n", "abstract": " The European Galen project aims to promote the sharing and re-use of medical data by providing a concept model which can be used by application designers as a exible and extensible classi cation schema. A description logic style terminological knowledge representation system called Grail has been developed speci cally for this task. Using a description logic based schema has a number of important bene ts including coherence checking, schema enrichment and query optimisation.In order to support a variety of design requirements Grail includes transitive closure of roles and general concept inclusions. Replacing the Grail classi er's existing structural subsumption algorithm with a sound, provably complete and decidable tableaux calculus based algorithm would have many attractions if the intractability problem could be mitigated by suitable optimisations. The optimisation of non-deterministic constraint expansion would be of particular importance as large numbers of these constraints can be introduced by general concept inclusions. Both intelligent back-tracking and the use of meta-knowledge to guide constraint expansion are being studied as possible methods of tackling this problem.", "num_citations": "57\n", "authors": ["1168"]}
{"title": "GIMS-a data warehouse for storage and analysis of genome sequence and functional data\n", "abstract": " Effective analysis of genome sequences and associated functional data requires access to many different kinds of biological information. For example, when analysing gene expression data, it may be useful to have access to the sequences upstream of the genes, or to the cellular location of their protein products. Such information is currently stored in different formats at different sites in a way that does not readily allow integrated analyses. The Genome Information Management System (GIMS) is an object database that integrates genome sequence data with functional data on the transcriptome and on protein-protein interactions in a single data warehouse. We have used GIMS to store the Saccharomyces cerevisiae (yeast) genome and to demonstrate how the integrated storage of diverse kinds of genomic data can be beneficial for analysing data using context-rich queries and analyses. GIMS allows data to be\u00a0\u2026", "num_citations": "56\n", "authors": ["1168"]}
{"title": "Linking multiple workflow provenance traces for interoperable collaborative science\n", "abstract": " Scientific collaboration increasingly involves data sharing between separate groups. We consider a scenario where data products of scientific workflows are published and then used by other researchers as inputs to their workflows. For proper interpretation, shared data must be complemented by descriptive metadata. We focus on provenance traces, a prime example of such metadata which describes the genesis and processing history of data products in terms of the computational workflow steps. Through the reuse of published data, virtual, implicitly collaborative experiments emerge, making it desirable to compose the independently generated traces into global ones that describe the combined executions as single, seamless experiments. We present a model for provenance sharing that realizes this holistic view by overcoming the various interoperability problems that emerge from the heterogeneity of workflow\u00a0\u2026", "num_citations": "55\n", "authors": ["1168"]}
{"title": "Metadata management in the taverna workflow system\n", "abstract": " There seems to be a general consensus on the crucial role metadata can play for enhancing the functionalities of scientific workflows systems, e.g., workflow and service discovery, composition and provenance browsing, among others. However, in most cases their management is under-specified, if not left unaddressed at all. A step in this direction, the main contribution of the work presented in this paper is an overview of metadata and their management in the Taverna workflow system. In Taverna, we consider metadata to be a first class citizen in the system, in the sense that we fully cover their life cycle from their creation, through their use and curation until their eventual removal. We present the main steps of this cycle and present the models used for metadata specification. In doing so, we distinguish two classes of metadata: metadata that describe workflow related entities, such as services, workflows and sub\u00a0\u2026", "num_citations": "54\n", "authors": ["1168"]}
{"title": "Towards the preservation of scientific workflows\n", "abstract": " Some of the shared digital artefacts of digital research are executable in the sense that they describe an automated process which generates results. One example is the computational scientific workflow which is used to conduct automated data analysis, predictions and validations. We describe preservation challenges of scientific workflows, and suggest a framework to discuss the reproducibility of workflow results. We describe curation techniques that can be used to avoid theworkflow decay'that occurs when steps of the workflow are vulnerable to external change. Our approach makes extensive use of provenance information and also considers aggregate structures called Research Objects as a means for promoting workflow preservation.", "num_citations": "53\n", "authors": ["1168"]}
{"title": "myGrid and the drug discovery process\n", "abstract": " In its early development, Grid computing has focused on providing the computational power necessary for solving computationally intensive scientific problems. However, the scientific process in the life sciences is less demanding on computational power but contains a high degree of inherent heterogeneity, and semantic and task complexity. The myGrid project has developed a Grid-enabled middleware framework to manage this complexity associated with the scientific process within the bioinformatics domain. The drug discovery process is an example of a complex scientific problem that involves managing vast amounts of information. The technology developed by the myGrid project is applicable for managing many aspects of drug discovery and development by leveraging its technology for data storage, workflow enactment, change event notification, resource discovery and provenance management.", "num_citations": "53\n", "authors": ["1168"]}
{"title": "Delivering web service coordination capability to users\n", "abstract": " As web service technology matures there is growing interest in exploiting workflow techniques to coordinate web services. Bioinformaticians are a user community who combine web resources to perform in silico experiments. These users are scientists and not information technology experts they require workflow solutions that have a low cost of entry for service users and providers. Problems satisfying these requirements with current techniques led to the development of the Simple conceptual unified flow language (Scufl). Scufl is supported by the Freefluo enactment engine [1], and the Taverna editing workbench [3]. The extensibility of Scufl, supported by these tools, means that workflows coordinating web services can be matched to how users view their problems. The Taverna workbench exploits the web to keep Scufl simple by retrieving detail from URIs when required, and by scavenging the web for services\u00a0\u2026", "num_citations": "53\n", "authors": ["1168"]}
{"title": "The SEEK: a platform for sharing data and models in systems biology\n", "abstract": " Systems biology research is typically performed by multidisciplinary groups of scientists, often in large consortia and in distributed locations. The data generated in these projects tend to be heterogeneous and often involves high-throughput \u201comics\u201d analyses. Models are developed iteratively from data generated in the projects and from the literature. Consequently, there is a growing requirement for exchanging experimental data, mathematical models, and scientific protocols between consortium members and a necessity to record and share the outcomes of experiments and the links between data and models. The overall output of a research consortium is also a valuable commodity in its own right. The research and associated data and models should eventually be available to the whole community for reuse and future analysis.The SEEK is an open-source, Web-based platform designed for the management and\u00a0\u2026", "num_citations": "51\n", "authors": ["1168"]}
{"title": "Guiding the user: An ontology driven interface\n", "abstract": " We describe a novel query interface allowing the construction and manipulation of description logic expressions. The construction process is driven by the content of a conceptual model, guiding the user towards appropriate choices and providing a lucid interface.", "num_citations": "51\n", "authors": ["1168"]}
{"title": "Describing and classifying multimedia using the description logic GRAIL\n", "abstract": " Many applications would benefit if media objects such as images could be selected and classified (or clustered) such that 'conceptually similar' images are grouped together by content. This requires that image content be described by some coherent semantic domain model rather than relying on the use of keywords as in most commercial image database systems. However, a description of image contents cannot be predefined by prescribing what should be in the images but must incrementally evolve to link image instances with descriptions of what is actually there. Flexibility is required as the same image may be reused from many different application perspectives, and classified and reclassified by many different, unpredictable, and possibly contradictory interpretations of the same contents. We present preliminary work on the incremental and flexible description of image and video semantic content by the use of\u00a0\u2026", "num_citations": "51\n", "authors": ["1168"]}
{"title": "Ergot: A semantic-based system for service discovery in distributed infrastructures\n", "abstract": " The increasing number of available online services demands distributed architectures to promote scalability as well as semantics to enable their precise and efficient retrieval. Two common approaches toward this goal are Semantic Overlay Networks (SONs) and Distributed Hash Tables (DHTs) with semantic extensions. This paper presents ERGOT, a system that combines DHTs and SONs to enable semantic-based service discovery in distributed infrastructures such as Grids and Clouds. ERGOT takes advantage of semantic annotations that enrich service specifications in two ways: (i) services are advertised in the DHT on the basis of their annotations, thus allowing to establish a SON among service providers, (ii) annotations enable semantic-based service matchmaking, using a novel similarity measure between service requests and descriptions. Experimental evaluations confirmed the efficiency of ERGOT in\u00a0\u2026", "num_citations": "50\n", "authors": ["1168"]}
{"title": "Community-driven computational biology with Debian Linux\n", "abstract": " The Open Source movement and its technologies are popular in the bioinformatics community because they provide freely available tools and resources for research. In order to feed the steady demand for updates on software and associated data, a service infrastructure is required for sharing and providing these tools to heterogeneous computing environments. The Debian Med initiative provides ready and coherent software packages for medical informatics and bioinformatics. These packages can be used together in Taverna workflows via the UseCase plugin to manage execution on local or remote machines. If such packages are available in cloud computing environments, the underlying hardware and the analysis pipelines can be shared along with the software. Debian Med closes the gap between developers and users. It provides a simple method for offering new releases of software and data resources\u00a0\u2026", "num_citations": "49\n", "authors": ["1168"]}
{"title": "myExperiment\u2013a web 2.0 virtual research environment\n", "abstract": " INTRODUCTION e-Science was defined at the launch of the UK e-Science programme as being \u201cabout global collaboration in key areas of science and the next generation of infrastructure that will enable it\u201d(John Taylor, Director General of Research Councils). The techniques of e-Science help the scientist deal with increasingly large and increasingly complex scientific applications. Key to this is automation, and several scientific workflow tools have become established as a means of automating the processing of scientific data in a scalable and reusable way.The myExperiment Virtual Research Environment (VRE) provides a personalised environment which enables users to share, re-use and repurpose experiments. Our vision is that scientists should be able to swap workflows and other scientific objects as easily as citizens can share documents, photos and videos on the Web. Hence myExperiment owes far more to social networking websites such as MySpace (www. myspace. com) and YouTube (www. youtube. com) than to the traditional portals of Grid computing, and is immediately familiar to the new generation of scientists. Where many e-Science projects have focused on bringing computational resources to bear on \u201creducing time to discovery\u201d, we take a holistic view of the scholarly knowledge cycle and focus on reducing \u201ctime-toexperiment\u201d and \u201ctime-to-citation\u201d.", "num_citations": "49\n", "authors": ["1168"]}
{"title": "Rendering tables in audio: the interaction of structure and reading styles\n", "abstract": " Tables remain a persistent problem for visually impaired people using screen readers. Tables are complex structures that are widely used for different purposes such as spatial layout or data summarisation. The multi-dimensional nature of tables challenges the linear interaction styles typically supported by screen readers. To read a table, a user needs to maintain coherency of, and interact with more than one dimension. In this paper, we first characterise why tables are useful in print, but difficult to read in the audio. We present a survey of the relationship between table structure, intention and the reading styles employed to use the content of tables. We then present two different approaches for interacting with tables non-visually. These approaches are designed to support the characteristics of tables that make them such a popular and useful means of conveying information. The first approach provides a small table\u00a0\u2026", "num_citations": "49\n", "authors": ["1168"]}
{"title": "Kaleidoquery\u2014a flow-based visual language and its evaluation\n", "abstract": " This paper describes the Kaleidoquery visual query language for object databases and its comparative evaluation with object query language (OQL). The design philosophy behind the filter flow nature of Kaleidoquery and each of the language's constructs is described, and examples are given that allow comparisons to be made with OQL. This is followed by a description of an experiment with the Kaleidoquery and OQL languages. Two groups of subjects, programmers and non-programmers, were taught aspects of OQL and Kaleidoquery, and then tested under experimental conditions. Results show that both groups answered significantly more correct queries using certain constructs of the Kaleidoquery language.", "num_citations": "49\n", "authors": ["1168"]}
{"title": "Query processing with description logic ontologies over object-wrapped databases\n", "abstract": " This paper presents an approach to answering queries over an ontology modelled using a description logic. The ontology acts as a global schema, providing a declarative description of the concepts of the domain, the instances of which are stored in (potentially many) object-wrapped sources. Queries are expressed using terms from the rich vocabulary of the ontology, and are translated into an equivalent calculus expression, which references only the objects available in the source databases. The query is then optimized on the basis of information from the ontology and the source databases. Distinctive features of the approach include: the use of the expressive ALCQI description logic, which supports both ontology definition and query expression; the adoption of a global-as-view approach to relating the ontology to the sources; and the use of the ontology to direct semantic optimization of queries phrased over\u00a0\u2026", "num_citations": "48\n", "authors": ["1168"]}
{"title": "Web mobility guidelines for visually impaired surfers\n", "abstract": " The \u2018Towel\u2019project seeks to find solutions to problems encountered by visually impaired users when travelling in the World-Wide-Web (Web) by leveraging solutions found in real-world mobility. Visually impaired users find mobility on the Web particularly difficult because of the reliance of hypermedia on visual layout. Hypertext design and usability guidelines have traditionally concentrated upon navigation to facilitate this mobility; consequently other aspects of travel are neglected. This paper seeks to address these issues by extending current guidelines and design methods to include the real-world mobility concepts of orientation, memory, environment, preview and the purpose of the task at hand.", "num_citations": "48\n", "authors": ["1168"]}
{"title": "Complex Query Formulation Over Diverse Information Sources in TAMBIS.\n", "abstract": " Biologists increasingly need to ask complex questions over the large number of data and analysis tools that are available on the Internet. To do this, the individual resources need to be made to work together. The knowledge needed to accomplish this, for example about the locations of the sources and their capabilities, places barriers between biologists and the questions they would like to ask. The TAMBIS project (Transparent Access to Multiple Bioinformatics Information Sources) has sought to remove some of these barriers, thereby making the process of asking questions against multiple sources more straightforward. Central to the TAMBIS system is an ontology of bioinformatics and biological terms. Users express retrieval requests in terms of the concepts and relationships described in the ontology, rather than by making direct reference to individual sources. This allows TAMBIS to be used to formulate rich, declarative queries over multiple sources. The ontology is constructed in a manner that ensures only biologically meaningful queries can be posed. User\u2019s queries are constructed using an interactive ontology browsing and query construction tool, and are rewritten by a query planner for evaluation using a wrapper layer. This paper provides an overview of the TAMBIS approach to source integration, focusing on the way the ontology is used to support query formulation and refinement.", "num_citations": "47\n", "authors": ["1168"]}
{"title": "OILing the way to machine understandable bioinformatics resources\n", "abstract": " The complex questions and analyses posed by biologists, as well as the diverse data resources they develop, require the fusion of evidence from different, independently developed, and heterogeneous resources. The web, as an enabler for interoperability, has been an excellent mechanism for data publication and transportation. Successful exchange and integration of information, however, depends on a shared language for communication (a terminology) and a shared understanding of what the data means (an ontology). Without this kind of understanding, semantic heterogeneity remains a problem for both humans and machines. One means of dealing with heterogeneity in bioinformatics resources is through terminology founded upon an ontology. Bioinformatics resources tend to be rich in human readable and understandable annotation, with each resource using its own terminology. These resources are\u00a0\u2026", "num_citations": "47\n", "authors": ["1168"]}
{"title": "A comparison of using Taverna and BPEL in building scientific workflows: the case of caGrid\n", "abstract": " When the emergence of \u2018service\u2010oriented science,\u2019 the need arises to orchestrate multiple services to facilitate scientific investigation\u2014that is, to create \u2018science workflows.\u2019 We present here our findings in providing a workflow solution for the caGrid service\u2010based grid infrastructure. We choose BPEL and Taverna as candidates, and compare their usability in the lifecycle of a scientific workflow, including workflow composition, execution, and result analysis. Our experience shows that BPEL as an imperative language offers a comprehensive set of modeling primitives for workflows of all flavors; whereas Taverna offers a dataflow model and a more compact set of primitives that facilitates dataflow modeling and pipelined execution. We hope that this comparison study not only helps researchers to select a language or tool that meets their specific needs, but also offers some insight into how a workflow language and\u00a0\u2026", "num_citations": "45\n", "authors": ["1168"]}
{"title": "Building ontologies in DAML+ OIL\n", "abstract": " In this article we describe an approach to representing and building ontologies advocated by the Bioinformatics and Medical Informatics groups at the University of Manchester. The hand\u2010crafting of ontologies offers an easy and rapid avenue to delivering ontologies. Experience has shown that such approaches are unsustainable. Description logic approaches have been shown to offer computational support for building sound, complete and logically consistent ontologies. A new knowledge representation language, DAML + OIL, offers a new standard that is able to support many styles of ontology, from hand\u2010crafted to full logic\u2010based descriptions with reasoning support. We describe this language, the OilEd editing tool, reasoning support and a strategy for the language's use. We finish with a current example, in the Gene Ontology Next Generation (GONG) project, that uses DAML + OIL as the basis for moving the\u00a0\u2026", "num_citations": "45\n", "authors": ["1168"]}
{"title": "FAIR computational workflows\n", "abstract": " Computational workflows describe the complex multi-step methods that are used for  data collection, data preparation, analytics, predictive modelling, and  simulation that lead to new data products. They can inherently contribute to the  FAIR data principles: by processing data according to established metadata; by  creating metadata themselves during the processing of data; and by tracking and  recording data provenance. These properties aid data quality assessment and  contribute to secondary data usage. Moreover, workflows are digital objects in  their own right. This paper argues that FAIR principles for workflows need to  address their specific nature in terms of their composition of executable  software steps, their provenance, and their development.", "num_citations": "44\n", "authors": ["1168"]}
{"title": "Systematic integration of experimental data and models in systems biology\n", "abstract": " The behaviour of biological systems can be deduced from their mathematical models. However, multiple sources of data in diverse forms are required in the construction of a model in order to define its components and their biochemical reactions, and corresponding parameters. Automating the assembly and use of systems biology models is dependent upon data integration processes involving the interoperation of data and analytical resources. Taverna workflows have been developed for the automated assembly of quantitative parameterised metabolic networks in the Systems Biology Markup Language (SBML). A SBML model is built in a systematic fashion by the workflows which starts with the construction of a qualitative network using data from a MIRIAM-compliant genome-scale model of yeast metabolism. This is followed by parameterisation of the SBML model with experimental data from two repositories\u00a0\u2026", "num_citations": "44\n", "authors": ["1168"]}
{"title": "An informal description of Standard OIL and Instance OIL\n", "abstract": " This language has also been used as the basis for defining DAML-Ont, the ontology markup language from the DARPA sponsored DAML initiative (DAML stands for DARPA Agent Markup Language, see http://www. daml. org). A version of the DAML language, with working name DAML-OIL, was proposed in a message to the rdf-logic mailing list ( http://lists. w3. org/Archives/Public/www-rdf-logic/2000Nov/0094. html) and is very close to Standard OIL as described below.", "num_citations": "44\n", "authors": ["1168"]}
{"title": "CaGrid Workflow Toolkit: A taverna based workflow tool for cancer grid\n", "abstract": " In biological and medical domain, the use of web services made the data and computation functionality accessible in a unified manner, which helped automate the data pipeline that was previously performed manually. Workflow technology is widely used in the orchestration of multiple services to facilitate in-silico research. Cancer Biomedical Informatics Grid (caBIG) is an information network enabling the sharing of cancer research related resources and caGrid is its underlying service-based computation infrastructure. CaBIG requires that services are composed and orchestrated in a given sequence to realize data pipelines, which are often called scientific workflows. CaGrid selected Taverna as its workflow execution system of choice due to its integration with web service technology and support for a wide range of web services, plug-in architecture to cater for easy integration of third party extensions, etc. The\u00a0\u2026", "num_citations": "43\n", "authors": ["1168"]}
{"title": "The data playground: An intuitive workflow specification environment\n", "abstract": " Workflows systems are steadily finding their way into the work practices of scientists. This is particularly true in the in silico science of bioinformatics, where biological data can be processed by Web Services. In this paper, we investigate the potential of evolving the users\u2019 interaction with workflow environments so that it more closely relates to the mode in which their day to day work is carried out. We present the Data Playground, an environment designed to encourage the uptake of workflow systems in bioinformatics through more intuitive interaction by focusing the user on their data, rather than on the processes. We implement a prototype plug-in for the Taverna workflow environment and show how this can promote the creation of workflow fragments by automatically converting the users\u2019 interactions with data and Web Services into a more conventional workflow specification.", "num_citations": "43\n", "authors": ["1168"]}
{"title": "Best practices for workflow design: how to prevent workflow decay\n", "abstract": " In this position paper we present a set of best practices for workflow design to prevent workflow decay and increase reuse and re-purposing of scientific workflows. MyExperiment provides access to a large number of scientific workflows. However, scientists find it difficult to reuse or re-purpose these workflows for mainly two reasons: workflows suffer from decay over time and lack sufficient metadata to understand their purpose. We argue that good workflow design is a prerequisite for repairing a workflow, or redesigning an equivalent workflow pattern with new components. We present a set of best practices for workflow design and the semantic tooling that is being developed in the Workflow4Ever (Wf4Ever) project to support these best practices.", "num_citations": "42\n", "authors": ["1168"]}
{"title": "e-Science and the Semantic Web: a symbiotic relationship\n", "abstract": " e-Science is scientific investigation performed through distributed global collaborations between scientists and their resources, and the computing infrastructure that enables this. Scientific progress increasingly depends on pooling know-how and results; making connections between ideas, people, and data; and finding and reusing knowledge and resources generated by others in perhaps unintended ways. It is about harvesting and harnessing the \u201ccollective intelligence\u201d of the scientific community. The Semantic Web is an extension of the current Web in which information is given well-defined meaning to facilitate sharing and reuse, better enabling computers and people to work in cooperation. Applying the Semantic Web paradigm to e-Science has the potential to bring significant benefits to scientific discovery. We identify the benefits of lightweight and heavyweight approaches, based on our experiences\u00a0\u2026", "num_citations": "42\n", "authors": ["1168"]}
{"title": "Uk research software survey 2014\n", "abstract": " This spreadsheet contains the anonymised data collected as part of a survey of UK researchers in their use of research software.We asked people specifically about \u201cresearch software\u201d which we defined as:\u201cSoftware that is used to generate, process or analyse results that you intend to appear in a publication (either in a journal, conference paper, monograph, book or thesis).\u00a0Research software can be anything from a few lines of code written by yourself, to a professionally developed software package.\u00a0Software that does not generate, process or analyse results - such as word processing software, or the use of a web search - does not count as \u2018research software\u2019 for the purposes of this survey.\u201dWe contacted 1,000 randomly selected researchers at each of 15 Russell Group universities. From the 15,000 invitations to complete the survey, we received 417 responses \u2013 a rate of 3% which is fairly normal for a blind survey.\u00a0We used Google Forms to collect responses.The responses have good\u00a0representation from across the disciplines, seniorities and genders. This is a statistically significant number of responses that can be used to represent\u00a0the views of people in research-intensive universities in the UK.An overview of the data is available on the worksheet \"Summary data\". Responses to questions are\u00a0ordered by unique respondent ID. Please read the \"README\" worksheet for additional information about the collection and processing of this data.This survey\u00a0data is licensed under a Creative Commons by Attribution licence. Copyright resides with The University of Edinburgh on behalf of the Software Sustainability Institute.Please cite as: APA\u00a0\u2026", "num_citations": "41\n", "authors": ["1168"]}
{"title": "A semantic web-based approach to knowledge management for grid applications\n", "abstract": " Knowledge has become increasingly important to support intelligent process automation and collaborative problem solving in large-scale science over the Internet. This paper addresses distributed knowledge management, its approach and methodology, in the context of grid application. We start by analyzing the nature of grid computing and its requirements for knowledge support; then, we discuss knowledge characteristics and the challenges for knowledge management on the grid. A semantic Web-based approach is proposed to tackle the six challenges of the knowledge lifecycle - namely, those of acquiring, modeling, retrieving, reusing, publishing, and maintaining knowledge. To facilitate the application of the approach, a systematic methodology is conceived and designed to provide a general implementation guideline. We use a real-world Grid application, the GEODISE project, as a case study in which the\u00a0\u2026", "num_citations": "41\n", "authors": ["1168"]}
{"title": "A semi-automated workflow for biodiversity data retrieval, cleaning, and quality control\n", "abstract": " The compilation and cleaning of data needed for analyses and prediction of species distributions is a time consuming process requiring a solid understanding of data formats and service APIs provided by biodiversity informatics infrastructures. We designed and implemented a Taverna-based Data Refinement Workflow which integrates taxonomic data retrieval, data cleaning, and data selection into a consistent, standards-based, and effective system hiding the complexity of underlying service infrastructures. The workflow can be freely used both locally and through a web-portal which does not require additional software installations by users.", "num_citations": "39\n", "authors": ["1168"]}
{"title": "Elements of a computational infrastructure for social simulation\n", "abstract": " Applications of simulation modelling in social science domains are varied and increasingly widespread. The effective deployment of simulation models depends on access to diverse datasets, the use of analysis capabilities, the ability to visualize model outcomes and to capture, share and re-use simulations as evidence in research and policy-making. We describe three applications of e-social science that promote social simulation modelling, data management and visualization. An example is outlined in which the three components are brought together in a transport planning context. We discuss opportunities and benefits for the combination of these and other components into an e-infrastructure for social simulation and review recent progress towards the establishment of such an infrastructure.", "num_citations": "38\n", "authors": ["1168"]}
{"title": "Requirements and services for metadata management\n", "abstract": " Knowledge-intensive applications pose new challenges to metadata management, including distribution, access control, uniformity of access, and evolution in time. This paper identifies general requirements for metadata management and describes a simple model and service that focuses on RDF metadata to address these requirements.", "num_citations": "38\n", "authors": ["1168"]}
{"title": "A framework for describing visual interfaces to databases\n", "abstract": " In the field of HCI there exist many formalisms for analysing, describing and evaluating interactive systems. However, in developing and evaluating user interfaces to databases, we found it necessary to be able to describe presentation and interaction aspects that are catered for poorly or not at all in current formalisms. This paper presents a framework for the systematic description of data model, presentation and interaction components that together form a graphical user interface. The utility of the framework is then demonstrated by showing how it can be used to describe two existing visual query interfaces. These examples show that the framework provides a systematic method for the concise description of graphical interfaces to databases that can be used either during interface design or as a communication aid.", "num_citations": "38\n", "authors": ["1168"]}
{"title": "I'll take that to go: Big data bags and minimal identifiers for exchange of large, complex datasets\n", "abstract": " Big data workflows often require the assembly and exchange of complex, multi-element datasets. For example, in biomedical applications, the input to an analytic pipeline can be a dataset consisting thousands of images and genome sequences assembled from diverse repositories, requiring a description of the contents of the dataset in a concise and unambiguous form. Typical approaches to creating datasets for big data workflows assume that all data reside in a single location, requiring costly data marshaling and permitting errors of omission and commission because dataset members are not explicitly specified. We address these issues by proposing simple methods and tools for assembling, sharing, and analyzing large and complex datasets that scientists can easily integrate into their daily workflows. These tools combine a simple and robust method for describing data collections (BDBags), data descriptions\u00a0\u2026", "num_citations": "37\n", "authors": ["1168"]}
{"title": "The evolution of standards and data management practices in systems biology\n", "abstract": " Systems biology involves the integration of multiple heterogeneous data sets, in order to model and predict biological processes. The domain\u2019s interdisciplinary nature requires data, models and other research assets to be formatted and described in standard ways to enable exchange and reuse. Infrastructure for Systems Biology Europe (ISBE) is a project to establish essential, centralized services for systems biology researchers throughout the systems biology lifecycle. A key component of ISBE is to support the management, integration and exchange of data, models, results and protocols. To inform further ISBE development, we surveyed the community to evaluate the uptake of available standards, and current practices of researchers in data and model management. The survey addressed four key areas as follows:", "num_citations": "37\n", "authors": ["1168"]}
{"title": "Semantics\u2010assisted problem solving on the semantic grid\n", "abstract": " In this paper we propose a distributed knowledge management framework for semantics and knowledge creation, population, and reuse on the grid. Its objective is to evolve the Grid toward the Semantic Grid with the ultimate purpose of facilitating problem solving in e\u2010Science. The framework uses ontology as the conceptual backbone and adopts the service\u2010oriented computing paradigm for information\u2010 and knowledge\u2010level computation. We further present a semantics\u2010based approach to problem solving, which exploits the rich semantic information of grid resource descriptions for resource discovery, instantiation, and composition. The framework and approach has been applied to a UK e\u2010Science project\u2014Grid Enabled Engineering Design Search and Optimisation in Engineering (GEODISE). An ontology\u2010enabled problem solving environment (PSE) has been developed in GEODISE to leverage the semantic\u00a0\u2026", "num_citations": "37\n", "authors": ["1168"]}
{"title": "DANTE: annotation and transformation of web pages for visually impaired users\n", "abstract": " Most Web pages are designed for visual interaction so the mobility, or ease of travel, of visually impaired Web travellers is reduced [2]. Objects that support travel and mobility are not in an appropriate form for nonvisual interaction. Our goal is to enhance the mobility of visually impaired Web travellers by annotating pages with a travel ontology that aims to encapsulate rich structural and navigational knowledge. We propose a semi-automated tool'Dante'which aims to analyse Web pages to extract travel objects, discover their roles, annotate them with a travel ontology and transform pages based on the annotations to enhance the provided mobility support. This poster introduces the travel ontology and presents how Web pages are annotated with this ontology to guide the transformations.", "num_citations": "37\n", "authors": ["1168"]}
{"title": "A pilot study to examine the mobility problems of visually impaired users travelling the web\n", "abstract": " The Towel project seeks to find solutions to problems encountered by both visually impaired and sighted users when travelling in the World Wide Web by leveraging solutions found in real-world mobility and applying them to the virtual world. Visually impaired users find mobility on the Web particularly difficult because of the reliance of hypermedia on visual layout and large viewable areas that facilitate and enhance sighted mobility. Hypertext design and usability has traditionally concentrated upon navigation to facilitate this mobility; consequently other aspects of travel are neglected and web mobility has suffered. Similarly, the Web Accessibility Initiative (WAI) Guidelines do not take a holistic view of travel and therefore in both these cases a fully rounded view of mobility cannot be formulated. This paper presents the basis for these assertions by drawing analogies between real-world and virtual-world mobility, and\u00a0\u2026", "num_citations": "37\n", "authors": ["1168"]}
{"title": "Understanding collaborative studies through interoperable workflow provenance\n", "abstract": " The provenance of a data product contains information about how the product was derived, and is crucial for enabling scientists to easily understand, reproduce, and verify scientific results. Currently, most provenance models are designed to capture the provenance related to a single run, and mostly executed by a single user. However, a scientific discovery is often the result of methodical execution of many scientific workflows with many datasets produced at different times by one or more users. Further, to promote and facilitate exchange of information between multiple workflow systems supporting provenance, the Open Provenance Model (OPM) has been proposed by the scientific workflow community. In this paper, we describe a new query model that captures implicit user collaborations. We show how this model maps to OPM and helps to answer collaborative queries, e.g., identifying combined\u00a0\u2026", "num_citations": "36\n", "authors": ["1168"]}
{"title": "Semantic and personalised service discovery\n", "abstract": " One of the most pervasive classes of services needed to support e-Science applications are those responsible for the discovery of resources. We have developed a solution to the problem of service discovery in a Semantic Web/Grid setting. We do this in the context of bioinformatics, which is the use of computational and mathematical techniques to store, manage, and analyse the data from molecular biology in order to answer questions about biological phenomena. Our specific application is myGrid (http://www. mygrid. org. uk) that is developing open source, service-based middleware upon which bioinformatics applications can be built. myGrid is specifically targeted at developing open source high-level service Grid middleware for bioinformatics.", "num_citations": "36\n", "authors": ["1168"]}
{"title": "Terminologies and terminology servers for information environments\n", "abstract": " Terminologies, or constrained vocabularies, are a potentially rich means of representing the metadata required for applications that have partially structured and incomplete dynamic data to describe, and exploratory and inexact queries to express. Such applications include digital libraries and multimedia repositories and software management; specific application communities include medicine and art. The authors propose that terminologies are ideal for meeting today's information requirements and that a dynamic terminology service is the appropriate architectural approach. They present the requirements of a terminology server, and describe the implementation and practical use of one developed by the authors that uses a description logic, GRAIL, to represent the terms. The terminology server has been extensively used in applications relating to medicine and is being used in the integration of diverse\u00a0\u2026", "num_citations": "36\n", "authors": ["1168"]}
{"title": "Sharing interoperable workflow provenance: A review of best practices and their practical application in CWLProv\n", "abstract": " Background           The automation of data analysis in the form of scientific workflows has become a widely adopted practice in many fields of research. Computationally driven data-intensive experiments using workflows enable automation, scaling, adaptation, and provenance support. However, there are still several challenges associated with the effective sharing, publication, and reproducibility of such workflows due to the incomplete capture of provenance and lack of interoperability between different technical (software) platforms.                             Results           Based on best-practice recommendations identified from the literature on workflow design, sharing, and publishing, we define a hierarchical provenance framework to achieve uniformity in provenance and support comprehensive and fully re-executable workflows equipped with domain-specific information. To realize this framework, we present\u00a0\u2026", "num_citations": "35\n", "authors": ["1168"]}
{"title": "Addendum: The FAIR Guiding Principles for scientific data management and stewardship\n", "abstract": " Addendum: The FAIR Guiding Principles for scientific data management and stewardship \u2014 Research@WUR Skip to main navigation Skip to search Skip to main content Research@WUR Logo Help & FAQ Home Researchers Research Units Research output Datasets Press / Media Activities Projects Prizes Search by expertise, name or affiliation Addendum: The FAIR Guiding Principles for scientific data management and stewardship Mark D. Wilkinson, Michel Dumontier, Ijsbrand Jan Aalbersberg, Gabrielle Appleton, Myles Axton, Arie Baak, Niklas Blomberg, Jan Willem Boiten, Luiz Bonino Da Silva Santos, Philip E. Bourne, Jildau Bouwman, Anthony J. Brookes, Tim Clark, Merc\u00e8 Crosas, Ingrid Dillo, Olivier Dumon, Scott Edmunds, Chris T. Evelo, Richard Finkers, Alejandra Gonzalez-Beltran Show 33 more Show less Alasdair JG Gray, Paul Groth, Carole Goble, Jeffrey S. Grethe, Jaap Heringa, Peter AC 't Hoen, \u2026", "num_citations": "35\n", "authors": ["1168"]}
{"title": "The evolution of myexperiment\n", "abstract": " The myExperiment social website for sharing scientific workflows, designed according to Web 2.0 principles, has grown to be the largest public repository of its kind. It is distinctive for its focus on sharing methods, its researcher-centric design and its facility to aggregate content into sharable `research objects'. This evolution of myExperiment has occurred hand in hand with its users. myExperiment now supports Linked Data as a step toward our vision of the future research environment, which we categorise here as 3rd generation e-Research.", "num_citations": "35\n", "authors": ["1168"]}
{"title": "Middleware to expand context and preview in hypertext\n", "abstract": " Movement, or mobility, is key to the accessibility, design, and usability of many hypermedia resources (websites); and key to good mobility is context and preview by probing. This is especially the case for visually impaired users when a hypertext anchor is inaccurately described or is described out of context. This means confusion and disorientation. Mobility is similarly reduced when the link target of the anchor has no relationship to the expected information present on the hypertext node (web-page). We suggest that confident movement with purpose, ease, and accuracy can only be achieved when complete contextual information and an accurate description of the proposed destination (preview) are available. Our past work (1) deriving mobility heuristics from mobility models,(2) transforming web-pages based on these heuristics, and (3) building tools to analyse and access these transformed pages; has shown us\u00a0\u2026", "num_citations": "35\n", "authors": ["1168"]}
{"title": "Automatic annotation of web services based on workflow definitions\n", "abstract": " Semantic annotations of web services can facilitate the discovery of services, as well as their composition into workflows. At present, however, the practical utility of such annotations is limited by the small number of service annotations available for general use. Resources for manual annotation are scarce, and therefore some means is required by which services can be automatically (or semi-automatically) annotated. In this paper, we show how information can be inferred about the semantics of operation parameters based on their connections to other (annotated) operation parameters within tried-and-tested workflows. In an open-world context, we can infer only constraints on the semantics of parameters, but these so-called loose annotations are still of value in detecting errors within workflows, annotations and ontologies, as well as in simplifying the manual annotation task.", "num_citations": "34\n", "authors": ["1168"]}
{"title": "Exploiting model-based techniques for user interfaces to databases\n", "abstract": " Model-based systems provide methods for supporting the systematic and efficient development of application interfaces. This paper examines how model-based technologies can be exploited to develop user interfaces to databases. To this end five model-based systems, namely Adept, HUMANOID, Mastermind, TADEUS and DRIVE are discussed through the use of a unifying case study which allows the examination of the approaches followed by the different systems.", "num_citations": "34\n", "authors": ["1168"]}
{"title": "BioVeL: a virtual laboratory for data analysis and modelling in biodiversity science and ecology\n", "abstract": " Making forecasts about biodiversity and giving support to policy relies increasingly on large collections of data held electronically, and on substantial computational capability and capacity to analyse, model, simulate and predict using such data. However, the physically distributed nature of data resources and of expertise in advanced analytical tools creates many challenges for the modern scientist. Across the wider biological sciences, presenting such capabilities on the Internet (as \u201cWeb services\u201d) and using scientific workflow systems to compose them for particular tasks is a practical way to carry out robust \u201cin silico\u201d science. However, use of this approach in biodiversity science and ecology has thus far been quite limited. BioVeL is a virtual laboratory for data analysis and modelling in biodiversity science and ecology, freely accessible via the Internet. BioVeL includes functions for accessing and analysing data through curated Web services; for performing complex in silico analysis through exposure of R programs, workflows, and batch processing functions; for on-line collaboration through sharing of workflows and workflow runs; for experiment documentation through reproducibility and repeatability; and for computational support via seamless connections to supporting computing infrastructures. We developed and improved more than 60 Web services with significant potential in many different kinds of data analysis and modelling tasks. We composed reusable workflows using these Web services, also incorporating R programs. Deploying these tools into an easy-to-use and accessible \u2018virtual laboratory\u2019, free via the Internet, we applied the\u00a0\u2026", "num_citations": "33\n", "authors": ["1168"]}
{"title": "Structuring research methods and data with the research object model: genomics workflows as a case study\n", "abstract": " One of the main challenges for biomedical research lies in the computer-assisted integrative study of large and increasingly complex combinations of data in order to understand molecular mechanisms. The preservation of the materials and methods of such computational experiments with clear annotations is essential for understanding an experiment, and this is increasingly recognized in the bioinformatics community. Our assumption is that offering means of digital, structured aggregation and annotation of the objects of an experiment will provide necessary meta-data for a scientist to understand and recreate the results of an experiment. To support this we explored a model for the semantic description of a workflow-centric Research Object (RO), where an RO is defined as a resource that aggregates other resources, e.g., datasets, software, spreadsheets, text, etc. We applied this model to a case study where we\u00a0\u2026", "num_citations": "33\n", "authors": ["1168"]}
{"title": "Small is beautiful: Summarizing scientific workflows using semantic annotations\n", "abstract": " Scientific workflows have become the workhorse of Big Data analytics for scientists. As well as being repeatable and optimizable pipelines that bring together datasets and analysis tools, workflows make-up an important part of the provenance of data generated from their execution. By faithfully capturing all stages in the analysis, workflows play a critical part in building up the audit-trail (a.k.a. provenance) meta-data for derived datasets and contributes to the veracity of results. Provenance is essential for reporting results, reporting the method followed, and adapting to changes in the datasets or tools. These functions, however, are hampered by the complexity of workflows and consequently the complexity of data-trails generated from their instrumented execution. In this paper we propose the generation of workflow description summaries in order to tackle workflow complexity. We elaborate reduction primitives for\u00a0\u2026", "num_citations": "33\n", "authors": ["1168"]}
{"title": "Discovering scientific workflows: The myexperiment benchmarks\n", "abstract": " Automation in science is increasingly marked by the use of workflow technology. The sharing of workflows through publication mechanisms or repositories supports the verifiability, reproducibility and extensibility of computational experiments. However, the subsequent discovery of workflows remains a challenge, both from a technological and sociological viewpoint. We investigate current practices in workflow sharing, re-use and discovery amongst life scientists chiefly using the Taverna workflow management system. The study draws on two key sources:(i) a survey of researchers drawn from 19 research labs and (ii) an analysis of scientists\u2019 behaviour on the myExperiment social network site, designed to encourage workflow exchange. The results reveal a multi-modal approach to workflow discovery, based on a mix of search on the content of the workflow and its situated context. We go on to develop a benchmark specifically for the evaluation of workflow discovery and to demonstrate it on two example approaches.", "num_citations": "33\n", "authors": ["1168"]}
{"title": "Knowledge discovery for biology with taverna\n", "abstract": " Life Science research has extended beyond in vivo and in vitro bench-bound science to incorporate in silico knowledge discovery, using resources that have been developed over time by different teams for different purposes and in different forms. The myGrid project has developed a set of software components and a workbench, Taverna, for building, running and sharing workflows that link third party bioinformatics services, such as databases, analytic tools and applications. Intelligently discovering prior services, workflow or data is aided by a Semantic Web of annotations, as is the building of the workflows themselves. Metadata associated with the workflow experiments, the provenance of the data outcomes and the record of the experimental process need to be flexible and extensible. Semantic Web metadata technologies would seem to be well-suited to building a Semantic Web of provenance. We have\u00a0\u2026", "num_citations": "33\n", "authors": ["1168"]}
{"title": "Engineering knowledge for engineering grid applications\n", "abstract": " Computing increasingly addresses collaboration; sharing; and interaction involving distributed resources. This has been fuelled in part by the emergence of Grid technologies and web services. Drawing on our expertise in the Geodise project 1 . We argue that there is a growing requirement for knowledge engineering methods that provide a semantic foundation for such distributed computing. Such methods also support the sharing and coordinated use of knowledge itself. In this paper we introduce a service-oriented knowledge engineering approach that seeks to provide knowledge orientated support for distributed grid-based computing. This approach has been implemented in a generic integrated architecture. The application context is the process of design search and optimisation in engineering. It demonstrates how knowledge has been captured and modelled, as well as illustrating how ontologies have been developed and deployed. The knowledge acquired has been made available and accessible through a portal that invokes a number of basic services.", "num_citations": "33\n", "authors": ["1168"]}
{"title": "Bioschemas: from potato salad to protein annotation\n", "abstract": " The life sciences have a wealth of data resources with a wide range of overlapping content. Key repositories, such as UniProt for protein data or Entrez Gene for gene data, are well known and their content easily discovered through search engines. However, there is a long-tail of bespoke datasets with important content that are not so prominent in search results. Building on the success of Schema. org for making a wide range of structured web content more discoverable and interpretable, eg food recipes, the Bioschemas community (http://bioschemas. org) aim to make life sciences datasets more findable by encouraging data providers to embed Schema. org markup in their resources.", "num_citations": "32\n", "authors": ["1168"]}
{"title": "Distilling structure in Taverna scientific workflows: a refactoring approach\n", "abstract": " Scientific workflows management systems are increasingly used to specify and manage bioinformatics experiments. Their programming model appeals to bioinformaticians, who can use them to easily specify complex data processing pipelines. Such a model is underpinned by a graph structure, where nodes represent bioinformatics tasks and links represent the dataflow. The complexity of such graph structures is increasing over time, with possible impacts on scientific workflows reuse. In this work, we propose effective methods for workflow design, with a focus on the Taverna model. We argue that one of the contributing factors for the difficulties in reuse is the presence of \"anti-patterns\", a term broadly used in program design, to indicate the use of idiomatic forms that lead to over-complicated design. The main contribution of this work is a method for automatically detecting such anti-patterns, and replacing them\u00a0\u2026", "num_citations": "32\n", "authors": ["1168"]}
{"title": "Enhancing and abstracting scientific workflow provenance for data publishing\n", "abstract": " Many scientists are using workflows to systematically design and run computational experiments. Once the workflow is executed, the scientist may want to publish the dataset generated as a result, to be, eg, reused by other scientists as input to their experiments. In doing so, the scientist needs to curate such dataset by specifying metadata information that describes it, eg its derivation history, origins and ownership. To assist the scientist in this task, we explore in this paper the use of provenance traces collected by workflow management systems when enacting workflows. Specifically, we identify the shortcomings of such raw provenance traces in supporting the data publishing task, and propose an approach whereby distilled, yet more informative, provenance traces that are fit for the data publishing task can be derived.", "num_citations": "32\n", "authors": ["1168"]}
{"title": "Servicemap: Providing map and gps assistance to service composition in bioinformatics\n", "abstract": " The wide use of Web services and scientific workflows has enabled bioinformaticians to reuse experimental resources and streamline data processing. This paper presents a follow-up work of our network analysis on my Experiment, an online scientific workflow repository. The motivation comes from two common questions raised by bio-scientists: 1) Given the services that I plan to use, what are other services usually used together with them? and 2) Given two or more services I plan to use together, can I find an operation chain to connect them based on others' past usage? Aiming to provide a system-level GPS-like support to answer the two questions, we present Service Map, a network model established to study the best practice of service use. Two approaches are proposed over the Service Map: association rule mining and relation-aware, cross workflow searching. Both approaches were validated using the real\u00a0\u2026", "num_citations": "32\n", "authors": ["1168"]}
{"title": "Benchmarking workflow discovery: a case study from bioinformatics\n", "abstract": " Automation in science is increasingly marked by the use of workflow technology. The sharing of workflows through repositories supports the verifiability, reproducibility and extensibility of computational experiments. However, the subsequent discovery of workflows remains a challenge, both from a sociological and technological viewpoint. Based on a survey with participants from 19 laboratories, we investigate the current practices in workflow sharing, re\u2010use and discovery among life scientists chiefly using the Taverna workflow management system. To address their perceived lack of effective workflow discovery tools, we go on to develop benchmarks for the evaluation of discovery tools, drawing on a series of practical exercises. We demonstrate the value of the benchmarks on two tools: one using graph matching and the other relying on text clustering. Copyright \u00a9 2009 John Wiley & Sons, Ltd.", "num_citations": "32\n", "authors": ["1168"]}
{"title": "TourisT: the application of a description logic based semantic hypermedia system for tourism\n", "abstract": " ABSTRACT Web-based Public Information Systems of the kind common in tourism do not satisfy the needs of the customer because they do not offer a sufficiently flexible linking environment capable of emulating the mediation role of a tourist adviser. We present the requirements of a tourism hypermedia system resulting from ethnographic studies of tourist advisers, and conclude that an open semantic hypermedia (SH) approach is appropriate. We present a novel and powerful SH prototype based on the use of a semantic model expressed as a terminology. The terminological model is implemented by a Description Logic, GRAIL, capable of the automatic and dynamic multi-dimensional classification of concepts, and hence the web pages they describe, We show how GRAIL-Link has been used within the TourisT hypermedia system and conclude with a discussion.", "num_citations": "30\n", "authors": ["1168"]}
{"title": "Panoply of utilities in Taverna\n", "abstract": " The Taverna e-Science Workbench is a central component of myGrid, a loosely coupled suite of middleware services designed to support in silico experiments in biology. Taverna enables the construction and enactment of complex workflows over resources on local and remote machines, allowing the automation of otherwise labour-intensive multi-step bioinformatics tasks. As the Taverna user community has grown, so has the demand for new features and additions. This paper outlines the functional requirements that have become apparent over the last year of working with domain scientists, along with the solutions implemented in both the Taverna workbench and the Freefluo enactment engine to address concerns relating to workflow construction and enactment, respectively", "num_citations": "29\n", "authors": ["1168"]}
{"title": "Accelerating scientists\u2019 knowledge turns\n", "abstract": " A \u201cknowledge turn\u201d is a cycle of a process by a professional, including the learning generated by the experience, deriving more good and leading to advance. The majority of scientific advances in the public domain result from collective efforts that depend on rapid exchange and effective reuse of results. We have powerful computational instruments, such as scientific workflows, coupled with widespread online information dissemination to accelerate knowledge cycles. However, turns between researchers continue to lag. In particular method obfuscation obstructs reproducibility. The exchange of \u201cResearch Objects\u201d rather than articles proposes a technical solution; however the obstacles are mainly social ones that require the scientific community to rethink its current value systems for scholarship, data, methods and software.", "num_citations": "28\n", "authors": ["1168"]}
{"title": "proximity: Walking the Link\n", "abstract": " Our society is consistently told that the world is becoming increasingly connected, that the Internet can join physically disparate people by means of email, Web sites, and chatrooms, and that the one'must have'is a personal domain name; in effect, that the virtual should be more respected than the physical. People are led to believe that computers, with the'net'as their focus, are their portal to other worlds, their communication mechanism to remote peoples,'blogging'their primary form of self expression. All this is in part true, but we think there are fundamental issues that are not addressed. The focus on only the virtual is skewing our perception to over-estimate the Web's importance. The increased complexity inherent in all large systems will become too great for many users as the Web develops and grows. The local environment, often most pertinent to the user, is currently completely ignored with regard to dynamic information giving. The Web's focus on information belies the fact that the world is also composed of physical artifacts. Therefore, we think that the next direction for the Web is the conjoining of the physical and virtual. We suggest that they must be connected because without a physical presence the virtual world cannot attain its full potential. To reduce the complexity and stress placed on the user, the Web should relate to the users' physical location and real-world artifacts encountered to make meaningful choices about what information is currently useful or required. In effect, the user acquires a real-world centric view of the Web in which the Web conforms to reality, not reality to the Web. The primary goal of our system,'proXimity', is to\u00a0\u2026", "num_citations": "28\n", "authors": ["1168"]}
{"title": "Meeting report from the fourth meeting of the Computational Modeling in Biology Network (COMBINE)\n", "abstract": " The Computational Modeling in Biology Network (COMBINE) is an initiative to coordinate the development of community standards and formats in computational systems biology and related fields. This report summarizes the topics and activities of the fourth edition of the annual COMBINE meeting, held in Paris during September 16\u201320 2013, and attended by a total of 96 people. This edition pioneered a first day devoted to modeling approaches in biology, which attracted a broad audience of scientists thanks to a panel of renowned speakers. During subsequent days, discussions were held on many subjects including the introduction of new features in the various COMBINE standards, new software tools that use the standards, and outreach efforts. Significant emphasis went into work on extensions of the SBML format, and also into community-building. This year\u2019s edition once again demonstrated that the\u00a0\u2026", "num_citations": "27\n", "authors": ["1168"]}
{"title": "Building a reason-able bioinformatics ontology using OIL\n", "abstract": " Ontologies will play an important role in bioinformatics, as they do in other disciplines, where they will provide a source of precisely defined terms that can be communicated across people and applications. The Ontology Inference Layer (OIL), is an ontology language that has an easy to use frame feel, yet at the same time allows users to exploit the full power of an expressive description logic. OilEd, an editor for OIL, uses reasoning to support ontology design, facilitating the development of ontologies that are both more detailed and more accurate. This paper presents a bioinformatics ontology building case study using OilEd to highlight the features of the combination of a frame representation and an expressive description logic.", "num_citations": "27\n", "authors": ["1168"]}
{"title": "Enabling precision medicine via standard communication of HTS provenance, analysis, and results\n", "abstract": " A personalized approach based on a patient's or pathogen\u2019s unique genomic sequence is the foundation of precision medicine. Genomic findings must be robust and reproducible, and experimental data capture should adhere to findable, accessible, interoperable, and reusable (FAIR) guiding principles. Moreover, effective precision medicine requires standardized reporting that extends beyond wet-lab procedures to computational methods. The BioCompute framework (https://w3id.org/biocompute/1.3.0) enables standardized reporting of genomic sequence data provenance, including provenance domain, usability domain, execution domain, verification kit, and error domain. This framework facilitates communication and promotes interoperability. Bioinformatics computation instances that employ the BioCompute framework are easily relayed, repeated if needed, and compared by scientists, regulators, test developers, and clinicians. Easing the burden of performing the aforementioned tasks greatly extends the range of practical application. Large clinical trials, precision medicine, and regulatory submissions require a set of agreed upon standards that ensures efficient communication and documentation of genomic analyses. The BioCompute paradigm and the resulting BioCompute Objects (BCOs) offer that standard and are freely accessible as a GitHub organization (https://github.com/biocompute-objects) following the \u201cOpen-Stand.org principles for collaborative open standards development.\u201d With high-throughput sequencing (HTS) studies communicated using a BCO, regulatory agencies (e.g., Food and Drug Administration [FDA\u00a0\u2026", "num_citations": "26\n", "authors": ["1168"]}
{"title": "Scientific lenses to support multiple views over linked chemistry data\n", "abstract": " When are two entries about a small molecule in different datasets the same? If they have the same drug name, chemical structure, or some other criteria? The choice depends upon the application to which the data will be put. However, existing Linked Data approaches provide a single global view over the data with no way of varying the notion of equivalence to be applied.               In this paper, we present an approach to enable applications to choose the equivalence criteria to apply between datasets. Thus, supporting multiple dynamic views over the Linked Data. For chemical data, we show that multiple sets of links can be automatically generated according to different equivalence criteria and published with semantic descriptions capturing their context and interpretation. This approach has been applied within a large scale public-private data integration platform for drug discovery. To cater for different\u00a0\u2026", "num_citations": "26\n", "authors": ["1168"]}
{"title": "XGAP: a uniform and extensible data model and software platform for genotype and phenotype experiments\n", "abstract": " We present an extensible software model for the genotype and phenotype community, XGAP. Readers can download a standard XGAP (                   http://www.xgap.org                                    ) or auto-generate a custom version using MOLGENIS with programming interfaces to R-software and web-services or user interfaces for biologists. XGAP has simple load formats for any type of genotype, epigenotype, transcript, protein, metabolite or other phenotype data. Current functionality includes tools ranging from eQTL analysis in mouse to genome-wide association studies in humans.", "num_citations": "26\n", "authors": ["1168"]}
{"title": "An identity crisis in the life sciences\n", "abstract": " myGrid is an e-Science project assisting life scientists to build workflows that gather data from distributed, autonomous, replicated and heterogeneous resources. The provenance logs of workflow executions are recorded as RDF graphs. The log of one workflow run is used to trace the history of its execution process. However, by aggregating provenance logs of many workflow runs, one may gather the provenance of a common data product shared in multiple derivation paths. A successful aggregation relies on accurate and universal identification of each data product. The nature of bioinformatics data and services, however, makes this difficult. We describe the identity problem in bioinformatics data, and present a protocol for managing identity co-references and allocating identity to gathered and computed data products. The ability to overcome this problem means that the provenance of\u00a0\u2026", "num_citations": "26\n", "authors": ["1168"]}
{"title": "The research object suite of ontologies: sharing and exchanging research data and methods on the open web\n", "abstract": " Research in life sciences is increasingly being conducted in a digital and online environment. In particular, life scientists have been pioneers in embracing new computational tools to conduct their investigations. To support the sharing of digital objects produced during such research investigations, we have witnessed in the last few years the emergence of specialized repositories, e.g., DataVerse and FigShare. Such repositories provide users with the means to share and publish datasets that were used or generated in research investigations. While these repositories have proven their usefulness, interpreting and reusing evidence for most research results is a challenging task. Additional contextual descriptions are needed to understand how those results were generated and/or the circumstances under which they were concluded. Because of this, scientists are calling for models that go beyond the publication of datasets to systematically capture the life cycle of scientific investigations and provide a single entry point to access the information about the hypothesis investigated, the datasets used, the experiments carried out, the results of the experiments, the people involved in the research, etc. In this paper we present the Research Object (RO) suite of ontologies, which provide a structured container to encapsulate research data and methods along with essential metadata descriptions. Research Objects are portable units that enable the sharing, preservation, interpretation and reuse of research investigation results. The ontologies we present have been designed in the light of requirements that we gathered from life scientists. They have been\u00a0\u2026", "num_citations": "25\n", "authors": ["1168"]}
{"title": "RSA-based certified delivery of e-goods using verifiable and recoverable signature encryption\n", "abstract": " Delivering electronic goods over the Internet is one of the e-commerce applications that will proliferate in the coming years. Certified e-goods delivery is a process where valuable e-goods are exchanged for an acknowledgement of their reception. This paper proposes an efficient security protocol for certified e-goods delivery with the following features:(1) it ensures strong fairness for the exchange of e-goods and proof of reception,(2) it ensures nonrepudiation of origin and non-repudiation of receipt for the delivered e-goods,(3) it allows the receiver of e-goods to verify, during the exchange process, that the e-goods to be received are the one he is signing the receipt for,(4) it uses an off-line and transparent semi-trusted third party (STTP) only in cases when disputes arise,(5) it provides the confidentiality protection for the exchanged items from the STTP, and (6) achieves these features with less computational and\u00a0\u2026", "num_citations": "25\n", "authors": ["1168"]}
{"title": "The montagues and the capulets\n", "abstract": " Prologue Two households, both alike in dignity, In fair Genomics, where we lay our scene, (One, comforted by its logic's rigour, Claims ontology for the realm of pure, The other, with blessed scientist's vigour, Acts hastily on models that endure), From ancient grudge break to new mutiny, When \u2018being\u2019 drives a fly\u2010man to blaspheme. From forth the fatal loins of these two foes, Researchers to unlock the book of life; Whole misadventured piteous overthrows, Can with their work bury their clans' strife. The fruitful passage of their GO\u2010mark'd love, And the continuance of their studies sage, Which, united, yield ontologies undreamed\u2010of, Is now the hour's traffic of our stage; The which if you with patient ears attend, What here shall miss, our toil shall strive to mend. Copyright \u00a9 2005 John Wiley & Sons, Ltd.", "num_citations": "25\n", "authors": ["1168"]}
{"title": "COHSE: Conceptual Open Hypermedia\n", "abstract": " The COHSE (Conceptual Open Hypermedia Service) Project brought together an Open Hypermedia system with an Ontology Service in order to provide an architecture for the semantic web. Open Hypermedia provides an extensible framework within which to support document linking while Ontology Services provide machine-processable descriptions of content. Here we describe COHSE and what an-notation means to COHSE.", "num_citations": "25\n", "authors": ["1168"]}
{"title": "An overview of the PEN & PAD project\n", "abstract": " The PEN (Practitioners Entering Notes) and PAD (Practitioners Accessing Data) project is designing a prototype human-computer interface for general medical practice employing user-centred design techniques[2]. It aims to \u2018throw light\u2019 on patient care by summarising information and by making patterns obvious. Doctors are already overloaded with information. The PENPAD project is trying to demonstrate how to use information technology without aggravating the information overload.", "num_citations": "25\n", "authors": ["1168"]}
{"title": "FAIR data reuse\u2013the path through data citation\n", "abstract": " One of the key goals of the FAIR guiding principles is defined by its final  principle \u2013 to optimize data sets for reuse by both  humans and machines. To do so, data providers need to implement and support  consistent machine readable metadata to describe their data sets. This can seem  like a daunting task for data providers, whether it is determining what level of  detail should be provided in the provenance metadata or figuring out what common  shared vocabularies should be used. Additionally, for existing data sets it is  often unclear what steps should be taken to enable maximal, appropriate reuse.  Data citation already plays an important role in making  data findable and accessible, providing persistent and unique identifiers plus  metadata on over 16 million data sets. In this paper, we discuss how data  citation and its underlying infrastructures, in particular associated metadata,  provide an important\u00a0\u2026", "num_citations": "24\n", "authors": ["1168"]}
{"title": "Extending semantic provenance into the web of data\n", "abstract": " In this article, the authors provide an example workflow-and a simple classification of user questions on the workflow's data products-to combine and interchange contextual metadata through a semantic data model and infrastructure. They also analyze their approach's potential to support enhanced semantic provenance applications.", "num_citations": "24\n", "authors": ["1168"]}
{"title": "Scientific workflows as services in caGrid: a Taverna and gRAVI approach\n", "abstract": " In scientific collaboration platforms such as caGrid, workflow-as-a-service is a useful concept for various reasons, such as easy reuse of workflows, access to remote resources, security concerns, and improved execution performance. We propose a solution for facilitating workflow-as-a-service based on Taverna as the workflow engine and gRAVI as a service wrapping tool. We provide both a generic service to execute all Taverna workflows, and an easy-to-use tool (gRAVI-t) for users to wrap their workflows as workflow-specific services, without developing service code. The signature of the specific service is identical to the corresponding workflow's input/output definition and is therefore more self-explained to workflow users. These two categories of services are useful in different scenarios, respectively. We use a tumor analysis workflow as an example to demonstrate how the workflow-as-a-service approach\u00a0\u2026", "num_citations": "24\n", "authors": ["1168"]}
{"title": "GUIDANCE: Making it Easy for the User to be an Expert\n", "abstract": " This paper describes an information retrieval system\u2014GUIDANCE1\u2014that is accessible and usable by people who are not experts in computing but are experts in their own domain. This particular user group needs to be supported by a system that is easy to use and reflects their own knowledge of the world. The system presented is based on descriptions \u2014 the logical structure of the database is concealed in favour of an interface which supports the question \u2018What can I say about People?\u2019 regardless of how many objects, roles, or attributes represent \u2018People\u2019. A full and relevant description is implemented by two models, one containing conceptual knowledge, and the other database specific information. These models are represented in, and related by, a descriptive subsumption-based classification formalism GRAIL2, which has a system of semantic sanctions to control the creation of implied concepts and\u00a0\u2026", "num_citations": "24\n", "authors": ["1168"]}
{"title": "A descriptive semantic formalism for medicine\n", "abstract": " It is argued that current clinical information systems incorporate oversimplistic, prescriptive data models that are not faithful to clinicians' observations. A non-prescriptive descriptive semantic formalism, Structured Meta Knowledge (SMK), which unifies a terminological knowledge base with controlled assertional capabilities with the medical record and supports the semantic control necessitated by such an approach, is proposed. The three-layer model of categories, individuals, and occurrences described is more appropriate to medical applications than the two layers of classes and instances. The application of SMK in predictive data entry is considered.< >", "num_citations": "24\n", "authors": ["1168"]}
{"title": "Theoretical and technological building blocks for an innovation accelerator\n", "abstract": " Abstract                  Modern science is a main driver of technological innovation. The efficiency of the scientific system is of key importance to ensure the competitiveness of a nation or region. However, the scientific system that we use today was devised centuries ago and is inadequate for our current ICT-based society: the peer review system encourages conservatism, journal publications are monolithic and slow, data is often not available to other scientists, and the independent validation of results is limited. The resulting scientific process is hence slow and sloppy. Building on the Innovation Accelerator paper by Helbing and Balietti [1], this paper takes the initial global vision and reviews the theoretical and technological building blocks that can be used for implementing an innovation (in first place: science) accelerator platform driven by re-imagining the science system. The envisioned platform would rest\u00a0\u2026", "num_citations": "23\n", "authors": ["1168"]}
{"title": "Workflows to open provenance graphs, round-trip\n", "abstract": " The Open Provenance Model is designed to capture relationships amongst data values, and amongst processors that produce or consume those values. While OPM graphs are able to describe aspects of a workflow execution, capturing the structure of the workflows themselves is understandably beyond the scope of the OPM specification, since the graphs may be generated by a broad variety of processes, which may not be formal workflows at all. In particular, the OPM does not address two questions: firstly, whether for any OPM graph there exists a plausible workflow, in some model, which could have generated the graph, and secondly, which information should be captured as part of an OPM graph that is derived from the execution of some known type of workflow, so that the workflow structure and the execution trace can both be inferred back from the graph. Motivated by the need to address the Third\u00a0\u2026", "num_citations": "23\n", "authors": ["1168"]}
{"title": "S-OGSA as a Reference Architecture for OntoGrid and for the Semantic Grid\n", "abstract": " The Grid aims to support secure, flexible and coordinated resource sharing through providing a middleware platform for advanced distributing computing. Consequently, the Grid\u2019s infrastructural machinery aims to allow collections of any kind of resources\u2014computing, storage, data sets, digital libraries, scientific instruments, people, etc\u2014to easily form Virtual Organisations (VOs) that cross organisational boundaries in order to work together to solve a problem. A Grid depends on understanding the available resources, their capabilities, how to assemble them and how to best exploit them. Thus Grid middleware and the Grid applications they support thrive on the metadata that describes resources in all their forms, the VOs, the policies that drive then and so on, together with the knowledge to apply that metadata intelligently. The Semantic Grid is a recent initiative to systematically expose semantically rich information associated with Grid resources to build more intelligent Grid services1. The idea is to make structured semantic descriptions real and visible first class citizens with an associated identity and behaviour. We can then define mechanisms for their creation and management and protocols for their processing, exchange and customisation. We can separate these issues from both the languages used to encode the descriptions (from natural language text right through to logical-based assertions) and the structure and content of the descriptions themselves, which may vary from application to application. In practice, work on Semantic Grids has primarily meant introducing technologies from the Semantic Web2 to the Grid. The background\u00a0\u2026", "num_citations": "23\n", "authors": ["1168"]}
{"title": "Using the Semantic Web for e-Science: inspiration, incubation, irritation\n", "abstract": " We are familiar with the idea of e-Commerce \u2013 the electronic trading between consumers and suppliers. In recent years there has been a commensurate paradigm shift in the way that science is conducted. e-Science is science performed through distributed global collaborations between scientists and their resources enabled by electronic means, in order to solve scientific problems. No one scientific laboratory has the resources or tools, the raw data or derived understanding or the expertise to harness the knowledge available to a scientific community. Real progress depends on pooling know-how and results. It depends on collaboration and making connections between ideas, people, and data. It depends on finding and interpreting results and knowledge generated by scientific colleagues you do not know and who do not know you, to be analysed in ways they did not anticipate, to generate new\u00a0\u2026", "num_citations": "23\n", "authors": ["1168"]}
{"title": "A linkable identity privacy algorithm for healthgrid\n", "abstract": " The issues of confidentiality and privacy have become increasingly important as Grid technology is being adopted in public sectors such as healthcare. This paper discusses the importance of protecting the confidentiality and privacy of patient health/medical records, and the challenges exhibited in enforcing this protection in a Grid environment. It proposes a novel algorithm to allow traceable/linkable identity privacy in dealing with de-identified medical records. Using the algorithm, de-identified health records associated to the same patient but generated by different healthcare providers are given different pseudonyms. However, these pseudonymised records of the same patient can still be linked by a trusted entity such as the NHS trust or HealthGrid manager. The paper has also recommended a security architecture that integrates the proposed algorithm with other data security measures needed to achieve the desired security and privacy in the HealthGrid context.", "num_citations": "23\n", "authors": ["1168"]}
{"title": "PEN&PAD: a doctors' workstation with intelligent data entry and summaries\n", "abstract": " Structured descriptions and their text formats Designing the PEN&PAD system has required a comprehensive re-appraisal of the structure and function of medical records. Although information appears in the encounter notes in a form which resembles free-text, all of the information in the current prototypes is structured, and stored in the form ofdescriptions using the Structured Meta-Knowledge (SMK) formalism described elsewhere [1]. True free-text comments can be appended at any point but their use is kept to a minimum. Virtually the entire contents of the record are therefore available for processing, decision support, retrieval, audit and research. The model of the medical record also allows any statement to act as a problem. Thus diagnoses, findings, management decisions, or even modifiers can all define a problem. Furthermore, problems are independent objects which can be redefined as thedoctor's understanding of the patient's illness changes.Data entry Doctors cite the time and effort required to enter data as the single biggest barrier to using computers. This was confirmed for British GPs by a questionnaire circulated by the UK Department of Health early in the life of the project. One of the project's main tasks has been to see how much structured", "num_citations": "23\n", "authors": ["1168"]}
{"title": "ISPIDER Central: an integrated database web-server for proteomics\n", "abstract": " Despite the growing volumes of proteomic data, integration of the underlying results remains problematic owing to differences in formats, data captured, protein accessions and services available from the individual repositories. To address this, we present the ISPIDER Central Proteomic Database search ( http://www.ispider.manchester.ac.uk/cgi-bin/ProteomicSearch.pl ), an integration service offering novel search capabilities over leading, mature, proteomic repositories including PRoteomics IDEntifications database (PRIDE), PepSeeker, PeptideAtlas and the Global Proteome Machine. It enables users to search for proteins and peptides that have been characterised in mass spectrometry-based proteomics experiments from different groups, stored in different databases, and view the collated results with specialist viewers/clients. In order to overcome limitations imposed by the great variability in protein accessions\u00a0\u2026", "num_citations": "22\n", "authors": ["1168"]}
{"title": "Ontology based semantic annotation for enhancing mobility support for visually impaired web users\n", "abstract": " We have previously shown that the mobility, or ease of travel, of visually impaired Web users is reduced since most Web pages are usually designed for visual interaction [7]. Therefore, in a visually impaired person\u2019s environment objects that support travel are missing or inaccessible altogether. We aim to enhance the experience of visually impaired Web travellers by using annotation and Semantic Web technologies. In [17], we have proposed a semi-automated tool which encodes techniques for the support of travel upon the Web. The main goal of this tool is to analyse Web pages to identify objects that support mobility and travel, discover their roles, annotate them and transform pages based on these annotations to enhance the provided mobility support. This paper mainly presents the annotation part of the tool and provides some transformation examples which are based on the annotations. The main message of this paper is that visually impaired Web users could also benefit from the Semantic Web technologies and here we demonstrate a possible approach to achieve that.", "num_citations": "22\n", "authors": ["1168"]}
{"title": "Towel: Real world mobility on the web\n", "abstract": " The \u2018Towel\u2019 project seeks to find solutions to problems encountered by both visually impaired and sighted users when travelling in the World Wide Web. Drawing similarities between real-world travel metaphors of visually impaired people and web-based travel metaphors of both visually impaired and sighted people, enhances an understanding of the problem and therefore enables solutions to these travel problems to be more easily identified. By likening web-travel to real-world travel in terms of mobility, navigation, orientation, and mapping, and by fitting web-travel into a real-world travel framework a number of similarities can be identified, and problems characterised. These problems have solutions in the real-world and so these real-world solutions may be of use in addressing web-based travel problems.", "num_citations": "22\n", "authors": ["1168"]}
{"title": "Reputationnet: A reputation engine to enhance servicemap by recommending trusted services\n", "abstract": " The concept of Service Oriented Architecture (SOA) enables flexible and dynamic collaborations among different service providers. Backed up by SOA, scientific workflows can bring together various scientific computing tools and resources all offered as services to answer complex research questions. However, studies conducted on my Experiment show that although the sharing of service-based capabilities opens a gateway to resource reuse, in practice, the degree of reuse is very low. This motivates us to propose ServiceMap to provide navigation support through the network of services in building scientific workflows. In this paper, we propose an extension of ServiceMap, i.e., ReputationNet that incorporates the reputation of services/workflows and their publishers to reinforce its capability in terms of service and workflow recommendations. We develop a novel model of the reputation aspects of the services\u00a0\u2026", "num_citations": "21\n", "authors": ["1168"]}
{"title": "{BioCatalogue}: A Curated Web Service Registry for the Life Science Community\n", "abstract": " Web services have gained a momentum as a means for packaging existing data and computational resources in a form that is amenable for use and composition by third party applications. They provide a well defined programming interface to integrate tools into applications over networks. Software applications written in various programming languages and running on various platforms can use web services to exchange data over the Internet. The life science community is certainly among the first adopters of web services.", "num_citations": "21\n", "authors": ["1168"]}
{"title": "Putting semantics into e-science and grids\n", "abstract": " What is the semantic grid? How can e-Science benefit from the technologies of the semantic grid? Can we build a semantic Web for e-Science? Would that differ from a semantic grid? Given our past experiences with scientists, grid developers and semantic Web researchers, what are the prospects, and pitfalls, of putting semantics into e-Science applications and grid infrastructure?", "num_citations": "21\n", "authors": ["1168"]}
{"title": "Classification Based Navigation and Retrieval for Picture Archives\n", "abstract": " Current state of the art in image retrieval and indexing doesn\u2019t meet all the needs of users of electronic picture collections. Content based retrieval provides little support for semantic metadata, in particular descriptions of what the image contains or represents. We present the approach being taken by the STARCH project, which is using a Description Logic (DL) for semantic metadata. The structured representation of the DL can assist in providing more powerful environments for retrieval, through the support of browsing, navigation and the serendipitous discovery of information. The conceptual space can also prove useful for defining notions of similarity and semantic closeness. We illustrate these claims with a series of examples taken from our prototype system.", "num_citations": "21\n", "authors": ["1168"]}
{"title": "The Human Physiome: how standards, software and innovative service infrastructures are providing the building blocks to make it achievable\n", "abstract": " Reconstructing and understanding the Human Physiome virtually is a complex mathematical problem, and a highly demanding computational challenge. Mathematical models spanning from the molecular level through to whole populations of individuals must be integrated, then personalized. This requires interoperability with multiple disparate and geographically separated data sources, and myriad computational software tools. Extracting and producing knowledge from such sources, even when the databases and software are readily available, is a challenging task. Despite the difficulties, researchers must frequently perform these tasks so that available knowledge can be continually integrated into the common framework required to realize the Human Physiome. Software and infrastructures that support the communities that generate these, together with their underlying standards to format, describe and interlink\u00a0\u2026", "num_citations": "20\n", "authors": ["1168"]}
{"title": "Fostering scientific workflow preservation through discovery of substitute services\n", "abstract": " Scientific workflows are increasingly gaining momentum as the new paradigm for modeling and enacting scientific experiments. The value of a workflow specification does not end once it is enacted. Indeed, workflow specifications encapsulate knowledge that documents scientific experiments, and are, therefore, worth preserving. Our experience suggests that workflow preservation is frequently hampered by the volatility of the constituent service operations when these operations are supplied by third-party providers. To deal with this issue, we propose a heuristic for locating substitutes that are able to replace unavailable service operations within workflows. The proposed method uses the data links connecting inputs and outputs of service operations in existing workflow specifications to locate operations with parameters compatible with those of the missing operations. Furthermore, it exploits provenance traces\u00a0\u2026", "num_citations": "20\n", "authors": ["1168"]}
{"title": "Scientific social objects: The social objects and multidimensional network of the myexperiment website\n", "abstract": " Scientific research is increasingly conducted digitally and online, and consequently we are seeing the emergence of new digital objects shared as part of the conduct and discourse of science. These Scientific Social Objects are more than lumps of domain-specific data: they may comprise multiple components which can also be shared separately and independently, and some contain descriptions of scientific processes from which new objects will be generated. Using the my Experiment social website as a case study we explore Scientific Social Objects and discuss their evolution.", "num_citations": "20\n", "authors": ["1168"]}
{"title": "BioCatalogue: a curated web service registry for the Life Science community\n", "abstract": " Web Services have gained a momentum as a means for packaging existing data and computational resources in a form that is amenable for use and composition by third party applications. The life science community is certainly among the first adopters of Web Services. For example,\" Taverna\": http://www. mygrid. org. uk, a workflow workbench that is popular within the life science community, provides access to over 3500 thousands web services that can be composed by scientists for constructing and enacting their in silico experiments. However, one of the main issues that hinders the wide adoption and use of Web Services is the difficulty in locating the \u201cappropriate\u201d Web Service, ie, the Web Service that performs the analysis the scientist is interested in. The descriptions of available Web Services are often poor providing little information to the scientist about their usefulness for the analysis s/he is after. With the\u00a0\u2026", "num_citations": "20\n", "authors": ["1168"]}
{"title": "Looking into the future of workflows: The challenges ahead\n", "abstract": " In this chapter, we take a step back from the individual applications and software systems and attempt to categorize the types of issues that we are facing today and the challenges we see ahead. This is by no means a complete picture of the challenges but rather a set of observations about the various aspects of workflow management. In a broad sense, we are organizing our thoughts in terms of the different workflow systems discussed in this book, from the user interface down to the execution environment.", "num_citations": "20\n", "authors": ["1168"]}
{"title": "Semantic web based content enrichment and knowledge reuse in e-science\n", "abstract": " We address the life cycle of semantic web based knowledge management from ontology modelling to instance generation and reuse. We illustrate through a semantic web based knowledge management approach the potential of applying semantic web technologies in GEODISE, an e-Science pilot project in the domain of Engineering Design Search and Optimization (EDSO). In particular, we show how ontologies and semantically enriched instances are acquired through knowledge acquisition and resource annotation. This is illustrated not only in Prot\u00e9g\u00e9 with an OWL plug-in, but also in a light weight function annotator customized for resource providers to semantically describe their own resources to be published. In terms of reuse, advice mechanisms, in particular a knowledge advisor based on semantic matching, are designed to consume the semantic information and facilitate service discovery\u00a0\u2026", "num_citations": "20\n", "authors": ["1168"]}
{"title": "How much is too much in a hypertext link? investigating context and preview--a formative evaluation\n", "abstract": " A high quality of free movement, or mobility, is key to the accessibility, design, and usability of many'common-use'hypermedia resources (Web sites) and key to good mobility is context and preview. This is especially the case when a hypertext anchor is inaccurately described or is described out of context as confusion and disorientation can ensue. Mobility is similarly reduced when the link target of the anchor has no relationship to the expected information present on the hypertext node (Web page). Confident movement with purpose, ease, and accuracy can only be achieved when complete contextual information and an accurate description of the proposed destination (preview) are available. We suggest that sighted people can benefit from additional context and preview information included in hyperlinks and disprove the empirical evidence that suggests these users do not benefit from link descriptions which\u00a0\u2026", "num_citations": "19\n", "authors": ["1168"]}
{"title": "Are bioinformaticians doing e-Business?\n", "abstract": " We have models of commerce in a Web setting: business to business (B2B) and business to consumer (B2C). Now scientists commonly use Web based services to perform in-silico experiments. Thus we are prompted to ask the question \u201cAre e-Scientists doing e-Business?\u201d. Do the infra-structure and models offered by e-Commerce support the activities e-Scientists need to perform? In this position paper we compare e-Science and e-Business using the discipline of bioinformatics. Such a comparison should inform the reuse of existing e-Business technologies in e-Science projects. We argue that the individual e-Scientist is now demanding more than the simple web interfaces prevalent in consumer e-commerce. Individual e-Scientists need to interact in a manner more akin to the B2B model than the B2C style previously used. We examine how the infrastructure prevalent in the B2B arena of e-commerce can be reused and extended to support the needs of today\u2019s e-Scientists. We illustrate this argument with reference to the myGrid e-Science middleware project.", "num_citations": "19\n", "authors": ["1168"]}
{"title": "A medical terminology server\n", "abstract": " GRAIL is a semantically constrained, generative compositional descriptive logic with subsumption and multiple inheritance designed to cope with the scale, complexity and variable granularity of medical concepts. It effectively represents a medical vocabulary for any application such as decision support systems, hospital record systems, clinical workstations and bibliographic systems. The EU-funded GALEN project aims to create a multilingual Terminology Server capable of being used as a resource by any medical application. The Terminology Server relates descriptive medical concepts represented in GRAIL to their natural language terms in a number of European languages and to corresponding (or best match) codes in conventional clinical coding schemes. In this paper we present GRAIL, the GALEN Terminology Server and make some remarks on the coupling of the classification-based clinical\u00a0\u2026", "num_citations": "19\n", "authors": ["1168"]}
{"title": "Conceptual, semantic and information models for medicine\n", "abstract": " This paper presents the intensional requirements of the medical record. It suggests that at least four models are required in order to cover the complexity of the medical domain. It goes onto propose that description logics are more appropriate to the modelling of medicine information than conventional prescriptive semantic data models and propose a description logic capable of constrained generation of complex compositional concepts from elementary ones and their relationships. This formalism, named GRAIL, is described in detail as a representation for medical concepts, and is shown to be capable of integrating all the intensional and extensional requirements of a medical record. Experiences with GRAIL in use in a clinical workstation, PEN&PAD, and a large medical terminology server, part of the EU-funded project GALEN, are extensively described. The support requirements of distributed large-scale collaborative modelling for re-use, as required for GALEN, are discussed. Examples, both medical and non-medical, are given throughout.", "num_citations": "19\n", "authors": ["1168"]}
{"title": "Unique, persistent, resolvable: Identifiers as the foundation of FAIR\n", "abstract": " The FAIR principles describe characteristics intended to support access to and  reuse of digital artifacts in the scientific research ecosystem. Persistent,  globally unique identifiers, resolvable on the Web, and associated with a set of  additional descriptive metadata, are foundational to FAIR data. Here we describe  some basic principles and exemplars for their design, use and orchestration with  other system elements to achieve FAIRness for digital research objects.", "num_citations": "18\n", "authors": ["1168"]}
{"title": "A roadmap for caGrid, an enterprise Grid architecture for biomedical research\n", "abstract": " caGrid is a middleware system which combines the Grid computing, the service oriented architecture, and the model driven architecture paradigms to support development of interoperable data and analytical resources and federation of such resources in a Grid environment. The functionality provided by caGrid is an essential and integral component of the cancer Biomedical Informatics Grid (caBIG\u2122) program. This program is established by the National Cancer Institute as a nationwide effort to develop enabling informatics technologies for collaborative, multi-institutional biomedical research with the overarching goal of accelerating translational cancer research. Although the main application domain for caGrid is cancer research, the infrastructure provides a generic framework that can be employed in other biomedical research and healthcare domains. The development of caGrid is an ongoing effort, adding new\u00a0\u2026", "num_citations": "18\n", "authors": ["1168"]}
{"title": "$^{\\textrm {\\small {my}}} $ Grid and UTOPIA: An Integrated Approach to Enacting and Visualising in Silico Experiments in the Life Sciences\n", "abstract": " In silico experiments have hitherto required ad hoc collections of scripts and programs to process and visualise biological data, consuming substantial amounts of time and effort to build, and leading to tools that are difficult to use, are architecturally fragile and scale poorly. With examples of the systems applied to real biological problems, we describe two complimentary software frameworks that address this problem in a principled manner; $^{\\textrm{\\small{my}}}$Grid Taverna, a workflow design and enactment environment enabling coherent experiments to be built, and UTOPIA, a flexible visualisation system to aid in examining experimental results.", "num_citations": "18\n", "authors": ["1168"]}
{"title": "Towards an authentication middleware to support ubiquitous Web access\n", "abstract": " This extended abstract reports our on-going work on the design and development of a flexible multifactor authentication middleware to support secure, finegrained and ubiquitous data access through Web services.", "num_citations": "18\n", "authors": ["1168"]}
{"title": "Report on the EDBT'02 panel on scientific data integration\n", "abstract": " As scientific research becomes an increasingly larger portion of corporate expenditures, pressure is mounting to make the processes more efficient. Data acquisition, access, management, analysis and the sharing of all available resources will be at the core of the transformations needed to achieve the next level of efficiency in research organizations. However current data management technology is geared toward business data and several technical challenges remain to make it suitable for scientific data. A panel entitled Scientific Data Integration was held on March 25 h, 2002, at the 8 th Conference on Extending Database Technology (EDBT) in Prague, in the Czech Republic. The panel focused on the issues needed to be addressed to enable scientific data integration and discussed solutions. Omar Boucelma, Universit~ de Provence, and Zod Lacroix, Arizona State University, moderated the panel which\u00a0\u2026", "num_citations": "18\n", "authors": ["1168"]}
{"title": "An Open-Model-Based Interface Development System: The Teallach Approach.\n", "abstract": " The goal of the Teallach project is to provide facilities for the systematic development of interfaces to object databases in a manner which is independent of both a specific underlying database and operating system. Teallach\u2019s open architecture also allows the creation of interfaces to nondatabase applications in a platform-independent manner. To this end Teallach adopts model-based techniques in the process of interface construction, exploits the cross-platform capabilities of Java, and utilises the Java Beans API to allow third-party interface components to be exploited. Through the use of a simple case study this paper introduces the Teallach approach to interface development, providing an overview of the system, its motivations, and underlying technology.", "num_citations": "18\n", "authors": ["1168"]}
{"title": "Putting the media into hypermedia\n", "abstract": " This paper discusses the issues involved in putting the media into a hypermedia system. The main argument of the paper is that to-date media representation and underlying link strategies have been too closely tied together in the move from hypertext to hypermedia. We argue that it is necessary to separate the issues of media from link structure, and we present a model which solves some of the problems of genuine media integration in a hypermedia system. At the same time the model provides support for the creation of links between data of different media types in a conceptually meaningful way. The paper describes the design of Microcosm++, and object oriented extensible service-based architecture for building consistent integrated hypermedia systems. It is based on the Microcosm hypermedia system which was developed at Southampton. The current implementation of Microcosm++ demonstrates the\u00a0\u2026", "num_citations": "18\n", "authors": ["1168"]}
{"title": "Lessons from myexperiment: Research objects for data intensive research\n", "abstract": " The myExperiment Virtual Research Environment [1] has successfully adopted a Web 2.0 approach in delivering a social web site where scientists can discover, publish and curate scientific workflows and other artefacts. While it shares many characteristics with other Web 2.0 sites, myExperiment\u2019s distinctive features to meet the needs of its research user base include support for credit, attributions and licensing, fine control over privacy, a federation model and the ability to execute workflows. Figure 1 shows a workflow in myExperiment, with its associated \u201csocial metadata\u201d. myExperiment now has over 2000 registered users, with thousands more downloading public content, and the largest public collection of workflows, for systems which include Microsoft\u2019s Trident. Created in close collaboration with its research users [2], myExperiment gives important insights into emerging research practice. As it moves into its second phase we see new forms of sharable Research Object which challenge traditional scholarly publishing and provide an important basis for data-intensive science. To support this, semantic technologies are increasingly coming into play to maximise the potential for reuse and repurposing of experiments.", "num_citations": "17\n", "authors": ["1168"]}
{"title": "In situ migration of handcrafted ontologies to reason-able forms\n", "abstract": " A methodology for in situ migration of a handcrafted Directed Acyclic Graph (DAG), to a formal and expressive OWL version is presented. Well-known untangling methodologies recommend wholesale re-coding. Unable to do this, we have tackled portions of the DAG, lexically dissecting term names to property-based descriptions in OWL. The different levels of expressivity are presented in a model called the \u201cfeature escalator\u201d, where the user can choose the level needed for the application and the expressivity that delivers requirement. The results of applying the methodology to some areas of the gene ontology (GO) demonstrate the validity of the methodology.", "num_citations": "17\n", "authors": ["1168"]}
{"title": "Proteome data integration: Characteristics and challenges\n", "abstract": " The aim of the ISPIDER project is to create a proteomics grid; that is, a technical platform that supports bioinformaticians in constructing, executing and evaluating in silico analyses of proteomics data. It will be constructed using a combination of generic e-science and Grid technologies, plus proteomics specific components and clients that embody knowledge of the proteomics domain and the available resources. In this paper, we describe some of our earlier results in prototyping specific examples of proteomics data integration, and draw from it lessons about the kinds of domain-specific components that will be required.", "num_citations": "17\n", "authors": ["1168"]}
{"title": "Annotation and transformation of Web pages to improve mobility for visually impaired users\n", "abstract": " In the last decade, the growth and the popularity of the World Wide Web (Web) have been phenomenal. Originally, it was a purely text-based system that allowed assistive technologies to be designed to transform pages into alternative forms (eg, audio) for disabled people. This meant that for the first time, a vast amount of information was available and easily accessible to disabled people. However, advances in technologies and changes in the main authoring language, transformed the Web into a true visual communication medium. These changes eventually made the Web inaccessible to visually impaired users. In particular, travelling around the Web became a complicated task, since the richness of visual navigational objects presented to their sighted counterparts are neither appropriate nor accessible to visually impaired users. This thesis investigates principles and derived techniques to enhance the travel\u00a0\u2026", "num_citations": "17\n", "authors": ["1168"]}
{"title": "Delivering terminological services\n", "abstract": " Terminologies or controlled vocabularies are important as they provide a framework within which communities can share knowledge. We describe three applications using a terminology represented in a Description Logic and delivered via a Terminology Server and suggest that such a service oriented architecture is essential to the adoption of terminologies within the component based architectures of today\u2019s software environments.", "num_citations": "17\n", "authors": ["1168"]}
{"title": "Marine long-term biodiversity assessment suggests loss of rare species in the Skagerrak and Kattegat region\n", "abstract": " Studies of cumulative and long-term effects of human activities in the ocean are essential for developing realistic conservation targets. Here, we report the results of a recent national marine biodiversity inventory along the Swedish West coast between 2004 and 2009. The expedition revisited many historical localities that have been sampled with the same methods in the early twentieth century. We generated comparable datasets from our own investigation and the historical data to compare species richness, abundance, and geographic distribution of diversity. Our analysis indicates that the benthic ecosystems in the region have lost a large part of its original species richness over the last seven decades. We find evidence that especially rare species have disappeared. This process has caused a more homogenized community structure in the region and diminished historical biodiversity hotspots. We argue\u00a0\u2026", "num_citations": "16\n", "authors": ["1168"]}
{"title": "LabelFlow: Exploiting Workflow Provenance to Surface Scientific Data Provenance\n", "abstract": " Provenance traces captured by scientific workflows can be useful for designing, debugging and maintenance. However, our experience suggests that they are of limited use for reporting results, in part because traces do not comprise domain-specific annotations needed for explaining results, and the black-box nature of some workflow activities. We show that by basic mark-up of the data processing within activities and using a set of domain specific label generation functions, standard workflow provenance can be utilised as a platform for the labelling of data artefacts. These labels can in turn aid selection of data subsets and proxy for data descriptors for shared datasets.", "num_citations": "16\n", "authors": ["1168"]}
{"title": "Towards an ontology for psychosis\n", "abstract": " There is a pressing need for data interoperability in neuroscience especially in mental health and psychiatric research. Heterogeneity of data in the domain is a combination of a plethora of assessment methods and two clinical classification systems with no formal method of interconversion. Ontologies with their formal logical basis have been successfully used to achieve interoperability in other fields of biology. We discuss the need for an ontology in the domain of psychosis and propose a methodology for building such an ontology. We outline the various factors that are important for building a unifying ontology and how this might serve as a good start for building better classification systems in psychiatry.", "num_citations": "16\n", "authors": ["1168"]}
{"title": "Medical image processing workflow support on the EGEE grid with taverna\n", "abstract": " Resource-intensive and complex medical imaging applications can benefit from the use of scientific workflow technology for their design, rapid implementation and reuse, but at the same time they require a grid computing infrastructure to execute efficiently. In this paper we describe a technical architecture that bridges the gap between the Taverna workflow management system and the EGEE grid infrastructure. This is achieved through a novel Taverna gLite activity plugin that makes the interface between the multi-threaded, centralized workflow enactor and the massively distributed, batch-oriented grid infrastructure. The plugin significantly increases the performance of Medical Imaging workflows over an equivalent plain Taverna workflow.", "num_citations": "16\n", "authors": ["1168"]}
{"title": "Applying descriptions logics for workflow reuse and repurposing\n", "abstract": " Workflow techniques are an important part of in silico experimentation, potentially allowing a scientist to describe and enact their experimental processes in a structured, repeatable and verifiable way. The myGrid (www. mygrid. org. uk) workbench, a set of components to build workflows in bioinformatics, currently allows access to a thousand globally distributed services and a hundred workflows, some of which orchestrate up to fifty services.", "num_citations": "16\n", "authors": ["1168"]}
{"title": "Traversing the web: Mobility heuristics for visually impaired surfers\n", "abstract": " Movement, or mobility, is a key to the accessibility, design, and usability of many Web sites. While some peripheral mobility issues have been addressed few have centered on the mobility problems of visually impaired users. We use our past work to address these issues and derive mobility heuristics from mobility models, use these heuristics to place mobility objects within a Web page, and describe the construction of a prototype mobility instrument, in the form of a Netscape plug-in, to process these objects. Our past work extends the notion of movement to include environment, feedback and the purpose of the current travel task. Specifically, we likened web use to travelling in a virtual space, compared it to travelling in a physical space, and introduced the idea of mobility - the ease of travel - as opposed to travel opportunity.", "num_citations": "16\n", "authors": ["1168"]}
{"title": "Description Logics and Multimedia-Applying Lessons Learnt from the GALEN Project\n", "abstract": " We describe some preliminary exploratory work in the use of the GRAIL [2] description logic and GALEN terminology server architecture [24] to support classification of art images, based on our experiences of using GRAIL to support medical applications using clinical terminology [24].", "num_citations": "16\n", "authors": ["1168"]}
{"title": "Shedding light on patients' problems: integrating knowledge based systems into medical practice\n", "abstract": " Shedding Light on Patients' Problems: Integrating Knowledge Based Systems into Medical Practice | Research Explorer | The University of Manchester The University of Manchester Menu Search the University of Manchester site Search Search text Search type Research Explorer Website Staff directory Alternatively, use our A\u2013Z index Home Study Undergraduate Undergraduate Courses Prospectus (undergraduate) Offer-holders Undergraduate Teaching and learning Expanding your study (undergraduate) After you graduate (undergraduate) Undergraduate Applications Undergraduate Student finance Webinars (undergraduate) Contextual admissions Mature students Parents and supporters Contact us (undergraduate) Taught master's Why study a master's? Why Manchester? (taught master's) Taught master's Courses Teaching and learning (taught master's) After you graduate (taught master's) Postgraduate \u2026", "num_citations": "16\n", "authors": ["1168"]}
{"title": "Supporting a humanly impossible task: The clinical human computer environment\n", "abstract": " http://dblp. uni-trier. de/db/conf/interact/interact1990. html# HoranRSGHKNW90http://dblp. uni-trier. de/rec/bibtex/conf/interact/HoranRSGHKNW90. xmlhttp://dblp. uni-trier. de/rec/bibtex/conf/interact/HoranRSGHKNW90", "num_citations": "16\n", "authors": ["1168"]}
{"title": "An ActOn-based semantic information service for Grids\n", "abstract": " We describe a semantic information service that aggregates metadata from a large number of information sources of a large-scale Grid infrastructure. It uses an ontology-based information integration architecture (ActOn) suitable for the highly dynamic distributed information sources available in Grid systems, where information changes frequently and where the information of distributed sources has to be aggregated in order to solve complex queries. These two challenges are addressed by a Metadata Cache that works with an update-on-demand policy and by an information source selection module that selects the most suitable source at a given point in time. We have evaluated the quality of this information service, and compared it with other similar services from the EGEE production testbed, with promising results.", "num_citations": "15\n", "authors": ["1168"]}
{"title": "Anchors in shifting sand: the primacy of method in the web of data\n", "abstract": " The wealth of new government and scientific data appearing on the Web is to be welcomed and makes it possible for citizens and scientists to interpret evidence and obtain new insights. But how will they do this, and how will people trust the results? We suggest the Linked Data Web must embrace the \u201cmethods\u201d by which results are obtained as well as the results themselves. By making methods first class citizens, results can be explained, interpreted and assessed, and the methods themselves can be shared, discussed, reused and repurposed. We present the myExperiment. org website, a social network of people sharing reusable methods for processing research data, and make some observations on the nature of first class methods in the Web of Data.", "num_citations": "15\n", "authors": ["1168"]}
{"title": "Agile management: Strategies for developing a social networking site for scientists\n", "abstract": " Research 2.0 (or Science 2.0) is the term commonly used to describe Web 2.0-based platforms for supporting collaboration in scientific research (eg, Waldrop, 2008). This paper, based on our experience of developing myExperiment 1, a site that enables scientists to share digital resources associated with their research (De Roure, Goble and Stevens, 2007), aims to identify and detail good practice for the development of Research 2.0 sites. We are especially interested in explicating how the project is managed so as, on the one hand, to maintain rich user engagement in the face of uncertain and evolving requirements and to exploit the malleability of Web 2.0 technologies, while, on the other, keeping the project on \u2018track\u2019. Our interest then is in \u2018agile management\u2019, with how the flexibility and responsiveness encouraged by \u2018agile\u2019approaches and facilitated by ICT tools can be combined with managerial requirements for coordination, measuring progression, identifying and meeting targets and so on; and with the organizational rationale and consequences of \u2018perpetual beta\u2019. myExperiment is funded under the second phase of the UK JISC Virtual Research Environments programme 2 and aims to enable scientists to share digital resources associated with their research, in particular, to share and execute scientific workflows through familiar social networking and social collaboration features. To achieve its aims, the myExperiment project team have adopted a so-called \u2018agile\u2019approach to software development. For the project team, whose members are distributed across multiple sites (and often mobile), making an agile approach work in practice\u00a0\u2026", "num_citations": "15\n", "authors": ["1168"]}
{"title": "Achieving fine\u2010grained access control in virtual organizations\n", "abstract": " In a virtual organization environment, where services and data are provided and shared among organizations from different administrative domains and protected with dissimilar security policies and measures, there is a need for a flexible authentication framework that supports the use of various authentication methods and tokens. The authentication strengths derived from the authentication methods and tokens should be incorporated into an access\u2010control decision\u2010making process, so that more sensitive resources are available only to users authenticated with stronger methods. This paper reports our on\u2010going efforts in designing and implementing such a framework to facilitate multi\u2010level and multi\u2010factor adaptive authentication and authentication strength linked fine\u2010grained access control. The proof\u2010of\u2010concept prototype is designed and implemented in the Shibboleth and PERMIS infrastructures, which\u00a0\u2026", "num_citations": "15\n", "authors": ["1168"]}
{"title": "Semantic description, publication and discovery of workflows in myGrid\n", "abstract": " The bioinformatics scientific process relies on in silico experiments, which are experiments executed in full in a computational environment. Scientists wish to encode the designs of these experiments as workflows because they provide minimal, declarative descriptions of the designs, overcoming many barriers to the sharing and re-use of these designs between scientists and enable the use of the most appropriate services available at any one time. We anticipate that the number of workflows will increase quickly as more scientists begin to make use of existing workflow construction tools to express their experiment designs. Discovery then becomes an increasingly hard problem, as it becomes more difficult for a scientist to identify the workflows relevant to their particular research goals amongst all those on offer. While many approaches exist for the publishing and discovery of services, there have been few attempts to address where and how authors of experimental designs should advertise the availability of their work or how relevant workflows can be discovered with minimal effort from the user. As the users designing and adapting experiments will not necessarily have a computer science background, we also have to consider how publishing and discovery can be achieved in such a way that they are not required to have detailed technical knowledge of workflow scripting languages. Furthermore, we believe they should be able to make use of others\u2019 expert knowledge (the semantics) of the given scientific domain. In this paper, we define the issues related to the semantic description, publishing and discovery of workflows, and demonstrate\u00a0\u2026", "num_citations": "15\n", "authors": ["1168"]}
{"title": "The Grid needs you! Enlist now\n", "abstract": " \u201cThe next big thing will be grid computing\u201d \u2013 John Patrick, IBM\u2019s vice-president for Internet strategies. Are you involved with Grid applications or infrastructure? If not, why not? Not sure what Grid is? Think it isn\u2019t relevant to you or you aren\u2019t relevant to it? Think its just high performance computing for high energy physicists? Think that it\u2019s a semantic-free or data-free zone? Think it isn\u2019t important? In this talk I\u2019ll give an introduction to the state of play of Grid today and the applications using it. I\u2019ll give a summary of its evolution, paying particular attention to three significant initiatives that impact the OTM community: the emergence of the Open Grid Service Architecture, the prominence of Data/Information Grids and the appearance of Semantic Grids. I\u2019ll give a few examples of early Semantic Grid projects, and show why distributed information and knowledge management are key-enablers of Grid Services and\u00a0\u2026", "num_citations": "15\n", "authors": ["1168"]}
{"title": "The Semantic Web: an evolution for a revolution\n", "abstract": " The Web is an indisputable success. It has revolutionised the publication and dissemination of information. However, to access and interpret that information necessitates human intervention. The vision of the Semantic Web is to evolve the current Web to one where information and services are understandable and usable by computers as well as humans-to create a Web for machines. The automated processing of Web content requires, at its heart, that explicit machine-processable semantics be associated with Web resources as metadata so that it can be interpreted and combined. The Semantic Web does not replace the Web; it sits on top of the Web as an integrating descriptive fabric. Such an environment forms a platform for search engines, information brokers and ultimately the intelligent agents.", "num_citations": "15\n", "authors": ["1168"]}
{"title": "Towards the semantic grid: Enriching content for management and reuse\n", "abstract": " Knowledge and Semantic Web technologies are evolving the Grid towards the Semantic Grid [18] to facilitate knowledge reuse and collaboration within a community of practice. In the Geodise project we are exploring the application of a range of knowledge and Semantic Web technologies to assist users in solving complex problems in Engineering Design Search and Optimization (EDSO), in particular enabling semantically enriched resource sharing and reuse.The target of content enrichment in Geodise ranges from command usage described in software manuals, a set of profile data, to a workflow customized to solve a particular problem. They become semantically enriched when their representations are delivered using a set of shared semantics which are well recognized in the domain. Knowledge acquisition and knowledge modelling (in particular ontology building) are the key steps to build these semantics.", "num_citations": "15\n", "authors": ["1168"]}
{"title": "Scientific Lenses over Linked Data: An approach to support task specific views of the data. A vision.\n", "abstract": " Within complex scientific domains such as pharmacology, operational equivalence between two concepts is often context-, user-and task-specific. Existing Linked Data integration procedures and equivalence services do not take the context and task of the user into account. We present a vision for enabling users to control the notion of operational equivalence by applying scientific lenses over Linked Data. The scientific lenses vary the links that are activated between the datasets which affects the data returned to the user.", "num_citations": "14\n", "authors": ["1168"]}
{"title": "Semantically Annotated Provenance in the Life Science Grid.\n", "abstract": " Selected semantic annotation on raw provenance data can help bridge the gap between low level provenance events (eg, service invocations, data creation, message passing) and the high-level view that the user has of his/her investigation (eg, data retrieval and analysis). In this initial investigation we added semantically annotated provenance to the Life Science Grid, a cyber-infrastructure framework supporting interactive data exploration and automated data analysis tools, through (i) automated data provenance collection and (ii) automated semantic enrichment of the collected provenance metadata. We use a paradigmatic life sciences use case of interactive data exploration to show that semantically annotated provenance can help users recognize the occurrence of specific patterns of investigation from an otherwise low-level sequence of elementary interaction events.", "num_citations": "14\n", "authors": ["1168"]}
{"title": "Using Description Logics to Drive Query Interfaces\n", "abstract": " Description Logics have long been advocated as a suitable framework for partially structured data. A conceptual model can provide a space within which a user can navigate when constructing queries. In particular, the hierarchical compositional models provided by a Description Logic have the potential to support complex incremental manipulations of query expressions. However, interacting with a Description Logic is not always easy. Systems generally use textual interfaces, where the user requires not only an understanding of the underlying representation but also its particular concrete syntax. We present some ideas on the use of a Description Logic with the addition of sanctions to drive graphical user interfaces facilitating the construction and manipulation of queries. This use of sanctions allows the constrained formulation of queries through a dialogue, with the interface providing feedback relating to both the schema and contents of the database. We have two current prototypes. The rst, developed as part of the TAMBIS project Tam], allows molecular biologists to construct queries over concepts in biology. These description logic queries are then rewritten to database queries directed to various information sources. The second uses a small demonstration database of people and documents, and provides additional feedback about query results.", "num_citations": "14\n", "authors": ["1168"]}
{"title": "Opening new gateways to workflows for life scientists\n", "abstract": " The combination of highly complex biology problems and varying IT skills among life scientists poses a unique challenge in designing bioinformatics programs. The set of tools and initiatives described in this work shows new ways of making life science workflows more accessible to the community. Our aim is to help bioinformaticians help biologists. We present how to make Taverna workflows available from within Galaxy, both widely used bioinformatics platforms. Calling Galaxy tools from Taverna is also discussed. In addition, we describe a web application that allows a user to run arbitrary Taverna workflows by only using a web browser.", "num_citations": "13\n", "authors": ["1168"]}
{"title": "The myexperiment open repository for scientific workflows\n", "abstract": " ABSTRACT myExperiment is an open repository solution for the born-digital items arising in contemporary research practice, in particular scientific workflows and experiment plans. Launched in November 2007, the public repository (myexperiment. org) has established a significant collection of scientific workflows, spanning multiple disciplines and multiple workflow systems, which has been accessed by over 16,000 users worldwide. Built according to Web 2.0 design principles, myExperiment demonstrates the success of blending modern social curation methods with the demands of researchers sharing hard-won intellectual assets and research works within a scholarly communication lifecycle. myExperiment is an important component in the revolution in creating, sharing and publishing scientific results, and has already established itself as a valuable and unique repository with a growing international presence.", "num_citations": "13\n", "authors": ["1168"]}
{"title": "Grid metadata management: Requirements and architecture\n", "abstract": " Metadata annotations of grid resources can potentially be used for a number of purposes, including accurate resource allocation to jobs, discovery of services, and precise retrieval of information resources. In order to realize this potential on a large scale, various aspects of metadata must be managed. These include uniform and secure access to distributed and independently maintained metadata repositories, as well as management of metadata lifecycle. In this paper we analyze these issues and present a service-oriented architecture for metadata management, called S-OGSA, that addresses them in a systematic way.", "num_citations": "13\n", "authors": ["1168"]}
{"title": "Managing semantic metadata for web/grid services\n", "abstract": " Web/Grid services\u2019 metadata and semantics are becoming increasing important for service sharing and effective reuse. In this paper we present a generic framework for engineering and managing services\u2019 Semantic Metadata (SMD) with the ultimate purpose of facilitating interoperability, automation, and knowledgeable reuse of services for problem solving. The framework addresses fundamental issues, approaches, and tools for the whole lifecycle of SMD management, in other words, those of acquiring, modeling, representing, publishing, and reusing services\u2019 SMD. It adopts ontologies and the Semantic Web technologies as the enabling technologies by which services\u2019 metadata are semantically enriched and made interoperable, understandable, and accessible on the Web/Grid for both humans and machines. In particular, mechanisms are proposed to make use of service SMD for service discovery and\u00a0\u2026", "num_citations": "13\n", "authors": ["1168"]}
{"title": "The semantic web and knowledge grids\n", "abstract": " The Semantic Web and the Knowledge Grid are recently proposed technological solutions to distributed knowledge management. Early experimental applications from the Life Science community indicate that the approaches have promise and suggest that this community be an appropriate nursery for grounding, developing and hardening the current, rather immature, machinery needed to deliver on the technological visions, which thus far have been dominated by technological curiosity rather than application-led practicality and relevance. Further necessary developments in theory, infrastructure, tools, and content management should and could be steered opportunistically by the needs and applications of Life Science.", "num_citations": "13\n", "authors": ["1168"]}
{"title": "Augmenting the mobility of profoundly blind Web travellers\n", "abstract": " Use the word \u2018accessibility\u2019 in the presence of any HCI     specialist and they will immediately think of creating open interfaces that can be accessed both visually and audibly. Further, mention \u2018accessability\u2019 to any forward thinking group of Web developers and they will start to quote the Web Accessability Initiative Guidelines (WAI) and extol the virtues of accessability checking tools like \u2018Bobby\u2019. Either way, both groups will focus on the obviously important area of \u2018sensory translation\u2019 but will miss one fundamental truth; profoundly blind people interact with their environment in a markedly different way from that of sighted individuals. We have realized that the ease of movement (mobility) around systems and information space (the hypertext/Web docuverse) is central to good accessibility; and that to achieve this we require additional mobility semantics within systems and information as a way of enhancing the user\u00a0\u2026", "num_citations": "13\n", "authors": ["1168"]}
{"title": "Semantic Grid-Convergence of Technologies\n", "abstract": " s other than the originators. Often these schemas are fixed, which makes them rather inflexible. Much of the metadata is hard-coded and buried in code libraries, type systems, or grid applications. This makes it hard to adapt and configure. Finally, understanding and know-how is frequently tacit, embedded in best practice and experience rather than explicitly recorded. This makes sharing and adaptation extremely difficult. Thus, existing Grid Services deal with knowledge in the form of metadata and its associated semantics in an implicit fashion, providing poor mechanisms for sharing this knowledge with other Grid components. The Semantic Grid is an initiative to systematically expose semantically rich information associated with resources to build more intelligent Grid services. It is an extension of the current Grid in which information and services are given well defined and explicitly represented meaning, better enabling computers and people to work in cooperation (see eg [6", "num_citations": "13\n", "authors": ["1168"]}
{"title": "Applying DLs to workflow reuse and repurposing\n", "abstract": " Workflow techniques are an important part of in silico experimentation, potentially allowing a scientist to describe and enact their experimental processes in a structured, repeatable and verifiable way. The myGrid (www. mygrid. org. uk) workbench, a set of components to build workflows in bioinformatics, currently allows access to a thousand globally distributed services and a hundred workflows, some of which orchestrate up to fifty services. Figure 2 shows the example of a myGrid workflow which gathers information about genetic sequences in support of research on Williams-Beuren syndrome [10]. Much of the research geared towards the construction of on-line processes (ie workflows) is led by a vision of automatic composition of services based on extensive formalisation (see for example www. daml. org/services/owl-s/pubarchive. html). Such research can be complemented with techniques that exploit those cases where existing workflows and fragments of workflows can be reused, thereby benefitting from hard-won human experience in composing services. A workflow fragment is a piece of an experimental description that is a coherent sub-workflow that makes sense to a domain specialist. Each fragment forms a useful resource in its own right and is identified and annotated at publication time. We distinguish between reuse, where workflows and workflow fragments created by one user might be used as is, and repurposing, where they are used as a starting point by others. The idea of repurposing is that a user looks for workflows that are close enough to the user\u2019s requirements so that these workflows can be fit to a new purpose. In Figure\u00a0\u2026", "num_citations": "13\n", "authors": ["1168"]}
{"title": "Building Large-scale, Service-Oriented Distributed Systems using Semantic Models\n", "abstract": " Service-oriented computing is a set of principles for building loosely-coupled distributed applications through the composition of autonomous services. For example, OGSA aims to define a core set of composable services for the Grid application domain that includes job submission and monitoring, accounting, reservation, execution planning and database access, and the goal is for applicationdevelopers to be able to extensively re-use these services when building large scale service-oriented distributed systems without duplicating existing functionality. Over time, there is the hope that a healthy ecosystem of Grid services will be developed, with rich new services feeding off the capabilities of existing services, and the new services themselves providing the basis for future higherlevel services.The tasks of discovering and composing services to meet a distributed application\u2019s functional and non-functional requirements are not always straightforward. This is especially true in a distributed environment where services are autonomous\u2013that is, they have been designed, implemented, deployed and managed in different administrative domains\u2013and is currently difficult as:", "num_citations": "13\n", "authors": ["1168"]}
{"title": "BioExcel Building Blocks, a software library for interoperable biomolecular simulation workflows\n", "abstract": " In the recent years, the improvement of software and hardware performance has made biomolecular simulations a mature tool for the study of biological processes. Simulation length and the size and complexity of the analyzed systems make simulations both complementary and compatible with other bioinformatics disciplines. However, the characteristics of the software packages used for simulation have prevented the adoption of the technologies accepted in other bioinformatics fields like automated deployment systems, workflow orchestration, or the use of software containers. We present here a comprehensive exercise to bring biomolecular simulations to the \u201cbioinformatics way of working\u201d. The exercise has led to the development of the BioExcel Building Blocks (BioBB) library. BioBB\u2019s are built as Python wrappers to provide an interoperable architecture. BioBB\u2019s have been integrated in a chain of usual\u00a0\u2026", "num_citations": "12\n", "authors": ["1168"]}
{"title": "Mim: A minimum information model vocabulary and framework for scientific linked data\n", "abstract": " Linked Data holds great promise in the Life Sciences as a platform to enable an interoperable data commons, supporting new opportunities for discovery. Minimum Information Checklists have emerged within the Life Sciences as a means of standardising the reporting of experiments in an effort to increase the quality and reusability of the reported data. Existing tooling built around these checklists is aimed at supporting experimental scientists in the production of experiment reports that are compliant. It remains a challenge to quickly and easily assess an arbitrary set of data against these checklists. We present the MIM (Minimum Information Model) vocabulary and framework which aims to provide a practical, and scalable approach to describing and assessing Linked Data against minimum information checklists. The MIM framework aims to support three core activities: (1) publishing well described minimum\u00a0\u2026", "num_citations": "12\n", "authors": ["1168"]}
{"title": "Functional units: Abstractions for web service annotations\n", "abstract": " Computational and data-intensive science increasingly depends on a large Web Service infrastructure, as services that provide a broad array of functionality can be composed into workflows to address complex research questions. In this context, the goal of service registries is to offer accurate search and discovery functions to scientists. Their effectiveness, however, depends not only on the model chosen to annotate the services, but also on the level of abstraction chosen for the annotations. The work presented in this paper stems from the observation that current annotation models force users to think in terms of service interfaces, rather than of high-level functionality, thus reducing their effectiveness. To alleviate this problem,we introduce Functional Units (FU) as the elementary units of information used to describe a service. Using popular examples of services for the Life Sciences, we define FUs as\u00a0\u2026", "num_citations": "12\n", "authors": ["1168"]}
{"title": "Literature review on patient-friendly documentation systems\n", "abstract": " This literature review forms a deliverable in the European Network of Excellence on Semantic Interoperability and Data Mining in Biomedicine. More specifically, it is part of a work package (wp27) which aims to develop and evaluate generic methods and tools for assisting patients to understand their health and healthcare by generating patient-friendly readable texts that paraphrase the content of their electronic health records. We have reviewed the literature in topics that we consider to be relevant to this work package. When appropriate, we cover variations in conditions in the four countries of the collaborating research groups (France, Germany, Sweden and the uk) and we cover corpora, tools and language technologies for the European languages of interest to these groups. First, we consider legal issues involved in patients gaining access to their medical records. Who can view the records? What data do they have the right to access? Are there any data that patients cannot access? Who can access records of dead patients? What about security and data protection? See chapter 2 for brief surveys of the current state of affairs in France, Sweden and the uk.Patient records are packed with jargon, acronyms and medical terms that clinical staff know and understand. Often there is a learning curve before patients become familiar with medical terms associated with their own particular illnesses and they may require more familiar words and phrases to describe medical concepts in an accessible form. The development of largescale medical term banks, thesauri and lexicons (eg ums specialist and Metathesaurus) enable Language Technology\u00a0\u2026", "num_citations": "12\n", "authors": ["1168"]}
{"title": "Managing semantic metadata for the semantic grid\n", "abstract": " Research on the Semantic Web and Web/Grid resource description, discovery and composition is booming but there is currently little effort on a systematic and integrated approach to the management of resources\u2019 Semantic Metadata (SMD), nor on key tools that add, store and reuse SMD. In this paper we propose a generic framework for managing resource SMD, in which ontologies are used for metadata modeling and the Web Ontology Language (OWL) for semantic representation. Generated resource SMD are archived in a knowledge repository enhanced with Description Logic (DL) based reasoning capability. A raft of tools, mechanisms and APIs are developed to support SMD management lifecycle, including metadata generation, semantic annotation, knowledge storage and semantic reuse. Both the framework and its supporting technologies have been applied to a large existing e-Science project, which has produced a working resource management prototype. While SMD can be exploited in many ways with regards to resource discovery, provenance and trust, we illustrate their usage through a knowledge advisor that assists resource assembly and configuration in the context of engineering design search and optimisation", "num_citations": "12\n", "authors": ["1168"]}
{"title": "Empowering resource providers to build the semantic grid\n", "abstract": " The future success of Grid-enabled e-Science depends on the availability of semantic/knowledge-rich resources on the Grid, i.e., the so-called semantic Grid. This requires not only novel knowledge modelling and representation formalisms but also a shift of knowledge acquisition and population from a limited number of specialised knowledge engineers to resource providers. To this end we have developed a lightweight ontology-enabled tool, \"Function Annotator\", to support resource providers in capturing and publishing resource semantics and knowledge. Function Annotator takes a different line to most knowledge acquisition tools in that it is designed for use by resource providers, probably in the absence of a knowledge engineer. Its aim is to facilitate large scale knowledge population on the Grid. Function Annotator is built on the state-of-the-art of semantic web technologies, such as ontologies, OWL, instance\u00a0\u2026", "num_citations": "12\n", "authors": ["1168"]}
{"title": "Statistical composites: A transformation-bound representation of statistical datasets\n", "abstract": " Statistical data processing makes use of data matrices and tables as primary structures for data representation. Embedding these structures into processing-relevant context information gives rise to enhanced data structures linking data and metadata. The paper describes a framework for statistical data processing utilising metadata computationally.", "num_citations": "12\n", "authors": ["1168"]}
{"title": "Requirements of ontology languages\n", "abstract": " This deliverable provides an overview and summary of work in requirements for Ontology Languages. This is taken from a variety of third party sources, primarily the W3C\u2019s WebOnt working group along with requirements produced by communities such as the bioinformatics and medical communities. Note that as with Deliverable 4.0, this document is not concerned specifically with content, rather with the languages that may be used to deliver content. Particular domains will, by their nature, exact particular requirements from a representation language, but our interest here is in identifying a global collection of requirements (or at least the identification of such requirements).", "num_citations": "12\n", "authors": ["1168"]}
{"title": "Grid enabled optimisation and design search (geodise)\n", "abstract": " During the process of optimisation and design search, the modelling nad analysis of engineering problems are explited to yield improved designs. The engineer explores various design parameters that he wishes to optimise and a measure of the quality of a particular design (the objective function) is computed using an appropriate model. A number of algorithms may be used to yield more information about the behaviour of a model, and to minimise/maximise the objective function, and hence improve the quality of the design. This process may include lengthy and repetitive calculations to obtain the value of the objective function with respect to the design variables.", "num_citations": "12\n", "authors": ["1168"]}
{"title": "Supporting web based biology with ontologies\n", "abstract": " Complex questions posed by biologists require the fusion of evidence from different, independently developed and heterogeneous resources. The Web as an enabler for interoperability has been an excellent mechanism for data publication and transportation. However, successful exchange and integration of information depends on a shared language for communication (a terminology) and a shared understanding of what the data means (an ontology). One means of integrating bioinformatic resources is through mediation through a single terminology founded on an ontology. Our TAMBIS system is an example of such an approach. We show how TAMBIS uses a single terminology for interoperating between resources to answer complex questions. The approach is, however, \"high gain but high pain\". We conclude that for bioinformatics to realise the full potential of the Web requires a fundamental movement of the\u00a0\u2026", "num_citations": "12\n", "authors": ["1168"]}
{"title": "SYNBIOCHEM\u2013a SynBio foundry for the biosynthesis and sustainable production of fine and speciality chemicals\n", "abstract": " The Manchester Synthetic Biology Research Centre (SYNBIOCHEM) is a foundry for the biosynthesis and sustainable production of fine and speciality chemicals. The Centre's integrated technology platforms provide a unique capability to facilitate predictable engineering of microbial bio-factories for chemicals production. An overview of these capabilities is described.", "num_citations": "11\n", "authors": ["1168"]}
{"title": "Reputationnet: Reputation-based service recommendation for e-science\n", "abstract": " In the paradigm of service oriented science, scientific computing applications and data are all wrapped as web accessible services. Scientific workflows further integrate these services to answer complex research questions. However, our earlier study conducted on myExperiment has revealed that although the sharing of service-based capabilities opens a gateway to resource reuse, in practice, the degree of reuse is very low. This finding has motivated us to propose ServiceMap to provide navigation facility through the network of services to facilitate the design and development of scientific workflows. This paper proposes ReputationNet as an enhancement of ServiceMap, to incorporate the often-ignored reputation aspects of services/workflows and their publishers, in order to offer better service and workflow recommendations. We have developed a novel model to reflect the reputation of e-Science services\u00a0\u2026", "num_citations": "11\n", "authors": ["1168"]}
{"title": "Software in reproducible research: advice and best practice collected from experiences at the collaborations workshop\n", "abstract": " The Collaborations Workshop 2014 (CW14) brought together representatives from across the research community to discuss the issues around software's role in reproducible research. In this paper we summarise the themes, practices and ideas raised at the workshop. We also consider how the\" unconference\" format of the CW14 helps in eliciting information and forming future collaborations around aspects of reproducible research. In particular, we describe three distinct areas of concern which emerged from the event: collaboration readiness, capability enhancement and advocacy.", "num_citations": "11\n", "authors": ["1168"]}
{"title": "A Checklist-Based Approach for Quality Assessment of Scientific Information.\n", "abstract": " A Checklist-Based Approach for Quality Assessment of Scientifi c Information Page 1 A Checklist-Based Approach for Quality Assessment of Scientifi c Information Jun Zhao, Graham Klyne Department of Zoology, University of Oxford, Oxford, OX1 3PS, UK jun.zhao, graham.klyne@zoo.ox.ac.uk Matthew Gamble, Carole Goble Computer Science, University of Manchester, Manchester, M13 9PL, UK m.gamble@cs.man.ac.uk, carole.goble@manchester.ac.uk Page 2 47 of 53 \u201clandmark\u201d publications could not be replicated \u201cUnquestionably, a significant contributor to failure in oncology trials is the quality of published preclinical data.\u201d \u201cThe scientific process demands the highest standards of quality, ethics and rigour.\u201d http://www.nature.com/nature/journal/ v483/n7391/full/483531a.html Adapted from: Carole Goble Page 3 Validating in silico research Kristina Hettne Page 4 The Goal of Our Work We aim to provide tools that \u2026", "num_citations": "11\n", "authors": ["1168"]}
{"title": "Incorporating commercial and private data into an open linked data platform for drug discovery\n", "abstract": " The Open PHACTS Discovery Platform aims to provide an integrated information space to advance pharmacological research in the area of drug discovery. Effective drug discovery requires comprehensive data coverage, i.e. integrating all available sources of pharmacology data. While many relevant data sources are available on the linked open data cloud, their content needs to be combined with that of commercial datasets and the licensing of these commercial datasets respected when providing access to the data. Additionally, pharmaceutical companies have built up their own extensive private data collections that they require to be included in their pharmacological dataspace. In this paper we discuss the challenges of incorporating private and commercial data into a linked dataspace: focusing on the modelling of these datasets and their interlinking. We also present the graph-based access control\u00a0\u2026", "num_citations": "11\n", "authors": ["1168"]}
{"title": "SSemantic Data and Models Sharing in Systems Biology: The Just Enough Results Model and the SEEK Platform\n", "abstract": " Research in Systems Biology involves integrating data and knowledge about the dynamic processes in biological systems in order to understand and model them. Semantic web technologies should be ideal for exploring the complex networks of genes, proteins and metabolites that interact, but much of this data is not natively available to the semantic web. Data is typically collected and stored with free-text annotations in spreadsheets, many of which do not conform to existing metadata standards and are often not publically released.               Along with initiatives to promote more data sharing, one of the main challenges is therefore to semantically annotate and extract this data so that it is available to the research community. Data annotation and curation are expensive and undervalued tasks that have enormous benefits to the discipline as a whole, but fewer benefits to the individual data producers\u00a0\u2026", "num_citations": "11\n", "authors": ["1168"]}
{"title": "RO-Manager: a tool for creating and manipulating research objects to support reproducibility and reuse in sciences\n", "abstract": " In this position paper we present a lightweight command-line tool RO Manager, which provides a straightforward way for scientists to assemble an aggregation of their experiment materials and methods which can then be published and shared with colleagues or linked to scientific publications, to enhance the reproducibility and trustworthiness of experiment results. The tool is currently being tested by a small group of scientists from two different domains, who would like to preserve sufficient materials and information along with their scientific results in order to improve their reproducibility in the future.", "num_citations": "11\n", "authors": ["1168"]}
{"title": "FAME: Adding multi-level authentication to shibboleth\n", "abstract": " The paper describes the design of FAME (Flexible Access Middleware Extension) architecture aimed at providing multi-level user authentication service for Shibboleth, which is endorsed by the Joint Information Systems Committee (JISC) as the next generation authentication and authorisation infrastructure for the UK Higher Education community. FAME derives authentication assurance level based upon the strength of the authentication token and protocol used by the user when authenticating and feeds it to the PERMIS authorisation decision engine via Shibboleth to enable more fine-grained access control. In this way, access to resources is controlled according to the strength of the authentication method so that more sensitive resources may require users to identify themselves using a higher level of authentication.", "num_citations": "11\n", "authors": ["1168"]}
{"title": "Pedro ontology services: A framework for rapid ontology markup\n", "abstract": " Semantic Web technologies offer the possibility of increased accuracy and completeness in search and retrieval operations. In recent years, curators of data resources have begun favouring the use of ontologies over the use of free text entries. Generally this has been done by marking up existing database records with \u201cannotations\u201d that contain ontology term references. Although there are a number of tools available for developing ontologies, there are few generic resources for enabling this annotation process. This paper examines the requirements for such an annotation tool, and describes the design and implementation of the Pedro Ontology Service Framework, which seeks to fulfill these requirements.", "num_citations": "11\n", "authors": ["1168"]}
{"title": "Semantics of a Networked World. Semantics for Grid Databases First International IFIP Conference, ICSNW 2004, Paris, France, June 17-19, 2004, Revised Selected Papers\n", "abstract": " Semantics of a Networked World. Semantics for Grid Databases | SpringerLink Skip to main content Skip to table of contents Advertisement Hide SpringerLink Search SpringerLink Search Home Log in ICSNW: International Conference on Semantics for the Networked World Semantics of a Networked World. Semantics for Grid Databases First International IFIP Conference, ICSNW 2004, Paris, France, June 17-19, 2004, Revised Selected Papers Editors (view affiliations) Mokrane Bouzeghoub Carole Goble Vipul Kashyap Stefano Spaccapietra Conference proceedings ICSNW 2004 215 Citations 13 Mentions 9.2k Downloads Part of the Lecture Notes in Computer Science book series (LNCS, volume 3226) Download book PDF Papers Table of contents (25 papers) About About these proceedings Table of contents Search within event 1.Front Matter PDF 2.Invited Talks 1.Databases and the Grid: JDBC in WSDL, or \u2026", "num_citations": "11\n", "authors": ["1168"]}
{"title": "Exploiting semantics for e-science on the semantic grid\n", "abstract": " In this paper we address the problem of exploiting semantics for e-Science [1] in the emerging future e-Science infrastructure - the Semantic Grid [2]. The discussion is taken in the context of Grid enabled optimisation and design search in engineering (\u201cGeodise\u201d project) [3]. In our work we have developed a semantics based Grid-enabled computing architecture for Geodise. The architecture has incorporated a service-oriented distributed knowledge management framework for providing semantic and knowledge support. It uses ontologies as the conceptual backbone for information level and knowledge-level computation. Geodise resources including computational codes, capabilities and knowledge are semantically enriched using ontologies through annotations, thus facilitating seamless access, flexible interoperation and resource sharing on the Grid. We describe ontological engineering work and various approaches to semantic enrichment in Geodise. The semantically enriched content together with the Semantic Grid paradigm have been used as the foundation for the development of an ontology-enabled Geodise problem solving environment prototype (PSE). We have partially implemented the workflow construction environment in the Geodise PSE in which semantics is exploited to describe, discover and compose engineering computation resources for engineering problem-solving.", "num_citations": "11\n", "authors": ["1168"]}
{"title": "Towards an architecture for personalization and adaptivity in the semantic web\n", "abstract": " A semantic web is expected to underpin richer, and more automated, forms of interaction than web services and data allow today. However unlike syntactic forms, interpretations cannot be standardized across all potential users, or even be kept constant over time for a specific user. While a semantic web allows users to go beyond the syntax of resources by appealing to meta-resources and, more expressively, ontologies, it also raises the issue as to how are users to specify their own interpretations of web components and to avoid being overburdened with the need to keep those interpretations afresh. This paper argues that personalization and adaptivity are key technologies in addressing this particular issue for a semantic web. Without a proper solution the semantic web will be less usable than the vision would suggest it to be. The paper motivates the need for such technologies via an e-commerce use case and sketches an architecture that can be used to underpin their realization as concrete software components.", "num_citations": "11\n", "authors": ["1168"]}
{"title": "Database Challenges for Genome Information in the Post Sequencing Phase Moussouni\n", "abstract": " Genome sequencing projects are making available to scientists complete records of the genetic make-up of organisms. The resulting data sets, along with the results of experiments that seek systematically to find new information on the functions of genes, will present numerous opportunities and challenges to biologists. However, the complexity and variety of both the data and the analyses required over such data sets also pose significant challenges to computer scientists charged with providing effective information management systems for use with genome data. This paper presents models for the sorts of information that are being produced on genomes and genome-wide experiments, and outlines a project developing an information management system aimed at supporting analyses over genomic data. This information management system replicates data from other sources, with a view to providing an\u00a0\u2026", "num_citations": "11\n", "authors": ["1168"]}
{"title": "Kaleidoscape: a 3D environment for querying ODMG compliant databases\n", "abstract": " Kaleidoscape is a three dimensional (3D) implementation of a data-flow oriented visual query language, which has been implemented in 3D to examine the advantages and disadvantages of such an interface paradigm over current WIMP GUIs. This paper describes a version of Kaleidoscape that allows the user to construct queries from within a 3D environment. These queries are then translated into the ODMG standard textual query language OQL for evaluation, the results of which can be viewed and browsed from within the Kaleidoscape environment.", "num_citations": "11\n", "authors": ["1168"]}
{"title": "ELIXIR-UK role in bioinformatics training at the national level and across ELIXIR\n", "abstract": " ELIXIR-UK is the UK node of ELIXIR, the European infrastructure for life science data. Since its foundation in 2014, ELIXIR-UK has played a leading role in training both within the UK and in the ELIXIR Training Platform, which coordinates and delivers training across all ELIXIR members. ELIXIR-UK contributes to the Training Platform\u2019s coordination and supports the development of training to address key skill gaps amongst UK scientists. As part of this work it acts as a conduit for nationally-important bioinformatics training resources to promote their activities to the ELIXIR community. ELIXIR-UK also leads ELIXIR\u2019s flagship Training Portal, TeSS, which collects information about a diverse range of training and makes it easily accessible to the community. ELIXIR-UK also works with others to provide key digital skills training, partnering with the Software Sustainability Institute to provide Software Carpentry training to the\u00a0\u2026", "num_citations": "10\n", "authors": ["1168"]}
{"title": "On specifying and sharing scientific workflow optimization results using research objects\n", "abstract": " Reusing and repurposing scientific workflows for novel scientific experiments is nowadays facilitated by workflow repositories. Such repositories allow scientists to find existing workflows and re-execute them. However, workflow input parameters often need to be adjusted to the research problem at hand. Adapting these parameters may become a daunting task due to the infinite combinations of their values in a wide range of applications. Thus, a scientist may preferably use an automated optimization mechanism to adjust the workflow set-up and improve the result. Currently, automated optimizations must be started from scratch as optimization meta-data are not stored together with workflow provenance data. This important meta-data is lost and can neither be reused nor assessed by other researchers. In this paper we present a novel approach to capture optimization meta-data by extending the Research Object\u00a0\u2026", "num_citations": "10\n", "authors": ["1168"]}
{"title": "DSA-based verifiable and recoverable encryption of signatures and its application in certified e-goods delivery\n", "abstract": " We present a new method for verifiable and recoverable encryption of DSA signatures, and apply this cryptographic primitive in the design of a novel certified e-goods delivery (DSA-CEGD) protocol. The DSA-CEGD protocol has the following features: (1) ensures strong fairness, (2) ensures non-repudiation of origin and non-repudiation of receipt, (3) allows the receiver of an e-goods to verify, during the protocol execution, that the e-goods he is about to receive is the one he is signing the receipt for, (4) does not require the on-line involvement of a fully trusted third party (TTP), but rather an off-line and transparent semi-trusted third party (STTP), and (5) provides the confidentiality protection for the exchanged items from the STTP.", "num_citations": "10\n", "authors": ["1168"]}
{"title": "Knowledge services for distributed service integration\n", "abstract": " Key Objectives: Introduce a service-oriented approach to providing knowledge support for distributed computing, develop a generic knowledge service architecture to realise such an approach, apply this approach design search and optimisation (Geodise) to enhance the design process and also for architecture validation. Motivation for the work: While computing increasingly addresses collaboration, sharing and interaction involve distributed services, there is a growing demand for knowledge services that provide underlying semantic support for such distributed services and also support the sharing and coordinated use of knowledge itself.", "num_citations": "10\n", "authors": ["1168"]}
{"title": "The low down on e\u2010science and grids for biology\n", "abstract": " The Grid is touted as a next generation Internet/Web, designed primarily to support e\u2010Science. I hope to shed some light on what the Grid is, its purpose, and its potential impact on scientific practice in biology. The key message is that biologists are already primarily working in a manner that the Grid is intended to support. However, to ensure that the Grid's good intentions are appropriate and fulfilled in practice, biologists must become engaged in the process of its development. Copyright \u00a9 2001 John Wiley & Sons, Ltd.", "num_citations": "10\n", "authors": ["1168"]}
{"title": "Information Management for Genome Level Bioinformatics.\n", "abstract": " Information Management for Genome Level Bioinformatics Page 1 Information Management for Genome Level Bioinformatics Norman Paton and Carole Goble Department of Computer Science University of Manchester Manchester, UK <norm, carole>@cs.man.ac.uk Structure of Tutorial \u220e Introduction - why it matters. \u220e Genome level data. \u220e Genomic databases. \u220e Modelling challenges. \u220e Integrating biological databases. \u220e Analysing genomic data. \u220e Summary and challenges. Information Management for Genome Level Bioinformatics - Paton, Goble 213 Page 2 Information Management for Genome Level Bioinformatics Norman Paton and Carole Goble Department of Computer Science University of Manchester Manchester, UK <norm, carole>@cs.man.ac.uk Structure of Tutorial \u220e Introduction - why it matters. \u220e Genome level data. \u220e Modelling challenges. \u220e Genomic databases. \u220e Integrating biological databases. \u2026", "num_citations": "10\n", "authors": ["1168"]}
{"title": "A lightweight approach to research object data packaging\n", "abstract": " A Research Object (RO) provides a machine-readable mechanism to communicate the diverse set of digital and real-world resources that contribute to an item of research. The aim of an RO is to evolve from traditional academic publication as a static PDF, to rather provide a complete and structured archive of the items (such as people, organisations, funding, equipment, software etc) that contributed to the research outcome, including their identifiers, provenance, relations and annotations.", "num_citations": "9\n", "authors": ["1168"]}
{"title": "Cloud computing for data\u2010driven science and engineering\n", "abstract": " During the past decade, data-driven science and engineering have emerged as a key paradigm for performing scientific research, enabling innovations through new kinds of experiments that were earlier impossible. Today\u2019s science has access to advanced instruments like next generation genome sequencers, gigapixel survey telescopes, and networks of sensors that monitor cyber-physical systems, and these are generating datasets that are growing exponentially in complexity and data volume. Big Data, across all dimensions of volume, velocity, variety, and veracity, are offering unique opportunities to enable scientific discovery as well as novel challenges to scientific platforms. Such dynamic, distributed, and data-intensive applications hold the solutions to vital scientific and societal problems of the 21st century. In order to achieve breakthrough in new knowledge, there is a need to develop data-driven system\u00a0\u2026", "num_citations": "9\n", "authors": ["1168"]}
{"title": "Scientific workflows\n", "abstract": " The use of data processing workflows within the business sector has been commonplace for many years. Their use within the scientific community, however, has only just begun. With the uptake of workflows within scientific research, an unprecedented level of data analyses is now at the fingertips of individual researchers, leading to a change in the way research is carried out. This chapter describes the advantages of using workflows in modern biological research; demonstrating research from the field where the application of workflow technologies was vital for understanding the processes involved in resistance and susceptibility of infection by a parasite. Specific attention is drawn to the Taverna Workflow Workbench (Hull et al. 2006), a workflow management system that provides a suite of tools to support the design, execution, and management of complex analyses in the data intensive research, for example, in the Life Sciences.", "num_citations": "9\n", "authors": ["1168"]}
{"title": "OntoGrid: A semantic grid reference architecture\n", "abstract": " Grid Services currently deal with this semantic infrastructure in ad-hoc and hidden ways, providing poor mechanisms for sharing and openly processing knowledge. This makes the knowledge hard to share, and hard to interpret by services other than the originators. Often these schemas are fixed, which makes them rather inflexible. Much of the metadata is hard-coded and buried in code libraries, type systems, or grid applications. This makes it hard to adapt and configure. Finally, understanding and know-how is frequently tacit, embedded in best practice and experience rather than explicitly recorded. This makes sharing, customisation and adaptation difficult, and dependent on scarce human effort. The Semantic Grid aims to provision a semantic infrastructure for Grid infrastructure to improve sharing, enable unanticipated reuse of resources, support interoperability and enable more flexible and configurable\u00a0\u2026", "num_citations": "9\n", "authors": ["1168"]}
{"title": "\" Engineering accessible design\" W4A--international crossdisciplinary workshop on web accessibility 2005 workshop report\n", "abstract": " Previous engineering approaches seem to have precluded the engineering of accessible systems. This is plainly unsatisfactory. Designers, authors, and technologist are at present playing 'catch-up' with a continually moving target in an attempt to retrofit systems. Infact engineering accessible interfaces is as important as their functionality's and should be an indivisible part of the development. We should be engineering accessibility as part of the development and not as afterthought or because government restrictions and civil law requires us to. Our workshop brought together a cross section of the Web design and engineering communities; to report on developments, discuss the issues, and suggest cross-pollinated solutions.Conventional workshops on accessibility tended to be single disciplinary in nature. However, we were concerned that a single disciplinary approach prevents the cross-pollination of ideas\u00a0\u2026", "num_citations": "9\n", "authors": ["1168"]}
{"title": "Plugging a scalable authentication framework into Shibboleth\n", "abstract": " In a VO (virtual organization) environment where services are provided and shared by dissimilar organizations from different administrative domains and are protected with dissimilar security policies and measures, there is a need for a flexible authentication framework that supports the use of various authentication tokens. The authentication strengths derived from these tokens should be fed into an access control decision making process. This paper reports our ongoing efforts in designing and implementing such a framework to facilitate multi-level and multi-factor authentication and authentication strength linked fine-grained access control in Shibboleth. The proof-of-concept prototype using a Java smart card is reported.", "num_citations": "9\n", "authors": ["1168"]}
{"title": "The Origin and History of in-silico Experiments\n", "abstract": " It is not enough to be able to just run an e-Science in silico experiment; it is also vital to be able to understand and interpret the outputs of those experiments. The results have little value if other scientists, or even the same scientist at a later date, are unable to identify their origin, or provenance. In myGrid, in silico experiments are run as workflows; these produce three kinds of results: data outcomes, knowledge outcomes and provenance about the experiment. These results have a complex interlinking relationship between each other, within the context of the workflow that gave rise to them, as well as across workflows executed in the same or a different study. This poster describes the kind of provenance data recorded in myGrid during a workflow. It introduces myGrid\u2019s provenance data model and the Semantic Web-based technology used to support provenance-based tasks. These tasks include the verification and validation of results; the sharing and annotation of results; and the management of resources. For e-Science to succeed it must have provenance data support as its cornerstone.", "num_citations": "9\n", "authors": ["1168"]}
{"title": "Semantic web applications to e-science in silico experiments\n", "abstract": " This paper explains our research and implementations of manual, automatic and deep annotations of provenance logs for e-Science insilico experiments. Compared to annotating general Web documents, annotations for scientific data require more sophisticated professional knowledge to recognize concepts from documents, and more complex text extraction and mapping mechanisms. A simple automatic annotation approach based on\" lexicons\" and a deep annotation implemented by semantically populating, translating and annotating provenance logs are introduced in this paper. We used COHSE (Conceptual Open Hypermedia Services Environment) to annotate and browse provenance logs from my Grid project, which are conceptually linked together as a hypertext Web of provenance logs and experiment resources, based on the associated conceptual metadata and reasoning over these metadata.", "num_citations": "9\n", "authors": ["1168"]}
{"title": "Clustering techniques in biological sequence analysis\n", "abstract": " In biological sequence analysis many DNA and RNA sequences discovered in laboratory experiments are not properly identified. Here the focus is on using clustering algorithms to provide a structure to the data. The approach is inter-disciplinary using domain knowledge to identify such sequences. The enormous volume and high dimensionality of unidentified biological sequence data presents a challenge. Nonetheless useful and interesting results have been obtained, both directly and indirectly, by applying clustering to the data.", "num_citations": "9\n", "authors": ["1168"]}
{"title": "Workflows Community Summit: Bringing the Scientific Workflows Community Together\n", "abstract": " Scientific workflows have been used almost universally across scientific domains, and have underpinned some of the most significant discoveries of the past several decades. Many of these workflows have high computational, storage, and/or communication demands, and thus must execute on a wide range of large-scale platforms, from large clouds to upcoming exascale high-performance computing (HPC) platforms. These executions must be managed using some software infrastructure. Due to the popularity of workflows, workflow management systems (WMSs) have been developed to provide abstractions for creating and executing workflows conveniently, efficiently, and portably. While these efforts are all worthwhile, there are now hundreds of independent WMSs, many of which are moribund. As a result, the WMS landscape is segmented and presents significant barriers to entry due to the hundreds of seemingly comparable, yet incompatible, systems that exist. As a result, many teams, small and large, still elect to build their own custom workflow solution rather than adopt, or build upon, existing WMSs. This current state of the WMS landscape negatively impacts workflow users, developers, and researchers. The \"Workflows Community Summit\" was held online on January 13, 2021. The overarching goal of the summit was to develop a view of the state of the art and identify crucial research challenges in the workflow community. Prior to the summit, a survey sent to stakeholders in the workflow community (including both developers of WMSs and users of workflows) helped to identify key challenges in this community that were translated into 6\u00a0\u2026", "num_citations": "8\n", "authors": ["1168"]}
{"title": "LabelFlow framework for annotating workflow provenance\n", "abstract": " Scientists routinely analyse and share data for others to use. Successful data (re) use relies on having metadata describing the context of analysis of data. In many disciplines the creation of contextual metadata is referred to as reporting. One method of implementing analyses is with workflows. A stand-out feature of workflows is their ability to record provenance from executions. Provenance is useful when analyses are executed with changing parameters (changing contexts) and results need to be traced to respective parameters. In this paper we investigate whether provenance can be exploited to support reporting. Specifically; we outline a case-study based on a real-world workflow and set of reporting queries. We observe that provenance, as collected from workflow executions, is of limited use for reporting, as it supports queries partially. We identify that this is due to the generic nature of provenance, its lack of domain-specific contextual metadata. We observe that the required information is available in implicit form, embedded in data. We describe Label Flow, a framework comprised of four Labelling Operators for decorating provenance with domain-specific Labels. Label Flow can be instantiated for a domain by plugging it with domain-specific metadata extractors. We provide a tool that takes as input a workflow, and produces as output a Labelling Pipeline for that workflow, comprised of Labelling Operators. We revisit the case-study and show how Labels provide a more complete implementation of reporting queries. View Full-Text", "num_citations": "8\n", "authors": ["1168"]}
{"title": "FAIRsharing: working with and for the community to describe and link data standards, repositories and policies\n", "abstract": " Community-developed standards, such as those for the identification and reporting of data, underpin reproducible and reusable research. The number of community-driven efforts has been on the rise since the early 2000s, their uptake, however, is slow and uneven. Analyzing 70 journals and publishers data policies, we find that these recommend databases and repositories 37 times more often than standards. When a reporting standard is recommended by a publisher, it is more likely to be a minimal reporting guideline than a model, format or ontology even if the latter are the machine-readable standards that underpin the utility of databases and repositories. Here, we evaluate the standards landscape, focusing on those for reporting data and metadata, and their implementation by databases and repositories; we also propose key performance indicators, and highlight the importance of developing open linked data models that instantiate these community standards. Lastly, we launch a call to action highlighting the role producers and consumers of standards and repositories must play to maximize the visibility and adoption of these resources.", "num_citations": "8\n", "authors": ["1168"]}
{"title": "Static analysis of Taverna workflows to predict provenance patterns\n", "abstract": " Workflows have found adoption in scientific domains particularly due to their automation and provenance features. Using workflows scientists can repeat analyses with different input parameters and later use provenance to access and compare results based on these respective parameters. A common assumption is that by designing an analysis as a workflow we get parameter-to-result traceability for free by using workflow provenance. This assumption holds for cases of coarse-grained traceability where an entire workflow is subjected to repetition and all workflow parameters contribute to all results. However, this assumption is not guaranteed to hold for cases requiring finer grained traceability: where a workflow is configured with collections of parameters and analyses within a workflow are repeated with combinations of parameters from collections. In this paper we identify two dimensions that affect fine-grained\u00a0\u2026", "num_citations": "8\n", "authors": ["1168"]}
{"title": "Developing a strategy for computational lab skills training through Software and Data Carpentry: Experiences from the ELIXIR Pilot action\n", "abstract": " Quality training in computational skills for life scientists is essential to allow them to deliver robust, reproducible and cutting-edge research. A pan-European bioinformatics programme, ELIXIR, has adopted a well-established and progressive programme of computational lab and data skills training from Software and Data Carpentry, aimed at increasing the number of skilled life scientists and building a sustainable training community in this field. This article describes the Pilot action, which introduced the Carpentry training model to the ELIXIR community.", "num_citations": "8\n", "authors": ["1168"]}
{"title": "Computing Identity Co-Reference Across Drug Discovery Datasets.\n", "abstract": " This paper presents the rules used within the Open PHACTS (http://www. openphacts. org) Identity Management Service to compute co-reference chains across multiple datasets. The web of (linked) data has encouraged a proliferation of identifiers for the concepts captured in datasets; with each dataset using their own identifier. A key data integration challenge is linking the co-referent identifiers, ie identifying and linking the equivalent concept in every dataset. Exacerbating this challenge, the datasets model the data differently, so when is one representation truly the same as another? Finally, different users have their own task and domain specific notions of equivalence that are driven by their operational knowledge. Consumers of the data need to be able to choose the notion of operational equivalence to be applied for the context of their application. We highlight the challenges of automatically computing co-reference and the need for capturing the context of the equivalence. This context is then used to control the co-reference computation. Ultimately, the context will enable data consumers to decide which co-references to include in their applications.", "num_citations": "8\n", "authors": ["1168"]}
{"title": "Research object management: opportunities and challenges\n", "abstract": " While electronic papers have played and continue to play a primordial role in the dissemination of research results, researchers now recognize that papers are by no means sufficient to communicate and share research results. As a step in this direction, we present research objects as an abstraction for communicating, sharing and reusing research results. As well as the paper describing the contribution made by the scientist, a research object bundles information about the hypothesis the scientist investigated, the workflow implementing the experiment ran to assess the hypothesis, the data set used, the results obtained, and the conclusions drawn by the scientist, and identify a set of research problems that together aim to enable the management of research objects. We also underline the important role that end-users and automation techniques can play to enable scalable management of research objects.", "num_citations": "8\n", "authors": ["1168"]}
{"title": "eScience\n", "abstract": " This chapter looks into how the use of semantic technologies can provide support to common needs in eScience projects, including data-intensive science, facilitating experiment knowledge reuse and recycle among scientists, lowering the barriers of knowledge exchange for interdisciplinary research, and bridging the gap between data from different sources and the gap between data sharing and digital scholarly publication. To illustrate this, we describe a set of pioneering semantic eScience projects that cover a diversity of application domains including bioinformatics, biology, chemistry, physics, environmental science, and astronomy, and we summarize some of the open issues and future lines of research and development in this area.", "num_citations": "8\n", "authors": ["1168"]}
{"title": "Workflow discovery: requirements from e-science and a graph-based solution\n", "abstract": " Much has been written on the promise of Web service discovery and (semi-) automated composition. In this discussion, the value to practitioners of discovering and reusing existing service compositions, captured in workflows, is mostly ignored. We present the case for workflows and workflow discovery in science and develop one discovery solution. Through a survey with 21 scientists and developers from the myGrid/Taverna workflow environment, workflow discovery requirements are elicited. Through a user experiment with 13 scientists, an attempt is made to build a benchmark for workflow ranking. Through the design and implementation of a workflow discovery tool, a mechanism for ranking workflow fragments is provided based on graph sub-isomorphism detection. The tool evaluation, drawing on a corpus of 89 public workflows and the results of the user experiment, finds that, for a simple showcase, the\u00a0\u2026", "num_citations": "8\n", "authors": ["1168"]}
{"title": "Building the Mobile Web: rediscovering accessibility?\n", "abstract": " After the launch of the Mobile Web Initiative at the World Wide Web Conference 2005, awareness is emerging that, today, mobile Web access suffers from interoperability and usability problems that make the Web difficult to use. With the move to small screen size, low bandwidth, and different operating modalities, technology is in effect simulating the sensory and cognitive impairments experienced by disabled users within the wider population of mobile device users. The Third International Cross-Disciplinary Workshop on Web Accessibility (W4A 2006) was targeted to bring together different communities working on similar problems to share ideas, discuss overlaps, and make the fledging mobile Web community aware of accessibility work that may have been overlooked. The main question asked was:", "num_citations": "8\n", "authors": ["1168"]}
{"title": "Knowledge-driven hyperlinks: Linking in the wild\n", "abstract": " Since Ted Nelson coined the term \u201cHypertext\u201d, there has been extensive research on non-linear documents. With the enormous success of the Web, non-linear documents have become an important part of our daily life activities. However, the underlying hypertext infrastructure of the Web still lacks many features that Hypertext pioneers envisioned. With advances in the Semantic Web, we can address and improve some of these limitations. In this paper, we discuss some of these limitations, developments in Semantic Web technologies and present a system \u2013 COHSE \u2013 that dynamically links Web pages. We conclude with remarks on future directions for semantics-based linking.", "num_citations": "8\n", "authors": ["1168"]}
{"title": "ODEGSG framework, knowledge-based annotation and design of grid services\n", "abstract": " The convergence of the Semantic Web and Grid technologies has resulted in the Semantic Grid. The great effort devoted in by the Semantic Web community to achieve the semantic markup of Web services (what we call Semantic Web Services) has yielded many markup technologies and initiatives, from which the Semantic Grid technology should benefit as, in recent years, it has become Web service-oriented. Keeping this fact in mind, our first premise in this work is to reuse the ODESWS Framework for the Knowledge-based markup of Grid services. Initially ODESWS was developed to enable users to annotate, design, discover and compose Semantic Web Services at the Knowledge Level. But at present, if we want to reuse it for annotating Grid services, we should carry out a detailed study of the characteristics of Web services and Grid services and thus, we will learn where they differ and why. Only\u00a0\u2026", "num_citations": "8\n", "authors": ["1168"]}
{"title": "Superstrate index control of waveguide grating reflectivity\n", "abstract": " Control of reflectivity of a relief grating in a high-index film overlaid on a monomode glass waveguide was achieved by adjustment of the superstrate refractive index with liquids. Weak gratings reflecting at 1531 nm were inscribed by UV laser ablation. The grating reflectivity was measured as the ratio of the transmission outside the reflection band, at 1536 nm, to that at the Bragg wavelength. Reflectivity with an air superstrate was <0.3 dB for both polarizations. In the TE polarization the grating strength increased to 20.5 dB after application of a liquid of index 1.45. In the TM polarization the strength increased to 27 dB with a superstrate index of 1.50. Good agreement was found with a theoretical model based on beam propagation.", "num_citations": "8\n", "authors": ["1168"]}
{"title": "Semantic constraints in a medical information system\n", "abstract": " Classical constraint handling in data processing systems is concerned with ensuring the integrity of the database. Constraints are usually based on attribute values in a domain, or on syntactic (structural) relationships between the number of values under one attribute related to one under another. In the area of medical informatics, and probably in other application areas, the classical types of constraint are insufficient to ensure the integrity required. A knowledge representation formalism called Structured Meta Knowledge (SMK) is presented which describes conceptual medical terms and their occurrences in individual patient records. The prototype clinical workstation PEN&PAD which uses SMK is introduced. The requirements for constraints in medical informatics are matched against classical constraint types, the extensions defined and compared with those existing in SMK. A scheme for constraints in\u00a0\u2026", "num_citations": "8\n", "authors": ["1168"]}
{"title": "ELIXIR\u2010EXCELERATE: establishing Europe's data infrastructure for the life science research of the future\n", "abstract": " Creating knowledge by connecting and analysing large amounts of life science data is transforming our society, allowing us to start addressing major scientific and societal challenges, such as adaptation to climate change or pathogen outbreaks in an interconnected world. Modern biology is dependent on the generation, sharing and integrated analysis of digital data at scale. A deeper understanding of biological systems is now becoming possible thanks to breakthroughs in technologies that study life systematically at different scales, from molecules and single-cell pathogens to complex animal or plant models and ecosystems as well as across temporal ranges spanning splitsecond reactions to multi-year clinical or agronomic trials, and beyond. The key to analyse and leverage this complex, fragmented and geographically dispersed life science data landscape is to ensure it is easy to find and reuse by\u00a0\u2026", "num_citations": "7\n", "authors": ["1168"]}
{"title": "Landscape analysis for the specimen data refinery\n", "abstract": " This report reviews the current state-of-the-art applied approaches on automated tools, services and workflows for extracting information from images of natural history specimens and their labels. We consider the potential for repurposing existing tools, including workflow management systems; and areas where more development is required. This paper was written as part of the SYNTHESYS+ project for software development teams and informatics teams working on new software-based approaches to improve mass digitisation of natural history specimens.", "num_citations": "7\n", "authors": ["1168"]}
{"title": "SYNBIOCHEM Synthetic Biology Research Centre, Manchester\u2013A UK foundry for fine and speciality chemicals production\n", "abstract": " The UK Synthetic Biology Research Centre, SYNBIOCHEM, hosted by the Manchester Institute of Biotechnology at the University of Manchester is delivering innovative technology platforms to facilitate the predictable engineering of microbial bio-factories for fine and speciality chemicals production. We provide an overview of our foundry activities that are being applied to grand challenge projects to deliver innovation in bio-based chemicals production for industrial biotechnology.", "num_citations": "7\n", "authors": ["1168"]}
{"title": "RightField: scientific knowledge acquisition by stealth through ontology-enabled spreadsheets\n", "abstract": " RightField is a Java application that provides a mechanism for embedding ontology annotation support for scientific data in Microsoft Excel or Open Office spreadsheets. The result is semantic annotation by stealth, with an annotation process that is less error-prone, more efficient, and more consistent with community standards. By automatically generating RDF statements for each cell a rich, Linked Data querying environment allows scientists to search their data and other Linked Data resources interchangeably, and caters for queries across heterogeneous spreadsheets. RightField has been developed for Systems Biologists but has since adopted more widely. It is open source (BSD license) and freely available from http://www.rightfield.org.uk.", "num_citations": "7\n", "authors": ["1168"]}
{"title": "ERGOT: Combining DHTs and SONs for semantic-based service discovery on the grid\n", "abstract": " The Grid has rapidly moved from a toolkit-centered approach, composed of a set of middleware tools, toward a more application-oriented Service Oriented Architecture in which resources are exposed as services. The soaring number of available services advocates distributed and semantic-based discovery architectures. Distribution promotes scalability and fault-tolerance whereas semantics is required to provide for meaningful descriptions of services and support their efficient retrieval. Current approaches exploit either Semantic Overlay Networks (SONs) or Distributed Hash Tables (DHTs) sweetened with some\u201d semantic sugar\u201d. SONs enable semantic driven query answering but are less scalable than DHTs, which on their turn, feature efficient but semantic-free query answering based on\u201d exact\u201d matching. This paper presents the ERGOT system combining DHTs and SONs to enable distributed and semanticbased service discovery on the Grid. We argue that these two models can benefit from each other in the sense that SONs can be constructed by exploiting DHTs mechanisms thus enlightening the way to the semantics-free content publishing and retrieval mechanisms of the latter. In particular, ERGOT allows establishing semantic links among peers, and the SONs, by scrutinizing semantic service descriptions they advertise on the DHT. As we will show semantic links can also be viewed as semantic shortcuts on the DHT. Finally, ERGOT exploits an ad hoc semantic similarity metric to perform service matchmaking and numerically rank results.", "num_citations": "7\n", "authors": ["1168"]}
{"title": "An ActOn-based semantic information service for EGEE\n", "abstract": " We describe an information service that aggregates metadata available in hundreds of information sources of the EGEE Grid infrastructure. It uses an ontology-based information integration architecture (ActOn), which is suitable the highly dynamic distributed information sources available in Grid systems, where information changes frequently and where the information of distributed sources has to be aggregated in order to solve complex queries. These two challenges are addressed by a metadata cache that works with an update-on-demand policy and by an information source selection module that selects the most suitable source at a given point in time, respectively. We have evaluated the quality of this information service, and compared it with other similar services from the EGEE production testbed, with promising results.", "num_citations": "7\n", "authors": ["1168"]}
{"title": "State of the nation in data integration\n", "abstract": " Data integration has been discussed as a major challenge in bioinformatics for many years, ever since the beginning of dataset collection and distributed publication. A whole bunch of approaches have been tried over the years. We\u2019ve had web-based systems that cross-linked data resources through massive cross indexing. We\u2019ve had federated systems, ontology-based mediation systems, data warehouses, and workflows. We\u2019ve seen lightweight protocols and heavyweight centralized systems; schema-reconcilation schemes and instance-linking through common vocabulary terms. We\u2019ve seen data standards proliferate and web service standards promise a new future. Now we have data mashing and specialist wikipedia. In 2002, Lincoln Stein argued that we were building a \u201cBioinformatics Nation\u201d[1]. Here I\u2019ll survey the State of the", "num_citations": "7\n", "authors": ["1168"]}
{"title": "Building the mobile web: rediscovering accessibility? W4A-International Cross-Disciplinary Workshop on Web Accessibility workshop report--2006\n", "abstract": " The Third W4A International Cross-Disciplinary Workshop on Web Accessibility was held on Monday 22nd and Tuesday 23rd May 2006 as part of the Fifteenth International World Wide Web Conference (WWW2006) located at the Edinburgh International Conference Centre. We ran over 2 days, welcomed 73 attendees, and were the biggest workshop at the conference. We accepted 41.6% of all submissions, each paper was peer reviewed by three of our programme committee. We published ISBN'ed proceedings as part of the ACM Digital Library, and eight of our authors have been invited to submit extended papers to the Springer Journal, Universal Access in the Information Society. Comments from our attendees, and our workshop evaluation questionnaires, suggested that they enjoyed the workshop and would be participating again next year. Our social programme also attracted 20 of our delegates. Overall we\u00a0\u2026", "num_citations": "7\n", "authors": ["1168"]}
{"title": "Recycling services and workflows through discovery and reuse\n", "abstract": " Workflows are a central component for representing e-Science procedures in myGrid. For myGrid to support their design, scientists must be able to discover appropriate services to orchestrate and also discover if colleagues have already designed something similar. myGrid integrates a number of software components to address these requirements. The myGrid registry stores service and workflow descriptions. PeDRo, a structured data entry tool, enables uses to annotate these descriptions. Taverna, the workflow workbench, closely integrates with the registry and PeDRo to ensure description and reuse of services and workflows is simple.", "num_citations": "7\n", "authors": ["1168"]}
{"title": "Sentinel: towards an ambient mobility network\n", "abstract": " Purpose: We are concerned with aiding the mobility of visually impaired travellers around often complex and unfamiliar internal and urban environments. To do this we focus on a users interaction with ambient devices because these device types provide an easy entry point for visually impaired individuals to interact with their surroundings. By augmenting the physical environment with mobility focused ambient devices and making existing devices universally accessible our goal of easy, focused, and confident mobility can be achieved. Method: We identify, through paper reviews and studies of empirical and anecdotal evidence, the social and technical problems that have so far barred consistent and cohesive development of an ambient mobility-network. Results: We suggest that multi-model sensory-interaction with objects and assistive devices within an environment is the only way to accomplish easy, focused\u00a0\u2026", "num_citations": "7\n", "authors": ["1168"]}
{"title": "TourisT\u2014conceptual hypermedia tourist information\n", "abstract": " The TourisT project is developing a prototype conceptual hypermedia tourism information system, using GRAIL, a terminological logic devised at the University of Manchester [1], to maintain the conceptual model. A primary concern of the work is to develop a system which assists the tourist seeking information. The project has thus used the results of ethnographic studies, carried out in tourist information centres, to inform the structure and content of the conceptual model, and to determine what styles of interaction should be supported.", "num_citations": "7\n", "authors": ["1168"]}
{"title": "Image database prototypes\n", "abstract": " Image database prototypes | The handbook of multimedia information management ACM Digital Library home ACM home Google, Inc. (search) Advanced Search Browse About Sign in Register Advanced Search Journals Magazines Proceedings Books SIGs Conferences People More Search ACM Digital Library SearchSearch Advanced Search Browse Browse Digital Library Collections More HomeBrowse by TitleBooksThe handbook of multimedia information managementImage database prototypes chapter Image database prototypes Share on Author: Carole Anne Goble profile image Carole Goble View Profile Authors Info & Affiliations Publication: The handbook of multimedia information managementApril 1997 Pages 365\u2013404 weeks0 Get Citation Alerts New Citation Alert added! This alert has been successfully added \u2026", "num_citations": "7\n", "authors": ["1168"]}
{"title": "Methods included: Standardizing computational reuse and portability with the common workflow language\n", "abstract": " Computational Workflows are widely used in data analysis, enabling innovation and decision-making. In many domains (bioinformatics, image analysis, & radio astronomy) the analysis components are numerous and written in multiple different computer languages by third parties. However, many competing workflow systems exist, severely limiting portability of such workflows, thereby hindering the transfer of workflows between different systems, between different projects and different settings, leading to vendor lock-ins and limiting their generic re-usability. Here we present the Common Workflow Language (CWL) project which produces free and open standards for describing command-line tool based workflows. The CWL standards provide a common but reduced set of abstractions that are both used in practice and implemented in many popular workflow systems. The CWL language is declarative, which allows expressing computational workflows constructed from diverse software tools, executed each through their command-line interface. Being explicit about the runtime environment and any use of software containers enables portability and reuse. Workflows written according to the CWL standards are a reusable description of that analysis that are runnable on a diverse set of computing environments. These descriptions contain enough information for advanced optimization without additional input from workflow authors. The CWL standards support polylingual workflows, enabling portability and reuse of such workflows, easing for example scholarly publication, fulfilling regulatory requirements, collaboration in/between academic research\u00a0\u2026", "num_citations": "6\n", "authors": ["1168"]}
{"title": "CWLProv: Interoperable retrospective provenance capture and computational analysis sharing\n", "abstract": " Background: The automation of data analysis in the form of scienti c work ows has become a widely adopted practice in many elds of research. Computationally driven data-intensive experiments using work ows enable Automation, Scaling, Adaption and Provenance support (ASAP). However, there are still several challenges associated with the e ective sharing, publication, understandability and reproducibility of such work ows due to the incomplete capture of provenance and lack of interoperability between di erent technical (software) platforms. Results: Based on best practice recommendations identi ed from literature on work ow sharing and publishing, we de ne four hierarchical levels of provenance that collectively result in comprehensive and fully re-executable work ows when used with domain-speci c information. To realise these levels, we present CWLProv, a standard-based format to represent any work ow-based computational analysis to produce work ow output artefacts that satisfy the various levels of provenance. We utilise open source community-driven standards; interoperable work ow de nitions in Common Work ow Language (CWL), structured provenance representation using the W3C PROV model, and resource aggregation and sharing as work ow-centric Research Objects (RO) generated along with the nal outputs of a given work ow enactment. We illustrate this approach through a practical demonstration of CWLProv applied to real-life genomic work ows developed by independent groups.Conclusions: Our approach to work ow sharing and publication mitigates work ow decay. The underlying principles of the standards\u00a0\u2026", "num_citations": "6\n", "authors": ["1168"]}
{"title": "UK research software survey 2014\n", "abstract": " This spreadsheet contains the anonymised data collected as part of a survey of UK researchers in their use of research software.  We asked people specifically about \u201cresearch software\u201d which we defined as:  \u201cSoftware that is used to generate, process or analyse results that you intend to appear in a publication (either in a journal, conference paper, monograph, book or thesis). Research software can be anything from a few lines of code written by yourself, to a professionally developed software package. Software that does not generate, process or analyse results - such as word processing software, or the use of a web search - does not count as \u2018research software\u2019 for the purposes of this survey.\u201d We contacted 1,000 randomly selected researchers at each of 15 Russell Group universities. From the 15,000 invitations to complete the survey, we received 417 responses \u2013 a rate of 3% which is fairly normal for a blind\u00a0\u2026", "num_citations": "6\n", "authors": ["1168"]}
{"title": "Results may vary: reproducibility, open science and all that jazz\n", "abstract": " How could we evaluate research and researchers? Reproducibility underpins the scientific method: at least in principle if not practice. The willing exchange of results and the transparent conduct of research can only be expected up to a point in a competitive environment. Contributions to science are acknowledged, but not if the credit is for data curation or software. From a bioinformatics view point, how far could our results be reproducible before the pain is just too high? Is open science a dangerous, utopian vision or a legitimate, feasible expectation? How do we move bioinformatics from one where results are post-hoc made reproducible, to pre-hoc born reproducible? And why, in our computational information age, do we communicate results through fragmented, fixed documents rather than cohesive, versioned releases? In this talk, which I gave as a keynote at the 2013 joint conference Intelligent Systems in\u00a0\u2026", "num_citations": "6\n", "authors": ["1168"]}
{"title": "Standing on the shoulders of the trusted web: Trust, Scholarship and Linked Data.\n", "abstract": " The web of linked data is incompatible with the modern \u201cselfish scientist\u201d. What is missing is a mechanism that supports both what scientists share, and how they share. Solutions must be informed by social, technical and cultural issues surrounding the sharing of scientific data in the web of linked data. We propose the adoption of social trust techniques to share a new emerging class of scientific digital object-Research Objects. We suggest a mechanism for introducing social trust metrics into the distributed social web to facilitate access control to aggregations of linked data resources. Through the application and analysis of two established trust metrics, we then present the grounding of the Colleague of a Colleague (Cocoa) trust metric suited to the sharing of scientific knowledge delivered as Research Objects.", "num_citations": "6\n", "authors": ["1168"]}
{"title": "The return of the prodigal web: 1\n", "abstract": " We argue that the Semantic Web and Web 2.0 herald a return to hypertext's original visions and provide a means and an opportunity to bring full hypermedia capability to the Web.", "num_citations": "6\n", "authors": ["1168"]}
{"title": "Active ontology: An information integration approach for dynamic information sources\n", "abstract": " In this paper we describe an ontology-based information integration approach that is suitable for highly dynamic distributed information sources, such as those available in Grid systems. The main challenges addressed are: 1) information changes frequently and information requests have to be answered quickly in order to provide up-to-date information; and 2) the most suitable information sources have to be selected from a set of different distributed ones that can provide the information needed. To deal with the first challenge we use an information cache that works with an update-on-demand policy. To deal with the second we add an information source selection step to the usual architecture used for ontology-based information integration. To illustrate our approach, we have developed an information service that aggregates metadata available in hundreds of information services of the EGEE Grid infrastructure.", "num_citations": "6\n", "authors": ["1168"]}
{"title": "An Authorization Scenario for S-OGSA\n", "abstract": " The Semantic Grid initiative aims to exploit knowledge in the Grid to increase the automation, interoperability and flexibility of Grid middleware and applications. To bring a principled approach to developing Semantic Grid Systems, and to outline their core capabilities and behaviors, we have devised a reference Semantic Grid Architecture called S-OGSA. We present the implementation of an S-OGSA observant semantically-enabled Grid authorization scenario, which demonstrates two aspects: 1) the roles of different middleware components, be them semantic or non-semantic, and 2) the utility of explicit semantics for undertaking an essential activity in the Grid: resource access control.", "num_citations": "6\n", "authors": ["1168"]}
{"title": "Towards a semantic grid architecture\n", "abstract": " services are given well defined and explicitly represented meaning, better enabling computers and people to work in cooperation. Existing Grid Services deal with knowledge in the form of metadata and its associated semantics in an implicit fashion, providing no way to share this knowledge with other Grid components. Semantic Grids not only share computational and data resources, but also explicitly share and process metadata and knowledge.In the last few years, several projects have embraced this vision and there are already successful pioneering applications that combine the strengths of the Grid and of semantic technologies. However, the Semantic Grid currently lacks a reference architecture, or a systematic approach for designing Semantic Grid components or applications.", "num_citations": "6\n", "authors": ["1168"]}
{"title": "Towards a semantic grid architecture\n", "abstract": " The Semantic Grid is an extension of the current Grid in which information and services are given well defined and explicitly represented meaning, better enabling computers and people to work in cooperation. In the last few years, several projects have embraced this vision and there are already successful pioneering applications that combine the strengths of the Grid and of semantic technologies. However, the Semantic Grid currently lacks a reference architecture, or a systematic approach for designing Semantic Grid components or applications. We need a Reference Semantic Grid Architecture that extends the Open Grid Services Architecture by explicitly defining the mechanisms that will allow for the explicit use of semantics and the associated knowledge to support a spectrum of service capabilities. An architecture would have (at least) three major components which are depicted in the extended abstract.", "num_citations": "6\n", "authors": ["1168"]}
{"title": "Semantic support for grid-enabled design search in engineering\n", "abstract": " Semantic Web technologies are evolving the Grid towards the Semantic Grid [2] to yield an intelligent grid which allows seamless process automation, easy knowledge reuse and collaboration within a community of practice. We discuss our endeavours in this direction in the context of Grid enabled optimisation and design search in engineering (\u201cGeodise\u201d project)[3]. In our work we have developed a semantics-based Grid-enabled computing architecture for Geodise. The architecture incorporates a service-oriented distributed knowledge management framework for providing various semantic and knowledge support. It uses ontologies as the conceptual backbone for information-level and knowledge-level computation. We also describe ontological engineering work and a service-oriented approach to ontology deployment. We present several application examples that show the benefit of semantic support in Geodise.", "num_citations": "6\n", "authors": ["1168"]}
{"title": "Ontological Linking: Motivation and Analysis\n", "abstract": " This paper discusses the motivation for applying an ontological knowledge model to hypertext documents in an attempt to improve aspects of link quality. An implementation used in the COHSE project is briefly discussed and a study of its application to a well-developed, high-traffic Web site is presented, together with a quantitative analysis of its effect on the navigation and link structuring of the site.", "num_citations": "6\n", "authors": ["1168"]}
{"title": "Conceptual modelling for database user interfaces\n", "abstract": " Model-based user interface development environments show promise for improving the speed of production and quality of user interfaces. Such systems usually have separate description of domain, task and presentation structure. The Teallach system applies model based techniques to the important area of database interfaces, which increases the importance of domain information. This exists in the form of a schema and can be captured in a high-level format, so that the developer need not build a domain description arbitrarily. This paper describes such a Domain Model, how it is captured and how it contributes to the systematic development of a user interface.", "num_citations": "6\n", "authors": ["1168"]}
{"title": "Afrinesians of the Americas: A New Concept of Ethnic Identity.\n", "abstract": " Proposes new term, Afrinesians, to describe persons of African descent who reside in North America, South America, Central America, and the Caribbean and who primarily have mixed ethnic backgrounds--African plus Indian and/or Caucasian. Contends that healthy ethnic identity must encompass acceptance of one's whole ethnic heritage and eventual acceptance of or tolerance for other ethnicities and cultures.(NB)", "num_citations": "6\n", "authors": ["1168"]}
{"title": "The design and implementation of a multimedia information system with automatic content retrieval\n", "abstract": " Traditional databases use simple data types chiefly numbers and strings\u2014to represent information such as payroll records or scientific data. Queries commonly take the form of relational calculus or relational algebra expressions and the results require little more than a textual terminal for presentation. The natural successor to a traditional database is the Multimedia Information System which also uses richer data types\u2014images, text, sound and so on\u2014to describe some application domain. This calls for more advanced technologies such as graphical workstations for data creation and presentation, high-bandwidth networks for fast access to distributed data and large repositories for the storage of objects. These technologies are now widely available.", "num_citations": "6\n", "authors": ["1168"]}
{"title": "Advances in the processing and management of multimedia information\n", "abstract": " Sauf mention contraire ci-dessus, le contenu de cette notice bibliographique peut \u00eatre utilis\u00e9 dans le cadre d\u2019une licence CC BY 4.0 Inist-CNRS/Unless otherwise stated above, the content of this bibliographic record may be used under a CC BY 4.0 licence by Inist-CNRS/A menos que se haya se\u00f1alado antes, el contenido de este registro bibliogr\u00e1fico puede ser utilizado al amparo de una licencia CC BY 4.0 Inist-CNRS", "num_citations": "6\n", "authors": ["1168"]}
{"title": "TeSS: a platform for discovering life-science training opportunities\n", "abstract": " Summary           Dispersed across the Internet is an abundance of disparate, disconnected training information, making it hard for researchers to find training opportunities that are relevant to them. To address this issue, we have developed a new platform\u2014TeSS\u2014which aggregates geographically distributed information and presents it in a central, feature-rich portal. Data are gathered automatically from content providers via bespoke scripts. These resources are cross-linked with related data and tools registries, and made available via a search interface, a data API and through widgets.                             Availability and implementation                        https://tess.elixir-europe.org.", "num_citations": "5\n", "authors": ["1168"]}
{"title": "Alan Turing Intitute Symposium on Reproducibility for Data-Intensive Research\u2013Final Report\n", "abstract": " 1.1 ObjectivesThe Alan Turing Institute Symposium on Reproducibility for Data-Intensive Research was held on 6th-7th April 2016 at the University of Oxford. It was organised by senior academics, publishers and library professionals representing the Alan Turing Institute (ATI) joint venture partners (the universities of Cambridge, Edinburgh, Oxford, UCL and Warwick), the University of Manchester, Newcastle University and the British Library. The key aim of the symposium was to address the challenges around reproducibility of data-intensive research in science, social science and the humanities. This report presents an overview of the discussions and makes some recommendations for the ATI to take forwards.As the UK\u2019s leading data science institute, the ATI has key role in supporting and promoting the reproducibility of dataintensive research, from three perspectives. Firstly, reproducibility is a data science research area in its own right, requiring the development of novel analytical techniques, computational methods, technical architectures and other foundational research. Secondly, reproducibility is a technical-socio-cultural issue, requiring the implementation of new workflows, research working practices and policies in an increasingly open and transparent environment, enabled through supporting infrastructure. One area of focus of the workshop was the ATI\u2019s own outputs, which will include algorithms, computations, data and code, and the techniques that the ATI should use to ensure these are reproducible, cite-able and re-useable. Thirdly, the ATI has an important role to play as an advocate for reproducibility. A major goal of the\u00a0\u2026", "num_citations": "5\n", "authors": ["1168"]}
{"title": "An open and transparent process to select ELIXIR Node Services as implemented by ELIXIR-UK\n", "abstract": " ELIXIR is the European infrastructure established specifically for the sharing and sustainability of life science data. To provide up-to-date resources and services, ELIXIR needs to undergo a continuous process of refreshing the services provided by its national Nodes. Here we present the approach taken by ELIXIR-UK to address the advice by the ELIXIR Scientific Advisory Board that Nodes need to develop \u201cmechanisms to ensure that each Node continues to be representative of the Bioinformatics efforts within the country\u201d. ELIXIR-UK put in place an open and transparent process to identify potential ELIXIR resources within the UK during late 2015 and early to mid-2016. Areas of strategic strength were identified and Expressions of Interest in these priority areas were requested from the UK community. Criteria were established, in discussion with the ELIXIR Hub, and prospective ELIXIR-UK resources were\u00a0\u2026", "num_citations": "5\n", "authors": ["1168"]}
{"title": "Natural language search interfaces: Health data needs single-field variable search\n", "abstract": " Background                     Data discovery, particularly the discovery of key variables and their inter-relationships, is key to secondary data analysis, and in-turn, the evolving field of data science. Interface designers have presumed that their users are domain experts, and so they have provided complex interfaces to support these \u201cexperts.\u201d Such interfaces hark back to a time when searches needed to be accurate first time as there was a high computational cost associated with each search. Our work is part of a governmental research initiative between the medical and social research funding bodies to improve the use of social data in medical research.                                                       Objective                     The cross-disciplinary nature of data science can make no assumptions regarding the domain expertise of a particular scientist, whose interests may intersect multiple domains. Here we consider the common requirement for scientists to seek archived data for secondary analysis. This has more in common with search needs of the \u201cGoogle generation\u201d than with their single-domain, single-tool forebears. Our study compares a Google-like interface with traditional ways of searching for noncomplex health data in a data archive.                                                       Methods                     Two user interfaces are evaluated for the same set of tasks in extracting data from surveys stored in the UK Data Archive (UKDA). One interface, Web search, is \u201cGoogle-like,\u201d enabling users to browse, search for, and view metadata about study variables, whereas the other, traditional search, has standard multioption user interface\u00a0\u2026", "num_citations": "5\n", "authors": ["1168"]}
{"title": "Crowdsourcing and the semantic web (dagstuhl seminar 14282)\n", "abstract": " Semantic technologies provide flexible and scalable solutions to master and make sense of an increasingly vast and complex data landscape. However, while this potential has been acknowledged for various application scenarios and domains, and a number of success stories exist, it is equally clear that the development and deployment of semantic technologies will always remain reliant of human input and intervention. This is due to the very nature of some of the tasks associated with the semantic data management life cycle, which are famous for their knowledge-intensive and/or context-specific character; examples range from conceptual modeling in almost any flavor, to labeling resources (in different languages), describing their content in terms of ontological terms, or recognizing similar concepts and entities. For this reason, the Semantic Web community has always looked into applying the latest theories, methods and tools from CSCW (Computer Supported Cooperative Work), participatory design, Web 2.0, social computing, and, more recently crowdsourcing to find ways to engage with users and encourage their involvement in the execution of technical tasks. Existing approaches include the usage of wikis as semantic content authoring environments, leveraging folksonomies to create formal ontologies, but also human computation approaches such as games with a purpose or micro-tasks. This document provides a summary of the Dagstuhl Seminar 14282: Crowdsourcing and the Semantic Web, which in July 2014 brought together researchers of the emerging scientific community at the intersection of crowdsourcing and Semantic Web\u00a0\u2026", "num_citations": "5\n", "authors": ["1168"]}
{"title": "On assisting scientific data curation in collection-based dataflows using labels\n", "abstract": " Thanks to the proliferation of computational techniques and the availability of datasets, data-intensive research has become commonplace in science. Sharing and re-use of datasets is key to scientific progress. A critical requirement for enabling data re-use, is for data to be accompanied by lineage metadata that describes the context in which data is produced, the source datasets from which it was derived and the tooling or settings involved in its generation. By and large, this metadata is provided through a manual curation process, which is tedious, repetitive and time consuming.", "num_citations": "5\n", "authors": ["1168"]}
{"title": "Including Co-referent URIs in a SPARQL Query.\n", "abstract": " Linked data relies on instance level links between potentially differing representations of concepts in multiple datasets. However, in large complex domains, such as pharmacology, the inter-relationship of data instances needs to consider the context (eg task, role) of the user and the assumptions they want to apply to the data. Such context is not taken into account in most linked data integration procedures. In this paper we argue that dataset links should be stored in a stand-off fashion, thus enabling different assumptions to be applied to the data links during query execution. We present the infrastructure developed for the Open PHACTS Discovery Platform to enable this and show through evaluation that the incurred performance cost is below the threshold of user perception.", "num_citations": "5\n", "authors": ["1168"]}
{"title": "Taverna mobile: Taverna workflows on android\n", "abstract": " Researchers are often on the move, say at conferences or projects meetings, and as workflows are becoming ubiquitous in the scientific process, having access to scientific workflows from a mobile device would be a significant advantage. We therefore have developed Taverna Mobile, an application for Android phones which allows browsing of existing workflows, executing them, and reviewing the results. Taverna Mobile does not aim to reproduce the full experience of building workflows in the Taverna Workbench, rather it focuses on tasks we have deemed relevant to a scientist that is not at her desk. For instance, when visiting a conference she might hear about someone's workflow, which she can quickly locate and mark for later exploration. When in the biology lab, faced with updated scientific data, the scientist can rerun her own workflow with new inputs. While commuting, she can monitor the status of a long-running job.", "num_citations": "5\n", "authors": ["1168"]}
{"title": "Ontology building as a social-technical process: a case study\n", "abstract": " The NeuroPsyGrid Project aims to build an ontology of psychosis that will be usable by practitioners and scientists across a range of different disciplines as a common and formalised knowledge framework. This paper provides a socio-technical perspective on the collaborative work involved in the development of such an ontology of psychosis. After introducing the context and nature of the project, we will elaborate on the challenges of such an undertaking, focussing on the process of ontology building and the socio-technical aspects of collaboration, including the meaningfulness of documents as boundary objects.", "num_citations": "5\n", "authors": ["1168"]}
{"title": "Description Logics: OWL and DAML+ OIL\n", "abstract": " In this chapter we introduce Description Logics. These logics have achieved mainstream credibility as ontology languages by forming the basis of the W3C Web Ontology Language OWL, and its predecessor, DAML+ OIL. From a case study, we explain how the rich expressivity of OWL can be used to model the complexities of biology and bioinformatics. We discuss automated reasoning technologies and the roles that they can play in supporting the process of building ontologies.OWL and its predecessor, DAML+ OIL, are ontology languages developed for the Semantic Web. As such, they support its aim of increasing the amount of information on the web that is computationally accessible (ie, that can be unambiguously interpreted and processed by software as well as humans). With the acceptance of OWL as a recommendation by the W3C (World Wide Web Consortium, the standards body for web technologies), this language is moving from research into mainstream technology with increasing use and availability of tools such as the editors Prot\u00e9g\u00e9 (see g408410) and OilEd. Underlying a fragment of OWL called OWL-DL is a Description Logic (DL) which supports the definition and description of concepts, relationships, individuals and axioms (constraints) and the organisation of concepts and relationships into hierarchies.", "num_citations": "5\n", "authors": ["1168"]}
{"title": "The Semantic Web: Service discovery and provenance in my-Grid\n", "abstract": " The vision of the Semantic Web (SW) is one \u201cin which information is given well-defined meaning, better enabling computers and people to work in cooperation.\u201d(Berners-Lee et al., 2001). It emphasises the decentralised and autonomous nature of the data over which it operates, as well as the complexity of this data. At first sight this seems to fit extremely well with the requirements of the life sciences. The nature of the area makes extreme complexity the rule rather than the exception. Moreover, the history of the subject has ensured that most resources are decentralised and autonomous. On the face of it then, SW technologies offer a good technological solution for some of the difficulties of the Life Sciences, while the Life Sciences offer a perfect use-case for Semantic Web. Here we discuss two applications that have used semantic web technology and discuss the strengths and weaknesses of this technology, as well as its implications.", "num_citations": "5\n", "authors": ["1168"]}
{"title": "Applying the Semantic Web to Manage Knowledge on the Grid\n", "abstract": " Geodise [2] uses a toolbox of Grid enabled Matlab functions as building blocks on which higher-level problem solving workflows can be built. The aim is to help domain engineers utilize the Grid and engineering design search packages to yield optimized designs more efficiently. In order to capture the knowledge needed to describe the functions & workflows so that they may be best reused by other less experienced engineers we have developed a layered semantic infrastructure. A generic knowledge development and management environment (OntoView) that is used to develop an ontology encapsulating the semantics of the functions and workflows, and that underpins the domain specific components. These include: an annotation mechanism used to associate concepts with functions (Function Annotator); a semantic retrieval mechanism and GUI that allows engineers to locate suitable functions based on a list of ontology-driven searching criteria; and a GUI-based function advisor that uses the functions\u2019 semantic information in order to help function configuration and recommend semantically compatible candidates for function assembly and workflow composition (Domain Script Editor and Workflow Construction Advisor). This paper describes this infrastructure, which we plan to extend to include the semantic reuse of workflows as well as functions.", "num_citations": "5\n", "authors": ["1168"]}
{"title": "PRECIS: an automated pipeline for producing concise reports about proteins\n", "abstract": " There have been several attempts at addressing the problem of annotating sequence data computationally. Annotation generation can be considered a pipeline of processes: first harvesting data from a variety of data sources, then distilling and transforming it into a form more appropriate for the end database. This task is usually performed by human annotators, a solution that is clearly not scaleable. There have been several attempts to mimic some of these pipelines in software. However, these have generally focused on low level annotation, such as database cross-references, or by harvesting data from computational techniques such as gene finding or similarity searches. Higher level annotation such as that seen in the PRINTS database is usually formed from data that is free text, or only partly structured. This presents a much greater computational challenge. Therefore we studied the pipeline that is used to\u00a0\u2026", "num_citations": "5\n", "authors": ["1168"]}
{"title": "Toward more intelligent annotation tools: a prototype\n", "abstract": " Several research teams have addressed the problem of annotating sequence data computationally, but no genetic tools have emerged to help gather information on a given sequence or set of sequences. The authors present Precis, a prototype tool that automatically creates protein reports from concise information.", "num_citations": "5\n", "authors": ["1168"]}
{"title": "Supporting public browsing of an art gallery collections database\n", "abstract": " Increased public awareness and usage of the Web suggests that a commensurate Web presence is required from providers of cultural heritage information such as Galleries and Museums. While galleries have traditionally supplied goal-driven search facilities to specialists, they must now provide browsing and query facilities to casual users, with less precise information seeking requirements. Hypertext systems provide an appropriate technology to support the networks of associations required in order to provide path-based browsing. Requirements are twofold: browsing support for the users; and authoring support in the creation of pathways. In our prototype, we combine techniques from Hypertext and Information Retrieval to provide access to artifacts drawn from the costume collection of the Manchester City Art Gallery. We provide similarity based browsing, using terms from the artifacts\u2019 metadata to\u00a0\u2026", "num_citations": "5\n", "authors": ["1168"]}
{"title": "Task modelling for database interface development\n", "abstract": " Task Modelling for Database Interface Development | Proceedings of HCI International (the 8th International Conference on Human-Computer Interaction) on Human-Computer Interaction: Ergonomics and User Interfaces-Volume I - Volume I ACM Digital Library home ACM home Google, Inc. (search) Advanced Search Browse About Sign in Register Advanced Search Journals Magazines Proceedings Books SIGs Conferences People More Search ACM Digital Library SearchSearch Advanced Search Browse Browse Digital Library Collections More HomeBrowse by TitleProceedingsProceedings of HCI International (the 8th International Conference on Human-Computer Interaction) on Human-Computer Interaction: Ergonomics and User Interfaces-Volume I - Volume ITask Modelling for Database Interface Development Article Task Modelling for Database Interface Development Share on Authors: Tony Griffiths View \u2026", "num_citations": "5\n", "authors": ["1168"]}
{"title": "What Should We Mean by \u2018An Electronic Medical Record\u2019?\n", "abstract": " Development of the PEN&PAD prototype patient care workstation[1] has made us acutely aware of the need to re-examine and analyse the basic requirements of the medical record. We present the work emerging from this analysis which we believe applies to any \u2018electronic medical record\u2019, and argue that the principal purpose of the medical record is to support direct patient care[2]. This is a fundamentally different position to many existing medical record systems whose designs derive, explicitly or implicitly, from the need to use aggregated data. Furthermore such a view has important implications for the standardisation of the electronic medical record. The goal is to create an architecture for the medical record which is faithful to the process of patient care and useful to and usable by clinicians.", "num_citations": "5\n", "authors": ["1168"]}
{"title": "FAIR principles for research software (FAIR4RS principles)\n", "abstract": " Research software is a fundamental and vital part of research worldwide, yet there remain significant challenges to software productivity, quality, reproducibility, and sustainability. Improving the practice of scholarship is a common goal of the open science, open source software and FAIR (Findable, Accessible, Interoperable and Reusable) communities, but improving the sharing of research software has not yet been a strong focus of the latter. To improve the FAIRness of research software, the FAIR for Research Software (FAIR4RS) Working Group has sought to understand how to apply the FAIR Guiding Principles for scientific data management and stewardship to research software, bringing together existing and new community efforts. Many of the FAIR Guiding Principles can be directly applied to research software by treating software and data as similar digital research objects. However, specific characteristics of software\u2014such as its executability, composite nature, and continuous evolution and versioning\u2014make it necessary to revise and extend the principles. This document presents the first version of the FAIR Principles for Research Software (FAIR4RS Principles). It is an outcome of the FAIR for Research Software Working Group (FAIR4RS WG). The FAIR for Research Software Working Group is jointly convened as an RDA Working Group, FORCE11 Working Group, and Research Software Alliance (ReSA) Task Force.", "num_citations": "4\n", "authors": ["1168"]}
{"title": "Framing the community data system interface\n", "abstract": " Researchers in public funded science consortia agree that making their data accessible with the community is their obligation. Those mandated to use Community Data Systems (CDSs) prefer to share data with their collaborators and funders rather than make it open access. Their rationale to choose against open sharing includes the lack of incentives and lapses of memory. Features that address these two aspects are not included in current CDS implementations. We speculate that an interface framed as a device to secure data citations would positively influence researchers choices. We are performing a series of on-line experiments with subjects from the Life Sciences using the SEEK4Science platform as test-bed. One possible implication of our results is that Libertarian paternalism could be included in the Community Data Systems' design toolkit as a viable alternative to the current practices.", "num_citations": "4\n", "authors": ["1168"]}
{"title": "Obesity e-Lab: connecting social science via research objects\n", "abstract": " Despite a progressive approach to open access datasets, Social Science does not routinely capture and re-use its research processes. This is a barrier to inter-disciplinary research. The public health problem of obesity, with its interwoven social, behavioural and biomedical factors, illustrates the need for more sharable research processes facilitating insights across disciplines. Within this broad need we have identified the central requirement to support secondary research from large surveys such as the Health Surveys for England\u2013a requirement that generalises to other social research topics. We present the e-Laboratory (e-Lab) architecture, for bringing together datasets, investigators and methods around specific questions and packaging the research process into a sharable entity\u2013the Research Object (RO). The Obesity e-Lab project is using obesity research questions and communities to generate a variety of ROs supporting, for example, information mapping between different survey years, transformation of child body mass index measures into research-ready forms, and geo-visualisation of obesity measurements and models. Our collaborators are building e-Labs in other disciplines including biology, health sciences and chemistry. By participating in a programme of building different but interoperable e-Labs, Social Science could stimulate and sustain new research with other disciplines\u2013exporting, importing and coproducing ROs.", "num_citations": "4\n", "authors": ["1168"]}
{"title": "myExperiment: A web 2.0 virtual research environment for research using computation and services\n", "abstract": " ABSTRACT myExperiment is a social web site for the born-digital items arising in contemporary research practice, in particular scientific workflows and Research Objects. myExperiment can be seen from many perspectives: as a Virtual Research Environment which majors on social sharing, as \u201cFacebook for scientists\u201d but without the implicit openness which is actually a deterrent to scientists, as a second generation digital library which combines a repository with a place for conducting in silico research, or as the foundation of the future e-Laboratory.", "num_citations": "4\n", "authors": ["1168"]}
{"title": "Issues for the sharing and re-use of scientific workflows\n", "abstract": " In this paper, we outline preliminary findings from an ongoing study we have been conducting over the past 18 months of researchers\u2019 use of myExperiment, a Web 2.0-based repository with a focus on social networking around shared research artefacts such as workflows. We present evidence of myExperiment users\u2019 workflow sharing and re-use practices, motivations, concerns and potential barriers. The paper concludes with. a discussion of the implications of these our findings for community formation, diffusion of innovations, emerging drivers and incentives for research practice, and IT systems design.", "num_citations": "4\n", "authors": ["1168"]}
{"title": "SysMO-DB: A pragmatic approach to sharing information amongst Systems Biology projects in Europe\n", "abstract": " Systems Biology of Microorganisms (SysMO) is a European trans-national research initiative to record and describe the dynamic molecular processes occurring in microorganisms in a comprehensive way and to present these processes in the form of computerized mathematical models. Its 11 projects have each independently established their own data and model management solutions. SysMO-DB is an effort to retrospectively and sensitively establish a platform for the exchange of results between SysMO projects as quickly and as lightweight as possible, and in a sociopolitical setting where the projects were reluctant to share and under-resourced. With a very small team we delivered a first release of the SysMO-SEEK web-based exchange forum within 12 months and have defined a Just Enough Results Model (JERM) for the metadata of each data type compliant with standards but realistic. Success of SysMO-DB is due in large part to the establishment of the PALS\u2013a cross-project team of young scientists who set requirements, design the SEEK, define the JERM and establish a forum for discussion, communication and trust-building.", "num_citations": "4\n", "authors": ["1168"]}
{"title": "Scientific workflows\n", "abstract": " From molecular biology and chemistry to astronomy, earth sciences and particle physics, modern experimental science increasingly relies on the acquisition, manipulation, and processing of large amounts of data, and the systematic orchestration of computationally intensive simulations and analyses. Much of the analysis is expensive, in terms of the data processing and storage resources required, sometimes running for weeks and needing careful monitoring. It is also laboriously repetitive as scientists explore different settings for their algorithms, data sets are continually updated from instruments, and new information prompts the whole \u201cin silico\u201d experiment to be rerun and cross-checked.Over the past six years, workflow technology has been increasingly adopted by scientists in response to the need to specify and repeatedly execute these pipelines. A scientific workflow is the description of a process, often completely automated, that specifies the co-ordinated execution of multiple tasks. The tasks are software programs, run locally or remotely and increasingly published as Web services. Thus workflows are a particular form of scripted distributed computing over service oriented architectures.", "num_citations": "4\n", "authors": ["1168"]}
{"title": "Computational Science-ICCS 2007: 7th International Conference, Beijing China, May 27-30, 2007, Proceedings, Part III\n", "abstract": " Part of a four-volume set, this book constitutes the refereed proceedings of the 7th International Conference on Computational Science, ICCS 2007, held in Beijing, China in May 2007. The papers cover a large volume of topics in computational science and related areas, from multiscale physics to wireless networks, and from graph theory to tools for program development.", "num_citations": "4\n", "authors": ["1168"]}
{"title": "Information quality evaluation for grid information services\n", "abstract": " The quality of the information provided by information services deployed in the EGEE production testbed differs from one system to another. Under the same conditions, the answers provided for the same query by different information services can be different. Developers of these services and of other services that are based on them must be aware of this fact and understand the capabilities and limitations of each information service in order to make appropriate decisions about which and how to use a specific information service. This paper proposes an evaluation framework for these information services and uses it to evaluate two deployed information services (BDII and RGMA) and one prototype that is under development (ActOn). We think that these experiments and their results can be helpful for information service developers, who can use them as a benchmark suite, and for developers of information-intensive\u00a0\u2026", "num_citations": "4\n", "authors": ["1168"]}
{"title": "Certified e-mail delivery with DSA receipts\n", "abstract": " In this paper we present two variant protocols DSA-CEMD1 and DSA-CEMD2 for certified e-mail delivery with DSA receipts. The protocols are based on a cryptographic primitive called Verifiable and Recoverable Encryption of a Signature (VRES) and are capable of achieving non-repudiation and strong fairness security properties. The novel design of the VRES primitive allows efficiency improvements in comparison with the related certified e-mail delivery protocols based on similar primitives. The protocols employ the services of an off-line and invisible trusted third party (TTP) only in case of dispute. In DSA-CEMD1 the content of the e-mail message is not revealed to the TTP during possible recovery and this is achieved at the cost of some additional cryptographic operations. In DSA-CEMD2 the confidentiality of the message is not protected from the TTP, but the protocol is slightly more efficient.", "num_citations": "4\n", "authors": ["1168"]}
{"title": "Engineering grid resources metadata for resource and knowledge sharing\n", "abstract": " Web/Grid services\u2019 metadata and semantics are becoming increasing important for service sharing and effective reuse. In this paper we present a generic framework for engineering and managing services\u2019 Semantic Metadata (SMD) with the ultimate purpose of facilitating interoperability, automation, and knowledgeable reuse of services for problem solving. The framework addresses fundamental issues, approaches, and tools for the whole lifecycle of SMD management, in other words, those of acquiring, modeling, representing, publishing, and reusing services\u2019 SMD. It adopts ontologies and the Semantic Web technologies as the enabling technologies by which services\u2019 metadata are semantically enriched and made interoperable, understandable, and accessible on the Web/Grid for both humans and machines. In particular, mechanisms are proposed to make use of service SMD for service discovery and composition. The paper also describes a service SMD management system in the context of the UK e-Science project GEODISE. A suite of tools are developed, which forms the core of the SMD management infrastructure. We demonstrate the added value of the use of SMD through the integration of SMD management with GEODISE application systems.", "num_citations": "4\n", "authors": ["1168"]}
{"title": "Ontologies and hypertext\n", "abstract": " Hypertext is a discipline which deals with the composition and arrangement of documents. Common hypertext preoccupations are the ways in which web pages can be linked together and the types of navigation pathways that provide users with convenient access to relevant information. Until now, the linking task has been left to the intelligence of the document author or the web site designer because the rationale for linking is inevitably based upon human understanding of the document contents.               Previous attempts to automate the linking process have often yielded disappointing results. Now, the ability to model a domain of discourse with an ontology holds out the promise of computationally reasoned and reasonable linking services. This chapter describes some attempts to augment hypermedia systems with ontologies in order to provide, or improve, hypertext.", "num_citations": "4\n", "authors": ["1168"]}
{"title": "Genome science performed with e-science tools\n", "abstract": " In this paper we describe the use of myGrid middleware services to create and manage the information from running in silico bioinformatics experiments in a semantically enriched Grid aware environment. Through the development and application of workflows using myGrid components in the context of mapping the complex genomic region associated with the sporadically occurring genetic disorder Williams-Beuren Syndrome, scientists have not only produced biologically interesting and valid results, but also dramatically improved their productivity by undertaking repetitive tasks significantly faster and with an increased diligence compared to previous manual undertakings. In this work we have demonstrated the utility of an e-Science approach to the management of in silico experiments.", "num_citations": "4\n", "authors": ["1168"]}
{"title": "Knowledge and the Grid\n", "abstract": " 23.1 Introduction: Knowledge on and for the GridVirtual organisations are formed to solve problems. Problem solving involves the use of knowledge for the interpretation of existing information, for prediction, to change the way that scientific research or business is done, and ultimately for the pursuit, creation and dissemination of further knowledge. Scientists use knowledge to steer instruments or experiments; businesses use knowledge to link data together in new insightful ways. The collaborative problem solving environments that exploit and generate domain knowledge need the sophisticated computational infrastructure that is the Grid [Foster01]. We can characterise this as application knowledge on the Grid, generated by using the Grid itself or acquired by other means. A Computational Grid gives users access to host of computational resources providing the illusion of an extended virtual computing fabric, a\u00a0\u2026", "num_citations": "4\n", "authors": ["1168"]}
{"title": "The Semantic Web: A killer app for AI?\n", "abstract": " The Semantic Web is a vision to move the Web from a place where information is processedb y humans to one where processing can be automated. Currently, AI seems to be making an impact on bringing the vision to reality. To add semantics to the web requires languages for representing knowledge. To infer relationships between resources or new facts requires web-scale automatedreasoning. However, there is some skepticism in the web community that AI can be made \u201cweb appropriate\u201d andw ork on a web scale. I will introduce the Semantic Web concept andgiv e a number of examples of how AI has already contributedto its development, primarily through knowledge representation languages. I will explore the reasons why the Semantic Web is a challenging environment for AI. I will suggest that this couldb e a killer app for AI, but we must recognize that the web is a vast andun tidy place, andonly a\u00a0\u2026", "num_citations": "4\n", "authors": ["1168"]}
{"title": "DAML+ OIL is not Enough\n", "abstract": " As is well recognised within the Semantic Web community, ontologies will play a crucial part in the delivery of the Semantic Web, facilitating the sharing of information between communities, both of people and software agents.In order to support this use of ontologies, a number of representational formats have been proposed, including RDF Schema (RDF (S))[RDF], the Ontology Interchange Language (OIL)[OIL] and the Darpa Agent Markup Language (DAML)[DAM]. These last two have been brought together to form DAML+ OIL, a language now being proposed as a W3C standard for ontological and metadata representation.", "num_citations": "4\n", "authors": ["1168"]}
{"title": "TAMBIS Online: a bioinformatics source integration tool\n", "abstract": " Conducting bioinformatic analyses involves biologists in expressing requests over a range of heterogeneous information sources. The TAMBIS (Transparent Access to Multiple Bioinformatics Information Sources) project seeks to make the diversity in data structures, call interfaces and locations of bioinformatics sources transparent to users. TAMBIS is available at .", "num_citations": "4\n", "authors": ["1168"]}
{"title": "Workflows Community Summit: Advancing the State-of-the-art of Scientific Workflows Management Systems Research and Development\n", "abstract": " Scientific workflows are a cornerstone of modern scientific computing, and they have underpinned some of the most significant discoveries of the last decade. Many of these workflows have high computational, storage, and/or communication demands, and thus must execute on a wide range of large-scale platforms, from large clouds to upcoming exascale HPC platforms. Workflows will play a crucial role in the data-oriented and post-Moore's computing landscape as they democratize the application of cutting-edge research techniques, computationally intensive methods, and use of new computing platforms. As workflows continue to be adopted by scientific projects and user communities, they are becoming more complex. Workflows are increasingly composed of tasks that perform computations such as short machine learning inference, multi-node simulations, long-running machine learning model training, amongst others, and thus increasingly rely on heterogeneous architectures that include CPUs but also GPUs and accelerators. The workflow management system (WMS) technology landscape is currently segmented and presents significant barriers to entry due to the hundreds of seemingly comparable, yet incompatible, systems that exist. Another fundamental problem is that there are conflicting theoretical bases and abstractions for a WMS. Systems that use the same underlying abstractions can likely be translated between, which is not the case for systems that use different abstractions. More information: https://workflowsri.org/summits/technical", "num_citations": "3\n", "authors": ["1168"]}
{"title": "A Fresh Look at FAIR for Research Software\n", "abstract": " This document captures the discussion and deliberation of the FAIR for Research Software (FAIR4RS) subgroup that took a fresh look at the applicability of the FAIR Guiding Principles for scientific data management and stewardship for research software. We discuss the vision of research software as ideally reproducible, open, usable, recognized, sustained and robust, and then review both the characteristic and practiced differences of research software and data. This vision and understanding of initial conditions serves as a backdrop for an attempt at translating and interpreting the guiding principles to more fully align with research software. We have found that many of the principles remained relatively intact as written, as long as considerable interpretation was provided. This was particularly the case for the \"Findable\" and \"Accessible\" foundational principles. We found that \"Interoperability\" and \"Reusability\" are particularly prone to a broad and sometimes opposing set of interpretations as written. We propose two new principles modeled on existing ones, and provide modified guiding text for these principles to help clarify our final interpretation. A series of gaps in translation were captured during this process, and these remain to be addressed. We finish with a consideration of where these translated principles fall short of the vision laid out in the opening.", "num_citations": "3\n", "authors": ["1168"]}
{"title": "COVID-19 and beyond: a call for action and audacious solidarity to all the citizens and nations, it is humanity\u2019s fight\n", "abstract": " Background: SARS-CoV-2 belongs to a subgroup of coronaviruses rampant in bats for centuries. It has caused the COVID-19 pandemic. Most patients recover, but a minority of severe cases experience acute respiratory distress or an inflammatory storm devastating many organs that can lead to patient death. The spread of SARS-CoV-2 has been facilitated by the increasing intensity of air travel, urban congestion and human contact during the last half century. Until therapies and vaccines are available, tests for virus and exposure, confinement measures and physical distancing have helped curb the pandemic.", "num_citations": "3\n", "authors": ["1168"]}
{"title": "Common Workflow Language Viewer:(Talk)\n", "abstract": " The Common Workflow Language (CWL) project emerged from the BOSC 2014 Codefest as a grassroots, multi-vendor working group to tackle the portability of data analysis workflows. It\u2019s specification for describing workflows and command line tools aims to make them portable and scalable across a variety of computing platforms.", "num_citations": "3\n", "authors": ["1168"]}
{"title": "Minimal information for reusable scientific software\n", "abstract": " Minimal information for reusable scientific software \u2013 Projects \u2014 University of Edinburgh Research Explorer Skip to main navigation Skip to search Skip to main content University of Edinburgh Research Explorer Logo Help & FAQ Home Research output Profiles Research Units Projects Datasets Prizes Activities Press / Media Equipment Search by expertise, name or affiliation Minimal information for reusable scientific software Neil Chue Hong School of Physics and Astronomy Edinburgh Parallel Computing Centre Computer Systems Research output: Working paper \u203a Discussion paper Overview Fingerprint Projects (1) Research output (1) Activities (1) Projects Projects per year 2010 2016 1 Finished 1 results Status, start date (descending) Title Start date End date Type Status, start date(ascending) Search results Finished UK Software Sustainability Institute Chue Hong, N., Parsons, M., De Roure, D. & Goble, C. \u2026", "num_citations": "3\n", "authors": ["1168"]}
{"title": "Influence Factor: Extending the PROV Model With a Quantitative Measure of Influence.\n", "abstract": " A central tenet of provenance is to support the assessment of the quality, reliability, or trustworthiness of data. The World Wide Web Consortium\u2019s (W3C) PROV provenance data model shares this goal, and provides a domain-agnostic interchange language for provenance representation. In this paper we suggest that given the PROV model as it stands, there are cases where information relating to how one entity has influenced another falls short of that required to make these assessments. In light of this, we propose a simple extension to the model to capture a quantitative measure of influence.", "num_citations": "3\n", "authors": ["1168"]}
{"title": "RightField: semantic enrichment of systems biology data using spreadsheets\n", "abstract": " The interpretation and integration of experimental data depends on consistent metadata and uniform annotation. However, there are many barriers to the acquisition of this rich semantic metadata, not least the overhead and complexity of its collection by scientists. We present RightField, a lightweight spreadsheet-based annotation tool for lowering the barrier of manual metadata acquisition; and a data integration application for extracting and querying RDF data from these enriched spreadsheets. By hiding the complexities of semantic annotation, we can improve the collection of rich metadata, at source, by scientists. We illustrate the approach with results from the SysMO program, showing that RightField supports the whole workflow of semantic data collection, submission and RDF querying in Systems Biology. The RightField tool is freely available from http://www.rightfield.org.uk, and the code is open source\u00a0\u2026", "num_citations": "3\n", "authors": ["1168"]}
{"title": "Detecting duplicate records in scientific workflow results\n", "abstract": " Scientific workflows are often data intensive. The data sets obtained by enacting scientific workflows have several applications, e.g., they can be used to identify data correlations or to understand phenomena, and therefore are worth storing in repositories for future analyzes. Our experience suggests that such datasets often contain duplicate records. Indeed, scientists tend to enact the same workflow multiple times using the same or overlapping datasets, which gives rise to duplicates in workflow results. The presence of duplicates may increase the complexity of workflow results interpretation and analyzes. Moreover, it unnecessarily increases the size of datasets within workflow results repositories. In this paper, we present an approach whereby duplicates detection is guided by workflow provenance trace. The hypothesis that we explore and exploit is that the operations that compose a workflow are likely to\u00a0\u2026", "num_citations": "3\n", "authors": ["1168"]}
{"title": "Data Provenance in Scientific Workflows\n", "abstract": " Data provenance is key to understanding and interpreting the results of scientific experiments. This chapter introduces and characterises data provenance in scientific workflows using illustrative examples taken from real-world workflows. The characterisation takes the form of a taxonomy that is used for comparing and analysing provenance capabilities supplied by existing scientific workflow systems.", "num_citations": "3\n", "authors": ["1168"]}
{"title": "The open provenance model (v1. 01)\n", "abstract": " In this paper, we introduce the Open Provenance Model, a model for provenance that is designed to meet the following requirements: (1) To allow provenance information to be exchanged between systems, by means of a compatibility layer based on a shared provenance model. (2) To allow developers to build and share tools that operate on such a provenance model. (3) To define the model in a precise, technology-agnostic manner. (4) To support a digital representation of provenance for any \"thing\", whether produced by computer systems or not. (5) To define a core set of rules that identify the valid inferences that can be made on provenance graphs.", "num_citations": "3\n", "authors": ["1168"]}
{"title": "Understanding semantic aware Grid middleware for e-Science\n", "abstract": " In this paper we analyze several semantic-aware Grid middleware services used in e-Science applications. We describe them according to a common analysis framework, so as to find their commonalities and their distinguishing features. As a result of this analysis we categorize these services into three groups: information services, data access services and decision support services. We make comparisons and provide additional conclusions that are useful to understand better how these services have been developed and deployed, and how similar services would be developed in the future, mainly in the context of e-Science applications.", "num_citations": "3\n", "authors": ["1168"]}
{"title": "Metadata management in s-ogsa\n", "abstract": " Metadata-intensive applications pose strong requirements for metadata management infrastructures, which need to deal with a large amount of distributed and dynamic metadata. Among the most relevant requirements we can cite those related to access control and authorisation, lifecycle management and notification, and distribution transparency. This paper discusses such requirements and proposes a systematic approach to deal with them in the context of S-OGSA.", "num_citations": "3\n", "authors": ["1168"]}
{"title": "Architectural patterns for the semantic grid\n", "abstract": " The Semantic Grid reference architecture, S-OGSA, includes semantic provisioning services that are able to produce semantic annotations of Grid resources, and semantically aware Grid services that are able to exploit those annotations in various ways. In this paper we describe the dynamic aspects of S-OGSA by presenting the typical patterns of interaction among these services. A use case for a Grid meta-scheduling service is used to illustrate how the patterns are applied in practice.", "num_citations": "3\n", "authors": ["1168"]}
{"title": "Report on the 2006 NSF Workshop on Challenges of Scientific Workflows\n", "abstract": " Workflows have recently emerged as a paradigm for representing and managing complex distributed scientific computations and therefore accelerate the pace of scientific progress. A recent workshop on the Challenges of Scientific Workflows, sponsored by the National Science Foundation and held on May 1-2, 2006, brought together domain scientists, computer scientists, and social scientists to discuss requirements of future scientific applications and the challenges that they present to current workflow technologies. This paper reports on the discussions and recommendations of the workshop, the full report can be found at http://www. isi. edu/nsf-workflows06.", "num_citations": "3\n", "authors": ["1168"]}
{"title": "ODESGS framework, knowledge-based markup for semantic grid services\n", "abstract": " The convergence of the Semantic Web and Grid technologies has resulted in the Semantic Grid. The Semantic Grid should be service-oriented, as the Grid is, so the formal description of Grid Services (GS) turns to be a crucial issue. In this paper we present our approach for this issue. ODESGS Framework will enable the annotation of all the aspects of a GS and the design, discovery and composition Semantic Grid Services (SGS).", "num_citations": "3\n", "authors": ["1168"]}
{"title": "Accessible Hypermedia and Multimedia\n", "abstract": " Pascal 001 Exact sciences and technology/001A Sciences and techniques of general use/001A01 Information science. Documentation/001A01G Information and communication technologies/001A01G02 Information technologies: storage media, equipment/001A01G02B Applications (eg Digitizing,...)", "num_citations": "3\n", "authors": ["1168"]}
{"title": "Workshop Report: W4A\u2014International Cross Disciplinary Workshop on Web Accessibility 2004\n", "abstract": " Layout and strncture are key to good visual design. They are the conduit for both the content and the graphics. Moreover, they are important for disabled people (eg dyslexic users) and specifically visually impaired users, as they need to be quickly and easily interacted with. The workshop aimed to address layout, structure, and presentation from the viewpoint of accessibility and good visual design; where these are in opposition, the workshop aimed to facilitate discussion between interested p~ trties so that a solution (or at least the beginnings of a solution) can be formulated-in effect we ask the question'Does Accessible Mean Dull?'. We support inclusive design; however, how can this be the case if users have differing needs? The organizers also assert that no one should be hindered when interacting with layout. Will making layout accessible hinder sighted or'conventional'users? Conventional workshops on\u00a0\u2026", "num_citations": "3\n", "authors": ["1168"]}
{"title": "myGrid: in silico experiments in bioinformatics\n", "abstract": " An in silico experiment is a procedure using combinations of computer based information repositories and computational analysis to test a hypothesis, derive a summary, search for patterns or to demonstrate a known fact. my Grid is a UK e-Science pilot project specifically targeted at developing open source high-level service-based middleware to support the construction, management and sharing of data-intensive in silico experiments in biology. The consortium is made up of five UK universities (Manchester, Southampton, Newcastle, Nottingham, Sheffield) and the EMBL-European Bioinformatics Institute.Biologists, aided by bioinformaticians, have become knowledge workers, intelligently weaving together the information available to the community, linking and correlating it meaningfully, and generating even more information. Many BioGrid projects focus on the sharing of computational resources, large scale\u00a0\u2026", "num_citations": "3\n", "authors": ["1168"]}
{"title": "A new journal for a new era of the World Wide Web\n", "abstract": " A new journal for a new era of the World Wide Web \u00d7 Close The Infona portal uses cookies, ie strings of text saved by a browser on the user's device. The portal can access those files and use them to remember the user's data, such as their chosen settings (screen view, interface language, etc.), or their login data. By using the Infona portal the user accepts automatic saving and using this information for portal operation purposes. More information on the subject can be found in the Privacy Policy and Terms of Service. By closing this window the user confirms that they have read the information on cookie usage, and they accept the privacy policy and the way cookies are used by the portal. You can change the cookie settings in your browser. I accept Polski English Login or register account remember me Password recovery INFONA - science communication portal INFONA Search advanced search Browse series books \u2026", "num_citations": "3\n", "authors": ["1168"]}
{"title": "Special Section on Semantic Web and Data Management\n", "abstract": " SIGMOD Record - web edition / December 2002 ACM SIGMOD ONLINE ACM SIGMOD Online ACM Search SIGMOD Join SIGMOD Feedback What's New Home Home About SIGMOD SIGMOD/PODS Conferences SIGMOD Record DBLP Bibliography SIGMOD Digital Symposium Collection SIGMOD Anthology SIGMOD Digital Review Industry Pages The PODS Pages Post/Read DB World Messages Literature Resources Calendar Volume 31 Number 4 December 2002 SIGMOD Record Current Issue XML Edition Previous Issue About SIGMOD Info for Authors FAQ Record Editors Credits Special Section on Semantic Web and Data Management | Book Reviews | Distinguished Database Profiles Database Principles | Influential Papers | Reports | Standards | Research Centers SIGMOD Officers, Committees and Awards (also in: PDF) Editor's Notes (in: Postscript and PDF) Chair's Message (in: PDF) Rights of TODS \u2026", "num_citations": "3\n", "authors": ["1168"]}
{"title": "Grid services in action: Grid enabled optimisation and design search\n", "abstract": " We are developing a Grid Enabled Optimisation and Design Search system (GEODISE). It offers grid-based access to a state-of-the-art collection of optimisation and design search tools, industrial strength analysis codes, and distributed computing and data resources.", "num_citations": "3\n", "authors": ["1168"]}
{"title": "Prototype mobility tools for visually impaired surfers\n", "abstract": " In [1] we extended the notion of travel to include environment, feedback and the purpose of the current travel task. Specifically, we likened web use to travelling in a virtual space, compared it to travelling in a physical space, and introduced the idea of mobility-the ease of travel-as opposed to travel opportunity. This paper describes our continuing work in building a prototype mobility tool to address some of these issues.", "num_citations": "3\n", "authors": ["1168"]}
{"title": "Managing Biological Information Using Biological Knowledge\n", "abstract": " The Transparent Access to Multiple Bioinformatics Information Sources (TAMBIS) system uses an ontology to give the illusion of a common query interface to multiple, diverse, heterogeneous bioinformatics resources. The knowledge in the TAMBIS ontology (TaO) allows biologists to form complex, multi-source queries without having to know which source to use, the location of the source, the meaning of terms within the source and how to transfer information between resources. The TaO can be seen to add a semantic layer over these bioinformatics resources. Ontologies are finding a wider use in the bioinformatics arena, emphasising the usefulness of knowledge in bioinformatics. Just as the vision for the Semantic Web envisages the use of knowledge to give a machine processable web, so semantic bioinformatics resources will enable machine processable, rather than only machine readable, bioinformatics resources.", "num_citations": "3\n", "authors": ["1168"]}
{"title": "Ontology Language Version\n", "abstract": " Ontologies are a popular research topic in various communities such as knowledge engineering, natural language processing, cooperative information systems, intelligent information integration, knowledge management. They provide a shared and common understanding of a domain that can be communicated across people and application systems. They have been developed in Artificial Intelligence to facilitate knowledge sharing and reuse. Recent articles covering various aspects of ontologies can be found in [Uschold & Gr\u00fcninger, 1996],[van Heijst et al., 1997],[Studer et al., 1998],[Benjamins et al., 1999 (a)],[Gomez Perez & Benjamins, 1999],[Fensel, 2000]. An ontology provides an explicit conceptualization (ie, meta information) that describe the semantics of the data. They have a similar function as a database schema. The differences are1:", "num_citations": "3\n", "authors": ["1168"]}
{"title": "Putting the tourist into Tourist Information\n", "abstract": " This paper introduces TourisT, a prototype hypermedia tourism information system, which was developed at the University of Manchester. A key theme of the TourisT project was that the system must address the real needs of tourists, and this was investigated using the following approach. The project used ethnographic studies [1, 2] to determine how tourists ask for information, and how tourist information advisers respond; the findings were used to inspire the design of the prototype. The prototype uses a conceptual hypermedia architecture [3, 4, 5], which is able to capture the diverse interaction styles suggested by the studies.", "num_citations": "3\n", "authors": ["1168"]}
{"title": "A comparison of morpheme and word based document retrieval for Asian languages\n", "abstract": " Most document retrieval systems are word based. Words are very convenient retrieval units in English but not so in some Asian languages. The task of determining which morphemes constitute words in Vietnamese and Chinese is problematic, and has been assumed to be the reason that word based retrieval does not work so well. The paper examines a number of segmentation algorithms, and then reports on some experiments comparing morpheme and word based retrieval. It shows that morpheme based retrieval is hard to improve on.", "num_citations": "3\n", "authors": ["1168"]}
{"title": "A Consulting Room System with Added Value\n", "abstract": " The PEN&PAD computer based projects are concerned with the entry and presentation of clinical data during the brief consultation between the patient and the doctor. The doctor?s requirement to manage and to effectively process the volume and complexity of patient?centred data is one which can easily detract from the primary purpose of the consultation, i.e. providing time and care for the patient.", "num_citations": "3\n", "authors": ["1168"]}
{"title": "A national initiative in data science for health: an evaluation of the UK Farr Institute\n", "abstract": " ObjectiveTo evaluate the extent to which the inter-institutional, inter-disciplinary mobilisation of data and skills in the Farr Institute contributed to establishing the emerging field of data science for health in the UK.", "num_citations": "2\n", "authors": ["1168"]}
{"title": "Data management in computational systems biology: exploring standards, tools, databases, and packaging best practices\n", "abstract": " Computational systems biology involves integrating heterogeneous datasets in order to generate models. These models can assist with understanding and prediction of biological phenomena. Generating datasets and integrating them into models involves a wide range of scientific expertise. As a result these datasets are often collected by one set of researchers, and exchanged with others researchers for constructing the models. For this process to run smoothly the data and models must be FAIR\u2014findable, accessible, interoperable, and reusable. In order for data and models to be FAIR they must be structured in consistent and predictable ways, and described sufficiently for other researchers to understand them. Furthermore, these data and models must be shared with other researchers, with appropriately controlled sharing permissions, before and after publication. In this chapter we explore the different\u00a0\u2026", "num_citations": "2\n", "authors": ["1168"]}
{"title": "Automatic vs manual provenance abstractions: mind the gap\n", "abstract": " In recent years the need to simplify or to hide sensitive information in provenance has given way to research on provenance abstraction. In the context of scientific workflows, existing research provides techniques to semi-automatically create abstractions of a given workflow description, which is in turn used as filters over the workflow\u2019s provenance traces. An alternative approach that is commonly adopted by scientists is to build workflows with abstractions embedded into the workflow\u2019s design, such as using subworkflows. This paper reports on the comparison of manual versus semi-automated approaches in a context where result abstractions are used to filter report-worthy results of computational scientific analyses. Specifically; we take a real-world workflow containing user-created design abstractions and compare these with abstractions created by ZOOM* UserViews andWorkflow Summaries systems. Our comparison shows that semi-automatic and manual approaches largely overlap from a process perspective, meanwhile, there is a dramatic mismatch in terms of data artefacts retained in an abstracted account of derivation. We discuss reasons and suggest future research directions.", "num_citations": "2\n", "authors": ["1168"]}
{"title": "Tracking workflow execution with TavernaProv\n", "abstract": " Apache Taverna is a scientific workflow system for combining web services and local tools. Taverna records provenance of workflow runs, intermediate values and user interactions, both as an aid for debugging while designing the workflow, but also as a record for later reproducibility and comparison.", "num_citations": "2\n", "authors": ["1168"]}
{"title": "FAIRDOM approach for semantic interoperability of systems biology data and models.\n", "abstract": " Motivation: The ability to collect and interlink heterogeneous data and model collections is essential in systems biology. Effective data exchange and comparison requires sufficient data annotation. This is particularly apparent in systems biology, where data heterogeneity means that multiple community metadata standards are required for the annotation of a whole investigation, including data, models and protocols. Results: FAIRDOM (http://fair-dom. org/) is an initiative to enable the systems biology community to produce and publish FAIR Data, Operating procedures and Models. It allows research assets to be aggregated, interlinked and shared in the context of the systems biology investigations that produced them. Here we present the FAIRDOM strategy in the context of semantic data integration, and how it supports the whole life cycle of data collection, annotation, sharing and reuse of systems biology data and resources.Availability: https://fairdomhub. org* Contact: olga. krebs@ h-its. org", "num_citations": "2\n", "authors": ["1168"]}
{"title": "Discoveries and Anti-Discoveries on the Web of Argument and Data\n", "abstract": " Too many scientific arguments in peer-reviewed articles are backed by flawed or inadequate data. Articles can now be annotated in pre-or post-publication review using models such as W3C Open Annotation, to produce multipolar argumentation networks as an overlay on Web documents. This would assist in improving scientific reproducibility by integrating discussion and analysis of doubtful results.", "num_citations": "2\n", "authors": ["1168"]}
{"title": "The Pharmacology Workspace: A Platform for Drug Discovery.\n", "abstract": " We present the Open PHACTS linked data platform that is being developed to address a set of example drug discovery research questions and which supports several drug discovery applications. The platform retrieves data from many complementary, but overlapping, data sources to present an integrated view of the data. The platform exploits two entity resolution services: respectively for transforming text and chemical structures to a concept. The single concept URI provided by the resolution service is then expanded to a set of equivalent URIs used by the data sources. Availability. An alpha version is currently available to the Open PHACTS consortium. A first public release of the platform will be made in late 2012, see http://www. openphacts. org/.", "num_citations": "2\n", "authors": ["1168"]}
{"title": "Rightfield: embedding ontology term selection into spreadsheets for the annotation of biological data\n", "abstract": " RightField is an open source application that provides a mechanism for embedding ontology annotation support for Life Science data in Microsoft Excel spreadsheets. Individual cells, columns, or rows can be restricted to particular ranges of allowed classes or instances from chosen ontologies. Informaticians, with experience in ontologies and data annotation prepare RightField-enabled spreadsheets with embedded ontology term selection for use by a wider community of laboratory scientists. The RightField-enabled spreadsheet presents selected ontology terms to the users as a simple drop-down list, enabling scientists to consistently annotate their data without the need to understand the numerous metadata standards and ontologies available to them. The spreadsheets are self-contained and remain \u201cvanilla\u201d Excel so that they can be readily exchanged, processed offline and are usable by regular Excel tooling. The result is semantic annotation by stealth, with an annotation process that is less error-prone, more efficient, and more consistent with community standards. RightField has been developed and deployed for a consortium of some 300 Systems Biologists. RightField is open source under a BSD license and freely available from http://www. sysmo-db. org/RightField.", "num_citations": "2\n", "authors": ["1168"]}
{"title": "Community-driven computational biology with debian and taverna\n", "abstract": " Computational biology manifests itself in many flavours. It comprises the data analysis and-management of sequences, structures, the observed and synthetical variants of the prior, static or dynamic interactions, and serves the modelling of biological processes in physiological and pathophysiological conditions. The field gained an enormous momentum over the past two decades. The information gathered today covers biological properties of many organisms and serves as a reference and general source for derived work also for neighbouring disciplines. Biologist, physicians and chemists all started using bioinformatics tools, data and models in their routine. The latest trend is to integrate the thinking of engineers and physicis, who construct compounds in silico to later prove the predicted function in the lab. The approach became known as synthetic biology and is perceived by many to allow a fluent transition towards nanotechnologies. With research questions becoming increasingly complex, they demand the interaction of highly specialised disciplines. This leads to a steady increase in the number of non-redundant tools and databases that researchers need to interact with-both the computational developer and the biological users.The dependency of the biological research community on such services will increase over the upcoming years. The strong computational demands of the services, and the sheer complexity of the research fosters the collaboration of individuals from many sites, computationally in form of grid and cloud computing, but also between computationally and biologically primed groups. To maintain the software installation\u00a0\u2026", "num_citations": "2\n", "authors": ["1168"]}
{"title": "Open workflow infrastructure: a research agenda\n", "abstract": " While current Distributed Computation Platforms (DCPs) provide environments for composition and enactment of distributed applications, a comprehensive methodology and tool that facilitates open, generic and rapid development, sharing and utilization of distributed applications represented through the workflow methodology, is still missing. The goal is to incorporate workflow editing, sharing and enactment capabilities directly into the Internet, thus making distributed applications available and usable in a wide range of pervasive settings. In this position paper, we outline a research agenda to build such an infrastructure with a flexible design to work on an Internet-wide scale. Research and technology development activities are intended to address the following end-user needs:(1) abstracting individual DCPs so that end-users can use common interfaces,(2) allowing rapid customization of the distributed process\u00a0\u2026", "num_citations": "2\n", "authors": ["1168"]}
{"title": "Supporting e-Science Using Semantic Web Technologies\u2013The Semantic Grid\n", "abstract": " The capabilities of the Web have had a very significant impact in facilitating new practice in science: it supports wide-scale information discovery and sharing, facilitating collaboration and enabling widespread participation in digital science, and increasingly it also provides a platform for developing and delivering software and services to support science. In this chapter we focus on the role of the ideas and technologies of the Semantic Web in providing and utilising the infrastructure for science. Our emphasis on Semantic Web and on the joined-up infrastructure to support the increasing scale of data, computation, collaboration and automation as science and computing advance has led to this field being known as the \u201cSemantic Grid\u201d. Since its instigation in 2001 the Semantic Grid community has established a significant body of work, and the approach continues to underpin new scientific practice in multiple\u00a0\u2026", "num_citations": "2\n", "authors": ["1168"]}
{"title": "Scientific Process Automation and...\n", "abstract": " We introduce and describe scientific workflows, ie, executable descriptions of automatable scientific processes such as computational science simulations and data analyses. Scientific workflows are often expressed in terms of tasks and their (dataflow) dependencies. This chapter first provides an overview of the characteristic features of scientific workflows and outlines their life cycle. A detailed case study highlights workflow challenges and solutions in simulation management. We then provide a brief overview of how some concrete systems support the various phases of the workflow life cycle, ie, design, resource management, execution, and provenance management. We conclude with a discussion on communitybased workflow sharing.", "num_citations": "2\n", "authors": ["1168"]}
{"title": "Grid 3.0: Services, semantics and society\n", "abstract": " The trend in recent years in distributed computing and distributed information systems has been to open up: to expose interfaces and content outside the bounds of the originating application, resource or middleware; to simplify access to third party resources, data and capability; and to actively encourage and support creativity through the reuse and combination of already available components and content, be they ours or others. The ubiquity of the Service Oriented Architecture (SOA) is testament to the driver, in both industry and scientific research, for more agile solutions, more rapid development, more flexibility and more opportunity for effective use of what has gone before. The rise of the web service and its adoption for Grids are examples. In the sciences the web service has become established as the delivery mechanism for publicly available data sets and tools. Designing reusable components and enabling\u00a0\u2026", "num_citations": "2\n", "authors": ["1168"]}
{"title": "The seven deadly sins of bioinformatics\n", "abstract": " Intractable problems in bioinformatics often lie in issues of data management and tool provisioning. Bioinformaticians use data and tools to perform analyses. Others build those tool or prepare that data. There are challenging problems. A good number of bioinformaticians think these issues make their discipline unique or somehow special. Sadly not. Many of these problems are of bioinformatics own making.", "num_citations": "2\n", "authors": ["1168"]}
{"title": "Taverna workflows for systems biology\n", "abstract": " Peter Li1, Giles Verlarde1, Andy Brass1, John Pinney1, Tom Oinn2, Carole A. Goble1 and Douglas B. Kell1 1University of Mancheste Page 1 Taverna workflows for systems biology Peter Li1, Giles Verlarde1, Andy Brass1, John Pinney1, Tom Oinn2, Carole A. Goble1 and Douglas B. Kell1 1University of Manchester, UK and 2European Bioinformatics Institute, UK Integration of array data onto pathway maps Construction of models in SBML Acknowledgements addR eactant W orkflow Outputs W orkflow Inputs addProduct W orkflow Outputs W orkflow Inputs W orkflow Outputs W orkflow Inputs createS peciesR eferenceR eactant S peciesR eferenceR eactants createModel addR eaction toS B ML addS pecies addCompartment sbml createS pecies setCompartment createS peciesR eferenceP roduct S peciesR eferenceP roducts createC ompartment createR eaction R eaction reaction addR eactant R eactions E cho_list \u2026", "num_citations": "2\n", "authors": ["1168"]}
{"title": "Managing semantic grid metadata in S-OGSA\n", "abstract": " Grid resources such as data, services, and equipment, are increasingly being annotated with descriptive metadata that facilitates their discovery and their use in the context of Virtual Organizations (VO). Making such growing body of metadata explicit and available to Grid services is key to the success of the VO paradigm. In this paper we present a model and management architecture for Semantic Bindings, ie, firstclass Grid entities that encapsulate metadata on the Grid and make it available through predictable access patterns. The model is at the core of the S-OGSA reference architecture for the Semantic Grid.", "num_citations": "2\n", "authors": ["1168"]}
{"title": "A Brief History of the Semantic Grid\n", "abstract": " The story of the Semantic Grid, from its originas in the UK eScience programme in 2001 through to the Dagstuhl event in 2005.", "num_citations": "2\n", "authors": ["1168"]}
{"title": "What can the Semantic Grid do for science and engineering?\n", "abstract": " Scientists and Engineers have been happily performing research and Analyses for hundreds of years without the Semantic Grid. What\u00c3\u00a2 \u00e2 \u201a\u00ac \u00e2 \u201e\u00a2 s changing in their world now that would motivate them to look to the Semantic Grid? Which of their problems can it solve? And how can we recognize the low-hanging fruit \u00c3\u00a2 \u00e2 \u201a\u00ac \u00e2\u20ac \u0153 the combinations of communities and issues where introducing the Semantic grid now will create the most scientific value? Traditional science is being done \u00c3\u00a2 \u00e2 \u201a\u00ac \u00cb\u0153faster\u00c3\u00a2 \u00e2 \u201a\u00ac \u00e2 \u201e\u00a2 and community-level discovery-based science and systems approaches are emerging. Semantic Grid technologies can provide a critical capability to reuse data, software, and services while evolving the underlying grid and science models involved. While not often mentioned by name, SG technologies \u00c3\u00a2 \u00e2 \u201a\u00ac \u00e2\u20ac \u0153 exposing and reasoning over model-level descriptions of resources within and on the Grid \u00c3\u00a2 \u00e2 \u201a\u00ac \u00e2\u20ac \u0153 are directly relevant to problems of managing large amounts of heterogeneous data in a fluid scientific and technological environment. This presentation will attempt to map between language of science and that of grids and the semantic web to identify use cases where deploying a \u00c3\u00a2 \u00e2 \u201a\u00ac \u00cb\u0153Semantic Grid\u00c3\u00a2 \u00e2 \u201a\u00ac \u00e2 \u201e\u00a2 could have significant scientific value.", "num_citations": "2\n", "authors": ["1168"]}
{"title": "Towards Mapping-Based Document Retrieval in Heterogeneous Digital Libraries\n", "abstract": " In many scientific domains, researchers depend on a timely and efficient access to available publications in their particular area. The increasing availability of publications in electronic form via digital libraries is a reaction to this need. A remaining problem is the fact that the pool of all available publications is distributed between different libraries. In order to increase the availability of information, these different libraries should be linked in such a way, that all the information is available via any one of them. Peer-to-peer technologies provide sophisticated solutions for this kind of loose integration of information sources. In our work, we consider digital libraries that organize documents according to a dedicated classification hierarchy or provide access to information on the basis of a thesaurus. These kinds of access mechanisms have proven to increase the retrieval result and are therefore widely used. On the other hand, this causes new problems as different sources will use different classifications and thesauri to organize information. This means, that we have to be able to mediate between these different structures. Integrating this mediation into the information retrieval process is a problem that to the best of our knowledge has not been addressed before.", "num_citations": "2\n", "authors": ["1168"]}
{"title": "proximity: Ad-hoc networks for enhanced mobility\n", "abstract": " David tries not to use unfamiliar trains and buses, he doesn\u2019t travel to places he doesn\u2019t know, and he doesn\u2019t travel in unusual environments without a companion. David is visually impaired and in such cases he becomes disoriented from a lack of preview, knowledge of the environment, and orientation information and is consequently without the foundations on which to base mobility decisions. While his experiences are not always true for all visually impaired travellers they do represent the majority experience of David\u2019s peer user group. proXimity is our attempt to address David\u2019s travel problems and is based on our previous work in Hypermedia and Real-World Mobility. By using combined Hypertext and mobility paradigms we move toward a system that will assist David in his travels. The primary goal of proXimity is to augment David\u2019s reality by giving hypertext a physical presence in the real world. We\u00a0\u2026", "num_citations": "2\n", "authors": ["1168"]}
{"title": "User-oriented semantic service discovery\n", "abstract": " The concept of discovery has been a focal point of attention since the emergence of networks and distributed computing. It is possible to find incarnations of the discovery concept within nearly every distributed computing paradigm. These components have a crucial role in sharing and orchestrated use of diverse types of resources in closed or open environments. Recently, the emergence and widespread use of web based protocols and service orientation has introduced possibilities for open and flexible service based architectures where services can be discovered and composed into workflows. Moreover, openness of the environment has fuelled research on providing unambigious semantic descriptions of services to cater for increased automation in service discovery and composition activities known as the Semantic Web Services research.In this thesis we focus on discovery. Initially we take a general approach and describe its role in distributed environments, the motivations behind it, and the common way it is performed. Then we describe the reflections of these as a set of discovery requirements in the bioinformatics domain, and particularly the myGrid project, where the users are chiefly in charge of selecting and composing services rather than unattended software agents. We then analyze stereotypical discovery systems, Traders, in different distributed environments explaining how each adopts an information model and a discovery mechanism to meet the particular environment\u2019s needs.", "num_citations": "2\n", "authors": ["1168"]}
{"title": "Carole Goble discusses the impact of semantic technologies on the life sciences\n", "abstract": " Carole Goble is Professor in the Department of Computer Science in the University of Manchester, from where she graduated. Her research interests are centred on the accessibility of information, primarily through the use of ontologies for the representation and classification of metadata. She works in many application areas, and in particular Life Sciences. The Information Management Group that she co-leads is renowned for its work on ontology languages (OIL, DAML+OIL, OWL), reasoning systems (FaCT) and their practical application to real problems. Her work on the application of ontologies to biology and bioinformatics has been particularly influential. She currently has a leading role in two major international initiatives: the Semantic Web and the Grid. She has combined these into the Semantic Grid, co-chairing the Semantic Grid Research Group in the Global Grid Forum standards organisation and directing\u00a0\u2026", "num_citations": "2\n", "authors": ["1168"]}
{"title": "Sentinel: Universal Access to Ambient Devices\n", "abstract": " We suggest that with the advent of ambient devices, pervasive computing systems, mobile userdevices and the associated move towards accessing mobile information the HCI community has a perfect opportunity to influence the design of mobile interfaces early in their lifecycle. The Sentinel activity (part of the wider proXimity project) seeks to decouple the interface (siren) from the ambient object (fire alarm) and place that interface with the user\u2019s mobile device (PDA,\u2018Braille n Speak\u2019). The user device is specific to that user and so too is the interface; however, the problem of static interface creation (normally by sighted designers) still exists. If we are truly to make access to the real world a universal activity the presentation of the interface (buttons, sliders, etc) must be separated from the functionality of the device, and therefore the functionality the interface is required to fulfill (data and control instructions). Sentinel aims to address this issue by using functional prototypes (written in XML) to separate these two areas. It does this without specifying the type of interface-control (button, tick-box, etc) required and instead delegates the interface-control presentation task to the user-device which can more individually react to the accessibility needs of its user.", "num_citations": "2\n", "authors": ["1168"]}
{"title": "Towel: Experiences of Augmenting the Web with Journey Knowledge.\n", "abstract": " Movement, or mobility, is key to the accessibility, design, and usability of many websites. While some peripheral mobility issues have been addressed few have centered on the mobility problems of visually impaired users. We use our past work to address these issues and derive mobility heuristics from mobility models, use these heuristics to place mobility objects within a web page, and describe the construction of a prototype mobility instrument, in the form of a Netscape plug-in, to process these objects. Specifically, we likened web use to travelling in a virtual space, compared it to travelling in a physical space, and introduced the idea of mobility-the ease of travel-as opposed to travel opportunity. Our hypothesis is that travel and mobility within the web mirrors travel and mobility within real-world environments. We suggest that the Web community has typically concentrated on navigation and/or orientation rather than the whole travel experience, and that this neglect is crucial when dealing with browsing by visually impaired users. We therefore extend the definition of travel to mean: confident navigation and orientation with purpose, ease and accuracy within an environment. Work, including ours, has shown that: Visually impaired users are hindered in their efforts to access the largest repository of electronic information in the world, namely the World Wide Web (WWW)[4]; A visually impaired user\u2019s cognition, perception, and world view are highly egocentric, meaning that information feedback should be tailored to these mental processes [1, 4]; Visually impaired web travellers are at a severe disadvantage, when moving around the web, compared\u00a0\u2026", "num_citations": "2\n", "authors": ["1168"]}
{"title": "Querying Objects with Description Logics.\n", "abstract": " This paper presents an approach to answering queries over an ontology modelled using a description logic. The ontology acts as a global schema, providing a declarative description of the concepts of the domain, the instances of which are stored in (potentially many) object-wrapped sources. Queries are expressed using terms from the rich vocabulary of the ontology, and are translated into an equivalent calculus expression, which references only the objects available in the source databases. The query is then optimised on the basis of information from the ontology and the source databases.", "num_citations": "2\n", "authors": ["1168"]}
{"title": "OIL: a slick way to represent knowledge for bioinformatics and the web\n", "abstract": " The web has proved to be an excellent mechanism for the ready publication and availability of information. It has been an effective technology for supporting the biologists\u2019 culture of co-ordinated research, and the sharing and rapid dissemination of information. What is missing is the next level of interoperability\u2014not just making information available, but understanding what the information means so that it can be linked in appropriate and insightful ways, and providing automated support for this process.The integration and sharing of information requires a consistent shared understanding of the meaning of that information. The biologist\u2019s knowledge of molecular biology and bioinformatics, and their interpretation of the resources with respect to this knowledge, is essential to the task of combining resources to answer queries. A shared understanding requires three things: metadata, terminologies and ontologies.", "num_citations": "2\n", "authors": ["1168"]}
{"title": "\u201cFetch me a picture representing triumph or similar\u201d: classification based navigation and retrieval for picture archives\n", "abstract": " The structured representation of the description logic can assist in providing more powerful environments for retrieval, through the support of browsing, navigation and the serendipitous discovery of information. The conceptual space can also prove useful for defining notions of similarity and semantic closeness. Our navigation interface goes some way to supporting our users' searching requirements, helping to close the gap between the indexer and the searcher. However, it is not without its flaws. For example the explicit exposure of a detailed and abstract ontology can lead to excessive interaction during query formulation. We have focused on T-Box reasoning and the support of conceptual queries rather than capturing and reasoning about the incompleteness and inconsistency of document descriptors. Most research has concentrated on T-Box reasoning; very few real DL systems have sound and complete A\u00a0\u2026", "num_citations": "2\n", "authors": ["1168"]}
{"title": "Schemas for telling stories in medical records\n", "abstract": " To accurately support a patient's medical record, at least four interrelated models are required: a simple static one-level schema is inadequate. The models must support the medical record as a coherent story reconstructed from the sequence of recorded events within the medical record. We propose one representation which unifies all four models by a three space approach, each space acting as a schema for the space below. The three spaces assist atemporal summarisation of a patient's medical record and illustrate the difficulties of recording retrospective or contradictory observations. The approach uses a generative, descriptive subsumption-based classification formalism with a sophisticated system of semantic constraints controlling the generation of implied intensional concepts. We report our experiences in its use in a prototype clinical workstation. We believe that this model can be used for complex\u00a0\u2026", "num_citations": "2\n", "authors": ["1168"]}
{"title": "TAMBIS: Transparent Access to Bioinformatics Information Sources\n", "abstract": " 1. BackgroundThe biological community is a distributed one, with a culture of sharing and rapid dissemination of information. Each separate area of molecular biology generates its own data and therefore its own information sources, including those for protein sequences, genome projects, DNA sequences, protein structures and motifs. Also available are a range of specialist interrogation and analysis tools, each typically associated with a particular database format. Frequently the information sources have different structures, content and query languages; and the tools have no common user interface and often only work on a limited subset of the data.", "num_citations": "2\n", "authors": ["1168"]}
{"title": "A Community Roadmap for Scientific Workflows Research and Development\n", "abstract": " The landscape of workflow systems for scientific applications is notoriously convoluted with hundreds of seemingly equivalent workflow systems, many isolated research claims, and a steep learning curve. To address some of these challenges and lay the groundwork for transforming workflows research and development, the WorkflowsRI and ExaWorks projects partnered to bring the international workflows community together. This paper reports on discussions and findings from two virtual \"Workflows Community Summits\" (January and April, 2021). The overarching goals of these workshops were to develop a view of the state of the art, identify crucial research challenges in the workflows community, articulate a vision for potential community efforts, and discuss technical approaches for realizing this vision. To this end, participants identified six broad themes: FAIR computational workflows; AI workflows; exascale challenges; APIs, interoperability, reuse, and standards; training and education; and building a workflows community. We summarize discussions and recommendations for each of these themes.", "num_citations": "1\n", "authors": ["1168"]}
{"title": "Packaging research artefacts with RO-Crate\n", "abstract": " An increasing number of researchers support reproducibility by including pointers to and descriptions of datasets, software and methods in their publications. However, scientific articles may be ambiguous, incomplete and difficult to process by automated systems. In this paper we introduce RO-Crate, an open, community-driven, and lightweight approach to packaging research artefacts along with their metadata in a machine readable manner. RO-Crate is based on Schemaorg annotations in JSON-LD, aiming to establish best practices to formally describe metadata in an accessible and practical way for their use in a wide variety of situations. An RO-Crate is a structured archive of all the items that contributed to a research outcome, including their identifiers, provenance, relations and annotations. As a general purpose packaging approach for data and their metadata, RO-Crate is used across multiple areas, including bioinformatics, digital humanities and regulatory sciences. By applying \"just enough\" Linked Data standards, RO-Crate simplifies the process of making research outputs FAIR while also enhancing research reproducibility. An RO-Crate for this article is available at https://www.researchobject.org/2021-packaging-research-artefacts-with-ro-crate/", "num_citations": "1\n", "authors": ["1168"]}
{"title": "Identifiers. org: Compact Identifier services in the cloud\n", "abstract": " Motivation Since its launch in 2010, Identifiers.org has become an important tool for the annotation and cross-referencing of Life Science data. In 2016, we established the Compact Identifier (CID) scheme (prefix: accession) to generate globally unique identifiers for data resources using their locally assigned accession identifiers. Since then, we have developed and improved services to support the growing need to create, reference and resolve CIDs, in systems ranging from human readable text to cloud-based e-infrastructures, by providing high availability and low-latency cloud-based services, backed by a high-quality, manually curated resource.  Results We describe a set of services that can be used to construct and resolve CIDs in Life Sciences and beyond. We have developed a new front end for accessing the Identifiers.org registry data and APIs to simplify integration of Identifiers.org CID services with third\u00a0\u2026", "num_citations": "1\n", "authors": ["1168"]}
{"title": "Workflows Community Summit: Bringing the Scientific Workflows Community Together\n", "abstract": " Scientific workflows have been used almost universally across scientific domains, and have underpinned some of the most significant discoveries of the past several decades. Many of these workflows have high computational, storage, and/or communication demands, and thus must execute on a wide range of large-scale platforms, from large clouds to upcoming exascale high-performance computing (HPC) platforms. These executions must be managed using some software infrastructure. Due to the popularity of workflows, workflow management systems (WMSs) have been developed to provide abstractions for creating and executing workflows conveniently, efficiently, and portably. While these efforts are all worthwhile, there are now hundreds of independent WMSs, many of which are moribund. As a result, the WMS landscape is segmented and presents significant barriers to entry due to the hundreds of\u00a0\u2026", "num_citations": "1\n", "authors": ["1168"]}
{"title": "RO-Crate Metadata Specification 1.1. 1\n", "abstract": " This document specifies a method, known as RO-Crate (Research Object Crate), of aggregating and describing research data with associated metadata. RO-Crates can aggregate and describe any resource including files, URI-addressable resources, or use other addressing schemes to locate digital or physical data. RO-Crates can describe data in aggregate and at the individual resource level, with metadata to aid in discovery, re-use and long term management of data. Metadata includes the ability to describe the context of data and the entities involved in its production, use and reuse. For example: who created it, using which equipment, software and workflows, under what licenses can it be re-used, where was it collected, and/or where is it about.", "num_citations": "1\n", "authors": ["1168"]}
{"title": "Identifiers. org-Compact Identifier Services in the Cloud.\n", "abstract": " MOTIVATION: Since its launch in 2010, Identifiers. org has become an important tool for the annotation and cross-referencing of Life Science data. In 2016, we established the Compact Identifier (CID) scheme (prefix: accession) to generate globally unique identifiers for data resources using their locally assigned accession identifiers. Since then, we have developed and improved services to support the growing need to create, reference and resolve CIDs, in systems ranging from human readable text to cloud based e-infrastructures, by providing high availability and low latency cloud-based services, backed by a high quality, manually curated resource. RESULTS: We describe a set of services that can be used to construct and resolve CIDs in Life Sciences and beyond. We have developed a new front end for accessing the Identifiers. org registry data and APIs to simplify integration of Identifiers. org CID services with third party applications. We have also deployed the new Identifiers. org infrastructure in a commercial cloud environment, bringing our services closer to the data. AVAILABILITY: https://identifiers. org. SUPPLEMENTARY INFORMATION: Supplementary data are available at Bioinformatics online.", "num_citations": "1\n", "authors": ["1168"]}
{"title": "Introducing RO-Crate: research object data packaging\n", "abstract": " This presentation will introduce a new specification for describing and distributing research datasets as re-usable objects: RO-Crate. The workis an amalbam of the DataCrate specification and Research Object and is intended to distill a number of community efforts into a single easy to implement specification for describing datasets at rest, on the web, and packaged for distribution using standard mechanisms such as BagIt.", "num_citations": "1\n", "authors": ["1168"]}
{"title": "Bioexcel building blocks test cases\n", "abstract": " Bioexcel building blocks test cases for Lysozyme and Pyruvate Kinase executed in Workstations, OpenNebula VMs and MareNostrum4 HPC.", "num_citations": "1\n", "authors": ["1168"]}
{"title": "RO-Crate, a lightweight approach to Research Object data packaging\n", "abstract": " A Research Object (RO) provides a machine-readable mechanism to communicate the diverse set of digital and real-world resources that contribute to an item of research. The aim of an RO is to replace traditional academic publications of static PDFs, to rather provide a complete and structured archive of the items (such as people, organisations, funding, equipment, software etc) that contributed to the research outcome, including their identifiers, provenance, relations and annotations. This is increasingly important as researchers now rely heavily on computational analysis, yet we are facing a reproducibility crisis as key components are often not sufficiently tracked, archived or reported. We are developing Research Object Crate (or RO-Crate for short), a lightweight approach to package research data with their structured metadata, based on schema. org annotations in a formalized JSON-LD format that can be used\u00a0\u2026", "num_citations": "1\n", "authors": ["1168"]}
{"title": "Research Object Community Update\n", "abstract": " We highlight recent developments and approaches in the ResearchObject. org community. Research Object, originally proposed in [1] and since developed and expanded on [2][3], is a framework by which the many, nested and contributed components of research can be packaged together in a systematic way, and their context, provenance and relationships richly described. Research Objects (ROs) define ontology-based mechanisms for richly describing the contents of container manifests, and the relationships between them.", "num_citations": "1\n", "authors": ["1168"]}
{"title": "Capturing interoperable reproducible workflows\n", "abstract": " We present our ongoing work on integrating Research Object practices with Common Workflow Language, capturing and describing prospective and retrospective provenance.", "num_citations": "1\n", "authors": ["1168"]}
{"title": "FAIRDOM: Reproducible Systems Biology through FAIR Asset Management\n", "abstract": " The FAIR data principles state [1] that data should be Findable (and citable), Accessible (with appropriate caveats for sensitive data), Interoperable (can be combined, typically through adherence to standards) and Reusable (can be reused later or reproduced later from a publication). These principles lie at the heart of a number of Research Infrastructures within Europe such as ELIXIR (http://www. elixir-europe. org) and ISBE\u2013Infrastructure for Systems Biology in Europe (http://project. isbe. eu/). FAIRDOM (http://fair-dom. org)[2] is a European Research Infrastructure initiative sponsored by ISBE and the ERANet ERASysAPP. FAIRDOM\u2019s primary mission is to support researchers, students, trainers, funders and publishers by enabling Systems and Synthetic Biology projects-their team members and collaborators-to make their Data, Operating procedures and Models FAIR. Integrative biology inherently has multiple parts, typically includes multiple parties and practically scatters its outcomes across multiple resources. These characteristics have consequences for reproducibility, sharing and ultimately wider publication.Multiple and interrelated components-Mathematical modelling methods and laboratory experiments are combined in order to understand and predict dynamic processes in living systems. Heterogeneous data, including multiple \u2018omics datasets, are integrated and interlinked with mathematical models to allow results to be interpreted, compared and/or reused. The Standard Operating Procedures associated with the data are essential in order to interpret the data and to inform the modelling. Thus any investigation will include many data\u00a0\u2026", "num_citations": "1\n", "authors": ["1168"]}
{"title": "Help me describe my data: A demonstration of the Open PHACTS VoID Editor.\n", "abstract": " The Open PHACTS VoID Editor helps non-Semantic Web experts to create machine interpretable descriptions for their datasets. The web app guides the user, an expert in the domain of the data, through a series of questions to capture details of their dataset and then generates a VoID dataset description. The generated dataset description conforms to the Open PHACTS dataset description guidelines that ensure suitable provenance information is available about the dataset to enable its discovery and reuse.", "num_citations": "1\n", "authors": ["1168"]}
{"title": "DistillFlow: removing redundancy in scientific workflows\n", "abstract": " Scientific workflows management systems are increasingly used by scientists to specify complex data processing pipelines. Workflows are represented using a graph structure, where nodes represent tasks and links represent the dataflow. However, the complexity of workflow structures is increasing over time, reducing the rate of scientific workflows reuse. Here, we introduce DistillFlow, a tool based on effective methods for workflow design, with a focus on the Taverna model. DistillFlow is able to detect\" anti-patterns\" in the structure of workflows (idiomatic forms that lead to over-complicated design) and replace them with different patterns to reduce the workflow's overall structural complexity. Rewriting workflows in this way is beneficial both in terms of user experience and workflow maintenance.", "num_citations": "1\n", "authors": ["1168"]}
{"title": "Emerging Computational Methods for the Life Sciences Workshop 2012\n", "abstract": " 1 BACKGROUNDComputing systems are rapidly changing with multicore, graphics processing units (GPUs), clusters, volunteer systems, clouds, and grids offering a confusing dazzling array of opportunities. New programming paradigms such as Google MapReduce and many-task computing have joined the traditional repertoire of workflow and parallel computing for the highest performance systems. Meanwhile, the life sciences are continuing to expand in data generated with continuing improvement in the instruments for high-throughput analysis. This \u2018fourth paradigm\u2019(data driven science) is joined by complex systems or biocomplexity that can build phenomenological models of biological systems and processes. This special issue for Emerging Computational Methods for the Life Sciences Workshop ECMLS2012 1, juxtaposes these trends seeking those computational methods that will enhance scientific\u00a0\u2026", "num_citations": "1\n", "authors": ["1168"]}
{"title": "Open PHACTS explorer bringing the web to the semantic web\n", "abstract": " The Open PHACTS Explorer is a web application that supports drug discovery via the Open PHACTS API without requiring knowledge of SPARQL or the RDF data being searched. It provides a UI layer on top of the Open PHACTS linked data cache and also provides a javascript library to facilitate easy access to the Open PHACTS API. 1.1", "num_citations": "1\n", "authors": ["1168"]}
{"title": "Stealthy annotation of experimental biology by spreadsheets\n", "abstract": " The increase in volume and complexity of biological data has led to increased requirements to reuse that data. Consistent and accurate metadata is essential for this task, creating new challenges in semantic data annotation and in the constriction of terminologies and ontologies used for annotation. The BioSharing community are developing standards and terminologies for annotation, which have been adopted across bioinformatics, but the real challenge is to make these standards accessible to laboratory scientists. Widespread adoption requires the provision of tools to assist scientists whilst reducing the complexities of working with semantics. This paper describes unobtrusive \u2018stealthy\u2019 methods for collecting standards compliant, semantically annotated data and for contributing to ontologies used for those annotations. Spreadsheets are ubiquitous in laboratory data management. Our spreadsheet\u2010based\u00a0\u2026", "num_citations": "1\n", "authors": ["1168"]}
{"title": "Distilling structure in scientific workflows\n", "abstract": " MethodsOur study has been conducted on a set of 1,014 distinct workflows extracted from the Taverna workflows available in myExperiment in May 2012. We have implemented the algorithm of (Valdes et al., 1979) to detect whether workflow graphs are SP. Intuitively, SP structures are graph structures having one main input (I in figure 1 (a)) and one main output (O in Figure 1 (a)), without loops and which can be synchronized. In particular the pattern highlighted in Figure 1 (b) is forbidden (in this pattern, arcs can be replaced by paths involving intermediate nodes). In this pattern, node w is responsible for the graph non to be SP. Such a node is called a reduction node (Bein et al., 1992) and is duplicated in SPFlow. In the workflow depicted in Figure 1 (a) the getGeneInfo processor is a reduction node so that the workflow is not SP. Among the 390 workflows with non SP structures (38, 5%), we have focused on identifying reduction nodes and analyzed the forbidden pattern in which they were involved. We have then driven two series of experiments:", "num_citations": "1\n", "authors": ["1168"]}
{"title": "Why Linked Data Is Not Enough For Scientists\n", "abstract": " Scientific data represents a significant portion of the linked open data cloud and scientists stand to benefit from the data fusion capability this will afford. Publishing linked data into the cloud, however, doesn\u2019t ensure the required reusability. Publishing has requirements of provenance, quality, credit, attribution and methods to provide the reproducibility that enables validation of results. In this paper we make the case for a scientific data publication model on top of linked data and introduce the notion of Research Objects as first class citizens for sharing and publishing.", "num_citations": "1\n", "authors": ["1168"]}
{"title": "Janus: From Work\ufb02ows to Semantic Provenance and Linked Open Data\n", "abstract": " Data provenance graphs are form of metadata that can be used to establish a variety of properties of data products that undergo sequences of transformations, typically speci\ufb01ed as work\ufb02ows. Their usefulness for answering user provenance queries is limited, however, unless the graphs are enhanced with domain-speci\ufb01c annotations. In this paper we propose a model and architecture for semantic, domain-aware provenance, and demonstrate its usefulness in answering typical user queries. Furthermore, we discuss the additional bene\ufb01ts and the technical implications of publishing provenance graphs as a form of Linked Data. A prototype implementation of the model is available for data produced by the Taverna work\ufb02ow system.", "num_citations": "1\n", "authors": ["1168"]}
{"title": "Scientific Social Objects\n", "abstract": " Scientific research is increasingly conducted digitally and online, and consequently we are seeing the emergence of new digital objects shared as part of the conduct and discourse of science. These Scientific Social Objects are more than lumps of domain-specific data: they may comprise multiple components which can also be shared separately and independently, and some contain descriptions of scientific processes from which new objects will be generated. Using the myExperiment social website as a case study we explore Scientific Social Objects and discuss their evolution.", "num_citations": "1\n", "authors": ["1168"]}
{"title": "Seamless Provenance Representation and Use in Collaborative Science Scenarios\n", "abstract": " The notion of sharing scientific data has only recently begun to gain ground in science, where data is still considered a private asset. There is growing evidence, however, that the benefits of scientific collaboration through early data sharing during the course of a science project may outgrow the risk of losing exclusive ownership of the data. As exemplar success stories are making the headlines [1], principles of effective information sharing have become the subject of e-science research. In particular, any piece of published data should be self-describing, to the extent necessary for consumers to determine its suitability for reuse in their own projects. This is accomplished by associating a body of formally specified and machine-processable metadata to the data. When data is produced and reused by independent groups, however, metadata interoperability issues emerge. This is the case for provenance, a form of\u00a0\u2026", "num_citations": "1\n", "authors": ["1168"]}
{"title": "Scientific workflow 2009\n", "abstract": " This special issue of Journal of Computer and System Sciences is devoted to papers on scientific workflow management based on WaGe07\u2013The 2nd International Workshop on Workflow Management and Application in Grid Environments held in Xinjiang, China, August 16\u201318, 2007, and WaGe08\u2013The 3rd International Workshop on Workflow Management and Applications in Grid Environments held in Kunming, China, May 25\u201328, 2008. Scientific workflow aims to enable complex scientific applications such as climate modelling and astrophysics simulation to be able to be executed step by step. It falls in the area of parallel and distributed computing and the area of computer modelling of complex systems in the context of Journal of Computer and System Sciences. It is becoming a dedicated area and attracting high interest internationally. Hence, this special issue is a timely and strategic one in the journey of this area. The eight papers in this special issue offer a view from different perspectives on current research in scientific workflow. The first paper presents techniques for analysing how information propagates in scientific workflows. This can help to ensure that sensitive information in scientific workflows can be accessed by and propagated to only authorised parties. The second paper focuses on fault tolerance in scientific workflows. It proposes a workflow-level checkpointing scheme and its corresponding rollback recovery process for hierarchical scientific workflows. The third paper is on the topic of scientific workflows across multiple organisational domains. It proposes a collaborative scheduling approach that can deal with temporal\u00a0\u2026", "num_citations": "1\n", "authors": ["1168"]}
{"title": "Public Health e-Labs: An ethical model and architecture for distributed epidemiology using healthcare records\n", "abstract": " With electronic health records, individual privacy is preserved by: i) integrating and de-identifying records using locally-governed procedures; and ii) deploying e-Lab as a population analysis layer on the integrated, de-identified health records, but only within the firewall and governance of the local health community. A community can choose to join a federation of e-Labs, sharing data-extracts and/or analyses, applying locally-acceptable procedures for minimizing deductive-disclosure risks.", "num_citations": "1\n", "authors": ["1168"]}
{"title": "Semantic Technologies in the SIMDAT Grid Project\n", "abstract": " The SIMDAT project aims at developing generic grid technology for the solution of complex application problems and using this new technology in several industrial application sectors. Semantic technologies are expected to offer a significant added value to other technologies with respect to the management of resources on the process level and on the data level. The strategic objectives of SIMDAT are (i) to test and enhance data grid technology for product development and production process design,(ii) to develop federated versions of problem-solving environments (PSEs) by leveraging enhanced grid services,(iii) to exploit data grids as a basis for distributed knowledge discovery,(iv) to promote de facto standards for these enhanced grid technologies across a range of disciplines and sectors as well as (v) to raise awareness for the advantages of data grids in important industrial sectors.", "num_citations": "1\n", "authors": ["1168"]}
{"title": "TAMBIS: transparent access to multiple bioinformatics services\n", "abstract": " Transparent Access to Multiple Bioinformatics Information Sources (TAMBIS) addresses the perennial problem of heterogeneity and distribution of bioinformatics resources in performing bioinformatics analyses. Asking questions of these resources usually requires multiple resources to be used and data transferred between those resources. A biologist using these resources needs much knowledge of which resources to use, where they are to be found, in which order they should be used, and how to overcome the heterogeneity between those resources. TAMBIS seeks to make this knowledge burden transparent by cap turing knowledge about molecular biology and bioinformatics tasks in an ontology. The TAMBIS ontology acts as a global schema over diverse resources and drives a query formulation interface offering a common language over those resources. High\u2010level, conceptual, source\u2010independent\u00a0\u2026", "num_citations": "1\n", "authors": ["1168"]}
{"title": "Next Generation Grids 2-Requirements and Options for European Grids Research 2005-2010 and Beyond\n", "abstract": " OPUS 4 | Next Generation Grids 2 - Requirements and Options for European Grids Research 2005-2010 and Beyond Deutsch Login Home Search Browse Publish FAQ Next Generation Grids 2 - Requirements and Options for European Grids Research 2005-2010 and Beyond Stefano Campadello, David De Roure, Bahador Farshchian, Meike Fehse, Carole A. Goble, Yunmiao Gui, Manuel V. Hermenegildo, Karl Jeffery, Domenico Laforenza, Peter Maccallum, Jordi Masso, Thierry Priol, Alexander Reinefeld, Michel Riguidel, David Snelling, Domenico Talia, Theodora A. Varvarigou Export metadata BibTeX RIS XML Additional Services Share in Twitter Search Google Scholar Metadaten Author: Stefano Campadello, David De Roure, Bahador Farshchian, Meike Fehse, Carole A. Goble, Yunmiao Gui, Manuel V. Hermenegildo, Karl Jeffery, Domenico Laforenza, Peter Maccallum, Jordi Masso, Thierry Priol, Alexander \u2026", "num_citations": "1\n", "authors": ["1168"]}
{"title": "Mobility support for visually impaired web travellers\n", "abstract": " This short report presents the progress of the PhD project which aims to improve the mobility of visually impaired Web users. The thesis of this PhD project is that \u201cWeb pages could be analysed for identifying travel objects and their roles, in consequence they could be annotated with semantic metadata so that a tool could be devised to transform Web pages in a way that the objects could play their intended roles and enhance the provided mobility support\u201d. This report summarises:\u2022 the motivation and objectives of the project,\u2022 the work performed in the first two years,\u2022 a work plan for the final year of the project, and\u2022 proposes a time management scheme for the project and outlines the anticipated thesis structure.", "num_citations": "1\n", "authors": ["1168"]}
{"title": "X^ pmi an eXtensible Personal Mobility Interface (A Semantic Web of Mobility Devices)\n", "abstract": " Introduction Personal mobility-mobility can be thought of as the ease as opposed to the opportunity for travel-is important not just for the implicit benefits of easy and accurate navigation and orientation, but also because it supports the less tangible factors of self worth, freedom, and independence. A high level of mobility can be difficult for visually impaired and sighted individuals alike, especially when in unfamiliar and complex internal or urban environments such as airports, bus or train stations, shopping centres, and unfamiliar cities. Signage can help both groups (as long as it is appropriate), however complex environments seem to encourage complex and confusing signage that can handicap everyone. The problem is further complicated because signage is not descriptive, dynamic, or tailored to an individual's personal journey and therefore mobility. Some systems have already been proposed to alleviate these problems but these have mainly been concerned with large-sca", "num_citations": "1\n", "authors": ["1168"]}
{"title": "What have the Romans (and Germans) ever done for us? or There are real applications for Description Logics some of which even take advantage of their reasoning services.\n", "abstract": " One of the nagging worries of the Description Logic community is what applications are really there that make use of their e orts. So the hunt is on for some sort of\\killer application\" that can demonstrate an improvement, or success where previously there was none, by the use of Description Logic based reasoning.As an application builder outside the Description Logic community, who tries to deploy Description Logics as part of a portfolio of techniques in real applications, I hope to reassure this community that, yes, there are useful applications of their endeavours-that it is all worthwhile. These applications may not necessarily guarantee the silver bullet that the Description Logic community yearns for, and maybe they don't all use the\\full power\" of a Description Logic, but the message is: the Description Logic helped and was thus helpful, and we believe that it could potentially help more.", "num_citations": "1\n", "authors": ["1168"]}
{"title": "COHSE: informed WWW link navigation using ontologys\n", "abstract": " Hypermedia assists the user in navigating the available information space: this space is diverse and distributed, populated by multimedia objects, which may be persistent, constantly updated, and perhaps dynamically generated. Navigation may involve preauthored links ('buttons\") and links computed dynamically, but navigation techniques based on links alone have proved insufficient for the WWW and keyword-based search engines have to be used to augment navigation strategies. By indexing a large proportion of the documents on the WWW these can provide pointers to huge numbers of apparently relevant documents, although frequently with low precision measures.However, there is no standard for the use of keywords and consistent keyword descriptions are as elusive as consistent links. In a global information environment, where the number and size of potential information resources is huge, of variable\u00a0\u2026", "num_citations": "1\n", "authors": ["1168"]}
{"title": "Visualizing Complex Schemas in Description Logic using Movable Lens Filters\n", "abstract": " In recent years, the distinctions between the requirements and problems in knowledge representation and databases have receded. As the information stored in databases becomes more complex they benefit from the more expressive reasoning and intelligent retrieval techniques offered by knowledge representation techniques. Description Logics, such as GRAIL, can be used as object-based complex database schemas However, the complexity of visualizing the relationships between concepts in a large Description Logic model is not straightforward; their isKindOf inheritance lattice is inferred on the basis of the composition of the concepts, the legality of various concept compositions can be hard to determine, and frequent patterns of concepts and their relationships are obscured. We have developed a visualization environment based on Movable Lens filters to support GRAIL modelers building a very\u00a0\u2026", "num_citations": "1\n", "authors": ["1168"]}
{"title": "Towards an efficient description logics system with individuals\n", "abstract": " This report presents the status of the research the author is conducting on the development of a complete Description Logics knowledge representation system.Description Logics knowledge representation systems are based of a family of formal languages for describing complex structured classes. These languages comprise boolean operators and quanti cation over class attributes, as well as the declaration of elements of the classes and their properties. DL knowledge bases are partitioned in an intensional part which describes the general organisation of the classes, and an extensional part for describing class elements.", "num_citations": "1\n", "authors": ["1168"]}
{"title": "The pragmatics of naive database enquiry\n", "abstract": " Accessing information in databases has traditionally been the preserve of the computer professional, but as new interfaces become available, new users need to be less knowledgeable about the nuts and bolts of computing. It is the contention here that the designers of systems for these \u2018naive\u2019 users need to examine the model of the communicative process used in everyday life to support a co-operative dialogue between human and machine. In this paper we investigate at the pragmatics of the communicative process and how it relates to recent interface development, and present GUIDANCE, a system that implements some of its conclusions.", "num_citations": "1\n", "authors": ["1168"]}
{"title": "Experiences of One Day Workshops for Schoolgirls\n", "abstract": " Manchester University first ran computing2 workshops for schoolgirls in 1985. The original motivation was to encourage more women to take up careers in science and, hopefully, to increase the numbers of women studying science subjects at university level.", "num_citations": "1\n", "authors": ["1168"]}
{"title": "Grassroot Groups\u2014Experiences in Horticultural Cultivation\n", "abstract": " In 1989 we started a WiC group in Greater Manchester (GM-WiC). The first author had tried unsuccessfully to form a group with the help of fellow computing lecturers in higher education, and turned to the Women\u2019s Technology Centre hoping for (and getting!) more positive support. Together with Annie Rafferty, manager of the Women\u2019s Technology and Enterprise Centre (WOTEC), we founded and still form the cornerstone of the group. At the start we had to be clear in our own minds what we wanted GM-WiC to be like, what its objectives should be and the people we wanted to be part of such a group. We intended the group to have three main aims:                                         to produce and disseminate information;                                                           to be a discussion and support network;                                                           to raise the awareness of the local people to WiC.", "num_citations": "1\n", "authors": ["1168"]}