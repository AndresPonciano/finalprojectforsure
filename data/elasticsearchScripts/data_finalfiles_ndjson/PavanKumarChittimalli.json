{"title": "Test-suite augmentation for evolving software\n", "abstract": " One activity performed by developers during regression testing is test-suite augmentation, which consists of assessing the adequacy of a test suite after a program is modified and identifying new or modified behaviors that are not adequately exercised by the existing test suite and, thus, require additional test cases. In previous work, we proposed MATRIX, a technique for test-suite augmentation based on dependence analysis and partial symbolic execution. In this paper, we present the next step of our work, where we (I) improve the effectiveness of our technique by identifying all relevant change-propagation paths, (2) extend the technique to handle multiple and more complex changes, (3) introduce the first tool that fully implements the technique, and (4) present an empirical evaluation performed on real software. Our results show that our technique is practical and more effective than existing test-suite\u00a0\u2026", "num_citations": "183\n", "authors": ["2155"]}
{"title": "Recomputing coverage information to assist regression testing\n", "abstract": " This paper presents a technique that leverages an existing regression test selection algorithm to compute accurate, updated coverage data on a version of the software, P i+1 , without rerunning any test cases that do not execute the changes from the previous version of the software, P i  to P i+1 . The technique also reduces the cost of running those test cases that are selected by the regression test selection algorithm by performing a selective instrumentation that reduces the number of probes required to monitor the coverage data. Users of our technique can avoid the expense of rerunning the entire test suite on P i+1  or the inaccuracy produced by previous approaches that estimate coverage data for P i+1  or that reuse outdated coverage data from Pi. This paper also presents a tool, RECOVER, that implements our technique, along with a set of empirical studies on a set of subjects that includes several industrial\u00a0\u2026", "num_citations": "102\n", "authors": ["2155"]}
{"title": "Re-computing Coverage Information to Assist Regression Testing\n", "abstract": " This paper presents a technique that leverages an existing regression test selection algorithm to compute accurate, updated coverage data on a version of the software, P i+1 , without rerunning any test cases that do not execute the changes from the previous version of the software, P i  to P i+1 . The technique also reduces the cost of running those test cases that are selected by the regression test selection algorithm by performing a selective instrumentation that reduces the number of probes required to monitor the coverage data. Users of our technique can avoid the expense of rerunning the entire test suite on P i+1  or the inaccuracy produced by previous approaches that estimate coverage data for P i+1  or that reuse outdated coverage data from Pi. This paper also presents a tool, RECOVER, that implements our technique, along with a set of empirical studies on a set of subjects that includes several industrial\u00a0\u2026", "num_citations": "102\n", "authors": ["2155"]}
{"title": "Matrix: Maintenance-oriented testing requirements identifier and examiner\n", "abstract": " This paper presents a new test-suite augmentation technique for use in regression testing of software. Our technique combines dependence analysis and symbolic evaluation and uses information about the changes between two versions of a program to (1) identify parts of the program affected by the changes, (2) compute the conditions under which the effects of the changes are propagated to such parts, and (3) create a set of testing requirements based on the computed information. Testers can use these requirements to assess the effectiveness of the regression testing performed so far and to guide the selection of new test cases. The paper also presents MATRIX, a tool that partially implements our technique, and its integration into a regression-testing environment. Finally, the paper presents a preliminary empirical study performed on two small programs. The study provides initial evidence of both the\u00a0\u2026", "num_citations": "70\n", "authors": ["2155"]}
{"title": "Regression test selection on system requirements\n", "abstract": " Regression testing, which is performed after changes are made to a software system, can be used before release of new versions of the system. However, practitioners often have little time to perform this regression testing because of the quick-release cycles of such modified systems. Thus, they may use a random-testing approach or perform little regression testing. This lack of adequate regression testing can cause bugs in untested parts of the program to be exposed only during production or field usage. To improve the efficiency of the regression testing, and thus enable its use before release, techniques that select and run only those test cases that are related to the changes or prioritize the test cases based on criticality or perceived effectiveness have been presented. These technique typically use some representation of the software such as a system model or the source code to perform the test selection and\u00a0\u2026", "num_citations": "46\n", "authors": ["2155"]}
{"title": "Automated test cycle estimation system and method\n", "abstract": " A system and method is disclosed to estimate both, the time and number of resources required to execute a test suite or a subset of test suite in parallel, with the objective of providing a balanced workload distribution. The present invention partitions test suite for parallelization, given the dependencies that exists between test cases and test execution time.", "num_citations": "36\n", "authors": ["2155"]}
{"title": "GEMS: a generic model based source code instrumentation framework\n", "abstract": " Software Programmers need to monitor and measure dynamic behavior of programs. Program instrumentation tools and techniques have aided profiling, debugging, coverage analysis, and dynamic program analysis. While several open source and commercial instrumentation tools are available, that support multitude of techniques and source languages, none of the tools support a cross section of languages. Moreover, instrumentation tools lack support for systems that have been developed using multiple languages. The output produced by each tool is different, leading to problems in usage of the same by other tools to be a challenge. As an IT service provider, our organization maintains systems developed using a large number of programming languages. We develop in-house dynamic program analysis and testing tools, that use instrumentation to help with the maintenance activities. To address the availability\u00a0\u2026", "num_citations": "15\n", "authors": ["2155"]}
{"title": "Domain-independent method of detecting inconsistencies in SBVR-based business rules\n", "abstract": " Traditionally, business rules are expressed informally in English, captured eventually, as a part of UML use-cases. Detecting anomalies in business rules is extremely difficult to automate, due to their informal nature, and manually error-prone due to the size and complexity. In recent times, business rules are being expressed increasingly using standard representations (such as Semantics of Business Vocabularies and Rules (SBVR)). We present a method to detect inconsistencies amongst the rules, based on the model checking. We exploit the First Order Logic (FOL) basis of SBVR representation to propose a method that is independent of the business domain. We present a case-study of business rules for well-known example of car-rental, and our method shows promising results to detect inconsistencies.", "num_citations": "13\n", "authors": ["2155"]}
{"title": "Assigning an annotation to a variable and a statement in a source code of a software application\n", "abstract": " A method and system for assigning an annotation to a statement in a source code. The method comprises generating intermediate representation of the source code by parsing the source code. The method comprises identifying one or more instances of definition of a variable and one or more instances of use of the variable. The method comprises categorizing the variable into a group of variables based on the one or more instances of definition of the variable, the one or more instances of use of the variable, a description of the variable, and mathematical operators defining a correlation between the variable and one or more other variables. Further, a data description table and a data dictionary of the plurality of variables are created. The method assigns an annotation to the variable present in the statement of the source code based on the data description table and the data dictionary.", "num_citations": "10\n", "authors": ["2155"]}
{"title": "An automated detection of inconsistencies in sbvr-based business rules using many-sorted logic\n", "abstract": " Business rules control and constrain the behavior and structure of the business system in terms of its policies and principles. Business rules are restructured frequently as per the internal or external circumstances based on market opportunities, statutory regulations, and business focus. The current practice in industry, of detecting inconsistencies manually, is error prone, due to the size, complexity and ambiguity in representation using natural language.                 Our work detects inconsistencies in business rules based on model checking that exploits the FOL basis of SBVR specification. We aim to reduce the burden on solvers and obtain effective system level test data, leading to the development of a novel inconsistency rule checker based on extracting the unsatisfiable cores using solvers like Z3, CVC4, etc. We introduce the concept of graphical clusters, to partition SBVR vocabularies and represent the\u00a0\u2026", "num_citations": "7\n", "authors": ["2155"]}
{"title": "Relation identification in business rules for domain-specific documents\n", "abstract": " This paper focuses on an approach to mine business rules from documents and facilitates a methodology to represent them in a formal notation. Businesses are operated abiding by some rules and complying with respect to regulation and guidelines. The business rules are often written using English in operating procedures, terms and conditions, and various other supporting documents. The manual analysis of these rules for activities like impact analysis, maintenance, business transformation leads to potential discrepancies, ambiguities, and quality issues. In this paper, we discuss our approach of mining relations among the rule intents (atomic facts) defined for business rules. We also present our preliminary studies on a couple of openly available documents.", "num_citations": "6\n", "authors": ["2155"]}
{"title": "An approach to mine business rule intents from domain-specific documents\n", "abstract": " An enterprise system enables business by providing various services that are guided by set of well-defined processes, and adhere to certain business rules and constraints. The business rules are usually written using English in operating procedures, terms and conditions, and various other supporting documents. For implementing the business rules in a software system, or expressing them as UML use-case specifications, analysts manually interpret the documents, leading to potential discrepancies, ambiguities, and quality issues in the software system that can be resolved only after testing.", "num_citations": "6\n", "authors": ["2155"]}
{"title": "An approach to mine SBVR vocabularies and rules from business documents\n", "abstract": " Enterprises model the behavior of their business to prepare a communication standard for business analysts and to specify requirements to Information Technology (IT) people. The communication gap between IT group and business analysts, who lie on the opposite end of the business spectrum exists due to the different terminologies used in their respective fields regarding the same context. This gap has led to major software failures which prompted the OMG group has come up with a new standard-Semantic of Business Vocabulary and Business Rules (SBVR). Declarative models are provided by SBVR to represent Business Vocabulary and Business Rules which can be understood by everyone working throughout the business spectrum. Each business is governed by business rules which are constrained by the regulation policy set up by the policy guidelines of the organization and government regulations\u00a0\u2026", "num_citations": "4\n", "authors": ["2155"]}
{"title": "Matgap: A systematic approach to perform match and gap analysis among sbvr-based domain specific business rules\n", "abstract": " In the modern age, the need for automation has led to Business Organizations representing their functionality as structured Business Rules. SBVR has come up as an universally popular format for representation of Business Rules. The presence of different Business Organizations working in a particular real life domain results in generation of different rules for each of the organization. Due to the varying business practices, like mergers & acquisitions, upgrades, incorporation of a new application, etc., it becomes necessary to compare a set of Business Rules of a particular organization with the rules of a reference model, to get a measure of similarity among the business functionality of the two. Presently, this comparison is carried out manually by business experts or by executing the rules of one organization with the data of another and checking if they are compliant. Both the approaches are extremely tedious and\u00a0\u2026", "num_citations": "4\n", "authors": ["2155"]}
{"title": "A Systematic Review of Methods for Consistency Checking in SBVR-based Business Rules.\n", "abstract": " Business enterprises in today\u2019s world have complex rules and process as their foundation. The rules and processes continuously change to reflect the enterprise\u2019s evolution and progress, market demands and regulations. This constant flux demands an automatic way to check these business rules for correctness and consistency. In the recent times, few methods were proposed for automated checks. In our literature review for this subject matter, however, we did not find a proper survey, which could present a consolidated picture of the properties, advantages and drawbacks of the different methods. The randomly scattered state of art for specifying business rules and analyzing them for inconsistencies is hindering the relevant research space. We conduct a systematic literature review of various solutions discussed in the field of consistency checking for business rules, especially rules in Semantics of Business Vocabularies and Rules (SBVR) format. We highlight the progress made in the field, aspects that can be developed further, and the current gaps in the methods so that future work can be channelized to address and close the gaps by proposing new or enhanced methods.", "num_citations": "4\n", "authors": ["2155"]}
{"title": "Semantic Search and Query Over SBVR-based Business Rules using SMT based Approach and Information Retrieval Method.\n", "abstract": " Presently, business organizations are regulating their activities with the aid of Business Rules (BR\u2019s). A single rule set of an organization contains large and diverse categories of BR\u2019s, thereby making it difficult for Business Analysts and end users to analyze and extract relevant BR\u2019s. Rule Search with natural language terms fail due to their inability to capture logical semantics present in BR\u2019s. In this paper, we present a novel approach to give correct and complete sets of SBVR (Semantics of Business Vocabulary and Business Rules) based BR\u2019s based on a specified query. We integrate conventional Information Retrieval Approach of text based searches over the rule base and corresponding meta-data with a SMT (Satisfiability Modulo Theory) based approach capturing the higher first order logic of the rules. The major applications of this approach are change impact analysis when rules are added, deleted or modified from a rule set, identifying the candidate set of rules affected due to change in the rule set and during match and gap analysis where we compare two sets of BR\u2019s identifying similarity and difference in business functionality between them. We show the implementation of our tool along with its performance on industry level datasets.", "num_citations": "3\n", "authors": ["2155"]}
{"title": "Identifying Anomalies in SBVR-based Business Rules using Directed Graphs and SMT-LIBv2.\n", "abstract": " In modern times, business rules have grown exponentially with enterprises becoming more complex in diverse fields. Due to this growth, different forms of anomalies creep into the business rules, causing business enterprise to take wrong decisions, which can impact it\u2019s performance and reputation. It is time and resource consuming to examine the rules manually due to the large number of rules intermingled with each other. The process of manual verification is also not free of human induced errors. Thus, automatic verification of business rules is the need of the hour. We present a tool to detect different anomalies in business rules represented in SBVR format. The tool uses a combination of Directed Graphs and SMT solvers to perform the verification task. We show the implementation of our tool along with it\u2019s evaluation on industry level benchmarks.", "num_citations": "3\n", "authors": ["2155"]}
{"title": "Variable Provenance in Software Systems\n", "abstract": " Data Provenance is defined as lineage or history of the given dataitem. Knowing the source of the data or the transformation of the data-source to compute the given data is critical for analyzing the quality of the data. Many transformations of data are done in software (source code). We introduce and define the concept of Variable Provenance for source code. We argue that determining the origin (s) of the data held by a variable and the history of modifications of the variable can provide critical information along many dimensions about what happens in the source code. We use understanding of source code and creating business rules from source code as use-cases to illustrate our view-point. To compute the variable provenance, we combine program slicing techniques and operational rules associated with mathematical operators in computations to propagate the annotations. We predict that the solution to the\u00a0\u2026", "num_citations": "3\n", "authors": ["2155"]}
{"title": "Sbvr-based business rule creation for legacy programs using variable provenance\n", "abstract": " Functionality of a software system that implements business operations can be captured using business processes and rules. To understand the'as-is' processes and rules, the source-code is arguably the best source of knowledge. We present a novel method that combines program analysis and domain knowledge to create the descriptions for\" IT rules\", as a critical step towards extracting business rules automatically. We introduce and use the concept of'variable provenance'to propagate the domain descriptions into the source code to create Semantics of Business Vocabularies and Rules (SBVR) rules. In our experiments on sample, near-real-life systems, we could successfully annotate very large percentage (> 90%) of IT rules and enable to create SBVR rules. We present and describe the ProgAnnotator tool which is based on variable provenance and generates descriptions for IT rules in the source code and\u00a0\u2026", "num_citations": "2\n", "authors": ["2155"]}
{"title": "BuRRiTo: a framework to extract, specify, verify and analyze business rules\n", "abstract": " An enterprise system operates business by providing various services that are guided by set of certain business rules (BR) and constraints. These BR are usually written using plain Natural Language in operating procedures, terms and conditions, and other documents or in source code of legacy enterprise systems. For implementing the BR in a software system, expressing them as UML use-case specifications, or preparing for Merger & Acquisition (M&A) activity, analysts manually interpret the documents or try to identify constraints from the source code, leading to potential discrepancies and ambiguities. These issues in the software system can be resolved only after testing, which is a very tedious and expensive activity. To minimize such errors and efforts, we propose BuRRiTo framework consisting of automatic extraction of BR by mining documents and source code, ability to clean them of various anomalies like\u00a0\u2026", "num_citations": "1\n", "authors": ["2155"]}
{"title": "Fault Localization during System testing\n", "abstract": " Functional testing of business applications in the enterprise is carried out by independent test teams. Test scripts are generated manually or automatically from requirements, treating the IT systems as a black box. For every release, when test scripts fail to execute, the test teams need to ascertain the cause of failure, which could be due to mismatch between the requirements and the test models and test scripts, or faults in the test scripts or faults in the source code. The process is cumbersome and time consuming. While several techniques have been developed to localize source code faults, these target testing carried out by the developer. To help test teams localize faults, we propose the novel idea of applying source code based fault localization technique to process models that represent the system functionality. Experimental results show that the techniques when applied to models, were able to localize both test\u00a0\u2026", "num_citations": "1\n", "authors": ["2155"]}