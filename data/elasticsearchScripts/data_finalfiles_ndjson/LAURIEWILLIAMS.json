{"title": "Strengthening the case for pair programming\n", "abstract": " The software industry has practiced pair programming (two programmers working side by side at one computer on the same problem) with great success for years, but people who haven't tried it often reject the idea as a waste of resources. The authors demonstrate that using pair programming in the software development process yields better products in less time-and happier, more confident programmers.", "num_citations": "1063\n", "authors": ["144"]}
{"title": "The Costs and Benefits of Pair Programming\n", "abstract": " Pair or collaborative programming is where two programmers develop software side by side at one computer. Using interviews and controlled experiments, the authors investigated the costs and benefits of pair programming. They found that for a development-time cost of about 15%, pair programming improves design quality, reduces defects, reduces staffing risk, enhances technical skills, improves team communications and is considered more enjoyable at statistically significant levels.", "num_citations": "791\n", "authors": ["144"]}
{"title": "Pair programming illuminated\n", "abstract": " At face value, pair programming appears to be a simple, straightforward concept. Two programmers work side-by-side at one computer, continuously collaborating on the same design, algorithm, code, and test. If it was as simple as plopping two skilled programmers at one desktop, there would be no need for this book. However, there are people and personalities involved, and these people are accustomed to programming alone. Pair programming offers significant benefits: quality is increased, time is saved, morale is improved, trust and teamwork grow, knowledge is shared, and learning is enhanced. However, before any pair programming can take place, participants have to accept a new way of thinking. In Pair Programming Illuminated, Laurie Williams and Robert Kessler help you fight through the exceptions, gain collective acceptance of this approach, and experience remarkable success with it. Two case studies show pair programming in practice using Extreme Programming and Collaborative Software Process as methods.", "num_citations": "754\n", "authors": ["144"]}
{"title": "Agile software development: it\u2019s about feedback and change\n", "abstract": " In February 2001, 17 of these practitioners and authors met in Snowbird, Utah, to discuss the fundamental similarities of their experiences and their then-called lightweight methodologies. 2 Finding that their work habits had much in common, they recognized that the lightness in their work process provided a means to achieve the more relevant end: customer satisfaction and high quality. They categorized their methodologies as \u201cagile\u201d\u2014a term with a decade of use in flexible manufacturing practices. The participants wrote the \u201cManifesto for Agile Software Development\u201d(www. agilemanifesto. org), which describes the four comparative values underlying the agile position:\u2022 individuals and interactions over processes and tools,\u2022 working software over comprehensive documentation,\u2022 customer collaboration over contract negotiation, and\u2022 responding to change over following a plan.", "num_citations": "595\n", "authors": ["144"]}
{"title": "Evaluating complexity, code churn, and developer activity metrics as indicators of software vulnerabilities\n", "abstract": " Security inspection and testing require experts in security who think like an attacker. Security experts need to know code locations on which to focus their testing and inspection efforts. Since vulnerabilities are rare occurrences, locating vulnerable code locations can be a challenging task. We investigated whether software metrics obtained from source code and development history are discriminative and predictive of vulnerable code locations. If so, security experts can use this prediction to prioritize security inspection and testing efforts. The metrics we investigated fall into three categories: complexity, code churn, and developer activity metrics. We performed two empirical case studies on large, widely used open-source projects: the Mozilla Firefox web browser and the Red Hat Enterprise Linux kernel. The results indicate that 24 of the 28 metrics collected are discriminative of vulnerabilities for both projects. The\u00a0\u2026", "num_citations": "522\n", "authors": ["144"]}
{"title": "All I really need to know about pair programming I learned in kindergarten\n", "abstract": " Pair programming is a practice in which two programmers work side-by-side at one computer, continuously collaborating on the same design, algorithm, code, or test. This method has been demonstrated to improve productivity and the quality of software products. Moreover, a recent survey (hereafter referred to as \u201cthe pair programming survey\u201d) found that programmers were universally more confident in their solutions when programming in pairs as opposed to working alone. Likewise, 96% agreed they enjoy their jobs more when pair programming [12].However, most programmers are long conditioned to working alone and often resist the transition to pair programming. Ultimately, most make this transition with great success. The goal of this article is to help programmers become effective pair programmers. The transition to and on-going success as a pair programmer often involves practicing everyday civility, as\u00a0\u2026", "num_citations": "405\n", "authors": ["144"]}
{"title": "Assessing test-driven development at IBM\n", "abstract": " In a software development group of IBM Retail Store Solutions, we built a non-trivial software system based on a stable standard specification using a disciplined, rigorous unit testing and build approach based on the test-driven development (TDD) practice. Using this practice, we reduced our defect rate by about 50 percent compared to a similar system that was built using an ad-hoc unit testing approach. The project completed on time with minimal development productivity impact. Additionally, the suite of automated unit test cases created via TDD is a reusable and extendable asset that will continue to improve quality over the lifetime of the software system. The test suite will be the basis for quality checks and will serve as a quality contract between all members of the team.", "num_citations": "391\n", "authors": ["144"]}
{"title": "In support of pair programming in the introductory computer science course\n", "abstract": " A formal pair programming experiment was run at North Carolina to empirically assess the educational efficacy of the technique in a CS1 course. Results indicate that students who practice pair programming perform better on programming projects and are more likely to succeed by completing the class with a C or better. Student pairs are more self-sufficient which reduces their reliance on the teaching staff. Qualitatively, paired students demonstrate higher order thinking skills than students who work alone. These results are supportive of pair programming as a collaborative learning technique.", "num_citations": "334\n", "authors": ["144"]}
{"title": "A structured experiment of test-driven development\n", "abstract": " Test Driven Development (TDD) is a software development practice in which unit test cases are incrementally written prior to code implementation. We ran a set of structured experiments with 24 professional pair programmers. One group developed a small Java program using TDD while the other (control group), used a waterfall-like approach. Experimental results, subject to external validity concerns, tend to indicate that TDD programmers produce higher quality code because they passed 18% more functional black-box test cases. However, the TDD programmers took 16% more time. Statistical analysis of the results showed that a moderate statistical correlation existed between time spent and the resulting quality. Lastly, the programmers in the control group often did not write the required automated test cases after completing their code. Hence it could be perceived that waterfall-like approaches do not\u00a0\u2026", "num_citations": "289\n", "authors": ["144"]}
{"title": "Predicting failures with developer networks and social network analysis\n", "abstract": " Software fails and fixing it is expensive. Research in failure prediction has been highly successful at modeling software failures. Few models, however, consider the key cause of failures in software: people. Understanding the structure of developer collaboration could explain a lot about the reliability of the final product. We examine this collaboration structure with the developer network derived from code churn information that can predict failures at the file level. We conducted a case study involving a mature Nortel networking product of over three million lines of code. Failure prediction models were developed using test and post-release failure data from two releases, then validated against a subsequent release. One model's prioritization revealed 58% of the failures in 20% of the files compared with the optimal prioritization that would have found 61% in 20% of the files, indicating that a significant correlation exists\u00a0\u2026", "num_citations": "285\n", "authors": ["144"]}
{"title": "Test-driven development as a defect-reduction practice\n", "abstract": " Test-driven development is a software development practice that has been used sporadically for decades. With this practice, test cases (preferably automated) are incrementally written before production code is implemented. Test-driven development has recently re-emerged as a critical enabling practice of the extreme programming software development methodology. We ran a case study of this practice at IBM. In the process, a thorough suite of automated test cases was produced after UML design. In this case study, we found that the code developed using a test-driven development practice showed, during functional verification and regression tests, approximately 40% fewer defects than a baseline prior product developed in a more traditional fashion. The productivity of the team was not impacted by the additional focus on producing automated test cases. This test suite aids in future enhancements and\u00a0\u2026", "num_citations": "266\n", "authors": ["144"]}
{"title": "An initial investigation of test driven development in industry\n", "abstract": " Test Driven Development (TDD) is a software development practice in which unit test cases are incrementally written prior to code implementation. In our research, we ran a set of structured experiments with 24 professional pair programmers. One group developed code using TDD while the other a waterfall-like approach. Both groups developed a small Java program. We found that the TDD developers produced higher quality code, which passed 18% more functional black box test cases. However, TDD developer pairs took 16% more time for development. A moderate correlation between time spent and the resulting quality was established upon analysis. It is conjectured that the resulting high quality of code written using the TDD practice may be due to the granularity of TDD, which may encourage more frequent and tighter verification and validation. Lastly, the programmers which followed a waterfall-like\u00a0\u2026", "num_citations": "266\n", "authors": ["144"]}
{"title": "In support of student pair-programming\n", "abstract": " Industry, particularly those following the eXtreme Programming (XP) methodology [2], has popularized the use of pair-programming. The pair-programming model has also been found to be beneficial for student programmers. Initial quantitative and qualitative results, which will be discussed in this paper, demonstrate that the use of pair-programming in the computer science classroom enhances student learning and satisfaction and reduces the frustration common among students. Additionally, the use of pair-programming relieves the burden on the educators because students no longer view the teaching staff as their sole form of technical information. We explore the nature of pair-programming, then examine the ways such a practice may enhance teaching and learning in computer science education.", "num_citations": "265\n", "authors": ["144"]}
{"title": "System test case prioritization of new and regression test cases\n", "abstract": " Test case prioritization techniques have been shown to be beneficial for improving regression-testing activities. With prioritization, the rate of fault detection is improved, thus allowing testers to detect faults earlier in the system-testing phase. Most of the prioritization techniques to date have been code coverage-based. These techniques may treat all faults equally. We build upon prior test case prioritization research with two main goals: (1) to improve user-perceived software quality in a cost effective way by considering potential defect severity and (2) to improve the rate of detection of severe faults during system-level testing of new code and regression testing of existing code. We present a value-driven approach to system-level test case prioritization called the prioritization of requirements for test (PORT). PORT prioritizes system test cases based upon four factors: requirements volatility, customer priority\u00a0\u2026", "num_citations": "251\n", "authors": ["144"]}
{"title": "What agile teams think of agile principles\n", "abstract": " Even after almost a dozen years, they still deliver solid guidance for software development teams and their projects.", "num_citations": "224\n", "authors": ["144"]}
{"title": "The collaborative software process\n", "abstract": " Anecdotal and qualitative evidence from industry indicates that two programmers working side-by-side at one computer, collaborating on the same design, algorithm, code, or test, perform substantially better than the two working alone. Statistical evidence has shown that programmers perform better when following a defined, repeatable process such as the Personal Software Process (PSP). Bringing these two ideas together, the Collaborative Software Process (CSP) has been formulated. The CSP is a defined, repeatable process for two programmers working collaboratively. The CSP is an extension of the PSP, and it relies upon the foundation of the PSP. To validate the effectiveness of CSP, an experiment was run in 1999 with approximately 40 senior Computer Science students at the University of Utah. All students learned both the CSP and the PSP. Two-thirds of the students worked in two-person collaborative teams using the CSP to develop their programming assignments. The other students worked independently using the PSP to develop the same assignments. Additionally, a significant amount of input and confirmation from professional engineers who practice collaborative programming was factored into the research. The research contributed a defined, repeatable process, the Collaborative Software Process, for collaborative programming pairs. The experiment validated the following quantitative findings about collaborative teams using the CSP:", "num_citations": "188\n", "authors": ["144"]}
{"title": "On establishing a benchmark for evaluating static analysis alert prioritization and classification techniques\n", "abstract": " Benchmarks provide an experimental basis for evaluating software engineering processes or techniques in an objective and repeatable manner. We present the FAULTBENCH v0. 1 benchmark, as a contribution to current benchmark materials, for evaluation and comparison of techniques that prioritize and classify alerts generated by static analysis tools. Static analysis tools may generate an overwhelming number of alerts, the majority of which are likely to be false positives (FP). Two FP mitigation techniques, alert prioritization and classification, provide an ordering or classification of alerts, identifying those likely to be anomalies. We evaluate FAULTBENCH using three versions of a FP mitigation technique within the AWARE adaptive prioritization model. Individual FAULTBENCH subjects vary in their optimal FP mitigation techniques. Together, FAULTBENCH subjects provide a precise and general evaluation of\u00a0\u2026", "num_citations": "183\n", "authors": ["144"]}
{"title": "Experiments with industry's \u201cpair-programming\u201d model in the computer science classroom\n", "abstract": " Anecdotal evidence from several sources, primarily in industry, indicates that two programmers working collaboratively on the same design, algorithm, code, or test perform substantially better than the two working alone. Two courses taught at the University of Utah studied the use of this technique, often called pair-programming or collaborative programming, in the undergraduate computer science classroom. The students applied a positive form of \u201cpair-pressure\u201d on each other, which proved beneficial to the quality of their work products. The students also benefit from \u201cpair-learning,\u201d which allowed them to learn new languages faster and better than with solitary learning. The workload of the teaching staff is reduced because the students more often look to each other for technical support and advice.", "num_citations": "173\n", "authors": ["144"]}
{"title": "Can traditional fault prediction models be used for vulnerability prediction?\n", "abstract": " Finding security vulnerabilities requires a different mindset than finding general faults in software\u2014thinking like an attacker. Therefore, security engineers looking to prioritize security inspection and testing efforts may be better served by a prediction model that indicates security vulnerabilities rather than faults. At the same time, faults and vulnerabilities have commonalities that may allow development teams to use traditional fault prediction models and metrics for vulnerability prediction. The goal of our study is to determine whether fault prediction models can be used for vulnerability prediction or if specialized vulnerability prediction models should be developed when both models are built with traditional metrics of complexity, code churn, and fault history. We have performed an empirical study on a widely-used, large open source project, the Mozilla Firefox web browser, where 21% of the source code files\u00a0\u2026", "num_citations": "171\n", "authors": ["144"]}
{"title": "The effects of\" pair-pressure\" and\" pair-learning\" on software engineering education\n", "abstract": " Anecdotal evidence from several sources, primarily in industry, indicates that two programmers working collaboratively on the same design, algorithm, code, or test perform substantially better than the two would working alone. In this technique, often called \"pair programming\" or \"collaborative programming\", one person is the \"driver\" and has control of the pencil/mouse/keyboard and is writing the design or code. The other person continuously and actively observes the work of the driver-watching for defects, thinking of alternatives, looking up resources, and considering strategic implications of the work at hand. A course in Web programming was taught at the University of Utah in Summer Semester 1999. In this course, the students worked in pairs, continuously collaborating on all programming assignments. Using the technique, the students applied a positive form of \"pair-pressure\" on each other, which proved\u00a0\u2026", "num_citations": "171\n", "authors": ["144"]}
{"title": "An empirical model to predict security vulnerabilities using code complexity metrics\n", "abstract": " Complexity is often hypothesized to be the enemy of software security. If this hypothesis is true, complexity metrics may be used to predict the locale of security problems and can be used to prioritize inspection and testing efforts. We performed statistical analysis on nine complexity metrics from the JavaScript Engine in the Mozilla application framework to find differences in code metrics between vulnerable and nonvulnerable code and to predict vulnerabilities. Our initial results show that complexity metrics can predict vulnerabilities at a low false positive rate, but at a high false negative rate.", "num_citations": "170\n", "authors": ["144"]}
{"title": "Agile software development methodologies and practices\n", "abstract": " Beginning in the mid-1990s, a number of consultants independently created and evolved what later came to be known as agile software development methodologies. Agile methodologies and practices emerged as an attempt to more formally and explicitly embrace higher rates of change in software requirements and customer expectations. Some prominent agile methodologies are Adaptive Software Development, Crystal, Dynamic Systems Development Method, Extreme Programming (XP), Feature-Driven Development (FDD), Pragmatic Programming, and Scrum. This chapter presents the principles that underlie and unite the agile methodologies. Then, 32 practices used in agile methodologies are presented. Finally, three agile methodologies (XP, FDD, and Scrum) are explained. Most often, software development teams select a subset of the agile practices and create their own hybrid software development\u00a0\u2026", "num_citations": "169\n", "authors": ["144"]}
{"title": "On understanding compatibility of student pair programmers\n", "abstract": " In recent years, educators have increasingly used pair programming in their computer science courses. Pair programming has been shown to be beneficial for both the teaching staff and the students in the courses. Occasionally, though, students are not compatible with their partners. An extensive study was done at the North Carolina State University to provide guidance on forming student pairs to improve the chances that pairs will be compatible and have a productive work relationship. We examined compatibility among freshman, advanced undergraduate, and graduate students. We have found that the students' perception of their partner's skill level has a significant influence on their compatibility. Graduate students work well with partners of similar actual skill level. Freshmen seem to work better with partners with different Myers Briggs personality type. Students' self-esteem does not appear to be a major\u00a0\u2026", "num_citations": "166\n", "authors": ["144"]}
{"title": "One technique is not enough: A comparison of vulnerability discovery techniques\n", "abstract": " Security vulnerabilities discovered later in the development cycle are more expensive to fix than those discovered early. Therefore, software developers should strive to discover vulnerabilities as early as possible. Unfortunately, the large size of code bases and lack of developer expertise can make discovering software vulnerabilities difficult. To ease this difficulty, many different types of techniques have been devised to aid developers in vulnerability discovery. The goal of this research is to improve vulnerability detection by comparing the effectiveness of vulnerability discovery techniques and to provide specific recommendations to improve vulnerability discovery with these techniques. We conducted a case study on two electronic health record systems to compare four discovery techniques: systematic and exploratory manual penetration testing, static analysis, and automated penetration testing. In our case study\u00a0\u2026", "num_citations": "164\n", "authors": ["144"]}
{"title": "Is complexity really the enemy of software security?\n", "abstract": " Software complexity is often hypothesized to be the enemy of software security. We performed statistical analysis on nine code complexity metrics from the JavaScript Engine in the Mozilla application framework to investigate if this hypothesis is true. Our initial results show that the nine complexity measures have weak correlation (\u03c1= 0.30 at best) with security problems for Mozilla JavaScript Engine. The study should be replicated on more products with design and code-level metrics. It may be necessary to create new complexity metrics to embody the type of complexity that leads to security problems.", "num_citations": "151\n", "authors": ["144"]}
{"title": "Secure open source collaboration: an empirical study of linus' law\n", "abstract": " Open source software is often considered to be secure. One factor in this confidence in the security of open source software lies in leveraging large developer communities to find vulnerabilities in the code. Eric Raymond declares Linus' Law\" Given enough eyeballs, all bugs are shallow.\" Does Linus' Law hold up ad infinitum? Or, can the multitude of developers become\" too many cooks in the kitchen\", causing the system's security to suffer as a result? In this study, we examine the security of an open source project in the context of developer collaboration. By analyzing version control logs, we quantified notions of Linus' Law as well as the\" too many cooks in the kitchen\" viewpoint into developer activity metrics. We performed an empirical case study by examining correlations between the known security vulnerabilities in the open source Red Hat Enterprise Linux 4 kernel and developer activity metrics. Files\u00a0\u2026", "num_citations": "143\n", "authors": ["144"]}
{"title": "A systematic literature review of actionable alert identification techniques for automated static code analysis\n", "abstract": " ContextAutomated static analysis (ASA) identifies potential source code anomalies early in the software development lifecycle that could lead to field failures. Excessive alert generation and a large proportion of unimportant or incorrect alerts (unactionable alerts) may cause developers to reject the use of ASA. Techniques that identify anomalies important enough for developers to fix (actionable alerts) may increase the usefulness of ASA in practice.ObjectiveThe goal of this work is to synthesize available research results to inform evidence-based selection of actionable alert identification techniques (AAIT).MethodRelevant studies about AAITs were gathered via a systematic literature review.ResultsWe selected 21 peer-reviewed studies of AAITs. The techniques use alert type selection; contextual information; data fusion; graph theory; machine learning; mathematical and statistical models; or dynamic detection to\u00a0\u2026", "num_citations": "136\n", "authors": ["144"]}
{"title": "Continuous deployment at Facebook and OANDA\n", "abstract": " Continuous deployment is the software engineering practice of deploying many small incremental software updates into production, leading to a continuous stream of 10s, 100s, or even 1,000s of deployments per day. High-profile Internet firms such as Amazon, Etsy, Facebook, Flickr, Google, and Netflix have embraced continuous deployment. However, the practice has not been covered in textbooks and no scientific publication has presented an analysis of continuous deployment. In this paper, we describe the continuous deployment practices at two very different firms: Facebook and OANDA. We show that continuous deployment does not inhibit productivity or quality even in the face of substantial engineering team and code size growth. To the best of our knowledge, this is the first study to show it is possible to scale the size of an engineering team by 20X and the size of the code base by 50X without negatively\u00a0\u2026", "num_citations": "129\n", "authors": ["144"]}
{"title": "Sangam: a distributed pair programming plug-in for Eclipse\n", "abstract": " In pair programming, two programmers traditionally work side-by-side at one computer. However, in globally distributed organizations, long-distance collaboration is frequently necessary. Sangam is an Eclipse plug-in that allows Eclipse users in different locations to share a workspace so that they may work as if they were using the same computer. In this paper, we discuss the Sangam plug-in, and our experience developing it via distributed and collocated pair programming.", "num_citations": "118\n", "authors": ["144"]}
{"title": "Integrating pair programming into a software development process\n", "abstract": " Anecdotal and statistical evidence indicates that pair programmers - two programmers working side-by-side at one computer collaborating on the same design, algorithm, code or test - outperform individual programmers. One of the programmers (the driver) has control of the keyboard/mouse and actively implements the program. The other programmer (the observer) continuously observes the work of the driver to identify tactical (syntactic, spelling, etc.) defects, and also thinks strategically about the direction of the work. On demand, the two programmers can brainstorm any challenging problem. Because the two programmers periodically switch roles, they work together as equals to develop software. This practice of pair programming can be integrated into any software development process. As an example, this paper describes the changes that were made to the Personal Software Process (PSP) to leverage the\u00a0\u2026", "num_citations": "115\n", "authors": ["144"]}
{"title": "Automated extraction of non-functional requirements in available documentation\n", "abstract": " While all systems have non-functional requirements (NFRs), they may not be explicitly stated in a formal requirements specification. Furthermore, NFRs may also be externally imposed via government regulations or industry standards. As some NFRs represent emergent system proprieties, those NFRs require appropriate analysis and design efforts to ensure they are met. When the specified NFRs are not met, projects incur costly re-work to correct the issues. The goal of our research is to aid analysts in more effectively extracting relevant non-functional requirements in available unconstrained natural language documents through automated natural language processing. Specifically, we examine which document types (data use agreements, install manuals, regulations, request for proposals, requirements specifications, and user manuals) contain NFRs categorized to 14 NFR categories (e.g. capacity, reliability\u00a0\u2026", "num_citations": "110\n", "authors": ["144"]}
{"title": "Voices of women in a software engineering course: Reflections on collaboration\n", "abstract": " Those science, mathematics, and engineering faculty who are serious about making the education they offer as available to their daughters as to their sons are, we posit, facing the prospect of dismantling a large part of its traditional pedagogical structure, along with the assumptions and practice which support it. [Seymour and Hewett 1997].Prior research indicates that female students can be concerned about the insularity of working alone for long periods of time, as they perceive to be the case with computer science and information technology careers. We studied an advanced undergraduate software engineering course at North Carolina State University to characterize the potential of collaborative learning environments created via pair-programming and agile software development to ameliorate this concern. A collective case study of three representative women in the course revealed that they held the following\u00a0\u2026", "num_citations": "108\n", "authors": ["144"]}
{"title": "Challenges with applying vulnerability prediction models\n", "abstract": " Vulnerability prediction models (VPM) are believed to hold promise for providing software engineers guidance on where to prioritize precious verification resources to search for vulnerabilities. However, while Microsoft product teams have adopted defect prediction models, they have not adopted vulnerability prediction models (VPMs). The goal of this research is to measure whether vulnerability prediction models built using standard recommendations perform well enough to provide actionable results for engineering resource allocation. We define'actionable'in terms of the inspection effort required to evaluate model results. We replicated a VPM for two releases of the Windows Operating System, varying model granularity and statistical learners. We reproduced binary-level prediction precision (~ 0.75) and recall (~ 0.2). However, binaries often exceed 1 million lines of code, too large to practically inspect, and\u00a0\u2026", "num_citations": "102\n", "authors": ["144"]}
{"title": "Prioritizing software security fortification throughcode-level metrics\n", "abstract": " Limited resources preclude software engineers from finding and fixing all vulnerabilities in a software system. We create predictive models to identify which components are likely to have the most security risk. Software engineers can use these models to make measurement-based risk management decisions and to prioritize software security fortification efforts, such as redesign and additional inspection and testing. We mined and analyzed data from a large commercial telecommunications software system containing over one million lines of code that had been deployed to the field for two years. Using recursive partitioning, we built attack-prone prediction models with the following code-level metrics: static analysis tool alert density, code churn, and count of source lines of code. One model identified 100% of the attack-prone components (40% of the total number of components) with an 8% false positive rate. As\u00a0\u2026", "num_citations": "102\n", "authors": ["144"]}
{"title": "Computer science attitude survey\n", "abstract": " A survey was developed to measure attitudes towards computer programming and computer science in general. This instrument was derived from the Fennema-Sherman mathematics attitudes scales (Fennema, 1976), modified to reflect programming and computer science rather than mathematics. The survey consists of a series of positive and negative statements. Participants respond to these statements on a five-point scale, ranging from strongly agree to strongly disagree. The negative statements are reverse coded prior to summing the subscale scores. The survey uses five of the seven subscale categories used in the Fennema-Sherman instrument. In addition, the survey opens with a statement concerning the participant\u2019s intent to major in computer science. The reliability of the new instrument was evaluated for internal consistency of the subscales with the responses from 162 students at the beginning of the semester taking an introductory computer science course (Williams, Wiebe, Yang, Ferzli & Miller, 2002). Values of Cronbach\u2019s alpha ranged from 0.83 and 0.91 for the five subscales. The five subscales, the survey questions that belong to each subscale, and the alpha score are given in Table 1.", "num_citations": "85\n", "authors": ["144"]}
{"title": "Socio-technical developer networks: Should we trust our measurements?\n", "abstract": " Software development teams must be properly structured to provide effectiv collaboration to produce quality software. Over the last several years, social network analysis (SNA) has emerged as a popular method for studying the collaboration and organization of people working in large software development teams. Researchers have been modeling networks of developers based on socio-technical connections found in software development artifacts. Using these developer networks, researchers have proposed several SNA metrics that can predict software quality factors and describe the team structure. But do SNA metrics measure what they purport to measure? The objective of this research is to investigate if SNA metrics represent socio-technical relationships by examining if developer networks can be corroborated with developer perceptions. To measure developer perceptions, we developed an online survey\u00a0\u2026", "num_citations": "83\n", "authors": ["144"]}
{"title": "On the economics of requirements-based test case prioritization\n", "abstract": " Software testing is a strenuous and expensive process. At least 50% of the total software cost is spent on testing activities [12]. Companies are often faced with time and resource constraints that limit their ability to effectively complete testing efforts. Companies generally save suites for reuse; test suite reuse accounts for almost half of the maintenance cost [9]. As the product goes thru several versions, executing all the test cases in a test suite can be expensive [9]. Prioritization of test cases can be cost effective when the time allocated to complete testing is limited [9]. Test case prioritization (TCP) involves the explicit planning of the execution of test cases in a specific order and is shown to improve the rate of fault detection [3, 9]. The current software TCP techniques are primarily coverage-based (statement, branch or other coverage) [3,9]. Coverage-based white-box prioritization techniques are most applicable for\u00a0\u2026", "num_citations": "82\n", "authors": ["144"]}
{"title": "Prioritization of regression tests using singular value decomposition with empirical change records\n", "abstract": " During development and testing, changes made to a system to repair a detected fault can often inject a new fault into the code base. These injected faults may not be in the same files that were just changed, since the effects of a change in the code base can have ramifications in other parts of the system. We propose a methodology for determining the effect of a change and then prioritizing regression test cases by gathering software change records and analyzing them through singular value decomposition. This methodology generates clusters of files that historically tend to change together. Combining these clusters with test case information yields a matrix that can be multiplied by a vector representing a new system modification to create a prioritized list of test cases. We performed a post hoc case study using this technique with three minor releases of a software product at IBM. We found that our methodology\u00a0\u2026", "num_citations": "78\n", "authors": ["144"]}
{"title": "On the sustained use of a test-driven development practice at ibm\n", "abstract": " Test-driven development (TDD) is an agile practice that is widely accepted and advocated by most agile methods and methodologists. In this paper, we report on a post hoc analysis of the results of an IBM team who has sustained use of TDD for five years and over ten releases of a Java-implemented product. The team worked from a design and wrote tests incrementally before or while they wrote code and, in the process, developed a significant asset of automated tests. The IBM team realized sustained quality improvement relative to a pre-TDD project and consistently had defect density below industry standards. As a result, our data indicate that the TDD practice can aid in the production of high quality products. This quality improvement would compensate for the moderate perceived productivity losses. Additionally, our data indicates that the use of TDD may decrease the degree to which code complexity\u00a0\u2026", "num_citations": "75\n", "authors": ["144"]}
{"title": "Towards increasing the compatibility of student pair programmers\n", "abstract": " As pair programming is used widely in software engineering education, instructors may wish to proactively form pairs to increase the likelihood of compatible pairs. A study involving 361 software engineering students was carried out at North Carolina State University to understand and predict pair compatibility. We have found that students are compatible with partners whom they perceive of similar skill, although instructors cannot proactively manage this perception. Pairing of two minority students is more likely and mixed gender pairs are less likely to be compatible. Additionally, pairing of students with similar actual skill level as measured by midterm grades in class, GPA, and SAT/GRE scores also likely results in compatible pairs. Our research addresses the following challenges faced by instructors in software engineering: 1) organizational concern in pairing of students; 2) increasing the retention rates of female\u00a0\u2026", "num_citations": "75\n", "authors": ["144"]}
{"title": "Protection poker: The new software security\" game\"\n", "abstract": " Without infinite resources, software development teams must prioritize security fortification efforts to prevent the most damaging attacks. The Protection Poker \"game\" is a collaborative means for guiding this prioritization and has the potential to improve software security practices and team software security knowledge.", "num_citations": "73\n", "authors": ["144"]}
{"title": "Requirements-based test case prioritization\n", "abstract": " Test case prioritization techniques have been shown to improve regression-testing activities by increasing the rate of fault detection, thus allowing testers to fix faults earlier. The current techniques, mostly code coveragebased, treat all faults equally. We build upon this work to develop a prioritization scheme with two main goals: identifying the severe faults earlier and minimizing the cost of test case prioritization.", "num_citations": "73\n", "authors": ["144"]}
{"title": "A model building process for identifying actionable static analysis alerts\n", "abstract": " Automated static analysis can identify potential source code anomalies early in the software process that could lead to field failures. However, only a small portion of static analysis alerts may be important to the developer (actionable). The remainder are false positives (unactionable). We propose a process for building false positive mitigation models to classify static analysis alerts as actionable or unactionable using machine learning techniques. For two open source projects, we identify sets of alert characteristics predictive of actionable and unactionable alerts out of 51 candidate characteristics. From these selected characteristics, we evaluate 15 machine learning algorithms, which build models to classify alerts. We were able to obtain 88-97% average accuracy for both projects in classifying alerts using three to 14 alert characteristics. Additionally, the set of selected alert characteristics and best models differed\u00a0\u2026", "num_citations": "70\n", "authors": ["144"]}
{"title": "Applying regression test selection for COTS-based applications\n", "abstract": " ABB incorporates a variety of commercial-off-the-shelf (COTS) components in its products. When new releases of these components are made available for integration and testing, source code is often not provided. Various regression test selection processes have been developed and have been shown to be cost effectiveness. However, the majority of these test selection techniques rely on access to source code for change identification. In this paper we present the application of the lightweight Integrated-Black-box Approach for Component Change Identification (I-BACCI) Version 3 process that select regression tests for applications that use COTS components. Two case studies, examining a total of nine new component releases, were conducted at ABB on products written in C/C++ to determine the effectiveness of I-BACCI. The results of the case studies indicate this process can reduce the required number of\u00a0\u2026", "num_citations": "70\n", "authors": ["144"]}
{"title": "Evolving beyond requirements creep: a risk-based evolutionary prototyping model\n", "abstract": " Evolutionary prototyping focuses on gathering a correct and consistent set of requirements. The process lends particular strength to building quality software by means of the ongoing clarification of existing requirements and the discovery of previously missing or unknown requirements. Traditionally, the iterative reexamination of a systems requirements has not been the panacea that practitioners sought, due to the predisposition for requirements creep and the difficulty in managing it. The paper proposes the combination of evolutionary prototyping and an aggressive risk mitigation strategy. Together, these techniques support successful requirements discovery and clarification, and they guard against the negative effects of requirements creep. We embody these techniques in a comprehensive software development model, which we call the EPRAM (Evolutionary Prototyping with Risk Analysis and Mitigation) model\u00a0\u2026", "num_citations": "70\n", "authors": ["144"]}
{"title": "Matching attack patterns to security vulnerabilities in software-intensive system designs\n", "abstract": " Fortifying software applications from attack is often an effort that occurs late in the software development process. Applying patches to fix vulnerable applications in the field is a common approach to securing applications. Abstract representations of attacks such as attack trees and attack nets can be used for identifying potential threats before a system is released. We have constructed attack patterns that can illuminate security vulnerabilities in a software-intensive system design. Matching our attack patterns to vulnerabilities in the design phase may stimulate security efforts to start early and to become integrated with the software process. The intent is that our attack patterns can be used to effectively encode software vulnerabilities in vulnerability databases. A case study of our approach with undergraduate students in a security course indicated that our attack patterns can provide general descriptions of\u00a0\u2026", "num_citations": "67\n", "authors": ["144"]}
{"title": "Hidden in plain sight: Automatically identifying security requirements from natural language artifacts\n", "abstract": " Natural language artifacts, such as requirements specifications, often explicitly state the security requirements for software systems. However, these artifacts may also imply additional security requirements that developers may overlook but should consider to strengthen the overall security of the system. The goal of this research is to aid requirements engineers in producing a more comprehensive and classified set of security requirements by (1) automatically identifying security-relevant sentences in natural language requirements artifacts, and (2) providing context-specific security requirements templates to help translate the security-relevant sentences into functional security requirements. Using machine learning techniques, we have developed a tool-assisted process that takes as input a set of natural language artifacts. Our process automatically identifies security-relevant sentences in the artifacts and classifies\u00a0\u2026", "num_citations": "66\n", "authors": ["144"]}
{"title": "Teaching PSP: Challenges and lessons learned\n", "abstract": " Software engineering educators need to provide environments where students learn about the size and complexity of modern software systems and the techniques available for managing these difficulties. Five universities used the personal software process to teach software engineering concepts in a variety of contexts.", "num_citations": "65\n", "authors": ["144"]}
{"title": "The top 10 adages in continuous deployment\n", "abstract": " Continuous deployment involves automatically testing incremental software changes and frequently deploying them to production environments. With it, developers' changes can reach customers in days or even hours. Such ultrafast changes create a new reality in software development. To understand the emerging practices surrounding continuous deployment, researchers facilitated a one-day Continuous Deployment Summit at the Facebook campus in July 2015, at which participants from 10 companies described how they used continuous deployment. From the resulting conversation, the researchers derived 10 adages about continuous-deployment practices. These adages represent a working set of approaches and beliefs that guide current practice and establish a tangible target for empirical validation by the research community.", "num_citations": "64\n", "authors": ["144"]}
{"title": "The xp programmer: the few-minutes programmer\n", "abstract": " XP programmers gather requirements as short, natural-language statements that are essentially the customer\u2019s words written on small index cards. Called user stories, these cards are commitments for further conversation between the customer and developers and are not intended to completely specify requirements. Generally, they represent desired working code that the developers can produce for the customer to try, as opposed to documents, designs, or database schemas customers can only review.The customers prioritize their user stories, and the developers estimate how long implementing the requirement will take. If an estimate exceeds the length of one iteration (which is typically one to three weeks), the customer and developer work together to break the user story into multiple stories. Ultimately, each user story\u2019s developer estimate will be no longer than one iteration.", "num_citations": "64\n", "authors": ["144"]}
{"title": "Extreme programming for software engineering education?\n", "abstract": " The eXtreme Programming (XP) software development methodology, has received considerable attention in recent years. The adherents of XP anecdotally extol its benefits, particularly as a method that is highly responsive to changing customer's desires. While XP has acquired numerous vocal advocates, the interactions and dependencies between XP practices have not been adequately studied. Good software engineering practice requires expertise in a complex set of activities that involve the intellectual skills of planning, designing, evaluating, and revising. The authors explore the practices of XP in the context of software engineering education. To do so, one must examine the practices of XP as they influence the acquisition of software engineering skills. The practices of XP, in combination or isolation, may provide critical features to aid or hinder the development of increasingly capable practitioners. This paper\u00a0\u2026", "num_citations": "64\n", "authors": ["144"]}
{"title": "Empirical software change impact analysis using singular value decomposition\n", "abstract": " Verification and validation techniques often generate various forms of software development artifacts. Change records created from verification and validation efforts show how files in the system tend to change together in response to fixes for identified faults and failures. We propose a methodology for determining the impact of a new system modification by analyzing software change records through singular value decomposition. This methodology generates clusters of files that historically tend to change together to address faults and failures found in the code base. We performed a post hoc case study using this technique on five open source software systems. We determined that our technique was effective in identifying impacted files in a system from an introduced change when the developers tended to make small, targeted updates to the source system regularly. We further compared our technique against two\u00a0\u2026", "num_citations": "63\n", "authors": ["144"]}
{"title": "Exploring pair programming in distributed object-oriented team projects\n", "abstract": " Previous research [1, 4] has indicated that pair programming is better than individual programming when the pairs are physically colocated. However, important questions arise: How effective is pair programming if the pairs are not physically next to each other? What if the programmers are geographically distributed? An experiment was conducted at North Carolina State University to compare the different working arrangements of student teams developing objectoriented software. The teams were both colocated and in distributed environments; some teams practiced pair programming while others did not. The results of the experiment indicate that it is feasible to develop software using distributed pair programming, and that the resulting software is comparable to software developed in colocated or virtual teams. Our findings will be of significant help to educators dealing with team projects for distance-learning students, as well as organizations that are involved in distributed development of software.", "num_citations": "62\n", "authors": ["144"]}
{"title": "Adapting extreme programming for a core software engineering course\n", "abstract": " Over a decade ago, the manufacturing industry determined it needed to be more agile to thrive and prosper in a changing, nonlinear, uncertain and unpredictable business environment The software engineering community has come to the same realization. A group of software methodologists has created a set of software development processes, termed agile methodologies that have been specifically designed to respond to the demands of the turbulent software industry. Each of the processes in the set of agile processes comprises a set of practices. As educators, we must assess the emerging agile practices, integrate them into our courses (carefully), and share our experiences and results from doing so. The paper discusses the use of extreme programming, a popular agile methodology, in a senior software engineering course at North Carolina State University. It then provides recommendations for integrating\u00a0\u2026", "num_citations": "62\n", "authors": ["144"]}
{"title": "Advanced source apportionment of size-resolved trace elements at multiple sites in London during winter\n", "abstract": " Trace element measurements in PM 10\u20132.5, PM 2.5\u20131.0 and PM 1.0\u20130.3 aerosol were performed with 2 h time resolution at kerbside, urban background and rural sites during the ClearfLo winter 2012 campaign in London. The environment-dependent variability of emissions was characterized using the Multilinear Engine implementation of the positive matrix factorization model, conducted on data sets comprising all three sites but segregated by size. Combining the sites enabled separation of sources with high temporal covariance but significant spatial variability. Separation of sizes improved source resolution by preventing sources occurring in only a single size fraction from having too small a contribution for the model to resolve. Anchor profiles were retrieved internally by analysing data subsets, and these profiles were used in the analyses of the complete data sets of all sites for enhanced source apportionment. A total of nine different factors were resolved (notable elements in brackets): in PM 10\u20132.5, brake wear (Cu, Zr, Sb, Ba), other traffic-related (Fe), resuspended dust (Si, Ca), sea/road salt (Cl), aged sea salt (Na, Mg) and industrial (Cr, Ni); in PM 2.5\u20131.0, brake wear, other traffic-related, resuspended dust, sea/road salt, aged sea salt and S-rich (S); and in PM 1.0\u20130.3, traffic-related (Fe, Cu, Zr, Sb, Ba), resuspended dust, sea/road salt, aged sea salt, reacted Cl (Cl), S-rich and solid fuel (K, Pb). Human activities enhance the kerb-to-rural concentration gradients of coarse aged sea salt, typically considered to have a natural source, by 1.7\u20132.2. These site-dependent concentration differences reflect the effect of local resuspension processes\u00a0\u2026", "num_citations": "61\n", "authors": ["144"]}
{"title": "Characterizing experimentation in continuous deployment: a case study on bing\n", "abstract": " The practice of continuous deployment enables product teams to release content to end users within hours or days, rather than months or years. These faster deployment cycles, along with rich product instrumentation, allows product teams to capture and analyze feature usage measurements. Product teams define a hypothesis and a set of metrics to assess how a code or feature change will impact the user. Supported by a framework, a team can deploy that change to subsets of users, enabling randomized controlled experiments. Based on the impact of the change, the product team may decide to modify the change, to deploy the change to all users, or to abandon the change. This experimentation process enables product teams to only deploy the changes that positively impact the user experience. The goal of this research is to aid product teams to improve their deployment process through providing an empirical\u00a0\u2026", "num_citations": "57\n", "authors": ["144"]}
{"title": "Approximating attack surfaces with stack traces\n", "abstract": " Security testing and reviewing efforts are a necessity for software projects, but are time-consuming and expensive to apply. Identifying vulnerable code supports decision-making during all phases of software development. An approach for identifying vulnerable code is to identify its attack surface, the sum of all paths for untrusted data into and out of a system. Identifying the code that lies on the attack surface requires expertise and significant manual effort. This paper proposes an automated technique to empirically approximate attack surfaces through the analysis of stack traces. We hypothesize that stack traces from user-initiated crashes have several desirable attributes for measuring attack surfaces. The goal of this research is to aid software engineers in prioritizing security efforts by approximating the attack surface of a system via stack trace analysis. In a trial on Windows 8, the attack surface approximation\u00a0\u2026", "num_citations": "56\n", "authors": ["144"]}
{"title": "On agile performance requirements specification and testing\n", "abstract": " Underspecified performance requirements can cause performance issues in a software system. However, a complete, upfront analysis of a software system is difficult, and usually not desirable. We propose an evolutionary model for performance requirements specifications and corresponding validation testing. The principles of the model can be integrated into agile development methods. Using this approach, the performance requirements and test cases can be specified incrementally, without big upfront analysis. We also provide a post hoc examination of a development effort at IBM that had a high focus on performance requirements. The examination indicates that our evolutionary model can be used to specify performance requirements such that the level of detail is commensurate with the nature of the project. Additionally, the IBM experience indicates that test driven development-type validation testing\u00a0\u2026", "num_citations": "55\n", "authors": ["144"]}
{"title": "An initial exploration of the relationship between pair programming and Brooks' law\n", "abstract": " Through his law, \"adding manpower to a late software project makes it later,'' Brooks asserts that the assimilation, training, and intercommunication costs of adding new team members outweigh the associated team productivity gain in the short term. Anecdotes suggest that adding manpower to a late project yields productivity gains to the team more quickly if the team employs the pair programming technique when compared to teams where new team members work alone. We utilize a system dynamics model which demonstrates support of these observations. Parameter values for the model were obtained via a small-scale, nonprobabilistic, convenience survey. Our initial findings suggest that managers should incorporate the pair programming practice when growing their team.", "num_citations": "54\n", "authors": ["144"]}
{"title": "A comparison of the efficiency and effectiveness of vulnerability discovery techniques\n", "abstract": " ContextSecurity vulnerabilities discovered later in the development cycle are more expensive to fix than those discovered early. Therefore, software developers should strive to discover vulnerabilities as early as possible. Unfortunately, the large size of code bases and lack of developer expertise can make discovering software vulnerabilities difficult. A number of vulnerability discovery techniques are available, each with their own strengths.ObjectiveThe objective of this research is to aid in the selection of vulnerability discovery techniques by comparing the vulnerabilities detected by each and comparing their efficiencies.MethodWe conducted three case studies using three electronic health record systems to compare four vulnerability discovery techniques: exploratory manual penetration testing, systematic manual penetration testing, automated penetration testing, and automated static analysis.ResultsIn our case\u00a0\u2026", "num_citations": "51\n", "authors": ["144"]}
{"title": "Lessons learned from seven years of pair programming at North Carolina State University\n", "abstract": " A recent survey conducted on the SIGCSE mailing list indicated that up to 80% of CS1, CS2, and data structures instructors allow students to collaborate. The use of collaboration increases as students advance through the computer science curriculum. Some computer science educators use pair programming as the model for their student collaboration, sometimes with mixed results. At North Carolina State University, over a thousand students have pair programmed in CS1, undergraduate software engineering, and graduate level courses over the last seven years. This paper provides a summary of the lessons we have learned through experience and through extensive research over this period.", "num_citations": "51\n", "authors": ["144"]}
{"title": "Strengthening the empirical analysis of the relationship between Linus' Law and software security\n", "abstract": " Open source software is often considered to be secure because large developer communities can be leveraged to find and fix security vulnerabilities. Eric Raymond states Linus' Law as\" many eyes make all bugs shallow\", reasoning that a diverse set of perspectives improves the quality of a software product. However, at what point does the multitude of developers become\" too many cooks in the kitchen\", causing the system's security to suffer as a result? In a previous study, we quantified Linus' Law and\" too many cooks in the kitchen\" with developer activity metrics and found a statistical association between these metrics and security vulnerabilities in the Linux kernel. In the replication study reported in this paper, we performed our analysis on two additional projects: the PHP programming language and the Wireshark network protocol analyzer. We also updated our Linux kernel case study with 18 additional\u00a0\u2026", "num_citations": "50\n", "authors": ["144"]}
{"title": "Pair programming in an introductory computer science course: Initial results and recommendations\n", "abstract": " Prior research indicates that pair programming, whereby two programmers work collaboratively on the same design, algorithm, code, or test, produces higher quality code in essentially half the time taken by solo programmers. An experiment was run at North Carolina to assess the efficacy of pair programming in the introductory CS1 course. Results indicate that relative to students who program individually, pair programmers are more self-sufficient, perform better on projects, and are more likely to complete the class with a C or better", "num_citations": "49\n", "authors": ["144"]}
{"title": "Synchronization of group state data when rejoining a member to a primary-backup group in a clustered computer system\n", "abstract": " An apparatus, program product and method to synchronize group state data in a primary-backup group in connection with the rejoining of a member to the primary-backup group in a clustered computer system. Each member in the group includes a copy of replicated group state data for the primary-backup group. In connection with rejoining the member, it is determined whether the rejoining member is the primary member for the primary-backup group. Then, a selection is made between member and group overwrite operations based upon such determination. The member overwrite operation includes overwriting the copy of the replicated group state data for the rejoining member with data from the copy of the replicated group state data for an existing member in the primary-backup group. The group overwrite operation includes overwriting the copy of the replicated group state data for the existing member in the\u00a0\u2026", "num_citations": "43\n", "authors": ["144"]}
{"title": "SQLUnitgen: Test case generation for SQL injection detection\n", "abstract": " More than half of all of the vulnerabilities reported can be classified as input manipulation, such as SQL injection, cross site scripting, and buffer overflows. Increasingly, automated static analysis tools are being used to identify input manipulation vulnerabilities. However, these tools cannot detect the presence or the effectiveness of black or white list input filters and, therefore, may have a high false positive rate. Our research objective is to facilitate the identification of true input manipulation vulnerabilities via the combination of static analysis, runtime detection, and automatic testing. We propose an approach for SQL injection vulnerability detection, automated by a prototype tool SQLUnitGen. We performed case studies on two small web applications for the evaluation of our approach compared to static analysis for identifying true SQL injection vulnerabilities. In our case study, SQLUnitGen had no false positives, but had a small number of false negatives while the static analysis tool had a false positive for every vulnerability that was actually protected by a white or black list. Future work will focus on removing false negatives from SQLUnitGen and at generalizing the approach for other types of input manipulation vulnerabilities.", "num_citations": "43\n", "authors": ["144"]}
{"title": "Driving process improvement via comparative agility assessment\n", "abstract": " Rather than striving to be \u201cperfectly agile, \u201d some organizations desire to be more agile than their competition and/or the industry. The Comparative Agility\u2122 (CA) assessment tool can be used to aid organizations in determining their relative agility compared with other teams who responded to CA. The results of CA can be used by a team to guide process improvement related to the use of agile software development practices. This paper provides an overview of industry trends in agility based upon 1, 235 CA respondents in a range of domains and geographical locations. Additionally, the paper goes further in depth on the results of four industrial teams who responded to the CA, explaining why their results were relatively high or low based upon experiences with the teams. The paper also discusses the resultant process improvement reactions and plans of these teams subsequent to reviewing their CA results.", "num_citations": "41\n", "authors": ["144"]}
{"title": "On pair rotation in the computer science course\n", "abstract": " In a course environment, pairing a student with one partner for the entire semester is beneficial, but may not be optimal. We conduct a study in two undergraduate level courses to observe the advantages and disadvantages of pair rotation whereby a student pairs with several different students throughout the semester. We summarize teaching staff and student perceptions on the viability of pair rotation. Teachers find pair rotation valuable because the teaching staff can obtain multiple peer evaluations on each student and because dysfunctional pairs are regularly disbanded. However, pair rotation adds to the burden of assigning pairs multiple times per semester. The majority of students in the study perceived pair rotation to be a desirable approach. Additionally, most students considered peer evaluation to be an effective means of providing feedback to teaching staff. However, they did not significantly believe that\u00a0\u2026", "num_citations": "41\n", "authors": ["144"]}
{"title": "Pair Programming.\n", "abstract": " Pair programming is a style of programming in which two programmers work side-by-side at one computer, continuously collaborating on the same design, algorithm, code, or test. In industry, the practice of pair programming has been shown to improve product quality, improve team spirit, aid in knowledge management, and reduce product risk. In education, pair programming also improves student morale, helps students to be more successful, and improves student retention in an information technology major. This chapter provides an overview and history of pair programming followed by a summary of the use of pair programming in industry and academia. The chapter also provides insight into the principles that make pair programming successful, the economics of pair programming, and the challenges in the adoption of pair programming.", "num_citations": "40\n", "authors": ["144"]}
{"title": "On the design of more secure software-intensive systems by use of attack patterns\n", "abstract": " Retrofitting security implementations to a released software-intensive system or to a system under development may require significant architectural or coding changes. These late changes can be difficult and more costly than if performed early in the software process. We have created regular expression-based attack patterns that show the sequential events that occur during an attack. By performing a Security Analysis for Existing Threats (SAFE-T), software engineers can match the symbols of a regular expression to their system design. An architectural analysis that identifies security vulnerabilities early in the software process can prepare software engineers for which security implementations are necessary when coding starts. A case study involving students in an upper-level undergraduate security course suggests that SAFE-T can be performed by relatively inexperienced engineers who are not experts in\u00a0\u2026", "num_citations": "40\n", "authors": ["144"]}
{"title": "On preparing students for distributed software development with a synchronous, collaborative development platform\n", "abstract": " Working remotely is becoming the norm for both professionals and students alike. Software development has become a global industry due to outsourcing, teleworking, flex time, and companies' desire to use the best and/or most economical talent regardless of where that talent is located. Professionals are not alone because students usually work from home despite having sufficient resources on campus. In this paper we share our experiences from using Jazz, a synchronous, collaborative development platform, with our inevitably distributed software engineering students. Eleven students optionally used the tool while working on a five-week team project. Students primarily used the version control, chat, and work item features in Jazz. We collected their reactions in retrospective essays and found that all Jazz students supported using Jazz in future semesters of the course. We also examined grade differences and\u00a0\u2026", "num_citations": "38\n", "authors": ["144"]}
{"title": "How have we evaluated software pattern application? A systematic mapping study of research design practices\n", "abstract": " ContextSoftware patterns encapsulate expert knowledge for constructing successful solutions to recurring problems. Although a large collection of software patterns is available in literature, empirical evidence on how well various patterns help in problem solving is limited and inconclusive. The context of these empirical findings is also not well understood, limiting applicability and generalizability of the findings.ObjectiveTo characterize the research design of empirical studies exploring software pattern application involving human participants.MethodWe conducted a systematic mapping study to identify and analyze 30 primary empirical studies on software pattern application, including 24 original studies and 6 replications. We characterize the research design in terms of the questions researchers have explored and the context of empirical research efforts. We also classify the studies in terms of measures used for\u00a0\u2026", "num_citations": "37\n", "authors": ["144"]}
{"title": "Predicting attack-prone components\n", "abstract": " Limited resources preclude software engineers from finding and fixing all vulnerabilities in a software system. This limitation necessitates security risk management where security efforts are prioritized to the highest risk vulnerabilities that cause the most damage to the end user. We created a predictive model that identifies the software components that pose the highest security risk in order to prioritize security fortification efforts. The input variables to our model are available early in the software life cycle and include security-related static analysis tool warnings, code churn and size, and faults identified by manual inspections. These metrics are validated against vulnerabilities reported by testing and those found in the field. We evaluated our model on a large Cisco software system and found that 75.6% of the system's vulnerable components are in the top 18.6% of the components predicted to be vulnerable. The\u00a0\u2026", "num_citations": "37\n", "authors": ["144"]}
{"title": "Debunking the nerd stereotype with pair programming\n", "abstract": " Our studies show that using pair programming as a structure for incorporating collaboration in the classroom helps increase and broaden participation in computing fields and helps debunk the myth that programmers work alone all the time. It's also a way for students to get a better view of and feel more confident in their preparation for working in the real world. The face of the IT workforce is changing. As the millennial generation makes its way into the working world, the archetype of the nerd as the introverted, obsessed computer programmer must share equal space with the chatty, social software engineer.", "num_citations": "37\n", "authors": ["144"]}
{"title": "Integrating agile practices into software engineering courses\n", "abstract": " Agile software development methodologies are gaining popularity in industry although they comprise a mix of accepted and controversial software engineering practices. It is quite likely that the software industry will find that specific project characteristics will determine the prudence of using an agile or a plan-driven methodology \u2013 or a hybrid of the two. Educators must assess the value and applicability of these emerging agile practices and decide what role they have in software engineering curricula. This paper provides a brief overview of several agile methodologies, including a discussion of evaluative research of agile practices in academia. The paper also considers instructional issues related to agile methods and the impact of agile methodologies on existing curricular references such as SWEBOK.", "num_citations": "36\n", "authors": ["144"]}
{"title": "Toward the use of automated static analysis alerts for early identification of vulnerability-and attack-prone components\n", "abstract": " Extensive research has shown that software metrics can be used to identify fault- and failure-prone components. These metrics can also give early indications of overall software quality. We seek to parallel the identification and prediction of fault- and failure-prone components in the reliability context with vulnerability- and attack-prone components in the security context. Our research will correlate the quantity and severity of alerts generated by source code static analyzers to vulnerabilities discovered by manual analyses and testing. A strong correlation may indicate that automated static analyzers (ASA), a potentially early technique for vulnerability identification in the development phase, can identify high risk areas in the software system. Based on the alerts, we may be able to predict the presence of more complex and abstract vulnerabilities involved with the design and operation of the software system. An early\u00a0\u2026", "num_citations": "34\n", "authors": ["144"]}
{"title": "Regression test selection for black-box dynamic link library components\n", "abstract": " Software products are often configured with commercial-off-the-shelf (COTS) components. When new releases of these components are made available for integration and testing, source code is usually not provided. Various regression test selection processes have been developed and have been shown to be cost effective. However, the majority of these test selection techniques rely on access to source code for change identification. Based on our prior work, we are studying the solution to regression testing COTS-based applications that incorporate components of dynamic link library (DLL) files. We evolved the Integrated - Black-box Approach for Component Change Identification (I-BACCI) process that selects regression tests for applications based upon static binary code analysis to Version 4 to support DLL components. A feasibility case study was conducted at ABB on products written in C/C++ to determine\u00a0\u2026", "num_citations": "34\n", "authors": ["144"]}
{"title": "Distributed pair programming: empirical studies and supporting environments\n", "abstract": " Previous research [1, 2] has indicated that pair programming is better than individual programming when the pairs are physically colocated. However, important questions arise: How effective is pair programming if the pairs are not physically next to each other? What if the programmers are geographically distributed? An experiment was conducted to compare the different working arrangements of student teams developing object-oriented software. The teams were both colocated and in distributed environments; some teams practiced pair programming while others did not. The results of the experiment indicate that it is feasible to develop software using distributed pair programming, and that the resulting software is comparable to software developed in colocated or virtual teams. Our early experiments have led to the creation of a more comprehensive environment for support of distributed pair programming, using dual screen projectors and hymermedia-enhanced video streams. Our findings will be of significant help to educators dealing with team projects for distance-learning students, as well as organizations that are involved in distributed development of software.", "num_citations": "34\n", "authors": ["144"]}
{"title": "Towards the prioritization of system test cases\n", "abstract": " During software development, companies are frequently faced with lack of time and resources, which limits their ability to effectively complete testing efforts. This paper presents a system\u2010level, value\u2010driven approach to test case prioritization called the Prioritization of Requirements for Test (PORT). PORT involves analysing and assigning value to each requirement using the following four factors: requirements volatility, customer priority, implementation complexity, and fault proneness. System test cases are prioritized such that the test cases for requirements with higher priority are executed earlier during system test. PORT was applied to four student team projects as well as an industrial case study. The results show that PORT improves the rate of detection of severe failures over random prioritization. Additionally, the results indicate that customer priority was the most important contributor towards improved rate of\u00a0\u2026", "num_citations": "33\n", "authors": ["144"]}
{"title": "Incorporating performance testing in test-driven development\n", "abstract": " Our performance-testing approach required manually inspecting the performance logs. During the project's development, JUnit-based performance testing tools, such as JUnitPerf, weren't available. Such tools provide better visibility of performance problems than manual inspection of performance logs. Although we believe manual inspection of performance trends is necessary, specifying the bottom-line performance in assert-based test cases can complement the use of performance log files, making the TFP testing results more visible to the developers. We're investigating the design of assert-based performance testing to improve the TFP process. Another direction of future work is automatic performance test generation. In this project, we relied on the performance architect's experience to identify the execution paths and measurement points for performance testing. We can derive this crucial information for\u00a0\u2026", "num_citations": "33\n", "authors": ["144"]}
{"title": "Personas: Moving beyond role-based requirements engineering\n", "abstract": " A primary vehicle for understanding the user in the context of the requirements for a system has been the role. For example, the role is captured through the use of actors in the use case diagram and use case descriptions. Recently, personas have been used in conjunction with scenarios in participatory design to go deeper into examining the different types of people who could play a role. A persona is an archetype of a fictional user representing a specific group of typical users. This paper expands the use of personas to scenario-based requirements engineering. Personas and scenarios are being used together for specifying requirements at Microsoft. The result of this combination has been a more comprehensive understanding of the target customers' behaviors to drive and refine our scenarios and subsequently our product development.", "num_citations": "33\n", "authors": ["144"]}
{"title": "How good is a security policy against real breaches? A HIPAA case study\n", "abstract": " Policy design is an important part of software development. As security breaches increase in variety, designing a security policy that addresses all potential breaches becomes a nontrivial task. A complete security policy would specify rules to prevent breaches. Systematically determining which, if any, policy clause has been violated by a reported breach is a means for identifying gaps in a policy. Our research goal is to help analysts measure the gaps between security policies and reported breaches by developing a systematic process based on semantic reasoning. We propose SEMAVER, a framework for determining coverage of breaches by policies via comparison of individual policy clauses and breach descriptions. We represent a security policy as a set of norms. Norms (commitments, authorizations, and prohibitions) describe expected behaviors of users, and formalize who is accountable to whom and for\u00a0\u2026", "num_citations": "32\n", "authors": ["144"]}
{"title": "Security requirements patterns: understanding the science behind the art of pattern writing\n", "abstract": " Security requirements engineering ideally combines expertise in software security with proficiency in requirements engineering to provide a foundation for developing secure systems. However, security requirements are often inadequately understood and improperly specified, often due to lack of security expertise and a lack of emphasis on security during early stages of system development. Software systems often have common and recurrent security requirements in addition to system-specific security needs. Security requirements patterns can provide a means of capturing common security requirements while documenting the context in which a requirement manifests itself and the tradeoffs involved. The objective of this paper is to aid in understanding of the process for pattern development and provide considerations for writing effective security requirements patterns. We analyzed existing literature on software\u00a0\u2026", "num_citations": "32\n", "authors": ["144"]}
{"title": "ROSE: a repository of education-friendly open-source projects\n", "abstract": " Open-source project artifacts can be used to inject realism into software engineering courses or lessons on open-source software development. However, the use of open-source projects presents challenges for both educators and for students. Educators must search for projects that meet the constraints of their classes, and often must negotiate the scope and terms of the project with project managers. For students, many available open-source projects have a steep learning curve that inhibits them from making significant contributions to the project and benefiting from a\" realistic\" experience. To alleviate these problems and to encourage cross-institution collaboration, we have created the Repository for Open Software Education (ROSE) and have contributed three open-source projects intended for an undergraduate computer science or software engineering course. The projects in ROSE are education-friendly in\u00a0\u2026", "num_citations": "32\n", "authors": ["144"]}
{"title": "Dynamic cluster versioning for a group\n", "abstract": " An apparatus, program product, and method update the cluster infrastructure version used by a group resident in a clustered computer system without requiring a shut down of the group during the update. The cluster infrastructure software in individual nodes in the clustered computer system is updated while the group is maintained in an active state. After the cluster infrastructure software is updated, the group is then notified of the update. In response to the notification, the cluster infrastructure version used by the group is dynamically updated to that of the updated cluster infrastructure software, thus making additional functions supported by the new version of the cluster infrastructure software available for use by all group members.", "num_citations": "32\n", "authors": ["144"]}
{"title": "Merge protocol for clustered computer system\n", "abstract": " An apparatus, program product and method utilize ordered messages in a clustered computer system to defer the execution of a merge protocol in a cluster group until all pending protocols in each partition of a group are handled, typically by ensuring either cancellation or completion of each pending protocol prior to execution of the merge protocol. From the perspective of each group member, the execution of the merge protocol is deferred by inhibiting processing of the merge request by such member until after processing of all earlier-received pending requests has been completed.", "num_citations": "32\n", "authors": ["144"]}
{"title": "Extreme Programming and Agile Methods-XP/Agile Universe 2002: Second XP Universe and First Agile Universe Conference Chicago, IL, USA, August 4-7, 2002. Proceedings\n", "abstract": " The second XP Universe and? rst Agile Universe brought together many p-ple interested in building software in a new way. Held in Chicago, August 4\u20137, 2002 it attracted software experts, educators, and developers. Unlike most c-ferences the venue was very dynamic. Many activities were not even well de? ned in advance. All discussions were encouraged to be spontaneous. Even so, there were some written words available and you are holding all of them now. We have collected as much material as possible together into this small volume. It is just the tip of the iceberg of course. A reminder to us of what we learned, the people we met, and the ideas we expressed. The conference papers, including research and experience papers, are rep-duced in these proceedings. Forty-one (41) papers were submitted. Each subm-ted paper received three reviews by program committee members. The program committee consisted of 40 members. Papers submitted by program committee members were refereed separately. This ensured that reviewers could provide an honest feedback not seen by the paper submitters. In many cases, the program committee shepherded authors to signi? cantly improve their initial submission prior to completing the version contained in these proceedings. In the end, the program committee chose 25 papers for publication (60% acceptance).", "num_citations": "32\n", "authors": ["144"]}
{"title": "Toward non-security failures as a predictor of security faults and failures\n", "abstract": " In the search for metrics that can predict the presence of vulnerabilities early in the software life cycle, there may be some benefit to choosing metrics from the non-security realm. We analyzed non-security and security failure data reported for the year 2007 of a Cisco software system. We used non-security failure reports as input variables into a classification and regression tree (CART) model to determine the probability that a component will have at least one vulnerability. Using CART, we ranked all of the system components in descending order of their probabilities and found that 57% of the vulnerable components were in the top nine percent of the total component ranking, but with a 48% false positive rate. The results indicate that non-security failures can be used as one of the input variables for security-related prediction models.", "num_citations": "31\n", "authors": ["144"]}
{"title": "Engineering security vulnerability prevention, detection, and response\n", "abstract": " Around the turn of the 21st century, practices began to emerge to guide teams toward engineering software to stop attackers and users from utilizing unintended functionality by violating the system designer's assumptions to cause a security breach. Yet, breaches are reported daily in the news in all domains-from the casual to the critical. The goal of this article is to help software engineers, software engineering educators, and security researchers understand opportunities for education and research through an analysis of current software security practices. The analysis is conducted on data on the use of a subset of 113 software security practices by 109 firms over 42 months, as reported in the Building Security In Maturity Model (BSIMM) Version 8 report. This article is part of a theme issue on software engineering's 50th anniversary.", "num_citations": "29\n", "authors": ["144"]}
{"title": "Improving developer activity metrics with issue tracking annotations\n", "abstract": " Understanding and measuring how groups of developers collaborate on software projects can provide valuable insight into software quality and the software development process. Current practices of measuring developer collaboration (eg with social network analysis) usually employ metrics based on version control change log data to determine who is working on which part of the system. Version control change logs, however, do not tell the whole story. Information about the collaborative problem-solving process is also documented in the issue tracking systems that record solutions to failures, feature requests, or other development tasks. To enrich the data gained from version control change logs, we propose two annotations to be used in issue tracking systems: solution originator and solution approver. We examined the online discussions of 602 issues from the OpenMRS healthcare web application\u00a0\u2026", "num_citations": "29\n", "authors": ["144"]}
{"title": "Evaluating access control of open source electronic health record systems\n", "abstract": " Incentives and penalties for healthcare providers as laid out in the American Recovery and Reinvestment Act of 2009 have caused tremendous growth in the development and installation of electronic health record (EHR) systems in the US. For the benefit of protecting patient privacy, regulations and certification criteria related to EHR systems stipulate the use of access control of protected health information. The goal of this research is to guide development teams, regulators, and certification bodies by assessing the state of the practice in EHR access control. In this paper, we present a compilation of 25 criteria relative to access control in EHR systems found in the Health Insurance Portability and Accountability Act (HIPAA) regulation, meaningful use certification criteria, best practices embodied in the National Institute for Standards and Technology (NIST) role-based access control standard, and other best practices\u00a0\u2026", "num_citations": "28\n", "authors": ["144"]}
{"title": "A lightweight process for change identification and regression test selection in using COTS components\n", "abstract": " Various regression test selection techniques have been developed and have shown fault detection effectiveness. The majority of these test selection techniques rely on access to source code for change identification. However, when new releases of COTS components are made available for integration and testing, source code is often not available. In this paper, we present a lightweight integrated-black-box approach for component change identification (I-BACCI) process for selection of regression tests for user/glue code that uses COTS components. I-BACCI is applicable when component licensing agreements do not preclude analysis of the binary files. A case study of the process was conducted on an ABB product that uses a medium-scale internal ABB software component. Five releases of the component were examined to evaluate the efficacy of the proposed process. The result of the case study indicates this\u00a0\u2026", "num_citations": "28\n", "authors": ["144"]}
{"title": "An initial study of a lightweight process for change identification and regression test selection when source code is not available\n", "abstract": " Various regression test selection techniques have been developed and have shown to improve testing cost effectiveness via improving efficiency. The majority of these test selection techniques rely on access to source code for change identification. However, when new releases of COTS components are made available for integration and testing, source code is often not available to guide in regression test selection. In this paper we describe a lightweight integrated-black-box approach for component change identification (I-BACCI) process for selection of regression tests for user/glue code that uses COTS components. I-BACCI is applicable when component licensing agreements do not preclude binary code analysis. A case study of the process was conducted on an ABB product that uses a medium-scale internal ABB software component. Six releases of the component were examined to evaluate the efficacy of\u00a0\u2026", "num_citations": "28\n", "authors": ["144"]}
{"title": "Access control policy extraction from unconstrained natural language text\n", "abstract": " While access control mechanisms have existed in computer systems since the 1960s, modern system developers often fail to ensure appropriate mechanisms are implemented within particular systems. Such failures allow for individuals, both benign and malicious, to view and manipulate information that they should not otherwise be able to access. The goal of our research is to help developers improve security by extracting the access control policies implicitly and explicitly defined in natural language project artifacts. Developers can then verify and implement the extracted access control policies within a system. We propose a machine-learning based process to parse existing, unaltered natural language documents, such as requirement or technical specifications to extract the relevant subjects, actions, and resources for an access control policy. To evaluate our approach, we analyzed a public requirements\u00a0\u2026", "num_citations": "26\n", "authors": ["144"]}
{"title": "Protection poker: Structuring software security risk assessment and knowledge transfer\n", "abstract": " Discovery of security vulnerabilities is on the rise. As a result, software development teams must place a higher priority on preventing the injection of vulnerabilities in software as it is developed. Because the focus on software security has increased only recently, software development teams often do not have expertise in techniques for identifying security risk, understanding the impact of a vulnerability, or knowing the best mitigation strategy. We propose the Protection Poker activity as a collaborative and informal form of misuse case development and threat modeling that plays off the diversity of knowledge and perspective of the participants. An excellent outcome of Protection Poker is that security knowledge passed around the team. Students in an advanced undergraduate software engineering course at North Carolina State University participated in a Protection Poker session conducted as a laboratory\u00a0\u2026", "num_citations": "25\n", "authors": ["144"]}
{"title": "A (partial) introduction to software engineering practices and methods\n", "abstract": " Software engineering is concerned with all aspects of software production from the early stages of system specification through to maintaining the system after it has gone into use. In this chapter, we will explain the following:\u2022 the definition of computer science and software engineering and how the two are different\u2022 how software engineering is similar to other engineering disciplines and what that means for software engineers\u2022 the unique challenges of software engineering\u2022 software development models and processes and their component parts, software development practices", "num_citations": "25\n", "authors": ["144"]}
{"title": "But, isn't that cheating?[collaborative programming]\n", "abstract": " Can university computer science students benefit from collaborative programming? The author discusses his experiences with a class he taught at the University of Utah which set out to study pair programming in an educational setting. The class, an Active Server Pages (ASP) web programming class, consisted of 20 juniors and seniors. The students were very familiar with programming, but not with the web programming languages learned and used in the class. Each student was paired with another student to work with for the entire semester. Tests were, however, taken individually. They understood that the idea was not to break the class project into two pieces and integrate later. The idea was to work together (almost) all the time on one product. These requirements were stated in the course announcement and were re-stated at the start of the class. Most skeptically, but enthusiastically, embarked on making the\u00a0\u2026", "num_citations": "22\n", "authors": ["144"]}
{"title": "Risk-based attack surface approximation: how much data is enough?\n", "abstract": " Proactive security reviews and test efforts are a necessary component of the software development lifecycle. Resource limitations often preclude reviewing the entire code base. Making informed decisions on what code to review can improve a team's ability to find and remove vulnerabilities. Risk-based attack surface approximation (RASA) is a technique that uses crash dump stack traces to predict what code may contain exploitable vulnerabilities. The goal of this research is to help software development teams prioritize security efforts by the efficient development of a risk-based attack surface approximation. We explore the use of RASA using Mozilla Firefox and Microsoft Windows stack traces from crash dumps. We create RASA at the file level for Firefox, in which the 15.8% of the files that were part of the approximation contained 73.6% of the vulnerabilities seen for the product. We also explore the effect of\u00a0\u2026", "num_citations": "21\n", "authors": ["144"]}
{"title": "DIGS: A framework for discovering goals for security requirements engineering\n", "abstract": " Context: The security goals of a software system provide a foundation for security requirements engineering. Identifying security goals is a process of iteration and refinement, leveraging the knowledge and expertise of the analyst to secure not only the core functionality but the security mechanisms as well. Moreover, a comprehensive security plan should include goals for not only preventing a breach, but also for detecting and appropriately responding in case a breach does occur. Goal: The objective of this research is to support analysts in security requirements engineering by providing a framework that supports a systematic and comprehensive discovery of security goals for a software system. Method: We develop a framework, Discovering Goals for Security (DIGS), that models the key entities in information security, including assets and security goals. We systematically develop a set of security goal patterns that\u00a0\u2026", "num_citations": "21\n", "authors": ["144"]}
{"title": "Experiences in applying agile software development practices in new product development.\n", "abstract": " Experiences with software technology development projects at ABB Inc. indicated a need for additional flexibility and speed during explorations of applying new technologies to future products. A case study was conducted at ABB to compare and contrast the use of an evolutionary-agile approach with a more traditional incremental approach in two different technology development projects. The study indicated benefits associated with the evolutionary approach with agile practices, such as streamlined documentation, increased customer involvement, enhanced customer satisfaction, increased capability to include emergent requirements, and increased risk management ability. This paper suggests that using agile practices during the Research and Development (R&D) phase of new product development contributes to improving productivity, to increasing value-added activities, to showing progress early in the development project, and to enhancing customer satisfaction. Another observation derived from this study is that by offering a carefully selected subset of agile practices, ABB R&D groups are more likely to successfully incorporate them into their existing processes.", "num_citations": "20\n", "authors": ["144"]}
{"title": "The collaborative lipid research clinics program family study: III. Transformations and covariate adjustments of lipid and lipoprotein levels\n", "abstract": " Green, P. P., K. K. Namboodiri (Dept of Biostatistics, U. of North Carolina, Chapei Hill, NC 27514), P. Hannan, J. Martin, A. R. G. Owen, G. A. Chase, E. B. Kapian, L. Williams and R. C. Eiston. The Coliaborative Lipid Research Clinics Program Famliy Study. III. Transformations and covariate adjustments of lipid and lipoprotein levels. Am J Epidemiol 1984; 119: 959\u201374.         Several methods of transformation and covarlate adjustment have been applied to the Coliaborative Lipid Research Clinics Program Famliy Study data to facilitate analysis of lipld levels of examinees of different sex and age groups. After several exploratory analyses, choiesterol, high density lipoprotein cholesterol, and low density lipoprotein cholesterol were logarithmicaily transformed, and triglycerides were transformed by the power \u22120.25. Two types of covariate adjustment procedures, the regression and Z score methods, were used. A\u00a0\u2026", "num_citations": "20\n", "authors": ["144"]}
{"title": "Using templates to elicit implied security requirements from functional requirements-a controlled experiment\n", "abstract": " Context: Security requirements for software systems can be challenging to identify and are often overlooked during the requirements engineering process. Existing functional requirements of a system can imply the need for security requirements. Systems having similar security objectives (eg, confidentiality) often also share security requirements that can be captured in the form of reusable templates and instantiated in the context of a system to specify security requirements.Goal: We seek to improve the security requirements elicitation process by automatically suggesting appropriate security requirement templates implied by existing functional requirements.Method: We conducted a controlled experiment involving 50 graduate students enrolled in a software security course to evaluate the use of automatically-suggested templates in eliciting implied security requirements. Participants were divided into treatment\u00a0\u2026", "num_citations": "19\n", "authors": ["144"]}
{"title": "Agile software development in practice\n", "abstract": " Agile software development methods have been around since the mid 1990s. Over these years, teams have evolved the specific software development practices used. Aims: The goal of this paper is to provide a view of the agile practices used by new teams, and the relationship between the practices used, project outcomes, and the agile principles. Method: This paper provides a summary and analysis of 2,229 Comparative AgilityTM (CA) assessment surveys completed between March 2011 and October 2012 by agile developers who knew about the survey. \u00a0The CA tool assesses a team\u2019s agility and project outcomes using a 65-statement Likert survey. Results: The agile principle of respect for individuals occurs the most frequently, while simplicity occurs least. Progress/Planning is correlated strongly to nine principles. Conclusion: Subject to sampling issues, successful teams report more positive results\u00a0\u2026", "num_citations": "19\n", "authors": ["144"]}
{"title": "Examining the relationships between performance requirements and \u201cnot a problem\u201d defect reports\n", "abstract": " Missing or imprecise requirements can lead stakeholders to make incorrect assumptions. A \"not a problem\" defect report (NaP) describes a software behavior that a stakeholder regards as a problem while the developer believes this behavior is acceptable and chooses not to take any action. As a result, a NaP wastes the time of the development team because resources are spent analyzing the problem but the quality of the software is not improved. Performance requirements specification and analysis are instance-based. System performance can change based upon the execution environment or usage patterns. To understand how the availability and precision of performance requirements can affect NaP occurrence rate, we conducted a case study on an embedded control module. We applied the performance refinement and evolution model to examine the relationship between each factor in the performance\u00a0\u2026", "num_citations": "19\n", "authors": ["144"]}
{"title": "A longitudinal study of the use of a test-driven development practice in industry\n", "abstract": " Test-Driven Development (TDD) is an agile practice that is widely accepted and advocated by most agile methods and methodologists. In this paper, we report on a longitudinal case study of an IBM team who has sustained use of TDD for five years and over ten releases of a Java-implemented product. The team worked from a design and wrote tests incrementally before or while they wrote code and, in the process, developed a significant asset of automated tests. The IBM team realized sustained quality improvement relative to a pre-TDD project and consistently had defect density below industry standards. As a result, our data indicate that the TDD practice can aid in the production of high quality products. This quality improvement would compensate for the moderate perceived productivity losses. Additionally, the use of TDD may decrease the degree to which code complexity increases as software ages.", "num_citations": "18\n", "authors": ["144"]}
{"title": "Surveying security practice adherence in software development\n", "abstract": " Software development teams are increasingly incorporating security practices in to their software development processes. However, little empirical evidence exists on the costs and benefits associated with the application of security practices. Balancing the trade off between the costs in time, effort, and complexity of applying security practices and the benefit of an appropriate level of security in delivered software requires measuring security practice benefits and costs. The goal of this research is to support researcher investigations of software development security practice adherence by building and validating a set of security practices and adherence measures through literature review and survey data analysis. We extracted 16 software development security practices from a review of the literature, and established a set of adherence measures based on technology acceptance theory. We built a survey around the\u00a0\u2026", "num_citations": "16\n", "authors": ["144"]}
{"title": "A grounded analysis of experts\u2019 decision-making during security assessments\n", "abstract": " Security analysis requires specialized knowledge to align threats and vulnerabilities in information technology. To identify mitigations, analysts need to understand how threats, vulnerabilities, and mitigations are composed together to yield security requirements. Despite abundant guidance in the form of checklists and controls about how to secure systems, evidence suggests that security experts do not apply these checklists. Instead, they rely on their prior knowledge and experience to identify security vulnerabilities. To better understand the different effects of checklists, design analysis, and expertise, we conducted a series of interviews to capture and encode the decision-making process of security experts and novices during three security analysis exercises. Participants were asked to analyze three kinds of artifacts: source code, data flow diagrams, and network diagrams, for vulnerabilities, and then to apply\u00a0\u2026", "num_citations": "16\n", "authors": ["144"]}
{"title": "Does adding manpower also affect quality? an empirical, longitudinal analysis\n", "abstract": " With each new developer to a software development team comes a greater challenge to manage the communication, coordination, and knowledge transfer amongst teammates. Fred Brooks discusses this challenge in The Mythical Man-Month by arguing that rapid team expansion can lead to a complex team organization structure. While Brooks focuses on productivity loss as the negative outcome, poor product quality is also a substantial concern. But if team expansion is unavoidable, can any quality impacts be mitigated? Our objective is to guide software engineering managers by empirically analyzing the effects of team size, expansion, and structure on product quality. We performed an empirical, longitudinal case study of a large Cisco networking product over a five year history. Over that time, the team underwent periods of no expansion, steady expansion, and accelerated expansion. Using team-level metrics\u00a0\u2026", "num_citations": "16\n", "authors": ["144"]}
{"title": "Jazz Sangam: A real-time tool for distributed pair programming on a team development platform\n", "abstract": " Pair programming has proven to be a useful technique for developing high quality code while sharing knowledge throughout a team. Rapid global dispersion of software development teams, however, makes co-located pair programming a challenge, motivating the need for development tools tailored specifically for distributed pair programming. Previously, the Sangam Eclipse plug-in was developed to support distributed pair programming. More recently, the Jazz collaborative software development platform was built to support team communication and the sharing of life-cycle resources and to integrate a variety of disparate tools used by team members. We have ported Sangam to the Jazz platform to enable teams to pair program within their integrated team environment. In this paper, we describe Jazz Sangam, highlight the choices that lead to Sangam\u2019s current design, and discuss how Jazz Sangam can improve the distributed pair programming experience.", "num_citations": "16\n", "authors": ["144"]}
{"title": "Developing software performance with the performance refinement and evolution model\n", "abstract": " Performance is an important attribute of a software system. To develop a software system of acceptable performance, the team needs to specify precise performance requirements, design appropriate test cases, and use suitable techniques to analyze the performance characteristics. However, the lack of a structured framework for performance engineering may impair the effectiveness of the techniques. We propose the Performance Refinement and Evolution Model (PREM) as a performance management framework. Based on the specification of quantitative measurement and workloads, PREM classifies performance requirements specification, analysis activities, and testing into four levels. In this paper, we provide an overview of this model.", "num_citations": "16\n", "authors": ["144"]}
{"title": "Examining the impact of pair programming on female students\n", "abstract": " There has been low representation of women in Computer Science. Numerous studies have been conducted to identify the cause of this under-representation and to provide suggestions to improve the situation. Still not much progress in attracting women to computer science has been observed. The research discussed in this paper was done during the pilot study phase of a three-year project about women in information technology field. During the first semester of this project, pair programming was used in a junior/senior Software Engineering class at North Carolina State University. The goal of this research is to examine the effect of pair programming on female students. We interviewed three female students and analyzed all female students\u2019 project retrospective reports. Theoretical models were developed to describe (a) the source of project enjoyment,(b) context that influenced female students\u2019 study habits, and (c) the effectiveness of pair programming. The cause and effect of each component of the theoretical models were identified and are illustrated with narrative data.", "num_citations": "16\n", "authors": ["144"]}
{"title": "Better security bug report classification via hyperparameter optimization\n", "abstract": " When security bugs are detected, they should be (a)~discussed privately by security software engineers; and (b)~not mentioned to the general public until security patches are available. Software engineers usually report bugs to bug tracking system, and label them as security bug reports (SBRs) or not-security bug reports (NSBRs), while SBRs have a higher priority to be fixed before exploited by attackers than NSBRs. Yet suspected security bug reports are often publicly disclosed because the mislabelling issues ( i.e., mislabel security bug reports as not-security bug report). The goal of this paper is to aid software developers to better classify bug reports that identify security vulnerabilities as security bug reports through parameter tuning of learners and data pre-processor. Previous work has applied text analytics and machine learning learners to classify which reported bugs are security related. We improve on that work, as shown by our analysis of five open source projects. We apply hyperparameter optimization to (a)~the control parameters of a learner; and (b)~the data pre-processing methods that handle the case where the target class is a small fraction of all the data. We show that optimizing the pre-processor is more useful than optimizing the learners. We also show that improvements gained from our approach can be very large. For example, using the same data sets as recently analyzed by our baseline approach, we show that adjusting the data pre-processing results in improvements to classification recall of 35% to 65% (median to max) with moderate increment of false positive rate.", "num_citations": "15\n", "authors": ["144"]}
{"title": "Log your CRUD: design principles for software logging mechanisms\n", "abstract": " According to a 2011 survey in healthcare, the most commonly reported breaches of protected health information involved employees snooping into medical records of friends and relatives. Logging mechanisms can provide a means for forensic analysis of user activity in software systems by proving that a user performed certain actions in the system. However, logging mechanisms often inconsistently capture user interactions with sensitive data, creating gaps in traces of user activity. Explicit design principles and systematic testing of logging mechanisms within the software development lifecycle may help strengthen the overall security of software. The objective of this research is to observe the current state of logging mechanisms by performing an exploratory case study in which we systematically evaluate logging mechanisms by supplementing the expected results of existing functional black-box test cases to\u00a0\u2026", "num_citations": "15\n", "authors": ["144"]}
{"title": "An experience report for software quality evaluation in highly iterative development methodology using traditional metrics\n", "abstract": " The use of highly iterative software development methodologies, such as Agile and Lean, have been growing. However, these methodologies do not explicitly provide practices for managing and measuring software quality. This deficiency may prevent software development organizations in critical domains from transforming from traditional development to highly iterative development. These organizations may need to manage quality during development and may desire to compare with the quality of a large number of products produced in the past. In this paper, we focus on the reliability aspects of software quality and discuss the applicability of conventional reliability metrics to iterative development. First, we defined the general process structure in iterative development. Then, we present an associated quality evaluation scheme closely. Our experimental results indicate that traditional quality metrics were\u00a0\u2026", "num_citations": "15\n", "authors": ["144"]}
{"title": "Pallino: automation to support regression test selection for COTS-based applications\n", "abstract": " Software products are often built from commercial-off-the-shelf (COTS) components. When new releases of these components are made available for integration and testing, source code is usually not provided by the vendors. Various regression test selection techniques have been developed and have been shown to be cost effective. However, the majority of these test selection techniques rely on source code for change identification and impact analysis. In our research, we have evolved a regression test selection (RTS) process called Integrated-Black-box Approach for Component Change Identification (I-BACCI) for COTS-based applications. I-BACCI reduces the test suite based upon changes in the binary code of the COTS component using the firewall regression test selection method. In this paper, we present the Pallino tool. Pallino statically analyzes binary code to identify the code change and the impact of\u00a0\u2026", "num_citations": "15\n", "authors": ["144"]}
{"title": "Improving vulnerability inspection efficiency using active learning\n", "abstract": " Software engineers can find vulnerabilities with less effort if they are directed towards code that might contain more vulnerabilities. HARMLESS is an incremental support vector machine tool that builds a vulnerability prediction model from the source code inspected to date, then suggests what source code files should be inspected next. In this way, HARMLESS can reduce the time and effort required to achieve some desired level of recall for finding vulnerabilities. The tool also provides feedback on when to stop (at that desired level of recall) while at the same time, correcting human errors by double-checking suspicious files. This paper evaluates HARMLESS on Mozilla Firefox vulnerability data. HARMLESS found 80, 90, 95, 99% of the vulnerabilities by inspecting 10, 16, 20, 34% of the source code files. When targeting 90, 95, 99% recall, HARMLESS could stop after inspecting 23, 30, 47% of the source code files\u00a0\u2026", "num_citations": "14\n", "authors": ["144"]}
{"title": "Continuously integrating security\n", "abstract": " Continuous deployment is a software engineering process where incremental software changes are automatically tested and frequently deployed to production environments. With continuous deployment, the elapsed time for a change made by a developer to reach a customer can now be measured in days or even hours. To understand the emerging practices surrounding continuous deployment, three annual one-day Continuous Deployment Summits have been held at Facebook, Netflix, and Google in 2015--2017, where 17 companies have described how they used continuous deployment. This short paper will describe the practices and environment used by these companies as they strive to develop secure and privacy-preserving products while making ultra-fast changes.", "num_citations": "13\n", "authors": ["144"]}
{"title": "To log, or not to log: using heuristics to identify mandatory log events\u2013a controlled experiment\n", "abstract": " Context                 User activity logs should capture evidence to help answer who, what, when, where, why, and how a security or privacy breach occurred. However, software engineers often implement logging mechanisms that inadequately record mandatory log events (MLEs), user activities that must be logged to enable forensics.                                               Goal                                    The objective of this study is to support security analysts in performing forensic analysis by evaluating the use of a heuristics-driven method for identifying mandatory log events.                                                                Method                 We conducted a controlled experiment with 103 computer science students enrolled in a graduate-level software security course. All subjects were first asked to identify MLEs described in a set of requirements statements during the pre-period task. In the post-period task, subjects were randomly\u00a0\u2026", "num_citations": "13\n", "authors": ["144"]}
{"title": "Paired programming project: Focus groups with teaching assistants and students\n", "abstract": " By implementing paired programming protocols in an introductory level computer programming course (CS1), a group of researchers at NC State University analyzed the effects of paired programming on students and instructors 1. Paired programming is a practice in which two programmers work collaboratively at one computer to generate designs, algorithms, or codes. It mimics real-world scenarios for computer programmers, since a lot of computer programming is done as part of a team.", "num_citations": "13\n", "authors": ["144"]}
{"title": "A comparative evaluation of static analysis actionable alert identification techniques\n", "abstract": " Automated static analysis (ASA) tools can identify potential source code anomalies that could lead to field failures. Developer inspection is required to determine if an ASA alert is important enough to fix, or an actionable alert. Supplementing current ASA tools with automated identification of actionable alerts could reduce developer inspection overhead, leading to an increase in industry adoption of ASA tools. The goal of this research is to inform the selection of an actionable alert identification technique for ranking the output of automated static analysis through a comparative evaluation of actionable alert identification techniques. We investigated six actionable alert identification techniques on three subject projects. Among these six techniques, the systematic actionable alert identification (SAAI) technique reported an average accuracy of 82.5% across the three subject projects when considering both ASA tools\u00a0\u2026", "num_citations": "12\n", "authors": ["144"]}
{"title": "Predictive models for identifying software components prone to failure during security attacks\n", "abstract": " Sometimes software security engineers are given a product that they not familiar with, but are asked to have a security analysis done for it in a relatively short time. An early knowledge of where the most vulnerable regions of a software-based system are likely to reside can help prioritize their efforts. In general, software metrics can be used to predict fault-and failure-prone components for prioritizing inspection, testing, and redesign efforts. We believe that the security community can leverage this knowledge to design tools and metrics that can identify vulnerability-and attack-prone software components early in the software life cycle. We analyzed a large commercial telecommunications software-based system and found that the presence of security faults correlates strongly with the presence of a more general category of reliability faults. This, of course, is not surprising if one accepts the notion that security faults are in many instances a subset of a reliability fault set. We discuss a model that can be useful for identifying attack-prone components and for prioritizing security efforts early in the software life-cycle.", "num_citations": "12\n", "authors": ["144"]}
{"title": "Using groupings of static analysis alerts to identify files likely to contain field failures\n", "abstract": " In this paper, we propose a technique for leveraging historical field failure records in conjunction with automated static analysis alerts to determine which alerts or sets of alerts are predictive of a field failure. Our technique uses singular value decomposition to generate groupings of static analysis alert types, which we call alert signatures, that have been historically linked to field failure-prone files in previous releases of a software system. The signatures can be applied to sets of alerts from a current build of a software system. Files that have a matching alert signature are identified as having similar static analysis alert characteristics to files with known field failures in a previous release of the system. We performed a case study involving an industrial software system at IBM and found three distinct alert signatures that could be applied to the system. We found that 50% of the field failures reported since the last static\u00a0\u2026", "num_citations": "12\n", "authors": ["144"]}
{"title": "An early testing and defense web application framework for malicious input attacks\n", "abstract": " Input validation vulnerabilities, one of the largest problems in software security today, are readily identified by software assurance (SA) tools. Software development organizations are increasingly adopting SA tools to quickly identify vulnerabilities in their software systems. These tools, usually applied when implementation is complete, have a comprehensive and extendable rule set to detect known and new vulnerabilities. A simple and effective framework is needed to provide developers with a strategic approach to securing against malicious input attacks early in software development. We introduce a Java Web Application Reliability and Defense (WARD) framework, a two-part security solution composed of a vulnerability detection component, SecureUnit, and a vulnerability protection component, SecureFilter. SecureUnit enables developers to write automated, reusable, and customizable JUnit penetration tests that launch attacks on their systems to reveal security vulnerabilities. SecureFilter is a customizable server-side choke point containing a regular expression-based filter to match legal input according to system requirements. WARD provides an attack-then-defend approach for developers to build security into a software system early in the software process. We integrated WARD v1. 0 with WebGoat, an open-source web application security test bed, and successfully \u201cwarded off\u201d 38 of 43 (88%) injected cross-site scripting exploits. WARD v2. 0 will address the encoded (eg with HTML entities, hex characters) exploits that were not stopped from entering WebGoat in WARD v1. 0.", "num_citations": "12\n", "authors": ["144"]}
{"title": "Nane: Identifying misuse cases using temporal norm enactments\n", "abstract": " Recent data breaches in domains such as healthcare where confidentiality of data is crucial indicate that breaches often originate from misuses, not only from vulnerabilities in the technical (software or hardware) architecture. Current requirements engineering (RE) approaches determine what access control mechanisms are needed to protect sensitive resources (assets). However, current RE approaches inadequately characterize how a user is expected to interact with others in relation to the relevant assets. Consequently, a requirements analyst cannot readily identify misuses by legitimate users. We adopt social norms as a natural, formal means of characterizing user interactions whereby potential misuses map to norm violations. Our research goal is to help analysts identify misuse cases by formal reasoning about norm enactments. We propose Nane, a formal framework for identifying such misuse cases using\u00a0\u2026", "num_citations": "11\n", "authors": ["144"]}
{"title": "Software security education at scale\n", "abstract": " Massively Open Online Courses (MOOCs) provide a unique opportunity to reach out to students who would not normally be reached by alleviating the need to be physically present in the classroom. However, teaching software security coursework outside of a classroom setting can be challenging. What are the challenges when converting security material from an on-campus course to the MOOC format? The goal of this research is to assist educators in constructing software security coursework by providing a comparison of classroom courses and MOOCs. In this work, we compare demographic information, student motivations, and student results from an on-campus software security course and a MOOC version of the same course. We found that the two populations of students differed, with the MOOC reaching a more diverse set of students than the on-campus course. We found that students in the on-campus\u00a0\u2026", "num_citations": "11\n", "authors": ["144"]}
{"title": "Improving Performance Requirements Specifications from Field Failure Reports\n", "abstract": " Customer-reported field failures provide valuable information for the requirements of the next release. Without a systematic approach, the requirements of the next release may not address the field failures, and the same problems may reoccur. In this paper, we propose a procedure for improving performance requirements based on a retrospective analysis of field failures reports and original requirements. In our procedure, the performance information from field failure reports and original requirements specifications is extracted based on a performance meta-model. The extracted information is used to construct new and revised performance requirements, following the performance refinement and evolution model. We applied this procedure on the requirements specifications and field failure reports for a commercial distributed software system. The results from our case study demonstrate that the resulting\u00a0\u2026", "num_citations": "11\n", "authors": ["144"]}
{"title": "Collaboration vs plagiarism in computer science programming courses\n", "abstract": " In some circles, all programming is collaborative, yet in many CS1 and CS2 courses, individual programming assignments are made, collaboration with other students is cheating, and tailoring a program found on the web is plagiarism. Many educators feel that collaboration belongs only in a very few upper division courses. Others have experience to show that early collaboration broadens the learning of students, to become more effective professional individuals. Most conclude that a blend of the two styles is best for students, and can reduce cheating/plagiarism.Does collaboration belong in programming classes? Where does collaboration end and cheating/plagiarism begin? What are the advantages, problems and techniques of allowing collaboration on programming assignments in CS1 and CS2? The moderator created six discussion questions. Each member of the panel has chosen the position they can\u00a0\u2026", "num_citations": "11\n", "authors": ["144"]}
{"title": "How do developers act on static analysis alerts? an empirical study of coverity usage\n", "abstract": " Static analysis tools (SATs) often fall short of developer satisfaction despite their many benefits. An understanding of how developers in the real-world act on the alerts detected by SATs can help improve the utility of these tools and determine future research directions. The goal of this paper is to aid researchers and tool makers in improving the utility of static analysis tools through an empirical study of developer action on the alerts detected by Coverity, a state-of-the-art static analysis tool. In this paper, we analyze five open source projects as case studies (Linux, Firefox, Samba, Kodi, and Ovirt-engine) that have been actively using Coverity over a period of at least five years. We investigate the alert occurrences and developer triage of the alerts from the Coverity database; identify the alerts that were fixed through code changes (i.e. actionable) by mining the commit history of the projects; analyze the time an alert\u00a0\u2026", "num_citations": "10\n", "authors": ["144"]}
{"title": "Mapping the field of software security metrics\n", "abstract": " While security, or its absence, is a property of running software, many aspects of software requirements, design, implementation, and testing contribute to the presence or absence of security in the finished product. Assessing whether a given piece of software meets a set of security objectives is a multi-dimensional problem, and we do not yet have a clear picture of all of the dimensions. The goal of this research is to support researcher and practitioner use of security measurement by cataloging available metrics, their validation, and the subjects they measure through conducting a systematic mapping study. Our study began with 1,561 papers and narrowed down to 63 papers reporting on 346 metrics. For each metric, we identify the subject being measured, how the metric has been evaluated by researcher (s), and how the metric is being used. Approximately 85% of security-specific metrics have been proposed and evaluated solely by their authors. Approximately 40% of the metrics are not empirically evaluated, and many artifacts and processes remain unmeasured. Approximately 15% of the metrics focus on the early stages of development or on testing (1.5%). At present, despite the abundance of metrics found in the literature, those available give us an incomplete, disjointed, hazy view of software security.", "num_citations": "10\n", "authors": ["144"]}
{"title": "Defect density estimation through verification and validation\n", "abstract": " In industry, information on defect density of a product tends to become available too late in the software development process to affordably guide corrective actions. Our research objective is to build a parametric model which utilizes a persistent record of the validation and verification (V&V) practices used with a program to estimate the defect density of that program. The persistent record of the V&V practices are recorded as certificates which are automatically recorded and maintained with the code. To date, we have created a parametric modeling process with the help of the Center for Software Engineering at the University of Southern California and have created the second version of an Eclipse plug-in for recording V&V certificates.", "num_citations": "10\n", "authors": ["144"]}
{"title": "A process for identifying changes when source code is not available\n", "abstract": " Various regression test selection techniques have been developed and shown to improve fault detection effectiveness. The majority of these test selection techniques rely on access to source code for change identification. However, when new releases of COTS components are made available for integration and testing, source code is often not available to guide regression test selection. This paper describes a process for identifying changed functions when code is not available. This change information is beneficial for selecting white-box regression tests of customer/glue code. This process is applicable when COTS licensing agreements do not preclude decompilation. A feasibility study of the process was conducted with four releases of a medium-scale internal ABB product. The results of the feasibility study indicate that this process can be effective in identifying changed functions.", "num_citations": "10\n", "authors": ["144"]}
{"title": "Towards a framework to measure security expertise in requirements analysis\n", "abstract": " Research shows that commonly accepted security requirements are not generally applied in practice. Instead of relying on requirements checklists, security experts rely on their expertise and background knowledge to identify security vulnerabilities. To understand the gap between available checklists and practice, we conducted a series of interviews to encode the decision-making process of security experts and novices during security requirements analysis. Participants were asked to analyze two types of artifacts: source code, and network diagrams for vulnerabilities and to apply a requirements checklist to mitigate some of those vulnerabilities. We framed our study using Situation Awareness-a cognitive theory from psychology-to elicit responses that we later analyzed using coding theory and grounded analysis. We report our preliminary results of analyzing two interviews that reveal possible decision-making\u00a0\u2026", "num_citations": "9\n", "authors": ["144"]}
{"title": "The role of data use agreements in specifying legally compliant software requirements\n", "abstract": " Security and privacy requirements are often not explicitly stated and are often not easy to elicit. In this paper, we discuss data use agreements (DUAs) as a source of security and privacy requirements that can be leveraged by requirements engineers. Within the healthcare domain, regulations created pursuant to the U.S. Health Insurance Portability and Accountability Act (HIPAA) specify that a DUA must exist for certain uses and disclosures of protected health information as a limited data set. For compliance reasons, it is important for requirements engineers to ask for and evaluate DUAs, as they are legally binding on the parties. We discuss HIPAA-governed DUAs and the information contained within them. Using four DUAs, we apply commitment, privilege, and right (CPR) analysis to identify legally compliant requirements. Through this work, we have identified contractual compliance requirements while also\u00a0\u2026", "num_citations": "9\n", "authors": ["144"]}
{"title": "Ranking attack-prone components with a predictive model\n", "abstract": " Limited resources preclude software engineers from finding and fixing all vulnerabilities in a software system. An early security risk analysis that ranks software components by probability of being attacked can provide an affordable means to prioritizing fortification efforts to the highest risk components. We created a predictive model using classification and regression trees and the following internal metrics: quantity of Klocwork static analysis warnings, file coupling, and quantity of changed and added lines of code. We validated the model against pre-release security testing failures on a large commercial telecommunications system. The model assigned a probability of attack to each file where upon ranking the probabilities in descending order we found that 72% of the attack-prone files are in the top 10% of the ranked files and 90% in the top 20% of the files.", "num_citations": "9\n", "authors": ["144"]}
{"title": "Work in progress: Exploring security and privacy concepts through the development and testing of the iTrust medical records system\n", "abstract": " University computer science and software engineering students must build reliability and security into their software applications from the start of development. A graduate-level Software Testing and Reliability course at North Carolina State University has a learning objective of using appropriate testing techniques for the development of a reliable and secure system. Beginning in Fall 2005, the semester project has involved the development and testing of the open source and freely-available iTrust Medical Records system prototype. Our vision is to build a community of educators, students, and medical professionals that can collaborate and use the iTrust project as a development platform and testbed for secure and reliable application development", "num_citations": "9\n", "authors": ["144"]}
{"title": "Deriving performance requirements and test cases with the performance refinement and evolution model (PREM)\n", "abstract": " Performance is one important attribute of a software system. To develop a software system of acceptable performance, the team needs to specify precise performance requirements, design appropriate test cases, and use appropriate techniques to analyze the performance characteristics. However, the lack of a management framework for performance engineering may impair the effectiveness of the techniques. In this paper, we propose the Performance Refinement and Evolution Model (PREM) as a performance management framework. Based on the specification of quantitative measurement and workloads, PREM classifies performance requirements into four levels. At each level, we show what information should be included in the requirement, and what techniques can be used to estimate the performance. We use PREM on a Web-based student project to define the performance requirements. The process of performance requirements specification and example results are provided in this paper.", "num_citations": "9\n", "authors": ["144"]}
{"title": "Expediting Programmer AWAREness of Anomalous Code\n", "abstract": " Long fix latency, the amount of time between fault injection and fault removal, could substantially increase the cost of a fault fix. To mitigate this cost, software engineers could be made aware of potentially-anomalous code almost as soon as it is written. Test-driven development is a style of programming in which code and tests are written in tight cycles, therefore providing feedback to software engineers early and often. Enhancing test-driven development feedback loops to automatically and continuously provide ranked, prioritized, and filtered alerts to the software engineer on the correctness and security of their code implementation should reduce the cost of fixing software faults. The Automated Warning Application for Reliability Engineering (AWARE) tool is being developed to support enhanced test-driven development.", "num_citations": "9\n", "authors": ["144"]}
{"title": "Agile software development\n", "abstract": " The software engineering discipline has been striving to define best practices for predictably producing high quality software. Often the focus of these best practices has been on``do it right the first time.''Practices have emphasized such things as traceable requirements, detailed high-and low-level designs, inspections, and reviews. In recent years, some methodologists have sought to shake up this view. These methodologists have defined what are termed``agile''methodologies; these methodologies instead have a``get it right the last time''philosophy. The practices of agile methodologies guide software developers to continuously revisit requirements and to minimize up front design. In short, agile practices challenge many of the things software engineering educators have been teaching for decades. Software engineering educators can often be confused on how to incorporate agile practices in their classes, or if\u00a0\u2026", "num_citations": "9\n", "authors": ["144"]}
{"title": "Hypervideo support for distributed extreme programming\n", "abstract": " We present a hardware/software system for support of distributed Extreme Programming, or DXP. It consists of a dual video projector setup with NetMeeting running on a single PC, and a hypervideo system we built called OvalTine. OvalTine allows the creation and automatic tracking of hyperlinks in a video stream, both archived and real-time (as in video conferencing). DXP supports the development of software by a pair of programmers that is non-co-located. One projector displays a shared PC desktop, and another projector displays a life-sized image of each collaborator to the other. OvalTine integration means developers can add hyperlinks to the collaborator camera stream, integrating it with Web pages or other video streams. The DXP work environment gives a better sense of\u2014being there \u201cto the pairs in a pair-programming team. We are experimenting with DXP to measure the productivity of distributed software developers working via different technical processes (paired, individual, full XP, traditional).", "num_citations": "9\n", "authors": ["144"]}
{"title": "Hacker or hero?-extreme programming today (panel session)\n", "abstract": " Extreme programming is the latest rage, everyone is talking extreme, but who is doing it? XP is in the words of one proponent, is a \u201clightweight, efficient, low-risk, predictable, scientific, and fun way to develop software\u201d. XP bundles much conventional software engineering wisdom into a practice with a high degree of appeal as a cool technology. Questions for inquiring minds include: Will XP deliver? Will XP scale? How will products based on software developed by XP practices age? What are the elements of XP that can be effectively adopted by organizations outside the XP envelop, eg large teams, real-time systems, etc. Is XP the next \u201csilver bullet\u201d?Steven Fraser is a senior manager in Global External Research in the Nortel Networks Disruptive Technology program in Santa Clara, California. Previous to this he was an advisor in technology transfer for objectoriented software development best practices. In 1994\u00a0\u2026", "num_citations": "9\n", "authors": ["144"]}
{"title": "Adjusting the instruction of the personal software process to improve student participation\n", "abstract": " No customer is fully satisfied unless they receive a product that does what they want, and they receive it when they want it, defect-free and at an agreed upon price. To address all four of these requirements, Watts Humphrey, at the Software Engineering Institute (SEI), developed the personal software process (PSP), which applies proven quality principles to the work of individual software engineers. PSP was initially taught to practicing software engineers in industry and to graduate students. Earlier this year, Humphrey published a new text, suitable for the beginning software engineering student. This text outlines a process for managing and producing high quality software, reducing the mathematical and statistical rigor of the original PSP, but maintaining a solid base of disciplined practices and an overriding quality philosophy. This paper describes how the PSP has been incorporated into the undergraduate\u00a0\u2026", "num_citations": "9\n", "authors": ["144"]}
{"title": "Systematically developing prevention, detection, and response patterns for security requirements\n", "abstract": " The security community has established a number of knowledge sources, including security catalogues and controls, that capture security expertise and can support elicitation of security requirements. Providing additional guidance on how and when to leverage the security information available in the existing knowledge sources in the context of the given system can support security requirements engineering efforts. The objective of this research is to support analysts in identifying and specifying security requirements by developing and utilizing a systematic process for identifying security requirements patterns from existing knowledge sources. We document our process for systematically analyzing and synthesizing existing knowledge sources to identify a set of security requirements patterns that support a diverse set of security goals. We demonstrate the feasibility of our process by applying it to NIST Special\u00a0\u2026", "num_citations": "8\n", "authors": ["144"]}
{"title": "Using software reliability models for security assessment\u2014Verification of assumptions\n", "abstract": " Can software reliability models be used to assess software security? One of the issues is that security problems are relatively rare under \u201cnormal\u201d operational profiles, while \u201cclassical\u201d reliability models may not be suitable for use in attack conditions. We investigated a range of Fedora open source software security problems to see if some of the basic assumptions behind software reliability growth models hold for discovery of security problems in non-attack situations. We find that in some cases, under \u201cnormal\u201d operational use, security problem detection process may be described as a Poisson process. In those cases, we can use appropriate classical software reliability growth models to assess \u201csecurity reliability\u201d of that software in non-attack situations.", "num_citations": "8\n", "authors": ["144"]}
{"title": "Classifying natural language sentences for policy\n", "abstract": " Organizations derive policies from a wide variety of sources, such business plans, laws, regulations, and contracts. However, an efficient process does not yet exist for quickly finding or automatically deriving policies from uncontrolled natural language sources. The goal of our research is to assure compliance with established policies by ensuring policies in existing natural language texts are discovered, appropriately represented, and implemented. We propose a tool-based process to parse natural language documents, learn which statements signify policy, and then generate appropriate policy representations. To evaluate the initial work on our process, we analyze four data use agreements for a particular project and classify sentences as to whether or not they pertain to policy, requirements, or neither. Our k-nearest neighbor classifier with a unique distance metric had a precision of 0.82 and a recall of 0.81\u00a0\u2026", "num_citations": "8\n", "authors": ["144"]}
{"title": "Towards a taxonomy of techniques to detect cross-site scripting and SQL injection vulnerabilities\n", "abstract": " Since 2002, over half of reported cyber vulnerabilities are caused by input validation vulnerabilities. Over50% of input validation vulnerabilities were cross-site scripting and SQL injection vulnerabilities in 2006, based on the (US) National Vulnerability Database. Techniques to mitigate cross-site scripting and SQL injection vulnerabilities have been proposed. However, applying those techniques without precise understanding can result in a false sense of security. Clearly understanding the advantages and disadvantages of each security technique can provide a basis for comparison of those techniques. This survey provides a taxonomy of techniques to detect cross-site scripting and SQL injection vulnerabilities based upon of 21 papers published in the IEEE and ACM databases. Our taxonomy characterizes the detection methods and evaluation criteria of the techniques. The taxonomy provides a foundation for comparison among techniques to detect crosssite scripting and SQL injection vulnerabilities. Organizations can use the comparison results to choose appropriate techniques depending on available resources.", "num_citations": "8\n", "authors": ["144"]}
{"title": "Identifying fault-prone files using static analysis alerts through singular value decomposition\n", "abstract": " Static analysis tools tend to generate more alerts than a development team can reasonably examine without some form of guidance. In this paper, we propose a technique for leveraging field failures and historical change records to determine which sets of alerts are often associated with a field failure using singular value decomposition. We performed a case study on six major components of an industrial software system at IBM over six builds spanning eighteen months of development. Our technique identified fourteen alert types that comprised sets of alerts that could identify, on average, 45% of future fault-prone files and up to 65% in some instances.", "num_citations": "8\n", "authors": ["144"]}
{"title": "Tool Support For Estimating Software Reliability in Haskell Programs\n", "abstract": " In late-stage phases of development, action to correct defects can be cost prohibitive. Effective, efficient, and expressive measures of reliability during the development cycle could aid developers by showing early warning indications of where the system might require modification or more testing. In this paper, we present initial research in creating an Eclipse plugin that utilizes two methods for estimating reliability in-process in a functional programming environment. One method is based on testing and static code metrics that can be gathered automatically during the coding process. A feasibility study involving a subset of these metrics was performed. The other method is based on the certification of individual lines or sections of code. These certifications are used in conjunction with the operational profiles of these lines or sections to estimate overall system reliability.", "num_citations": "8\n", "authors": ["144"]}
{"title": "Empirical evaluation of agile processes\n", "abstract": " Presently, there is a lot of anecdotal evidence and very little empirical and validated data of agile methods effectiveness. Much of the evidence is based on the stories and preferences of those who practice it. Imagine the benefits of knowing that an XP project expends more effort understanding software requirements than does a team using a typical traditional, or waterfall approach. Imagine the benefits of being able to predict that for this particular combination of customer, product, and project team, a small bit of modeling is going to benefit the team more than a strict XP implementation. The world we seek is one that demystifies the success of agile methods and supports everyday practitioners to apply the right method at the right time. We believe measurements are key to making these decisions and to making agile methods more accessible.", "num_citations": "8\n", "authors": ["144"]}
{"title": "A video-enhanced environment for distributed extreme programming\n", "abstract": " We present a hardware/software system for support of distributed Extreme Programming, or dXP. It consists of a dual video projector, dual PC setup, and an enhanced projected video display. dXP supports the development of software by pairs of programmers that are non-co-located. One projector displays a shared PC desktop, and another projector displays a life-sized image of each collaborator to the other. Developers can add hyperlinks to the collaborator camera stream, integrating it with Web pages or other video streams. A digitized whiteboard serves as the projection surface for the video stream, opening several possibilities for support of pair design work. The dXP work environment gives a better sense of\u2014being there \u201cto the pairs in a pair-programming team. We are experimenting with dXP to measure the productivity of distributed software developers working via different technical processes (paired, individual, full XP, traditional). In full form, this paper will be an experience report about experiments in using this environment for dXP software development.", "num_citations": "8\n", "authors": ["144"]}
{"title": "Pair Programming: Why Have Two Do the Work of One\n", "abstract": " As a head of the team, I initially thought that we might be wasting human resources by having two engineers do the work of one. Moreover, I thought, they'll be chatting and giggling more than working.\u201cToo many cooks spoil the broth,\u201d you know. Though I agreed that there would be fewer syntax errors, I could not com-prehend their timesaving effect. To be frank, quite like everyone else in the team, I did not appreciate the idea. And you know how difficult it was initially to get going with it! But as we delved deeper into the project, I realized that it gave us fewer errors, no slippage, and more than everything... team strength and companionship.", "num_citations": "8\n", "authors": ["144"]}
{"title": "Better together: Comparing vulnerability prediction models\n", "abstract": " ContextVulnerability Prediction Models (VPMs) are an approach for prioritizing security inspection and testing to find and fix vulnerabilities. VPMs have been created based on a variety of metrics and approaches, yet widespread adoption of VPM usage in practice has not occurred. Knowing which VPMs have strong prediction and which VPMs have low data requirements and resources usage would be useful for practitioners to match VPMs to their project\u2019s needs. The low density of vulnerabilities compared to defects is also an obstacle for practical VPMs.ObjectiveThe goal of the paper is to help security practitioners and researchers choose appropriate features for vulnerability prediction through a comparison of Vulnerability Prediction Models.MethodWe performed replications of VPMs on Mozilla Firefox with 28,750 source code files featuring 271 vulnerabilities using software metrics, text mining, and crash data\u00a0\u2026", "num_citations": "7\n", "authors": ["144"]}
{"title": "Improved recognition of security bugs via dual hyperparameter optimization\n", "abstract": " Background: Security bugs need to be handled by small groups of engineers before being widely discussed (otherwise the general public becomes vulnerable to hackers that exploit those bugs). But learning how to separate the security bugs from other bugs is challenging since they may occur very rarely. Data mining that can find such scarce targets required extensive tuning effort. Goal: The goal of this research is to aid practitioners as they struggle to tune methods that try to distinguish security-related bug reports in a product's bug database, through the use of a dual hyperparameter optimizer that learns good settings for both learners and for data pre-processing methods. Method: The proposed method, named SWIFT, combines learner hyperparameter optimization and pre-processor hyperparameter optimization. SWIFT uses a technique called epsilon-dominance, the main idea of which is to ignore operations\u00a0\u2026", "num_citations": "7\n", "authors": ["144"]}
{"title": "DevCOP: A software certificate management system for Eclipse\n", "abstract": " During the course of software development, developers will employ several different verification and validation (V&V) practices with their software. However, these efforts might not be recorded or maintained in an effective manner. We have built defect estimation with V&V Certificates on Programming (DevCOP), a software certificate management system. With DevCOP, developers can automatically track and maintain a persistent record of the V&V practices used during development via certificates. With this V&V information, developers and managers can better manage their V&V efforts within a system. Detailed information such as coverage of particular V&V techniques over the system or the amount of V&V performed on a single function can be provided. Developers can also use this V&V information post hoc to see which techniques were more effective at removing defects. In our future work, we are researching a\u00a0\u2026", "num_citations": "7\n", "authors": ["144"]}
{"title": "A Method for Verification and Validation Certificate Management in Eclipse\n", "abstract": " Software certification demonstrates the reliability, safety, or security of software systems in such a way that it can be checked by an independent authority with minimal trust in the techniques and tools used in the certification process itself. It can build on existing validation and verification (V&V) techniques but introduces the notion of explicit software certificates, which contain all the information necessary for an independent assessment of the demonstrated properties. Software certificates support a product-oriented assurance approach, combining different techniques and forms of evidence (eg, fault trees,\u201csign-offs\u201d, safety cases, formal proofs,...) and linking them to the details of the underlying software.A software certificate management system provides the infrastructure to create, maintain, and analyze software certificates. It combines functionalities of a database (eg, storing and retrieving certificates) and a make-tool (eg, incremental re-certification). It can also maintain links between system artifacts (eg, design documents, engineering data sets, or programs) and different varieties of certificates, check the validity of certificates, provide access to explicit audit trails, enable browsing of certification histories, and enforce system-wide certification and release policies. It can at any time provide current information about the certification status of each component in the system, check whether certificates have been audited, compute which certificates remain valid after a system modification, or even automatically start an incremental recertification.", "num_citations": "7\n", "authors": ["144"]}
{"title": "Using in-process metrics to predict defect density in haskell programs\n", "abstract": " In late-stage phases of development, action to correct defects can be cost prohibitive. Effective, efficient, and expressive measures of reliability during the development cycle could aid developers by providing early warning signs of where the system might require modification or further testing. To this end, this paper presents a method for estimating defect density in a system using a suite of internal metrics for Haskell programs. A feasibility study of this method was conducted by analyzing the source code of seven released versions of the Glasgow Haskell Compiler. Further studies are being conducted to refine the metric suite and to examine the potential of the method.", "num_citations": "7\n", "authors": ["144"]}
{"title": "\" If this is what it's really like, maybe I better major in English\": integrating realism into a sophomore software engineering course\n", "abstract": " In the Fall 1998, our sophomore computer science students took a new course called \"Software Practice. \" At this early stage in their academic career, the course gave them practical, realistic exposure to the process of creating large software systems in teams. Their previous CS1/CS2 experience focused on the development of C++ language proficiency skills on the Unix platform, with short, programming-in-the-small assignments reinforcing the \"skill of the week.\" Much to the students' periodic frustration and anguish, the Software Practice class differed in most every way-having one, programming-in-the-large, full semester project on the Windows NT/Visual C++ platform that often forced them to independently research new programming techniques. Course evaluations indicate that the students found the course, with its stark view into the realism of software development, enlightening, highly valuable and fun.", "num_citations": "7\n", "authors": ["144"]}
{"title": "Measuring security practice use: A case study at IBM\n", "abstract": " Software development teams apply security practices to prevent vulnerabilities in the software they ship. However, vulnerabilities can be difficult to find, and security practices take time and effort. Stakeholders can better guide software development if they have empirical data on how security practices are applied by development teams. The goal of this paper is to inform managers and developers on the use of security practices through a case study of an industrial software team so that managers and developers can base their security practice adoption decisions on empirical evidence. We present a case study of security practice use in a typical software development project at IBM. We collected empirical data from three perspectives: qualitative observations, a survey of the team members, and text mining of the team's development history. The team's top three practices were \"Track Vulnerabilities\", \"Apply Secure\u00a0\u2026", "num_citations": "6\n", "authors": ["144"]}
{"title": "Metric-based quality evaluations for iterative software development approaches like Agile\n", "abstract": " Presents a collection of slides covering the following topics: software reliability engineering; process structure; metrics re-evaluation; integration testing; project profile; quality metrics; code stability; quality control chart; and fault convergence.", "num_citations": "6\n", "authors": ["144"]}
{"title": "Does hardware configuration and processor load impact software fault observability?\n", "abstract": " Intermittent failures and nondeterministic behavior complicate and compromise the effectiveness of software testing and debugging. To increase the observability of software faults, we explore the effect hardware configurations and processor load have on intermittent failures and the nondeterministic behavior of software systems. We conducted a case study on Mozilla Firefox with a selected set of reported field failures. We replicated the conditions that caused the reported failures ten times on each of nine hardware configurations by varying processor speed, memory, hard drive capacity, and processor load. Using several observability tools, we found that hardware configurations that had less processor speed and memory observed more failures than others. Our results also show that by manipulating processor load, we can influence the observability of some faults.", "num_citations": "6\n", "authors": ["144"]}
{"title": "Automated adaptive ranking and filtering of static analysis alerts\n", "abstract": " Static analysis tools are useful in finding recurring software faults and weaknesses during the development process. However, these tools often report a high number of false positives, dissuading software engineers from frequent use of the tools during development. By ranking static analysis alerts by the probability the alert is a true positive, software engineers can be directed to the faults that are most likely to need attention. The ranking is based on historical data from the filtering of alerts previously found to be false positives by a software engineer. The Automated Warning Application for Reliability Engineering (AWARE) v 0.2 has been created to support static analysis alert ranking and filtering. Initial results from a feasibility study show that with AWARE, true positive alerts appear at the top of the ranking and the distance between true positive alerts are better than a random ordering of alerts. By filtering a small number of false positives, AWARE can provide true positive alerts to the software engineer.", "num_citations": "6\n", "authors": ["144"]}
{"title": "OpenSeminar: Web-based Collaboration Tool for Open Educational Resources\n", "abstract": " Today, it is common for university instructors to compile links to online resources for the courses they teach, either as a supplement to a textbook, reading packet or, in some cases, as a replacement. Finding resources and keeping them up to date is time consuming. Instructors who teach similar subjects at different universities might benefit from working together to compile and share online resources for their courses. This paper describes a tool for sharing instructional materials called OpenSeminar. The purpose of OpenSeminar is to facilitate the structured compilation of open, online resources among a group of collaborators who share a common area of interest. Using OpenSeminar, the selection of resources can be customized and deployed to meet the specific needs of each professor. OpenSeminar adopts an editorial peer review framework to ensure content quality control. An illustration of OpenSeminar in the\u00a0\u2026", "num_citations": "6\n", "authors": ["144"]}
{"title": "Towards the prioritization of system test cases\n", "abstract": " During software development companies are frequently faced with lack of time and resources, which limits their ability to effectively complete testing efforts. Often, the engineering team is compelled to stop their testing efforts abruptly due to schedule pressures. We build upon prior test case prioritization research and present a system-level, value-driven approach to test case prioritization called the Prioritization of Requirements for Test (PORT). PORT involves analyzing and assigning value to each requirement based on four factors: requirements volatility, customer priority, implementation complexity, and fault proneness. System test cases are prioritized for execution based on the assigned priority value of the requirements they originate from such that the test cases for requirements with higher priority are executed earlier during system test. We applied PORT to four student team projects in an advanced graduate software testing class. Our results show that PORT prioritization at the system level improves the rate of detection of severe failures. Additionally, we found that customer priority was the most important contributor towards improved rate of failure detection.", "num_citations": "6\n", "authors": ["144"]}
{"title": "A multidisciplinary virtual student team\n", "abstract": " \u201cIn education, we should not strive for getting perfect answers, but instead we should learn from mistakes\u201d(Haataja, J., 1998)This paper summarizes the initial assessment on the multidisciplinary virtual teaming aspect of a unique educational project conducted among five universities and a telecommunication company. The students were tasked with providing tactical and strategic recommendations for improving the company\u2019s business. The project was unique in the sense that it challenged each student to learn from his/her mistakes, while collaborating with students from other disciplines and incrementally improve their recommendation. A \u201cperfect\u201d recommendation did not exist for the project as the solution demands a holistic approach with intuitive thinking. The paper in particular, investigates the communication and collaboration pattern that occurred during the virtual teaming session. Virtual teams are increasingly global, creating communication and coordination challenges due to factors such as distance, multiple time zones, and sometimes, cultural differences. The various aspects of virtual team as well as recommendation on technology/social norms involved in setting up such a team are also depicted. Through this study, we intend to provide insights in formulating multidisciplinary virtual teams for students.", "num_citations": "6\n", "authors": ["144"]}
{"title": "Instilling a defect prevention philosophy [software engineering education]\n", "abstract": " In software, teaching a programming language and how to compile and execute programs allows people to write programs immediately. Very likely, such programs will require considerable debugging. Errors and unit debugging are just an expected and integral part of programming. However, people with the right education and training need not unit debug their software any more than people need to look at the keys when they type. Serious programming begins only with formal methods, more explicit design, and verification from specifications. A proven software development process, Cleanroom Software Engineering, adds engineering rigor to the process and focuses on defect prevention and statistical quality control. Cleanroom has historically produced software with significantly superior quality and improved productivity. As a result, the University of Utah offers a new undergraduate course in Cleanroom\u00a0\u2026", "num_citations": "6\n", "authors": ["144"]}
{"title": "Software development with feature toggles: practices used by practitioners\n", "abstract": " BackgroundUsing feature toggles is a technique that allows developers to either turn a feature on or off with a variable in a conditional statement. Feature toggles are increasingly used by software companies to facilitate continuous integration and continuous delivery. However, using feature toggles inappropriately may cause problems which can have a severe impact, such as code complexity, dead code, and system failure. For example, the erroneous repurposing of an old feature toggle caused Knight Capital Group, an American global financial services firm, to go bankrupt due to the implications of the resultant incorrect system behavior.AimThe goal of this research project is to aid software practitioners in the use of practices to support software development with feature toggles through an empirical study of feature toggle practice usage by practitioners.MethodWe conducted a qualitative analysis of 99 artifacts\u00a0\u2026", "num_citations": "5\n", "authors": ["144"]}
{"title": "Identifying security issues in software development: are keywords enough?\n", "abstract": " Identifying security issues before attackers do has become a critical concern for software development teams and software users. While methods for finding programming errors (eg fuzzers 1, static code analysis [3] and vulnerability prediction models like Scandariato et al.[10]) are valuable, identifying security issues related to the lack of secure design principles and to poor development processes could help ensure that programming errors are avoided before they are committed to source code.", "num_citations": "5\n", "authors": ["144"]}
{"title": "Teaching Secure Software Development Through an Online Course.\n", "abstract": " With an increasing number of cybersecurity attacks threatening consumers, organizations, and governments, the need for trained software security professionals is greater than ever. However, the industry is experiencing a shortage in security professionals for roles at all levels of cybersecurity. Massively Open Online Courses (MOOCs) offer educators an opportunity to retrain current professionals on cybersecurity topics to meet new and ongoing threats. The goal of this paper is to assist instructors of online software security courses in making their courses engaging and effective. In this paper, we present the details of our online software security course, including the technologies used and the material presented. We conducted a pre-and post-survey of course participants and report information on their backgrounds, motivations, and learning objectives. Based on our reflection on the course, we recommend that future instructors of online security courses seed peer discussion on online discussion forums, carefully choose their course platform, and have professionally shot lecture videos.", "num_citations": "5\n", "authors": ["144"]}
{"title": "Branching taxonomy\n", "abstract": " The development of software products are often managed through the use of Version Control Systems (VCS). VSCs allow for versioning of source code and manage the process of merging newly developed code into the product primarily through the use of branches. The branching architecture and the development processes used by the product groups are inter-related and have often evolved over many releases to reflect unique aspects of the product, the development preferences of the team, and the structure of the development organization. Within Microsoft, there is a lot of commonality in the development tools used by the product teams, but there is great diversity in the development processes, and the structure and use of branching. This diversity in the development processes across Microsoft provides a unique learning platform to characterize the effectiveness of specific practices when applied to specific product developments.Understanding the characteristics of different software development processes is becoming increasingly important as a number of product groups are migrating from developing a box set product to simultaneously developing a product deployed as a service, released as a stand-alone product, and shared as a component with other product groups within Microsoft. To assist the product group in making their decisions regarding the most appropriate branching architecture for their product, this document examines the benefits and drawbacks for different branching architectures. It also delineates the issues product groups often have in managing the simultaneous development and release of multiple products that\u00a0\u2026", "num_citations": "5\n", "authors": ["144"]}
{"title": "Proposing regulatory-driven automated test suites for electronic health record systems\n", "abstract": " In regulated domains such as finance and health care, failure to comply with regulation can lead to financial, civil and criminal penalties. While systems vary from organization to organization, regulations apply across organizations. We propose the use of Behavior-Driven-Development (BDD) scenarios as the basis of an automated compliance test suite for standards such as regulation and interoperability. Such test suites could become a shared asset for use by all systems subject to these regulations and standards. Each system, then, need only create their own system-specific test driver code to automate their compliance checks. The goal of this research is to enable organizations to compare their systems to regulation in a repeatable and traceable way through the use of BDD. To evaluate our proposal, we developed an abbreviated HIPAA test suite and applied it to three open-source electronic health record\u00a0\u2026", "num_citations": "5\n", "authors": ["144"]}
{"title": "Cataloging and comparing logging mechanism specifications for electronic health record systems\n", "abstract": " Electronic health record (EHR) systems must log all transactions with protected health information (PHI) to deter unauthorized behavior and prevent users from denying that they created, read, updated, or deleted PHI. However, a plethora of standardization and governing organizations publish documentation (such as standards, suggestions, and requirements) to outline transactions that should be logged and the data that should be captured for each log entry. The objective of this research is to guide the design of electronic health record systems by cataloging suggested information that should be captured by logging mechanisms from both healthcare and non-healthcare documentation. In this paper, we focus on three types of information: data transactions, security events, and log entry content. We collect a set of ten healthcare-related and six non-healthcare related documents that contain specifications for logging mechanisms. From these 16 sources, we catalog 11 data transactions, 77 security events, and 22 data elements for log entry content. Overall, we identify 14 security events and 2 data elements for log entry content that are not explicitly addressed by healthcare documents. We found that developers must consider 13 of the 16 documents to extract 100% of the security events and log entry content cataloged.", "num_citations": "5\n", "authors": ["144"]}
{"title": "On the use of issue tracking annotations for improving developer activity metrics\n", "abstract": " Understanding and measuring how teams of developers collaborate on software projects can provide valuable insight into the software development process. Currently, researchers and practitioners measure developer collaboration with social networks constructed from version control logs. Version control change logs, however, do not tell the whole story. The collaborative problem-solving process is also documented in the issue tracking systems that record solutions to failures, feature requests, or other development tasks. We propose two annotations to be used in issue tracking systems: solution originator and solution approver. We annotated which developers were originators or approvers of the solution to 602 issues from the OpenMRS healthcare system. We used these annotations to augment the version control logs and found 47 more contributors to the OpenMRS project than the original 40 found in the version control logs. Using social network analysis, we found that approvers are likely to score high in centrality and hierarchical clustering. Our results indicate that our two issue tracking annotations identify project collaborators that version control logs miss. Thus, issue tracking annotations are an improvement in developer activity metrics that strengthen the connection between what we can measure in the project development artifacts and the team's collaborative problem-solving process.", "num_citations": "5\n", "authors": ["144"]}
{"title": "How should software reliability engineering (SRE) be taught?\n", "abstract": " This article on teaching software reliability engineering (SRE) represents a consensus of views of experienced software reliability engineering leaders from diverse backgrounds but with ties to education: directors of software reliability and software reliability training in industry, a consultant who teaches SRE practice to industry, and university professors. The first topic covered is how to attract participants to SRE courses. We then analyze the job-related educational needs of current and future (those now university students) software practitioners, SRE practitioners, researchers, and nonsoftware professionals. Special needs relating to backgrounds, limited proficiency in the course language, and work conflicts are outlined. We discuss how the needs presented should influence course content and structure, teaching methods, and teaching materials. Finally, we cover our experiences with distance learning and its\u00a0\u2026", "num_citations": "5\n", "authors": ["144"]}
{"title": "Certification of code during development to provide an estimate of defect density\n", "abstract": " In industry, information on defect density of a product tends to become available too late in the software development process to affordably guide corrective actions. Our research objective is to build a parametric model which utilizes a persistent record of the validation and verification (V&V) practices used with a program to estimate the program\u2019s defect density. The persistent record of the V&V practices are recorded as certificates which are automatically recorded and maintained with the code.", "num_citations": "5\n", "authors": ["144"]}
{"title": "Debunking the Geek Stereotype with Software Engineering Education.\n", "abstract": " \u2022 Summary\u2022 Acknowledgement: This material is based upon work supported by the National Science Foundation under Grant", "num_citations": "5\n", "authors": ["144"]}
{"title": "Collaboration through agile software development practices: Student interviews and lab observations\n", "abstract": " The first semester of a 3-year longitudinal study in advanced undergraduate software engineering class, CSC326, has been completed. A group of researchers at NC", "num_citations": "5\n", "authors": ["144"]}
{"title": "The Collaborative Software Process (SM)\n", "abstract": " Anecdotal and qualitative evidence from industry indicates that two programmers working side by side at one computer, collaborating on the same design, algorithm, code, or test, perform substantially better than the two working alone. Statistical evidence has shown that programmers perform better when following a defined, repeatable process such as the Personal Software Process SM (PSP SM). Bringing these two ideas together, the Collaborative Software Process SM (CSP SM) has been formulated. The CSP is a defined, repeatable process for two programmers working collaboratively. The CSP is an extension of the PSP, and it relies upon the foundation of the PSP.", "num_citations": "5\n", "authors": ["144"]}
{"title": "Understanding software security from design to deployment\n", "abstract": " Analyzing, implementing and maintaining security requirements of software-intensive systems and achieving truly secure software requires planning for security from ground up, and continuously assuring that security is maintained across the software's lifecycle and even after deployment when software evolves. Given the increasing complexity of software systems, new application domains, dynamic and often critical operating conditions, the distributed nature of many software systems, and fast moving markets which put pressure on software vendors, building secure systems from ground up becomes even more challenging. Security-related issues have previously been targeted in software engineering sub-communities and venues. In the second edition of the International Workshop on Security from Design to Deployment (SEAD) at the International Conference on Automated Software Engineering (ASE) 2020, we\u00a0\u2026", "num_citations": "4\n", "authors": ["144"]}
{"title": "Secure software lifecycle knowledge area\n", "abstract": " The purpose of this Secure Software Lifecycle knowledge area is to provide an overview of software development processes for implementing secure software from the design of the software to the operational use of the software. This implementation may involve new coding as well as the incorporation of third party libraries and components. The goal of this overview is for use in academic courses in the area of software security; and to guide industry professionals who would like to use a secure software lifecycle.The Software Security Knowledge Area in the Cyber Security Body of Knowledge provides a structured overview of secure software development and coding and the known categories of software implementation vulnerabilities and of techniques that can be used to prevent or detect such vulnerabilities or to mitigate their exploitation. By contrast, this Secure Software Lifecycle Knowledge Area focuses on the components of a comprehensive software development process to prevent and detect security defects and to respond in the event of an exploit.", "num_citations": "4\n", "authors": ["144"]}
{"title": "Discovering Decision-Making Patterns for Security Novices and Experts\n", "abstract": " Security analysis requires some degree of knowledge to align threats to vulnerabilities in information technology. Despite the abundance of security requirements, the evidence suggests that security experts are not applying these checklists. Instead, they default to their background knowledge to identify security vulnerabilities. To better understand the different effects of security checklists, analysis and expertise, we conducted a series of interviews to capture and encode the decisionmaking process of security experts and novices during three security requirements analysis exercises. Participants were asked to analyze three kinds of artifacts: source code, data flow diagrams, and network diagrams, for vulnerabilities, and then to apply a requirements checklist to demonstrate their ability to mitigate vulnerabilities. We framed our study using Situation Awareness theory to elicit responses that were analyzed using coding theory and grounded analysis. Our results include decision-making patterns that characterize how analysts perceive, comprehend and project future threats, and how these patterns relate to selecting security mitigations. Based on this analysis, we discovered new theory to measure how security experts and novices apply attack models and how structured and unstructured analysis enables increasing security requirements coverage. We discuss suggestions of how our method could be adapted and applied to improve training and education instruments of security analysts.", "num_citations": "4\n", "authors": ["144"]}
{"title": "Access control policy evolution: An empirical study\n", "abstract": " Access control policies (ACPs) are necessary mechanisms for protection of critical resources and applications. As operational and security requirements of a system evolve, so do access control policies. It is important to help policy authors in effectively managing access control policies by providing insights into historical trends and evolution patterns of access control policies. We analyzed ACP evolution in three systems: Security Enhanced Linux (SELinux) operating system, Virtual Computing Laboratory (VCL) cloud, and a network intrusion detection (Snort) application. We propose an approach, which extracts evolution patterns based on the analysis of ACP historical change data. An evolution pattern indicates an abstraction of change in the permissions/privileges assigned to a group or a user. We then developed a model of ACPs evolution. We found eight frequently occurring evolution patterns across the three\u00a0\u2026", "num_citations": "4\n", "authors": ["144"]}
{"title": "Determining\" Grim Reaper\" Policies to Prevent Languishing Bugs\n", "abstract": " Long-lived software products commonly have a large number of reported defects, some of which may not be fixed for a lengthy period of time, if ever. These so-called languishing bugs can incur various costs to project teams, such as wasted time in release planning and in defect analysis and inspection. They also result in an unrealistic view of the number of bugs still to be fixed at a given time. The goal of this work is to help software practitioners mitigate their costs from languishing bugs by providing a technique to predict and pre-emptively close them. We analyze defect fix times from an ABB program and the Apache HTTP server, and find that both contain a substantial number of languishing bugs. We also train decision tree classification models to predict whether a given bug will be fixed within a desired time period. We propose that an organization could use such a model to form a \"grim reaper\" policy, whereby\u00a0\u2026", "num_citations": "4\n", "authors": ["144"]}
{"title": "Access control policy identification and extraction from project documentation\n", "abstract": " While access control mechanisms have existed in computer svstems since the 1960s, modern system developers often fail to ensure appropriate mechanisms are implemented within particular systems. Such failures allow for individuals, both benign and malicious, to view and manipulate information that they should not otherwise be able to access. The qoal of our research is to help developers improve security by ertractin q the access control policies implicitly and erplicitlu defined in natural lanquaqe project artifacts. Developers can then verify and implement the extracted access control policies within a svstem. We propose a machine-learning based process to parse existing, unaltered natural language documents, such as requirement or technical specifications to extract the relevant subjects, actions, and resources for an access control policy. To evaluate our approach, we analyzed a public requirements\u00a0\u2026", "num_citations": "4\n", "authors": ["144"]}
{"title": "Proposing Regulatory-Driven Automated Test Suites\n", "abstract": " In regulated domains such as finance and health care, failure to comply with regulation can lead to financial, civil and criminal penalties. While systems vary from organization to organization, the same regulations apply for all systems. As a result, efficiencies could be gained if the commonalities between systems could be captured in public, shared, test suites for regulations. We propose the use of Behavior-Driven-Development (BDD) technology to create these test suites. With BDD, desired system behavior with respect to regulatory requirements can be captured as constrained natural language 'scenarios'. The scenarios can then be automated through system-specific test drivers. The goal of this research is to enable organizations to compare their systems to regulation in a repeatable and traceable way through the use of BDD. To evaluate our approach, we developed seven scenarios based on the HITECH Act\u00a0\u2026", "num_citations": "4\n", "authors": ["144"]}
{"title": "Secure logging and auditing in electronic health records systems: what can we learn from the payment card industry\n", "abstract": " Secure logging and auditing in electronic health records systems | Proceedings of the 3rd USENIX conference on Health Security and Privacy ACM Digital Library home ACM home Google, Inc. (search) Advanced Search Browse About Sign in Register Advanced Search Journals Magazines Proceedings Books SIGs Conferences People More Search ACM Digital Library SearchSearch Advanced Search Browse Browse Digital Library Collections More HomeBrowse by TitleProceedingsHealthSec'12Secure logging and auditing in electronic health records systems: what can we learn from the payment card industry Article Secure logging and auditing in electronic health records systems: what can we learn from the payment card industry Share on Authors: Jason King Department of Computer Science, North Carolina State University Department of Computer Science, North Carolina State University View Profile , Laurie '\u2026", "num_citations": "4\n", "authors": ["144"]}
{"title": "A measurement framework of alert characteristics for false positive mitigation models\n", "abstract": " Automated static analysis tools can be used to identify potential source code anomalies early in the software process that could lead to field failures. However, only a small portion of static analysis alerts may be important to the developer (actionable). The remainder are false positives (unactionable). Static analysis tools may generate an overwhelming number of alerts, the majority of which are likely to be unactionable. False positive mitigation techniques utilize information about static analysis alerts, called alert characteristics, to predict actionable and unactionable alerts. This paper presents a measurement framework for generating static analysis alert characteristics for false positive mitigation models.", "num_citations": "4\n", "authors": ["144"]}
{"title": "Examining time as a factor in young women\u2019s information technology career decisions\n", "abstract": " The research problem addressed in this chapter is a synthesis of findings from several projects conducted at North Carolina State University. From the synthesis we address possible reasons for young women\u2019s disinterest in information technology careers. Interviews with high school girls and college computer science women revealed that time was a major factor in the high school girls\u2019 career choices and a possible factor in the retention of some women computer science majors in careers. Our results speak to some of the research findings of those who study the millennium generation, especially around work/life balance issues. While all of these girls and women did not explicitly state the concern, they perceive a need for independence, flexibility, and part-time work that is embedded within issues of work/life balance.", "num_citations": "4\n", "authors": ["144"]}
{"title": "An introduction to performance testing\n", "abstract": " Performance Testing is an important technique for ensuring a system will run as expected by the customer. In this chapter, we will explain the following:\u2022 the role of performance requirements in performance testing\u2022 information on how to do performance testing\u2022 when to consider performance in the software development lifecycle", "num_citations": "4\n", "authors": ["144"]}
{"title": "Managing software performance engineering activities with the performance refinement and evolution model (prem)\n", "abstract": " Performance is one of the important non-functional requirements for a software system. A system that runs too slowly is likely to be rejected by all users. Failure to achieve some expected performance level might make the system unusable, and the project might fail or get cancelled if the system performance objective is not met [38]. To build performance into a software system, the development team needs to take performance into consideration through the whole development cycle [13]. Many models, techniques, and methodologies are proposed, trying to address the software performance problems. However, those solutions require different levels of understanding of the performance characteristics. For example, using an unnecessarily complicated performance model during the early stages may require the development team to make assumptions for unknown performance factors. The model may be inaccurate or even useless. The development team needs to choose a proper performance technique, based on the understanding of the performance characteristics, for the technique to provide useful feedback.In this report, we propose the Performance Refinement and Evolution Model (PREM) to manage software performance development. As shown in Figure 1, PREM is a four-level model, in which a higher level means the better understanding of the performance characteristics. The model shows, at each level, how the performance requirements and test cases are. With PREM, the development team starts from Level 0 performance requirements specification. Then the team can apply the techniques from PREM-0 to understand the\u00a0\u2026", "num_citations": "4\n", "authors": ["144"]}
{"title": "Resources for agile software development in the software engineering course\n", "abstract": " Agile software development is an emerging topic in software engineering. Industrial use of agile techniques is growing faster than our educational resources are being updated. Using an open courseware platform, OpenSeminar, we provide course resources for software engineering students about agile software development methodologies. These resources include readings, sites, lab exercises, lectures, examples, testing practices, and testing frameworks", "num_citations": "4\n", "authors": ["144"]}
{"title": "Workshop 3: Integrating Agile Practices into Software Engineering Courses\n", "abstract": " Agile software development methodologies claim to be superior for adapting to the changing needs of customers and projects and to the people on the team. As a result, these methodologies are steadily gaining interest and popularity in industry. Some examples of agile methodologies are Extreme Programming, Feature-Driven Development, Scrum, and Win-Win Spiral. Each of these processes comprises a set of practices, some of which are currently considered best practices and are consistent with what is taught in software engineering courses today. Other practices, however, are controversial and run contrary to the focus of most curricular materials. As educators, we must assess the academic and technical values of emerging technologies and, if convinced of their worthiness, we owe it to our students to integrate them into our curricula.", "num_citations": "4\n", "authors": ["144"]}
{"title": "Tailored CMM for a Small e-Commerce Company Level 2: Repeatable\n", "abstract": " E-commerce software developers are under pressure to develop applications at a record pace. The widespread lack of process discipline and procedural guidance available for such market driven environments highlights the need for a framework to support practitioners\u2019 development efforts. The EPRAM Model (NCSU Computer Science TR-2001-08) explicitly addresses the challenges inherent in small-team rapid development projects requiring no more than 12 people. The model was intentionally designed to comply with the Level 2 KPAs (Key Process Areas) of the Software Engineering Institute\u2019s Capability Maturity Model (CMM); it combines evolutionary prototyping with an aggressive risk mitigation strategy to ensure that proposed requirements adhere to all established security and privacy policies. This document presents the Tailored Capability Maturity Model for small E-Commerce development teams.", "num_citations": "4\n", "authors": ["144"]}
{"title": "EPRAM: Evolutionary prototyping risk analysis & mitigation (e-commerce software development process document)\n", "abstract": " E-commerce software developers are under pressure to develop applications at a record pace. The widespread lack of process discipline and procedural guidance available for such market driven environments highlights the need for a framework to support practitioners\u2019 development efforts. We present a validated evolutionary software development process, the EPRAM (Evolutionary Prototyping with Risk Analysis and Mitigation) Model, for rapid development in e-commerce (electronic commerce) environments. The EPRAM Model explicitly addresses the challenges inherent in small-team rapid development projects requiring no more than 12 people. The model was intentionally designed to comply with the Level 2 KPAs (Key Process Areas) of the Software Engineering Institute\u2019s Capability Maturity Model (CMM); it combines evolutionary prototyping with an aggressive risk mitigation strategy to ensure that proposed requirements adhere to all established security and privacy policies. This process document details the model and includes a set of 12 document templates for practitioners to employ as needed in a rapid development effort.", "num_citations": "4\n", "authors": ["144"]}
{"title": "A synopsis of static analysis alerts on open source software\n", "abstract": " Static application security testing (SAST) tools detect potential code defects (alerts) without having to execute the code. SASTs are now widely used in practice by both commercial and open source software (OSS). Prior work found that half of the state-of-the-art OSS projects have already employed automated static analysis [1]. However, little public information is available regarding the actionability (important to developers to act upon) of SAST alerts.", "num_citations": "3\n", "authors": ["144"]}
{"title": "Categorizing Defects in Infrastructure as Code\n", "abstract": " Infrastructure as code (IaC) scripts are used to automate the maintenance and configuration of software development and deployment infrastructure. IaC scripts can be complex in nature, containing hundreds of lines of code, leading to defects that can be difficult to debug, and lead to wide-scale system discrepancies such as service outages at scale. Use of IaC scripts is getting increasingly popular, yet the nature of defects that occur in these scripts have not been systematically categorized. A systematic categorization of defects can inform practitioners about process improvement opportunities to mitigate defects in IaC scripts. The goal of this paper is to help software practitioners improve their development process of infrastructure as code (IaC) scripts by categorizing the defect categories in IaC scripts based upon a qualitative analysis of commit messages and issue report descriptions. We mine open source version control systems collected from four organizations namely, Mirantis, Mozilla, Openstack, and Wikimedia Commons to conduct our research study. We use 1021, 3074, 7808, and 972 commits that map to 165, 580, 1383, and 296 IaC scripts, respectively, collected from Mirantis, Mozilla, Openstack, and Wikimedia Commons. With 89 raters we apply the defect type attribute of the orthogonal defect classification (ODC) methodology to categorize the defects. We also review prior literature that have used ODC to categorize defects, and compare the defect category distribution of IaC scripts with 26 non-IaC software systems. Respectively, for Mirantis, Mozilla, Openstack, and Wikimedia Commons, we observe (i) 49.3 contain syntax and\u00a0\u2026", "num_citations": "3\n", "authors": ["144"]}
{"title": "How bad is it, really? an analysis of severity scores for vulnerabilities: poster\n", "abstract": " To date, vulnerability research has focused on the binary classification of code as vulnerable or not vulnerable. To better understand the conditions in which vulnerabilities occur, researchers must consider the severity of these vulnerabilities in addition to a binary classification system. To explore this issue, we mined 2,979 publicly disclosed vulnerabilities from Fedora 24 and 25. We then found severity scores from the Common Vulnerability Scoring System (CVSS) and plotted the distribution of these vulnerabilities. We found that publicly scored vulnerabilities skew high, with few vulnerabilities rated lower than a 5. We then explore other potential issues with the use of CVSS in practice, such as imbalances in Confidentiality, Availability, and Integrity scores.", "num_citations": "3\n", "authors": ["144"]}
{"title": "On coverage-based attack profiles\n", "abstract": " Automated cyber attacks tend to be schedule and resource limited. The primary progress metric is often \"coverage\" of pre-determined \"known\" vulnerabilities that may not have been patched, along with possible zero-day exploits (if such exist). We present and discuss a hypergeometric process model that describes such attack patterns. We used web request signatures from the logs of a production web server to assess the applicability of the model.", "num_citations": "3\n", "authors": ["144"]}
{"title": "Kerb and urban increment of highly time-resolved trace elements in PM10, PM2. 5 and PM1. 0 winter aerosol in London during ClearfLo 2012\n", "abstract": " Ambient concentrations of trace elements with 2h time resolution were measured in PM10\u2212 2.5, PM2. 5\u2212 1.0 and PM1. 0\u2212 0.3 size ranges at kerbside, urban background and rural sites in London during winter 2012. Samples were collected using rotating drum impactors (RDIs) and subsequently analysed with synchrotron radiation-induced X-5 ray fluorescence spectrometry (SR-XRF). Quantification of kerb and urban increments (defined as kerb-to-urban and urban-to-rural concentration ratios, respectively), and assessment of diurnal and weekly variability provided insight into sources governing urban air quality and the effects of urban micro-environments on human exposure. Traffic-related elements yielded the highest kerb increments, with values in the range 10 of 11.6 to 18.5 for SW winds (3.6\u20139.4 for NE) observed for elements influenced by brake wear (eg Cu, Sb, Ba) and 5.6 to 8.0 for SW (2.6\u20136.5 for NE) for other trafficrelated processes (eg Cr, Fe, Zn). Kerb increments for these elements were highest in the PM10\u2212 2.5 mass fraction, roughly 3 times that of the PM1. 0\u2212 0.3 fraction. These elements also showed the highest urban increments (\u223c 3.0), although no difference was 15 observed between brake wear and other traffic-related elements. Traffic-related elements exhibited higher concentrations during morning and evening rush hour, and on weekdays compared to weekends, with the strongest trends observed at the kerbside site, and additionally enhanced by winds coming directly from the road, consistent with street canyon effects. Elements related to mineral dust (eg Al, Ca, Sr) showed sig-20 nificant influences from traffic-induced\u00a0\u2026", "num_citations": "3\n", "authors": ["144"]}
{"title": "Designing Technology to Impact Classroom Practice: How Technology Design for Learning Can Support Both Students and Teachers.\n", "abstract": " Abstract Title Page Title: Designing Technology to Impact Classroom Practice: How Technology Design for Learning Can Support Bot", "num_citations": "3\n", "authors": ["144"]}
{"title": "Can fault prediction models and metrics be used for vulnerability prediction?\n", "abstract": " Finding security vulnerabilities requires a different mindset than finding general faults in software-thinking like an attacker. Therefore, security engineers looking to prioritize security inspection and testing efforts may be better served by a prediction model that indicates security vulnerabilities rather than faults. At the same time, faults and vulnerabilities have commonalities that may allow development teams to use traditional fault prediction models and metrics for vulnerability prediction. The goal of our study is to determine whether fault prediction models can be used for vulnerability prediction or if specialized vulnerability prediction models should be developed when both are built with traditional metrics of complexity, code churn, and fault history. We have performed an empirical study on a widely-used, large open source project, the Mozilla Firefox web browser, where 20% of the source code files have faults and only 3% of the files have vulnerabilities. Both the fault prediction model and the vulnerability prediction model predicted vulnerabilities with high recall (over 90%) and low precision (9%). The precision from these vulnerability predictions was much lower than the precision from fault prediction (47%). Our results suggest that fault prediction models based upon traditional metrics can be substituted for specialized vulnerability prediction models, but requires significant improvement to reduce false positives.", "num_citations": "3\n", "authors": ["144"]}
{"title": "Auto mutual information analysis with order patterns for epileptic EEG\n", "abstract": " In this study, we investigated auto mutual information (AMI), based on order patterns analysis, as a tool to evaluate the dynamical characteristics of electroencephalogram (EEG) during interictal, preictal and ictal phase, respectively. Permutation entropy quantifies regularity in time series, while AMI detects the mutual information (MI) between a time series and a delayed version of itself. The results show that AMI method was able to reveal that the highest entropy values were assigned to interictal EEG recordings and the lowest entropy values were assigned to ictal EEG recordings. The classification ability of the AMI measures is tested using ANFIS classifier. Test results confirm that AMI method has potential in classifying the epileptic EEG signals.", "num_citations": "3\n", "authors": ["144"]}
{"title": "Case Study Retrospective: Kent Beck's XP Versions 1 and 2\n", "abstract": " \u25cf Communication: Often when a problem arises, someone knows the solution but knowledge doesn\u2019t get around to person who needs it.[sustained]\u25cf Simplicity: Making a bet that it is better to do a simple thing today and pay a little more to change it if it needs it, than to do a more complicated thing today that may never be used.[sustained]", "num_citations": "3\n", "authors": ["144"]}
{"title": "Work in progress-unexpected student outcome from collaborative agile software development practices and paired programming in a software engineering course\n", "abstract": " There has been low representation of women in computer science. This paper describes the initial findings of a three-year research project about women in the field of information technology. The goal of this research is to examine the effect of pair programming and agile software development on students. During the first semester of this project, pair programming was used in a junior/senior software engineering class at North Carolina State University. In this paper, we share the grounded theory analysis of three interviews and thirteen project retrospective essays of the female students. Theoretical models were developed to describe: (a) the factors of students' enjoyment in a software design course that employs agile software methods, (b) context that influenced students' study habits, and (c) the effectiveness of pair programming and agile methods. Initial findings indicate that pair programming is an effective\u00a0\u2026", "num_citations": "3\n", "authors": ["144"]}
{"title": "Pair programming and the factors affecting Brooks' law\n", "abstract": " Software project managers often add new team members to late software projects with mixed results. Through his law,\u201cadding manpower to a late software project makes it later,\u201d Brooks asserts that the assimilation, training and intercommunication costs of adding new team members outweigh the associated team productivity gain in the short term. Our studies show that pair programming reduces these three costs and enables new team members to contribute to a team effort sooner when compared to teams where new team members work alone. Adding manpower to a late project will yield productivity gains to the team more quickly if the team employs the pair programming technique.", "num_citations": "3\n", "authors": ["144"]}
{"title": "corba: crowdsourcing to obtain requirements from regulations and breaches\n", "abstract": " Context Modern software systems are deployed in sociotechnical settings, combining social entities (humans and organizations) with technical entities (software and devices). In such settings, on top of technical controls that implement security features of software, regulations specify how users should behave in security-critical situations. No matter how carefully the software is designed and how well regulations are enforced, such systems are subject to breaches due to social (user misuse) and technical (vulnerabilities in software) factors. Breach reports, often legally mandated, describe what went wrong during a breach and how the breach was remedied. However, breach reports are not formally investigated in current practice, leading to valuable lessons being lost regarding past failures. Objective Our research aim is to aid security analysts and software developers in obtaining a set of legal, security, and privacy\u00a0\u2026", "num_citations": "2\n", "authors": ["144"]}
{"title": "Empirical study of software quality evaluation in agile methodology using traditional metrics\n", "abstract": " The use of highly iterative software development methodologies, such as Agile and Lean, have been growing. However, these methodologies do not explicitly provide practices for managing and measuring software quality. This deficiency may prevent software development organizations in critical domains from transforming from traditional development to highly iterative development. These organizations may need to manage quality during development and may desire to compare with the quality of a large number of products produced in the past. In this paper, we focus on the reliability aspects of software quality and discuss the applicability of conventional reliability metrics to iterative development. First, we defined the general process structure in iterative development. Then, we present an associated quality evaluation scheme closely. Our experimental results indicate that traditional quality metrics were applicable to iterative development and that the iterative development achieved maturity sufficient for the commercial release.", "num_citations": "2\n", "authors": ["144"]}
{"title": "Discovering security requirements from natural language project artifacts\n", "abstract": " Project documentation often contains security-relevant statements that are indicative of the security requirements of a system. However these statements may not be explicitly specified or straightforward to locate. At best, requirements analysts manually extract applicable security requirements from project documents. However, security requirements that are not explicitly stated may not be considered during implementation.", "num_citations": "2\n", "authors": ["144"]}
{"title": "How can research about software developers generalize?\n", "abstract": " Research that studies software developers can have a larger impact when its results generalize to other contexts. However, it is often unclear if, how, and why such results generalize. In this position paper, we aim to motivate software engineering researchers to generalize their findings. To meet this aim, we firstly enumerate the dimensions in which software engineering generalizations are made, and secondly, propose several practical ways researchers can make their work more generalizable. In meeting our aim, we hope to help the software engineering research community maximize their impact outside of software engineering.", "num_citations": "2\n", "authors": ["144"]}
{"title": "On Increasing System Test Effectiveness through a Test Case Prioritization Model Using Static Metrics and System Failure Data\n", "abstract": " System testing is the last phase before the product is delivered for customer use and thus represents the last opportunity for verifying that the system functions correctly and as desired by customers. System test is time consuming in that it involves configuring and testing multiple complete, integrated systems (including hardware, operating system, and cooperating and coexisting applications) that are representative of a subset of customer environments. As a result, prioritizing the execution order of system test cases to maximize system test effectiveness would be beneficial. We are developing a statistical test case prioritization model that uses static metrics and system failure data with the goal of improving system test effectiveness.", "num_citations": "2\n", "authors": ["144"]}
{"title": "Correlating Automated Static Analysis Alert Density to Reported Vulnerabilities in Sendmail\n", "abstract": " Extensive reliability-based research including [2, 8, 10] has shown that software metrics can be used to identify fault-and failure-prone components early in the software process. A fault is an incorrect step, process, or data definition in a computer program [5] whereas a failure is the inability of a software system or component to perform its required function [5]. Our research objective is to identify vulnerability-and attack-prone components as security parallels of reliability-based fault-and failure-prone components. A vulnerability is \u201can instance of a [fault] in the specification, development, or configuration of software such that its execution can violate the [implicit or explicit] security policy [6]. A vulnerability, like a fault, may lie latent until it is revealed by a tester or by an attacker with malicious intent. An attack occurs when an attacker has a motive or reason to attack and takes advantage of a vulnerability to threaten an asset [4]. From these definitions we propose the following:\u2756 A component is vulnerability-prone if it has a high probability that internal verification and validation (V&V) efforts will find security vulnerabilities prior to release [3].\u2756 A component is attack-prone if it has a high probability that security vulnerabilities will be exploited after release [3]. Recent research [9, 12] has shown that alerts generated by automated static analyses (ASA) of source code are good indicators of fault-prone components. We define an alert as an issue raised by the ASA tool that may or may not be a true problem in the software. Alert density is the number of alerts per line of code. We are currently investigating if alert density is a good metric for vulnerability-and\u00a0\u2026", "num_citations": "2\n", "authors": ["144"]}
{"title": "Agile Software Development Methods: When and Why Do They Work?\n", "abstract": " Agile software development challenges traditional software development methods. Rapidly changing environments, evolving requirements, and tight schedule constraints require software developers to take a fast cycle approach to the process of software development. Agile software development occurs in a dynamic and learning environment rather than in a mature and standardized software market (Cockburn 2001). Agile methods support shorter project lifecycles in order to respond to complex, fast-moving, and competitive marketplaces. The features of the system emerge throughout the development process, while heavily relying on feedback from the customer. The rise of software development on Internet time has created tremendous interest among practitioners in agile development. Organizational agillty, the ability to react quickly and flexibly to environmental or market changes, IS an intended outcome ofthe", "num_citations": "2\n", "authors": ["144"]}
{"title": "Teaching an active-participation university course in software reliability and testing\n", "abstract": " A large percentage of software development costs are spent on software reliability and testing. However, many practicing software engineers and graduate students in computer science have never taken a course in software reliability or software testing. A graduate-level software engineering course at North Carolina State University provides instruction in these topics to better prepare current and future software engineers for the software reliability and testing challenges. The course takes place in a laboratory setting such that students can learn testing and reliability theory and apply this theory immediately", "num_citations": "2\n", "authors": ["144"]}
{"title": "On Identifying Deficiencies in a Knowledge Management System.\n", "abstract": " The Abou-Zeid Knowledge Management Reference Model provides a structure for the elements necessary for a holistic knowledge management support system. The model underscores the importance for an effective system to consist of three layers: business aspects and associated knowledge nuggets; processes which enable the manipulation and update of these nuggets; and technology that makes this manipulation and access to the nuggets effective and straightforward. This paper supports the Abou-Zeid model through its application at ABB, Inc. in the implementation of the ABB Software Process Initiative (ASPI) Knowledge Base. The ASPI group has strengthened the process and the technology aspects of its previous experience database. The reengineered system demonstrates strength at all three Abou-Zeid layers, leveraging the utility of the captured knowledge nuggets. As structured, initial indications support the reengineered system as significantly more valuable to ABB employees than had been the earlier system.", "num_citations": "2\n", "authors": ["144"]}
{"title": "Pair Programming: Experience the Difference\n", "abstract": " Pair programming is emerging as an important technique for developing higher quality code, faster. With pair programming, two software developers work on one computer, collaborating on the same design, algorithm, code, or test. This tutorial examines pair programming research results and anecdotal experiences of programmers who have transitioned to pair programming. It will discuss what works and what doesn\u2019t and will also explain techniques for fostering support in making a transition to pair programming \u2014 support from management and support from peers. Hands-on activities will be used to demonstrate pair programming benefits.", "num_citations": "2\n", "authors": ["144"]}
{"title": "\u201cGood Enough\u201d Reliability for Extreme Programming\n", "abstract": " Extreme Programming (XP), a software development process designed for small to mid-size projects, has strong customer involvement, a simplified requirements gathering and prioritization practice, and an emphasis on testing. These characteristics enable an extension of XP to encompass a measure of reliability. We examine the enhancement of XP practices to include explicit estimation of the probability that the software system performs according to its requirements based on a specified usage profile.", "num_citations": "2\n", "authors": ["144"]}
{"title": "Jungles\n", "abstract": " Describes the climate and plant and animal life of the jungles of the world and discusses the impact and effect of humanity in studying, invading, or destroying these jungles.", "num_citations": "2\n", "authors": ["144"]}
{"title": "Structuring a Comprehensive Software Security Course Around the OWASP Application Security Verification Standard\n", "abstract": " Lack of security expertise among software practitioners is a problem with many implications. First, there is a deficit of security professionals to meet current needs. Additionally, even practitioners who do not plan to work in security may benefit from increased understanding of security. The goal of this paper is to aid software engineering educators in designing a comprehensive software security course by sharing an experience running a software security course for the eleventh time. Through all the eleven years of running the software security course, the course objectives have been comprehensive \u2013 ranging from security testing, to secure design and coding, to security requirements to security risk management. For the first time in this eleventh year, a theme of the course assignments was to map vulnerability discovery to the security controls of the Open Web Application Security Project (OWASP) Application\u00a0\u2026", "num_citations": "1\n", "authors": ["144"]}
{"title": "Omni: Automated Ensemble with Unexpected Models against Adversarial Evasion Attack\n", "abstract": " BACKGROUND: Machine learning-based security detection models have become prevalent in modern malware and intrusion detection systems. However, previous studies show that such models are susceptible to adversarial evasion attacks. In this type of attack, inputs (i.e., adversarial examples) are specially crafted by intelligent malicious adversaries, with the aim of being misclassified by existing state-of-the-art models (e.g., deep neural networks). Once the attackers can fool a classifier to think that a malicious input is actually benign, they can render a machine learning-based malware or intrusion detection system ineffective. GOAL: To help security practitioners and researchers build a more robust model against adversarial evasion attack through the use of ensemble learning. METHOD: We propose an approach called OMNI, the main idea of which is to explore methods that create an ensemble of \"unexpected models\"; i.e., models whose control hyperparameters have a large distance to the hyperparameters of an adversary's target model, with which we then make an optimized weighted ensemble prediction. RESULTS: In studies with five adversarial evasion attacks (FGSM, BIM, JSMA, DeepFool and Carlini-Wagner) on five security datasets (NSL-KDD, CIC-IDS-2017, CSE-CIC-IDS2018, CICAndMal2017 and the Contagio PDF dataset), we show that the improvement rate of OMNI's prediction accuracy over attack accuracy is about 53% (median value) across all datasets, with about 18% (median value) loss rate when comparing pre-attack accuracy and OMNI's prediction accuracy. CONCLUSIONWhen using ensemble learning as a\u00a0\u2026", "num_citations": "1\n", "authors": ["144"]}
{"title": "Metadata report for the Cardiff superficial deposits 3D geological model\n", "abstract": " The objective of this work was to assemble a geological framework model and calculated surfaces for the superficial deposits for the city of Cardiff. The model has a number of purposes:  - To investigate the superficial deposits within the city of Cardiff and surrounding area. - To provide a communication tool to present the geological understanding of the superficial deposits of the area. - To provide information on the subsurface distribution and thickness of the main superficial deposits (principally Till, Glaciofluvial Deposits, Alluvium and Tidal Flat Deposits) of Cardiff to inform a project which is presently investigating the potential for shallow geothermal energy in the city of Cardiff (BGS, 2018).  A series of 340 cross-sections have been constructed to form a fence diagram using Geological Surveying and Investigation in 3D (GSI3D) software. The sections cover much of the most densely populated areas of the city and include an area of boreholes, presently being investigated to evaluate the potential for ground source heat in part of the city. Geologists\u2019 knowledge has been applied to incorporate subsurface data (1330 boreholes from a total of 3090 boreholes within the project area) and surface geological mapping to provide an interpretation of the geological succession, utilising BGS 1:50 000 and 1:10 000 scale geological maps.", "num_citations": "1\n", "authors": ["144"]}
{"title": "Bp: Profiling vulnerabilities on the attack surface\n", "abstract": " Security practitioners use the attack surface of software systems to prioritize areas of systems to test and analyze. To date, approaches for predicting which code artifacts are vulnerable have utilized a binary classification of code as vulnerable or not vulnerable. To better understand the strengths and weaknesses of vulnerability prediction approaches, vulnerability datasets with classification and severity data are needed. The goal of this paper is to help researchers and practitioners make security effort prioritization decisions by evaluating which classifications and severities of vulnerabilities are on an attack surface approximated using crash dump stack traces. In this work, we use crash dump stack traces to approximate the attack surface of Mozilla Firefox. We then generate a dataset of 271 vulnerable files in Firefox, classified using the Common Weakness Enumeration (CWE) system. We use these files as an\u00a0\u2026", "num_citations": "1\n", "authors": ["144"]}
{"title": "Highlights of the ACM student research competition\n", "abstract": " Since 2003, ACM in conjunction with Microsoft have sponsored research competitions for undergraduate and graduate students in computing. The competitions provide a vehicle for these students to present their original research before a panel of judges and attendees at well-known ACM-sponsored and co-sponsored conferences. The students have the opportunity to experience a research conference, get feedback on their research, meet academic and industrial researchers and other students, and appreciate the practical applications of their research. Student competitors also have the opportunity to sharpen communication, visual, organizational, and presentation skills in preparation for the SRC. Participation by undergraduates may be literally life-changing if they alter their career path to pursue graduate studies and research careers after experiencing a conference and competition.", "num_citations": "1\n", "authors": ["144"]}
{"title": "Non-operational testing of software for security issues\n", "abstract": " We are studying extension of the classical Software Reliability Engineering (SRE) methodology into the security space. We combine \u201cclassical\u201d reliability modeling, when applied to reported vulnerabilities found under \u201cnormal\u201d operational profile conditions, with safety oriented fault management processes. We illustrate with open source Fedora software.", "num_citations": "1\n", "authors": ["144"]}
{"title": "An Investigation of the Effect of Competition on the Way Students Engage in Game-Based Deliberate Practice\n", "abstract": " This paper reports the results of an experiment that used qualitative and quantitative methods to investigate the effect of competition on students' use of game-based deliberate practice. We hypothesized that the results of the experiment would show that competition has a positive effect on performance outcomes, but it also increases students' tendency to game the system. The actual results of the experiment showed only very modest support for these hypotheses but have other implications for improving the design of educational games.", "num_citations": "1\n", "authors": ["144"]}
{"title": "Towards Automated Support for Small-Group Instruction: Using Data from an ITS to Automatically Group Students.\n", "abstract": " Abstract Title Page Title: Towards Automated Support for Small-Group Instruction: Using Data from an ITS to Automatically Group", "num_citations": "1\n", "authors": ["144"]}
{"title": "On the Use of Software Metrics as a Predictor of Software Security Problems\n", "abstract": " Relying on one validation and verification VV alone cannot detect all of the security problems of a software system. Each class of VV effort detects different classs of faults in software. Even composing a series of VV efforts, one can never be completely sure that all faults have been detected. Additionally, security-related VV efforts must continuously be updated to handle the newest forms of exploits of underlying vulnerabilities in software. The alerts produced by automated static analysis ASA tools and other static metrics have been shown to be an effective estimator of the actual reliability in a software system. Predictions of defect density and high-risk components can be identified using static analyzers early in the development phase. Our research hypothesis is the actual number of security vulnerabilities in a software system can be predicted based upon the number of security-related alerts reported by one or more static analyzers and by other static metrics. We built and evaluated statistical prediction model are used to predict the actual overall security of a system.Descriptors:", "num_citations": "1\n", "authors": ["144"]}
{"title": "Building regulatory-driven automated test suites\n", "abstract": " In regulated domains such as finance and health care, failure to comply with regulation can lead to financial, civil and criminal penalties. While systems vary from organization to organization, regulations apply across organizations. We propose the use of Behavior-Driven-Development (BDD) technology to develop scenarios of regulated behavior that can be automated and run to assess system behavior in comparison with the regulation. The objective of this research is to demonstrate a test suite with which multiple organizations can compare their systems to regulation in a repeatable and traceable way. To evaluate our approach, we built a set of seven scenarios based on US government regulations for Electronic Health Record (EHR) system security. We then wrote system-specific test driver code to execute the scenarios on three EHR systems. The scenarios covered all security behavior defined by the selected regulation. The system-specific test driver code covered all security behavior defined in the scenarios, and identified where the tested system lacked such behavior.", "num_citations": "1\n", "authors": ["144"]}
{"title": "Extracting database role based access control from unconstrained natural language text\n", "abstract": " Although software can and does implement access control at the application layer, failure to enforce data access at the persistence layer often allows uncontrolled data access when individuals bypass application controls. The goal of our research is to help developers improve security by ensuring the access controls defined within unaltered natural language texts are appropriately implemented within a system\u2019s persistence layer. Access control implemented in both the application and persistence layers strongly supports a defense in depth strategy. We propose a tool-based process to 1) parse existing, unaltered natural language documents such as requirements and policy statements; 2) classify a statement as being access control related or not; and, as appropriate, 3) extract subjects, actions, and data objects; and 4) automatically generate the necessary SQL commands to enforce role-based access control within a relational database. To evaluate our process, we analyzed a public requirements specification. Our classifier correctly predicted 78% of the access control statements while recalling 92% of the applicable statements. The process correctly identified and mapped 92% of the actual database tables to the extracted resources. We successfully executed the application with application and persistence layer role based access control in place.", "num_citations": "1\n", "authors": ["144"]}
{"title": "Secure Logging and Auditing in Electronic Health Records Systems: What Can We Learn from the Payment Card Industry Position Paper\n", "abstract": " Both health information technology (HIT) and the payment card industry (PCI) involve the exchange and management of sensitive, protected information. Compared to the PCI, HIT could consider protected health information (PHI) more sensitive than PCI cardholder data. If cardholder data is breached in the PCI, payment card companies may then remove fraudulent charges from the customer\u2019s account and/or issue the customer a new payment card. However, once a person\u2019s PHI has been breached, the PHI has been breached forever. Healthcare organizations cannot issue new health histories or new identities to affected individuals. Secure logging and auditing may deter users from performing unauthorized transactions with PHI since an irrefutable trace of the user\u2019s activity is recorded. Logging and auditing also provides an accounting of PHI disclosures for assisting data breach investigations.", "num_citations": "1\n", "authors": ["144"]}
{"title": "Predictive Models for Identifying Software Components Prone to Failure During Security Attacks\n", "abstract": " Sometimes software security engineers are given a product that they not familiar with and are asked to do a security analysis of it in a relatively short time. A knowledge of where vulnerabilities are most likely to reside can help prioritize their efforts. In general, software metrics can be used to predict fault-and failure-prone components for prioritizing inspection, testing, and redesign efforts. We believe that the security community can leverage this knowledge to design tools and metrics that can identify vulnerability-and attackprone components early in the software life cycle. We analyzed a large commercial telecommunications software-based system and found that the presence of security faults correlates strongly with the presence of a more general category of reliability faults. This, of course, is not surprising if one accepts the notion that security faults are in many instances a subset of a reliability fault set. We discuss a model that can be useful for identifying attack-prone components and for prioritizing security efforts early in the software life cycle.", "num_citations": "1\n", "authors": ["144"]}
{"title": "Managing intended group membership using domains\n", "abstract": " System and method for managing membership of a group of jobs in a computing environment is provided. In one embodiment, a domain group, a set of interfaces to manage the domain group, and cluster-assigned member names to identify the members in a group is provided. The interfaces allow a group to be created and allow members to be added, removed and joined. A copy of the domain group is associated with each member job and indicates each job that is a member of a particular group. Management of a group is made by configuring each of the jobs of the group to assess its respective copy of the domain group in order to service requests, such as a request to join the group.", "num_citations": "1\n", "authors": ["144"]}
{"title": "Evaluating a suite of developer activity metrics as measures of security vulnerabilities\n", "abstract": " Deploying vulnerable software can be costly both in terms of patches and security breaches. Since software development primarily involves people, researchers are increasingly analyzing version control data to observe developer collaboration and contribution. We conducted a case study of the Linux kernel to evaluate a suite of developer activity metrics for the purpose of predicting security vulnerabilities. Our suite includes centrality and cluster metrics from network analysis of version control data. Our results support the hypothesis that source code files which have been developed by multiple clusters of developers are likely to be vulnerable. Furthermore, source code files are likely to be vulnerable when changed by many developers who themselves have made many changes to other files. Our results indicate that developer metrics predict vulnerabilities, but may be more likely to perform better in the presence of other code or process metrics.", "num_citations": "1\n", "authors": ["144"]}
{"title": "Workshop on secure software engineering education & training (WSSEET 2006)\n", "abstract": " The focus of this one-day workshop on April 18th, 2006 was the experience, current situation, and the future of education and training in software engineering of (more) secure software. The workshop was intended for those in education, government, and industry at all levels doing education or training in secure software engineering and for those who would like to? as well as others interested in the subject or the secure software engineering workforce.", "num_citations": "1\n", "authors": ["144"]}
{"title": "The \u201cGood Enough\u201d\u2019Reliability Tool (GERT)-Version 2\n", "abstract": " Some developers continuously write automated tests to verify their code, but may not have means to determine if their evolving test suite is adequate. We present a tool to complement the feedback loops created by automated testing. The tool utilizes static source code metrics throughout the development phase, to provide a post-release field quality estimate based on a linear combination of these values. Implemented as an open source plugin to the Eclipse IDE, the tool also provides colorcoded feedback, which highlights inadequate testing efforts.", "num_citations": "1\n", "authors": ["144"]}
{"title": "How should software reliability engineering be taught?\n", "abstract": " PurposeThe purpose of the panel is to provide an open exchange of how software reliability engineering is taught in universities and professional courses and how practitioners think it should be taught. Through the panel, educators can learn more about what other educators teach, how they teach it, and what resources they have to share. Practitioners (both panelists and audience) can provide insight into what they feel should be taught based upon practical application of software reliability concepts for product development.", "num_citations": "1\n", "authors": ["144"]}
{"title": "On the need for a process for making reliable quality comparisons with industrial data\n", "abstract": " Many factors influence quality data obtained from industrial case studies making comparisons difficult. In this paper, two longitudinal industrial case study experiences are shared which illustrate the complications that can arise. The first is a case study of an IBM team that transitioned to the use of test-driven development. The primary quality measure was functional verification test defects normalized by lines of code. The second case study was performed with an Extreme Programming team at Sabre Airline Solutions. Both test defects and field defects were compared. In both case studies, differences existed which made the comparisons indicative but not absolute.", "num_citations": "1\n", "authors": ["144"]}
{"title": "Voices of women in a software engineering course\n", "abstract": " Those science, mathematics, and engineering faculty who are serious about making the education they offer as available to their daughters as to their sons are, we posit, facing the prospect of dismantling a large part of its traditional pedagogical structure, along with the assumptions and practice which support it.[Seymour and Hewett 1997]. Prior research indicates that female students can be concerned about the insularity of working alone for long periods of time, as they perceive to be the case with computer science and information technology careers. We studied an advanced undergraduate software engineering course at North Carolina State University to characterize the potential of collaborative learning environments created via pair-programming and agile software development to ameliorate this concern. A collective case study of three representative women in the course revealed that they held the following\u00a0\u2026", "num_citations": "1\n", "authors": ["144"]}
{"title": "Software Engineering for Internet Applications.\n", "abstract": " Despite the dot-com debacle, the acceleration of change and intense competition brought on by the Internet revolution thrive. The stronger companies have survived, but these same firms need to constantly innovate or risk failure or lack of profitability. As a result, software applications for the Internethave compressed development schedules. Additionally, these applications must continuously evolve to meet new demands to outpace (or meet) competitive offerings. It is not unusual for some Internet applications to be updated on a daily basis; by moving a development file onto the server, the software could essentially be released daily to possibly millions of web users. Controlling change is a challenge. Internetapplication development is complex inthat it often involves consideration and implementation of attractively-presented content, procedural processing, and business rules. This unification proves significantly\u00a0\u2026", "num_citations": "1\n", "authors": ["144"]}
{"title": "Distributed pair programming\n", "abstract": " Agile methodologies stress the need for close physical proximity of team members. However, circumstances may prevent a team from working in close physical proximity. For example, a company or a project may have development teams physically distributed over multiple locations. As a result, increasingly many companies are looking at adapting agile methodologies for use in a distributed environment.", "num_citations": "1\n", "authors": ["144"]}
{"title": "An Alternative: The Collaborative Software Process (CSP)\n", "abstract": " Anecdotal and statistical evidence [1-3] indicates that pair programming two programmers working side-by-side at one computer, collaborating on the same design, algorithm, code or test is very effective. One of the programmers, the driver, has control of the keyboard/mouse and actively implements the program. The other programmer, the observer, continuously observes the work of the driver to identify tactical (syntactic, spelling, etc.) defects, and also thinks strategically about the direction of the work. On demand, the two programmers can brainstorm any challenging problem. Because the two programmers periodically switch roles, they work together as equals to develop software. This pair-programming model was recently popularized by the eXtreme Programming [4] discipline, though it has been used sparsely in industry for quite some time [5, 6].In 1999, Williams developed the Collaborative Software Process SM (CSP SM) as her dissertation research [7]. CSP, a disciplined process for pair programmers, is based on Watts Humphrey\u2019s Personal Software Process (PSP)[8]. CSP assumes basic PSP knowledge, such as that gained by taking a course using the Introduction to the Personal Software Process [9] book. However, the activities of the CSP are specifically geared towards leveraging the power of two software engineers working together on one computer. Like PSP, CSP can also be used as a foundation for moving into the Team Software Process (TSP)[10], whereby the members of the team work in pairs in for development engineer work.", "num_citations": "1\n", "authors": ["144"]}