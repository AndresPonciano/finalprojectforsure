{"title": "Pattern matching for clone and concept detection\n", "abstract": " A legacy system is an operational, large-scale software system that is maintained beyond its first generation of programmers. It typically represents a massive economic investment and is critical to the mission of the organization it serves. As such systems age, they become increasingly complex and brittle, and hence harder to maintain. They also become even more critical to the survival of their organization because the business rules encoded within the system are seldom documented elsewhere.               Our research is concerned with developing a suite of tools to aid the maintainers of legacy systems in recovering the knowledge embodied within the system. The activities, known collectively as \u201cprogram understanding\u201d, are essential preludes for several key processes, including maintenance and design recovery for reengineering.               In this paper we present three pattern-matching techniques\u00a0\u2026", "num_citations": "384\n", "authors": ["117"]}
{"title": "Advanced clone-analysis to support object-oriented system refactoring\n", "abstract": " Manual source code copy and modification is often used by programmers as an easy means for functionality reuse. Nevertheless, such practice produces duplicated pieces of code or clones whose consistent maintenance might be difficult to achieve. It also creates implicit links between classes sharing a functionality. Clones are therefore good candidates for system redesign. This paper presents a novel approach for computer-aided clone-based object-oriented system refactoring. The approach is based on an advanced clone analysis which focuses on the extraction of clone differences and their interpretation in terms of programming language entities. It also focuses on the study of contextual dependencies of cloned methods. The clone analysis has been applied to JDK 1.1.5, a large scale system of 150 KLOC.", "num_citations": "259\n", "authors": ["117"]}
{"title": "Measuring clone based reengineering opportunities\n", "abstract": " Code duplication, plausibly caused by copying source code and slightly modifying it, is often observed in large systems. Clone detection and documentation have been investigated by several researchers in the past years. Recently, research focus has shifted towards the investigation of software and process restructuring actions based on clone detection. This paper presents an original definition of a clone classification scheme useful to assess and measure different system reengineering opportunities. The proposed classification considers each group of cloned methods in terms of the meaning of the differences existing between them. The algorithm used for automatic classification of clones is presented together with results obtained by classifying cloned methods and measuring reengineering opportunities in six freely available systems whose total size is about 500 KLOC of Java code.", "num_citations": "215\n", "authors": ["117"]}
{"title": "Evaluation experiments on the detection of programming patterns using software metrics\n", "abstract": " Cloning of code fragments in large systems is a common practice that may result in redundant code, higher maintenance costs, and less modular systems. The paper examines and evaluates the use of five data and control flow related metrics for identifying similar code fragments. The metrics are used as signatures for a code fragment. Matching on such signatures results in fast matching that can be used to locate instances of code cloning even in the presence of modifications such as changes in variable names, and insertion of statements. The paper takes an information retrieval approach and reports on experiments conducted for retrieving code fragments in three different software systems.", "num_citations": "186\n", "authors": ["117"]}
{"title": "Towards portable source code representations using XML\n", "abstract": " One of the most important issues in source code analysis and software re-engineering is the representation of software code text at an abstraction level and form suitable for algorithmic processing. However, source code representation schemes must be compact, accessible by well defined application programming interfaces (APIs) and above all portable to different operating platforms and various CASE tools. This paper proposes a program representation technique that is based on language domain modes and the XML markup language. In this context, source code is represented as XML DOM trees that offer a higher level of openness and portability than custom-made tool specific abstract syntax trees. The DOM trees can be exchanged between tools in textual or binary form. Similarly the domain model allows for language entities to be associated with analysis services offered by various CASE tools, leading to\u00a0\u2026", "num_citations": "142\n", "authors": ["117"]}
{"title": "Partial redesign of Java software systems based on clone analysis\n", "abstract": " Code duplication, plausibly caused by copying source code and slightly modifying it, is often observed in large systems. Clone detection and documentation have been investigated by several researchers in past years. Recently, research focus has shifted towards the investigation of software and process restructuring actions based on clone detection. The paper presents a new redesign approach developed for Java software systems. The approach factorizes the common parts of cloned methods and parameterizes their differences using the strategy design pattern. The new entities created by such transformations are also decoupled from the original contexts of their use, thus facilitating reuse and increasing maintainability. The applicability and automation of the technique presented in the paper have been verified by partially redesigning JDK 1.1.5.", "num_citations": "133\n", "authors": ["117"]}
{"title": "A maintainability model for industrial software systems using design level metrics\n", "abstract": " Software maintenance is a time consuming and expensive phase of a software product's life-cycle. The paper investigates the use of software design metrics to statistically estimate the maintainability of large software systems, and to identify error prone modules. A methodology for assessing, evaluating and, selecting software metrics for predicting software maintainability is presented. In addition, a linear prediction model based on a minimal set of design level software metrics is proposed. The model is evaluated by applying it to industrial software systems.", "num_citations": "121\n", "authors": ["117"]}
{"title": "Improving design quality using meta\u2010pattern transformations: a metric\u2010based approach\n", "abstract": " Improving the design quality of large object\u2010oriented systems during maintenance and evolution is widely regarded as a high\u2010priority objective. Furthermore, for such systems that are subject to frequent modifications, detection and correction of design defects may easily become a very complex task that is even not tractable for manual handling. Therefore, the use of automatic or semi\u2010automatic detection and correction techniques and tools can assist reengineering activities. This paper proposes a framework whereby object\u2010oriented metrics can be used as indicators for automatically detecting situations for particular transformations to be applied in order to improve specific design quality characteristics. The process is based both on modeling the dependencies between design qualities and source code features, and on analyzing the impact that various transformations have on software metrics that quantify the\u00a0\u2026", "num_citations": "94\n", "authors": ["117"]}
{"title": "Tracing evolution changes of software artifacts through model synchronization\n", "abstract": " Software evolution encompasses all activities related to engineering software, from its inception to retirement. Propagating change across software models that are altered due to maintenance activities is a first step towards maintaining consistency between architectural design, and implementation models. Model synchronization techniques initially presented within the context of model driven architecture provide an instrument for achieving change trace-ability and consistency. We present a framework whereby software artifacts at different levels of abstraction such as architecture diagrams, object models, and abstract syntax trees are represented by graph-based MOF compliant models that can be synchronized using model transformations. In such a framework model dependencies are implicitly encoded using transformation rules and an equivalence relation is used to evaluate when two models become\u00a0\u2026", "num_citations": "87\n", "authors": ["117"]}
{"title": "Architectural design recovery using data mining techniques\n", "abstract": " The paper presents a technique for recovering the high level design of legacy software systems according to user defined architectural plans. Architectural plans are represented using a description language and specify system components and their interfaces. Such descriptions are viewed as queries that are applied on a large database which stores information extracted from the source code of the subject legacy system. Data mining techniques and a modified branch and bound search algorithm are used to control the matching process, by which the query is satisfied and query variables are instantiated. The matching process allows the alternative results to be ranked according to data mining associations and clustering techniques and, finally, be presented to the user.", "num_citations": "77\n", "authors": ["117"]}
{"title": "Transforming legacy Web applications to the MVC architecture\n", "abstract": " With the rapid changes that occur in the area of Web technologies, the porting and adaptation of existing Web applications into new platforms that take advantage of modern technologies has become an issue of increasing importance. This paper presents a reengineering framework whose target system is an architecture based on the model-view-controller (MVC) design pattern and enabled for the Java/spl trade/ 2 Platform, Enterprise Edition (J2EE). The proposed framework is mainly concerned with the decomposition of a legacy Web application by identifying software components to be transformed into Java objects such as JavaBeans, JavaServer Pages (JSP), and Java Servlet.", "num_citations": "63\n", "authors": ["117"]}
{"title": "A graph pattern matching approach to software architecture recovery\n", "abstract": " This paper presents a technique for recovering the high level design of legacy software systems based on pattern matching and user defined architectural patterns. Architectural patterns are represented using a description language that is mapped to an attributed relational graph and allows to specify the legacy system components and their data and control flow interactions. Such pattern descriptions are viewed as queries that are applied against an entity-relation graph that represents information extracted from the source code of the software system. A multi-phase branch and bound search algorithm with a forward checking mechanism controls the matching process of the two graphs by which, the query is satisfied and its variables are instantiated. An association based scoring mechanism is used to rank the alternative results generated by the matching process. Experimental results of applying the technique on\u00a0\u2026", "num_citations": "59\n", "authors": ["117"]}
{"title": "Pattern matching for design concept localization\n", "abstract": " The effective synergy of a number of different techniques is the key to the successful development of an efficient reverse engineering environment. Compiler technology, pattern matching techniques, visualization tools, and software repositories play an important role for the identification of procedural, data, and abstract-data-type related concepts in the source code. This paper describes a number of techniques used for the development of a distributed reverse engineering environments. Design recovery is investigated through code-to-code and abstract-descriptions-to-code pattern matching techniques used to locate code that may implement a particular plan or algorithm. The code-to-code matching uses dynamic programming techniques to locate similar code fragments and is targeted for large software systems (1MLOC). Patterns are specified either as source code or as a sequence of abstract statements written\u00a0\u2026", "num_citations": "56\n", "authors": ["117"]}
{"title": "Component clustering based on maximal association\n", "abstract": " Presents a supervised clustering framework for recovering the architecture of a software system. The technique measures the association between the system components (such as files) in terms of data and control flow dependencies among the groups of highly related entities that are scattered throughout the components. The application of data mining techniques allows us to extract the maximum association among the groups of entities. This association is used as a measure of closeness among the system files in order to collect them into subsystems using an optimization clustering technique. A two-phase supervised clustering process is applied to incrementally generate the clusters and control the quality of the system decomposition. In order to address the complexity, issues, the whole clustering space is decomposed into subspaces based on the association property. At each iteration, the subspaces are\u00a0\u2026", "num_citations": "55\n", "authors": ["117"]}
{"title": "On modeling software architecture recovery as graph matching\n", "abstract": " This paper presents a graph matching model for the software architecture recovery problem. Because of their expressiveness, the graphs have been widely used for representing both the software system and its high-level view, known as the conceptual architecture. Modeling the recovery process as graph matching is an attempt to identify a sub-optimal transformation from a pattern graph, representing the high-level view of the system, onto a subgraph of the software system graph. A successful match yields a restructured system that conforms to the given pattern graph. A failed match indicates the points where the system violates specific constraints. The pattern graph generation and the incrementality of the recovery process are the important issues to be addressed. The approach is evaluated through case studies using a prototype toolkit that implements the proposed interactive recovery environment.", "num_citations": "52\n", "authors": ["117"]}
{"title": "A framework for software architecture refactoring using model transformations and semantic annotations\n", "abstract": " Software-intensive systems evolve continuously under the pressure of new and changing requirements, generally leading to an increase in overall system complexity. In this respect, to improve quality and decrease complexity, software artifacts need to be restructured and refactored throughout their lifecycle. Since software architecture artifacts represent the highest level of implementation abstraction, and constitute the first step in mapping requirements to design, architecture refactorings can be considered as the first step in the quest of maintaining system quality during evolution. In this paper, we introduce an approach for refactoring software architecture artifacts using model transformations and quality improvement semantic annotations. First, the conceptual architecture view is represented as a UML 2.0 profile with corresponding stereotypes. Second, instantiated architecture models are annotated using\u00a0\u2026", "num_citations": "51\n", "authors": ["117"]}
{"title": "Extracting Java library subsets for deployment on embedded systems\n", "abstract": " Embedded systems provide means for enhancing the functionality delivered by small-sized electronic devices such as hand-held computers and cellular phones. Java is a programming language which incorporates a number of features that are useful for developing such embedded systems. However, the size and the complexity of the Java language and its libraries have slowed its adoption for embedded systems, due to the processing power and storage space limitations. A common approach to address storage space limitations is for the vendor to offer special versions of the libraries with reduced functionality and size to meet the constraints of embedded systems. However, such an approach will severely limit the type of applications that can be deployed. This paper presents a technique that is used for selecting, on an as needed basis, the subset of library entities that is exactly required for a given Java\u00a0\u2026", "num_citations": "50\n", "authors": ["117"]}
{"title": "The effect of call graph construction algorithms for object-oriented programs on automatic clustering\n", "abstract": " Call graphs are commonly used as input for automatic clustering algorithms, the goal of which is to extract the high level structure of the program under study. Determining the call graph for a procedural program is fairly simple. However this is not the case for programs written in object oriented languages, due to polymorphism. A number of algorithms for the static construction of an object oriented program's call graph have been developed in the compiler optimization literature in recent years. We investigate the effect of three such algorithms on the automatic clustering of the Java Expert System Shell (JESS). Object oriented programs have an inherently richer structure than those written in procedural languages, and so even medium sized programs such as JESS produce large graphs. Existing tools that we are aware of are not able to process such graphs. Consequently, we have developed our own algorithm for\u00a0\u2026", "num_citations": "50\n", "authors": ["117"]}
{"title": "A pattern matching framework for software architecture recovery and restructuring\n", "abstract": " The paper presents a framework for software architecture recovery and restructuring. The user specifies a high level abstraction view of the system using a structured pattern language. A pattern matching engine provides an optimal match between the given pattern and a decomposition of the legacy system entities by satisfying the inter/intramodule constraints defined by the pattern. The data mining technique Apriori is used by the matching engine to reveal meaningful data and control flow properties of the target system and limit the search space. A branch and bound search algorithm using a score function, models the constraints in the pattern as a Valued Constraint Satisfaction Problem (VCSP), and assists in searching for an optimal match between the given pattern and the target system.", "num_citations": "49\n", "authors": ["117"]}
{"title": "A user\u2010assisted approach to component clustering\n", "abstract": " In this paper, we present a user\u2010assisted clustering technique for software architecture recovery based on a proximity measure that we call component association. The component association measure is computed on the shared properties among groups of highly related system entities. In this approach, the software system is modeled as an attributed relational graph with the software constructs (entities) represented as nodes and data/control dependencies represented as edges. The application of data mining techniques on the system graph allows us to generate a component graph where the edges are labeled by the association strength values among the components. An interactive partitioning technique is used to partition a system into cohesive components. Graph visualization tools and cluster quality evaluation metrics are applied by the user to assess and fine tune the partition result. Copyright \u00a9 2003\u00a0\u2026", "num_citations": "44\n", "authors": ["117"]}
{"title": "Detecting code similarity using patterns\n", "abstract": " A key issue in design recovery is to localize patterns of code that may implement a particular plan or algorithm. This paper describes a set of code-to-code and abstract-description-to-code matching techniques. The code-to-code matching uses dynamic programming techniques to localize similar code fragments and is targeted for large software systems (1MLOC). Patterns are specified either as source code or as a sequence of abstract statements written in an concept language. Markov models are used to compute dissimilarity distances between an abstract description and a code fragment in terms of the probability that a given abstract statement can generate a given code fragment. The abstract-description-to-code matcher is under implementation and early experiments show it is a promising technique.", "num_citations": "44\n", "authors": ["117"]}
{"title": "Comprehension and maintenance of large-scale multi-language software applications\n", "abstract": " During the last decade, the number of software applications that have been deployed as a set of components built using different programming languages and paradigms has increased considerably. When such applications are maintained, traditional program comprehension and reengineering techniques may not be adequate. Hence, this working session aims to stimulate discussion around key issues relating to the comprehension, re engineering, and maintenance of multi-language software applications. Such issues include, but are not limited to, the formalization, management, exploration, and presentation of multi-language program dependencies, as well as the development of practical toolsets to automate and ease the comprehension and maintenance of multi-language software", "num_citations": "42\n", "authors": ["117"]}
{"title": "A framework for business model driven development\n", "abstract": " Typically, large companies in an effort to increase efficiency specify business processes using workflow languages, while software designers specify the systems that implement these processes with the use of languages like UML. This separation of domain expertise allows for software engineers from each individual area to work more efficiently using domain specific languages and tools. However, models in these two domains evolve independently and inconsistencies may occur when two models become unsynchronized due to constant revision or evolution of processes and design artifacts. In this paper, we present a set of transformations to automatically generate a specific set of UML artifacts from the business process specifications. In particular, we examine and investigate a preliminary framework for the necessary annotations that need be applied to a business process model so that the generation of UML\u00a0\u2026", "num_citations": "40\n", "authors": ["117"]}
{"title": "Evidence driven object identification in procedural code\n", "abstract": " Software evolution is an integrated part of software maintenance. It may take the form of porting a legacy system to a new hardware platform operating system, translating the system to a new language or rearchitecting the system to take advantage of new programming paradigms. This paper presents techniques for the identification and recognition of object-oriented structures in legacy systems that have been implemented using a procedural language. The paper examines methods for the selection of object classes and the recovery of the possible associations between the recovered classes.", "num_citations": "39\n", "authors": ["117"]}
{"title": "An XML-based framework for language neutral program representation and generic analysis\n", "abstract": " XML applications are becoming increasingly popular to define structured or semi-structured constrained data in XML for special application areas. In pursuit there is a growing momentum of activities related to XML representation of source code in the area of program comprehension and software re-engineering. The source code and the artifacts extracted from a program are necessarily structured information that needs to be stored and exchanged among different tools. This makes XML to be a natural choice to be used as the external representation formats for program representations. Most of the XML representations proposed so far abstract the source code at the AST level. These AST representations are tightly coupled with the language grammar of the source code and hence require development of different tools for different programming languages to perform the same type of analysis. Moreover AST\u00a0\u2026", "num_citations": "31\n", "authors": ["117"]}
{"title": "Source code modularization using lattice of concept slices\n", "abstract": " Most legacy systems have been altered due to prolonged maintenance to the point that they deviate significantly from their original and intended design and consequently, they lack modularity. Static source code analysis techniques like concept assignment, formal concept analysis and program slicing, have been successfully used by researchers for program understanding and for restoring system design properties. In our approach we combine these three techniques, aiming to gain on their individual strengths and overcoming their weaknesses. Here we present a program representation formalism that we call the lattice of concept slices and a program modularization technique that aims to separate statements in a code fragment according to the concept they implement or they may belong to. The lattice shows the relationship between the statements of a program and the domain concepts that might be\u00a0\u2026", "num_citations": "26\n", "authors": ["117"]}
{"title": "Localization of Design Concepts in Legacy Systems.\n", "abstract": " Complete automation of design recovery of large systems is a desirable but impractical goal due to complexity and size issues, so current research efforts focus on redocumentation and partial design recovery. Pattern matching lies at the center of any design recovery system. In the context of a larger project to develop an integrated reverse engineering environment, we are developing a framework for performing clone detection, code localization, and plan recognition. This paper discusses a plan localization and selection strategy based on a dynamic programming function that records the matching process and identifies parts of the plan and code fragment that are most \"similar\". Program features used for matching are currently based on data flow, control flow, and structural properties. The matching model uses a transition network and allows for the detection of insertions and deletions, and it is targeted for legacy\u00a0\u2026", "num_citations": "26\n", "authors": ["117"]}
{"title": "Partial evaluation of model transformations\n", "abstract": " Model Transformation is considered an important enabling factor for Model Driven Development. Transformations can be applied not only for the generation of new models from existing ones, but also for the consistent co-evolution of software artifacts that pertain to various phases of software lifecycle such as requirement models, design documents and source code. Furthermore, it is often common in practical scenarios to apply such transformations repeatedly and frequently; an activity that can take a significant amount of time and resources, especially when the affected models are complex and highly interdependent. In this paper, we discuss a novel approach for deriving incremental model transformations by the partial evaluation of original model transformation programs. Partial evaluation involves pre-computing parts of the transformation program based on known model dependencies and the type of the\u00a0\u2026", "num_citations": "25\n", "authors": ["117"]}
{"title": "On the role of services in enterprise application integration\n", "abstract": " Recent advances in Web and middleware technologies offer a promising solution for a number of enterprise integration problems. The convergence of the Internet and distributed-object technologies, which has been referred to as the Internet's third wave, extends this \"information-based\" Internet to a worldwide \"services-based\" Web. In this paper we present a model for Enterprise Application Integration (EAI) and we discuss the role of emerging technologies, as they relate to the specification of services, registration of services, data integration, and control integration.", "num_citations": "24\n", "authors": ["117"]}
{"title": "On the syllogistic structure of object-oriented programming\n", "abstract": " Recent works by J.F. Sowa (2000) and D. Rayside and G.T. Campbell (2000) demonstrate that there is a strong connection between object-oriented programming and the logical formalism of the syllogism, first set down by Aristotle in the Prior Analytics (1928). In this paper, we develop an understanding of polymorphic method invocations in terms of the syllogism, and apply this understanding to the design of a novel editor for object-oriented programs. This editor is able to display a polymorphic call graph, which is a substantially more difficult problem than displaying a non-polymorphic call graph. We also explore the design space of program analyses related to the syllogism, and find that this space includes Unique Name, Class Hierarchy Analysis, Class Hierarchy Slicing, Class Hierarchy Specialization, and Rapid Type Analysis.", "num_citations": "22\n", "authors": ["117"]}
{"title": "Refactoring web sites to the controller-centric architecture\n", "abstract": " A Web site is a hyperlinked network environment, which consists of hundreds of interconnected pages, usually without an engineered architecture. This is often a large, complex Web site that is difficult to understand and maintain. Here, we propose an approach that aims to restructure an existing Web site by adapting them to a controller-centric architecture. In particular, this approach is twofold. First, it defines a domain model to represent dependencies between Web pages in order to abstract current structure of the Web site. Second, it designs a system architecture as a reference model for restructuring the Web site to the new structure. These principles will be illustrated through a case study using a reengineering tool that implements the refactoring process for a JSP-based Web site.", "num_citations": "21\n", "authors": ["117"]}
{"title": "Enterprise integration\n", "abstract": " For several years, the SEI has been conducting architecture evaluations with a variety of companies and DoD organizations, 1 helping them to identify system and software architecture concerns and risks. We have noticed particular problems from an activity referred to as \u201centerprise integration.\u201d The goal of enterprise integration (EI) is to provide timely and accurate exchange of consistent information between business functions to support strategic and tactical business goals in a manner that appears to be seamless. However, problems often emerge from overly ambitious or imprecise requirements resulting from inadequate plans for integrating different systems (legacy or otherwise). In this column, we describe the challenges associated with EI and provide a road map toward a solution.", "num_citations": "20\n", "authors": ["117"]}
{"title": "Change and adaptive maintenance detection in Java software systems\n", "abstract": " Java is a relatively new programming language that is gaining popularity due to its network-centric features and platform independence (Write Once, Run Anywhere). This popularity has caused rapid evolution in the libraries that are available for Java applications. This evolution, in combination with Java's run-time linking, may cause incompatibilities between an application and the library it depends on: an application may execute with a different library version than the one it was compiled for. This paper presents techniques to automatically detect change in a library from its bytecode (binary) representation, and to apply the impact of those changes to any Java application. This paper also includes results of change detection experiments performed on the standard Java library (JDK).", "num_citations": "20\n", "authors": ["117"]}
{"title": "Extracting REST resource models from procedure-oriented service interfaces\n", "abstract": " During the past decade a number of procedure-oriented protocols and standards have emerged for making service-offering systems available on the Web. The WS-* stack of protocols is the most prevalent example. However, this procedure and message-oriented approach has not aligned with the true potential of the Web's own architectural principles, such as the uniform identification and manipulation of resources, caching, hypermedia, and layering. In this respect, Resource Oriented Architectures based on the REST architectural style, have been proposed as a possible alternative to the operation-based view of service offerings. To date, compiling a REST API for back-end procedure-oriented services is considered as a manual process that requires as input specialized models, such as, service requirements and behavioral models. In this paper, we propose a resource extraction method in which service\u00a0\u2026", "num_citations": "18\n", "authors": ["117"]}
{"title": "Program representation and behavioural matching for localizing similar code fragments\n", "abstract": " Reverse engineering focuses on the development of tools and techniques for understanding unfamiliar code. The main objective in design recovery is to understand program behavior. In order to understand th e behavioral aspects of a program, concepts of language semantics and flow analysis can be used. In this paper we consider a program representation method in whic h communication of a code fragment with the rest of th e system represents its behavior. Code fragments are viewed as objects capable of using resources and updating variables. Program similarity between simple cod e fragments can be proven in terms of the structure an d the information residing in the nodes of program description trees. Program description trees are labeled trees in which nodes represent either code fragment s or actions encoding the communication of a code fragment. Recognition and control algorithms for plan instance localization and recognition are investigated in order to assist partial design recovery when no complete recognition is possible. Finally, we discuss th e underlying\" Goal-Question-Analysis-Action\" strategy for modeling the design recovery.", "num_citations": "18\n", "authors": ["117"]}
{"title": "Towards automatic establishment of model dependencies using formal concept analysis\n", "abstract": " Software evolution is an iterative and incremental process that encompasses the modification and alteration of software models at different levels of abstraction. These modifications are usually performed independently, but the objects to which they are applied to, are in most cases mutually dependent. Inconsistencies and drift among related artifacts may be created if the effects of an alteration are not properly identified, recorded, and propagated in other dependent models. For large systems, it is possible that there is a considerable number of such model dependencies, for which manual extraction is not feasible. In this paper, we introduce an approach for automating the identification and encoding of dependence relations among software models and their elements. The proposed dependency extraction technique first uses association rules to map types between models at different levels of abstraction. Formal\u00a0\u2026", "num_citations": "17\n", "authors": ["117"]}
{"title": "Run-time requirements verification for reconfigurable systems\n", "abstract": " Context: Modern software systems often are distributed, run on virtualized platforms, implement complex tasks and operate on dynamically changing and unpredictable environments. Such systems need to be dynamically reconfigured or evolve in order to continue to meet their functional and non-functional requirements, as load and computation need to change. Such reconfiguration and/or evolution actions may cause other requirements to fail.Objective: Given models that describe with a degree of confidence the requirements that should hold in a running software system, along with their inter-dependencies, our objective is to propose a framework that can process these models and estimate the degree requirements hold as the system is dynamically altered or adapted.Method: We present an approach where requirements and their inter-dependencies are modeled using conditional goal models with weighted\u00a0\u2026", "num_citations": "15\n", "authors": ["117"]}
{"title": "Using formal concept analysis to establish model dependencies\n", "abstract": " Software models evolve at different levels of abstraction, from the requirements specification to development of the source code. The models underlying this process are related and their elements are usually mutually dependent. To preserve consistency and enable synchronization when models are altered due to evolution, the underlying model dependencies need to be established and maintained. As there is a potentially large number of such relations, this process should be automated for suitable scenarios. This paper introduces a tractable approach to automating identification and encoding of model dependencies that can be used for model synchronization. The approach first uses association rules to map types between models and different levels of abstraction. It then makes use of formal concept analysis (FCA) on attributes of extracted models to identify clusters of model elements.", "num_citations": "15\n", "authors": ["117"]}
{"title": "Pattern-based software architecture recovery\n", "abstract": " This paper presents a technique for recovering the high level design of legacy software systems based on pattern matching and user defined architectural patterns. Architectural patterns are represented using a description language that is mapped to an attributed relational graph and allows to specify the legacy system components and their data and control flow interactions. Such pattern descriptions are viewed as queries that are applied against an entity-relation graph that represents information extracted from the source code of the software system. A multi-phase\u00a5 \u00a7 \u00a6 search algorithm with a bounded path-queue mechanism controls the matching process of the two graphs by which, the query is satisfied and its variables are instantiated. An association based scoring mechanism is used to rank the alternative results generated by the matching process. Experimental results of applying the technique on the Xfig system are also presented.", "num_citations": "14\n", "authors": ["117"]}
{"title": "Case study on which relations to use for clustering-based software architecture recovery\n", "abstract": " Clustering-based software architecture recovery is an area that has received significant attention in the software engineering community over the years. Its key concept is the compilation and clustering of a system-wide graph that consists of source code entities as nodes, and source code relations as edges. However, the related research has mostly focused on investigating different clustering methods and techniques, and consequently there is limited work on addressing the question of what is a minimal set of relations that can be easily extracted from the system\u2019s source code, and yet can be accurately used for extracting its architecture. In this paper, we report on results obtained from an architecture recovery case study we have conducted, by considering all possible combinations which can be generated from thirteen commonly used source code relations. We have examined the similarity of the\u00a0\u2026", "num_citations": "12\n", "authors": ["117"]}
{"title": "Pattern matching techniques for clone detection\n", "abstract": " CiNii \u8ad6\u6587 - Pattern Matching Techniques for Clone Detection CiNii \u56fd\u7acb\u60c5\u5831\u5b66\u7814\u7a76\u6240 \u5b66\u8853\u60c5\u5831 \u30ca\u30d3\u30b2\u30fc\u30bf[\u30b5\u30a4\u30cb\u30a3] \u65e5\u672c\u306e\u8ad6\u6587\u3092\u3055\u304c\u3059 \u5927\u5b66\u56f3\u66f8\u9928\u306e\u672c\u3092\u3055\u304c\u3059 \u65e5\u672c\u306e\u535a\u58eb\u8ad6\u6587\u3092\u3055\u304c\u3059 \u65b0\u898f \u767b\u9332 \u30ed\u30b0\u30a4\u30f3 English \u691c\u7d22 \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u9589\u3058\u308b \u30bf\u30a4\u30c8\u30eb \u8457\u8005\u540d \u8457\u8005ID \u8457\u8005\u6240\u5c5e \u520a\u884c\u7269\u540d ISSN \u5dfb\u53f7\u30da\u30fc\u30b8 \u51fa\u7248\u8005 \u53c2\u8003\u6587\u732e \u51fa\u7248\u5e74 \u5e74\u304b\u3089 \u5e74\u307e\u3067 \u691c\u7d22 \u691c\u7d22 \u691c\u7d22 CiNii\u7a93\u53e3\u696d\u52d9 \u306e\u518d\u958b\u306b\u3064\u3044\u3066 Pattern Matching Techniques for Clone Detection KONTOGIANNIS K. \u88ab\u5f15\u7528 \u6587\u732e: 1\u4ef6 \u8457\u8005 KONTOGIANNIS K. \u53ce\u9332\u520a\u884c\u7269 Automated, J., Softw. Eng. Automated, J., Softw. Eng. 3, 77-108, 1996 Kluwer Academic Publishers \u88ab\u5f15\u7528\u6587\u732e: 1\u4ef6\u4e2d 1-1\u4ef6\u3092 \u8868\u793a 1 \u30b3\u30fc\u30c9 \u30af\u30ed\u30fc\u30f3\u691c\u51fa\u6280\u8853\u306e\u5c55\u958b \u795e\u8c37 \u5e74\u6d0b , \u80a5\u5f8c \u82b3\u6a39 , \u5409\u7530 \u5247\u88d5 \u30b3\u30f3\u30d4\u30e5\u30fc\u30bf\u30bd\u30d5\u30c8\u30a6\u30a7\u30a2 28(3), 29-42, 2011-07-26 \u53c2\u8003\u6587\u732e121\u4ef6 \u88ab\u5f15\u7528\u6587\u732e1\u4ef6 Tweet \u5404\u7a2e\u30b3\u30fc\u30c9 NII\u8ad6\u6587ID(NAID) 10029650951 \u8cc7\u6599\u7a2e\u5225 \u96d1\u8a8c\u8ad6\u6587 \u30c7\u30fc\u30bf\u63d0\u4f9b\u5143 CJP\u5f15\u7528 \u66f8\u304d\u51fa\u3057 RefWorks\u306b\u66f8\u304d\u51fa\u3057 EndNote\u306b\u66f8\u304d\u51fa\u3057 \u306b\u2026", "num_citations": "12\n", "authors": ["117"]}
{"title": "Challenges and opportunities related to the design, deployment and, operation of Web Services\n", "abstract": " Web services have been proposed almost a decade ago as an implementation technology for service oriented architecture. Web service technologies have since been adopted by many users as a vehicle to build such service provision-based software systems. Despite their widespread adoption, Web services still pose significant challenges as well as, opportunities both to the information technology community and, to business community. The challenges deal with engineering, and adoption issues while the opportunities deal mostly with business and operations issues. In this paper, we first discuss the state of the art in the area of Web services, and then we proceed on identifying challenges and opportunities related to designing, implementing, operating and maintaining such systems. Finally, we present emerging technologies that we believe may play a significant role in implementing and deploying the next\u00a0\u2026", "num_citations": "11\n", "authors": ["117"]}
{"title": "Pattern and policy driven log analysis for software monitoring\n", "abstract": " The component-based nature of large industrial software systems that consist of a number of diverse collaborating applications, pose significant challenges with respect to system maintenance, monitoring, auditing, and diagnosing. In this context, a monitoring and diagnostic system interprets log data to recognize patterns of significant events that conform to specific threat models. Threat models have been used by the software industry for analyzing and documenting a systempsilas risks in order to understand a systempsilas threat profile. In this paper, we propose a framework whereby patterns of significant events are represented as expressions of a specialized monitoring language that are used to annotate specific threat models. An approximate matching technique that is based on the Viterbi algorithm is then used to identify whether system generated events, fit the given patterns. The technique has been applied\u00a0\u2026", "num_citations": "11\n", "authors": ["117"]}
{"title": "Migration of Legacy Web Applications to Enterprise Java Environments\u2013Net. Data to JSP Transformation\n", "abstract": " As Web technologies advance, the porting and adaptation of existing Web applications to take advantage of the advancement has become an issue of increasing importance. Examples of such technology advancement include extensible architectural designs, more efficient caching protocols, and provision for customizable dynamic content delivery. This paper presents an experience report on the migration of legacy IBM\u00ae Net. Data\u00ae based applications to new enterprise Java\u2122 environments. In this respect, a Net. Data application is refactored into JavaBeans\u2122(Model), JavaServer Pages\u2122(View), and Java Servlet\u2122(Controller). To evaluate the effectiveness of the migration methodology, a tool has been developed to support the automatic translation of Net. Data to JavaServer Pages. Using such a tool, a case study is presented to deal with IBM WebSphere\u00ae Commerce applications.", "num_citations": "11\n", "authors": ["117"]}
{"title": "Identification of REST-like resources from legacy service descriptions\n", "abstract": " Service-oriented systems mainly follow two principles for accessing data and invoking back end applications: Remote Procedure Calls and Message-Orientation. However, a number of researchers and practitioners have criticized these paradigms as too complex and rigid. Instead, Representational State Transfer (REST) architectural style has lately gained significant attention as an alternative means for accessing services and data. RESTful HTTP systems depend on Uniform Resource Identifiers (URIs) to uniquely identify and denote data and services as \u201cresources\u201d. In this paper, we discuss a technique to analyze the descriptions of legacy data and services in order first, to model their roles and relationships and second, to use the discovered dependencies for extracting Unique Resource Identifiers and the available HTTP methods, so that these legacy service elements and data can be accessed using\u00a0\u2026", "num_citations": "10\n", "authors": ["117"]}
{"title": "End-to-end E-commerce application development based on XML tools\n", "abstract": " In this paper, we discuss an electronic business application framework and its related architecture. The framework is presented in the form of a prototype system which illustrates how XML tools can assist organizations on building and deploying e-commerce applications. The use of the system is presented in the form of a sample e-commerce application that involves shopping in a virtual store. The framework allows for customers and service providers to be linked through XML/EDI with back-end applications and to trigger e-procurement orders which are sent using either XML or EDI messages. Invoice handling and order tracking are also discussed along with extensions to the proposed architecture that allow for business process logic to be encoded at the server side, in the form of Event Condition Action scripts.", "num_citations": "10\n", "authors": ["117"]}
{"title": "A goal driven framework for software project data analytics\n", "abstract": " The life cycle activities of industrial software systems are often complex, and encompass a variety of tasks. Such tasks are supported by integrated environments (IDEs) that allow for project data to be collected and analyzed. To date, most such analytics techniques are based on quantitative models to assess project features such as effort, cost and quality. In this paper, we propose a project data analytics framework where first, analytics objectives are represented as goal models with conditional contributions; second, goal models are transformed to rules that yield a Markov Logic Network (MLN) and third, goal models are assessed by an MLN probabilistic reasoner. This approach has been applied with promising results to a sizeable collection of software project data obtained by ISBSG repository, and can yield results even with incomplete or partial data.", "num_citations": "9\n", "authors": ["117"]}
{"title": "A generic worklist algorithm for graph reachability problems in program analysis\n", "abstract": " Many program analyses involve, or can be expressed in terms of, a graph reachability problem. We present a generic worklist-style algorithm capable of expressing and solving the graph reachability components of many such analyses. We compare our framework with the language reachability framework proposed by Thomas Reps. (1998), and show that some problems are expressible in both frameworks, and some problems can be expressed in only one of the frameworks. Our two main case studies are Choi et al.'s escape analysis and Bacon-Sweeney's rapid type analysis. The reachability problems in these analyses can be directly expressed in the framework of our algorithm, but not in the language reachability framework. We discuss why this is the case, but important future work remains to formally characterize the kinds of problems that are amenable to our approach. The paper also summarizes our\u00a0\u2026", "num_citations": "9\n", "authors": ["117"]}
{"title": "Workshop report: The two-day workshop on Research Issues in the Intersection between Software Engineering and Artificial Intelligence (held in conjunction with ICSE-16)\n", "abstract": " As the need for more complex software systems increases so does the need for developing systematic and standardized methods for software design and maintenance. Artificial Intelligence can play an important role in this activity as it may provide efficient, adaptable and customizable solutions. Domain analysis, program representation, process modeling, software testing and software verification are all areas that can benefit from the use of A.I techniques, including knowledge acquisition, knowledge representation, problem solving algorithms and theorem proving. This paper discusses the use of Artificial Intelligence techniques in Software Engineering, as it was presented in the ICSE 16's workshop on Research Issues in the Intersection between Software Engineering and Artificial Intelligence.", "num_citations": "9\n", "authors": ["117"]}
{"title": "The development of a partial design recovery environment for legacy systems\n", "abstract": " Computer Assisted Program Understanding systems take input primarily in the form of source code and produce output representing system concepts in some useful form. This paper discusses the development of a program design recovery environment based on structural and behavioral recognition of programming plans. Our research investigates program and plan representation methods and the issues related to the detection of code fragments using pattern matching techniques. In particular, we consider the integration of diverse tools allowing selection from a choice of strategies. Our investigation focuses on technique s to allow partial design recovery when complete recognition is not feasible. Finally, we discuss the underlying process paradigm, called\" Goal-Question-Analysis-Action\".", "num_citations": "9\n", "authors": ["117"]}
{"title": "Task specification and reasoning in dynamically altered contexts\n", "abstract": " Software systems are prone to evolution in order to be kept operational and meet new requirements. However, for large systems such evolution activities cannot occur in a vacuum. Instead, specific action plans must be devised so that evolution goals can be achieved within an acceptable level of deviation or, risk. In this paper we present an approach that allows for the identification of plans in the form of actions that satisfy a goal model when the environment is constantly changing. The approach is based on sequences of mutations of an initial solution, using a local search algorithm. Experimental results indicate that even for medium size models, the approach outperforms in execution time the weighted Max-Sat algorithms, while it is able to achieve an almost optimal solution. The approach is demonstrated on an example scenario of re-configuring a dynamically provisioned system.", "num_citations": "8\n", "authors": ["117"]}
{"title": "Structural and behavioral code representation for program understanding\n", "abstract": " Methodologies which could assist the software maintainer are reported, with emphasis on an approach which combines structural and behavioral representation of the code. Structural representation occurs at lower levels of abstraction and uses compiler technology techniques, graph parsing, abstract syntax trees, and control and data flow. Behavioral representation can be achieved at higher levels of abstraction by using some formal representation of source code semantics, such as process algebra, lambda calculus, or denotational semantics. The complexity, concurrency, and interaction levels of the system are good indicators of the best formalism to be chosen. Artificial intelligence techniques can be used to define semantic distances between different behavioral representation plans in order to achieve a full or partial match of the source code to the underlying specifications.<>", "num_citations": "8\n", "authors": ["117"]}
{"title": "Domain independent event analysis for log data reduction\n", "abstract": " Analyzing the run time behavior of large software systems is a difficult and challenging task. Log analysis has been proposed as a possible solution. However, such an analysis poses unique challenges, mostly due to the volume and diversity of the logged data that is collected, thus making this analysis often intractable for practical purposes. In this paper, we present a log analysis technique that aims to compute a smaller, compared to the original, collection of events that relate to a given analysis objective. The technique is based on computing a similarity score between the logged events and a collection of significant events that we refer to as beacons. The major novelties of the proposed technique are that it is domain independent and that it does not require the use of a pre-existing training data set. The technique has been evaluated against the DARPA Intrusion Detection Evaluation 1999 and the KDD 1999\u00a0\u2026", "num_citations": "7\n", "authors": ["117"]}
{"title": "Towards an interpretation framework for assessing interface uniformity in REST\n", "abstract": " Interface uniformity is regarded as one of the most distinctive features of the REST architectural style among other network-based styles, because of the specific set of restrictions it imposes on the behavior paradigms of interacting components. However, in practice conforming to the REST's uniform interface constraint in Web-based services most often proves to be a difficult task, as identified by a number of researchers and practitioners. This implementation and conformance difficulty can be partly attributed to the lack of a systematic conceptual framework that could be used to interpret abstract architectural restrictions of interface uniformity to practical design decisions and strategies being generalized as interface design criteria. These criteria could be then mapped to domain-specific techniques that provide the context for guiding and/or examining the level of uniformity of a REST-based API. In this paper, we\u00a0\u2026", "num_citations": "6\n", "authors": ["117"]}
{"title": "Incremental model synchronization in model driven development environments\n", "abstract": " Most modern model driven software development environments rely heavily on model transformations for generating various software design artifacts and eventually even source code. However, during development, maintenance and evolution activities, these software artifacts are subject to updates and refactoring operations. In such model driven development environments, these software artifacts need to be re-synchronized every time one of them is altered, so that they all remain consistent according to some specific rules, relations and domain constraints. Until now, the standard approach to model synchronization has been the re-application of all transformation rules, aiming thus for the complete re-generation of all artifacts in all models involved. This complete re-application is a safe yet computationally expensive way to ensure consistency among models. In this paper, we present a method for re\u00a0\u2026", "num_citations": "6\n", "authors": ["117"]}
{"title": "Managing known clones: issues and open questions\n", "abstract": " Many software systems contained cloned code, ie, segments of code that are highly similar to each other, typically because one has been copied from the other, and then possibly modified. In some contexts, clones are of interest because they are targets for refactoring. This paper summarizes the results of a working session in which the problems of merely managing clones that are already known to exist. Six key issues in the space are briefly reviewed, and open questions raised in the working session are listed.", "num_citations": "6\n", "authors": ["117"]}
{"title": "Toward program representation and program understanding using process algebras\n", "abstract": " In this paper, we investigate the use of process algebras for program representation and program understanding. Process algebras can be used not only to represent process behaviour, but also to reason about, it by defining equivalence and partial order relations. In our approach, the programming language constructs are denoted as communicating agents, and the program semantics are given in a compositional way in terms of the semantics of the program's subcomponents. Semantic equivalences between code fragments and process algebra descriptions of standard programming plans can he derived in this formalism. This paper illustrates these ideas and identifies relevant. research issues.", "num_citations": "6\n", "authors": ["117"]}
{"title": "Towards a goal driven task personalization specification framework\n", "abstract": " Since its inception, Service Orientation allowed for distributed clients to invoke remote operations utilizing standardized protocols, programming paradigms and architectures. Furthermore, the problem of compiling complex service compositions, based on contextual information and user preferences, has been also extensively investigated by the research community. However, these techniques are mostly used within a single, or within coupled service domains that utilize predefined orchestration and composition service flows. In this paper, we propose an approach whereby service providers can specify complex service tasks as collections of goal model templates that can be instantiated and customized by the invoking clients. A reasoning process evaluates whether instantiated goals can be fulfilled based on the clients selections and consequently generates service flows that are compliant to the goal model and\u00a0\u2026", "num_citations": "5\n", "authors": ["117"]}
{"title": "Model synchronization as a problem of maximizing model dependencies\n", "abstract": " During the course of its evolution, software is modified through models at different levels of abstraction, from the requirements specification to source code. To enable systematic development and maintenance, related models need to be kept synchronized. In order to establish and maintain consistency, we need to precisely define what it means for two models to be synchronized. In this paper, we present our view of model synchronization as a problem of maximizing model dependencies. Our conceptual view of software models is as graphs, and model transformations are viewed in terms of basic graph transformations such as node insertions and deletions. Based on this view, a set of transformations applied to one model is traced and propagated to the other by choosing, from a set of possible transformation paths, a path that maximizes underlying model dependencies.", "num_citations": "5\n", "authors": ["117"]}
{"title": "Distributed Objects and Software Application Wrappers: A Vehicle for Software Reengineering\n", "abstract": " It is always difficult to ensure the success of a re-engineering project. It takes careful planning to set the objectives and pursue realistic solutions that can be both technically feasible, and have a high benefit to investment ratio. A fundamental requirement for software re-engineering is to \u201cunderstand\u201d what and how the existing system delivers its functionality. These tasks can be addressed by re-documentation and design recovery techniques. However, it is not always necessary to re-engineer a system from ground up, and by understanding all of its implementation details. It is a common a scenario in industry, to move towards a software evolutionary pattern in which a legacy system need to be migrated and used in a new operating environment, or be integrated as a component of a new application. Some refer to this pattern as continuous engineering. The requirement in which a re-engineering project is based on\u00a0\u2026", "num_citations": "5\n", "authors": ["117"]}
{"title": "Efficient parallel reasoning on fuzzy goal models for run time requirements verification\n", "abstract": " As software applications become highly interconnected in dynamically provisioned platforms, they form the so-called systems-of-systems. Therefore, a key issue that arises in such environments is whether specific requirements are violated, when these applications interact in new unforeseen ways as new resources or system components are dynamically provisioned. Such environments require the continuous use of frameworks for assessing compliance against specific mission critical system requirements. Such frameworks should be able to (a) handle large requirements models, (b) assess system compliance repeatedly and frequently using events from possibly high velocity and high frequency data streams, and (c) use models that can reflect the vagueness that inherently exists in big data event collection and in modeling dependencies between components of complex and dynamically re-configured\u00a0\u2026", "num_citations": "4\n", "authors": ["117"]}
{"title": "Schema independent reduction of streaming log data\n", "abstract": " Large software systems comprise of different and tightly interconnected components. Such systems utilize heterogeneous monitoring infrastructures which produce log data at high rates from various sources and in diverse formats. The sheer volume of this data makes almost impossible the real- or near real-time processing of these system logs. In this paper, we present a log schema independent approach that allows for the real time reduction of logged data based on a set of filtering criteria. The approach utilizes a similarity measure between features of the incoming events and a set of filtering features we refer to as beacons. The similarity measure is based on information theory principles and uses caching techniques so that infinite log data streams and log data schema alterations can be handled. The approach has been applied successfully on the KDD-99 intrusion detection benchmark data set.", "num_citations": "4\n", "authors": ["117"]}
{"title": "Towards a requirements-driven framework for detecting malicious behavior against software systems\n", "abstract": " Root cause determination for software failures that occurred due to intentional or unintentional third party activities is a difficult and challenging task. In this paper, we propose a new technique for identifying the root causes of system failures stemming from external interventions that is based first, on modeling the conditions by which a system delivers its functionality utilizing goal models, second on modeling the conditions by which system functionality can be compromised utilizing anti-goal models, third representing logged data as well as, goal and anti-goal models as rules and facts in a knowledge base and fourth, utilizing a probabilistic reasoning technique that is based on the use of Markov Logic Networks. The technique is evaluated in a medium size COTS based system and the DARPA 2000 Intrusion Detection data set.", "num_citations": "4\n", "authors": ["117"]}
{"title": "m-Roam: a service invocation and roaming framework for pervasive computing\n", "abstract": " This paper proposes an architectural framework for integrating services within an enterprise and accessing them from mobile devices in a pervasive-computing environment. Present network environments are prone to failures from disconnections and device crashes. Being able to maintain service transaction while moving to different locations and wireless networks (known as service roaming), is currently a major objective in pervasive and mobile computing research. This paper presents a framework (called m-Roam) that allows for mobile clients to perform service invocation and roaming. The framework addresses this problem by introducing a proxy-based architecture so that invocation context and transaction state can be maintained or updated accordingly, while the user roams in different locations.", "num_citations": "4\n", "authors": ["117"]}
{"title": "Partial design recovery using dynamic programming\n", "abstract": " Complete automation of design recovery of large systems is a desirable but impractical goal due to complexity and size issues, so current research efforts focus on redocumentation and partial design recovery. Pattern matching lies at the center of any design recovery system. In the context of a larger project to develop an integrated reverse engineering environment, we are developing a framework for performing clone detection, code localization, and plan recognition. This paper discusses a plan localization and selection strategy based on a dynamic programming function that records the matching process and identifies parts of the plan and code fragment that are most\" similar\". Program features used for matching are currently based on data flow, control flow, and structural properties. The matching model uses a transition network and allows for the detection of insertions and deletions, and it is targeted for legacy\u00a0\u2026", "num_citations": "4\n", "authors": ["117"]}
{"title": "A process algebra based program and system representation for reverse engineering\n", "abstract": " A reverse engineering approach based on process algebras for system representation and understanding is presented. Process algebras offer both a formal framework for representing communicating processes and a proof theory for proving semantic equivalences between them. Programs and program fragments are denoted as concurrent agents and code behaviour is defined in terms of interactions among agents in a process algebra representation suitable for subsequent analysis. Semantic and behavioural equivalences between programming plans, which represent programming stereo-types, and code fragments can be defined in this formal system together with a deduction system to prove them. Several advantages and further research issues on the use of process algebra for reverse engineering and maintenance are identified and discussed.< >", "num_citations": "4\n", "authors": ["117"]}
{"title": "Mining software logs for goal-driven root cause analysis\n", "abstract": " Root cause analysis for software systems is a challenging diagnostic task owing to the complex interactions between system components, the sheer volume of logged data, and the often partial and incomplete information available for root cause analysis purposes. This diagnostic task is usually performed by human experts who create mental models of the system at hand, generate root cause hypotheses, conduct log analysis, and identify the root causes of an observed system failure. In this chapter, we discuss a root cause analysis framework that is based on goal and antigoal models to represent the relationship between a system behavior or requirement, and the necessary conditions, configurations, properties, constraints, or external actions that affect this particular system behavior. We consequently use these models to generate a Markov logic network that allows probabilistic reasoning, so that conclusions\u00a0\u2026", "num_citations": "3\n", "authors": ["117"]}
{"title": "Model Contextual Variability for Agents Using Goals and Commitments.\n", "abstract": " Goal models have been extensively utilized in requirements engineering as they provide an expressive and qualitative way to represent requirements, while recent extensions related to contextual variability have further increased the expressiveness of the models. In addition to their application in requirements engineering however, goal models have been also proposed in the literature as a formal way to define the internal design of agents in multi-agent systems. In this paper we adopt the idea of applying goal models with contextual elements, ie conditional goals, decompositions and contributions, as a mean to model the internal design of agents. Furthermore, we express the way those agents can interact with each other in terms of commitments, a recently introduced modeling concept that can be used for the definition of communication protocols in multi-agent systems. In this context, we introduce a transformation process that maps all conditional elements to commitments and contributions, and hence, reasoning techniques that exist for commitments can be applied to contextual models with no further changes.", "num_citations": "3\n", "authors": ["117"]}
{"title": "Policy modeling and compliance verification in enterprise software systems: A survey\n", "abstract": " During the past few years we are witnessing a paradigm shift in enterprise computing, from the classic host-based service-oriented architecture pattern, to a more complex or elastic computing pattern that facilitates the provision of on-demand computing resources. This new computing paradigm offers numerous advantages but also, poses significant challenges. Advantages are related to the flexibility service providers have on deploying virtual resources on as-needed-basis, providing thus opportunities for large scale computing capabilities, while limiting the total cost of ownership. However, these benefits come at the cost of the user partially losing control over the deployed resources and the cost managing platforms and applications that are now provisioned at an unprecedented rate and interaction complexity. In order to address the above challenges, a service management and service assurance framework is\u00a0\u2026", "num_citations": "3\n", "authors": ["117"]}
{"title": "Special issue on the 12th conference on software maintenance and reengineering (CSMR 2008)\n", "abstract": " Software maintenance and reengineering are vital software engineering activities for facilitating the evolution of large software systems. However, software maintenance is not only to be considered for existing systems, but also for new systems, where software models and artefacts evolve as part of iterative and incremental development processes. The Conference on Software Maintenance and Reengineering is the premier European forum to discuss the theory and practice of software maintenance, reengineering, and evolution of software systems. CSMR promotes fruitful discussion and exchange of experiences among researchers and practitioners about the development of maintainable systems, and their evolution, migration and reengineering. This special issue comprises extended versions of three papers, presented at CSMR 2008. The 12th CSMR focused on \u2018Developing evolvable systems\u2019. The need for\u00a0\u2026", "num_citations": "3\n", "authors": ["117"]}
{"title": "Software architecture recovery\n", "abstract": " Syntax Trees and entity-relationship tuples (similar to RSF facts). RSF facts are presented in the form of tuples such as, function calls function, function references variable, file includes library [5]. In the second phase, user-defined queries provide a hypothesis about the system's assumed architecture. A pattern matching engine is used for finding the optimal variable instantiation for a given AQL query, generating thus a concrete architecture. Therefore, the query represents, in an abstract way, design concepts corresponding to a\" macroscopic design pattern\". For example, a query may focus on obtaining an optimal arrangement of the functions, types, and variables in the modules that conform to the user specified descriptions. The optimal arrangement is obtained with respect to the evidences gathered from the source code using Data Mining techniques. The proposed approach focuses on facilitating partial matching, a situation that is frequent in practice and has been addressed in...", "num_citations": "3\n", "authors": ["117"]}
{"title": "Event clustering for log reduction and run time system understanding\n", "abstract": " Large software systems are constantly monitored so that audits can be initiated, once a failure occurs or when maintenance operations are performed. However, the log files are usually sizeable, and require filtering and reduction in order to be processed efficiently. In this paper, we define the concept of the Event Dependency Graph, and we discuss an event filtering and a use case identification technique, that is based on event clustering. This technique can be used to reduce the size of system logs and assist on system analysis and, program understanding.", "num_citations": "2\n", "authors": ["117"]}
{"title": "The SOA programming model: challenges in a services oriented world\n", "abstract": " Over the past decade there has been a significant growth in the area of Service Oriented systems; in particular, systems that have been designed and built using the Service Oriented Architecture approach. The most popular and frequent implementation of SOA systems is based on Web Services, a technology that utilizes internet application layer protocols and standards for service description, service invocation and service composition. However, due to the plethora and diversity of types of services that are available as run time components and implemented in different languages, the middleware and the infrastructure required to make these components interoperable and deployable as services in different platforms grew in complexity. The technical skills required to design, build and test these systems become more difficult as the complexity and requirements of the systems increases and new deployment\u00a0\u2026", "num_citations": "2\n", "authors": ["117"]}
{"title": "Prototalk: A generative software engineering framework for prototyping protocols in smalltalk\n", "abstract": " Network protocols are complex systems implemented by collections of equally complex software components. In many cases, the realization of such protocols requires extensive prototyping and experimentation with different alternative implementations.In this paper, we present ProtoTalk, a generative, domain-specific software framework that utilizes model driven software engineering principles for prototyping state and message driven protocols with emphasis on telecommunication and network protocols. The framework allows first, for modeling a variety of common protocol features by using mappings from state machines, sequence diagrams and packet encodings to ProtoTalk models, and second, for the consequent automatic generation of prototype Smalltalk code from the aforementioned ProtoTalk models. In this respect, the paper attempts to shed light on the use of generative model driven programming\u00a0\u2026", "num_citations": "2\n", "authors": ["117"]}
{"title": "Semantic Web data description and discovery\n", "abstract": " Currently we are experiencing the emergence of the fourth generation of the World Wide Web which is geared towards service and data provision using semantic and ontological information. Specifically, the objective is for data available on the Web to be described, retrieved, and used using semantic and contextual information. This paper presents a framework that allows such a polymorphic service provision through the introduction of user personas, semantic data descriptions that extend the WSDL and the UDDI protocols. In this way, Web data content is associated at run-time with different services and presentation manifests, according to the context and the environment it is invoked and used in. The framework and its associated architecture have been implemented in a prototype system that utilizes Web services technology.", "num_citations": "2\n", "authors": ["117"]}
{"title": "A generic worklist algorithm for graph reachability problems in program analysis\n", "abstract": " Many program analyses involve, or can be expressed in terms of, a graph reachability problem. We present a generic worklist-style algorithm capable of expressing and solving the graph reachability components of many such analyses. We compare our framework with the language reachability framework proposed by Thomas Reps, and show that some problems are expressible in both frameworks, and some problems can be expressed in only one of the frameworks.", "num_citations": "2\n", "authors": ["117"]}
{"title": "selected papers from the 12th International Symposium on Web Systems Evolution (WSE 2010)\n", "abstract": " The International Symposium on Web Systems Evolution (WSE) is regarded as one of the premier conferences by the software evolution and maintenance community in its domain. As a symposium, an important focus of WSE is to enable stimulating discussions and interactions of researchers, ranging from the software maintenance and evolution community to the Web engineering and service-oriented computing community, from both academia and industry, while also providing a platform for presenting high-quality research papers. To give a visual impression of the topics covered by WSE from 2001 to 2011, Figure 1 shows a tag cloud constructed from the 170 paper titles published in the corresponding proceedings. The first edition of WSE was held in 1999 as \u201c1st International Workshop on Web Site Evolution\u201d, and since its sixth instantiation in 2005, it is held as a symposium. Since its third instantiation in 2001\u00a0\u2026", "num_citations": "1\n", "authors": ["117"]}
{"title": "Techniques for Software Maintenance\n", "abstract": " Software maintenance constitutes a major phase of the software life cycle. Studies indicate that software maintenance is responsible for a significant percentage of a system\u2019s overall cost and effort. The software engineering community has identified four major types of software maintenance, namely, corrective, perfective, adaptive, and preventive maintenance. Software maintenance can be seen from two major points of view. First, the classic view where software maintenance provides the necessary theories, techniques, methodologies, and tools for keeping software systems operational once they have been deployed to their operational environment. Most legacy systems subscribe to this view of software maintenance. The second view is a more modern emerging view, where maintenance is an integral part of the software development process and it should be applied from the early stages in the software life cycle. Regardless of the view by which we consider software maintenance, the fact is that it is the driving force behind software evolution, a very important aspect of a software system. This entry provides an in-depth discussion of software maintenance techniques, methodologies, tools, and emerging trends.", "num_citations": "1\n", "authors": ["117"]}
{"title": "Working Session: Program Comprehension and Migration Strategies for Web Service and Service-Oriented Architectures\n", "abstract": " The migration to Web services has the potential of providing significant value to keeping legacy systems and components operational in a networked environment, a number of critical issues still need to be addressed. These issues can be classified in three categories; a) program comprehension and analysis issues; b) legacy system migration issues and; c) technology and standards issues. This paper discusses and debates approaches for addressing these issues. The paper's objective is twofold. First, is to explore the current state of the art in methods, tools and technologies that can assist in carrying out the required work and to identify a taxonomy of current approaches as well as, their strengths and weaknesses. The second objective is to identify open issues and research opportunities in the area of migrating legacy systems in Web enabled environments", "num_citations": "1\n", "authors": ["117"]}
{"title": "Introduction to the special issue on software analysis, evolution and reengineering\n", "abstract": " Software analysis, evolution, and reengineering are important areas of the software life cycle. The quest to maintain and keep operational large mission critical systems has always been a challenge for software practitioners. This special issue presents a compilation of papers covering six major areas namely, program understanding, tools and environments, source code modeling, component recovery, migration to network-centric platforms, and binary reverse engineering and reengineering. These papers are re-worked and extended versions of papers that appeared in the Working Conference on Reverse Engineering. We hope that the readers will find this collection of papers useful as it provides an indicative view of the current work in the areas of software analysis, evolution, and reengineering.", "num_citations": "1\n", "authors": ["117"]}
{"title": "Interoperability and Integration of Enterprise Applications through Grammar-Based Model Synchronization\n", "abstract": " Enterprise applications consist of heterogeneous components and subsystems that communicate through externally exposed interfaces. Each of the component or system interfaces represents a set of specific constraints and relations to which the systems that implement them must conform. Therefore, the problem of enterprise application integration and interoperability can be seen as a problem of model synchronization of system descriptions and the underlying deployment component models. In this paper, we present an approach to integration and interoperability of enterprise applications through grammar-based model synchronization. As part of the approach, component and subsystem interfaces are viewed as instances of corresponding domains formally defined through annotated domain model grammars. The automated mapping between source and target interfaces is performed via association grammars that represent associated productions of the two domains.", "num_citations": "1\n", "authors": ["117"]}
{"title": "Model synchronization for efficient software application maintenance\n", "abstract": " A business software application often has two perspectives: the business flow and operation that the application intends to solve, and the source code and its design and flow upon which the application is built. As one changes and evolves, the other needs to be synchronized. This paper addresses an approach to synchronize these two models for effective software maintenance.", "num_citations": "1\n", "authors": ["117"]}
{"title": "3rd International Workshop on Net-Centric Computing (NCC 2001): Theme: Migrating to the Web\n", "abstract": " Enlarged ventricular size and/or asymmetry have been found markers for psychiatric illness, including schizophrenia. However, this morphometric feature is non-specific and occurs in many other brain diseases, and its variability in healthy controls is not sufficiently understood. We studied ventricular size and shape in 3D MRI (N= 20) of monozygotic (N= 5) and dizygotic (N= 5) twin pairs. Left and right lateral, third and fourth ventricles were segmented from high-resolution T1w SPGR MRI using supervised classification and 3D connectivity. Surfaces of binary segmentations of left and right lateral ventricles were parametrized and described by a series expansion using spherical harmonics. Objects were aligned using the intrinsic coordinate system of the ellipsoid described by the first order expansion. The metric for pairwise shape similarity was the mean squared distance (MSD) between object surfaces. Without\u00a0\u2026", "num_citations": "1\n", "authors": ["117"]}
{"title": "i-Cube: A tool-set for the dynamic extraction and integration of web data content\n", "abstract": " This paper presents the i-Cube environment, a tool-set that allows for Internet data and content originally available as HTML Web pages and programmatic scripts to be denoted, modeled, and represented in the form of XML documents. These XML documents conform to spe- cific Document Type Definitions and other structural constraints that are fully customizable by the end-user or the service provider. The approach is based on representing HTML document data content in the form of annotated trees. Specific areas of interest and data content in the original HTML document that need to be encoded in the form of an XML rep- resentation, are represented as a collection of annotated sub-trees in the tree that corresponds to a large HTML document. A service integration module allows for different categories of analysis and presentation rules to be invoked according to script based user-defined logic.", "num_citations": "1\n", "authors": ["117"]}
{"title": "Customizable service integration in Web-enabled environments\n", "abstract": " In recent years we have been experiencing a tremendous change in software development processes, where new systems are built by utilizing distributed, possibly heterogeneous, components. In this pa- per, we propose an infrastructure and a meta programming environment that allows for distributed components to be integrated, in a fully cus- tomizable manner, intoWeb-enabled environments. In particular, we pro- pose an architecture that conforms to the event-condition-action para- digm. A set of event-condition-action rules combined with a rule enact- ment engine serves as a driver that determines the transaction logic by which remote services are invoked. A prototype system using the pro- posed architecture applied to the domain of e-commerce is also presented.", "num_citations": "1\n", "authors": ["117"]}
{"title": "Reverse engineering questionnaire\n", "abstract": " This is a questionnaire on program understanding and reverse engineering. It may be filled out manually or on-line. The results of the questionnaire will be used to guide the research of the two authors, both of whom are Ph.D. students working in this area. Copies of the resulting report will be mailed to all who participate, and a summary of the results will be published in an appropriate forum.", "num_citations": "1\n", "authors": ["117"]}
{"title": "Customizable Service Integration in Web-enabled Environments\u201d s,\u201d\n", "abstract": " Over the recent years we are experiencing a tremendous change in software development processes, where new systems are built by utilizing distributed, possibly heterogeneous components. In this paper, we propose an infrastructure and a meta programming environment that allows for distributed components to be integrated in a fully customizable way in Web-enabled environments.In particular, we propose an architecture that conforms to the event-condition-action paradigm. A set of event-condition-action rules combined with, a rule enactment engine serves as a driver that determines the transaction logic by which remote services are invoked. A prototype system using the proposed architecture applied to the domain of e-commerce is also presented.", "num_citations": "1\n", "authors": ["117"]}