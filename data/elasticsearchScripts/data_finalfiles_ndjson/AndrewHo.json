{"title": "Studying learning in the worldwide classroom research into edX's first MOOC.\n", "abstract": " \u201cCircuits and Electronics\u201d(6.002 x), which began in March 2012, was the first MOOC developed by edX, the consortium led by MIT and Harvard. Over 155,000 students initially registered for 6.002 x, which was composed of video lectures, interactive problems, online laboratories, and a discussion forum. As the course ended in June 2012, researchers began to analyze the rich sources of data it generated. This article describes both the first stage of this research, which examined the students\u2019 use of resources by time spent on each, and a second stage that is producing an in-depth picture of who the 6.002 x students were, how their own background and capabilities related to their achievement and persistence, and how their interactions with 6.002 x\u2019s curricular and pedagogical components contributed to their level of success in the course.", "num_citations": "1415\n", "authors": ["1043"]}
{"title": "HarvardX and MITx: The first year of open online courses, fall 2012-summer 2013\n", "abstract": " HarvardX and MITx are collaborative institutional efforts between Harvard University and MIT to enhance campus-based education, advance educational research, and increase access to online learning opportunities worldwide. Over the year from the fall of 2012 to the summer of 2013, HarvardX and MITx launched 17 courses on edX, a jointly founded platform for delivering massive open online courses (MOOCs). In that year, 43,196 registrants earned certificates of completion. Another 35,937 registrants explored half or more of course content without certification. An additional 469,702 registrants viewed less than half of the content. And 292,852 registrants never engaged with the online content. In total, there were 841,687 registrations from 597,692 unique users across the first year of HarvardX and MITx courses. This report is a joint effort by institutional units at Harvard and MIT to describe the registrant and course data provided by edX in the context of the diverse efforts and intentions of HarvardX and MITx instructor teams.", "num_citations": "437\n", "authors": ["1043"]}
{"title": "Changing \u201ccourse\u201d reconceptualizing educational variables for massive open online courses\n", "abstract": " In massive open online courses (MOOCs), low barriers to registration attract large numbers of students with diverse interests and backgrounds, and student use of course content is asynchronous and unconstrained. The authors argue that MOOC data are not only plentiful and different in kind but require reconceptualization\u2014new educational variables or different interpretations of existing variables. The authors illustrate this by demonstrating the inadequacy or insufficiency of conventional interpretations of four variables for quantitative analysis and reporting: enrollment, participation, curriculum, and achievement. Drawing from 230 million clicks from 154,763 registrants for a prototypical MOOC offering in 2012, the authors present new approaches to describing and understanding user behavior in this emerging educational context.", "num_citations": "414\n", "authors": ["1043"]}
{"title": "The Reliability of Classroom Observations by School Personnel. Research Paper. MET Project.\n", "abstract": " ABOUT THis repOrT: This report presents an in-depth discussion of the technical methods, results, and implications of the MET project\u2019s study of video-based classroom observations by school personnel. 1 A non-technical summary of the analysis is in the policy and practitioner brief,", "num_citations": "277\n", "authors": ["1043"]}
{"title": "Descriptive statistics for modern test score distributions: Skewness, kurtosis, discreteness, and ceiling effects\n", "abstract": " Many statistical analyses benefit from the assumption that unconditional or conditional distributions are continuous and normal. More than 50 years ago in this journal, Lord and Cook chronicled departures from normality in educational tests, and Micerri similarly showed that the normality assumption is met rarely in educational and psychological practice. In this article, the authors extend these previous analyses to state-level educational test score distributions that are an increasingly common target of high-stakes analysis and interpretation. Among 504 scale-score and raw-score distributions from state testing programs from recent years, nonnormal distributions are common and are often associated with particular state programs. The authors explain how scaling procedures from item response theory lead to nonnormal distributions as well as unusual patterns of discreteness. The authors recommend that\u00a0\u2026", "num_citations": "244\n", "authors": ["1043"]}
{"title": "HarvardX and MITx: Two years of open online courses fall 2012-summer 2014\n", "abstract": " What happens when well-known universities offer online courses, assessments, and certificates of completion for free? Early descriptions of Massive Open Online Courses (MOOCs) have emphasized large enrollments, low certification rates, and highly educated registrants. We use data from two years and 68 open online courses offered by Harvard University (via HarvardX) and MIT (via MITx) to broaden the scope of answers to this question. We describe trends over this two-year span, depict participant intent using comprehensive survey instruments, and chart course participation pathways using network analysis. We find that overall participation in our MOOCs remains substantial and that the average growth has been steady. We explore how diverse audiences\u2014including explorers, teachers-as-learners, and residential students\u2014provide opportunities to advance the principles on which HarvardX and MITx were founded: access, research, and residential education.", "num_citations": "232\n", "authors": ["1043"]}
{"title": "The problem with \u201cproficiency\u201d: Limitations of statistics and policy under No Child Left Behind\n", "abstract": " The Percentage of Proficient Students (PPS) has become a ubiquitous statistic under the No Child Left Behind Act. This focus on proficiency has statistical and substantive costs. The author demonstrates that the PPS metric offers only limited and unrepresentative depictions of large-scale test score trends, gaps, and gap trends. The limitations are unpredictable, dramatic, and difficult to correct in the absence of other data. Interpretation of these depictions generally leads to incorrect or incomplete inferences about distributional change. The author shows how the statistical shortcomings of these depictions extend to shortcomings of policy, from exclusively encouraging score gains near the proficiency cut score to shortsighted comparisons of state and national testing results. The author proposes alternatives for large-scale score reporting and argues that a distribution-wide perspective on results is required for any\u00a0\u2026", "num_citations": "203\n", "authors": ["1043"]}
{"title": "Privacy, anonymity, and big data in the social sciences\n", "abstract": " Quality social science research and the privacy of human subjects require trust.", "num_citations": "171\n", "authors": ["1043"]}
{"title": "Development and validation of a novel patient educational booklet to enhance colonoscopy preparation\n", "abstract": " OBJECTIVES:The success of colonoscopy depends on high-quality bowel preparation by patients; yet inadequate preparation is common. We developed and tested an educational booklet to improve bowel preparation quality.METHODS:We conducted patient cognitive interviews to identify knowledge and belief barriers to colonoscopy preparation. We used these interviews to create an educational booklet to enhance preparatory behaviors. We then prospectively randomized patients scheduled for outpatient colonoscopy at a VA Medical Center to receive usual instructions vs. the booklet before colonoscopy. Patients in both groups received standard pharmacy instructions for single-dose bowel preparation; the protocol did not specify which purgatives to prescribe. The primary outcome was preparation quality based on blinded ratings using the validated Ottawa score. We performed bivariate analyses to compare\u00a0\u2026", "num_citations": "160\n", "authors": ["1043"]}
{"title": "HarvardX and MITx: Four years of open online courses--fall 2012-summer 2016\n", "abstract": " In 2014 and 2015, a joint research team from Harvard University and MIT released summary reports describing the first two years of Harvard and MIT open online courses launched on the nonprofit learning platform, edX. These reports set expectations for the demographics and behavior of course participants and established an analytic framework for understanding the then-nascent online learning context known as the Massive Open Online Course (MOOC).", "num_citations": "154\n", "authors": ["1043"]}
{"title": "A Practitioner's Guide to Growth Models.\n", "abstract": " We would also like to acknowledge the support of SCASS advisors Charlene Tucker, Duncan MacQuarrie, and Doug Rindone in providing feedback throughout the development of this report. Their vision for clear and accurate descriptions of growth models improved the content and the style of the document. Any remaining errors are ours.", "num_citations": "145\n", "authors": ["1043"]}
{"title": "Criteria for high-quality assessment\n", "abstract": " States and school districts across the nation are making critical decisions about student assessments as they move to implement the Common Core State Standards (CCSS), adopted by 45 states. The Standards feature an increased focus on deeper learning, or students\u2019 ability to analyze, synthesize, compare, connect, critique, hypothesize, prove, and explain their ideas. States are at different points in the CCSS transitions, but all will be assessing their K\u201312 students against these higher standards in the 2014\u201315 school year.Based on the changing demands of today\u2019s workforce, advances in other nations, and original analysis, this report provides a set of criteria for high-quality student assessments. These criteria can be used by assessment developers, policymakers, and educators as they work to create and adopt assessments that promote deeper learning of 21stcentury skills that students need to succeed in today\u2019s knowledge-based economy.", "num_citations": "90\n", "authors": ["1043"]}
{"title": "Detecting and preventing \u201cmultiple-account\u201d cheating in massive open online courses\n", "abstract": " We describe a cheating strategy enabled by the features of massive open online courses (MOOCs) and detectable by virtue of the sophisticated data systems that MOOCs provide. The strategy, Copying Answers using Multiple Existences Online (CAMEO), involves a user who gathers solutions to assessment questions using a \u201charvester\u201d account and then submits correct answers using a separate \u201cmaster\u201d account. We use a small-scale experiment to verify CAMEO and estimate a \u201clower bound\u201d for its prevalence among 1.9 million course participants in 115 MOOCs from two universities. Using conservative thresholds, we estimate CAMEO prevalence at 1237 certificates, accounting for 1.3% of the certificates in the 69 MOOCs with CAMEO users. Among earners of 20 or more certificates, 25% have used the CAMEO strategy. CAMEO users are more likely to be young, male, and international than other MOOC\u00a0\u2026", "num_citations": "88\n", "authors": ["1043"]}
{"title": "Contrasting OLS and quantile regression approaches to student \u201cgrowth\u201d percentiles\n", "abstract": " Regression methods can locate student test scores in a conditional distribution, given past scores. This article contrasts and clarifies two approaches to describing these locations in terms of readily interpretable percentile ranks or \u201cconditional status percentile ranks.\u201d The first is Betebenner\u2019s quantile regression approach that results in \u201cStudent Growth Percentiles.\u201d The second is an ordinary least squares (OLS) regression approach that involves expressing OLS regression residuals as percentile ranks. The study describes the empirical and conceptual similarity of the two metrics in simulated and real-data scenarios. The metrics contrast in their scale-transformation invariance and sample size requirements but are comparable in their dependence on the number of prior years used as conditioning variables. These results support guidelines for selecting the model that best fits the data and have implications for the\u00a0\u2026", "num_citations": "73\n", "authors": ["1043"]}
{"title": "Evaluating the geographic data in MOOCs\n", "abstract": " Massive Open Online Courses (MOOCs) exhibit a remarkable geographic diversity in terms of student population. While some researchers evaluated MOOCs geography within a single class, there is no framework for systematically studying the geographic component of MOOCs. Using the example of student population of 18 courses offered by HarvardX, Harvard\u2019s division for online learning, we formalize the process of evaluating the geographic data of MOOCs with regard to enrollment and certificate attainment. We report the absolute counts of students from various countries and relate them to baselines of potential learners such as population of the country and the number of English speakers. Our approach allows to identify clusters of countries with similar properties and opens discussion of country-specific factors influencing key metrics of MOOCs. Our findings are relevant for developers and marketers, as well as for future research involving MOOC geography.", "num_citations": "69\n", "authors": ["1043"]}
{"title": "Estimating achievement gaps from test scores reported in ordinal \u201cproficiency\u201d categories\n", "abstract": " Test scores are commonly reported in a small number of ordered categories. Examples of such reporting include state accountability testing, Advanced Placement tests, and English proficiency tests. This article introduces and evaluates methods for estimating achievement gaps on a familiar standard-deviation-unit metric using data from these ordered categories alone. These methods hold two practical advantages over alternative achievement gap metrics. First, they require only categorical proficiency data, which are often available where means and standard deviations are not. Second, they result in gap estimates that are invariant to score scale transformations, providing a stronger basis for achievement gap comparisons over time and across jurisdictions. The authors find three candidate estimation methods that recover full-distribution gap estimates well when only censored data are available.", "num_citations": "68\n", "authors": ["1043"]}
{"title": "Response switching and self-efficacy in Peer Instruction classrooms\n", "abstract": " Peer Instruction, a well-known student-centered teaching method, engages students during class through structured, frequent questioning and is often facilitated by classroom response systems. The central feature of any Peer Instruction class is a conceptual question designed to help resolve student misconceptions about subject matter. We provide students two opportunities to answer each question\u2014once after a round of individual reflection and then again after a discussion round with a peer. The second round provides students the choice to \u201cswitch\u201d their original response to a different answer. The percentage of right answers typically increases after peer discussion: most students who answer incorrectly in the individual round switch to the correct answer after the peer discussion. However, for any given question there are also students who switch their initially right answer to a wrong answer and students who\u00a0\u2026", "num_citations": "66\n", "authors": ["1043"]}
{"title": "A nonparametric framework for comparing trends and gaps across tests\n", "abstract": " Problems of scale typically arise when comparing test score trends, gaps, and gap trends across different tests. To overcome some of these difficulties, test score distributions on the same score scale can be represented by nonparametric graphs or statistics that are invariant under monotone scale transformations. This article motivates and then develops a framework for the comparison of these nonparametric trend, gap, and gap trend representations across tests. The connections between this framework and other nonparametric tools, including probability\u2013probability (PP) plots, the Mann-Whitney U test, and the statistic known as P(Y > X), are highlighted. The author describes the advantages of this framework over scale-dependent trend and gap statistics and demonstrates applications of these nonparametric methods to frequently asked policy questions.", "num_citations": "64\n", "authors": ["1043"]}
{"title": "Bringing student backgrounds online: MOOC user demographics, site usage, and online learning\n", "abstract": " MOOCs gather a rich array of click-stream information from students who interact with the platform. However, without student background information, inferences do not take advantage of a deeper understanding of students\u2019 prior experiences, motivation, and home environment. In this poster, we investigate the predictive power of student background factors as well as student experiences with learning materials provided in the first MITx course,\u201cCircuits and Electronics.\u201d We focus on a group of survey completers who were given background questions, and we use multiple regression methods to investigate the relationship between achievement, online resource use, and student background. Online course providers may be able to better tailor online experiences to students when they know how background characteristics mediate the online experience.", "num_citations": "60\n", "authors": ["1043"]}
{"title": "Big data analysis in higher education: Promises and pitfalls\n", "abstract": " The grand challenge in data-intensive research and analysis in higher education is to find the means to extract knowledge from the extremely rich data sets being generated today and to distill this into usable information for students, instructors, and the public.", "num_citations": "49\n", "authors": ["1043"]}
{"title": "Using heteroskedastic ordered probit models to recover moments of continuous test score distributions from coarsened data\n", "abstract": " Test score distributions of schools or demographic groups are often summarized by frequencies of students scoring in a small number of ordered proficiency categories. We show that heteroskedastic ordered probit (HETOP) models can be used to estimate means and standard deviations of multiple groups\u2019 test score distributions from such data. Because the scale of HETOP estimates is indeterminate up to a linear transformation, we develop formulas for converting the HETOP parameter estimates and their standard errors to a scale in which the population distribution of scores is standardized. We demonstrate and evaluate this novel application of the HETOP model with a simulation study and using real test score data from two sources. We find that the HETOP model produces unbiased estimates of group means and standard deviations, except when group sample sizes are small. In such cases, we demonstrate\u00a0\u2026", "num_citations": "47\n", "authors": ["1043"]}
{"title": "Do students know what they know? Exploring the accuracy of students\u2019 self-assessments\n", "abstract": " We have conducted an investigation into how well students in introductory science classes (both physics and chemistry) are able to predict which questions they will or will not be able to answer correctly on an upcoming assessment. An examination of the data at the level of students\u2019 overall scores reveals results consistent with the Dunning-Kruger effect, in which low-performing students tend to overestimate their abilities, while high-performing students estimate their abilities more accurately. Similar results have been widely reported in the science education literature. Breaking results out by students\u2019 responses to individual questions, however, reveals that students of all ability levels have difficulty distinguishing questions which they are able to answer correctly from those that they are not able to answer correctly. These results have implications for the future study and reporting of students\u2019 metacognitive abilities.", "num_citations": "47\n", "authors": ["1043"]}
{"title": "Practical issues in estimating achievement gaps from coarsened data\n", "abstract": " In an earlier paper, we presented methods for estimating achievement gaps when test scores are coarsened into a small number of ordered categories, preventing fine-grained distinctions between individual scores. We demonstrated that gaps can nonetheless be estimated with minimal bias across a broad range of simulated and real coarsened data scenarios. In this article, we extend this previous work to obtain practical estimates of the imprecision imparted by the coarsening process and of the bias imparted by measurement error. In the first part of this article, we derive standard error estimates and demonstrate that coarsening leads to only very modest increases in standard errors under a wide range of conditions. In the second part of this article, we describe and evaluate a practical method for disattenuating gap estimates to account for bias due to measurement error.", "num_citations": "45\n", "authors": ["1043"]}
{"title": "Characterizing video use in the catalogue of MITx MOOCs\n", "abstract": " Lecture videos intended to substitute or parallel the on-campus experience are a central component of nearly all current Massive Open Online Courses (MOOCs). Recent analysis of resources used in the inaugural course from MITx (6.002 x: Circuits and Electronics) revealed that only half of all certificate earners watched more than half the available lecture videos (Breslow et al. 2013, Seaton et al. 2014), with the distribution of videos accessed by certificate earners being distinctly bimodal. This study shows that bimodal lecture-video use by certificate earners persists in repeated offerings of 6.002 x, with the distribution of video accesses being nearly indistinguishable. However, there are generally two modes of video use spanning the catalogue of MITx courses: bimodal and high use, both characterized via analysis of the distribution of unique videos accessed in each course. For both modes of video use, country-of-origin significantly impacts the measurement of video accesses. In addition, preliminary results explore how course structure impacts overall video consumption across courses.", "num_citations": "45\n", "authors": ["1043"]}
{"title": "The dependence of growth\u2010model results on proficiency cut scores\n", "abstract": " States participating in the Growth Model Pilot Program reference individual student growth against \u201cproficiency\u201d cut scores that conform with the original No Child Left Behind Act (NCLB). Although achievement results from conventional NCLB models are also cut\u2010score dependent, the functional relationships between cut\u2010score location and growth results are more complex and are not currently well described. We apply cut\u2010score scenarios to longitudinal data to demonstrate the dependence of state\u2010 and school\u2010level growth results on cut\u2010score choice. This dependence is examined along three dimensions: 1) rigor, as states set cut scores largely at their discretion, 2) across\u2010grade articulation, as the rigor of proficiency standards may vary across grades, and 3) the time horizon chosen for growth to proficiency. Results show that the selection of plausible alternative cut scores within a growth model can change the\u00a0\u2026", "num_citations": "45\n", "authors": ["1043"]}
{"title": "Metric-free measures of test score trends and gaps with policy-relevant examples\n", "abstract": " Problems of scale typically arise when comparing test score trends, gaps, and gap trends across different tests. To overcome some of these difficulties, we can express the difference between the observed test performance of two groups with graphs or statistics that are metric-free (ie, invariant under positive monotonic transformations of the test score scale). In a series of studies broken into three parts, we develop a framework for the application of metric-free methods to routine policy questions. The first part introduces metric-free methodology and demonstrates the advantages of these methods when test score scales do not have defensible interval properties. The second part uses metric-free methods to compare gaps in Hispanic-White achievement in California across four testing programs over a 7-year period. The third part uses metric-free methods to compare trends for \u201chigh-stakes\u201d State Reading test scores to\u00a0\u2026", "num_citations": "44\n", "authors": ["1043"]}
{"title": "Addressing common analytic challenges to randomized experiments in MOOCs: Attrition and zero-inflation\n", "abstract": " Massive open online course (MOOC) platforms increasingly allow easily implemented randomized experiments. The heterogeneity of MOOC students, however, leads to two methodological obstacles in analyzing interventions to increase engagement.(1) Many MOOC participation metrics have distributions with substantial positive skew from highly active users as well as zero-inflation from high attrition.(2) High attrition means that in some experimental designs, most users assigned to the treatment never receive it; analyses that do not consider attrition result in\" intent-to-treat\"(ITT) estimates that underestimate the true effects of interventions. We address these challenges in analyzing an intervention to improve forum participation in the 2014 JusticeX course offered on the edX MOOC platform. We compare the results of four ITT models (OLS, logistic, quantile, and zero-inflated negative binomial regressions) and three\u00a0\u2026", "num_citations": "40\n", "authors": ["1043"]}
{"title": "Discrepancies Between Score Trends from NAEP and State Tests: A Scale\u2010Invariant Perspective\n", "abstract": " State test score trends are widely interpreted as indicators of educational improvement. To validate these interpretations, state test score trends are often compared to trends on other tests such as the National Assessment of Educational Progress (NAEP). These comparisons raise serious technical and substantive concerns. Technically, the most commonly used trend statistics\u2014for example, the change in the percent of proficient students\u2014are misleading in the context of cross\u2010test comparisons. Substantively, it may not be reasonable to expect that NAEP and state test score trends should be similar. This paper motivates then applies a \u201cscale\u2010invariant\u201d framework for cross\u2010test trend comparisons to compare \u201chigh\u2010stakes\u201d state test score trends from 2003 to 2005 to NAEP trends over the same period. Results show that state trends are significantly more positive than NAEP trends. The paper concludes with cautions\u00a0\u2026", "num_citations": "36\n", "authors": ["1043"]}
{"title": "Practical differences among aggregate-level conditional status metrics: From median student growth percentiles to value-added models\n", "abstract": " Aggregate-level conditional status metrics (ACSMs) describe the status of a group by referencing current performance to expectations given past scores. This article provides a framework for these metrics, classifying them by aggregation function (mean or median), regression approach (linear mean and nonlinear quantile), and the scale that supports interpretations (percentile rank and score scale), among other factors. This study addresses the question \u201chow different are these ACSMs?\u201d in three ways. First, using simulated data, it evaluates how well each model recovers its respective parameters. Second, using both simulated and empirical data, it illustrates practical differences among ACSMs in terms of pairwise rank differences incurred by switching between metrics. Third, it ranks ACSMs in terms of their robustness under scale transformations. The results consistently show that choices between mean- and\u00a0\u2026", "num_citations": "35\n", "authors": ["1043"]}
{"title": "Final report on the evaluation of the growth model pilot project\n", "abstract": " A key goal of the 1994 reauthorization of the Elementary and Secondary Education Act (ESEA) was to introduce a standards-based accountability system that \u201crequired states to define criteria for measuring adequate yearly progress (AYP) in school performance for Title I schools and districts.\u201d 3 States were given considerable latitude in how to determine AYP, with the majority relying on various types of aggregate school-improvement models rather than setting absolute proficiency targets for students. 4With the increasing availability of statewide longitudinally linked student performance records since 2001, it became possible for some states to measure student growth and use those data for accountability purposes. The US Department of Education initiated the Growth Model Pilot Project (GMPP) in November 2005 with the goal of approving up to ten states to incorporate growth models in school AYP determinations under ESEA. Growth models are defined as complements or alternatives to the standard status model for determining school AYP. The status model bases AYP on the proportion of a school\u2019s students attaining proficiency in reading and mathematics in a given year. Growth models, in contrast, base AYP in part on the proportion of individual students who are making sufficient annual progress to reach grade-level proficiency within a specific time horizon of three to five years or by grades 7, 8, or 9. Growth models promise to provide a fuller understanding of school effectiveness and the progress each school\u2019s students are making toward their proficiency goals. The main objectives of the GMPP are to help states develop and implement\u00a0\u2026", "num_citations": "34\n", "authors": ["1043"]}
{"title": "Analysis of student engagement in an online annotation system in the context of a flipped introductory physics class\n", "abstract": " We discuss student participation in an online social annotation forum over two semesters of a flipped, introductory physics course at Harvard University. We find that students who engage in high-level discussion online, especially by providing answers to their peers\u2019 questions, make more gains in conceptual understanding than students who do not. This is true regardless of students\u2019 physics background. We find that we can steer online interaction towards more productive and engaging discussion by seeding the discussion and managing the size of the sections. Seeded sections produce higher quality annotations and a greater proportion of generative threads than unseeded sections. Larger sections produce longer threads; however, beyond a certain section size, the quality of the discussion decreases.", "num_citations": "31\n", "authors": ["1043"]}
{"title": "Evaluating the flipped classroom in an undergraduate history course\n", "abstract": " Abstract In spring of 2012, Harvard University and MIT announced the launch of edX, an open online learning platform that allowed anyone with an internet connection to register and complete online courses. The edX mission included the improvement of on-campus, residential education as well as advancing educational research. This report documents the impact of an effort to transform an undergraduate history course using a \u201cflipped classroom\u201d model, where online videos and resources from the edX platform supplemented in-class discussions and activities. A baseline administration of the course was in the fall of 2011, prior to the launch of edX. The second administration used the flipped classroom format in the fall of 2013.By holding the midterm exam fixed from 2011 to 2013, as well as using deidentified institutional data, including achievement test scores and grade point averages, we employed a quasi-experimental design that supports interpretations about the causal impact of the flipped classroom on student learning.", "num_citations": "31\n", "authors": ["1043"]}
{"title": "Validation methods for aggregate-level test scale linking: A case study mapping school district test score distributions to a common scale\n", "abstract": " Linking score scales across different tests is considered speculative and fraught, even at the aggregate level. We introduce and illustrate validation methods for aggregate linkages, using the challenge of linking U.S. school district average test scores across states as a motivating example. We show that aggregate linkages can be validated both directly and indirectly under certain conditions such as when the scores for at least some target units (districts) are available on a common test (e.g., the National Assessment of Educational Progress). We introduce precision-adjusted random effects models to estimate linking error, for populations and for subpopulations, for averages and for progress over time. These models allow us to distinguish linking error from sampling variability and illustrate how linking error plays a larger role in aggregates with smaller sample sizes. Assuming that target districts generalize to the full\u00a0\u2026", "num_citations": "29\n", "authors": ["1043"]}
{"title": "Advancing educational research and student privacy in the \u201cbig data\u201d era\n", "abstract": " Education is data intensive. Teachers interpret data in the form of verbal and nonverbal cues from students to adjust and improve their pedagogy. Parents receive data in the form of daily schoolwork, formal report cards, and informal stories from their children in car rides and at dinner tables. Teachers and school leaders routinely collect administrative data for compliance, monitoring, feedback, and improvement. As educational researchers, we systematize data collection and, sometimes, control the context of data creation, all to improve understanding of educational policy and practice. These systematic processes increase the usefulness of educational data but also increase the risks of exposure and harm to subjects. Collected data can be misplaced, stolen, or subjected to malicious analysis that reveals identities. Controlling the context of data creation may have a worse impact on subjects than not intervening at all. Educational data systems are larger and more connected than ever before. As federal and state educational accountability systems have developed, administrative data have been centralized. State systems can increasingly describe the academic progress of individual students over time. Some have linkages to postsecondary and labor force outcomes. In the meantime, digital data collection and learning systems have proliferated in schools. Learning processes that were once informal, unstructured, and undocumented have become a data resource for students, teachers, parents, and researchers alike. Educational interactions are no longer bifurcated into informal classroom practices and formal administrative documentation\u00a0\u2026", "num_citations": "18\n", "authors": ["1043"]}
{"title": "Measuring Teaching Practices at Scale: Results from the Development and Validation of the Teach Classroom Observation Tool\n", "abstract": " What goes on inside the classroom is central to student learning. Despite its importance, low-and middle-income countries rarely measure teaching practices, in part due to a lack of access to adequate classroom observation tools and the high transaction costs associated with administering them. Teach, a new, open-source classroom observation tool for primary classrooms, was developed to capture the quantity and quality of teaching practices in these settings with a simple, easy-to-administer tool. This paper validates the use of Teach scores for system diagnostics by providing four types of evidence. First, it provides evidence that the practices included in the tool have a clear conceptual underpinning. Second, almost 90 percent of local observers in Mozambique, Pakistan, the Philippines, and Uruguay were highly accurate using Teach after a four-day training. Third, using data from 845 classrooms in Pakistan, the paper shows that Teach scores are internally consistent, present moderate to high inter-rater reliability in the field (. 75 intraclass correlation coefficient), and provide substantial information that allows to differentiate teachers, even those with similar but not equal scores. Finally, teachers who display effective practices, as measured by Teach, are associated with students who achieve higher learning outcomes.", "num_citations": "13\n", "authors": ["1043"]}
{"title": "HeroesX: The Ancient Greek Hero: Spring 2013 Course Report\n", "abstract": " CB22x: The Ancient Greek Hero, was offered as a HarvardX course in Spring 2013 on edX, a platform for massive open online courses (MOOCs). It was taught by Professor Greg Nagy. The report was prepared by researchers external to the course team, based on examination of the courseware, analyses of the data collected by the edX platform, and interviews and consultations with the course faculty and team members.", "num_citations": "13\n", "authors": ["1043"]}
{"title": "Metric-Free Measures of Test Score Trends and Gaps with Policy-Relevant Examples. CSE Report 665.\n", "abstract": " Problems of scale typically arise when comparing test score trends, gaps, and gap trends across different tests. To overcome some of these difficulties, we can express the difference between the observed test performance of two groups with graphs or statistics that are metric-free (ie, invariant under positive monotonic transformations of the test score scale). In a series of studies broken into three parts, we develop a framework for the application of metric-free methods to routine policy questions. The first part introduces metric-free methodology and demonstrates the advantages of these methods when test score scales do not have defensible interval properties. The second part uses metric-free methods to compare gaps in Hispanic-White achievement in California across four testing programs over a 7-year period. The third part uses metric-free methods to compare trends for \u201chigh-stakes\u201d State Reading test scores to State score trends on the National Assessment of Educational Progress from 2002 to 2003. As a whole, this series of studies represents an argument for the usefulness of metric-free methods for quantifying trends and gaps and the superiority of metric-free methods for comparing trends and gaps across tests with different score scales.", "num_citations": "11\n", "authors": ["1043"]}
{"title": "How Should Colleges Treat Multiple Admissions Test Scores?\n", "abstract": " The percentage of students retaking college admissions tests is rising. Researchers and college admissions offices currently use a variety of methods for summarizing these multiple scores. Testing organizations such as ACT and the College Board, interested in validity evidence like correlations with first\u2010year grade point average (FYGPA), often use the most recent test score available. In contrast, institutions report using a variety of composite scoring methods for applicants with multiple test records, including averaging and taking the maximum subtest score across test occasions (\u201csuperscoring\u201d). We compare four scoring methods on two criteria. First, we compare correlations between scores and FYGPA by scoring method. We find them similar (). Second, we compare the extent to which test scores differentially predict FYGPA by scoring method and number of retakes. We find that retakes account for additional\u00a0\u2026", "num_citations": "10\n", "authors": ["1043"]}
{"title": "Apples to apples? The underlying assumptions of state-NAEP comparisons\n", "abstract": " Mappings of State Performance Standards onto the NAEP Scale, anticipates an upcoming report from the National Center for Education Statistics (NCES). In this report, Professor Henry Braun of Boston College and Dr. Jiahe Qian of Educational Testing Service extend a method that locates state performance standards on the NAEP score scale. This mapping affords interpretations of state performance standards as \u201chigher\u201d or \u201clower\u201d than other state or NAEP standards. Our first brief asserts that such interpretations depend logically on an argument for the equivalence of state tests and NAEP that is rarely advanced and probably not entirely valid. Reasonable equivalence between state tests and NAEP may or may not exist. However, we show that substantial differences between state tests and NAEP will render the mapping illogical and subject to drift over time.", "num_citations": "10\n", "authors": ["1043"]}
{"title": "Due dates in MOOCs: Does stricter mean better?\n", "abstract": " Massive Open Online Courses (MOOCs) employ a variety of components to engage students in learning (eg. videos, forums, quizzes). Some components are graded, which means that they play a key role in a student's final grade and certificate attainment. It is not yet clear how the due date structure of graded components affects student outcomes including academic performance and alternative modes of learning of students. Using data from HarvardX and MITx, Harvard's and MIT's divisions for online learning, we study the structure of due dates on graded components for 10 completed MOOCs. We find that stricter due dates are associated with higher certificate attainment rates but fewer students who join late being able to earn a certificate. Our findings motivate further studies of how the use of graded components and deadlines affects academic and alternative learning of MOOC students, and can help inform the\u00a0\u2026", "num_citations": "9\n", "authors": ["1043"]}
{"title": "Variety and Drift in the Functions and Purposes of Assessment in K-12 Education.\n", "abstract": " Terms of Use This article was downloaded from Harvard University\u2019s DASH repository, and is made available under the terms and conditions applicable to Open Access Policy Articles, as set forth at http://nrs. harvard. edu/urn-3: HUL. InstRepos: dash. current. terms-ofuse# OAP", "num_citations": "9\n", "authors": ["1043"]}
{"title": "Linking US School District Test Score Distributions to a Common Scale. CEPA Working Paper No. 16-09.\n", "abstract": " There is no comprehensive database of US district-level test scores that is comparable across states. We describe and evaluate a method for constructing such a database. First, we estimate linear, reliabilityadjusted linking transformations from state test score scales to the scale of the National Assessment of Educational Progress (NAEP). We then develop and implement direct and indirect validation checks for linking assumptions. We conclude that the linking method is accurate enough to be used in analyses of national variation in district achievement, but that the small amount of linking error in the methods renders fine-grained distinctions among districts in different states invalid. Finally, we describe several different methods of scaling and pooling the linked scores to support a range of secondary analyses and interpretations.", "num_citations": "8\n", "authors": ["1043"]}
{"title": "The epidemiology of modern test score use: Anticipating aggregation, adjustment, and equating\n", "abstract": " In his thoughtful focus article, Haertel (this issue) pushes testing experts to broaden the scope of their validation efforts and to invite scholars from other disciplines to join them. He credits existing validation frameworks for helping the measurement community to identify incomplete or nonexistent validity arguments. However, he notes his sense that something is missing in these frameworks, particularly as they seem to identify questionable, poorly articulated, or inappropriate uses only after the milk, as it were, has been spilled. I found his description of these uses helpful, particularly his identification of \u201cindirect actions\u201d of tests that have been a blind spot, by accident or by design, for validation efforts. His piece represents a call to action for testing experts to embrace more responsibility for validation and forge new alliances to get the work done. In this brief response, I try to maintain the momentum that Haertel (this\u00a0\u2026", "num_citations": "8\n", "authors": ["1043"]}
{"title": "Using Test Content to Address Trend Discrepancies between NAEP and California State Tests.\n", "abstract": " Statewide accountability testing results always make headlines. Positive test score trends are interpreted as showing an improvement in the quality of public education, an increase in student learning, and evidence of educational policies functioning as intended. Under the mandates of PL 107-110, the federal No Child Left Behind Act of 2001 (NCLB), state accountability test scores have taken on significant consequences for schools, increasing the burden on the validity of test score interpretations. The high stakes for state test scores can be contrasted with the relatively low stakes for results from the National Assessment of Educational Progress (NAEP), which is designed to evaluate the conditions and progress of education at the state and national levels. It would be reassuring if state and NAEP results showed similar trends for the same subjects, grades and years. In fact, as the NCLB legislation was being drafted, there was much discussion of a possible \u201cconfirming\u201d role for NAEP vis \u00e0 vis state tests required under the new legislation. Even though there was no explicit provision for comparing gains and gaps on state assessments versus NAEP, the law did mandate NAEP participation for sampled districts as a condition for receipt of Title I funds. The Department of Education's Fact Sheet on the new legislation stated,\u201cUnder HR 1 [the No Child Left Behind Act] a small sample of students in each state will participate in the fourth-and eighth-grade National Assessment of Educational Progress (NAEP) in reading and math every other year in order to help the US Department of Education verify the results of statewide assessments required under\u00a0\u2026", "num_citations": "8\n", "authors": ["1043"]}
{"title": "Improving measurement efficiency of the Inner EAR scale with item response theory\n", "abstract": " Objectives(1) To assess the 11-item Inner Effectiveness of Auditory Rehabilitation (Inner EAR) instrument with item response theory (IRT). (2) To determine whether the underlying latent ability could also be accurately represented by a subset of the items for use in high-volume clinical scenarios. (3) To determine whether the Inner EAR instrument correlates with pure tone thresholds and word recognition scores.DesignIRT evaluation of prospective cohort data.SettingTertiary care academic ambulatory otolaryngology clinic.Subjects and MethodsModern psychometric methods, including factor analysis and IRT, were used to assess unidimensionality and item properties. Regression methods were used to assess prediction of word recognition and pure tone audiometry scores.ResultsThe Inner EAR scale is unidimensional, and items varied in their location and information. Information parameter estimates ranged from\u00a0\u2026", "num_citations": "7\n", "authors": ["1043"]}
{"title": "ER22x: JusticeX-Spring 2013 Course Report\n", "abstract": " ER22x was offered as a HarvardX course in Spring 2013 on edX, a platform for massive open online courses (MOOCs). It was taught by Professor Michael Sandel. The report was prepared by researchers external to the course team, based on an examination of the courseware, analyses of data collected by the edX platform, and interviews with the course faculty and team members.", "num_citations": "7\n", "authors": ["1043"]}
{"title": "Supporting growth interpretations using through-course assessments\n", "abstract": " One of the promises of the through-course assessment model is the support of inferences about student growth, both over the course of the academic year and toward career and college readiness in the future. The distinguishing features of through-course assessments, particularly the increased number of assessment time points and the relevance of these time points to the curriculum, are particularly well suited to support student growth inferences.", "num_citations": "7\n", "authors": ["1043"]}
{"title": "Fairness using derived scores\n", "abstract": " In testing, performances are elicited under standardized conditions, and one or more scores are then derived from those performances. Different scores often correspond to distinct constructs. One familiar case is the use of two or more scoring rubrics to evaluate several dimensions of the same performance, as when an essay is scored both for content and for grammatical usage. Another example is the reporting of both subtest and total scores, where the total score references a broader construct encompassing the more specific constructs referenced by the subtests. Sometimes, new scores may be created to measure constructs quite different from those envisioned when a test was first designed. In one early example, Jackson and Messick (1961) constructed new scales for the Minnesota Multiphasic Personality Inventory (MMPI) to measure response acquiescence and social desirability. They did so in part to study\u00a0\u2026", "num_citations": "6\n", "authors": ["1043"]}
{"title": "Closing the Gap? A Comparison of Changes over Time in White-Black and White-Hispanic Achievement Gaps on State Assessments versus State NAEP. CSE Report 721.\n", "abstract": " When a state test and National Assessment of Educational Progress (NAEP) are both measuring the same construct, the achievement gaps between subgroups on both tests should be the same. However, if a teacher or school engages in \u201cteaching to the test\u201d then student performance may improve on one test but not on another. We hypothesized that teaching to the test could have consequences for changes in achievement gaps over time because, for a variety of reasons, students in low-achieving schools or classrooms may be more likely to receive instruction narrowly focused on increasing their test scores. Our analysis proceeded by examining (at the state level) gaps between White students (the \u201creference\u201d group) and either Black or Hispanic students (a \u201cfocal\u201d group). The clearest conclusion from our state-by-state analyses of state and NAEP test data is that the pattern of gap changes varies widely both between and within states. Further, gap changes came in a variety of forms, and not all types of gap reduction are equally desirable.", "num_citations": "6\n", "authors": ["1043"]}
{"title": "PH207x: Health in numbers & PH278x: Human health and global environmental change-2012-2013 course report\n", "abstract": " In the 2012-2013 academic year, the first two Harvard School of Public Health courses were offered through HarvardX on the edX platform: PH207x: Health in Numbers and PH278x: Human Health and Global Environmental Change. They were taught by Professors Earl Francis Cook and Marcello Pagano, and Aaron Bernstein and Jack Spengler, respectively. This report describes the structure of these two courses, the demographic characteristics of registrants, and the activity of students. This report was prepared by researchers external to the course teams and is based on examination of the courseware, analyses of the data collected by the edX platform, and interviews and consultations with the course faculty and team members.", "num_citations": "5\n", "authors": ["1043"]}
{"title": "Learning from Recent Advances in Measuring Teacher Effectiveness (Washington, DC, August 9, 2012).\n", "abstract": " BackgroundRecent policy initiatives including Race to the Top and the ESEA waiver program have rapidly advanced the use of large-scale assessment results in analyses directed toward the evaluation of teacher quality. The vast majority of research conducted on the use of large-scale assessment outcomes for teacher evaluation has focused on the technical characteristics of the indicators (often referred to as teacher \u201cvalue-added\u201d effects). Though of great importance, the technical characteristics of value-added/student growth statistics goes only part way toward addressing larger issues associated with the development and implementation of complex indicator systems. The trajectory in the development of value-added/student growth and its use for teacher evaluation is not unlike the early development of large-scale assessments where technical considerations dominated the discussions about the tests and the ultimate (ab) use of the results was not as well mapped out in advance.", "num_citations": "5\n", "authors": ["1043"]}
{"title": "Estimating trends from censored assessment data under No Child Left Behind\n", "abstract": " Under the No Child Left Behind Act, large-scale test score trend analyses are widespread. These analyses often gloss over interesting changes in test score distributions and involve unrealistic assumptions. Further complications arise from analyses of unanchored, censored assessment data, or proportions of students lying within performance levels defined by unspecified cut scores. This article introduces \u2018\u2018shift models,\u2019\u2019 particularly the \u2018\u2018normal-shift\u2019\u2019 model, to summarize the limited information available in censored data and to support distribution-wide trend analyses. A simulation study exploring this model\u2019s estimation procedure\u2014an expectation-maximization algorithm for maximum likelihood estimates (MLEs) of normally distributed censored data\u2014found that the MLEs exhibit little to no bias over a range of sample sizes and cut scores. The normal-shift model was applied to two full state data sets and performed\u00a0\u2026", "num_citations": "5\n", "authors": ["1043"]}
{"title": "Non-parametric Comparisons of High-Stakes and Low-Stakes Trends: 2003-2007\n", "abstract": " The inflation of high-stakes score trends is a major threat to the validity of No Child Left Behind (NCLB). A flourishing approach to the validation of NCLB score gains compares NCLB trends to corresponding \u201clow-stakes\u201d results such as those from the National Assessment of Educational Progress (NAEP). However, statistics chosen for these comparisons frequently confound trend comparison with choices of cut scores or score scales. A nonparametric framework that overcomes these shortcomings is used to compare State and NAEP trends from 2003-07 in reading and mathematics for 4th and 8th grade. The assumptions underlying cross-test comparisons are discussed to offer possible explanations of State-NAEP trend discrepancies.In an effort to increase the academic achievement of all students and confront the \u201csoft bigotry of low expectations\u201d(Bush, 2000), the No Child Left Behind (NCLB) Act was passed into law on January 8, 2002. Like the Improving America\u2019s Schools Act (IASA) signed in 1994, NCLB required all states to develop content and performance standards; implement assessment systems to track student performance against those standards; and create adequate yearly progress (AYP) goals to ensure all students reach a proficient level of achievement (IASA, 1994; NCLB, 2001). Believing the IASA was ineffective in improving student achievement due to its status as \u201can undertaking without consequences,\u201d(Rotherham, 1999) NCLB granted the federal government the authority to impose sanctions upon states failing to meet AYP goals. The sanctions were intended to provide an incentive for educators to improve the quality of\u00a0\u2026", "num_citations": "5\n", "authors": ["1043"]}
{"title": "Measuring the quality of teaching practices in primary schools: assessing the validity of the Teach observation tool in Punjab, Pakistan\n", "abstract": " Monitoring the quality of teaching practices of primary school teachers in low-and-middle-income countries is often hampered by the lack of freely available classroom observation tools that are feasible to administer, validated in their own setting, and can be used as part of national monitoring systems. To address this discrepancy, Teach, an open-access classroom observation tool, was developed to measure the quality of teaching practices of primary school teachers in low-and-middle-income countries. This paper uses data from Punjab, Pakistan to evaluate the validity of Teach. Results show that Teach scores were internally consistent, presented good inter-rater reliability, and provided sufficient information to differentiate low from high-quality teaching practices. Further, higher Teach scores were associated with higher student outcomes.", "num_citations": "4\n", "authors": ["1043"]}
{"title": "Simple Choices among Aggregate-Level Conditional Status Metrics: From Median Student Growth Percentiles to Value-Added Models\n", "abstract": " Abstract Aggregate-Level Conditional Status Metrics (ACSMs) describe the status of a group by referencing current performance to expectations given past scores. This paper focuses on seven ACSMs that condition only on past scores, including median Student Growth Percentiles (Betebenner, 2009), aggregated Percentile Ranks of Residuals (Castellano & Ho, 2012) and covariate-adjustment \u201cvalue-added\u201d models (eg, McCaffrey et al., 2004). The authors define \u201csimple\u201d choices among these ACSMs (eg, between mean-and median-based aggregation functions, or between fixed and random effects). They evaluate the impact of these choices upon three practical factors: school rankings, robustness across inclusion of prior-year data, and invariance to nonlinear scale transformations. Findings include expressions of practical differences between ACSMs and suggest considerable advantages of mean-based over medianbased metrics.", "num_citations": "4\n", "authors": ["1043"]}
{"title": "Comparing score trends on high-stakes and low-stakes tests using metric-free statistics and multidimensional item response models\n", "abstract": " The most widely interpreted large-scale educational statistic is the test score trend. Positive trends are interpreted as an improvement in the education of students, as an increase in student learning, and as evidence of educational policies functioning as intended. An implicit assumption of this attention to test score trends is that they can be generalized to trends for other tests that measure the \u201csame\u201d desired learning outcomes. However, comparing trends across testing programs is not straightforward, nor are discrepancies readily interpretable when they are found.", "num_citations": "4\n", "authors": ["1043"]}
{"title": "Stanford education data archive.\u201d\n", "abstract": " The Stanford Education Data Archive (SEDA; seda. stanford. edu) 1 is an initiative aimed at harnessing data to help scholars, policymakers, educators, and parents learn how to improve educational opportunity for all children. SEDA includes a range of detailed data on educational conditions, contexts, and outcomes in school districts and counties across the United States. It includes measures of academic achievement and achievement gaps for school districts and counties, as well as district-level measures of racial and socioeconomic composition, racial and socioeconomic segregation patterns, and other features of schooling systems.By making the data files available to the public, we hope that anyone who is interested can obtain detailed information about American schools, communities, and student success. We hope that researchers will use these data to generate evidence about what policies and contexts are most effective at increasing educational opportunity, and that such evidence will inform educational policy and practices.", "num_citations": "3\n", "authors": ["1043"]}
{"title": "Discreteness causes bias in percentage-based comparisons: A case study from educational testing\n", "abstract": " Discretizing continuous distributions can lead to bias in parameter estimates. We present a case study from educational testing that illustrates dramatic consequences of discreteness when discretizing partitions differ across distributions. The percentage of test takers who score above a certain cutoff score (percent above cutoff, or \u201cPAC\u201d) often describes overall performance on a test. Year-over-year changes in PAC, or \u0394PAC, have gained prominence under recent U.S. education policies, with public schools facing sanctions if they fail to meet PAC targets. In this article, we describe how test score distributions act as continuous distributions that are discretized inconsistently over time. We show that this can propagate considerable bias to PAC trends, where positive \u0394PACs appear negative, and vice versa, for a substantial number of actual tests. A simple model shows that this bias applies to any comparison of PAC\u00a0\u2026", "num_citations": "3\n", "authors": ["1043"]}
{"title": "6.00 x Introduction to Computer Science and Programming MITx on edX Course Report-2013 Spring\n", "abstract": " This report describes 6.00 x: Introduction to Computer Science and Programming, one of the first 11 courses offered by MITx on edX, a platform for delivering massive open online courses (MOOCs). In the Spring of 2013, 6.00 x was rereleased as an MITx on edX course, with content largely mirroring the previous offering (Seaton et al, MITx Working Paper# 3). 6.00 x again covered an introduction to using computation to solve real problems, following the curriculum of a first-year course within the Department of Electrical Engineering and Computer Science. This report describes the course structure, in terms of the number of basic e-text, auto-graded problems, and video components. Following a methodology established for analysis of the first 17 HarvardX and MITx courses (Ho et al, HarvardX and MITx: The first year of open online courses), course registrants are described in terms of viewed, explored, and certified sub-populations, together with demographics. The diversity of student activity in the course, and the persistence of student interactions with the courseware, are addressed. The report concludes with a general comparison of features across the original (Fall 2012) and current offerings of 6.00 x.", "num_citations": "3\n", "authors": ["1043"]}
{"title": "Sa1203 The development of e-Health tools for the management of inflammatory bowel diseases\n", "abstract": " Sa1203 The Development of e-Health Tools for the Management of Inflammatory Bowel Diseases \u00d7 Close The Infona portal uses cookies, ie strings of text saved by a browser on the user's device. The portal can access those files and use them to remember the user's data, such as their chosen settings (screen view, interface language, etc.), or their login data. By using the Infona portal the user accepts automatic saving and using this information for portal operation purposes. More information on the subject can be found in the Privacy Policy and Terms of Service. By closing this window the user confirms that they have read the information on cookie usage, and they accept the privacy policy and the way cookies are used by the portal. You can change the cookie settings in your browser. I accept Polski English Login or register account remember me Password recovery INFONA - science communication portal INFONA \u2026", "num_citations": "3\n", "authors": ["1043"]}
{"title": "6.002 x Circuits and Electronics MITx on edX course report-2013 spring\n", "abstract": " This report describes 6.002 x: Circuits and Electronics, one of the first 11 courses offered by MITx on edX, a platform for delivering massive open online courses (MOOCs). In the Spring of 2013, 6.002 x was released as an open online course for the third time, with content largely mirroring previous offerings (Seaton et al, MITx Working Paper# 4). 6.002 x covered a first course within an undergraduate electrical engineering or electrical engineering and computer science curriculum. This report describes the course structure, in terms of the number of basic e-text, auto-graded problems, and video components. Following a methodology established for analysis of the first 17 HarvardX and MITx courses (Ho et al, HarvardX and MITx: The first year of open online courses), course registrants are described in terms of viewed, explored, and certified sub-populations, together with demographics. The diversity of student activity in the course, and the persistence of student interactions with the courseware, are addressed. The report concludes with a general comparison of features across the second iteration (Fall 2012) and current offering of 6.002 x.", "num_citations": "3\n", "authors": ["1043"]}
{"title": "How Can Released State Test Items Support Interim Assessment Purposes in an Educational Crisis?\n", "abstract": " State testing programs regularly release previously administered test items to the public. We provide an open\u2010source recipe for state, district, and school assessment coordinators to combine these items flexibly to produce scores linked to established state score scales. These would enable estimation of student score distributions and achievement levels. We discuss how educators can use resulting scores to estimate achievement distributions at the classroom and school level. We emphasize that any use of such tests should be tertiary, with no stakes for students, educators, and schools, particularly in the context of a crisis like the COVID\u201019 pandemic. These tests and their results should also be lower in priority than assessments of physical, mental, and social\u2013emotional health, and lower in priority than classroom and district assessments that may already be in place. We encourage state testing programs to\u00a0\u2026", "num_citations": "2\n", "authors": ["1043"]}
{"title": "DOP090 Results from a feasibility study with the telemedicine tool myIBDcoach in the Netherlands\n", "abstract": " DOP090 Results from a feasibility study with the telemedicine tool myIBDcoach in the Netherlands \u00d7 Close The Infona portal uses cookies, ie strings of text saved by a browser on the user's device. The portal can access those files and use them to remember the user's data, such as their chosen settings (screen view, interface language, etc.), or their login data. By using the Infona portal the user accepts automatic saving and using this information for portal operation purposes. More information on the subject can be found in the Privacy Policy and Terms of Service. By closing this window the user confirms that they have read the information on cookie usage, and they accept the privacy policy and the way cookies are used by the portal. You can change the cookie settings in your browser. I accept Polski English Login or register account remember me Password recovery INFONA - science communication portal INFONA \u2026", "num_citations": "2\n", "authors": ["1043"]}
{"title": "Accuracy, Transparency, and Incentives: Contrasting Criteria for Evaluating Growth Models.\n", "abstract": " Terms of Use This article was downloaded from Harvard University\u2019s DASH repository, and is made available under the terms and conditions applicable to Other Posted Material, as set forth at http://nrs. harvard. edu/urn-3: HUL. InstRepos: dash. current. terms-ofuse# LAA", "num_citations": "2\n", "authors": ["1043"]}
{"title": "T1459 Swallow Induces a Peristaltic Wave of Inhibition That Marches in Front of the Peristaltic Wave of Contraction Along the Length of the Esophagus in Normal Subjects\n", "abstract": " T1459 Swallow Induces a Peristaltic Wave of Inhibition That Marches in Front of the Peristaltic Wave of Contraction Along the Length of the Esophagus in Normal Subjects \u00d7 Close The Infona portal uses cookies, ie strings of text saved by a browser on the user's device. The portal can access those files and use them to remember the user's data, such as their chosen settings (screen view, interface language, etc.), or their login data. By using the Infona portal the user accepts automatic saving and using this information for portal operation purposes. More information on the subject can be found in the Privacy Policy and Terms of Service. By closing this window the user confirms that they have read the information on cookie usage, and they accept the privacy policy and the way cookies are used by the portal. You can change the cookie settings in your browser. I accept Polski English Login or register account remember \u2026", "num_citations": "2\n", "authors": ["1043"]}
{"title": "Closing the gap? A comparison of changes over time in white-black and white-Hispanic achievement gaps on state assessments versus state NAEP\n", "abstract": " When a state test and National Assessment of Educational Progress (NAEP) are both measuring the same construct, the achievement gaps between subgroups on both tests should be the same. However, if a teacher or school engages in \u201cteaching to the test\u201d then student performance may improve on one test but not on another. We hypothesized that teaching to the test could have consequences for changes in achievement gaps over time because, for a variety of reasons, students in low-achieving schools or classrooms may be more likely to receive instruction narrowly focused on increasing their test scores. Our analysis proceeded by examining (at the state level) gaps between White students (the \u201creference\u201d group) and either Black or Hispanic students (a \u201cfocal\u201d group). The clearest conclusion from our state-by-state analyses of state and NAEP test data is that the pattern of gap changes varies widely both between and within states. Further, gap changes came in a variety of forms, and not all types of gap reduction are equally desirable.", "num_citations": "2\n", "authors": ["1043"]}
{"title": "Stanford Education Data Archive Technical Documentation Version 4.1 June 2021\n", "abstract": " The Stanford Education Data Archive (SEDA) is part of the Educational Opportunity Project at Stanford University (https:\\\\edopportunity. org), an initiative aimed at harnessing data to help scholars, policymakers, educators, and parents learn how to improve educational opportunities for all children. SEDA includes a range of detailed data on educational conditions, contexts, and outcomes in schools, school districts, counties, commuting zones, and metropolitan statistical areas across the United States. Available measures differ by aggregation; see Sections IA and IB for a complete list of files and data. By making the data files available to the public, we hope that anyone who is interested can obtain detailed information about US schools, communities, and student success. We hope that researchers will use these data to generate evidence about what policies and contexts are most effective at increasing educational opportunity, and that such evidence will inform educational policy and practices.The construction of SEDA has been supported by grants from the Institute of Education Sciences, the Spencer Foundation, the William T. Grant Foundation, the Bill and Melinda Gates Foundation, the Overdeck Family Foundation, and by a visiting scholar fellowship from the Russell Sage Foundation. Some of the data used in constructing the SEDA files were provided by the National Center for Education Statistics (NCES). The findings and opinions expressed in the research and reported here are those of the authors alone; they do not represent the views of the US Department of Education, NCES, or any of the aforementioned funding agencies.", "num_citations": "1\n", "authors": ["1043"]}
{"title": "Measuring Teaching Practices at Scale\n", "abstract": " What goes on inside the classroom is             central to student learning. Despite its importance, low-             and middle-income countries rarely measure teaching             practices, in part due to a lack of access to adequate             classroom observation tools and the high transaction costs             associated with administering them. Teach, a new,             open-source classroom observation tool for primary             classrooms, was developed to capture the quantity and             quality of teaching practices in these settings with a             simple, easy-to-administer tool. This paper validates the             use of Teach scores for system diagnostics by providing four             types of evidence. First, it provides evidence that the             practices included in the tool have a clear conceptual             underpinning. Second, almost 90 percent of local observers             in Mozambique, Pakistan, the Philippines, and Uruguay were             highly accurate using Teach after a four-day training.             Third, using data from 845 classrooms in Pakistan, the paper             shows that Teach scores are internally consistent, present             moderate to high inter-rater reliability in the field (.75             intraclass correlation coefficient), and provide substantial             information that allows to differentiate teachers, even             those with similar but not equal scores. Finally, teachers             who display effective practices, as measured by Teach, are             associated with students who achieve higher learning outcomes.", "num_citations": "1\n", "authors": ["1043"]}
{"title": "The New (Educational) Statistics: Properties of Scales That Matter\n", "abstract": " David Thissen\u2019s essay, Bad Questions: An Essay Involving Item Response Theory (2016), is an excellent contribution to the genre of commentaries on the field. It joins the likes of the piece by Thissen\u2019s frequent collaborator, Howard Wainer (2010), who published 14 conversations about three things in this journal 6 years ago. Thissen asks and answers, dismissively, five of his titular \u2018\u2018bad questions.\u2019\u2019He concludes that what makes them bad \u2018\u2018is the framing of the question that demands a yes-or-no, black and white, cut and dried response\u2019\u2019(p. 10). He argues for a statistical education that values continua over dichotomies and categories, and I agree. However, I think Thissen, as well as scholars and students of statistics and measurement generally, underappreciate the utility and necessity of dichotomies by decision makers. Ultimately, I believe we can and should inform their dichotomies with meaningful scales and\u00a0\u2026", "num_citations": "1\n", "authors": ["1043"]}
{"title": "8.02 x Electricity and Magnetism MITx on edX Course Report-2013 Spring\n", "abstract": " This report describes 8.02x: Electricity and Magnetism, one of the first 11 courses offered by MITx on edX, a platform for delivering massive open online courses (MOOCs). Offered in the Spring of 2013, 8.02x covers concepts in electromagnetism, and follows the second course in the MIT on-campus introductory physics sequence. This report describes the course structure, in terms of the number of basic e-text, auto-graded problems, and video components. Following a methodology established for analysis of the first 17 HarvardX and MITx courses (Ho et al, HarvardX and MITx: The first year of open online courses), course registrants are described in terms of viewed, explored, and certified sub-populations, together with demographics. The diversity of student activity in the course, and the persistence of student interactions with the courseware, are addressed. The report concludes with descriptions of special features in 8.02x, including the integration of animations and interactive simulations originally developed for MIT\u2019s Technology Enabled Active Learning (TEAL) classroom.", "num_citations": "1\n", "authors": ["1043"]}
{"title": "7.00 x Introduction to biology: The secret of life MITx on edX course report-2013 Spring\n", "abstract": " This report describes 7.00 x: Introduction to Biology-The Secret of Life, one of the first 11 courses offered by MITx on edX, a platform for delivering massive open online courses (MOOCs). Offered in the Spring of 2013, the content of 7.00 x reflects the topics taught in introductory biology courses at MIT and many biology courses around the world. This report describes the course structure, in terms of the number of basic e-text, auto-graded problems, and video components. Following a methodology established for analysis of the first 17 HarvardX and MITx courses (Ho et al, HarvardX and MITx: The first year of open online courses), course registrants are described in terms of viewed, explored, and certified sub-populations, together with demographics. The diversity of student activity in the course, and the persistence of student interactions with the courseware, are addressed. The report concludes with descriptions of special features in 7.00 x, including descriptions of a variety of new interactive problem types aimed at introductory biology.", "num_citations": "1\n", "authors": ["1043"]}
{"title": "8. mrev mechanics review mitx on edx course report-2013 spring\n", "abstract": " This report describes 8. MReV: Mechanics ReView, one of the first 11 courses offered by MITx on edX, a platform for delivering massive open online courses (MOOCs). Offered in the Summer of 2013, 8. MReV offers a second look at introductory Newtonian Mechanics, incorporating research pedagogy developed by the RELATE education group at MIT. This report describes the course structure, in terms of the number of basic e-text, auto-graded problems, and video components. Following a methodology established for analysis of the first 17 HarvardX and MITx courses (Ho et al, HarvardX and MITx: The first year of open online courses), course registrants are described in terms of viewed, explored, and certified sub-populations, together with demographics. The diversity of student activity in the course, and the persistence of student interactions with the courseware, are addressed. The report concludes with descriptions of special features in 8. MReV, including innovative pedagogy and pre-post testing.", "num_citations": "1\n", "authors": ["1043"]}
{"title": "6.00 x Introduction to Computer Science and Programming MITx on edX Course Report-2012 Fall\n", "abstract": " This report describes 6.00 x: Introduction to Computer Science and Programming, one of the first 11 courses offered by MITx on edX, a platform for delivering massive open online courses (MOOCs). Offered in the Fall of 2012, 6.00 x is an introduction to using computation to solve real problems, aiming to instruct students with a variety of backgrounds and interests. This report describes the course structure, in terms of the number of basic e-text, auto-graded problems, and video components. Following a methodology established for analysis of the first 17 HarvardX and MITx courses (Ho et al, HarvardX and MITx: The first year of open online courses), course registrants are described in terms of viewed, explored, and certified sub-populations, together with demographics. The diversity of student activity in the course, and the persistence of student interactions with the courseware, are addressed. The report concludes with descriptions of special features in 6.00 x, with particular emphasis on assessment questions involving student submitted Python code.", "num_citations": "1\n", "authors": ["1043"]}
{"title": "Pregnant women and their willingness to be treated for hepatitis B during pregnancy\n", "abstract": " Pregnant Women and Their Willingness to Be Treated for Hepatitis B During Pregnancy \u00d7 Close The Infona portal uses cookies, ie strings of text saved by a browser on the user's device. The portal can access those files and use them to remember the user's data, such as their chosen settings (screen view, interface language, etc.), or their login data. By using the Infona portal the user accepts automatic saving and using this information for portal operation purposes. More information on the subject can be found in the Privacy Policy and Terms of Service. By closing this window the user confirms that they have read the information on cookie usage, and they accept the privacy policy and the way cookies are used by the portal. You can change the cookie settings in your browser. I accept Polski English Login or register account remember me Password recovery INFONA - science communication portal INFONA Search \u2026", "num_citations": "1\n", "authors": ["1043"]}
{"title": "The scope and dimensions of US civil rights and civil liberties organizations at the beginning of the 21st century\n", "abstract": " At the dawn of the 21 st Century, civil rights and civil liberties organizations in the", "num_citations": "1\n", "authors": ["1043"]}