{"title": "Coverage-based greybox fuzzing as markov chain\n", "abstract": " Coverage-based Greybox Fuzzing (CGF) is a random testing approach that requires no program analysis. A new test is generated by slightly mutating a seed input. If the test exercises a new and interesting path, it is added to the set of seeds; otherwise, it is discarded. We observe that most tests exercise the same few \u201chigh-frequency\u201d paths and develop strategies to explore significantly more paths with the same number of tests by gravitating towards low-frequency paths. We explain the challenges and opportunities of CGF using a Markov chain model which specifies the probability that fuzzing the seed that exercises path i generates an input that exercises path j. Each state (i.e., seed) has an energy that specifies the number of inputs to be generated from that seed. We show that CGF is considerably more efficient if energy is inversely proportional to the density of the stationary distribution and increases\u00a0\u2026", "num_citations": "505\n", "authors": ["272"]}
{"title": "Directed greybox fuzzing\n", "abstract": " Existing Greybox Fuzzers (GF) cannot be effectively directed, for instance, towards problematic changes or patches, towards critical system calls or dangerous locations, or towards functions in the stack-trace of a reported vulnerability that we wish to reproduce. In this paper, we introduce Directed Greybox Fuzzing (DGF) which generates inputs with the objective of reaching a given set of target program locations efficiently. We develop and evaluate a simulated annealing-based power schedule that gradually assigns more energy to seeds that are closer to the target locations while reducing energy for seeds that are further away. Experiments with our implementation AFLGo demonstrate that DGF outperforms both directed symbolic-execution-based whitebox fuzzing and undirected greybox fuzzing. We show applications of DGF to patch testing and crash reproduction, and discuss the integration of AFLGo into\u00a0\u2026", "num_citations": "345\n", "authors": ["272"]}
{"title": "Chronos: A timing analyzer for embedded software\n", "abstract": " Abstract Estimating the Worst Case Execution Time (WCET) of real-time embedded software is an important problem. WCET is defined as the upper bound b on the execution time of a program P on a processor X such that for any input the execution time of P on X is guaranteed to not exceed b. Such WCET estimates are crucial for schedulability analysis of real-time systems. In this paper, we present Chronos, a static analysis tool for generating WCET estimates of C programs. It performs detailed micro-architectural modeling to capture the timing effects of the underlying processor platform. Consequently, we can provide safe but tight WCET estimate of a given C program running on a complex modern processor. Chronos is an open-source distribution specifically suited to the needs of the research community. We support processor models captured by the popular SimpleScalar architectural simulator rather than\u00a0\u2026", "num_citations": "288\n", "authors": ["272"]}
{"title": "Detecting energy bugs and hotspots in mobile apps\n", "abstract": " Over the recent years, the popularity of smartphones has increased dramatically. This has lead to a widespread availability of smartphone applications. Since smartphones operate on a limited amount of battery power, it is important to develop tools and techniques that aid in energy-efficient application development. Energy inefficiencies in smartphone applications can broadly be categorized into energy hotspots and energy bugs. An energy hotspot can be described as a scenario where executing an application causes the smartphone to consume abnormally high amount of battery power, even though the utilization of its hardware resources is low. In contrast, an energy bug can be described as a scenario where a malfunctioning application prevents the smartphone from becoming idle, even after it has completed execution and there is no user activity. In this paper, we present an automated test generation\u00a0\u2026", "num_citations": "219\n", "authors": ["272"]}
{"title": "WCET centric data allocation to scratchpad memory\n", "abstract": " Scratchpad memory is a popular choice for on-chip storage in real-time embedded systems. The allocation of code/data to scratchpad memory is performed at compile time leading to predictable memory access latencies. Current scratchpad memory allocation techniques improve the average-case execution time of tasks. For hard real-time systems, on the other hand, worst case execution time (WCET) is a key metric. In this paper, we propose scratchpad allocation techniques for data memory that aim to minimize a task's WCET. We first develop an integer linear programming (ILP) based solution which constructs the optimal allocation assuming that all program paths are feasible. Next, we employ branch-and-bound search to more accurately construct the optimal allocation by exploiting infeasible path information. However, the branch-and-bound search is too time-consuming in practice. Therefore, we design fast\u00a0\u2026", "num_citations": "213\n", "authors": ["272"]}
{"title": "Timing analysis of concurrent programs running on shared cache multi-cores\n", "abstract": " Memory accesses form an important source of timing unpredictability. Timing analysis of real-time embedded software thus requires bounding the time for memory accesses. Multiprocessing, a popular approach for performance enhancement, opens up the opportunity for concurrent execution. However due to contention for any shared memory by different processing cores, memory access behavior becomes more unpredictable, and hence harder to analyze. In this paper, we develop a timing analysis method for concurrent software running on multi-cores with a shared instruction cache. Communication across tasks is by message passing where the message mailboxes are accessed via interrupt service routines. We do not handle data cache, shared memory synchronization and code sharing across tasks. Our method progressively improves the lifetime estimates of tasks that execute concurrently on multiple cores\u00a0\u2026", "num_citations": "177\n", "authors": ["272"]}
{"title": "Accurate estimation of cache-related preemption delay\n", "abstract": " Multitasked real-time systems often employ caches to boost performance. However the unpredictable dynamic behavior of caches makes schedulability analysis of such systems difficult. In particular, the effect of caches needs to be considered for estimating the inter-task interference. As the memory blocks of different tasks can map to the same cache blocks, preemption of a task may introduce additional cache misses. The time penalty introduced by these misses is called the Cache-Related Preemption Delay (CRPD). In this paper, we provide a program path analysis technique to estimate CRPD. Our technique performs path analysis of both the preempted and the preempting tasks. Furthermore, we improve the accuracy of the analysis by estimating the possible states of the entire cache at each possible preemption point rather than estimating the states of each cache block independently. To avoid incurring high\u00a0\u2026", "num_citations": "157\n", "authors": ["272"]}
{"title": "A unified WCET analysis framework for multicore platforms\n", "abstract": " With the advent of multicore architectures, worst-case execution time (WCET) analysis has become an increasingly difficult problem. In this article, we propose a unified WCET analysis framework for multicore processors featuring both shared cache and shared bus. Compared to other previous works, our work differs by modeling the interaction of shared cache and shared bus with other basic microarchitectural components (e.g., pipeline and branch predictor). In addition, our framework does not assume a timing anomaly free multicore architecture for computing the WCET. A detailed experiment methodology suggests that we can obtain reasonably tight WCET estimates in a wide range of benchmark programs.", "num_citations": "151\n", "authors": ["272"]}
{"title": "Modeling shared cache and bus in multi-cores for timing analysis\n", "abstract": " Timing analysis of concurrent programs running on multi-core platforms is currently an important problem. The key to solving this problem is to accurately model the timing effects of shared resources in multi-cores, namely shared cache and bus. In this paper, we provide an integrated timing analysis framework that captures timing effects of both shared cache and shared bus. We also develop a cycle-accurate simulation infra-structure to evaluate the precision of our analysis. Experimental results from a large fragment of an in-orbit spacecraft software show that our analysis produces around 20% over-estimation over simulation results.", "num_citations": "132\n", "authors": ["272"]}
{"title": "relifix: Automated Repair of Software Regressions\n", "abstract": " Regression occurs when code changes introduce failures in previously passing test cases. As software evolves, regressions may be introduced. Fixing regression errors manually is time-consuming and error-prone. We propose an approach of automated repair of software regressions, called relifix, that considers the regression repair problem as a problem of reconciling problematic changes. Specifically, we derive a set of code transformations obtained from our manual inspection of 73 real software regressions; this set of code transformations uses syntactical information from changed statements. Regression repair is then accomplished via a search over the code transformation operators - which operator to apply, and where. Our evaluation compares the repairability of relifix with GenProg on 35 real regression errors. relifix repairs 23 bugs, while GenProg only fixes five bugs. We also measure the likelihood of\u00a0\u2026", "num_citations": "128\n", "authors": ["272"]}
{"title": "Using compressed bytecode traces for slicing Java programs\n", "abstract": " Dynamic slicing is a well-known program debugging technique. Given a program P and input I, it finds all program statements which directly/indirectly affect the values of some variables' occurrences when P is executed with I. Dynamic slicing algorithms often proceed by traversing the execution trace of P produced by input I (or a dependence graph which captures control/data flow in the execution trace). Consequently, it is important to develop space efficient representations of the execution trace. In this paper, we use results from data compression to compactly represent bytecode traces of sequential Java programs. The major space savings come from the optimized representation of data (instruction) addresses used by memory reference (branch) bytecodes as operands. We give detailed experimental results on the space efficiency and time overheads for our compact trace representation. We then show how\u00a0\u2026", "num_citations": "128\n", "authors": ["272"]}
{"title": "Bus-aware multicore WCET analysis through TDMA offset bounds\n", "abstract": " In the domain of real-time systems, the analysis of the timing behavior of programs is crucial for guaranteeing the schedulability and thus the safeness of a system. Static analyses of the WCET (Worst-Case Execution Time) have proven to be a key element for timing analysis, as they provide safe upper bounds on a program's execution time. For single-core systems, industrial-strength WCET analyzers are already available, but up to now, only first proposals have been made to analyze the WCET in multicore systems, where the different cores may interfere during the access to shared resources. An important example for this are shared buses which connect the cores to a shared main memory. The time to gain access to the shared bus may vary significantly, depending on the used bus arbitration protocol and the access timings. In this paper, we propose a new technique for analyzing the duration of accesses to\u00a0\u2026", "num_citations": "114\n", "authors": ["272"]}
{"title": "Modeling out-of-order processors for WCET analysis\n", "abstract": " Estimating the Worst Case Execution Time (WCET) of a program on a given processor is important for the schedulability analysis of real-time systems. WCET analysis techniques typically model the timing effects of micro-architectural features in modern processors (such as pipeline, cache, branch prediction) to obtain safe and tight estimates. In this paper, we model out-of-order superscalar processor pipelines for WCET analysis. The analysis is, in general, difficult even for a basic block (a sequence of instructions with single-entry and single-exit points) if some of the instructions have variable latencies. This is because the WCET of a basic block on out-of-order pipelines cannot be obtained by assuming maximum latencies of the individual instructions. Our timing estimation technique for a basic block proceeds by a fixed-point analysis of the time intervals at which the instructions enter/leave a pipeline stage\u00a0\u2026", "num_citations": "112\n", "authors": ["272"]}
{"title": "Scope-aware data cache analysis for WCET estimation\n", "abstract": " Caches are widely used in modern computer systems to bridge the increasing gap between processor speed and memory access time. On the other hand, presence of caches, especially data caches, complicates the static worst case execution time (WCET) analysis. Access pattern analysis (e.g., cache miss equations) are applicable to only a specific class of programs, where all array accesses must have predictable access patterns. Abstract interpretation-based methods (must/persistence analysis) determines possible cache conflicts based on coarse-grained memory access information from address analysis, which usually leads to significantly pessimistic estimation. In this paper, we first present a refined persistence analysis method which fixes the potential underestimation problem in the original persistence analysis. Based on our new persistence analysis, we propose a framework to combine access pattern\u00a0\u2026", "num_citations": "111\n", "authors": ["272"]}
{"title": "Using formal techniques to debug the AMBA system-on-chip bus protocol\n", "abstract": " System-on-chip (SoC) designs use bus protocols for high performance data transfer among the intellectual property (IP) cores. These protocols incorporate advanced features such as pipelining, burst and split transfers. In this paper, we describe a case study in formally verifying a widely used SoC bus protocol: the advanced micro-controller bus architecture (AMBA) protocol from ARM. In particular, we develop a formal specification of the AMBA protocol. We then employ model checking, a state space exploration based formal verification technique, to verify crucial design invariants. The presence of pipelining and split transfer in the AMBA protocol gives rise to interesting corner cases, which are hard to detect via informal reasoning. Using the SMV model checker, we have detected a potential bus starvation scenario in the AMBA protocol. Such scenarios demonstrate the inherent intricacies in designing pipelined\u00a0\u2026", "num_citations": "101\n", "authors": ["272"]}
{"title": "Smart greybox fuzzing\n", "abstract": " Coverage-based greybox fuzzing (CGF) is one of the most successful approaches for automated vulnerability detection. Given a seed file (as a sequence of bits), a CGF randomly flips, deletes or copies some bits to generate new files. CGF iteratively constructs (and fuzzes) a seed corpus by retaining those generated files which enhance coverage. However, random bitflips are unlikely to produce valid files (or valid chunks in files), for applications processing complex file formats. In this work, we introduce smart greybox fuzzing (SGF) which leverages a high-level structural representation of the seed file to generate new files. We define innovative mutation operators that work on the virtual file structure rather than on the bit level which allows SGF to explore completely new input domains while maintaining file validity. We introduce a novel validity-based power schedule that enables SGF to spend more time generating\u00a0\u2026", "num_citations": "86\n", "authors": ["272"]}
{"title": "Corebench: Studying complexity of regression errors\n", "abstract": " Intuitively we know, some software errors are more complex than others. If the error can be fixed by changing one faulty statement, it is a simple error. The more substantial the fix must be, the more complex we consider the error.", "num_citations": "86\n", "authors": ["272"]}
{"title": "Model-based whitebox fuzzing for program binaries\n", "abstract": " Many real-world programs take highly structured and very complex inputs. The automated testing of such programs is non-trivial. If the test input does not adhere to a specific file format, the program returns a parser error. For symbolic execution-based whitebox fuzzing the corresponding error handling code becomes a significant time sink. Too much time is spent in the parser exploring too many paths leading to trivial parser errors. Naturally, the time is better spent exploring the functional part of the program where failure with valid input exposes deep and real bugs in the program. In this paper, we suggest to leverage information about the file format and the data chunks of existing, valid files to swiftly carry the exploration beyond the parser code. We call our approach Model-based Whitebox Fuzzing (MoWF) because the file format input model of blackbox fuzzers can be exploited as a constraint on the vast input\u00a0\u2026", "num_citations": "84\n", "authors": ["272"]}
{"title": "Timing analysis of a protected operating system kernel\n", "abstract": " Operating systems offering virtual memory and protected address spaces have been an elusive target of static worst-case execution time (WCET) analysis. This is due to a combination of size, unstructured code and tight coupling with hardware. As a result, hard real-time systems are usually developed without memory protection, perhaps utilizing a lightweight real-time executive to provide OS abstractions. This paper presents a WCET analysis of seL4, a third-generation micro kernel. seL4 is the world's first formally-verified operating-system kernel, featuring machine-checked correctness proofs of its complete functionality. This makes seL4 an ideal platform for security-critical systems. Adding temporal guarantees makes seL4 also a compelling platform for safety- and timing-critical systems. It creates a foundation for integrating hard real-time systems with less critical time-sharing components on the same processor\u00a0\u2026", "num_citations": "81\n", "authors": ["272"]}
{"title": "Automated path generation for software fault localization\n", "abstract": " Localizing the cause (s) of an observable error lies at the heart of program debugging. Fault localization often proceeds by comparing the failing program run with some\" successful\" run (a run which does not demonstrate the error). An issue here is to generate or choose a\" suitable\" successful run; this task is often left to the programmer. In this paper, we present an efficient technique where the construction of the successful run as well its comparison with the failing run is automated. Our method constructs a successful program run by toggling the outcomes of some conditional branch instances in the failing run. If such a successful run exists, program statements for these branches are returned as bug report. In our experiments with the Siemens benchmark suite, we found that the quality of our bug report compares well with those produced by existing fault localization approaches where the programmer manually\u00a0\u2026", "num_citations": "77\n", "authors": ["272"]}
{"title": "Efficient detection and exploitation of infeasible paths for software timing analysis\n", "abstract": " Accurate estimation of the worst-case execution time (WCET) of a program is important for real-time embedded software. Static WCET estimation involves program path analysis and architectural modeling. Path analysis is complex due to the inherent difficulty in detecting and exploiting infeasible paths in a program's control flow graph. In this paper, we propose an efficient method to exploit infeasible path information for WCET estimation without resorting to exhaustive path enumeration. We demonstrate the efficiency of our approach for some real-life control-intensive applications.", "num_citations": "74\n", "authors": ["272"]}
{"title": "Scratchpad allocation for concurrent embedded software\n", "abstract": " Software-controlled scratchpad memory is increasingly employed in embedded systems as it offers better timing predictability compared to caches. Previous scratchpad allocation algorithms typically consider single-process applications. But embedded applications are mostly multitasking with real-time constraints, where the scratchpad memory space has to be shared among interacting processes that may preempt each other. In this work, we develop a novel dynamic scratchpad allocation technique that takes these process interferences into account to improve the performance and predictability of the memory system. We model the application as a Message Sequence Chart (MSC) to best capture the interprocess interactions. Our goal is to optimize the Worst-Case Response Time (WCRT) of the application through runtime reloading of the scratchpad memory content at appropriate execution points. We propose\u00a0\u2026", "num_citations": "72\n", "authors": ["272"]}
{"title": "Memory model sensitive bytecode verification\n", "abstract": " Modern concurrent programming languages like C# and Java have a programming language level memory model, which captures the set of all allowed behaviors of programs on any implementation platform\u2014uni- or multi-processor. Such a memory model is typically weaker than Sequential Consistency and allows reordering of operations within a program thread. Therefore, programs verified correct by assuming Sequential Consistency (that is, each thread proceeds in program order) may not behave correctly on certain platforms! One solution to this problem is to develop program checkers which are memory model sensitive. In this paper, we develop a bytecode level invariant checker for the programming language C#. Our checker identifies program states which are reached only because the C# memory model is more relaxed than Sequential Consistency. It employs partial order reduction strategies to\u00a0\u2026", "num_citations": "72\n", "authors": ["272"]}
{"title": "Modeling out-of-order processors for software timing analysis\n", "abstract": " Estimating the worst case execution time (WCET) of a program on a given processor is important for the schedulability analysis of real-time systems. WCET analysis techniques typically model the timing effects of microarchitectural features in modern processors (such as the pipeline, caches, branch prediction, etc.) to obtain safe but tight estimates. In this paper, we model out-of-order processor pipelines for WCET analysis. This analysis is, in general, difficult even for a basic block (a sequence of instructions with single-entry and single-exit points) if some of the instructions have variable latencies. This is because the WCET of a basic block on out-of-order pipelines cannot be obtained by assuming maximum latencies of the individual instructions. Our timing estimation technique for a basic block is inspired by an existing performance analysis technique for tasks with data dependencies and resource contentions in\u00a0\u2026", "num_citations": "70\n", "authors": ["272"]}
{"title": "Accurately choosing execution runs for software fault localization\n", "abstract": " Software fault localization involves locating the exact cause of error for a \u201cfailing\u201d execution run \u2013 a run which exhibits an unexpected behavior. Given such a failing run, fault localization often proceeds by comparing the failing run with a \u201csuccessful\u201d run, that is, a run which does not exhibit the unexpected behavior. One important issue here is the choice of the successful run for such a comparison. In this paper, we propose a control flow based difference metric for this purpose. The difference metric takes into account the sequence of statement instances (and not just the set of these instances) executed in the two runs, by locating branch instances with similar contexts but different outcomes in the failing and the successful runs. Given a failing run \u03c0                                    f                  and a pool of successful runs S, we choose the successful run \u03c0                                    s                  from S whose execution trace is\u00a0\u2026", "num_citations": "68\n", "authors": ["272"]}
{"title": "oo7: Low-overhead defense against spectre attacks via program analysis\n", "abstract": " The Spectre vulnerability in modern processors has been widely reported. The key insight in this vulnerability is that speculative execution in processors can be misused to access secrets speculatively. Subsequently even though the speculatively executed instructions are squashed, the secret may linger in micro-architectural states such as cache, and can potentially be accessed by an attacker via side channels. In this paper, we take the analysis approach, and try to see how Spectre attacks can be mitigated using static analysis. We propose oo7, a static analysis approach that can mitigate Spectre attacks by detecting potentially vulnerable code snippets in program binaries and protecting them against the attack. Our key contribution is to balance the concerns of effectiveness, analysis time and run-time overheads. We employ control flow extraction, taint analysis and address analysis to detect tainted conditional\u00a0\u2026", "num_citations": "65\n", "authors": ["272"]}
{"title": "Test generation to expose changes in evolving programs\n", "abstract": " Software constantly undergoes changes throughout its life cycle, and thereby it evolves. As changes are introduced into a code base, we need to make sure that the effect of the changes is thoroughly tested. For this purpose, it is important to generate test cases that can stress the effect of a given change. In this paper, we propose an automatic test generation solution to this problem. Given a change c, we use dynamic symbolic execution to generate a test input t, which stresses the change. This is done by ensuring (i) the change c is executed by t, and (ii) the effect of c is observable in the output produced by the test t. To construct a change-reaching input, our technique uses distance in control-dependency graph to guide path exploration towards the change. Then, our technique identifies the common programming patterns that may prevent a given change from affecting the program's output. For each of these\u00a0\u2026", "num_citations": "65\n", "authors": ["272"]}
{"title": "Partition-based regression verification\n", "abstract": " Regression verification (RV) seeks to guarantee the absence of regression errors in a changed program version. This paper presents Partition-based Regression Verification (PRV): an approach to RV based on the gradual exploration of differential input partitions. A differential input partition is a subset of the common input space of two program versions that serves as a unit of verification. Instead of proving the absence of regression for the complete input space at once, PRV verifies differential partitions in a gradual manner. If the exploration is interrupted, PRV retains partial verification guarantees at least for the explored differential partitions. This is crucial in practice as verifying the complete input space can be prohibitively expensive. Experiments show that PRV provides a useful alternative to state-of-the-art regression test generation techniques. During the exploration, PRV generates test cases which can\u00a0\u2026", "num_citations": "63\n", "authors": ["272"]}
{"title": "Engineering multi-tenant software-as-a-service systems\n", "abstract": " Increasingly, Software-as-a-Service (SaaS) is becoming a dominant mechanism for the consumption of software by end users. From a vendor's perspective, the benefits of SaaS arise from leveraging economies of scale, by serving a large number of customers (\" tenants\") through a shared instance of a centrally hosted software service. Consequently, a SaaS provider would, in general, try to drive commonality amongst the requirements of different tenants, and at best, offer a fixed set of customization options. However, many tenants would also come with custom requirements, which may be a pre-requisite for them to adopt the SaaS system. These requirements should then be addressed by evolving the SaaS system in a controlled manner, while still supporting the needs of existing tenants. This need to balance tenant variability and commonality, and to optimize on development and testing effort, can make the\u00a0\u2026", "num_citations": "62\n", "authors": ["272"]}
{"title": "Path exploration based on symbolic output\n", "abstract": " Efficient program path exploration is important for many software engineering activities such as testing, debugging, and verification. However, enumerating all paths of a program is prohibitively expensive. In this article, we develop a partitioning of program paths based on the program output. Two program paths are placed in the same partition if they derive the output similarly, that is, the symbolic expression connecting the output with the inputs is the same in both paths. Our grouping of paths is gradually created by a smart path exploration. Our experiments show the benefits of the proposed path exploration in test-suite construction. Our path partitioning produces a semantic signature of a program\u2014describing all the different symbolic expressions that the output can assume along different program paths. To reason about changes between program versions, we can therefore analyze their semantic signatures. In\u00a0\u2026", "num_citations": "62\n", "authors": ["272"]}
{"title": "Dynamic slicing on Java bytecode traces\n", "abstract": " Dynamic slicing is a well-known technique for program analysis, debugging and understanding. Given a program P and input I, it finds all program statements which directly/indirectly affect the values of some variables' occurrences when P is executed with I. In this article, we develop a dynamic slicing method for Java programs. Our technique proceeds by backwards traversal of the bytecode trace produced by an input I in a given program P. Since such traces can be huge, we use results from data compression to compactly represent bytecode traces. The major space savings in our method come from the optimized representation of (a) data addresses used as operands by memory reference bytecodes, and (b) instruction addresses used as operands by control transfer bytecodes. We show how dynamic slicing algorithms can directly traverse our compact bytecode traces without resorting to costly decompression\u00a0\u2026", "num_citations": "61\n", "authors": ["272"]}
{"title": "Automated re-factoring of android apps to enhance energy-efficiency\n", "abstract": " Mobile devices, such as smartphones and tablets, are energy constrained by nature. Therefore, apps targeted for such platforms must be energy-efficient. However, due to the use of energy oblivious design practices often this is not the case. In this paper, we present a light-weight re-factoring technique that can assist in energy-aware app development. Our technique relies on a set of energy-efficiency guidelines that encodes the optimal usage of energy-intensive (hardware) resources in an app. Given a prototype for an app, our technique begins by generating a design-expression for it. A design-expression can be described as a regular-expression representing the ordering of energy-intensive resource usages and invocation of key functionalities (event-handlers) within the app. It also generates a set of defect-expressions, that are design-expressions representing the negation of energy-efficiency guidelines. A\u00a0\u2026", "num_citations": "59\n", "authors": ["272"]}
{"title": "Performance debugging of Esterel specifications\n", "abstract": " Synchronous languages like Esterel have been widely adopted for designing reactive systems in safety-critical domains such as avionics. Specifications written in Esterel are based on the underlying\" synchrony hypothesis\", where the computation/communication associated with the processing of all events occurring within the same\" clock tick\" are assumed to happen instantaneously (or in zero time). In reality, Esterel specifications get compiled to implementations (such as C code) which do not satisfy the perfect synchrony assumption. Hence, platform-specific timing analysis of such implementations is an important research topic. Interest in this area has lately been renewed with the recent advances in Worst-case Execution Time (WCET) analysis techniques. In this paper we perform WCET analysis on sequential C code and exploit the structure of the code generated from Esterel specifications to obtain tight\u00a0\u2026", "num_citations": "59\n", "authors": ["272"]}
{"title": "Accounting for cache-related preemption delay in dynamic priority schedulability analysis\n", "abstract": " Recently there has been considerable interest in incorporating timing effects of micro architectural features of processors (e.g. caches and pipelines) into the schedulability analysis of tasks running on them. Following this line of work, in this paper the authors show how to account for the effects of cache-related preemption delay (CRPD) in the standard schedulability tests for dynamic priority schedulers like EDF. Even if the memory space of tasks is disjoint, their memory blocks usually map into a shared cache. As a result, task preemption may introduce additional cache misses which are encountered when the preempted task resumes execution; the delay due to these additional misses is called CRPD. Previous work on accounting for CRPD was restricted to only static priority schedulers and periodic task models. Our work extends these results to dynamic priority schedulers and more general task models (e.g\u00a0\u2026", "num_citations": "58\n", "authors": ["272"]}
{"title": "Modeling control speculation for timing analysis\n", "abstract": " The schedulability analysis of real-time embedded systems requires worst case execution time (WCET) analysis for the individual tasks. Bounding WCET involves not only language-level program path analysis, but also modeling the performance impact of complex micro-architectural features present in modern processors. In this paper, we statically analyze the execution time of embedded software on processors with speculative execution. The speculation of conditional branch outcomes (branch prediction) significantly improves a program's execution time. Thus, accurate modeling of control speculation is important for calculating tight WCET estimates. We present a parameterized framework to model the different branch prediction schemes. We further consider the complex interaction between speculative execution and instruction cache performance, that is, the fact that speculatively executed blocks can\u00a0\u2026", "num_citations": "57\n", "authors": ["272"]}
{"title": "Automated partitioning of android applications for trusted execution environments\n", "abstract": " The co-existence of critical and non-critical applications on computing devices, such as mobile phones, is becoming commonplace. The sensitive segments of a critical application should be executed in isolation on Trusted Execution Environments (TEE) so that the associated code and data can be protected from malicious applications. TEE is supported by different technologies and platforms, such as ARM Trustzone, that allow logical separation of \"secure\" and \"normal\" worlds. We develop an approach for automated partitioning of critical Android applications into \"client\" code to be run in the \"normal\" world and \"TEE commands\" encapsulating the handling of confidential data to be run in the \"secure\" world. We also reduce the overhead due to transitions between the two worlds by choosing appropriate granularity for the TEE commands. The advantage of our proposed solution is evidenced by efficient partitioning\u00a0\u2026", "num_citations": "56\n", "authors": ["272"]}
{"title": "Static analysis of multi-core TDMA resource arbitration delays\n", "abstract": " In the development of hard real-time systems, knowledge of the Worst-Case Execution Time (WCET) is needed to guarantee the safety of a system. For single-core systems, static analyses have been developed which are able to derive guaranteed bounds on a program\u2019s WCET. Unfortunately, these analyses cannot directly be applied to multi-core scenarios, where the different cores may interfere with each other during the access to shared resources like for example shared buses or memories. For the arbitration of such resources, TDMA arbitration has been shown to exhibit favorable timing predictability properties. In this article, we review and extend a methodology for analyzing access delays for TDMA-arbitrated resources. Formal proofs of the correctness of these methods are given and a thorough experimental evaluation is carried out, where the presented techniques are compared to preexisting ones\u00a0\u2026", "num_citations": "54\n", "authors": ["272"]}
{"title": "Justifying proofs using memo tables\n", "abstract": " \u042c \u0430 \u0439\u0419 \u0437 \u0434\u0436\u0433\u0433 \u0437\u043d\u0437\u0438 \u0431\u0437 \u0432 \u0430 \u0432\u0438\u0430\u043d \u0437\u0434 \u040c \u0432 \u0436 \u0438\u0430\u043d \u043c \u0439\u0438 \u043d \u0438 \u0430 \u0424\u0433 \u0428\u0436\u0433 \u0436 \u0431\u0431 \u0432 \u0414\u0424\u0428\u0415 \u0437\u043d\u0437\u0419 \u0438 \u0431\u041a \u0427\u0439\u0436 \u043c\u0434 \u0436 \u0432 \u043b \u0438 \u0438 \u0425 \u0431\u0433 \u0430 \u0436 \u0437 \u0433\u043b\u0437 \u0438 \u0438 \u0437\u0439 \u0432 \u0432 \u0433 \u0432 \u0432 \u0439\u0437 \u0438\u0433 \u0437 \u0436 \u0433\u0436 \u0438 \u043c \u0437\u0419 \u0438 \u0432 \u0433 \u0434\u0436\u0433\u0433 \u043a \u0436\u043d \u0426 \u0432\u0438\u0430\u043d\u041a \u0420\u0433\u043b \u043a \u0436\u0418 \u0438 \u0439\u0437 \u0436\u0437 \u0433 \u0438 \u0430 \u0439 \u0437\u043d\u0437\u0438 \u0431 \u0436 \u0433 \u0438 \u0432 \u0432\u0438 \u0436 \u0437\u0438 \u0432 \u0438\u0438 \u0432 \u0437\u0439\u0426 \u0432\u0438 \u043a\u0419 \u0432 \u0414 \u0432 \u0438 \u0436\u0431\u0437 \u0433 \u0438 \u0438 \u0430 \u0439 \u0434\u0436\u0433\u0433 \u0436\u0439\u0430 \u0437\u0415 \u0433\u0432 \u043b \u043d \u0434\u0436\u0433\u0433 \u0433 \u0437 \u0433\u0436 \u0433 \u0437 \u0432\u0433\u0438 \u043c \u0437\u0438\u041a \u0421\u0432 \u0438 \u0437 \u0434 \u0434 \u0436\u0418 \u043b \u0436 \u0437\u0437 \u0438 \u0434\u0436\u0433 \u0419 \u0430 \u0431 \u0433 \u0433\u0432\u0437\u0438\u0436\u0439 \u0438 \u0432 \u0437\u0439 \u0432 \u043a \u0432 \u043b \u0438 \u0433\u0439\u0438 \u0432\u0438\u0436\u0433 \u0439 \u0432 \u0432\u043d \u0438 \u0433\u0432 \u0430 \u0433\u0431\u0434\u0439\u0438 \u0438 \u0433\u0432 \u0430 \u0433\u043a \u0436 \u0438\u0433 \u0438 \u0434\u0436\u0433\u0433 \u0437 \u0436 \u041a \u0438 \u0430 \u0424\u0428 \u0437\u043d\u0437\u0438 \u0431 \u0431 \u0432\u0438 \u0432\u0437 \u0431 \u0431\u0433 \u0438 \u0430 \u0433 \u0430 \u0431\u0431 \u0437 \u0438 \u0438 \u043b \u0436 \u0438\u0436 \u0432 \u0434\u0433\u0437\u0437 \u0430\u043d \u0434\u0436\u0433\u043a \u0439\u0436 \u0432 \u0435\u0439 \u0436\u043d \u043a \u0430\u0439 \u0438 \u0433\u0432\u041a \u042f \u0434\u0436\u0433\u0434\u0433\u0437 \u0438 \u0433\u0432 \u0434\u0438 \u0433 \u0439\u0437\u0438 \u040c \u0436 \u0433\u0436 \u043c\u0438\u0436 \u0438 \u0432 \u0437\u0439\u0426 \u0432\u0438 \u043a \u0432 \u0433\u0436 \u0438 \u0438\u0436\u0439\u0438 \u0433\u0436 \u0430\u0437 \u0433\u0433 \u0433 \u0430 \u0438 \u0436 \u0430\u0437 \u0432 \u0430\u0433 \u0434\u0436\u0433\u0419 \u0436 \u0431\u0418 \u043d \u0434\u0433\u0437\u0438\u0419\u0434\u0436\u0433 \u0437\u0437 \u0432 \u0438 \u0431 \u0431\u0433 \u0438 \u0430 \u0437 \u0436 \u0438 \u0439\u0436 \u0432 \u0435\u0439 \u0436\u043d \u043a \u0430\u0439 \u0438 \u0433\u0432\u041a \u0437 \u0433\u0432 \u0438 \u0437 \u0430\u0433 \u0434\u0436\u0433 \u0436 \u0431 \u0439\u0437\u0438 \u040c \u0436\u0418 \u043b \u0437 \u0433\u043b \u0433\u043b \u0438\u0433 \u0433\u0432\u0437\u0438\u0436\u0439 \u0438 \u043a \u0432 \u0433\u0436 \u0438 \u0434\u0436 \u0437 \u0432 \u041b \u0437 \u0432 \u0433 \u0438 \u0430 \u0439 \u0432 \u0438 \u0430 \u0439\u0419 \u0437 \u0434\u0436\u0433\u0433 \u0437\u043d\u0437\u0438 \u0431\u041a \u042f \u0434\u0436\u0433\u043a \u043c\u0434 \u0436\u0419 \u0431 \u0432\u0438 \u0430 \u0436 \u0437\u0439\u0430\u0438\u0437 \u0437 \u0433\u043b \u0432 \u0438 \u040b \u0438 \u043a \u0432 \u0437\u0437 \u0433 \u0438 \u0439\u0437\u0438 \u040c \u0436 \u0432 \u0433\u0432\u0437\u0438\u0436\u0439 \u0438 \u0432 \u0437\u0439 \u0432 \u0438 \u043a \u0432 \u0433 \u0438 \u043a \u0430\u0439 \u0438 \u0433\u0432 \u0434 \u0436 \u0433\u0436\u0431 \u043d \u0438 \u0425 \u0431\u0433 \u0430 \u0436\u041a \u0432 \u0430\u0430\u043d \u043b \u0437 \u0439\u0437\u0437 \u0438 \u0436\u0433\u0430 \u0433 \u0438 \u0439\u0437\u0438 \u040c \u0436 \u0437 \u0434\u0436\u0433 \u0436 \u0431\u0431 \u0432 \u0437\u0438\u0436 \u0438 \u0433\u0432 \u0433\u0436 \u0432 \u0433 \u0432 \u0419 \u040c \u0432\u0438 \u0430 \u0433\u0436 \u0438 \u0431\u0437 \u0437 \u0438 \u0430 \u0430\u0433 \u0434\u0436\u0433 \u0436 \u0431\u0437\u041a", "num_citations": "54\n", "authors": ["272"]}
{"title": "Unified cache modeling for WCET analysis and layout optimizations\n", "abstract": " Presence of instruction and data caches in processors create lack of predictability in execution timings. Hard real-time systems require absolute guarantees about execution time, and hence the timing effects of caches need to be modeled while estimating the worst-case execution time (WCET) of a program. In this work, we consider the modeling of a generic cache architecture which is most common in commercial processors - separate instruction and data caches in the first level and a unified cache in the second level (which houses code as well as data). Our modeling is used to develop a timing analysis method built on top of the Chronos WCET analysis tool. Moreover we use our unified cache modeling to develop WCET-driven code and data layout optimizations - where the code and data layout are optimized simultaneously for reducing WCET.", "num_citations": "52\n", "authors": ["272"]}
{"title": "Mining message sequence graphs\n", "abstract": " Dynamic specification mining involves discovering software behavior from traces for the purpose of program comprehension and bug detection. However, mining program behavior from execution traces is difficult for concurrent/distributed programs. Specifically, the inherent partial order relationships among events occurring across processes pose a big challenge to specification mining. In this paper, we propose a framework for mining partial orders so as to understand concurrent program behavior. Our miner takes in a set of concurrent program traces, and produces a message sequence graph (MSG) to represent the concurrent program behavior. An MSG represents a graph where the nodes of the graph are partial orders, represented as Message Sequence Charts. Mining an MSG allows us to understand concurrent program behaviors since the nodes of the MSG depict important\" phases\" or\" interaction snippets\u00a0\u2026", "num_citations": "51\n", "authors": ["272"]}
{"title": "Timing analysis of concurrent programs running on shared cache multi-cores\n", "abstract": " Memory accesses form an important source of timing unpredictability. Timing analysis of real-time embedded software thus requires bounding the time for memory accesses. Multiprocessing, a popular approach for performance enhancement, opens up the opportunity for concurrent execution. However due to contention for any shared memory by different processing cores, memory access behavior becomes more unpredictable, and hence harder to analyze. In this paper, we develop a timing analysis method for concurrent software running on multi-cores with a shared instruction cache. Communication across tasks is by message passing. Our method progressively improves the lifetime estimates of tasks that execute concurrently on multiple cores, in order to estimate potential conflicts in the shared cache. Possible conflicts arising from overlapping task lifetimes are accounted for in the hit-miss\u00a0\u2026", "num_citations": "50\n", "authors": ["272"]}
{"title": "Repairing Crashes in Android Apps\n", "abstract": " Android apps are omnipresent, and frequently suffer from crashes - leading to poor user experience and economic loss. Past work focused on automated test generation to detect crashes in Android apps. However, automated repair of crashes has not been studied. In this paper, we propose the first approach to automatically repair Android apps, specifically we propose a technique for fixing crashes in Android apps. Unlike most test-based repair approaches, we do not need a test-suite; instead a single failing test is meticulously analyzed for crash locations and reasons behind these crashes. Our approach hinges on a careful empirical study which seeks to establish common root-causes for crashes in Android apps, and then distills the remedy of these root-causes in the form of eight generic transformation operators. These operators are applied using a search-based repair framework embodied in our repair tool\u00a0\u2026", "num_citations": "48\n", "authors": ["272"]}
{"title": "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics): Preface\n", "abstract": " Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics): Preface \u2014 Macquarie University Skip to main navigation Skip to search Skip to main content Macquarie University Logo Help & FAQ Home Profiles Research Units Projects Research Outputs Prizes Activities Press / Media Impacts Search by expertise, name or affiliation Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics): Preface Masayuki Abe, Kazumaro Aoki, Giuseppe Ateniese, Roberto Avanzi, Zuzana Beerliov\u00e1, Olivier Billet, Alex Biryukov, Ian Blake, Colin Boyd, Eric Brier, Aniello Castiglione, Juyoung Cha, Aldar Chan, Liqun Chen, Kookrae Cho, Scott Contini, Paolo D'Arco, Jintai Ding, Christophe Doche, Orr Dunkelman Show 70 more Show less Matthias Fitzi, Pierre Alain Fouque, Jacques JA Fournier, \u2026", "num_citations": "47\n", "authors": ["272"]}
{"title": "Energypatch: Repairing resource leaks to improve energy-efficiency of android apps\n", "abstract": " Increased usage of mobile devices, such as smartphones and tablets, has led to widespread popularity and usage of mobile apps. If not carefully developed, such apps may demonstrate energy-inefficient behaviour, where one or more energy-intensive hardware components (such as Wifi, GPS, etc) are left in a high-power state, even when no apps are using these components. We refer to such kind of energy-inefficiencies as energy bugs. Executing an app with an energy bug causes the mobile device to exhibit poor energy consumption behaviour and a drastically shortened battery life. Since mobiles apps can have huge input domains, therefore exhaustive exploration is often impractical. We believe that there is a need for a framework that can systematically detect and fix energy bugs in mobile apps in a scalable fashion. To address this need, we have developed EnergyPatch, a framework that uses a\u00a0\u2026", "num_citations": "45\n", "authors": ["272"]}
{"title": "Regression tests to expose change interaction errors\n", "abstract": " Changes often introduce program errors, and hence recent software testing literature has focused on generating tests which stress changes. In this paper, we argue that changes cannot be treated as isolated program artifacts which are stressed via testing. Instead, it is the complex dependency across multiple changes which introduce subtle errors. Furthermore, the complex dependence structures, that need to be exercised to expose such errors, ensure that they remain undiscovered even in well tested and deployed software. We motivate our work based on empirical evidence from a well tested and stable project-Linux GNU Coreutils-where we found that one third of the regressions take more than two (2) years to be fixed, and that two thirds of such long-standing regressions are introduced due to change interactions for the utilities we investigated.", "num_citations": "45\n", "authors": ["272"]}
{"title": "Golden implementation driven software debugging\n", "abstract": " The presence of a functionally correct golden implementation has a significant advantage in the software development life cycle. Such a golden implementation is exploited for software development in several domains, including embedded software---a low resource-consuming version of the golden implementation. The golden implementation gives the functionality that the program is supposed to implement, and is used as a guide during the software development process. In this paper, we investigate the possibility of using the golden implementation as a reference model in software debugging. We perform a substantial case study involving the Busybox embedded Linux utilities while treating the GNU Core Utilities as the golden or reference implementation. Our debugging method consists of dynamic slicing with respect to the observable error in both the implementations (the golden implementation as well as the\u00a0\u2026", "num_citations": "43\n", "authors": ["272"]}
{"title": "Communicating transaction processes\n", "abstract": " Message sequence charts (MSC) have been traditionally used to depict execution scenarios in the early stages of design cycle. MSCs portray inter-process (inter-object) interactions. Synthesizing intra-process (intra-object) executable specifications from an MSC-based description is a nontrivial task. Here we present a model called communicating transaction processes (CTP) based on MSCs from which an executable specification can be extracted in a straight forward manner. Our model describes a network of communicating processes as a collection of high-level labeled transition systems, where processes interact via common action labels. Each action is a nonatomic interaction which is described by a guarded choice of MSCs. Thus our model achieves a separation of concerns: the high-level transition systems depicting intra-process control flow, while the actions in the transition system capture inter-process\u00a0\u2026", "num_citations": "43\n", "authors": ["272"]}
{"title": "Embedded systems and software validation\n", "abstract": " Modern embedded systems require high performance, low cost and low power consumption. Such systems typically consist of a heterogeneous collection of processors, specialized memory subsystems, and partially programmable or fixed-function components. This heterogeneity, coupled with issues such as hardware/software partitioning, mapping, scheduling, etc., leads to a large number of design possibilities, making performance debugging and validation of such systems a difficult problem. Embedded systems are used to control safety critical applications such as flight control, automotive electronics and healthcare monitoring. Clearly, developing reliable software/systems for such applications is of utmost importance. This book describes a host of debugging and verification methods which can help to achieve this goal. Covers the major abstraction levels of embedded systems design, starting from software analysis and micro-architectural modeling, to modeling of resource sharing and communication at the system level Integrates formal techniques of validation for hardware/software with debugging and validation of embedded system design flows Includes practical case studies to answer the questions: does a design meet its requirements, if not, then which parts of the system are responsible for the violation, and once they are identified, then how should the design be suitably modified?", "num_citations": "42\n", "authors": ["272"]}
{"title": "Specifying multithreaded Java semantics for program verification\n", "abstract": " Most current work on multithreaded Java program verification assumes a model of execution that is based on interleaving of the operations of the individual threads. However, the Java language specification supports a weaker model of execution, called the Java Memory Model (JMM). The JMM allows certain reordering of operations within a thread and thus permits more behaviors than the interleaving based execution model. Therefore, programs verified by assuming interleaved thread execution may not behave correctly for certain Java multithreading implementations. The main difficulty with the JMM is that it is informally described in an abstract rule-based declarative style, which is unsuitable for formal verification. We develop an equivalent formal executable specification of the JMM. Our specification is operational and uses guarded commands. We then use this executable model to verify popular software\u00a0\u2026", "num_citations": "42\n", "authors": ["272"]}
{"title": "Automated inductive verification of parameterized protocols?\n", "abstract": " A parameterized concurrent system represents an infinite family (of finite state systems) parameterized by a recursively defined type such as chains, trees. It is therefore natural to verify parameterized-systems by inducting over this type. We employ a program transformation based proof methodology to automate such induction proofs. Our proof technique is geared to automate nested induction proofs which do not involve strengthening of induction hypothesis. Based on this technique, we have designed and implemented a prover for parameterized protocols. The prover has been used to automatically verify safety properties of parameterized cache coherence protocols, including broadcast protocols and protocols with global conditions. Furthermore we also describe its successful use in verifying mutual exclusion in the Java Meta-Locking Algorithm, developed recently by Sun Microsystems for ensuring\u00a0\u2026", "num_citations": "39\n", "authors": ["272"]}
{"title": "Hercules: Reproducing Crashes in Real-World Application Binaries\n", "abstract": " Binary analysis is a well-investigated area in software engineering and security. Given real-world program binaries, generating test inputs which cause the binaries to crash is crucial. Generation of crashing inputs has many applications including off-line analysis of software prior to deployment, or online analysis of software patches as they are inserted. In this work, we present a method for generating inputs which reach a given \"potentially crashing\" location. Such potentially crashing locations can be found by a separate static analysis (or by gleaning crash reports submitted by internal / external users) and serve as the input to our method. The test input generated by our method serves as a witness of the crash. Our method is particularly suited for binaries of programs which take in complex structured inputs. Experiments on real-life applications such as the Adobe Reader and the Windows Media Player\u00a0\u2026", "num_citations": "38\n", "authors": ["272"]}
{"title": "Accurate timing analysis by modeling caches, speculation and their interaction\n", "abstract": " Schedulability analysis of real-time embedded systems requires worst case timing guarantees of embedded software performance. This involves not only language level program analysis, but also modeling the effects of complex micro-architectural features in modern processors. Speculative execution and caching are very common in current processors. Hence one needs to model the effects of these features on the Worst Case Execution Time (WCET) of a program. Even though the individual effects of these features have been studied recently, their combined effects have not been investigated. We do so in this paper. This is a non-trivial task because speculative execution can indirectly affect cache performance (eg, speculatively executed blocks can cause additional cache misses). Our technique starts from the control flow graph of the embedded program, and uses integer linear programming to estimate the\u00a0\u2026", "num_citations": "38\n", "authors": ["272"]}
{"title": "Hierarchical dynamic slicing\n", "abstract": " Dynamic slicing is a widely used technique for program analysis, debugging, and comprehension. However, the reported slice is often too large to be inspected by the programmer. In this work, we address this deficiency by hierarchically applying dynamic slicing at various levels of granularity. The basic observation is to divide a program execution trace into\" phases\", with data/control dependencies inside each phase being suppressed. Only the inter-phase dependencies are presented to the programmer. The programmer then zooms into one of these phases which is further divided into sub-phases and analyzed. We also discuss how our ideas can be used to augment debugging methods other then slicing (such as\" fault localization\", a recently proposed trace comparison method for software debugging).", "num_citations": "37\n", "authors": ["272"]}
{"title": "Context-sensitive timing analysis of Esterel programs\n", "abstract": " Traditionally, synchronous languages, such as Esterel, have been compiled into hardware, where timing analysis is relatively easy. When compiled into software--eg, into sequential C code--very conservative estimation techniques have been used, where the focus has only been on obtaining safe timing estimates and not on the cost of the implementation. While this was acceptable in avionics, efficient implementations and hence tight timing estimates are needed in more cost-sensitive application domains. Lately, a number of advances in Worst-Case Execution Time (WCET) analysis techniques, coupled with the growing use of software in domains such as automotives, have led to a considerable interest in timing analysis of code generated from Esterel specifications. In this paper we propose techniques to obtain tight estimates on the processing time of input events by sequential C code generated from Esterel\u00a0\u2026", "num_citations": "36\n", "authors": ["272"]}
{"title": "Exploiting branch constraints without exhaustive path enumeration\n", "abstract": " Statically estimating the worst case execution time (WCET) of a program is important for real-time software. This is difficult even in the programming language level due to the inherent difficulty in detecting and exploiting infeasible paths in a program\u00c3\u00a2 \u00e2 \u201a\u00ac \u00e2 \u201e\u00a2 s control flow graph. In this paper, we propose an efficient method to exploit infeasible path information for WCET estimation of a loop without resorting to exhaustive path enumeration. The ef-ficiency of our approach is demonstrated with a real-life control-intensive program.", "num_citations": "36\n", "authors": ["272"]}
{"title": "Design space exploration of caches using compressed traces\n", "abstract": " Memory subsystem, in particular, cache design is important for both high performance and embedded computing systems. The trend towards increased customization for embedded systems, in addition, requires the design of an optimal cache configuration for each application. Trace driven simulation is widely used to evaluate cache performance. However, traces are storage inefficient and simulation is too slow especially when hundreds of design points need to be evaluated. Trace based simulation has two sources of redundancies: multiple occurrences of the same sequence in the trace and containment relationship among cache configurations. We exploit both the redundancies in a unified manner by simulating multiple cache configurations in a single pass directly over a compressed trace (which has already identified the repetitive sequences). Experimental results indicate that our approach achieves\u00a0\u2026", "num_citations": "36\n", "authors": ["272"]}
{"title": "Scalable and precise refinement of cache timing analysis via model checking\n", "abstract": " Hard real time systems require absolute guarantees in their execution times. Worst case execution time (WCET) of a program has therefore become an important problem to address. However, performance enhancing features of a processor (e.g. cache) make WCET analysis a difficult problem. In this paper, we propose a novel approach of combining abstract interpretation and model checking for different varieties of cache analysis ranging from single to multi-core platforms. Our modeling is used to develop a precise yet scalable timing analysis method on top of the Chronos WCET analysis tool. Experimental results demonstrate that we can obtain significant improvement in precision with reasonable analysis time overhead.", "num_citations": "33\n", "authors": ["272"]}
{"title": "Timing analysis of embedded software for speculative processors\n", "abstract": " Static timing analysis of embedded software is important for systems with hard real-time constraints. To accurately estimate time bounds, it is essential to model the underlying micro architecture. In this paper, we study static timing analysis of embedded programs for modern processors with speculative execution. Speculation of conditional branch outcomes significantly improves processor performance, and hence program execution time. Although speculation is used in most modern processors, its effect on software timing has not been systematically studied before. The main contribution of our work is a parameterized framework to model different control flow speculation schemes. The accuracy of our framework is illustrated through tight timing estimates obtained for benchmark programs.", "num_citations": "33\n", "authors": ["272"]}
{"title": "AFLNet: a greybox fuzzer for network protocols\n", "abstract": " Server fuzzing is difficult. Unlike simple command-line tools, servers feature a massive state space that can be traversed effectively only with well-defined sequences of input messages. Valid sequences are specified in a protocol. In this paper, we present AFLNET, the first greybox fuzzer for protocol implementations. Unlike existing protocol fuzzers, AFLNET takes a mutational approach and uses state-feedback to guide the fuzzing process. AFLNET is seeded with a corpus of recorded message exchanges between the server and an actual client. No protocol specification or message grammars are required. AFLNET acts as a client and replays variations of the original sequence of messages sent to the server and retains those variations that were effective at increasing the coverage of the code or state space. To identify the server states that are exercised by a message sequence, AFLNET uses the server's\u00a0\u2026", "num_citations": "32\n", "authors": ["272"]}
{"title": "Inferring class level specifications for distributed systems\n", "abstract": " Distributed systems often contain many behaviorally similar processes, which are conveniently grouped into classes. In system modeling, it is common to specify such systems by describing the class level behavior, instead of object level behavior. While there have been techniques that mine specifications of such distributed systems from their execution traces, these methods only mine object-level specifications involving concrete process objects. This leads to specifications which are large, hard to comprehend, and sensitive to simple changes in the system (such as the number of objects). In this paper, we develop a class level specification mining framework for distributed systems. A specification that describes interaction snippets between various processes in a distributed system forms a natural and intuitive way to document their behavior. Our mining method groups together such interactions between\u00a0\u2026", "num_citations": "32\n", "authors": ["272"]}
{"title": "A framework to model branch prediction for wcet analysis\n", "abstract": " In this paper, we present a framework to model branch prediction for Worst Case Execution Time (WCET) analysis. Our micro-architectural modeling is completely generic, and parameterizable wrt the currently used branch prediction schemes. It automatically derives linear constraints on the total misprediction count from the control flow graph of the program. These constraints can be solved by any integer linear programming (ILP) solver to estimate the WCET.Current generation processors perform control flow speculation through branch prediction, which predicts the outcome of branch instructions. If the prediction is correct, then execution proceeds without any interruption. For incorrect prediction, the speculatively executed instructions are undone, incurring a branch misprediction penalty between 3-19 clock cycles. If branch prediction is not modeled, all the branches in the program must be conservatively assumed to be mispredicted for finding the WCET. This pessimism results in as much as 60\u2212 70% overestimation for some of the benchmarks in this paper, even assuming a 3 clock cycle branch misprediction penalty.", "num_citations": "30\n", "authors": ["272"]}
{"title": "Beyond Tamaki-Sato style unfold/fold transformations for normal logic programs\n", "abstract": " Unfold/fold transformation systems for logic programs have been extensively investigated. Existing unfold/fold transformation systems for normal logic programs typically fold using a single, non-recursive clause i.e. the folding transformation is very restricted. In this paper we present a transformation system that permits folding in the presence of recursion, disjunction, as well as negation. We show that the transformations are correct with respect to various model theoretic semantics of normal logic programs including the well-founded model and stable model semantics.", "num_citations": "30\n", "authors": ["272"]}
{"title": "Debugging energy-efficiency related field failures in mobile apps\n", "abstract": " Debugging field failures can be a challenging task for app-developers. Insufficient or unreliable information, improper assumptions and multitude of devices (smartphones) being used, are just some of the many factors that may contribute to its challenges. In this work, we design and develop an open-source framework that helps to communicate, localize and patch energy consumption related field failures in Android apps. Our framework consists of two sets of automated tools: one for the app-user to precisely record and report field failures observed in real-life apps, and the other assists the developer by automatically localizing the reported defects and suggesting patch locations. More specifically, the tools on the developer's side consist of an Eclipse-plugin that detects specific patterns of Android API calls, that are indicative of energy-inefficient behaviour. In our experiments with real-life apps we observed that our\u00a0\u2026", "num_citations": "29\n", "authors": ["272"]}
{"title": "Symbolic message sequence charts\n", "abstract": " Message sequence charts (MSCs) are a widely used visual formalism for scenario-based specifications of distributed reactive systems. In its conventional usage, an MSC captures an interaction snippet between concrete objects in the system. This leads to voluminous specifications when the system contains several objects that are behaviorally similar. MSCs also play an important role in the model-based testing of reactive systems, where they may be used for specifying (partial) system behaviors, describing test generation criteria, or representing test cases. However, since the number of processes in a MSC specification are fixed, model-based testing of systems consisting of process classes may involve a significant amount of rework: for example, reconstructing system models, or regenerating test cases for systems differing only in the number of processes of various types. In this article we propose a scenario\u00a0\u2026", "num_citations": "29\n", "authors": ["272"]}
{"title": "Approach for root causing regression bugs\n", "abstract": " A stable program, a new program version and a test case which passes (or fails) in the first program may be analyzed. Another new input may be found that either exhibits the similar (different) behavior as that of the test case in the first program (or second program) or follows different (similar) behavior as that of the test case in the new program version. In the first case, the trace of the test case and the new input in the second code version while in the second case, the trace of the test case and the new input in the original program are compared to produce a bug report. By reviewing the bug reports, divergences may be found and error causing code lines may be isolated.", "num_citations": "28\n", "authors": ["272"]}
{"title": "Symbolic execution of behavioral requirements\n", "abstract": " Message Sequence Charts (MSC) have traditionally been used as a weak form of behavioral requirements in software design; they denote scenarios which may happen. Live Sequence Charts (LSC) extend Message Sequence Charts by also allowing the designer to specify scenarios which must happen. Live Sequence Chart specifications are executable; their simulation allows the designer to play out potentially aberrant scenarios prior to software construction. In this paper, we propose the use of Constraint Logic Programming (CLP) for symbolic execution of requirements described as Live Sequence Charts. The utility of CLP stems from its ability to execute in the presence of uninstantiated variables. This allows us to simulate multiple scenarios at one go. For example, several scenarios which only differ from each other in the value of a variable may be executed as a single scenario where the variable is\u00a0\u2026", "num_citations": "28\n", "authors": ["272"]}
{"title": "An unfold/fold transformation framework for definite logic programs\n", "abstract": " Given a logic program P, an unfold/fold program transformation system derives a sequence of programs P = P0, P1, \u2026, Pn, such that Pi+1 is derived from Pi by application of either an unfolding or a folding step. Unfold/fold transformations have been widely used for improving program efficiency and for reasoning about programs. Unfolding corresponds to a resolution step and hence is semantics-preserving. Folding, which replaces an occurrence of the right hand side of a clause with its head, may on the other hand produce a semantically different program. Existing unfold/fold transformation systems for logic programs restrict the application of folding by placing (usually syntactic) conditions that are sufficient to guarantee the correctness of folding. These restrictions are often too strong, especially when the transformations are used for reasoning about programs. In this article we develop a transformation system\u00a0\u2026", "num_citations": "28\n", "authors": ["272"]}
{"title": "Crash-Avoiding Program Repair\n", "abstract": " Existing program repair systems modify a buggy program so that the modified program passes given tests. The repaired program may not satisfy even the most basic notion of correctness, namely crash-freedom. In other words, repair tools might generate patches which over-fit the test data driving the repair, and the automatically repaired programs may even introduce crashes or vulnerabilities. We propose an integrated approach for detecting and discarding crashing patches. Our approach fuses test and patch generation into a single process, in which patches are generated with the objective of passing existing tests, and new tests are generated with the objective of filtering out over-fitted patches by distinguishing candidate patches in terms of behavior. We use crash-freedom as the oracle to discard patch candidates which crash on the new tests. In its core, our approach defines a grey-box fuzzing strategy that\u00a0\u2026", "num_citations": "27\n", "authors": ["272"]}
{"title": "Symbolic execution with existential second-order constraints\n", "abstract": " Symbolic execution systematically explores program paths by solving path conditions---formulas over symbolic variables. Typically, the symbolic variables range over numbers, arrays and strings. We introduce symbolic execution with existential second-order constraints---an extension of traditional symbolic execution that allows symbolic variables to range over functions whose interpretations are restricted by a user-defined language. The aims of this new technique are twofold. First, it offers a general analysis framework that can be applied in multiple domains such as program repair and library modelling. Secondly, it addresses the path explosion problem of traditional first-order symbolic execution in certain applications. To realize this technique, we integrate symbolic execution with program synthesis. Specifically, we propose a method of second-order constraint solving that provides efficient proofs of\u00a0\u2026", "num_citations": "25\n", "authors": ["272"]}
{"title": "Scalable and precise refinement of cache timing analysis via path-sensitive verification\n", "abstract": " Hard real-time systems require absolute guarantees in their execution times. Worst case execution time (WCET) of a program has therefore become an important problem to address. However, performance enhancing features of a processor (e.g. cache) make WCET analysis a difficult problem. In this paper, we propose a novel analysis framework by combining abstract interpretation and program verification for different varieties of cache analysis ranging from single to multi-core platforms. Our framework can be instantiated with different program verification techniques, such as model checking and symbolic execution. Our modeling is used to develop a precise yet scalable timing analysis method on top of the Chronos WCET analysis tool. Experimental results demonstrate that we can obtain significant improvement in precision with reasonable analysis time overhead.", "num_citations": "25\n", "authors": ["272"]}
{"title": "Compactly representing parallel program executions\n", "abstract": " Collecting a program's execution profile is important for many reasons: code optimization, memory layout, program debugging and program comprehension. Path based execution profiles are more detailed than count based execution profiles, since they present the order of execution of the various blocks in a program: modules, procedures, basic blocks etc. Recently, online string compression techniques have been employed for collecting compact representations of sequential program executions. In this paper, we show how a similar approach can be taken for shared memory parallel programs. Our compaction scheme yields one to two orders of magnitude compression compared to the uncompressed parallel program trace on some of the SPLASH benchmarks. Our compressed execution traces contain detailed information about synchronization and control/data flow which can be exploited for post-mortem\u00a0\u2026", "num_citations": "25\n", "authors": ["272"]}
{"title": "Precise micro-architectural modeling for WCET analysis via AI+ SAT\n", "abstract": " Hard real-time systems are required to meet critical deadlines. Worst case execution time (WCET) is therefore an important metric for the system level schedulability analysis of hard real-time systems. However, performance enhancing features of a processor (e.g. pipeline, caches) makes WCET analysis a very difficult problem. In this paper, we propose a novel approach to combine abstract interpretation (AI) and satisfiability (SAT) checking (hence the name AI+SAT) for different varieties of micro-architectural modeling. Our work in this paper is inspired by the research advances in program flow analysis(e.g. infeasible path analysis). We show that the accuracy of WCET estimates can be improved in a scalable fashion by using SAT checkers to integrate infeasible path analysis results into micro-architectural modeling. Our modeling is implemented on top of the Chronos WCET analysis tool and we improve the\u00a0\u2026", "num_citations": "24\n", "authors": ["272"]}
{"title": "Tenant onboarding in evolving multi-tenant software-as-a-service systems\n", "abstract": " A multi-tenant software as a service (SaaS) system has to meet the needs of several tenant organizations, which connect to the system to utilize its services. To leverage economies of scale through re-use, a SaaS vendor would, in general, like to drive commonality amongst the requirements across tenants. However, many tenants will also come with some custom requirements that may be a pre-requisite for them to adopt the SaaS system. These requirements then need to be addressed by evolving the SaaS system in a controlled manner, while still supporting the requirements of existing tenants. In this paper, we focus on functional variability amongst tenants in a multi-tenant SaaS and develop a framework to help evolve such systems systematically. We adopt an intuitive formal model of services that is easily amenable to tenant requirement analysis and provides a robust way to support multiple tenant on\u00a0\u2026", "num_citations": "23\n", "authors": ["272"]}
{"title": "Timing analysis of Esterel programs on general-purpose multiprocessors\n", "abstract": " Synchronous languages like Esterel have gained wide popularity in certain domains such as avionics. However, platform-specific timing analysis of code generated from Esterel-like specifications have mostly been neglected so far. The growing volume of electronics and software in domains like automotive, calls for formal-specification based code generation to replace manually written and optimized code. Such cost-sensitive domains require tight estimation of timing properties of the generated code. Towards this goal, we propose a scheme for generating C code from Esterel specifications for a multiprocessor platform, followed by timing analysis of the generated code. Due to dependencies across program fragments mapped onto different processors, traditional Worst-Case Execution Time (WCET) analysis techniques for sequential programs cannot applied be to this setting. Our proposed timing analysis\u00a0\u2026", "num_citations": "23\n", "authors": ["272"]}
{"title": "A parameterized unfold/fold transformation framework for definite logic programs\n", "abstract": " Given a program P, an unfold/fold program transformation system derives a sequence of programs P = P                      0, P                      1, ..., P                                       n                , such that P                                               i\u2009+\u20091 is derived from P                                       i                by application of either an unfolding or a folding step. Existing unfold/fold transformation systems for definite logic programs differ from one another mainly in the kind of folding transformations they permit at each step. Some allow folding using a single (possibly recursive) clause while others permit folding using multiple non-recursive clauses. However, none allow folding using multiple recursive clauses that are drawn from some previous program in the transformation sequence. In this paper we develop a parameterized framework for unfold/fold transformations by suitably abstracting and extending the proofs of existing transformation systems\u00a0\u2026", "num_citations": "22\n", "authors": ["272"]}
{"title": "Kleespectre: Detecting information leakage through speculative cache attacks via symbolic execution\n", "abstract": " Spectre-style attacks disclosed in early 2018 expose data leakage scenarios via cache side channels. Specifically, speculatively executed paths due to branch mis-prediction may bring secret data into the cache, which are then exposed via cache side channels even after the speculative execution is squashed. Symbolic execution is a well-known test generation method to cover program paths at the level of the application software. In this article, we extend symbolic execution with modeling of cache and speculative execution. Our tool KLEESPECTRE, built on top of the KLEE symbolic execution engine, can thus provide a testing engine to check for data leakage through the cache side channel as shown via Spectre attacks. Our symbolic cache model can verify whether the sensitive data leakage due to speculative execution can be observed by an attacker at a given program point. Our experiments show that KLEE\u00a0\u2026", "num_citations": "21\n", "authors": ["272"]}
{"title": "Test-equivalence analysis for automatic patch generation\n", "abstract": " Automated program repair is a problem of finding a transformation (called a patch) of a given incorrect program that eliminates the observable failures. It has important applications such as providing debugging aids, automatically grading student assignments, and patching security vulnerabilities. A common challenge faced by existing repair techniques is scalability to large patch spaces, since there are many candidate patches that these techniques explicitly or implicitly consider. The correctness criteria for program repair is often given as a suite of tests. Current repair techniques do not scale due to the large number of test executions performed by the underlying search algorithms. In this work, we address this problem by introducing a methodology of patch generation based on a test-equivalence relation (if two programs are \u201ctest-equivalent\u201d for a given test, they produce indistinguishable results on this test). We\u00a0\u2026", "num_citations": "21\n", "authors": ["272"]}
{"title": "Efficient algorithms for vertex arboricity of planar graphs\n", "abstract": " Acyclic-coloring of a graph G = (V,E) is a partitioning of V, such that the induced subgraph of each partition is acyclic. The minimum number of such partitions of V is defined as the vertex arboricity of G. A linear time algorithm for acyclic-coloring of planar graphs with 3 colors is presented. Next, an O(n2) algorithm is proposed which produces a valid acyclic-2-coloring of a planar graph, if one exists, since there are planar graphs with arboricity 3.", "num_citations": "21\n", "authors": ["272"]}
{"title": "On testing embedded software\n", "abstract": " For the last few decades, embedded systems have expanded their reach into major aspects of human lives. Starting from small handheld devices (such as smartphones) to advanced automotive systems (such as anti-lock braking systems), usage of embedded systems has increased at a dramatic pace. Embedded software are specialized software that are intended to operate on embedded devices. In this chapter, we shall describe the unique challenges associated with testing embedded software. In particular, embedded software are required to satisfy several non-functional constraints, in addition to functionality-related constraints. Such non-functional constraints may include (but not limited to), timing/energy-consumption related constrains or reliability requirements, etc. Additionally, embedded systems are often required to operate in interaction with the physical environment, obtaining their inputs from\u00a0\u2026", "num_citations": "20\n", "authors": ["272"]}
{"title": "Cache-aware timing analysis of streaming applications\n", "abstract": " Of late, there has been a considerable interest in models, algorithms and methodologies specifically targeted towards designing hardware and software for streaming applications. Such applications process potentially infinite streams of audio/video data or network packets and are found in a wide range of devices, starting from mobile phones to set-top boxes. Given a streaming application and an architecture, the timing analysis problem is to determine the timing properties of the processed data stream, given the timing properties of the input stream. This problem arises while determining many common performance metrics related to streaming applications and the mapping of such applications onto hardware architectures. Such metrics include the maximum delay experienced by any data item of the stream and the maximum backlog or the buffer requirement to store the incoming stream. Most of the\u00a0\u2026", "num_citations": "20\n", "authors": ["272"]}
{"title": "Inductively verifying invariant properties of parameterized systems\n", "abstract": " Verification of distributed algorithms can be naturally cast as verifying parameterized systems, the parameter being the number of processes. In general, a parameterized concurrent system represents an infinite family (of finite state systems) parameterized by a recursively defined type such as chains, trees. It is therefore natural to verify parameterized systems by inducting over this type. However, construction of such proofs require combination of model checking with deductive capability. In this paper, we develop a logic program transformation based proof methodology which achieves this combination. One of our transformations (unfolding) represents a single resolution step. Thus model checking can be achieved by repeated application of unfolding. Other transformations (such as folding) represent deductive reasoning and help recognize the induction hypothesis in an inductive proof. Moreover the\u00a0\u2026", "num_citations": "20\n", "authors": ["272"]}
{"title": "Neuro-symbolic execution: The feasibility of an inductive approach to symbolic execution\n", "abstract": " Symbolic execution is a powerful technique for program analysis. However, it has many limitations in practical applicability: the path explosion problem encumbers scalability, the need for language-specific implementation, the inability to handle complex dependencies, and the limited expressiveness of theories supported by underlying satisfiability checkers. Often, relationships between variables of interest are not expressible directly as purely symbolic constraints. To this end, we present a new approach -- neuro-symbolic execution -- which learns an approximation of the relationship as a neural net. It features a constraint solver that can solve mixed constraints, involving both symbolic expressions and neural network representation. To do so, we envision such constraint solving as procedure combining SMT solving and gradient-based optimization. We demonstrate the utility of neuro-symbolic execution in constructing exploits for buffer overflows. We report success on 13/14 programs which have difficult constraints, known to require specialized extensions to symbolic execution. In addition, our technique solves \\% of the given neuro-symbolic constraints in  programs from standard verification and invariant synthesis benchmarks.", "num_citations": "19\n", "authors": ["272"]}
{"title": "Methods and apparatus for generating a verified algorithm for transforming a program from a first form to a second form\n", "abstract": " A method of synthesizing an algorithm for transforming a program from a first form to a second form includes first formalizing a language associated with the program to be transformed in accordance with a theorem proving system. Then, a proof is built in accordance with the theorem proving system based on a theorem asserting a representability associated with the program, the representability being expressed as inductive predicates over semantic domains. The method then extracts the algorithm based on the proof. The algorithm is capable of transforming the program from the first form to the second form. In one embodiment, the algorithm is a correctness verified abstraction algorithm and the theorem proving system is Nuprl.", "num_citations": "19\n", "authors": ["272"]}
{"title": "Time-travel Testing of Android Apps\n", "abstract": " Android testing tools generate sequences of input events to exercise the state space of the app-under-test. Existing search-based techniques systematically evolve a population of event sequences so as to achieve certain objectives such as maximal code coverage. The hope is that the mutation of fit event sequences leads to the generation of even fitter sequences. However, the evolution of event sequences may be ineffective. Our key insight is that pertinent app states which contributed to the original sequence's fitness may not be reached by a mutated event sequence. The original path through the state space is truncated at the point of mutation. In this paper, we propose instead to evolve a population of states which can be captured upon discovery and resumed when needed. The hope is that generating events on a fit program state leads to the transition to even fitter states. For instance, we can quickly\u00a0\u2026", "num_citations": "18\n", "authors": ["272"]}
{"title": "Future of mobile software for smartphones and drones: Energy and performance\n", "abstract": " The need for performance and energy efficiency in mobile devices is apparent with the obvious shifting of more intensive computation to mobile platforms. In this paper, we first make a clear distinction between performance and energy issues. Apart from showing that performance efficiency is neither co-related with energy-efficiency nor inefficiency, we focus on programming methodologies and software validation approaches for producing energy efficient mobile software. These include reviewing recent works on energy-aware programming and nonfunctional testing to expose energy and performance issues in mobile software. As mobile platforms continue to evolve, new scenarios and use-cases involving mobile devices are on the rise. We speculate on scenarios involving energy hungry mobile software in near future, and how existing software engineering techniques can evolve to combat energy inefficiency in\u00a0\u2026", "num_citations": "18\n", "authors": ["272"]}
{"title": "Static analysis driven cache performance testing\n", "abstract": " Real-time, embedded software are constrained by several non-functional requirements, such as timing. With the ever increasing performance gap between the processor and the main memory, the performance of memory subsystems often pose a significant bottleneck in achieving the desired performance for a real-time, embedded software. Cache memory plays a key role in reducing the performance gap between a processor and main memory. Therefore, analyzing the cache behaviour of a program is critical for validating the performance of an embedded software. In this paper, we propose a novel approach to automatically generate test inputs that expose the cache performance issues to the developer. Each such test scenario points to the specific parts of a program that exhibit anomalous cache behaviour along with a set of test inputs that lead to such undesirable cache behaviour. We build a framework that\u00a0\u2026", "num_citations": "18\n", "authors": ["272"]}
{"title": "Bucketing failing tests via symbolic analysis\n", "abstract": " A common problem encountered while debugging programs is the overwhelming number of test cases generated by automated test generation tools, where many of the tests are likely to fail due to same bug. Some coarse-grained clustering techniques based on point of failure (PFB) and stack hash (CSB) have been proposed to address the problem. In this work, we propose a new symbolic analysis-based clustering algorithm that uses the semantic reason behind failures to group failing tests into more \u201cmeaningful\u201d clusters. We implement our algorithm within the KLEE symbolic execution engine; our experiments on 21 programs drawn from multiple benchmark-suites show that our technique is effective at producing more fine grained clusters as compared to the FSB and CSB clustering schemes. As a side-effect, our technique also provides a semantic characterization of the fault represented by each\u00a0\u2026", "num_citations": "16\n", "authors": ["272"]}
{"title": "Interacting process classes\n", "abstract": " Many reactive control systems consist of classes of active objects involving both intraclass interactions (i.e., objects belonging to the same class interacting with each other) and interclass interactions. Such reactive control systems appear in domains such as telecommunication, transportation and avionics. In this article, we propose a modeling and simulation technique for interacting process classes. Our modeling style uses standard notations to capture behavior. In particular, the control flow of a process class is captured by a labeled transition system, unit interactions between process objects are described as transactions, and the structural relations are captured via class diagrams. The key feature of our approach is that our execution semantics leads to an abstract simulation technique which involves (i) grouping together active objects into equivalence classes according their potential futures, and (ii) keeping\u00a0\u2026", "num_citations": "16\n", "authors": ["272"]}
{"title": "BesFS: A POSIX Filesystem for Enclaves with a Mechanized Safety Proof\n", "abstract": " New trusted computing primitives such as Intel SGX have shown the feasibility of running user-level applications in enclaves on a commodity trusted processor without trusting a large OS. However, the OS can still compromise the integrity of an enclave by tampering with the system call return values. In fact, it has been shown that a subclass of these attacks, called Iago attacks, enables arbitrary logic execution in enclave programs. Existing enclave systems have very large TCB and they implement ad-hoc checks at the system call interface which are hard to verify for completeness. To this end, we present BesFS\u2014the first filesystem interface which provably protects the enclave integrity against a completely malicious OS. We prove 167 lemmas and 2 key theorems in 4625 lines of Coq proof scripts, which directly proves the safety properties of the BesFS specification. BesFS comprises of 15 APIs with compositional safety and is expressive enough to support 31 real applications we test. BesFS integrates into existing SGX-enabled applications with minimal impact to TCB. BesFS can serve as a reference implementation for hand-coded API checks.", "num_citations": "15\n", "authors": ["272"]}
{"title": "Symbolic verification of cache side-channel freedom\n", "abstract": " Cache timing attacks allow third-party observers to retrieve sensitive information from program executions. But, is it possible to automatically check the vulnerability of a program against cache timing attacks and then, automatically shield program executions against these attacks? For a given program, a cache configuration and an attack model, our CacheFix framework either verifies the cache side-channel freedom of the program or synthesizes a series of patches to ensure cache side-channel freedom during program execution. At the core of our framework is a novel symbolic verification technique based on automated abstraction refinement of cache semantics. The power of such a framework allows symbolic reasoning over counterexample traces and combines it with runtime monitoring for eliminating cache side channels during program execution. Our evaluation with routines from OpenSSL\u00a0\u2026", "num_citations": "15\n", "authors": ["272"]}
{"title": "Static bus schedule aware scratchpad allocation in multiprocessors\n", "abstract": " Compiler controlled memories or scratchpad memories offer more predictable program execution times than cache memories. Scratchpad memories are often employed in multi-processor system-on-chip (MPSoC) platforms which seek to meet the performance needs of embedded applications while limiting power consumption and timing unpredictability. Scratchpad allocation schemes optimize performance while ensuring predictable execution times (as compared to caches). In this work, we develop a compile-time scratchpad allocation framework for multi-processor platforms, where the processors (virtually) share on-chip scratchpad space and external memory is accessed through a shared bus. Our allocation method considers the waiting time for bus access while deciding which memory blocks to load into the shared scratchpad memory space. Incorporating the bus schedule into our scratchpad allocation\u00a0\u2026", "num_citations": "15\n", "authors": ["272"]}
{"title": "Automatic generation of protocol converters from scenario-based specifications\n", "abstract": " Reuse of IP blocks is an important design philosophy for embedded systems. This allows shorter design cycles under tight time-to-market constraints. However, reusing IP blocks often requires designing converters (glue logic) to enable their communication. In this paper, we study the problem of automatically generating a protocol converter which enables various embedded system components (possibly with incompatible protocols) to talk to each other. Our work takes as input, a rich description of inter-component interactions described as a collection of message sequence charts. We then automatically synthesize from this input a protocol converter in SystemC. Our work is not restricted to uni-directional communication and the converter can be used to broker communication among many components. We demonstrate the feasibility of our approach by modelling some simplified bus protocols that capture key\u00a0\u2026", "num_citations": "15\n", "authors": ["272"]}
{"title": "Android testing via synthetic symbolic execution\n", "abstract": " Symbolic execution of Android applications is challenging as it involves either building a customized VM for Android or modeling the Android libraries. Since the Android Runtime evolves from one version to another, building a high-fidelity symbolic execution engine involves modeling the effect of the libraries and their evolved versions. Without simulating the behavior of Android libraries, path divergence may occur due to constraint loss when the symbolic values flow into Android framework and these values later affect the subsequent path taken. Previous works such as JPF-Android have relied on the modeling of execution environment such as libraries. In this work, we build a dynamic symbolic execution engine for Android apps, without any manual modeling of execution environment. Environment (or library) dependent control flow decisions in the application will trigger an on-demand program synthesis step to\u00a0\u2026", "num_citations": "14\n", "authors": ["272"]}
{"title": "Effects of branch prediction on worst case execution time of programs\n", "abstract": " Estimating the Worst Case Execution Time (WCET) of a program on a given hardware platform is useful in the design of embedded real-time systems. These systems communicate with the external environment in a timely fashion, and thus impose constraints on the execution time of programs. Estimating the WCET of a program ensures that these constraints are met. WCET analysis schemes typically model microarchitectural features in modern processors, such as pipeline and caches, to obtain tight estimates.  In this paper, we study the effects of speculative execution on WCET analysis. Speculative execution uses branch prediction to predict the outcome of a branch instruction based on past execution history. This allows the program execution to proceed by speculating the control flow. Branch predictions schemes can be local or global. Local schemes predict a branch outcome based exclusively on its own execution history whereas global schemes take into account the outcome of other branches as well. Current WCET analysis schemes have largely ignored the effect of branch prediction.  Our technique combines program analysis and microarchitectural modeling to estimate the effects of branch prediction. Starting from  the control flow graph of the program, we derive linear inequalities for bounding the number of mispredictions during execution (for all possible inputs). These constraints are then solved by any standard (integer) linear programming solver. Our technique models local as well global branch prediction in a uniform fashion. Although global branch prediction schemes are used in most modern processors, their effect on WCET\u00a0\u2026", "num_citations": "14\n", "authors": ["272"]}
{"title": "Smart Contract Repair\n", "abstract": " Smart contracts are automated or self-enforcing contracts that can be used to exchange assets without having to place trust in third parties. Many commercial transactions use smart contracts due to their potential benefits in terms of secure peer-to-peer transactions independent of external parties. Experience shows that many commonly used smart contracts are vulnerable to serious malicious attacks, which may enable attackers to steal valuable assets of involving parties. There is, therefore, a need to apply analysis and automated repair techniques to detect and repair bugs in smart contracts before being deployed. In this work, we present the first general-purpose automated smart contract repair approach that is also gas-aware. Our repair method is search-based and searches among mutations of the buggy contract. Our method also considers the gas usage of the candidate patches by leveraging our novel\u00a0\u2026", "num_citations": "13\n", "authors": ["272"]}
{"title": "Java memory model aware software validation\n", "abstract": " The Java Memory Model (JMM) provides a semantics of Java multithreading for any implementation platform. The JMM is defined in a declarative fashion with an allowed program execution being defined in terms of existence of\" commit sequences\"(roughly, the order in which actions in the execution are committed). In this work, we develop OpMM, an operational under-approximation of the JMM. The immediate motivation of this work lies in integrating a formal specification of the JMM with software model checkers. We show how our operational memory model description can be integrated into a Java Path Finder (JPF) style model checker for Java programs.", "num_citations": "13\n", "authors": ["272"]}
{"title": "Cache-related preemption delay analysis for multilevel noninclusive caches\n", "abstract": " With the rapid growth of complex hardware features, timing analysis has become an increasingly difficult problem. The key to solving this problem lies in the precise and scalable modeling of performance-enhancing processor features (e.g., cache). Moreover, real-time systems are often multitasking and use preemptive scheduling, with fixed or dynamic priority assignment. For such systems, cache related preemption delay (CRPD) may increase the execution time of a task. Therefore, CRPD may affect the overall schedulability analysis. Existing works propose to bound the value of CRPD in a single-level cache. In this article, we propose a CRPD analysis framework that can be used for a two-level, noninclusive cache hierarchy. In addition, our proposed framework is also applicable in the presence of shared caches. We first show that CRPD analysis faces several new challenges in the presence of a multilevel\u00a0\u2026", "num_citations": "12\n", "authors": ["272"]}
{"title": "Integrated timing analysis of application and operating systems code\n", "abstract": " Real-time embedded software often runs on a supervisory operating system software layer on top of a modern processor. Thus, to give timing guarantees on the execution time and response time of such applications, one needs to consider the timing effects of the operating system, such as system calls and interrupts - over and above modeling the timing effects of micro-architectural features such as pipeline and cache. Previous works on Worst-case Execution Time (WCET) analysis have focused on micro-architectural modeling while ignoring the operating system's timing effects. As a result, WCET analyzers only estimate the maximum un-interrupted execution time of a program. In this work, we present a framework for RTOS aware WCET analysis - where the timing effects of system calls and interrupts can be accounted for. The key observation behind our analysis is to capture the timing effects of system calls and\u00a0\u2026", "num_citations": "12\n", "authors": ["272"]}
{"title": "Debugging statecharts via model-code traceability\n", "abstract": " Model-driven software development involves constructing behavioral models from informal English requirements. These models are then used to guide software construction. The compilation of behavioral models into software is the topic of many existing research works. There also exist a number of UML-based modeling tools which support such model compilation. In this paper, we show how Statechart models can be validated/debugged by (a) generating code from the Statechart models, (b) employing established software debugging methods like program slicing on the generated code, and (c) relating the program slice back to the Statechart level. Our study is presented concretely in terms of dynamic slicing of Java code produced from Statechart models. The slice produced at the code level is mapped back to the model level for enhanced design comprehension. We use the open-source JSlice tool for\u00a0\u2026", "num_citations": "12\n", "authors": ["272"]}
{"title": "A rule-based data standardizer for enterprise data bases\n", "abstract": " Whenever a database permits textual entry of information| for example when data is copied from a paper form| the database is likely to contain duplicates and inconsistencies. These duplicates must be removed and inconsistencies resolved in order to mine the data or to use the data for decision support. We term the domain-speci c solution to duplicate and inconsistency removal data standardization. In this paper, we describe a Name-Address Standardizer, one of a series of standardizers that have proven critical in creating a new enterprise-level database for the US Customs Service. The standardizers were used to clean several legacy databases. These standardized databases were combined into a central database for which data is now standardized upon input.In practice, a standardizer uses techniques both from natural language analysis and from rulebased expert systems. As a result Prolog is highly suitable as a basis for standardizers. All Customs standardizers were written almost entirely in Prolog and constitute a large programming e ort: the Name-Address Standardizer contains about 100,000 lines of code, including generated parse tables and a fact base.", "num_citations": "12\n", "authors": ["272"]}
{"title": "Fuzzing: Challenges and Reflections\n", "abstract": " Fuzzing is a method to discover software bugs and vulnerabilities by automatic test input generation which has found tremendous recent interest in both academia and industry. Fuzzing comes in the form of several techniques. On one hand, we have symbolic execution, which enables a particularly effective approach to fuzzing by systematically enumerating the paths of a program. On the other hand, we have random input generation, which generates large amounts of inputs per second with none or minimal program analysis overhead. In this article, we summarize the open challenges and opportunities for fuzzing and symbolic execution as they emerged in discussions among researchers and practitioners in a Shonan Meeting, and were validated in a subsequent survey. We take a forward-looking view of the software vulnerability discovery technologies and provide concrete directions for future research.", "num_citations": "10\n", "authors": ["272"]}
{"title": "Binary Rewriting without Control Flow Recovery\n", "abstract": " Static binary rewriting has many important applications in software security and systems, such as hardening, repair, patching, instrumentation, and debugging. While many different static binary rewriting tools have been proposed, most rely on recovering control flow information from the input binary. The recovery step is necessary since the rewriting process may move instructions, meaning that the set of jump targets in the rewritten binary needs to be adjusted accordingly. Since the static recovery of control flow information is a hard problem in general, most tools rely on a set of simplifying heuristics or assumptions, such as specific compilers, specific source languages, or binary file meta information. However, the reliance on assumptions or heuristics tends to scale poorly in practice, and most state-of-the-art static binary rewriting tools cannot handle very large/complex programs such as web browsers.", "num_citations": "10\n", "authors": ["272"]}
{"title": "Debugging as a Science, that too, when your Program is Changing\n", "abstract": " Program debugging is an extremely time-consuming process, and it takes up a large portion of software development time. In practice, debugging is still very much of an art, with the developer painstakingly going through volumes of execution traces to locate the actual cause of an observable error. In this work, we discuss recent advances in debugging which makes it systematic scientific activity in its own right. We explore the delicate connections between debugging and formal methods (such as model checking) in the overall task of validating software. Moreover, since any deployed software undergoes changes in its lifetime, we need debugging methods which can take the software evolution into account. We show how symbolic execution and Satisfiability Modulo Theories (SMT) solvers can be gainfully employed to greatly automate software debugging of evolving programs.", "num_citations": "10\n", "authors": ["272"]}
{"title": "A framework to model branch prediction for worst case execution time analysis\n", "abstract": " Estimating the Worst Case Execution Time (WCET) of a program on a given hardware platform is useful in the design of embedded real-time systems. These systems communicate with the external environment in a timely fashion, and thus impose constraints on the execution time of programs. Estimating the WCET of a program ensures that these constraints are met. WCET analysis schemes typically model micro-architectural features in modern processors, such as pipeline and caches, to obtain tight estimates. In this paper, we study the effects of dynamic branch prediction on WCET analysis. Branch prediction schemes predict the outcome of a branch instruction based on past execution history. This allows the program execution to proceed by speculating the control flow. Branch predictions schemes can be local or global. Local schemes predict a branch outcome based exclusively on its own execution history whereas global schemes take into account the outcome of other branches as well. Current WCET analysis schemes have largely ignored the effect of branch prediction. Our technique combines program analysis and microarchitectural modeling to estimate the effects of branch prediction. Starting from the control flow graph of the program, we derive linear inequalities for bounding the number of mispredictions during execution (for all possible inputs). These constraints are then solved by any standard integer linear programming solver. Our technique models local as well global branch prediction in a uniform fashion. Although global branch prediction schemes are used in most modern processors, their effect on WCET has not been\u00a0\u2026", "num_citations": "10\n", "authors": ["272"]}
{"title": "Program performance spectrum\n", "abstract": " Real-time and embedded applications often need to satisfy several non-functional properties such as timing. Consequently, performance validation is a crucial stage before the deployment of real-time and embedded software. Cache memories are often used to bridge the performance gap between a processor and memory subsystems. As a result, the analysis of caches plays a key role in the performance validation of real-time, embedded software. In this paper, we propose a novel approach to compute the cache performance signature of an entire program. Our technique is based on exploring the input domain through different path programs. Two paths belong to the same path program if they follow the same set of control flow edges but may vary in the iterations of loops encountered. Our experiments with several subject programs show that the different paths grouped into a path program have very similar and\u00a0\u2026", "num_citations": "9\n", "authors": ["272"]}
{"title": "Time-predictable embedded software on multi-core platforms: Analysis and optimization\n", "abstract": " Multi-core architectures have recently gained popularity due to their high-performance and low-power characteristics. Most of the modern desktop systems are now equipped with multi-core processors. Despite the wide-spread adaptation of multi-core processors in desktop systems, using such processors in embedded systems still poses several challenges. Embedded systems are often constrained by several extra-functional aspects, such as time. Therefore, providing guarantees for time-predictable execution is one of the key requirements for embedded system designers. Multi-core processors adversely affect the time-predictability due to the presence of shared resources, such as shared caches and shared buses. In this contribution, we shall first discuss the challenges imposed by multi-core architectures in designing time-predictable embedded systems. Subsequently, we shall describe, in details, a\u00a0\u2026", "num_citations": "8\n", "authors": ["272"]}
{"title": "Footprinter: Round-trip engineering via scenario and state based models\n", "abstract": " Formal behavioral models are used in model-driven software development to analyze and reason about system behavior. While scenario-based models highlighting interprocess communication are closer to distributed system requirements, state-based models highlighting intra-process behavior are suitable for code generation. In this paper we present dasiaFootprinterpsila, a tool which exploits the relative strengths of these two modeling styles in support of a roundtrip engineering approach from requirements, to test case generation and execution, to tracing test execution results back to requirements - enabling debugging of test execution failures at requirements level.", "num_citations": "8\n", "authors": ["272"]}
{"title": "Timing analysis of body area network application\n", "abstract": " Body area network (BAN) applications have stringent timing requirements. The timing behavior of a BAN application is determined not only by the software complexity, inputs, and architecture, but also by the timing behavior of the peripherals. This paper presents systematic timing analysis of such applications, deployed for health-care monitoring of patients staying at home. This monitoring is used to achieve prompt notification of the hospital when a patient shows abnormal vital signs. Due to the safetycritical nature of these applications, worst-case execution time (WCET) analysis is extremely important. 1.", "num_citations": "8\n", "authors": ["272"]}
{"title": "Synthesis and traceability of scenario-based executable models\n", "abstract": " Message Sequence Charts (MSCs) or Sequence Diagrams are one of the behavioral diagram types in the unified modeling language or UML. In system requirements modeling, MSCs are conventionally used for describing possible system scenarios. In the recent past, there have been concerted attempts to develop executable system modeling languages directly based on MSCs - Live Sequence Charts, Triggered Message Sequence Charts and Interacting Process Classes, to name a few. In this paper, we study the problem of model synthesis in these languages - how to translate informal requirements into formal models. We also discuss (a) test generation from these formal models, and (b) how the generated tests can be traced back to the informal requirements.", "num_citations": "8\n", "authors": ["272"]}
{"title": "Handling constraints in multi-objective ga for embedded system design\n", "abstract": " Design space exploration is central to embedded system design. Typically this is a multi-objective search problem, where performance, power, area etc. are the different optimization criteria, to find the Pareto-optimal points. Multi-objective genetic algorithms (GA) have been found to be a natural fit for such searches and have been used widely. However, for certain design spaces, a large part of the space being explored by GA may violate certain design constraints. In this paper, we use a multi-objective GA algorithm based on \"repair\", where an infeasible design point encountered during the search is repaired to a feasible design point. Our primary novelty is to use a multi-objective version of search algorithms, like branch and bound, as the repair strategy to optimize the objectives. We also pre-compute a layout of the genes such that infeasible design points are less likely to be encountered during the search. We\u00a0\u2026", "num_citations": "8\n", "authors": ["272"]}
{"title": "Unfold/fold transformations for automated verification of parameterized concurrent systems\n", "abstract": " Formal verification of reactive concurrent systems is important since many hardware and software components of our computing environment can be modeled as reactive concurrent systems. Algorithmic techniques for verifying concurrent systems such as model checking can be applied to finite state systems only. This chapter investigates the verification of a common class of infinite state systems, namely parameterized systems. Such systems are parameterized by the number of component processes, for example an n-process token ring for any n. Verifying the entire infinite family represented by a parameterized system lies beyond the reach of traditional model checking. On the other hand, deductive techniques to verify infinite state systems often require substantial user guidance.               The goal of this work is to integrate algorithmic and deductive techniques for automating proofs of temporal properties\u00a0\u2026", "num_citations": "8\n", "authors": ["272"]}
{"title": "Energy-aware design patterns for mobile application development (invited talk)\n", "abstract": " Developing energy-efficient application is crucial for mobile platforms such as smartphone and tablets, since such devices operate on a limited amount of battery power. However, until recently most of the smartphone applications have been developed in an energy-oblivious fashion. This is increasingly becoming a concern due to the fact that smartphone applications are progressively becoming complex and energy-intensive, whereas the battery technology is unable to keep up. Existing studies have proposed a number of testing and re-factoring techniques that can be used to increase the energy-efficiency of such applications, after the development has been completed. However, we feel that maximum level of energy-efficiency can be achieved only if energy-efficient design practices are used in the software development process. In this study, we propose a set of energy-aware design patterns, specifically\u00a0\u2026", "num_citations": "7\n", "authors": ["272"]}
{"title": "Schedulability analysis of MSC-based system models\n", "abstract": " Message sequence charts (MSCs) are widely used for describing interaction scenarios between the components of a distributed system. Consequently, worst-case response time estimation and schedulability analysis of MSC-based specifications form natural building blocks for designing distributed real-time systems. However, currently there exists a large gap between the timing and quantitative performance analysis techniques that exist in the real-time systems literature, and the modeling/specification techniques that are advocated by the formal methods community. As a result, although a number of schedulability analysis techniques are known for a variety of task graph-based models, it is not clear if they can be used to effectively analyze standard specification formalisms such as MSCs. In this paper we make an attempt to bridge this gap by proposing a schedulability analysis technique for MSC-based system\u00a0\u2026", "num_citations": "7\n", "authors": ["272"]}
{"title": "Communicating transaction processes: An msc-based model of computation for reactive embedded systems\n", "abstract": " Message Sequence Charts (MSC) have been traditionally used to depict execution scenarios in the early stages of design cycle. MSCs portray inter-object interactions. Synthesizing intra-object executable specifications from an MSC-based description is a non-trivial task. Here we present a model of computation called Communicating Transaction Processes (CTP) based on MSCs from which an executable specification can be extracted in a straightforward manner. Our model describes a network of communicating processes in which the processes interact via common action labels. Each action is a non-atomic interaction described as a guarded choice of MSCs. Thus our model achieves a separation of concerns: the high-level network of processes depicting intra-process computations and control flow, while the common non-atomic communication actions capture inter-process interaction viaMSCs. We\u00a0\u2026", "num_citations": "7\n", "authors": ["272"]}
{"title": "Formal reasoning about hardware and software memory models\n", "abstract": " The Java programming language allows multithreaded programming, where threads can be run on multiprocessor or uniprocessor platforms. The allowed behaviors of any multithreaded Java program on any implementation platform (multi- or uni-processor), are described in terms of a memory consistency model called the Java Memory Model (JMM). However, shared memory multiprocessors have a memory model of their own. To reason about the behavior of multithreaded Java programs on multiprocessors, we need a formal basis for understanding both the hardware memory model (of the multiprocessor platform) and the software memory model (the JMM). For this purpose, we have implemented formal executable specifications of the JMM and certain hardware memory models (such as TSO/PSO from SPARC). These executable specifications can be used for exhaustive search i.e. computing all\u00a0\u2026", "num_citations": "7\n", "authors": ["272"]}
{"title": "Program transformations for verifying parameterized systems\n", "abstract": " Formal verification of reactive concurrent systems is important since many hardware and software components of our computing environment can be modeled as reactive concurrent systems. Algorithmic techniques for verifying concurrent systems such as model checking can be applied to only finite state systems. This dissertation investigates the verification of a common class of infinite state systems, namely parameterized systems. Such systems are parameterized by the number of component processes, for example an n process token ring for any n. Verifying the entire infinite family represented by a parameterized system lies beyond the reach of traditional model checking. On the other hand, deductive techniques to verify infinite state systems often require substantial user guidance.", "num_citations": "7\n", "authors": ["272"]}
{"title": "Proofs by program transformations\n", "abstract": " Logic program transformation systems are often described as a collection of unfolding, folding and goal replacement transformation rules. Given a program \uff67, a logic program transformation system derives a sequence of programs \uff67\uff69 \uff68 \uff67 \uff67!\" \uff67 $#, such that for all%'&)(1 03 2, \uff67 465 is obtained from \uff67 4 by application of one of the above rules.", "num_citations": "7\n", "authors": ["272"]}
{"title": "Security analysis of unmanned aircraft systems\n", "abstract": " Security is a big concern in widely adopting security critical systems, such as Unmanned Aerial Vehicles (UAV). To ensure security of a system, the  rst step is to identify the required security properties as well as the potential attacks, i.e., security requirements. We identify a set of security requirements for UAV systems which is more complete and in more details than existing works. To facilitate formal analysis of a system against the set of requirements, we propose algorithms to automatically generate attack trees from the requirement set for the developers and designers to have a better understanding of the potential risks of a UAV system. Moreover, we propose an algorithm to automatically generate attack models from the attack tree and associate each attack model with the expected security properties. Given a UAV system model representing the honest behavior of participants, we are able to verify whether the system su ers from some potential attacks, by feeding the automatically generated attack models and the UAV system model to a model checker to verify the associated security properties.", "num_citations": "6\n", "authors": ["272"]}
{"title": "An executable specification language based on message sequence charts\n", "abstract": " Message Sequence Charts (MSCs) are an appealing visual formalism that play a useful role in the early design stages of reactive systems such as telecommunication protocols. They also constitute one of the behavioral diagram types in the UML framework [4]. MSCs are usually intended to capture system requirements. However there is no standard relationship between such requirements and an executable specification . Here we deploy MSCs instead as refinements of actions at the executable level by formulating a state-based model called Cyclic Transaction Processes. We provide a transition system semantics for the CTP model as also a detailed example to illustrate its modeling and behavioral features.", "num_citations": "6\n", "authors": ["272"]}
{"title": "Introducing model checking to undergraduates\n", "abstract": " Introducing temporal logics and model checking to undergraduate students is usually an involved activity. The difficulty stems from the students\u2019 lack of exposure to logics, unfamiliarity with reactive systems and lack of conviction that model checking search can lead to anything practical. Here, I narrate some experiences in attempting to overcome these stereotypes over a period of five years at the National University of Singapore.", "num_citations": "5\n", "authors": ["272"]}
{"title": "Interactive patch generation and suggestion\n", "abstract": " Automated program repair (APR) is an emerging technique that can automatically generate patches for fixing bugs or vulnerabilities. To ensure correctness, the auto-generated patches are usually sent to developers for verification before applied in the program. To review patches, developers must figure out the root cause of a bug and understand the semantic impact of the patch, which is not straightforward and easy even for expert programmers. In this position paper, we envision an interactive patch suggestion approach that avoids such complex reasoning by instead enabling developers to review patches with a few clicks. We first automatically translate patch semantics into a set of what and how questions. Basically, the what questions formulate the expected program behaviors, while the how questions represent how to modify the program to realize the expected behaviors. We could leverage the existing APR\u00a0\u2026", "num_citations": "4\n", "authors": ["272"]}
{"title": "Regression testing of evolving programs\n", "abstract": " Software changes, such as bug fixes or feature additions, can introduce software bugs and reduce the code quality. As a result tests which passed earlier may not pass any more\u2014thereby exposing a regression in software behavior. This survey overviews recent advances in determining the impact of the code changes onto the program\u2019s behavior and other syntactic program artifacts. Static program analysis can help determining change impact in an approximate manner while dynamic analysis determines change impact more precisely but requires a regression test suite. Moreover, as the program is changed, the corresponding test suite may, too. Some tests may become obsolete while other tests are to be augmented that stress the changes. This article surveys such test generation techniques to stress and propagate program changes. It concludes that a combination of dependency analysis and lightweight\u00a0\u2026", "num_citations": "4\n", "authors": ["272"]}
{"title": "Cache-aware optimization of BAN applications\n", "abstract": " Body-area sensor network or BAN-based health monitoring is increasingly becoming a popular alternative to traditional wired biomonitoring techniques. However, most biomonitoring applications need continuous processing of large volumes of data, as a result of which both power consumption and computation bandwidth turn out to be serious constraints for sensor network platforms. This has resulted in a lot of recent interest in design methods, modeling and software analysis techniques specifically targeted towards BANs and applications running on them. In this paper we show that appropriate optimization of the application running on the communication gateway of a wireless BAN and accurate modeling of the microarchitectural details of the gateway processor can lead to significantly better resource usage and power savings. In particular, we propose a method for deriving the optimal order in which the\u00a0\u2026", "num_citations": "4\n", "authors": ["272"]}
{"title": "Impact of Java memory model on out-of-order multiprocessors\n", "abstract": " The semantics of Java multithreading dictates all possible behaviors that a multithreaded Java program can exhibit on any platform. This is called the Java memory model (JMM) and describes the allowed reorderings among the memory operations in a thread. However, multiprocessor platforms traditionally have memory consistency models of their own. In this paper, we study the interaction between the JMM and the multiprocessor memory consistency models. In particular, memory barriers may have to be inserted to ensure that the multiprocessor execution of a multithreaded Java program respects the JMM. We study the impact of these additional memory barriers on program performance. Our experimental results indicate that the performance gain achieved by relaxed hardware memory consistency models far exceeds the performance degradation due to the introduction of JMM.", "num_citations": "4\n", "authors": ["272"]}
{"title": "SemFix and beyond: semantic techniques for program repair\n", "abstract": " Automated program repair is of great promise for future programming environments. It is also of obvious importance for patching vulnerabilities in software, or for building self-healing systems for critical infra-structure. Traditional program repair techniques tend to lift the fix from elsewhere in the program via syntax based approaches. In this talk, I will mention how the search problems in program repair can be solved by semantic analysis techniques. Here semantic analysis methods are not only used to guide the search, but also for extracting formal specifications from tests. I will conclude with positioning of the syntax based and semantic based methods for vulnerability patching, future generation programming, and self-healing systems.", "num_citations": "3\n", "authors": ["272"]}
{"title": "Depiction and playout of multi-threaded program executions\n", "abstract": " Execution of a shared memory multi-threaded program is non-deterministic even for a fixed input. Consequently, a limited amount of the program behavior should be traced and recorded during run-time. However, if the tracing overheads are too high, we have the risk of slowing down the program considerably and even distorting the program behavior. In this paper, we propose to collect and store only the synchronization dependencies during run-time. These dependences are visualized as a message sequence chart (MSC). We do not record the data dependences across threads resulting from unsynchronized reads and writes of a shared variable. Instead all possible orderings of unsynchronized reads/writes are analyzed post-mortem. To describe all these behaviors, we use an important extension of message sequence charts called live sequence charts (LSC). Our MSC/LSC based description of a multi\u00a0\u2026", "num_citations": "3\n", "authors": ["272"]}
{"title": "A generalized unfold/fold transformation system for definite logic programs\n", "abstract": " CiteSeerX \u2014 A Generalized Unfold/Fold Transformation System for Definite Logic Programs Documents Authors Tables Log in Sign up MetaCart DMCA Donate CiteSeerX logo Documents: Advanced Search Include Citations Authors: Advanced Search Include Citations Tables: DMCA A Generalized Unfold/Fold Transformation System for Definite Logic Programs (1998) Cached Download as a PDF Download Links [www.cs.sunysb.edu] Save to List Add to Collection Correct Errors Monitor Changes by Abhik Roychoudhury , K. Narayan Kumar , CR Ramakrishnan , IV Ramakrishnan Citations: 3 - 2 self Summary Citations Active Bibliography Co-citation Clustered Documents Version History Share Facebook Twitter Reddit Bibsonomy OpenURL Abstract this paper. Keyphrases generalized unfold fold transformation system definite logic program Powered by: Apache Solr About CiteSeerX Submit and Index Documents at \u2026", "num_citations": "3\n", "authors": ["272"]}
{"title": "Making success out of early failures\n", "abstract": " Promoting early failure of unsuccessful computations is a powerful optimization that enhances determinism in the evaluation of logic programs. This optimization has two principal components: 1) identifying and extracting conditions that yield information about the eventual success or failure of predicates, 2) exploiting these conditions to avoid unnecessary computation. For the rst part sophisticated compile-time global analysis methods exist that compute the necessary conditions for a predicate and its constituent clauses to succeed. In this paper we address the other problem, namely the exploitation of necessary conditions. Speci cally we develop a technique for selecting tests from necessary conditions based on an analysis of their costs and bene ts. The resulting optimized program does not create any more choice points or failure paths than the original program. More importantly, success path lengths in the optimized program can never grow any longer than the corresponding paths in the original program. We also discuss possible re nements to improve our optimization.", "num_citations": "3\n", "authors": ["272"]}
{"title": "Fitness guided vulnerability detection with greybox fuzzing\n", "abstract": " Greybox fuzzing is an automated test-input generation technique that aims to uncover program errors by searching for bug-inducing inputs using a fitness-guided search process. Existing fuzzing approaches are primarily coverage-based. That is, they regard a test input that covers a new region of code as being fit to be retained. However, a vulnerability at a program location may not get exhibited in every execution that happens to visit to this program location; only certain program executions that lead to the location may expose the vulnerability. In this paper, we introduce a unified fitness metric called headroom, which can be used within greybox fuzzers, and which is explicitly oriented towards searching for test inputs that come closer to exposing vulnerabilities.", "num_citations": "2\n", "authors": ["272"]}
{"title": "A Systems Approach to Cyber Security: Proceedings of the 2nd Singapore Cyber-Security R&D Conference (SG-CRC 2017)\n", "abstract": " With our ever-increasing reliance on computer technology in every field of modern life, the need for continuously evolving and improving cyber security remains a constant imperative. This book presents the 3 keynote speeches and 10 papers delivered at the 2nd Singapore Cyber Security R&D Conference (SG-CRC 2017), held in Singapore, on 21-22 February 2017. SG-CRC 2017 focuses on the latest research into the techniques and methodologies of cyber security. The goal is to construct systems which are resistant to cyber-attack, enabling the construction of safe execution environments and improving the security of both hardware and software by means of mathematical tools and engineering approaches for the design, verification and monitoring of cyber-physical systems. Covering subjects which range from messaging in the public cloud and the use of scholarly digital libraries as a platform for malware distribution, to low-dimensional bigram analysis for mobile data fragment classification, this book will be of interest to all those whose business it is to improve cyber security.", "num_citations": "2\n", "authors": ["272"]}
{"title": "Locating failure-inducing environment changes\n", "abstract": " Traditionally, debugging refers to the process of locating the program portions which are responsible for a program failure. However, a program also fails when the execution environment does not meet the requirement/assumption of the program. Unfortunately, few existing debugging techniques addresses the problem of changing operating system environment. In this paper, we propose an effective record-replay technique called Semi-replay to solve this problem. Semi-replay records all the essential interactions between an application and its underlying operating system environment where it successfully executed. Semi-replay then allows the recorded interactions to be partially replayed and partially executed in another operating system to identify those interactions which contribute to the root cause of the application failure induced by the environment changes. We have conducted three case studies on real-life\u00a0\u2026", "num_citations": "2\n", "authors": ["272"]}
{"title": "A systematic classification and detection of infeasible paths for accurate WCET analysis of esterel programs\n", "abstract": " Synchronous programming languages like Esterel are widely used in safety-critical domains like avionics. However, it is only with the recent development of mature worst-case execution time (WCET) analysis tools that progress is being made on systematically studying the WCET analysis problem for languages like Esterel. In this context, we present techniques for methodically classifying and detecting different types of infeasible paths that arise while compiling Esterel programs into executable code, via high-level languages such as C. Our experimental results with well-known benchmarks show that the infeasible paths detected using our techniques result in as much as 36.5% reduction in the WCET estimates, compared to when no infeasible path detection is employed.", "num_citations": "2\n", "authors": ["272"]}
{"title": "Timing analysis of body area network applications\n", "abstract": " Body area network (BAN) applications have stringent timing requirements. The timing behavior of a BAN application is determined not only by the software complexity, inputs, and architecture, but also by the timing behavior of the peripherals. This paper presents systematic timing analysis of such applications, deployed for health-care monitoring of patients staying at home. This monitoring is used to achieve prompt notification of the hospital when a patient shows abnormal vital signs. Due to the safetycritical nature of these applications, worst-case execution time (WCET) analysis is extremely important.", "num_citations": "2\n", "authors": ["272"]}
{"title": "Darwin: An approach for debugging evolving programs\n", "abstract": " Bugs in programs are often introduced when programs evolve from a stable version to a new version. In this paper, we propose an new approach called Darwin for automatically finding potential root causes of such bugs. Given two programs, a reference program and a modified program, and an input that fails on the modified program, our approach uses symbolic execution to automatically synthesize a new input that (a) is very similar to the failing input, and (b) does not fail. We find the potential cause (s) of failure by comparing control flow behavior of the passing and failing inputs and identifying code fragments where the control flow diverge. A notable feature of our approach is that it handles hard-to-explain bugs like code missing errors by pointing to code in the reference program. We have implemented this approach and conducted experiments using several real world applications such as the Apache web server, libPNG (a library for manipulating PNG images), and TCPflow (a program for displaying data sent through TCP connections). In each of these applications, Darwin was able to localize bugs with high accuracy. Even these applications contain several thousands lines of code, Darwin could usually narrow down the potential root causes to less than 10 lines. In addition, we find that the inputs synthesized by Darwin provide additional value by revealing other undiscovered errors or suggesting fixes to buggy inputs.", "num_citations": "2\n", "authors": ["272"]}
{"title": "Formal metatheory using implicit syntax, and an application to data abstraction for asynchronous systems\n", "abstract": " Abstraction is a useful tool in verification, often allowing the proof of correctness of a large and complex system to be reduced to showing the correctness of a much smaller simpler system. We use the Nuprl theorem prover to verify the correctness of a simple but commonly occurring abstraction. From the formal proof, we extract a program that succeeds when the abstraction method is applicable to the concrete input specification and in this case, computes the abstracted system specification. One of the main novelties of our work is our \u201cimplicit syntax\u201d approach to formal metatheory of programming languages. Our proof relies entirely on semantic reasoning, and thus avoids the complications that often arise when formally reasoning about syntax. The semantic reasoning contains an implicit construction of the result using inductive predicates over semantic domains that express representability in a particular\u00a0\u2026", "num_citations": "2\n", "authors": ["272"]}
{"title": "Program transformations for automated verification of parameterized concurrent systems\n", "abstract": " We show how the problem of verifying parameterized systems can be reduced to the problem of determining the equivalence of goals in a logic program. We further show how goal equivalences can be established using induction-based proofs. Such proofs rely on a powerful new theory of logic program transformations (encompassing unfold, fold and goal replacement transformations). We present this theory of logic program transformations which in particular, allows a more general folding rule (as compared to the state of the art). We show how our more general transformations are useful for constructing verification proofs of parameterized systems. Moreover these verification proofs can be largely automated, and are applicable to a variety of network topologies, including uni-and bi-directional chains, rings, and trees of processes. Unfold transformations in our system correspond to algorithmic model-checking steps, fold and goal replacement correspond to deductve steps. All three types of transfo...", "num_citations": "2\n", "authors": ["272"]}
{"title": "A conservative technique to improve deterministic evaluation of logic programs\n", "abstract": " The performance of logic programs can be significantly improved by reducing nondeterminism in their evaluation using techniques for early pruning of computation paths that would eventually fail. Using static information gleaned from the program, we can identify (simple) conditions that must hold for certain computation paths to succeed, and test them before searching along those paths. However, naive introduction of such tests can actually lead to performance degradation since tests may be repeated along a branch, and also because the tests themselves may create additional choice points. We therefore develop a program transformation algorithm that enables us to introduce only those tests that facilitate early pruning of failure branches, while providing formal guarantees against any performance degradation. Our transformation is based on a novel polyvariant program specialization technique that can reason\u00a0\u2026", "num_citations": "2\n", "authors": ["272"]}
{"title": "Welcome.\n", "abstract": " \u2022 Regional collaboration of 13 Northeast/Mid-Atlantic states, working to improve transportation, develop the clean energy economy, and reduce transportation emissions\u2022 Energy and Environmental Affairs (EEA), Department of Environmental Protection (MassDEP), Department of Transportation (MassDOT) working together to reduce carbon emissions through a \u201ccap-andinvest\u201d program or other mechanism that establishes a price for transportation emissions", "num_citations": "1\n", "authors": ["272"]}
{"title": "Singapore's cybersecurity ecosystem\n", "abstract": " WITTER. COM/SICWSG and distribution plant, to help operators investigate and respond to attacks timely and comprehensively without advanced cybersecurity skills. A key novelty of VVATER is its ability for visualizing the interconnection of various infrastructures in historical plant operation and path of attacks in complex scenarios, as well as the resulting process anomalies and whether or not the anomaly is detected.", "num_citations": "1\n", "authors": ["272"]}
{"title": "Cache-related preemption delay analysis for fifo caches\n", "abstract": " Hard real-time systems are typically composed of multiple tasks, subjected to timing constraints. To guarantee that these constraints will be respected, the Worst-Case Response Time (WCRT) of each task is needed. In the presence of systems supporting preemptible tasks, we need to take into account the time lost due to task preemption. A major part of this delay is the Cache-Related Preemption Delay (CRPD), which represents the penalties due to cache block evictions by preempting tasks. Previous works on CRPD have focused on caches with Least Recently used (LRU) replacement policy. However, for many real-world processors such as ARM9 or ARM11, the use of First-in-first-out (FIFO) cache replacement policy is common.", "num_citations": "1\n", "authors": ["272"]}
{"title": "Theoretical Aspects of Computing-ICTAC 2012: 9th International Colloquium, Bangalore, India, September 24-27, 2012, Proceedings\n", "abstract": " This book constitutes the refereed proceedings of the 9th International Colloquium on Theoretical Aspects of Computing, ICTAC 2012 held in Bangalore, India, in September 2012. The 16 revised full papers presented together with the abstracts of three keynote talks were carefully reviewed and selected from 73 submissions. The papers cover various topics related to both theoretical aspects of computing and the exploitation of theory through methods and tools for system development.", "num_citations": "1\n", "authors": ["272"]}
{"title": "Comprehensive test suite augmentation\n", "abstract": " Test Suite Augmentation (TSA) is based on the assumption that an existing test suite T already determines whether a program P works properly. When P evolves to P0, a comprehensive set of test cases that witness the changed behavior shall be generated and augmented to T. We observe that a syntactic change may be propagated in different ways, thereby manifesting itself as different semantic changes (in terms of observable program behavior). We present a path exploration technique that seeks to generate one change revealing test case for every semantic change, rather than generate multiple test cases which execute the same change and propagate it in the same fashion to the program output.   Our change-based path exploration technique 1) finds all paths that execute at least one changed statement, 2) finds all paths along which the impact of a syntactic change is propagated to at least one output statement and 3) generates for both programs P and P0 a comprehensive number of test cases, each of which exposes a different semantic change. If time is bounded during test generation, the semantic difference is computed over those change partitions that have been explored. Our method is not restricted by the number of changes across the two program versions being analyzed - there can be multiple changes across the program versions.  The results from our experiments suggest that by covering the syntactic changes, or even by considering possible ways of propagating the effects of these syntactic changes - we cannot hope to uncover all possible differences in behavior across two program versions. In other words, analysis of both\u00a0\u2026", "num_citations": "1\n", "authors": ["272"]}
{"title": "Tenant Onboarding in Evolving Multi-tenant SaaS\n", "abstract": " A multi-tenant software as a service (SaaS) system has to meet the needs of several tenant organizations, which connect to the system to utilize its services. To leverage economies of scale through re-use, a SaaS vendor would, in general, like to drive commonality amongst the requirements across tenants. However, many tenants will also come with some custom requirements that may be a pre-requisite for them to adopt the SaaS system. These requirements then need to be addressed by evolving the SaaS system in a controlled manner, while still supporting the requirements of existing tenants. In this paper, we study the challenges associated with engineering multi-tenant SaaS systems and develop a framework to help evolve such systems in a systematic manner. We adopt an intuitive formal model of services that is easily amenable to tenant requirement analysis and provides a robust way to support multiple tenant onboarding. We perform a substantial case study of a multi-tenant blog server to demonstrate the benefits of our proposed approach.", "num_citations": "1\n", "authors": ["272"]}
{"title": "WOMM: a weak operational memory model\n", "abstract": " Memory models of shared memory concurrent programs define the values a read of a shared memory location is allowed to see. Such memory models are typically weaker than the intuitive sequential consistency semantics to allow efficient execution. In this paper, we present WOMM (abbreviation for Weak Operational Memory Model) that formally unifies two sources of weak behavior in hardware memory models: reordering of instructions and weakly consistent memory. We show that a large number of optimizations are allowed by WOMM. We also show that WOMM is weaker than a number of hardware memory models. Consequently, if a program behaves correctly under WOMM, it will be correct with respect to those hardware memory models. Hence, WOMM can be used as a formally specified abstraction of the hardware memory models. Moreover, unlike most weak memory models, WOMM is described\u00a0\u2026", "num_citations": "1\n", "authors": ["272"]}
{"title": "Cache-aware optimization of BAN applications\n", "abstract": " Body-area sensor network or BAN-based health monitoring is increasingly becoming a popular alternative to traditional wired bio-monitoring techniques. However, most biomonitoring applications need continuous processing of large volumes of data, as a result of which both power consumption and computation bandwidth turn out to be serious constraints for sensor network platforms. This has resulted in a lot of recent interest in design methods, modeling and software analysis techniques specifically targeted towards BANs and applications running on them. In this paper we show that appropriate optimization of the application running on the communication gateway of a wireless BAN and accurate modeling of the microarchitectural details of the gateway processor can lead to significantly better resource usage and power savings. In particular, we propose a method for deriving the optimal order in which\u00a0\u2026", "num_citations": "1\n", "authors": ["272"]}
{"title": "Darwin-An Approach for Debugging Evolving Programs\n", "abstract": " Programmers do not write programs entirely from scratch. Rather, over time, a program gradually evolves with more features possibly being added. In industrial software development projects, this complexity (of software evolution) is explicitly managed via checking in of program versions. Validation of such evolving programs (say, to address possible bugs introduced via program changes) remains a huge problem in terms of program development. This adds to the cost for software maintenance, which is much larger than the initial software development cost. The cost of maintaining a software and managing its evolution is said to account for more than 90% of the total cost of a software project, prompting certain authors to call it as the \u201clegacy crisis\u201d[1]. To tackle the ever-growing problem of software evolution and maintenance, software testing methodologies have long been studied. Regression testing is a well-known concept which is currently employed in any software development project. In its simplest form, it involves re-testing a test-suite as a program moves from one version to another. In the past, lot of research work has been devoted to finding out which tests in a given test-suite do not need to be tested for the new program version (eg, see [2]). However, even among the tests which are tested in both program versions (old and new)\u2014how do we find the root cause of a failed test case? Such failed test cases expose so-called \u201cregression bugs\u201d\u2014the test cases pass in the old version, but fail in the new version. For any large software development project (particularly for commercial software products which regularly add features based on\u00a0\u2026", "num_citations": "1\n", "authors": ["272"]}
{"title": "Software model backward association\n", "abstract": " Model-driven software development involves generating code from behavioral models such as Statecharts. The compilation of behavioral models into software is the topic of many existing research works. There also exist a number of UML-based modeling tools which support such model compilation. In this paper, we bring up the backward link between these layers\u2013when the generated software is debugged, how do we reflect the bug-report at the model level? This backward association between software and its design models is crucial for communicating changes in the software to the design level. Our study is presented concretely in terms of slicing of Java code produced from Statechart models; the slice produced at the code level is mapped back to the model level for enhanced design comprehension.", "num_citations": "1\n", "authors": ["272"]}
{"title": "Analyzing loop paths for execution time estimation\n", "abstract": " Statically estimating the worst case execution time of a program is important for real-time embedded software. This is difficult even in the programming language level due to the inherent difficulty in detecting infeasible paths in a program\u2019s control flow graph. In this paper, we study the problem of accurately bounding the execution time of a program loop. This involves infeasible path detection followed by timing analysis. We employ constraint propagation methods to detect infeasible paths spanning across loop iterations. Our timing analysis is exact modulo the infeasible path information provided. Moreover, the analysis is efficient since it relies on memoization techniques to avoid exhaustive enumeration of all paths through a loop. The precision of our timing analysis is demonstrated on different benchmark programs.", "num_citations": "1\n", "authors": ["272"]}
{"title": "Performance impact of multithreaded Java semantics on multiprocessor memory consistency models\n", "abstract": " The semantics of Java multithreading dictates all possible behaviors that a multithreaded Java program can exhibit on any platform. This is called the Java memory model and describes the allowed re-orderings among the operations in a thread. However, multiprocessor platforms traditionally have a memory consistency model of their own. Consequently memory barriers may have to be inserted to ensure that the multiprocessor execution of a multithreaded Java program respects the Java Memory Model. In this paper, we study the impact of these additional memory barriers on multiprocessor performance. We also study how different choices of the Java Memory Model a_ect multiprocessor performance. Our experimental results are obtained by simulating multithreaded Java Grande benchmarks under various software and hardware memory models.", "num_citations": "1\n", "authors": ["272"]}
{"title": "Using formal techniques to Debug the AMBA System-on-Chip Bus Protocol\n", "abstract": " System-on-chip (SoC) designs use bus protocols for high performance data transfer among the Intellectual Property (IP) cores. These protocols incorporate advanced features such as pipelining, burst and split transfers. In this paper, we describe a case study in formally verifying a widely used SoC bus protocol: the Advanced Micro-controller Bus Architecture (AMBA) protocol from ARM. In particular, we develop a formal specification of the AMBA protocol. We then employ model checking, a state space exploration based formal verification technique, to verify crucial design invariants. The presence of pipelining and split transfer in the AMBA protocol gives rise to interesting corner cases, which are hard to detect via informal reasoning. Using the SMV model checker, we have detected a potential bus starvation scenario in the AMBA protocol. Such scenarios demonstrate the inherent intricacies in designing pipelined bus protocols.", "num_citations": "1\n", "authors": ["272"]}
{"title": "Java memory model aware software verification\n", "abstract": " The Java Memory Model (JMM) provides a semantics of Java multithreading for any implementation platform. The JMM is defined in a declarative fashion with an allowed program execution being defined in terms of existence of \u201ccommit sequences\u201d(roughly, the order in which actions in the execution are committed). In this work, we develop an operational approximation of the JMM. The immediate motivation of this work lies in integrating a formal specification of the JMM with software model checkers. We show how our operational description of the JMM can be integrated into a Java Path Finder (JPF) style model checker for Java programs.", "num_citations": "1\n", "authors": ["272"]}