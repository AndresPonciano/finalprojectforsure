{"title": "Online Discovery of Declarative Process Models from Event Streams\n", "abstract": " Today's business processes are often controlled and supported by information systems. These systems record real-time information about business processes during their executions. This enables the analysis at runtime of the process behavior. However, many modern systems produce \u201cbig data\u201d, i.e., collections of data sets so large and complex that it becomes impossible to store and process all of them. Moreover, few processes are in steady-state but, due to changing circumstances, they evolve and systems need to adapt continuously. In this paper, we present a novel framework for the discovery of LTL-based declarative process models from streaming event data in settings where it is impossible to store all events over an extended period of time or where processes evolve while being analyzed. The framework continuously updates a set of valid business constraints based on the events occurred in the event\u00a0\u2026", "num_citations": "65\n", "authors": ["607"]}
{"title": "Online process discovery to detect concept drifts in ltl-based declarative process models\n", "abstract": " Today\u2019s business processes are often controlled and supported by information systems. These systems record real-time information about business processes during their executions. This enables the analysis at runtime of the process behavior. However, many modern systems produce \u201cbig data\u201d, i.e., collections of data sets so large and complex that it becomes impossible to store and process all of them. Moreover, few processes are in steady-state and due to changing circumstances processes evolve and systems need to adapt continuously. In this paper, we present a novel framework for the discovery of LTL-based declarative process models from streaming event data in settings where it is impossible to store all events over an extended period or where processes evolve while being analyzed. The framework continuously updates a set of valid business constraints based on the events occurred in the\u00a0\u2026", "num_citations": "52\n", "authors": ["607"]}
{"title": "Generating event logs through the simulation of declare models\n", "abstract": " In the process mining field, several techniques have been developed during the last years, for the discovery of declarative process models from event logs. This type of models describes processes on the basis of temporal constraints. Every behavior that does not violate such constraints is allowed, and such characteristic has proven to be suitable for representing highly flexible processes. One way to test a process discovery technique is to generate an event log by simulating a process model, and then verify that the process discovered from such a log matches the original one. For this reason, a tool for generating event logs starting from declarative process models becomes vital for the evaluation of declarative process discovery techniques. In this paper, we present an approach for the automated generation of event logs, starting from process models that are based on Declare, one of the most used\u00a0\u2026", "num_citations": "38\n", "authors": ["607"]}
{"title": "Empirical investigation of the efficacy and efficiency of tools for transferring software engineering knowledge\n", "abstract": " Continuous pressure on behalf of enterprises leads to a constant need for innovation. This involves exchanging results of knowledge and innovation among research groups and enterprises in accordance to the Open Innovation paradigm. The technologies that seem to be apparently attractive for exchanging knowledge are the Internet and its search engines. Literature provides many discordant opinions on their efficacy, and no empirical evidence on the topic. This work starts from the definition of a Knowledge Acquisition Process, and presents a rigorous empirical investigation that evaluates the efficacy of the previous technologies within the Exploratory Search of Knowledge and of Relevant Knowledge according to specific knowledge requirements. The investigation has pointed out that these technologies are not effective for Explorative Search. The paper concludes with a brief analysis of other technologies to\u00a0\u2026", "num_citations": "25\n", "authors": ["607"]}
{"title": "Do activity lifecycles affect the validity of a business rule in a business process?\n", "abstract": " Traditional process mining techniques offer limited possibilities to analyze business processes working in low-predictable and dynamic environments. Recently, to close this gap, declarative process models have been introduced to represent process mining results since they allow for describing complex behaviors as a compact set of business rules. However, in this context, activities of a business process are still considered as atomic/instantaneous events. This is a strong limitation for these approaches because often, in realistic environments, process activities are not instantaneous but executed across a time interval and pass through a sequence of states of a lifecycle. This paper investigates how the existing techniques for the discovery of declarative process models can be adapted when the business process under analysis contains non-atomic activities. In particular, we base our proposed approach on the\u00a0\u2026", "num_citations": "23\n", "authors": ["607"]}
{"title": "Using discriminative rule mining to discover declarative process models with non-atomic activities\n", "abstract": " Process discovery techniques try to generate process models from execution logs. Declarative process modeling languages are more suitable than procedural notations for representing the discovery results deriving from logs of processes working in dynamic and low-predictable environments. However, existing declarative discovery approaches aim at mining declarative specifications considering each activity in a business process as an atomic/instantaneous event. In spite of this, often, in realistic environments, process activities are not instantaneous; rather, their execution spans across a time interval and is characterized by a sequence of states of a transactional lifecycle. In this paper, we investigate how to use discriminative rule mining in the discovery task, to characterize lifecycles that determine constraint violations and lifecycles that ensure constraint fulfillments. The approach has been implemented\u00a0\u2026", "num_citations": "23\n", "authors": ["607"]}
{"title": "Lights, camera, action! business process movies for online process discovery\n", "abstract": " Nowadays, organizational information systems are able to collect high volumes of data in event logs every day. Through process mining techniques, it is possible to extract information from such logs to support organizations in checking process conformance, detecting bottlenecks, and carrying on performance analysis. However, to analyze such \u201cbig data\u201d through process mining, events coming from process executions (in the form of event streams) must be processed on-the-fly as they occur. The work presented in this paper is built on top of a technique for the online discovery of declarative process models presented in our previous work. In particular, we introduce a tool providing a dynamic visualization of the models discovered over time showing, as a \u201cprocess movie\u201d, the sequence of valid business rules at any point in time based on the information retrieved from an event stream. The effectiveness of\u00a0\u2026", "num_citations": "21\n", "authors": ["607"]}
{"title": "Empirical validation of knowledge packages as facilitators for knowledge transfer\n", "abstract": " Transfer of research results in production systems requires, among others, that knowledge be explicit and understandable by stakeholders. Such transfer is demanding, as so many researchers have been studying alternative ways to classic approaches such as books and papers that favour knowledge acquisition on behalf of users. In this context, we propose the concept of Knowledge Experience Package (KEP) with a specific structure as an alternative. The KEP contains both the conceptual model(s) of the research results which make up the innovation, including all the necessary documentation ranging from papers or book chapters; and the experience collected in acquiring it in business processes, appropriately structured. The structure allows the identification of the knowledge chunk(s) that the developer, who is acquiring the knowledge, needs in order to simplify the acquisition process. The experience is\u00a0\u2026", "num_citations": "15\n", "authors": ["607"]}
{"title": "Evaluating coding behavior in software development processes: A process mining approach\n", "abstract": " Process mining is a family of techniques that aim at analyzing business process execution data recorded in event logs. Conformance checking is a branch of this discipline embracing approaches for verifying whether the behavior of a process, as recorded in a log, is in line with some expected behavior provided in the form of a process model. In the literature, process mining techniques have already been used to study software development processes starting from logs derived from version management systems or from document management systems. In this paper, we use conformance checking to test coding behaviors starting from event logs generated from IDE usage. Understanding how developers carry out coding activities and what hurdles they usually face should provide useful tips for improving and supporting software development processes. In particular, through conformance checking, we can compare\u00a0\u2026", "num_citations": "14\n", "authors": ["607"]}
{"title": "Web applications design recovery and evolution with RE\u2010UWA\n", "abstract": " This paper presents a semi\u2010automatic approach for the recovery and evolution of the design of existing Web applications. The proposed approach is structured in two main phases and is based on the Ubiquitous Web Applications (UWA) design framework, a methodology and a set of models and tools for the user\u2010centered design of multichannel context\u2010aware Web applications. In the first phase a representative set of the application's front\u2010end Web pages are analyzed to abstract the \u2018as\u2010is\u2019 design model of the application according to the UWA methodology. In the second phase, the recovered design model is evolved to define the \u2018to be\u2019 version of it. This evolution activity considers the up\u2010to\u2010date requirements available for the application and UWA design guidelines to identify shortcomings and opportunities of improvement in the \u2018as\u2010is\u2019 design. The reverse modeling phase exploits clustering and clone detection\u00a0\u2026", "num_citations": "13\n", "authors": ["607"]}
{"title": "Knowledge management integrated with e-learning in open innovation\n", "abstract": " This paper presents a framework aiming to support an \u00abinnovation chain\u00bb in an Open Innovation (OI) perspective. In order to transfer research results from producers to users, it is necessary to develop a Knowledge Manage-ment System supporting formalization, packaging and characterization to be able to select, understand and collect research results and/or innovations deriving from them. Suitable skills are required to transfer and collect innovation. Since in OI the knowledge producer and fi nal users are by defi nition geographically distant, the required specialist skills have to be acquired through an e-learning system. This system must offer Learning Objects that can be combined within a course that also takes into account the user\u2019s past experiences. This work proposes an approach based on the integration of these two systems, and presents PROMETHEUS, a tool supporting this approach. The results of\u00a0\u2026", "num_citations": "13\n", "authors": ["607"]}
{"title": "Discovering cross-organizational business rules from the cloud\n", "abstract": " Cloud computing is rapidly emerging as a new information technology that aims at providing improved efficiency in the private and public sectors, as well as promoting growth, competition, and business dynamism. Cloud computing represents, today, an opportunity also from the perspective of business process analytics since data recorded by process-centered cloud systems can be used to extract information about the underlying processes. Cloud computing architectures can be used in cross-organizational environments in which different organizations execute the same process in different variants and share information about how each variant is executed. If the process is characterized by low predictability and high variability, business rules become the best way to represent the process variants. The contribution of this paper consists in providing: (i) a cloud computing multi-tenancy architecture to support cross\u00a0\u2026", "num_citations": "12\n", "authors": ["607"]}
{"title": "Managing SOA system variation through business process lines and process oriented development\n", "abstract": " Software Product Lines (SPL) and Service-Oriented Architectures (SOA) are two emerging approaches to the software development currently receiving great attention both in research and in practice. Our work suggests an approach to transfer the main peculiarities of the SPL (ie asset reuse and variation mechanisms) to the SOA systems development, in order to realize a SOA systems line. In this way we provide a method to easily adapt a SOA application to different customer needs in changeable environments. All this is realized using the Business Process Lines (BPL) concept together with the Process Oriented Development (POD) paradigm. A BPL realizes process models suitable to different customers or market segments needs. The POD paradigm allows to transform a process model into a SOA system.", "num_citations": "12\n", "authors": ["607"]}
{"title": "Web applications design evolution with UWA\n", "abstract": " This paper presents a semi-automatic approach to Web applications design evolution which leverages the Ubiquitous Web Applications (UWA) design framework, a methodology and a set of models and tools for the user-centered design of multi-channels and context-aware Web applications. The approach is based on a two-step redesign process: first a semi-automatic reverse modeling phase analyzes the html pages of the application front-end to abstract a model of the \u201cas-is\u201d design, according to the UWA formalism; second, a forward design phase starts from the recovered models and the (new) requirements available for the application to identify lacks and opportunities of improvements in the \u201cas-is\u201d design and produce the \u201cto-be\u201d version of it. The reverse modeling phase applies clustering and clone detection techniques and is supported by an Eclipse IDE environment. The forward design phase is supported\u00a0\u2026", "num_citations": "10\n", "authors": ["607"]}
{"title": "Managing Business Process Flexibility and Reuse through Business Process Lines.\n", "abstract": " The ever greater pressure of competition to which enterprises are subjected has made the process continuous improvement a crucial issue today. For this reason it should be useful to compose the business processes reusing previously modeled business process parts characterizing them according to the current market needs. This work presents an approach based on the use of Business Process Lines (BPL) to compose and characterize a business process according to different contexts reusing existing process parts. The approach has been applied to realize a BPL for the Software extraordinary maintenance. This BPL can be used to model different process variants of the Software extraordinary maintenance processes corresponding to different context profiles. The results demonstrate the approach applicability in a real case and underline that it allows to reuse and specialize the same process parts for many different contexts.", "num_citations": "10\n", "authors": ["607"]}
{"title": "A fuzzy clustering-based approach to study malware phylogeny\n", "abstract": " Mobile devices are always more diffused in the last years, allowing the users to perform several tasks: communication, web surfing, requiring web services. Given the high amount of sensitive data and operations related to these tasks, securing the mobile devices is becoming a very critical issue. As matter of the fact, malware attacks are on the rise and new mobile malware are continually generated with the aim of stealing private data and performing illegal activities. Since this new malware is mainly obtained by reusing existing malicious code, malware detection is supported by the study and the tracking of the mobile malware phylogeny. This paper proposes a malware phylogeny model obtained by a declarative Process Mining (PM) approach from the analysis of some running malware applications. The main idea is that the set of relations and recurring execution patterns among the syscalls of a running malware\u00a0\u2026", "num_citations": "9\n", "authors": ["607"]}
{"title": "Automated development of constraint-driven web applications\n", "abstract": " Today's Web Applications (WAs) are complex multi-user and multi-tenant software systems, used by users with different roles and often developed to support and manage complex business processes. Due to the changing nature of such processes, WAs need to be easily and quickly modified, to be adapted and aligned to the processes they support. In recent years, Model Driven Engineering (MDE) approaches have been used to support the development and the evolution of WAs. However, the definition of appropriate MDE approaches for the development of flexible process-centric WAs is still limited. In particular, flexible workflow models have never been integrated with the models used in MDE approaches to develop this type of applications. This paper proposes an MDE approach for the development and the evolution of flexible process-centric WAs that integrates four MDE metamodels used to represent the\u00a0\u2026", "num_citations": "9\n", "authors": ["607"]}
{"title": "Improving design patterns finder precision using a model checking approach\n", "abstract": " In this paper we propose an approach exploiting the model checking technique to automatically refine the results produced by a Design Patterns mining tool called Design Pattern Finder (DPF) to improve the precision of its results by verifying the detected DPs automatically. To assess the feasibility of the proposed approach along with its effectiveness, we have applied it to an open source Object Oriented system with good results in improving the precision of the detected DPs.", "num_citations": "9\n", "authors": ["607"]}
{"title": "Mining Developer's Behavior from Web-Based IDE Logs\n", "abstract": " The birth of cloud-based development environments makes available an increasing number of data coming out from the interaction of different developers with a diverse level of expertise. This data, if opportunely captured and analyzed, can be useful to understand how developers head the coding activities and can suggest members of developers community how to improve their performances. This paper presents a framework allowing to generate event logs from cloud-based IDE. These event logs are then examined using a process mining technique to extract the developers' coding processes and compare them in the shared coding environment. The approach can be used to discover emergent and interesting developers' behavior. Thus, we compare the coding process extracted by developers with different skills. To validate our approach, we describe the results of a study in which we investigate the coding\u00a0\u2026", "num_citations": "8\n", "authors": ["607"]}
{"title": "Distributed software development with knowledge experience packages\n", "abstract": " In software production process, a lot of knowledge is created and remain silent. Therefore, it cannot be reused to improve the effectiveness and the efficiency of these processes. This problem is amplified in the case of a distributed production. In fact, distributed software development requires complex context specific knowledge regarding the particularities of different technologies, the potential of existing software, the needs and expectations of the users. This knowledge, which is gained during the project execution, is usually tacit and is completely lost by the company when the production is completed. Moreover, each time a new production unit is hired, despite the diversity of culture and capacity of people, it is necessary to standardize the working skills and methods of the different teams if the company wants to keep the quality level of processes and products. In this context, we used the concept of\u00a0\u2026", "num_citations": "8\n", "authors": ["607"]}
{"title": "Automatic Generation of Multi Platform Web Map Mobile Applications.\n", "abstract": " The development of current mobile applications is a challenging task because mobile devices are characterized by a variety of advanced services respect standard computers. In particular, these services as an example Map visualization, Web access, GPS localization, Camera, Accelerometer, and so on, interact with the applications in different ways, depending on the used mobile device platform. These challenges are further increased by the fact that each platform needs a different development process and provides a different framework to implement these mobile applications. In this paper we propose a common architecture and an unified development process implementing portable applications based on mobile services. This architecture is based on the Model-View-Control design pattern and provides a framework that generates the code starting from a formal algebraic specification. This specification integrates different formalisms such as LTL formulae and functional programming, permitting the description of structure and dynamic aspects of a mobile application. Moreover, we show how the proposed framework allows to generate Web map applications integrating different mobile services.", "num_citations": "8\n", "authors": ["607"]}
{"title": "Empirical experimentation for validating the usability of knowledge packages in transferring innovations\n", "abstract": " Transfer of research results following to technological innovation and to the experience collected in applying the innovation within an enterprise is a key success factor. A critical factor in transferring innovations to software processes concerns the knowledge transfer activity which requires the knowledge be explicit and understandable by stakeholders. As so many researchers have been studying alternative ways to conventional approaches i.e. books, papers, reports and other written communication means that favour knowledge acquisition on behalf of users. In this context, we propose the Knowledge Package (KP) structure as alternative. We have carried out an experiment which compared the usability of the proposed approach with conventional ones, along with the efficiency and the comprehensibility of the knowledge enclosed in a KP rather than in a set of Conventional Sources. The experiment has\u00a0\u2026", "num_citations": "8\n", "authors": ["607"]}
{"title": "Early Detection of Parkinson Disease using Deep Neural Networks on Gait Dynamics\n", "abstract": " Parkinson\u2019s disease is a degenerative movement disorder causing considerable disability. However, the early detection of this syndrome and of its progression rates may be decisive for the identification of appropriate therapies. For this reason, the adoption of Neural Networks to detect this disease on the base of walking information is gaining more and more interest. In this paper, we defined a Deep Neural Network based approach allowing one to exploit the information coming from various sensors located under the feet of a person. The proposed approach allows one to discriminate people affected by the Parkinson syndrome and detect the progression rates of the disease itself. To evaluate the proposed architecture we used a known dataset with the aim to compare its performance with other similar approaches. Moreover, we performed an in-depth hyper-parameter optimization to find out the best neural\u00a0\u2026", "num_citations": "7\n", "authors": ["607"]}
{"title": "Model Driven Development of Process-centric Web Applications.\n", "abstract": " Despite Model Driven Engineering (MDE) approaches are largely used to develop, update and evolve Web Applications (WAs), the use of these approaches for the development of process-centric WAs is still very limited. This is an important issue in the context of MDE considering that WAs are often used to support users in the execution of business processes. In this paper, we propose the integration of three MDE metamodels used to represent the structure of information, service and presentation layers of a WA with the metamodel of Declare, a declarative language for business process rapresentation. The declarative nature of Declare allows us to combine an efficient roundtrip engineering support with the advantages of an MDE approach. We present and discuss a case study where the proposed approach is used to develop a typical online shopping application with the aim to validate and verify the feasibility and the effectiveness of the approach.", "num_citations": "7\n", "authors": ["607"]}
{"title": "Knowledge packaging supporting risk management in software processes\n", "abstract": " KNOWLEDGE PACKAGING SUPPORTING RISK MANAGEMENT IN SOFTWARE PROCESSES Pasquale Ardimento*, Nicola Boffoli*, Marta Cimitile*, Aldo Persico+ , Aldo Tammaro+ *Dipartimento di Informatica \u2013 Universit\u00e0 di Bari - Via Orabona, 4, 70126 Bari \u2013 Italy email: ardimento@di.uniba.it, boffoli@di.uniba.it, cimitile@di.uniba.it + EDS Italia Software SpA-Via Antiniana,2/a, 80078 Pozzuoli (Naples) \u2013 Italy email: aldo.persico@eds.com, aldo.tammaro@eds.com ABSTRACT The need for using past acquired experience is a critical issue and has assumed strategic importance in the past years. For this reason the scientific community has focused its attention on the identification of methods, tools, and techniques for formalizing experience and know-how and making it available for other projects. An area where such issue is quite important is risk management. Here manager experience is crucial for making \u2026", "num_citations": "7\n", "authors": ["607"]}
{"title": "A systematic review on Deep Learning approaches for IoT security\n", "abstract": " The constant spread of smart devices in many aspects of our daily life goes hand in hand with the ever-increasing demand for appropriate mechanisms to ensure they are resistant against various types of threats and attacks in the Internet of Things (IoT) environment. In this context, Deep Learning (DL) is emerging as one of the most successful and suitable techniques to be applied to different IoT security aspects.This work aims at systematically reviewing and analyzing the research landscape about DL approaches applied to different IoT security scenarios. The contributions we reviewed are classified according to different points of view into a coherent and structured taxonomy in order to identify the gap in this pivotal research area.The research focused on articles related to the keywords \u2019deep learning\u2019, \u2019security\u2019 and \u2019Internet of Things\u2019 or \u2019IoT\u2019 in four major databases, namely IEEEXplore, ScienceDirect\u00a0\u2026", "num_citations": "6\n", "authors": ["607"]}
{"title": "An empirical study on software engineering knowledge/experience packages\n", "abstract": " This paper is concerned with characterization of software engineering knowledge and experience packages (EP) in the user perspective. It presents the first iteration of an evidence-based study. Results are presented from surveys conducted with many practitioners about the available experience bases, and on literature, to improve our understanding about the state of the practice and art for EP. Additionally, the paper presents attributes and their properties that, in the opinion of the participant practitioners, are relevant for characterizing an EP in the user perspective. Subsequently, with regard to this empirical system, the Acceptability indirect measurement model is provided for experience components. Moreover, the test of this measurement model is shown, which involved both developing qualitative evaluations with practitioners, and measuring ten Internet-available experience bases. Finally, the threats\u00a0\u2026", "num_citations": "6\n", "authors": ["607"]}
{"title": "Temporal convolutional neural networks for radar micro-Doppler based gait recognition\n", "abstract": " The capability of sensors to identify individuals in a specific scenario is a topic of high relevance for sensitive sectors such as public security. A traditional approach involves cameras; however, camera-based surveillance systems lack discretion and have high computational and storing requirements in order to perform human identification. Moreover, they are strongly influenced by external factors (eg, light and weather). This paper proposes an approach based on a temporal convolutional deep neural networks classifier applied to radar micro-Doppler signatures in order to identify individuals. Both sensor and processing requirements ensure a low size weight and power profile, enabling large scale deployment of discrete human identification systems. The proposed approach is assessed on real data concerning 106 individuals. The results show good accuracy of the classifier (the best obtained accuracy is 0.89 with an F1-score of 0.885) and improved performance when compared to other standard approaches. View Full-Text", "num_citations": "5\n", "authors": ["607"]}
{"title": "Fuzzy neural networks to detect parkinson disease\n", "abstract": " In this paper, we present a Deep Learning architecture, exploiting a fuzzy layer, applied to the data coming from various sensors located under the feet of a patient affected by the Parkinson's disease. The solution we propose permits one to cluster data coming from different sensors into different fuzzy partitions, according to the different parts of the feet, and to discriminate the illness of a person as well as the severity degree of the disease itself. We employed a known dataset to evaluate our solution and compared its performance with some similar approaches found in the relevant literature. Moreover, we performed an intensive parameter optimization step to find the best setting for the proposed fuzzy neural network. The evaluation shows that our solution obtains good classification results both in the binary and in the multiclassification approach.", "num_citations": "5\n", "authors": ["607"]}
{"title": "An ontological multi-criteria optimization system for Workforce Management\n", "abstract": " Workforce Management (WFM) is becoming a core decisional approach for optimizing different enterprise processes such as operational activities needed to maintain a high production rate. However, in order to solve complex optimization problems it is necessary to analyze and deal with a plethora of distributed and semantically different information defining the collection of criteria from which enterprise activities depend. For this reason, this paper introduces a novel WFM system that, by using an ontological representation of knowledge related to the different aspects of an enterprise activity, exploits a multi-criteria decision making approach for selecting the most suitable strategies to face WFM issues.", "num_citations": "5\n", "authors": ["607"]}
{"title": "Gait Recognition using FMCW Radar and Temporal Convolutional Deep Neural Networks\n", "abstract": " The capability of human identification in specific scenarios and in a quickly and accurately manner, is a critical aspect in various surveillance applications. In particular, in this context, classical survaillance systems are based on videocameras, requiring high computational/storing resources, which are very sensitive to light and weather conditions. In this paper, an efficient classifier based on deep learning is used for the purpose of identifying individuals features by resorting to the micro-Doppler data extracted from low-power frequency-modulated continuous-wave radar measurements. Results obtained through the application of a deep temporal convolutional neural networks confirms the applicability of deep learning to the problem at hand. Best obtained identification accuracy is 0.949 with an F-measure of 0.88 using a temporal window of four seconds.", "num_citations": "4\n", "authors": ["607"]}
{"title": "Higher Education Learning Methodologies and Technologies Online\n", "abstract": " This volume of Communication in Computer and Information Science (CCIS) contains the post-proceedings of HELMeTO 2019, the First International Workshop on Higher Education Methodologies and Technologies Online. The event was successfully held at the headquarters of the eCampus University, Novedrate (CO), Italy, during June 6\u20137, 2019. HELMeTO 2019 aimed to bring together researchers and practitioners working in Higher Distance Education Institutions or studying Online Learning Methodologies to present and share their research in a multidisciplinary context. The workshop provided a forum for the discussion of new research directions and applications in these fields, where different disciplines effectively met, divided into two main tracks: Online Pedagogy and Learning Methodologies, and Learning Technologies, Data Analytics and Educational Big Data Mining, together with their applications\u00a0\u2026", "num_citations": "4\n", "authors": ["607"]}
{"title": "A fuzzy-based autoscaling approach for process centered cloud systems\n", "abstract": " In the last years, the growing adoption of cloud-based multi-tiers systems has strongly increased the levels of resource sharing among companies, improving the enterprise efficiency, thanks to a refined business dynamism and a rapid decrease in costs. However, in spite of their advantages, this new business model highlights the emergence of new computational approaches aimed at the distribution and the optimization of resources sharing along so-called multi-tenants system, i.e., cloud-based architecture where a single instance of software runs on a single server and serves multiple companies (tenants). This paper faces this challenging gap by proposing an auto-scaling cloud computing multi-tenancy architecture where process mining and fuzzy-based load-balancing systems synergistically interact to provide an improved and optimized resource management distribution. A case study is carried out to show\u00a0\u2026", "num_citations": "4\n", "authors": ["607"]}
{"title": "Identification of walker identity using smartphone sensors: an experiment using ensemble learning\n", "abstract": " Nowadays MEMS sensors, like accelerometers, gyroscopes, and magnetometers, are spreading in a wide range of applications, because of their small size, cheapness and increasing performance. For instance, smartphones are currently equipped with this kind of sensors, which could be used to improve the user experience of the phone itself or the navigation functionalities. In this work, accelerometers, gyros, and orientation measurements are exploited to provide advanced information about the walker bringing the phone. In particular, smartphone sensors outputs are used to recognize the identity of the walker and the pose of the device during the walk. The aforesaid information, if known, could be used to improve specific smartphone functionalities. For instance, the recognition of walker identity can be used for theft protection or the device pose can be used to improve the performance of the pedestrian\u00a0\u2026", "num_citations": "3\n", "authors": ["607"]}
{"title": "Temporal Convolutional Networks for Just-in-Time Software Defect Prediction.\n", "abstract": " Defect prediction and estimation techniques play a significant role in software maintenance and evolution. Recently, several research studies proposed just-in-time techniques to predict defective changes. Such prediction models make the developers check and fix the defects just at the time they are introduced (commit level). Nevertheless, early prediction of defects is still a challenging task that needs to be addressed and can be improved by getting higher performances. To address this issue this paper proposes an approach exploiting a large set of features corresponding to source code metrics detected from commits history of software projects. In particular, the approach uses deep temporal convolutional networks to make the fault prediction. The evaluation is performed on a large data-set, concerning four well-known open-source projects and shows that, under certain considerations, the proposed approach has effective defect proneness prediction ability.", "num_citations": "3\n", "authors": ["607"]}
{"title": "Reusing bugged source code to support novice programmers in debugging tasks\n", "abstract": " Novice programmers often encounter difficulties performing debugging tasks effectively. Even if modern development environments (IDEs) provide high-level support for navigating through code elements and for identifying the right conditions leading to the bug, debugging still requires considerable human effort. Programmers usually have to make hypotheses that are based on both program state evolution and their past debugging experiences. To mitigate this effort and allow novice programmers to gain debugging experience quickly, we propose an approach based on the reuse of existing bugs of open source systems to provide informed guidance from the failure site to the fault position. The goal is to help novices in reasoning on the most promising paths to follow and conditions to define. We implemented this approach as a tool that exploits the knowledge about fault and bug position in the system, as long as\u00a0\u2026", "num_citations": "3\n", "authors": ["607"]}
{"title": "Learning analytics to improve coding abilities: a fuzzy-based process mining approach\n", "abstract": " Comprehension of how students and developers head the development of software and what specific hurdles they face, have a strong potential to better support the coding workflow. In this paper, we present the CodingMiner environment to generate event logs from IDE usage enabling the adoption of fuzzy-based process mining techniques to model and to study the developers' coding process. The logs from the development sessions have been analyzed using the fuzzy miner to highlight emergent and interesting developers' and students' behaviors during coding. The mined processes show different IDE usage patterns for students with different skills and performances. To validate our approach, we describe the results of a study in which the CodingMiner environment is used to investigate the coding activities of twenty students of a CS2 course performing a given programming task during four assignments\u00a0\u2026", "num_citations": "3\n", "authors": ["607"]}
{"title": "A Multi-source Machine Learning Approach to Predict Defect Prone Components.\n", "abstract": " Software code life cycle is characterized by continuous changes requiring a great effort to perform the testing of all the components involved in the changes. Given the limited number of resources, the identification of the defect proneness of the software components becomes a critical issue allowing to improve the resources allocation and distributions. In the last years several approaches to evaluating the defect proneness of software components are proposed: these approaches exploit products metrics (like the Chidamber and Kemerer metrics suite) or process metrics (measuring specific aspect of the development process). In this paper, a multisource machine learning approach based on a selection of both products and process metrics to predict defect proneness is proposed. With respect to the existing approaches, the proposed classifier allows predicting the defect proneness basing on the evolution of these features across the project development. The approach is tested on a real dataset composed of two well-known open source software systems on a total of 183 releases. The obtained results show that the proposed features have effective defect proneness prediction ability.", "num_citations": "3\n", "authors": ["607"]}
{"title": "Model driven development of cross-platform mobile applications\n", "abstract": " The high variability of mobile hardware platforms and frameworks makes the development and maintenance of mobile applications a challenging task. Moreover mobile devices must host a wide spectrum of services to handle both specific kind of hardware (eg cameras, sensors, etc.) and system management (eg user data persistence, configuration, security and privacy, etc.). These aspects get worse by the deep differences existing among commercial platforms in terms of user interface structure and controls, system services, frameworks and even in some cases programming languages. In this paper we propose a model-driven approach targeting a unified architecture to develop applications for multiple platform using model trasformation and code generation approach. The approach is based on a metamodel that describes the external requirements of the mobile app allowing developers to model it by means of a DSL. The approach generates an application adopting an unifying architecture centered on the Model-View-Control architectural pattern. Finally a case study is carried out in order to assess the validity and the effectiveness of the proposed approach.", "num_citations": "3\n", "authors": ["607"]}
{"title": "Software analytics to support students in object-oriented programming tasks: an empirical study\n", "abstract": " The computing education community has shown a long-time interest in how to analyze the Object-Oriented (OO) source code developed by students to provide them with useful formative tips. Instructors need to understand the student\u2019s difficulties to provide precise feedback on most frequent mistakes and to shape, design and effectively drive the course. This paper proposes and evaluates an approach allowing to analyze student\u2019s source code and to automatically generate feedback about the more common violations of the produced code. The approach is implemented through a cloud-based tool allowing to monitor how students use language constructs based on the analysis of the most common violations of the Object-Oriented paradigm in the student source code. Moreover, the tool supports the generation of reports about student\u2019s mistakes and misconceptions that can be used to improve the students\u00a0\u2026", "num_citations": "2\n", "authors": ["607"]}
{"title": "On the students\u2019 misconceptions in object-oriented language constructs\n", "abstract": " Analyze the Object-oriented (OO) source code developed by students provides useful formative tips to instructors. According to this, it is essential to understand the student\u2019s real difficulties allowing instructors to shape effective courses. To provide run-time feedback to students and to study and analyze the evolution of their performances offline and over time we designed a framework and developed a tool. It allows to identify students\u2019 misconceptions analysing source code and to create personalized student reports automatically. In this paper, we present an empirical study, conducted using our toolchain, that involves 1627 projects extracted from the multi-institution Blackbox dataset. We identified a violation model for Java language constructs based on established results in the computing education community. Afterwards, we grouped such violations in categories and analyzed the relations among them\u00a0\u2026", "num_citations": "2\n", "authors": ["607"]}
{"title": "An approach to digital business ecosystems based on process models\n", "abstract": " The Digital Business Ecosystem (DBE) is an organizational approach which allows competitive enterprises to cooperate with each other. It is necessary to put aside the operating realizations to formalize this new approach in order to build up a theory to base the empirical experimentation. This work is a contribution to the formalization with the goal of understanding how competitive enterprises with heterogeneous organizations and belonging to different areas, can coexist in a same DBE. The formalization proposed has a logical reading based on processes models. Thanks to the use of the process models it is possible to organize some tools which allow to quickly build platforms for DBE management and governance and to make this flexible to comply with the different operating modes foreseen by the management of the enterprises which are part of the DBE", "num_citations": "2\n", "authors": ["607"]}
{"title": "Key performance indicators to relate knowledge governance with knowledge process\n", "abstract": " The work proposes an approach based on the use of Knowledge Performance Indicators and Knowledge Experience Base aiming to support and correlate KG and KP activities. The proposed approach has been adopted in the SERLab laboratory for transferring innovations from academic to industrial contexts. The preliminary results are described and discussed. The proposed framework needs to be further validated in industrial context in the way to test and to improve it.", "num_citations": "2\n", "authors": ["607"]}
{"title": "Empirical investigation on knowledge packaging supporting risk management in software processes\n", "abstract": " Project risks management is a non trivial task based on manager experience and knowledge collected in past executed projects. The larger the project manager experience and the available enterprise risk knowledge, the better the enterprise ability in risk management will be.For this reason the scientific community has focused its attention on the identification of methods, tools, and techniques for formalizing experience and know-how and making it available for other projects. In this sense, the authors have already presented a Risk Knowledge Package [1] for managing risk knowledge during software process execution. The work here proposed represents the continuation of such studies. In particular, an empirical investigation in industrial field has been carried out. Such investigation, based on legacy projects of EDS Italia Software SpA, aims at validating the effectiveness and the precision of the proposed approach.", "num_citations": "2\n", "authors": ["607"]}
{"title": "Knowledge based risk management in software processes\n", "abstract": " Unexpected events frequently cause major problems to projects. Furthermore, due to software project uniqueness, uncertainty about the end results will always accompany software development. While risks cannot be removed from software development, software engineers should learn to manage them better. Risk Planning requires organization experience, as it is strongly centred on the experience and knowledge acquired in former projects. The larger the experience of the project manager, the better his ability in identifying risks, estimating their occurrence likelihood and impact, and defining an appropriate response plan. However, project manager risk knowledge cannot remain in an individual dimension; rather it must be made available to the entire organization. Risk planning can be enriched by using knowledge and experience acquired by the various managers while working on the various organization projects. In order to do so, it is necessary for risk knowledge to be packaged and stored throughout project development, in order to use it in future. This is a challenging activity. The aim of this paper is to propose a knowledge based approach for project risk management. It is made of: a conceptual architecture for knowledge storing and reuse; a knowledge package structure for collecting risk knowledge.", "num_citations": "2\n", "authors": ["607"]}
{"title": "Deep neural networks ensemble to detect COVID-19 from CT scans\n", "abstract": " Research on Coronavirus Disease 2019 (COVID-19) detection methods has increased in the last months as more accurate automated toolkits are required. Recent studies show that CT scan images contain useful information to detect the COVID-19 disease. However, the scarcity of large and well balanced datasets limits the possibility of using detection approaches in real diagnostic contexts as they are unable to generalize. Indeed, the performance of these models quickly becomes inadequate when applied to samples captured in different contexts (e.g., different equipment or populations) from those used in the training phase. In this paper, a novel ensemble-based approach for more accurate COVID-19 disease detection using CT scan images is proposed. This work exploits transfer learning using pre-trained deep networks (e.g., VGG, Xception, and ResNet) evolved with a genetic algorithm, combined into an\u00a0\u2026", "num_citations": "1\n", "authors": ["607"]}
{"title": "Malware Phylogeny Analysis using Data-Aware Declarative Process Mining\n", "abstract": " Mobile phones are currently the main targets of continuous malware attacks. Usually, new malicious code is generated conveniently changing the existing one. According to this, it becomes very useful to identify new approaches for the analysis of malware phylogeny. This paper proposes a data-aware process mining approach performing a malware dynamic analysis. The process mining is performed by using a multiperspective declarative approach allowing to model a malware family as a set of constraints (within their data attributes) among the system call traces gathered from infected applications. The models are used to detect execution patterns or other relationships among families. The obtained models can be used to verify if a checked malware is a potential member of a known malware family and its difference with respect to other malware variants of the family. The approach is implemented and applied on\u00a0\u2026", "num_citations": "1\n", "authors": ["607"]}
{"title": "Reducing Static Dependences Exploiting a Declarative Design Patterns Framework.\n", "abstract": " Object Oriented Design Patterns (DPs) are recurring solutions to common problems in software design aiming to improve code reusability, maintainability and comprehensibility. Despite such advantages, the adoption of DPs causes the presence of crosscutting code increasing, significantly, the code duplication and the dependencies between systems. The main idea of this research is that code crosscutting can be reduced by the integration of Model Driven Development (MDD) techniques with Aspect Oriented Programming (AOP). According to this, an approach based on a Domain Specification Language (DSL) to to define declaratively the structure of DPs and their adoption on classes to declarative, is proposed. The approach aims to support aspects derivation to compose, at run time, AOP-based version of the specified DPs. The approach has been applied in a case study where the developed supporting framework was used in a concrete refactoring scenario, and a subsequent maintenance task. The results from the case study are presented and discussed.", "num_citations": "1\n", "authors": ["607"]}
{"title": "Declarative design pattern-based development using aspect oriented programming\n", "abstract": " Aspect Oriented Programming (AOP) can help to reduce crosscutting in the implementation of Design Patterns (DP), due to typical deficiencies of Object Oriented languages that can affect negatively the quality of the overall software system. The implementation of DPs may be further improved by using Model Driven Development techniques together with AOP. We have defined an approach to specify and to apply, declaratively, DPs to the classes of the base system. A Domain Specification Language (DSL) has been defined to specify declaratively the structure of DPs and their adoption on classes. From the DSL specifications, aspects are derived to compose, at run-time, an AOP-based version of the specified DPs. The approach was validated by a case study where the developed supporting framework was used in a concrete development scenario, and subsequent maintenance task. The results from the case\u00a0\u2026", "num_citations": "1\n", "authors": ["607"]}
{"title": "Process Lines for Automatic Workflow Development.\n", "abstract": " In some business environments, processes of different organizations are very similar to each other. This produces families of processes with common characteristics but also portions that vary according to the specific organization. Two emerging approaches can be adopted and combined to easily model, implement and update families of business processes: Software Product Line (SPL) and Service-Oriented Architecture (SOA). Our work suggests a framework to transfer the main peculiarities of the SPL to the SOA system development, in order to realize a SOA system line. Starting from the SPL concept, we introduce process lines, ie, families of process models suitable for different customers or market segments. Moreover, we present an approach for the automatic generation of a SOA system starting from a process model. The combination of these approaches, can be used to easily develop a family of SOA systems each one appropriate for different context characteristics. In this work, an application of the proposed approach in a real project is also proposed.", "num_citations": "1\n", "authors": ["607"]}
{"title": "Controlled Experiment on Search Engine Knowledge Extraction Capabilities.\n", "abstract": " Continuous pressure on behalf of enterprises leads to a constant need for innovation. This involves exchanging results of knowledge and innovation among research groups and enterprises in accordance to the Open Innovation paradigm. The technologies that seem to be apparently attractive for exchanging knowledge are Internet and its search engines. Literature provides many discordant opinions on their efficacy, and to our best knowledge, no empirical evidence on the topic. This work starts from the definition of a Knowledge Acquisition Process, and presents a rigorous empirical investigation that evaluates the efficacy of the previous technologies within the Exploratory Search of Knowledge and of Relevant Knowledge according to specific requirements. The investigation has pointed out that these technologies are not effective for Explorative Search. The paper concludes with a brief analysis of other technologies to develop and analyze in order to overcome the weaknesses that this investigation has pointed out within the Knowledge Acquisition Process.", "num_citations": "1\n", "authors": ["607"]}
{"title": "Knowledge economy in software engineering\n", "abstract": " In recent years, because of the competitive pressure, companies are encouraged to make frequent and continuous innovation of production processes and products. Consequently, there has been deploying the commonly call, knowledge society. This is characterized by an economy where the role of knowledge has taken on a dimension and a very different importance and far greater than that which has been in industrial society. This role is such that the economists now talk of Knowledge Economy (KE) and introduce awareness among the primary factors of production.In knowledge society, the exchange of research results and innovations, among researchers, between companies and between companies and researchers, assumes a critical role according to the paradigm of Open Innovation (OI).", "num_citations": "1\n", "authors": ["607"]}