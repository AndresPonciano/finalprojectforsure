{"title": "Evaluating the lifespan of code smells using software repository mining\n", "abstract": " An anti-pattern is a commonly occurring solution to a recurring problem that will typically negatively impact code quality. Code smells are considered to be symptoms of anti-patterns and occur at source code level. The lifespan of code smells in a software system can be determined by mining the software repository on which the system is stored. This provides insight into the behaviour of software developers with regard to resolving code smells and anti-patterns. In a case study, we investigate the lifespan of code smells and the refactoring behaviour of developers in seven open source systems. The results of this study indicate that engineers are aware of code smells, but are not very concerned with their impact, given the low refactoring activity.", "num_citations": "157\n", "authors": ["112"]}
{"title": "Strategies for avoiding test fixture smells during software evolution\n", "abstract": " An important challenge in creating automated tests is how to design test fixtures, i.e., the setup code that initializes the system under test before actual automated testing can start. Test designers have to choose between different approaches for the setup, trading off maintenance overhead with slow test execution. Over time, test code quality can erode and test smells can develop, such as the occurrence of overly general fixtures, obscure inline code and dead fields. In this paper, we investigate how fixture-related test smells evolve over time by analyzing several thousand revisions of five open source systems. Our findings indicate that setup management strategies strongly influence the types of test fixture smells that emerge in code, and that several types of fixture smells often emerge at the same time. Based on this information, we recommend important guidelines for setup strategies, and suggest how tool support\u00a0\u2026", "num_citations": "54\n", "authors": ["112"]}
{"title": "Scalability solutions for program comprehension through dynamic analysis\n", "abstract": " Dynamic analysis has long been a subject of study in the context of (compiler) optimization, program comprehension, test coverage, etc. Ever-since, the scale of the event trace has been an important issue. This scalability issue finds its limits on the computational front, where time and/or space complexity of algorithms become too large to be handled by a computer, but also on the cognitive front, where the results presented to the user become too large to be easily understood. This research focuses on delivering a number of program comprehension solutions that help software engineers to focus on the software system during their initial program exploration and comprehension phases. The key concepts we use in our techniques are \"frequency of execution\" and runtime \"coupling\". To validate our techniques we used a number of open-source software systems, as well as an industrial legacy application", "num_citations": "47\n", "authors": ["112"]}
{"title": "A systematic literature review on quality criteria for agile requirements specifications\n", "abstract": " The quality of requirements is typically considered as an important factor for the quality of the end product. For traditional up-front requirements specifications, a number of standards have been defined on what constitutes good quality : Requirements should be complete, unambiguous, specific, time-bounded, consistent, etc. For agile requirements specifications, no new standards have been defined yet, and it is not clear yet whether traditional quality criteria still apply. To investigate what quality criteria for assessing the correctness of written agile requirements exist, we have conducted a systematic literature review. The review resulted in a list of 16 selected papers on this topic. These selected papers describe 28 different quality criteria for agile requirements specifications. We categorize and analyze these criteria and compare them with those from traditional requirements engineering. We discuss findings\u00a0\u2026", "num_citations": "42\n", "authors": ["112"]}
{"title": "Studying fine-grained co-evolution patterns of production and test code\n", "abstract": " Numerous software development practices suggest updating the test code whenever the production code is changed. However, previous studies have shown that co-evolving test and production code is generally a difficult task that needs to be thoroughly investigated. In this paper we perform a study that, following a mixed methods approach, investigates fine-grained co-evolution patterns of production and test code. First, we mine fine-grained changes from the evolution of 5 open-source systems. Then, we use an association rule mining algorithm to generate the co-evolution patterns. Finally, we interpret the obtained patterns by performing a qualitative analysis. The results show 6 co-evolution patterns and provide insights into their appearance along the history of the analyzed software systems. Besides providing a better understanding of how test code evolves, these findings also help identify gaps in the test\u00a0\u2026", "num_citations": "38\n", "authors": ["112"]}
{"title": "Aiding software developers to maintain developer tests\n", "abstract": " Unit and integration tests can be invaluable during software maintenance as they help to understand pieces of code, they help with quality assurance and they build up confidence amongst developers. Unfortunately then, previous research has shown that unit tests do not always co-evolve nicely with the production code, thus leaving the software vulnerable. This paper presents TestNForce, a tool that helps developers to identify the unit tests that need to be altered and executed after a code change, thereby reducing the effort needed to keep the unit tests in sync with the changes to the production code. In order to evaluate TestNForce, we perform a user study that evaluates the adequacy, usefulness and completeness of TestNForce.", "num_citations": "31\n", "authors": ["112"]}
{"title": "A quality framework for agile requirements: A practitioner's perspective\n", "abstract": " Verification activities are necessary to ensure that the requirements are specified in a correct way. However, until now requirements verification research has focused on traditional up-front requirements. Agile or just-in-time requirements are by definition incomplete, not specific and might be ambiguous when initially specified, indicating a different notion of 'correctness'. We analyze how verification of agile requirements quality should be performed, based on literature of traditional and agile requirements. This leads to an agile quality framework, instantiated for the specific requirement types of feature requests in open source projects and user stories in agile projects. We have performed an initial qualitative validation of our framework for feature requests with eight practitioners from the Dutch agile community, receiving overall positive feedback.", "num_citations": "28\n", "authors": ["112"]}
{"title": "An analysis of requirements evolution in open source projects: Recommendations for issue trackers\n", "abstract": " While requirements for open source projects originate from a variety of sources like eg mailing lists or blogs, typically, they eventually end up as feature requests in an issue tracking system. When analyzing how these issue trackers are used for requirements evolution, we witnessed a high percentage of duplicates in a number of high-prole projects. Further investigation of six open source projects and their users led us to a number of important observations and a categorization of the root causes of these duplicates. Based on this, we propose a set of improvements for future issue tracking systems.", "num_citations": "24\n", "authors": ["112"]}
{"title": "Horizontal traceability for just\u2010in\u2010time requirements: the case for open source feature requests\n", "abstract": " Agile projects typically employ just\u2010in\u2010time requirements engineering and record their requirements (so\u2010called feature requests) in an issue tracker. In open source projects, we observed large networks of feature requests that are linked to each other. Both when trying to understand the current state of the system and to understand how a new feature request should be implemented, it is important to know and understand all these (tightly) related feature requests. However, we still lack tool support to visualize and navigate these networks of feature requests. A first step in this direction is to see whether we can identify additional links that are not made explicit in the feature requests, by measuring the text\u2010based similarity with a vector space model (VSM) using term frequency\u2010inverse document frequency (TF\u2010IDF) as a weighting factor. We show that a high text\u2010based similarity score is a good indication for related\u00a0\u2026", "num_citations": "23\n", "authors": ["112"]}
{"title": "Old habits die hard: Why refactoring for understandability does not give immediate benefits\n", "abstract": " Depending on the context, the benefits of clean code with respect to understandability might be less obvious in the short term than is often claimed. In this study we evaluate whether a software system with legacy code in an industrial environment benefits from a \u201cclean code\u201d refactoring in terms of developer productivity. We observed both increases as well as decreases in understandability, showing that immediate increases in understandability are not always obvious. Our study suggests that refactoring code could result in a productivity penalty in the short term if the coding style becomes different from the style developers have grown attached to.", "num_citations": "22\n", "authors": ["112"]}
{"title": "A framework-based runtime monitoring approach for service-oriented software systems\n", "abstract": " The highly dynamic and loosely coupled nature of a service-oriented software system leads to the challenge of understanding it. In order to obtain insight into the runtime topology of a SOA system, we propose a framework-based runtime monitoring approach to trace the service interactions during execution. The approach can be transparently applied to all web services built on the framework and reuses parts of information and functionality already available in the framework to achieve our goals.", "num_citations": "22\n", "authors": ["112"]}
{"title": "Refactoring with unit testing: A match made in heaven?\n", "abstract": " Unit testing is a basic principle of agile development. Its benefits include early defect detection, defect cause localization and removal of fear to apply changes to the code. As such, unit tests seem to be ideal companions during refactoring, as they provide a safety net which enables to quickly verify that behaviour is indeed preserved. In this study we investigate whether having unit tests available during refactoring actually leads to quicker refactorings and more high-quality code after refactoring. For this, we set up a two-group controlled experiment involving 42 participants. Results indicate that having unit tests available during refactoring does not lead to quicker refactoring or to higher-quality code after refactoring.", "num_citations": "21\n", "authors": ["112"]}
{"title": "A framework for quality assessment of just-in-time requirements: the case of open source feature requests\n", "abstract": " Until now, quality assessment of requirements has focused on traditional up-front requirements. Contrasting these traditional requirements are just-in-time (JIT) requirements, which are by definition incomplete, not specific and might be ambiguous when initially specified, indicating a different notion of \"correctness.\" We analyze how the assessment of JIT requirements quality should be performed based on the literature of traditional and JIT requirements. Based on that analysis, we have designed a quality framework for JIT requirements and instantiated it for feature requests in open source projects. We also indicate how the framework can be instantiated for other types of JIT requirements. We have performed an initial evaluation of our framework for feature requests with eight practitioners from the Dutch agile community, receiving overall positive feedback. Subsequently, we have used our framework to\u00a0\u2026", "num_citations": "18\n", "authors": ["112"]}
{"title": "Identifying problems with legacy software; preliminary findings of the ARRIBA project\n", "abstract": " The goal of this experience report is to identify some of the key problems of today's enterprises that have to deal with managing their large business critical software systems. Our motivation to do so is based on preliminary find-ings from the ARRIBA project. The work we present here forn our preliminary conclusions of the first 6 months of the project, where we visited some of these enterprises, to identify their main needs of today.", "num_citations": "15\n", "authors": ["112"]}
{"title": "Spectrum-based fault diagnosis for service-oriented software systems\n", "abstract": " Due to the loosely coupled and highly dynamic nature of service-oriented systems, the actual configuration of such system only fully materializes at runtime, rendering many of the traditional quality assurance approaches useless. In order to enable service systems to recover from and adapt to runtime failures, an important step is to detect failures and diagnose problematic services automatically. This paper presents a lightweight, fully automated, spectrum-based diagnosis technique for service-oriented software systems that is combined with a framework-based online monitor. An experiment with a case system is set up to validate the feasibility of pinpointing problematic service operations. The results indicate that this approach is able to identify problematic service operations correctly in 72% of the cases.", "num_citations": "14\n", "authors": ["112"]}
{"title": "FireDetective: understanding ajax client/server interactions\n", "abstract": " Ajax-enabled web applications are a new breed of highly interactive, highly dynamic web applications. Although Ajax allows developers to create rich web applications, Ajax applications can be difficult to comprehend and thus to maintain. FireDetective aims to facilitate the understanding of Ajax applications. It uses dynamic analysis at both the client (browser) and server side and subsequently connects both traces for further analysis.", "num_citations": "14\n", "authors": ["112"]}
{"title": "Do as i do, not as i say: Do contribution guidelines match the github contribution process?\n", "abstract": " Developer contribution guidelines are used in social coding sites like GitHub to explain and shape the process a project expects contributors to follow. They set standards for all participants and \"save time and hassle caused by improperly created pull requests or issues that have to be rejected and re-submitted\" (GitHub). Yet, we lack a systematic understanding of the content of a typical contribution guideline, as well as the extent to which these guidelines are followed in practice. Additionally, understanding how guidelines may impact projects that use Continuous Integration as part of the contribution process is of particular interest. To address this knowledge gap, we conducted a mixed-methods study of 53 GitHub projects with explicit contribution guidelines and coded the guidelines to extract key themes. We then created a process model using GitHub activity data (e.g., commit, new issue, new pull request) to\u00a0\u2026", "num_citations": "13\n", "authors": ["112"]}
{"title": "An investigation of compression techniques to speed up mutation testing\n", "abstract": " Mutation testing is widely considered as a high-end test coverage criterion due to the vast number of mutants it generates. Although many efforts have been made to reduce the computational cost of mutation testing, in practice, the scalability issue remains. In this paper, we explore whether we can use compression techniques to improve the efficiency of strong mutation based on weak mutation information. Our investigation is centred around six mutation compression strategies that we have devised. More specifically, we adopt overlapped grouping and Formal Concept Analysis (FCA) to cluster mutants and test cases based on the reachability (code coverage) and necessity (weak mutation) conditions. Moreover, we leverage mutation knowledge (mutation locations and mutation operator types) during compression. To evaluate our method, we conducted a study on 20 open source Java projects using manually\u00a0\u2026", "num_citations": "13\n", "authors": ["112"]}
{"title": "Quality criteria for just-in-time requirements: just enough, just-in-time?\n", "abstract": " Just-in-time (JIT) requirements drive agile teams in planning and implementing software systems. In this paper, we start with the hypothesis that performing informal verification of JIT requirements is useful. For this purpose we propose a framework for quality criteria for JIT requirements. This framework can be used by JIT teams to define 'just-enough' quality criteria. The framework also includes a time dimension such that quality criteria can be defined as 'just-in-time'. We demonstrate the application of this framework to feature requests in open source projects and explain how it could be customized for other JIT environments. We present our results for feature requests in open source projects, to show that there is a difference between creation-time quality and just-in-time quality. As this is ongoing research, we also list several points for discussion and future work.", "num_citations": "13\n", "authors": ["112"]}
{"title": "Improving service diagnosis through increased monitoring granularity\n", "abstract": " Due to their loose coupling and highly dynamic nature, service-oriented systems offer many benefits for realizing fault tolerance and supporting trustworthy computing. They enable automatic system reconfiguration in case that a faulty service is detected. Spectrum-based fault localization (SFL) is a statistics-based diagnosis technique that can effectively be applied to pinpoint problematic services. It works by monitoring service usage in system transactions and comparing service coverage with pass/fail observations. SFL exhibits poor performance in diagnosing faulty services in cases when services are tightly coupled. In this paper, we study how and to which extent an increase in monitoring granularity can help to improve correct diagnosis of tightly coupled faulty services. We apply SFL in a real service-based system, for which we show that 100% correct identification of faulty services can be achieved through an\u00a0\u2026", "num_citations": "13\n", "authors": ["112"]}
{"title": "A lightweight approach to determining the adequacy of tests as documentation\n", "abstract": " Programming process paradigms such as the Agile process and eXtreme Programming (XP) tend to minimise ceremony, favouring working code over documentation. They do, however, advocate the use of tests as a form of \u201cliving documentation\u201d. This research tries to make an initial assessment of whether these unit tests can indeed serve as a form of full-fledged documentation. The lightweight approach we propose is mainly based on the number of units that is covered by each unit test. This paper discusses the approach, the corresponding tool and the results of a first case study.", "num_citations": "13\n", "authors": ["112"]}
{"title": "How developers debug\n", "abstract": " Debugging software is an inevitable chore, often difficult and more time-consuming than expected, giving it the nickname the \u201c dirty little secret of computer science.\u201d Surprisingly, we have little knowledge on how software engineers debug software problems in the real world, whether they use dedicated debugging tools, and how knowledgeable they are about debugging. This study aims to shed light on these aspects by following a mixed-methods research approach. We conduct an online survey capturing how 176 developers reflect on debugging. We augment this subjective survey data with objective observations from how 458 developers use the debugger included in their Integrated Development Environments (IDEs) by instrumenting the popular ECLIPSE and INTELLIJ IDEs with our purpose-built plugin WATCHDOG 2.0. To better explain the insights and controversies obtained from the previous steps, we followed up by conducting interviews with debugging experts and regular debugging users. Our results indicate that the the IDE-provided debugger is not used as often as expected, since \u201cprintf debugging\u201d remains a feasible choice for many programmers. Furthermore, both knowledge and use of advanced debugging features are low. Our results call to strengthen hands-on debugging experience in Computer Science curricula and can and have already influenced the design of modern IDE debuggers.", "num_citations": "9\n", "authors": ["112"]}
{"title": "Software engineering in the Netherlands: the state of the practice\n", "abstract": " In order to determine whether there is a gap between the current state-of-the-practice and state-of-the-art in software engineering, we performed a broad survey among Dutch software producing organizations. Our survey covers aspects of the software engineering cycle ranging from requirements engineering, over design and implementation to testing. From our analysis of the data that we have obtained from our 99 respondents, we extracted 22 interesting observations, some representing unexpected insights from an academic point of view. From these observations, we have identified a number of avenues for future research.", "num_citations": "9\n", "authors": ["112"]}
{"title": "A systematic literature review of how mutation testing supports test activities\n", "abstract": " Mutation testing has been very actively investigated by researchers since the 1970s and remarkable advances have been achieved in its concepts, theory, technology and empirical evidence. While the latest realisations have been summarised by existing literature review, we lack insight into how mutation testing is actually applied. Our goal is to identify and classify the main applications of mutation testing and analyse the level of replicability of empirical studies related to mutation testing. To this aim, this paper provides a systematic literature review on the application perspective of mutation testing based on a collection of 159 papers published between 1981 and 2015. In particular, we analysed in which testing activities mutation testing is used, which mutation tools and which mutation operators are employed. Additionally, we also investigated how the core inherent problems of mutation testing, ie the equivalent mutant problem and the high computational cost, are addressed during the actual usage. The results show that most studies use mutation testing as an assessment tool targeting unit tests, and many of the supporting techniques for making mutation testing applicable in practice are still underdeveloped. Based on our observations, we made nine recommendations for the future work, including an important suggestion on how to report mutation testing in testing experiments in an appropriate manner.", "num_citations": "6\n", "authors": ["112"]}
{"title": "Studying co-evolution of production and test code using association rule mining\n", "abstract": " Long version of the short paper accepted for publication in the proceedings of the 6th International Working Conference on Mining Software Repositories (MSR 2009). Unit tests are generally acknowledged as an important aid to produce high quality code, as they provide quick feedback to developers on the correctness of their code. In order to achieve high quality, well-maintained tests are needed. Ideally, tests co-evolve with the production code to test changes as soon as possible. In this paper, we explore an approach to determine whether production and test code co-evolve synchronously. Our approach is based on applying association rule mining to the change history of product and test code classes. Based on these co-evolution rules, we introduce a number of measures to assess the co-evolution of product and test code classes. Through two case studies, one with an open source and another one with an industrial software system, we show that association rule mining and our set of measures allows one to assess the co-evolution of product and test code in a software project and, moreover, to uncover the distribution of programmer effort over pure coding, pure testing, or a more test-driven-like practice.", "num_citations": "5\n", "authors": ["112"]}
{"title": "Mutation testing for physical computing\n", "abstract": " Physical computing, which builds interactive systems between the physical world and computers, has been widely used in a wide variety of domains and applications, e.g., the Internet of Things (IoT). Although physical computing has witnessed enormous realisations, testing these physical computing systems still face many challenges, such as potential circuit related bugs which are not part of the software problems, the timing issue which decreasing the testability, etc.; therefore, we proposed a mutation testing approach for physical computing systems to enable engineers to judge the quality of their tests in a more accurate way. The main focus is the communication between the software and peripherals. More particular, we first defined a set of mutation operators based on the common communication errors between the software and peripherals that could happen in the software. We conducted a preliminary\u00a0\u2026", "num_citations": "4\n", "authors": ["112"]}
{"title": "Program Comprehension through Dynamic Analysis\n", "abstract": " Software maintenance and evolution can be made easier with program comprehension techniques. The aim of this workshop is to gather together researchers working in the area of program comprehension with an emphasis on dynamic analysis. We are interested in investigating how dynamic analysis techniques are or can be used to enable better comprehension of a software system. The objective is to find common case studies, compare existing techniques, and find possible symbioses for existing solutions. Building upon the previous edition of the workshop, PCODA 2005, we aim to set up a forum for exchanging experiences, discussing solutions, and exploring new ideas.", "num_citations": "3\n", "authors": ["112"]}
{"title": "Massively parallel, highly efficient, but what about the test suite quality? applying mutation testing to gpu programs\n", "abstract": " Thanks to rapid advances in programmability and performance, GPUs have been widely applied in High-Performance Computing (HPC) and safety-critical domains. As such, quality assurance of GPU applications has gained increasing attention. This brings us to mutation testing, a fault-based testing technique that assesses the test suite quality by systematically introducing small artificial faults. It has been shown to perform well in exposing faults. In this paper, we investigate whether GPU programming can benefit from mutation testing. In addition to conventional mutation operators, we propose nine GPU-specific mutation operators based on the core syntax differences between CPU and GPU programming. We conduct a preliminary study on six CUDA systems. The results show that mutation testing can effectively evaluate the test quality of GPU programs: conventional mutation operators can guide the engineers\u00a0\u2026", "num_citations": "2\n", "authors": ["112"]}
{"title": "Analysis of service diagnosis improvement through increased monitoring granularity\n", "abstract": " Due to their loosely coupled and highly dynamic nature, service-oriented systems offer many benefits for realizing fault tolerance and supporting trustworthy computing. They enable automatic system reconfiguration when a faulty service is detected. Spectrum-based fault localization (SFL) is a statistics-based diagnosis technique that can be effectively applied to pinpoint problematic services. However, SFL exhibits poor performance in diagnosing services which are tightly interacted. Previous research suggests that an increase in the number of monitoring locations may improve the diagnosability for tight interaction. In this paper, we analyze the trade-offs between the diagnosis improvement through increased monitoring granularity and the overhead caused by the introduction of more monitors, when diagnosing tightly interacted faulty services. We apply SFL in a service-based system, for which we show\u00a0\u2026", "num_citations": "2\n", "authors": ["112"]}
{"title": "Comparing diagnostic performance of ochiai and relief in service-oriented systems\n", "abstract": " @inproceedings{ChenDX2013b, author = {Cuiting Chen and Brian Omoro and Hans-Gerhard Gross and Andy Zaidman}, title = {Comparing Diagnostic Performance of Ochiai and Relief in Service-oriented Systems}, booktitle = {Proceedings of the 24th International Workshop on Principles of Diagnosis (DX 2013)}, year = {2013}, editor = {Alexander Feldman and Meir Kalech and Gregory Provan}, pages = {39-44}, url = {http://www.dx-2013.org/dx13-proceedings.pdf}, project = {ScaleItUp,msc-thesis-project}, group = {SE} } @inproceedings{ChenDX2013a, author = {Cuiting Chen and Hans-Gerhard Gross and Andy Zaidman}, title = {Using Genetic Algorithms to Study the Effects of Topology on Spectrum Based Diagnosis}, booktitle = {Proceedings of the 24th International Workshop on Principles of Diagnosis (DX 2013)}, year = {2013}, editor = {Alexander Feldman and Meir Kalech and Gregory Provan}, pages = {166-173}\u2026", "num_citations": "2\n", "authors": ["112"]}
{"title": "Workshop on Program Comprehension through Dynamic Analysis (PCODA \u201805)\n", "abstract": " Software maintenance and evolution can be made easier if program comprehension techniques are used. Understanding a software system would typically necessitate a combination of static and dynamic analysis techniques. The aim of this workshop is to gather researchers working in the area of program comprehension with an emphasis on dynamic analysis. We are interested in investigating how dynamic analysis techniques are used or can be used to enable better comprehension of a software system.", "num_citations": "2\n", "authors": ["112"]}
{"title": "It Is Not Only About Control Dependent Nodes: Basic Block Coverage for Search-Based Crash Reproduction\n", "abstract": " Search-based techniques have been widely used for white-box test generation. Many of these approaches rely on the approach level and branch distance heuristics to guide the search process and generate test cases with high line and branch coverage. Despite the positive results achieved by these two heuristics, they only use the information related to the coverage of explicit branches (e.g.,\u00a0indicated by conditional and loop statements), but ignore potential implicit branchings within basic blocks of code. If such implicit branching happens at runtime (e.g.,\u00a0if an exception is thrown in a branchless-method), the existing fitness functions cannot guide the search process. To address this issue, we introduce a new secondary objective, called Basic Block Coverage (BBC), which takes into account the coverage level of relevant basic blocks in the control flow graph. We evaluated the impact of BBC on search\u00a0\u2026", "num_citations": "1\n", "authors": ["112"]}
{"title": "Improving service diagnosis through invocation monitoring\n", "abstract": " Service oriented architectures support software runtime evolution through reconfiguration of misbehaving services. Reconfiguration requires that the faulty services can be identified correctly. Spectrum-based fault localization is an automated diagnosis technique that can be applied to faulty service detection. It is based on monitoring service involvement in passed and failed system transactions. Monitoring only the involvement of services sometimes leads to inconclusive diagnoses. In this paper, we propose to extend monitoring to include also the invocation links between the services. We show through simulations and a case study with a real system under which circumstances service monitoring alone inhibits the correct detection of a faulty service, and how and to which extent the inclusion of invocation monitoring can lead to improved service diagnosis.", "num_citations": "1\n", "authors": ["112"]}
{"title": "Using genetic algorithms to study the effects of topology on spectrum based diagnosis\n", "abstract": " Spectrum-based fault localization (SFL) is a statistical fault diagnosis technique that infers diagnoses from runtime observations. It works by monitoring system transactions, and comparing activity information with pass/fail observations. SFL requires the monitors, which recover the activity data, to be organized to produce optimal information for the diagnosis. This organization is termed topology. Optimality of monitoring topology for diagnosability represents a search or optimization problem amenable to be addressed by metaheuristic algorithms. In order to study the effects of topology on the production of diagnoses through SFL, we use genetic algorithms (GA) to generate topologies that lead to improved diagnosability. We illustrate how monitoring topologies affect the diagnosability of systems, and how GA can help to study these effects. We derive general characteristics of topologies to facilitate SFL-based diagnoses.", "num_citations": "1\n", "authors": ["112"]}
{"title": "Workshop on program comprehension through dynamic analysis (PCODA'08)\n", "abstract": " Applying program comprehension techniques may render software maintenance and evolution easier. Understanding a software system typically requires a combination of static and dynamic analysis techniques. The aim of this workshop is to bring together researchers and practitioners working in the area of program comprehension with an emphasis on dynamic analysis. We are interested in investigating how dynamic analysis techniques are used or can be used to enable better comprehension of a software system. The objective is to compare existing techniques, identify common case studies and possible symbioses for existing solutions. Building upon three previous editions of the workshop, we aim to set up a forum for exchanging experiences, discussing solutions, and exploring new ideas.", "num_citations": "1\n", "authors": ["112"]}