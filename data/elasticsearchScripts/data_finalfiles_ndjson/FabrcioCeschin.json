{"title": "The Need for Speed: An Analysis of Brazilian Malware Classifers\n", "abstract": " Using a dataset containing about 50,000 samples from Brazilian cyberspace, we show that relying solely on conventional machine-learning systems without taking into account the change of the subject's concept decreases the performance of classification, emphasizing the need to update the decision model immediately after concept drift occurs.", "num_citations": "19\n", "authors": ["1100"]}
{"title": "Shallow Security: on the Creation of Adversarial Variants to Evade Machine Learning-Based Malware Detectors\n", "abstract": " The use of Machine Learning (ML) techniques for malware detection has been a trend in the last two decades. More recently, researchers started to investigate adversarial approaches to bypass these ML-based malware detectors. Adversarial attacks became so popular that a large Internet company has launched a public challenge to encourage researchers to bypass their (three) ML-based static malware detectors. Our research group teamed to participate in this challenge in August/2019, accomplishing the bypass of all 150 tests proposed by the company. To do so, we implemented an automatic exploitation method which moves the original malware binary sections to resources and includes new chunks of data to it to create adversarial samples that not only bypassed their ML detectors, but also real AV engines as well (with a lower detection rate than the original samples). In this paper, we detail our\u00a0\u2026", "num_citations": "13\n", "authors": ["1100"]}
{"title": "We need to talk about antiviruses: challenges & pitfalls of av evaluations\n", "abstract": " Security evaluation is an essential task to identify the level of protection accomplished in running systems or to aid in choosing better solutions for each specific scenario. Although antiviruses (AVs) are one of the main defensive solutions for most end-users and corporations, AV\u2019s evaluations are conducted by few organizations and often limited to compare detection rates. Moreover, other important factors of AVs\u2019 operating mode (e.g., response time and detection regression) are usually underestimated. Ignoring such factors create an \u201cunderstanding gap\u201d on the effectiveness of AVs in actual scenarios, which we aim to bridge by presenting a broader characterization of current AVs\u2019 modes of operation. In our characterization, we consider distinct file types, operating systems, datasets, and time frames. To do so, we daily collected samples from two distinct, representative malware sources and submitted them to the\u00a0\u2026", "num_citations": "12\n", "authors": ["1100"]}
{"title": "Predicting misinformation and engagement in covid-19 twitter discourse in the first months of the outbreak\n", "abstract": " Predicting Misinformation and Engagement in COVID-19 Twitter Discourse in the First Months of the Outbreak 111: 3 because the latter implies purposeful malice. When false information is spread unintentionally\u2014for example, when a user is successfully deceived by a piece of disinformation, and naively retweets the falsehood\u2014this spread of deceptive content is instead referred to as misinformation. For the remainder of this paper, we will use the term misinformation to refer to tweets spreading deceptive content, even though some misinformation tweets might have been created with malice. Using a dataset [17] originally containing over 3M tweets (later decreased to nearly\u223c 504K tweets after pre-processing) with the labels of fact and misinformation, collected during February and March 2020, just prior to the World Health Organization\u2019s (WHO) recognition of COVID-19 as a pandemic, we sought to answer the following research questions:\u2022 RQ1: Do Twitter bot accounts spread more misinformation than real accounts?\u2022 RQ2: Are misinformation tweets more engaging than factual tweets?\u2022 RQ3: Which features are most relevant to distinguish between factual and misinformation tweets?\u2022 RQ4: Which features are most relevant to predict high engagement in misinformation tweets?\u2022 RQ5: Which features are most relevant to predict high engagement in factual tweets? To do so, we first investigated bot-like behavior in the dataset using the Tweetbotornot2 1 a machine learning-based tool that estimates the likelihood of a given Twitter account being a bot. Then, we measured engagement as the summation of# likes and# of retweets and proceeded to\u00a0\u2026", "num_citations": "10\n", "authors": ["1100"]}
{"title": "L (a) ying in (Test) Bed\n", "abstract": " The number of malware variants released daily turned manual analysis into an impractical task. Although potentially faster, automated analysis techniques (e.g., static and dynamic) have shortcomings that are exploited by malware authors to thwart each of them, i.e., prevent malicious software from being detected or classified accordingly. Researchers then invested in traditional machine learning algorithms to try to produce efficient, effective classification methods. The produced models are also prone to errors and attacks. Novel representations of the \u201csubject\u201d were proposed to overcome previous limitations, such as malware textures. In this paper, our initial proposal was to evaluate the application of texture analysis for malware classification using samples collected in-the-wild in order to compare them with state-of-the-art results. During our tests, we discovered that texture analysis may be unfeasible for\u00a0\u2026", "num_citations": "6\n", "authors": ["1100"]}
{"title": "Challenges and pitfalls in malware research\n", "abstract": " As the malware research field became more established over the last two decades, new research questions arose, such as how to make malware research reproducible, how to bring scientific rigor to attack papers, or what is an appropriate malware dataset for relevant experimental results. The challenges these questions pose also brings pitfalls that affect the multiple malware research stakeholders. To help answering those questions and to highlight potential research pitfalls to be avoided, in this paper, we present a systematic literature review of 491 papers on malware research published in major security conferences between 2000 and 2018. We identified the most common pitfalls present in past literature and propose a method for assessing current (and future) malware research. Our goal is towards integrating science and engineering best practices to develop further, improved research by learning from\u00a0\u2026", "num_citations": "3\n", "authors": ["1100"]}
{"title": "The AV says: Your Hardware Definitions Were Updated!\n", "abstract": " Although malware is a threat for most systems, the main line of defense against them (AntiViruses, or AVs) are performance-intensive applications that cause slow down due to the need of constant target-system monitoring. An effective alternative for accelerating AVs operation is to move them from software to hardware, thus eliminating their imposed performance overhead. Hardware-AVs, in turn, present another drawback: the update of malicious definitions is essential for AVs working in constant changing scenarios, but challenging to be deployed in hardware. In this paper, we propose REHAB (REconfigurable, Hardware-Assisted Blocker for malware), a reconfigurable, hardware-based AV that eliminates the performance overhead imposed by standard AVs and streamlines malicious definitions updates. REHAB is based on low-level features (e.g., branch prediction and cache accesses rates) which are classified\u00a0\u2026", "num_citations": "3\n", "authors": ["1100"]}
{"title": "No Need to Teach New Tricks to Old Malware: Winning an Evasion Challenge with XOR-based Adversarial Samples\n", "abstract": " Adversarial attacks to Machine Learning (ML) models became such a concern that tech companies (Microsoft and CUJO AI\u2019s Vulnerability Research Lab) decided to launch contests to better understand their impact on practice. During the contest\u2019s first edition (2019), participating teams were challenged to bypass three ML models in a white box manner. Our team bypassed all the three of them and reported interesting insights about models\u2019 weaknesses. In the second edition (2020), the challenge evolved to an attack-and-defense model: the teams should either propose defensive models and attack other teams\u2019 models in a black box manner. Despite the difficulty increase, our team was able to bypass all models again. In this paper, we describe our insights for this year\u2019s contest regarding on attacking models, as well defending them from adversarial attacks. In particular, we show how frequency-based models (eg\u00a0\u2026", "num_citations": "2\n", "authors": ["1100"]}
{"title": "Understanding uses and misuses of similarity hashing functions for malware detection and family clustering in actual scenarios\n", "abstract": " An everyday growing number of malware variants target end-users and organizations. To reduce the amount of individual malware handling, security analysts apply techniques for finding similarities to cluster samples. A popular clustering method relies on similarity hashing functions, which create short representations of files and compare them to produce a score related to the similarity level between them. Despite the popularity of those functions, the limits of their application to malware samples have not been extensively studied so-far. To help in bridging this gap, we performed a set of experiments to characterize the application of these functions on long-term, realistic malware analysis scenarios. To do so, we introduce SHAVE, an ideal model of similarity hashing-based antivirus engine. The evaluation of SHAVE consisted of applying two distinct hash functions (ssdeep and sdhash) to a dataset of 21 thousand\u00a0\u2026", "num_citations": "1\n", "authors": ["1100"]}
{"title": "Need for Speed: analysis of brazilian malware classifiers' expiration date\n", "abstract": " Novos programas maliciosos s\u00e3o criados e liberados diariamente para enganar usu\u00e1rios e superar solu\u00e7\u00f5es de seguran\u00e7a, assim exigindo melhora continua nestes mecanismos (por exemplo, atualiza\u00e7\u00e3o constante de antiv\u00edrus). Apesar da maioria dos programas maliciosos serem \"gen\u00e9ricos suficiente para infectar o mesmo tipo de sistema operacional mundialmente, alguns deles est\u00e3o relacionados as especificidades de um ciberespa\u00e7o de certos pa\u00edses alvos. Neste trabalho, nos apresentemos uma analise de milhares de exemplares de malware coletados no ciberespa\u00e7o brasileiro ao longo de v\u00e1rios anos, incluindo suas evolu\u00e7\u00f5es e o impacto dessas evolu\u00e7\u00f5es na classifica\u00e7\u00e3o de malware. Nos tamb\u00e9m disponibilizamos um dataset desse conjunto de malware para permitir que outros experimentos e compara\u00e7\u00f5es sejam feitas pela comunidade. Este dataset representa o ciberespa\u00e7o brasileiro e contem perfis de programas que sao conhecidamente malignos e benignos, baseados em caracter\u00edsticas est\u00e1ticas de seus bin\u00e1rios. Nossa analise utilizou algoritmos de aprendizado de maquina (em particular, nos avaliamos quatro algoritmos populares off-the-shelf : Support Vector Machines, Multilayer Perceptron, KNN e Random Forest) para classificar os programas do nosso dataset como maligno ou benigno (incluindo experimentos com thresholds) e identificar o potencial concept drift que ocorre quando o modelo de classifica\u00e7\u00e3o evolui com o passar do tempo. Nos tamb\u00e9m providenciamos detalhes extensos sobre nosso dataset, que e composto por 38.000 programas - 20.000 rotulados como malignos, coletados de anexos de e-mails\u00a0\u2026", "num_citations": "1\n", "authors": ["1100"]}