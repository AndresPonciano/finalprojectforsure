{"title": "A C-LSTM Neural Network for Text Classification\n", "abstract": " Neural network models have been demonstrated to be capable of achieving remarkable performance in sentence and document modeling. Convolutional neural network (CNN) and recurrent neural network (RNN) are two mainstream architectures for such modeling tasks, which adopt totally different ways of understanding natural languages. In this work, we combine the strengths of both architectures and propose a novel and unified model called C-LSTM for sentence representation and text classification. C-LSTM utilizes CNN to extract a sequence of higher-level phrase representations, and are fed into a long short-term memory recurrent neural network (LSTM) to obtain the sentence representation. C-LSTM is able to capture both local features of phrases as well as global and temporal sentence semantics. We evaluate the proposed architecture on sentiment classification and question classification tasks. The experimental results show that the C-LSTM outperforms both CNN and LSTM and can achieve excellent performance on these tasks.", "num_citations": "751\n", "authors": ["1187"]}
{"title": "Relation Classification via Multi-Level Attention CNNs\n", "abstract": " Relation classification is a crucial ingredient in numerous information extraction systems seeking to mine structured facts from text. We propose a novel convolutional neural network architecture for this task, relying on two levels of attention in order to better discern patterns in heterogeneous contexts. This architecture enables endto-end learning from task-specific labeled data, forgoing the need for external knowledge such as explicit dependency structures. Experiments show that our model outperforms previous state-of-the-art methods, including those relying on much richer forms of prior knowledge.", "num_citations": "419\n", "authors": ["1187"]}
{"title": "End-to-end neural ad-hoc ranking with kernel pooling\n", "abstract": " This paper proposes K-NRM, a kernel based neural model for document ranking. Given a query and a set of documents, K-NRM uses a translation matrix that models word-level similarities via word embeddings, a new kernel-pooling technique that uses kernels to extract multi-level soft match features, and a learning-to-rank layer that combines those features into the final ranking score. The whole model is trained end-to-end. The ranking layer learns desired feature patterns from the pairwise ranking loss. The kernels transfer the feature patterns into soft-match targets at each similarity level and enforce them on the translation matrix. The word embeddings are tuned accordingly so that they can produce the desired soft matches. Experiments on a commercial search engine's query log demonstrate the improvements of K-NRM over prior feature-based and neural-based states-of-the-art, and explain the source of K\u00a0\u2026", "num_citations": "365\n", "authors": ["1187"]}
{"title": "Max-Margin DeepWalk: Discriminative Learning of Network Representation\n", "abstract": " DeepWalk is a typical representation learning method that learns low-dimensional representations for vertices in social networks. Similar to other network representation learning (NRL) models, it encodes the network structure into vertex representations and is learnt in unsupervised form. However, the learnt representations usually lack the ability of discrimination when applied to machine learning tasks, such as vertex classification. In this paper, we overcome this challenge by proposing a novel semisupervised model, max-margin DeepWalk (MMDW). MMDW is a unified NRL framework that jointly optimizes the max-margin classifier and the aimed social representation learning model. Influenced by the max-margin classifier, the learnt representations not only contain the network structure, but also have the characteristic of discrimination. The visualizations of learnt representations indicate that our model is more discriminative than unsupervised ones, and the experimental results on vertex classification demonstrate that our method achieves a significant improvement than other state-of-theart methods. The source code can be obtained from https://github. com/thunlp/MMDW.", "num_citations": "271\n", "authors": ["1187"]}
{"title": "Convolutional Neural Networks for Soft-Matching N-Grams in Ad-hoc Search\n", "abstract": " This paper presents\\textttConv-KNRM, a Convolutional Kernel-based Neural Ranking Model that models n-gram soft matches for ad-hoc search. Instead of exact matching query and document n-grams,\\textttConv-KNRM uses Convolutional Neural Networks to represent n-grams of various lengths and soft matches them in a unified embedding space. The n-gram soft matches are then utilized by the kernel pooling and learning-to-rank layers to generate the final ranking score.\\textttConv-KNRM can be learned end-to-end and fully optimized from user feedback. The learned model\u00bb s generalizability is investigated by testing how well it performs in a related domain with small amounts of training data. Experiments on English search logs, Chinese search logs, and TREC Web track tasks demonstrated consistent advantages of\\textttConv-KNRM over prior neural IR methods and feature-based methods.", "num_citations": "224\n", "authors": ["1187"]}
{"title": "Representation Learning of Knowledge Graphs with Hierarchical Types\n", "abstract": " Representation learning of knowledge graphs aims to encode both entities and relations into a continuous low-dimensional vector space. Most existing methods only concentrate on learning representations with structured information located in triples, regardless of the rich information located in hierarchical types of entities, which could be collected in most knowledge graphs. In this paper, we propose a novel method named Type-embodied Knowledge Representation Learning (TKRL) to take advantages of hierarchical entity types. We suggest that entities should have multiple representations in different types. More specifically, we consider hierarchical types as projection matrices for entities, with two type encoders designed to model hierarchical structures. Meanwhile, type information is also utilized as relation-specific type constraints. We evaluate our models on two tasks including knowledge graph completion and triple classification, and further explore the performances on long-tail dataset. Experimental results show that our models significantly outperform all baselines on both tasks, especially with long-tail distribution. It indicates that our models are capable of capturing hierarchical type information which is significant when constructing representations of knowledge graphs. The source code of this paper can be obtained from https://github. com/thunlp/TKRL.", "num_citations": "213\n", "authors": ["1187"]}
{"title": "PLDA+: Parallel latent dirichlet allocation with data placement and pipeline processing\n", "abstract": " Previous methods of distributed Gibbs sampling for LDA run into either memory or communication bottlenecks. To improve scalability, we propose four strategies: data placement, pipeline processing, word bundling, and priority-based scheduling. Experiments show that our strategies significantly reduce the unparallelizable communication bottleneck and achieve good load balancing, and hence improve scalability of LDA.", "num_citations": "210\n", "authors": ["1187"]}
{"title": "Understanding the Behaviors of BERT in Ranking\n", "abstract": " This paper studies the performances and behaviors of BERT in ranking tasks. We explore several different ways to leverage the pre-trained BERT and fine-tune it on two ranking tasks: MS MARCO passage reranking and TREC Web Track ad hoc document ranking. Experimental results on MS MARCO demonstrate the strong effectiveness of BERT in question-answering focused passage ranking tasks, as well as the fact that BERT is a strong interaction-based seq2seq matching model. Experimental results on TREC show the gaps between the BERT pre-trained on surrounding contexts and the needs of ad hoc document ranking. Analyses illustrate how BERT allocates its attentions between query-document tokens in its Transformer layers, how it prefers semantic matches between paraphrase tokens, and how that differs with the soft match patterns learned by a click-trained neural ranker.", "num_citations": "122\n", "authors": ["1187"]}
{"title": "KEPLER: A Unified Model for Knowledge Embedding and Pre-trained Language Representation\n", "abstract": " Pre-trained language representation models (PLMs) cannot well capture factual knowledge from text. In contrast, knowledge embedding (KE) methods can effectively represent the relational facts in knowledge graphs (KGs) with informative entity embeddings, but conventional KE models cannot take full advantage of the abundant textual information. In this paper, we propose a unified model for Knowledge Embedding and Pre-trained LanguagERepresentation (KEPLER), which can not only better integrate factual knowledge into PLMs but also produce effective text-enhanced KE with the strong PLMs. In KEPLER, we encode textual entity descriptions with a PLM as their embeddings, and then jointly optimize the KE and language modeling objectives. Experimental results show that KEPLER achieves state-of-the-art performances on various NLP tasks, and also works remarkably well as an inductive KE model\u00a0\u2026", "num_citations": "103\n", "authors": ["1187"]}
{"title": "Multi-Channel Graph Neural Network for Entity Alignment\n", "abstract": " Entity alignment typically suffers from the issues of structural heterogeneity and limited seed alignments. In this paper, we propose a novel Multi-channel Graph Neural Network model (MuGNN) to learn alignment-oriented knowledge graph (KG) embeddings by robustly encoding two KGs via multiple channels. Each channel encodes KGs via different relation weighting schemes with respect to self-attention towards KG completion and cross-KG attention for pruning exclusive entities respectively, which are further combined via pooling techniques. Moreover, we also infer and transfer rule knowledge for completing two KGs consistently. MuGNN is expected to reconcile the structural differences of two KGs, and thus make better use of seed alignments. Extensive experiments on five publicly available datasets demonstrate our superior performance (5% Hits@1 up on average).", "num_citations": "96\n", "authors": ["1187"]}
{"title": "Neural Collective Entity Linking\n", "abstract": " Entity Linking aims to link entity mentions in texts to knowledge bases, and neural models have achieved recent success in this task. However, most existing methods rely on local contexts to resolve entities independently, which may usually fail due to the data sparsity of local information. To address this issue, we propose a novel neural model for collective entity linking, named as NCEL. NCEL applies Graph Convolutional Network to integrate both local contextual features and global coherence information for entity linking. To improve the computation efficiency, we approximately perform graph convolution on a subgraph of adjacent entity mentions instead of those in the entire text. We further introduce an attention scheme to improve the robustness of NCEL to data noise and train the model on Wikipedia hyperlinks to avoid overfitting and domain bias. In experiments, we evaluate NCEL on five publicly available datasets to verify the linking performance as well as generalization ability. We also conduct an extensive analysis of time complexity, the impact of key modules, and qualitative results, which demonstrate the effectiveness and efficiency of our proposed method.", "num_citations": "73\n", "authors": ["1187"]}
{"title": "TransNet: Translation-Based Network Representation Learning for Social Relation Extraction\n", "abstract": " Conventional network representation learning (NRL) models learn low-dimensional vertex representations by simply regarding each edge as a binary or continuous value. However, there exists rich semantic information on edges and the interactions between vertices usually preserve distinct meanings, which are largely neglected by most existing NRL models. In this work, we present a novel Translation-based NRL model, TransNet, by regarding the interactions between vertices as a translation operation. Moreover, we formalize the task of Social Relation Extraction (SRE) to evaluate the capability of NRL methods on modeling the relations between vertices. Experimental results on SRE demonstrate that TransNet significantly outperforms other baseline methods by 10% to 20% on hits@ 1. The source code and datasets can be obtained from https://github. com/thunlp/TransNet.", "num_citations": "72\n", "authors": ["1187"]}
{"title": "Distant supervision for relation extraction with matrix completion\n", "abstract": " The essence of distantly supervised relation extraction is that it is an incomplete multi-label classification problem with sparse and noisy features. To tackle the sparsity and noise challenges, we propose solving the classification problem using matrix completion on factorized matrix of minimized rank. We formulate relation classification as completing the unknown labels of testing items (entity pairs) in a sparse matrix that concatenates training and testing textual features with training labels. Our algorithmic framework is based on the assumption that the rank of item-byfeature and item-by-label joint matrix is low. We apply two optimization models to recover the underlying low-rank matrix leveraging the sparsity of feature-label matrix. The matrix completion problem is then solved by the fixed point continuation (FPC) algorithm, which can find the global optimum. Experiments on two widely used datasets with different dimensions of textual features demonstrate that our low-rank matrix completion approach significantly outperforms the baseline and the state-of-the-art methods.", "num_citations": "69\n", "authors": ["1187"]}
{"title": "Relation structure-aware heterogeneous information network embedding\n", "abstract": " Heterogeneous information network (HIN) embedding aims to embed multiple types of nodes into a low-dimensional space. Although most existing HIN embedding methods consider heterogeneous relations in HINs, they usually employ one single model for all relations without distinction, which inevitably restricts the capability of network embedding. In this paper, we take the structural characteristics of heterogeneous relations into consideration and propose a novel Relation structure-aware Heterogeneous Information Network Embedding model (RHINE). By exploring the real-world networks with thorough mathematical analysis, we present two structure-related measures which can consistently distinguish heterogeneous relations into two categories: Affiliation Relations (ARs) and Interaction Relations (IRs). To respect the distinctive characteristics of relations, in our RHINE, we propose different models specifically tailored to handle ARs and IRs, which can better capture the structures and semantics of the networks. At last, we combine and optimize these models in a unified and elegant manner. Extensive experiments on three real-world datasets demonstrate that our model significantly outperforms the state-of-the-art methods in various tasks, including node clustering, link prediction, and node classification.", "num_citations": "49\n", "authors": ["1187"]}
{"title": "Differentiating Concepts and Instances for Knowledge Graph Embedding\n", "abstract": " Concepts, which represent a group of different instances sharing common properties, are essential information in knowledge representation. Most conventional knowledge embedding methods encode both entities (concepts and instances) and relations as vectors in a low dimensional semantic space equally, ignoring the difference between concepts and instances. In this paper, we propose a novel knowledge graph embedding model named TransC by differentiating concepts and instances. Specifically, TransC encodes each concept in knowledge graph as a sphere and each instance as a vector in the same semantic space. We use the relative positions to model the relations between concepts and instances (i.e., instanceOf), and the relations between concepts and sub-concepts (i.e., subClassOf). We evaluate our model on both link prediction and triple classification tasks on the dataset based on YAGO. Experimental results show that TransC outperforms state-of-the-art methods, and captures the semantic transitivity for instanceOf and subClassOf relation. Our codes and datasets can be obtained from https:// github.com/davidlvxin/TransC.", "num_citations": "48\n", "authors": ["1187"]}
{"title": "Chinese Relation Extraction with Multi-Grained Information and External Linguistic Knowledge\n", "abstract": " Chinese relation extraction is conducted using neural networks with either character-based or word-based inputs, and most existing methods typically suffer from segmentation errors and ambiguity of polysemy. To address the issues, we propose a multi-grained lattice framework (MG lattice) for Chinese relation extraction to take advantage of multi-grained language information and external linguistic knowledge. In this framework,(1) we incorporate word-level information into character sequence inputs so that segmentation errors can be avoided.(2) We also model multiple senses of polysemous words with the help of external linguistic knowledge, so as to alleviate polysemy ambiguity. Experiments on three real-world datasets in distinct domains show consistent and significant superiority and robustness of our model, as compared with other baselines. We will release the source code of this paper in the future.", "num_citations": "43\n", "authors": ["1187"]}
{"title": "Grounded conversation generation as guided traverses in commonsense knowledge graphs\n", "abstract": " Human conversations naturally evolve around related concepts and scatter to multi-hop concepts. This paper presents a new conversation generation model, ConceptFlow, which leverages commonsense knowledge graphs to explicitly model conversation flows. By grounding conversations to the concept space, ConceptFlow represents the potential conversation flow as traverses in the concept space along commonsense relations. The traverse is guided by graph attentions in the concept graph, moving towards more meaningful directions in the concept space, in order to generate more semantic and informative responses. Experiments on Reddit conversations demonstrate ConceptFlow's effectiveness over previous knowledge-aware conversation models and GPT-2 based models while using 70% fewer parameters, confirming the advantage of explicit modeling conversation structures. All source codes of this work are available at https://github.com/thunlp/ConceptFlow.", "num_citations": "42\n", "authors": ["1187"]}
{"title": "Low-Resource Name Tagging Learned with Weakly Labeled Data\n", "abstract": " Name tagging in low-resource languages or domains suffers from inadequate training data. Existing work heavily relies on additional information, while leaving those noisy annotations unexplored that extensively exist on the web. In this paper, we propose a novel neural model for name tagging solely based on weakly labeled (WL) data, so that it can be applied in any low-resource settings. To take the best advantage of all WL sentences, we split them into high-quality and noisy portions for two modules, respectively: (1) a classification module focusing on the large portion of noisy data can efficiently and robustly pretrain the tag classifier by capturing textual context semantics; and (2) a costly sequence labeling module focusing on high-quality data utilizes Partial-CRFs with non-entity sampling to achieve global optimum. Two modules are combined via shared parameters. Extensive experiments involving five low-resource languages and fine-grained food domain demonstrate our superior performance (6% and 7.8% F1 gains on average) as well as efficiency.", "num_citations": "41\n", "authors": ["1187"]}
{"title": "Does William Shakespeare really write Hamlet? knowledge representation learning with confidence\n", "abstract": " Knowledge graphs (KGs), which could provide essential relational information between entities, have been widely utilized in various knowledge-driven applications. Since the overall human knowledge is innumerable that still grows explosively and changes frequently, knowledge construction and update inevitably involve automatic mechanisms with less human supervision, which usually bring in plenty of noises and conflicts to KGs. However, most conventional knowledge representation learning methods assume that all triple facts in existing KGs share the same significance without any noises. To address this problem, we propose a novel confidence-aware knowledge representation learning framework (CKRL), which detects possible noises in KGs while learning knowledge representations with confidence simultaneously. Specifically, we introduce the triple confidence to conventional translation-based methods for knowledge representation learning. To make triple confidence more flexible and universal, we only utilize the internal structural information in KGs, and propose three kinds of triple confidences considering both local and global structural information. In experiments, We evaluate our models on knowledge graph noise detection, knowledge graph completion and triple classification. Experimental results demonstrate that our confidence-aware models achieve significant and consistent improvements on all tasks, which confirms the capability of CKRL modeling confidence with structural information in both KG noise detection and knowledge representation learning.", "num_citations": "40\n", "authors": ["1187"]}
{"title": "NumNet: Machine Reading Comprehension with Numerical Reasoning\n", "abstract": " Numerical reasoning, such as addition, subtraction, sorting and counting is a critical skill in human's reading comprehension, which has not been well considered in existing machine reading comprehension (MRC) systems. To address this issue, we propose a numerical MRC model named as NumNet, which utilizes a numerically-aware graph neural network to consider the comparing information and performs numerical reasoning over numbers in the question and passage. Our system achieves an EM-score of 64.56% on the DROP dataset, outperforming all existing machine reading comprehension models by considering the numerical relations among numbers.", "num_citations": "38\n", "authors": ["1187"]}
{"title": "Lexical Sememe Prediction via Word Embeddings and Matrix Factorization\n", "abstract": " Sememes are defined as the minimum semantic units of human languages. People have manually annotated lexical sememes for words and form linguistic knowledge bases. However, manual construction is time-consuming and labor-intensive, with significant annotation inconsistency and noise. In this paper, we for the first time explore to automatically predict lexical sememes based on semantic meanings of words encoded by word embeddings. Moreover, we apply matrix factorization to learn semantic relations between sememes and words. In experiments, we take a real-world sememe knowledge base HowNet for training and evaluation, and the results reveal the effectiveness of our method for lexical sememe prediction. Our method will be of great use for annotation verification of existing noisy sememe knowledge bases and annotation suggestion of new words and phrases.", "num_citations": "35\n", "authors": ["1187"]}
{"title": "Comprehend deepwalk as matrix factorization\n", "abstract": " Word2vec, as an efficient tool for learning vector representation of words has shown its effectiveness in many natural language processing tasks. Mikolov et al. issued Skip-Gram and Negative Sampling model for developing this toolbox. Perozzi et al. introduced the Skip-Gram model into the study of social network for the first time, and designed an algorithm named DeepWalk for learning node embedding on a graph. We prove that the DeepWalk algorithm is actually factoring a matrix M where each entry M_{ij} is logarithm of the average probability that node i randomly walks to node j in fix steps.", "num_citations": "34\n", "authors": ["1187"]}
{"title": "Few-Shot Generative Conversational Query Rewriting\n", "abstract": " Conversational query rewriting aims to reformulate a concise conversational query to a fully specified, context-independent query that can be effectively handled by existing information retrieval systems. This paper presents a few-shot generative approach to conversational query rewriting. We develop two methods, based on rules and self-supervised learning, to generate weak supervision data using large amounts of ad hoc search sessions, and to fine-tune GPT-2 to rewrite conversational queries. On the TREC Conversational Assistance Track, our weakly supervised GPT-2 rewriter improves the state-of-the-art ranking accuracy by 12%, only using very limited amounts of manual query rewrites. In the zero-shot learning setting, the rewriter still gives a comparable result to previous state-of-the-art systems. Our analyses reveal that GPT-2 effectively picks up the task syntax and learns to capture context dependencies\u00a0\u2026", "num_citations": "33\n", "authors": ["1187"]}
{"title": "Full-Scale Information Diffusion Prediction With Reinforced Recurrent Networks\n", "abstract": " Information diffusion prediction is an important task which studies how information items spread among users. With the success of deep learning techniques, recurrent neural networks (RNNs) have shown their powerful capability in modeling information diffusion as sequential data. However, previous works focused on either microscopic diffusion prediction which aims at guessing the next influenced user or macroscopic diffusion prediction which estimates the total numbers of influenced users during the diffusion process. To the best of our knowledge, no previous works have suggested a unified model for both microscopic and macroscopic scales. In this paper, we propose a novel multi-scale diffusion prediction model based on reinforcement learning (RL). RL incorporates the macroscopic diffusion size information into the RNN-based microscopic diffusion model by addressing the non-differentiable problem. We also employ an effective structural context extraction strategy to utilize the underlying social graph information. Experimental results show that our proposed model outperforms state-of-the-art baseline models on both microscopic and macroscopic diffusion predictions on three real-world datasets.", "num_citations": "32\n", "authors": ["1187"]}
{"title": "Adapting Meta Knowledge Graph Information for Multi-Hop Reasoning over Few-Shot Relations\n", "abstract": " Multi-hop knowledge graph (KG) reasoning is an effective and explainable method for predicting the target entity via reasoning paths in query answering (QA) task. Most previous methods assume that every relation in KGs has enough training triples, regardless of those few-shot relations which cannot provide sufficient triples for training robust reasoning models. In fact, the performance of existing multi-hop reasoning methods drops significantly on few-shot relations. In this paper, we propose a meta-based multi-hop reasoning method (Meta-KGR), which adopts meta-learning to learn effective meta parameters from high-frequency relations that could quickly adapt to few-shot relations. We evaluate Meta-KGR on two public datasets sampled from Freebase and NELL, and the experimental results show that Meta-KGR outperforms the current state-of-the-art methods in few-shot scenarios. Our code and datasets can be obtained from https://github.com/ THU-KEG/MetaKGR.", "num_citations": "32\n", "authors": ["1187"]}
{"title": "Linking GloVe with word2vec\n", "abstract": " The Global Vectors for word representation (GloVe), introduced by Jeffrey Pennington et al. is reported to be an efficient and effective method for learning vector representations of words. State-of-the-art performance is also provided by skip-gram with negative-sampling (SGNS) implemented in the word2vec tool. In this note, we explain the similarities between the training objectives of the two models, and show that the objective of SGNS is similar to the objective of a specialized form of GloVe, though their cost functions are defined differently.", "num_citations": "32\n", "authors": ["1187"]}
{"title": "Introduction to Graph Neural Networks\n", "abstract": " Graphs are useful data structures in complex real-life applications such as modeling physical systems, learning molecular fingerprints, controlling traffic networks, and recommending friends in social networks. However, these tasks require dealing with non-Euclidean graph data that contains rich relational information between elements and cannot be well handled by traditional deep learning models (e.g., convolutional neural networks (CNNs) or recurrent neural networks (RNNs)). Nodes in graphs usually contain useful feature information that cannot be well addressed in most unsupervised representation learning methods (e.g., network embedding methods). Graph neural networks (GNNs) are proposed to combine the feature information and the graph structure to learn better representations on graphs via feature propagation and aggregation. Due to its convincing performance and high interpretability, GNN\u00a0\u2026", "num_citations": "26\n", "authors": ["1187"]}
{"title": "Adaptive Graph Encoder for Attributed Graph Embedding\n", "abstract": " Attributed graph embedding, which learns vector representations from graph topology and node features, is a challenging task for graph analysis. Recently, methods based on graph convolutional networks (GCNs) have made great progress on this task. However, existing GCN-based methods have three major drawbacks. Firstly, our experiments indicate that the entanglement of graph convolutional filters and weight matrices will harm both the performance and robustness. Secondly, we show that graph convolutional filters in these methods reveal to be special cases of generalized Laplacian smoothing filters, but they do not preserve optimal low-pass characteristics. Finally, the training objectives of existing algorithms are usually recovering the adjacency matrix or feature matrix, which are not always consistent with real-world applications. To address these issues, we propose Adaptive Graph Encoder (AGE), a\u00a0\u2026", "num_citations": "25\n", "authors": ["1187"]}
{"title": "Event Detection with Trigger-Aware Lattice Neural Network\n", "abstract": " Event detection (ED) aims to locate trigger words in raw text and then classify them into correct event types. In this task, neural net-work based models became mainstream in re-cent years. However, two problems arise when it comes to languages without natural delim-iters, such as Chinese. First, word-based mod-els severely suffer from the problem of word-trigger mismatch, limiting the performance of the methods. In addition, even if trigger words could be accurately located, the ambi-guity of polysemy of triggers could still af-fect the trigger classification stage. To ad-dress the two issues simultaneously, we pro-pose the Trigger-aware Lattice Neural Net-work (TLNN).(1) The framework dynami-cally incorporates word and character informa-tion so that the trigger-word mismatch issue can be avoided.(2) Moreover, for polysemous characters and words, we model all senses of them with the help of an external linguistic knowledge base, so as to alleviate the prob-lem of ambiguous triggers. Experiments on two benchmark datasets show that our model could effectively tackle the two issues and outperforms previous state-of-the-art methods significantly, giving the best results. The source code of this paper can be obtained from https://github. com/thunlp/TLNN.", "num_citations": "24\n", "authors": ["1187"]}
{"title": "Joint Representation Learning of Cross-lingual Words and Entities via Attentive Distant Supervision\n", "abstract": " Joint representation learning of words and entities benefits many NLP tasks, but has not been well explored in cross-lingual settings. In this paper, we propose a novel method for joint representation learning of cross-lingual words and entities. It captures mutually complementary knowledge, and enables cross-lingual inferences among knowledge bases and texts. Our method does not require parallel corpora, and automatically generates comparable data via distant supervision using multi-lingual knowledge bases. We utilize two types of regularizers to align cross-lingual words and entities, and design knowledge attention and cross-lingual attention to further reduce noises. We conducted a series of experiments on three tasks: word translation, entity relatedness, and cross-lingual entity linking. The results, both qualitatively and quantitatively, demonstrate the significance of our method.", "num_citations": "24\n", "authors": ["1187"]}
{"title": "CJRC: A Reliable Human-Annotated Benchmark DataSet for Chinese Judicial Reading Comprehension\n", "abstract": " We present a Chinese judicial reading comprehension (CJRC) dataset which contains approximately 10K documents and almost 50K questions with answers. The documents come from judgment documents and the questions are annotated by law experts. The CJRC dataset can help researchers extract elements by reading comprehension technology. Element extraction is an important task in the legal field. However, it is difficult to predefine the element types completely due to the diversity of document types and causes of action. By contrast, machine reading comprehension technology can quickly extract elements by answering various questions from the long document. We build two strong baseline models based on BERT and BiDAF. The experimental results show that there is enough space for improvement compared to human annotators.", "num_citations": "23\n", "authors": ["1187"]}
{"title": "MOOCCube: a large-scale data repository for NLP applications in MOOCs\n", "abstract": " The prosperity of Massive Open Online Courses (MOOCs) provides fodder for many NLP and AI research for education applications, eg, course concept extraction, prerequisite relation discovery, etc. However, the publicly available datasets of MOOC are limited in size with few types of data, which hinders advanced models and novel attempts in related topics. Therefore, we present MOOCCube, a large-scale data repository of over 700 MOOC courses, 100k concepts, 8 million student behaviors with an external resource. Moreover, we conduct a prerequisite discovery task as an example application to show the potential of MOOCCube in facilitating relevant research. The data repository is now available at http://moocdata. cn/data/MOOCCube.", "num_citations": "21\n", "authors": ["1187"]}
{"title": "Selective Weak Supervision for Neural Information Retrieval\n", "abstract": " This paper democratizes neural information retrieval to scenarios where large scale relevance training signals are not available. We revisit the classic IR intuition that anchor-document relations approximate query-document relevance and propose a reinforcement weak supervision selection method, ReInfoSelect, which learns to select anchor-document pairs that best weakly supervise the neural ranker (action), using the ranking performance on a handful of relevance labels as the reward. Iteratively, for a batch of anchor-document pairs, ReInfoSelect back propagates the gradients through the neural ranker, gathers its NDCG reward, and optimizes the data selection network using policy gradients, until the neural ranker\u2019s performance peaks on target relevance metrics (convergence). In our experiments on three TREC benchmarks, neural rankers trained by ReInfoSelect, with only publicly available anchor data\u00a0\u2026", "num_citations": "21\n", "authors": ["1187"]}
{"title": "Pre-Trained Models: Past, Present and Future\n", "abstract": " Large-scale pre-trained models (PTMs) such as BERT and GPT have recently achieved great success and become a milestone in the field of artificial intelligence (AI). Owing to sophisticated pre-training objectives and huge model parameters, large-scale PTMs can effectively capture knowledge from massive labeled and unlabeled data. By storing knowledge into huge parameters and fine-tuning on specific tasks, the rich knowledge implicitly encoded in huge parameters can benefit a variety of downstream tasks, which has been extensively demonstrated via experimental verification and empirical analysis. It is now the consensus of the AI community to adopt PTMs as backbone for downstream tasks rather than learning models from scratch. In this paper, we take a deep look into the history of pre-training, especially its special relation with transfer learning and self-supervised learning, to reveal the crucial position\u00a0\u2026", "num_citations": "19\n", "authors": ["1187"]}
{"title": "Learning to appreciate the aesthetic effects of clothing\n", "abstract": " How do people describe clothing? The words like \u201cformal\u201d or\" casual\" are usually used. However, recent works often focus on recognizing or extracting visual features (eg, sleeve length, color distribution and clothing pattern) from clothing images accurately. How can we bridge the gap between the visual features and the aesthetic words? In this paper, we formulate this task to a novel three-level framework: visual features (VF)-image-scale space (ISS)-aesthetic words space (AWS). Leveraging the art-field image-scale space served as an intermediate layer, we first propose a Stacked Denoising Autoencoder Guided by CorrelativeLabels (SDAE-GCL) to map the visual features to the image-scale space; and then according to the semantic distances computed byWordNet:: Similarity, we map the most often used aesthetic words in online clothing shops to the image-scale space too. Employing upper body menswear images downloaded from several global online clothing shops as experimental data, the results indicate that the proposed three-level framework can help to capture the subtle relationship between visual features and aesthetic words better compared to several baselines. To demonstrate that our three-level framework and its implementation methods are universally applicable, we finally present some interesting analyses on the fashion trend of menswear in the last 10 years.", "num_citations": "19\n", "authors": ["1187"]}
{"title": "Exploring and Evaluating Attributes, Values, and Structures for Entity Alignment\n", "abstract": " Entity alignment (EA) aims at building a unified Knowledge Graph (KG) of rich content by linking the equivalent entities from various KGs. GNN-based EA methods present promising performances by modeling the KG structure defined by relation triples. However, attribute triples can also provide crucial alignment signal but have not been well explored yet. In this paper, we propose to utilize an attributed value encoder and partition the KG into subgraphs to model the various types of attribute triples efficiently. Besides, the performances of current EA methods are overestimated because of the name-bias of existing EA datasets. To make an objective evaluation, we propose a hard experimental setting where we select equivalent entity pairs with very different names as the test set. Under both the regular and hard settings, our method achieves significant improvements ( on average Hits@ in DBPk) over  baselines in cross-lingual and monolingual datasets. Ablation studies on different subgraphs and a case study about attribute types further demonstrate the effectiveness of our method. Source code and data can be found at https://github.com/thunlp/explore-and-evaluate.", "num_citations": "18\n", "authors": ["1187"]}
{"title": "Learning from explanations with neural execution tree\n", "abstract": " While deep neural networks have achieved impressive performance on a range of NLP tasks, these data-hungry models heavily rely on labeled data, which restricts their applications in scenarios where data annotation is expensive. Natural language (NL) explanations have been demonstrated very useful additional supervision, which can provide sufficient domain knowledge for generating more labeled data over new instances, while the annotation time only doubles. However, directly applying them for augmenting model learning encounters two challenges: (1) NL explanations are unstructured and inherently compositional, which asks for a modularized model to represent their semantics, (2) NL explanations often have large numbers of linguistic variants, resulting in low recall and limited generalization ability. In this paper, we propose a novel Neural Execution Tree (NExT) framework to augment training data for text classification using NL explanations. After transforming NL explanations into executable logical forms by semantic parsing, NExT generalizes different types of actions specified by the logical forms for labeling data instances, which substantially increases the coverage of each NL explanation. Experiments on two NLP tasks (relation extraction and sentiment analysis) demonstrate its superiority over baseline methods. Its extension to multi-hop question answering achieves performance gain with light annotation effort.", "num_citations": "18\n", "authors": ["1187"]}
{"title": "Expertise Style Transfer: A New Task Towards Better Communication between Experts and Laymen\n", "abstract": " The curse of knowledge can impede communication between experts and laymen. We propose a new task of expertise style transfer and contribute a manually annotated dataset with the goal of alleviating such cognitive biases. Solving this task not only simplifies the professional language, but also improves the accuracy and expertise level of laymen descriptions using simple words. This is a challenging task, unaddressed in previous work, as it requires the models to have expert intelligence in order to modify text with a deep understanding of domain knowledge and structures. We establish the benchmark performance of five state-of-the-art models for style transfer and text simplification. The results demonstrate a significant gap between machine and human performance. We also discuss the challenges of automatic evaluation, to provide insights into future research directions. The dataset is publicly available at https://srhthu.github.io/expertise-style-transfer.", "num_citations": "17\n", "authors": ["1187"]}
{"title": "DIAG-NRE: A Neural Pattern Diagnosis Framework for Distantly Supervised Neural Relation Extraction\n", "abstract": " Pattern-based labeling methods have achieved promising results in alleviating the inevitable labeling noises of distantly supervised neural relation extraction. However, these methods require significant expert labor to write relation-specific patterns, which makes them too sophisticated to generalize quickly.To ease the labor-intensive workload of pattern writing and enable the quick generalization to new relation types, we propose a neural pattern diagnosis framework, DIAG-NRE, that can automatically summarize and refine high-quality relational patterns from noise data with human experts in the loop. To demonstrate the effectiveness of DIAG-NRE, we apply it to two real-world datasets and present both significant and interpretable improvements over state-of-the-art methods.", "num_citations": "16\n", "authors": ["1187"]}
{"title": "Capturing Global Informativeness in Open Domain Keyphrase Extraction\n", "abstract": " An effective keyphrase extraction system requires to produce self-contained high quality phrases that are also key to the document topic. This paper presents BERT-JointKPE, a multitask BERT-based model for keyphrase extraction. JointKPE employs a chunking network to identify high-quality phrases and a ranking network to learn their salience in the document. The model is trained jointly on the chunking task and the ranking task, balancing the estimation of keyphrase quality and salience. Experiments on two benchmarks demonstrate JointKPE\u2019s robust effectiveness with different BERT variants. Our analyses show that JointKPE has advantages in predicting long keyphrases and extracting phrases that are not entities but also meaningful. The source code of this paper can be obtained from https://github. com/thunlp/BERT-KPE.", "num_citations": "14\n", "authors": ["1187"]}
{"title": "MAVEN: A Massive General Domain Event Detection Dataset\n", "abstract": " Event detection (ED), which means identifying event trigger words and classifying event types, is the first and most fundamental step for extracting event knowledge from plain text. Most existing datasets exhibit the following issues that limit further development of ED: (1) Data scarcity. Existing small-scale datasets are not sufficient for training and stably benchmarking increasingly sophisticated modern neural methods. (2) Low coverage. Limited event types of existing datasets cannot well cover general-domain events, which restricts the applications of ED models. To alleviate these problems, we present a MAssive eVENt detection dataset (MAVEN), which contains 4,480 Wikipedia documents, 118,732 event mention instances, and 168 event types. MAVEN alleviates the data scarcity problem and covers much more general event types. We reproduce the recent state-of-the-art ED models and conduct a thorough evaluation on MAVEN. The experimental results show that existing ED methods cannot achieve promising results on MAVEN as on the small datasets, which suggests that ED in the real world remains a challenging task and requires further research efforts. We also discuss further directions for general domain ED with empirical analyses. The source code and dataset can be obtained from https://github.com/THU-KEG/MAVEN-dataset.", "num_citations": "13\n", "authors": ["1187"]}
{"title": "DeepChannel: Salience Estimation by Contrastive Learning for Extractive Document Summarization\n", "abstract": " We propose DeepChannel, a robust, data-efficient, and interpretable neural model for extractive document summarization. Given any document-summary pair, we estimate a salience score, which is modeled using an attention-based deep neural network, to represent the salience degree of the summary for yielding the document. We devise a contrastive training strategy to learn the salience estimation network, and then use the learned salience score as a guide and iteratively extract the most salient sentences from the document as our generated summary. In experiments, our model not only achieves state-of-the-art ROUGE scores on CNN/Daily Mail dataset, but also shows strong robustness in the out-of-domain test on DUC2007 test set. Moreover, our model reaches a ROUGE-1 F-1 score of 39.41 on CNN/Daily Mail test set with merely 1/100 training set, demonstrating a tremendous data efficiency.", "num_citations": "13\n", "authors": ["1187"]}
{"title": "Course Concept Expansion in MOOCs with External Knowledge and Interactive Game\n", "abstract": " As Massive Open Online Courses (MOOCs) become increasingly popular, it is promising to automatically provide extracurricular knowledge for MOOC users. Suffering from semantic drifts and lack of knowledge guidance, existing methods can not effectively expand course concepts in complex MOOC environments. In this paper, we first build a novel boundary during searching for new concepts via external knowledge base and then utilize heterogeneous features to verify the high-quality results. In addition, to involve human efforts in our model, we design an interactive optimization mechanism based on a game. Our experiments on the four datasets from Coursera and XuetangX show that the proposed method achieves significant improvements(+0.19 by MAP) over existing methods. The source code and datasets have been published.", "num_citations": "11\n", "authors": ["1187"]}
{"title": "CMT in TREC-COVID Round 2: Mitigating the Generalization Gaps from Web to Special Domain Search\n", "abstract": " Neural rankers based on deep pretrained language models (LMs) have been shown to improve many information retrieval benchmarks. However, these methods are affected by their the correlation between pretraining domain and target domain and rely on massive fine-tuning relevance labels. Directly applying pretraining methods to specific domains may result in suboptimal search quality because specific domains may have domain adaption problems, such as the COVID domain. This paper presents a search system to alleviate the special domain adaption problem. The system utilizes the domain-adaptive pretraining and few-shot learning technologies to help neural rankers mitigate the domain discrepancy and label scarcity problems. Besides, we also integrate dense retrieval to alleviate traditional sparse retrieval's vocabulary mismatch obstacle. Our system performs the best among the non-manual runs in Round 2 of the TREC-COVID task, which aims to retrieve useful information from scientific literature related to COVID-19. Our code is publicly available at https://github.com/thunlp/OpenMatch.", "num_citations": "8\n", "authors": ["1187"]}
{"title": "Crossmodal language grounding in an embodied neurocognitive model\n", "abstract": " Human infants are able to acquire natural language seemingly easily at an early age. Their language learning seems to occur simultaneously with learning other cognitive functions as well as with playful interactions with the environment and caregivers. From a neuroscientific perspective, natural language is embodied, grounded in most, if not all, sensory and sensorimotor modalities, and acquired by means of crossmodal integration. However, characterizing the underlying mechanisms in the brain is difficult and explaining the grounding of language in crossmodal perception and action remains challenging. In this paper, we present a neurocognitive model for language grounding which reflects bio-inspired mechanisms such as an implicit adaptation of timescales as well as end-to-end multimodal abstraction. It addresses developmental robotic interaction and extends its learning capabilities using larger-scale\u00a0\u2026", "num_citations": "8\n", "authors": ["1187"]}
{"title": "Graph Policy Network for Transferable Active Learning on Graphs\n", "abstract": " Graph neural networks (GNNs) have been attracting increasing popularity due to their simplicity and effectiveness in a variety of fields. However, a large number of labeled data is generally required to train these networks, which could be very expensive to obtain in some domains. In this paper, we study active learning for GNNs, i.e., how to efficiently label the nodes on a graph to reduce the annotation cost of training GNNs. We formulate the problem as a sequential decision process on graphs and train a GNN-based policy network with reinforcement learning to learn the optimal query strategy. By jointly training on several source graphs with full labels, we learn a transferable active learning policy which can directly generalize to unlabeled target graphs. Experimental results on multiple datasets from different domains prove the effectiveness of the learned policy in promoting active learning performance in both settings of transferring between graphs in the same domain and across different domains.", "num_citations": "8\n", "authors": ["1187"]}
{"title": "Category Enhanced Word Embedding\n", "abstract": " Distributed word representations have been demonstrated to be effective in capturing semantic and syntactic regularities. Unsupervised representation learning from large unlabeled corpora can learn similar representations for those words that present similar co-occurrence statistics. Besides local occurrence statistics, global topical information is also important knowledge that may help discriminate a word from another. In this paper, we incorporate category information of documents in the learning of word representations and to learn the proposed models in a document-wise manner. Our models outperform several state-of-the-art models in word analogy and word similarity tasks. Moreover, we evaluate the learned word vectors on sentiment analysis and text classification tasks, which shows the superiority of our learned word vectors. We also learn high-quality category embeddings that reflect topical meanings.", "num_citations": "8\n", "authors": ["1187"]}
{"title": "Few-NERD: A Few-Shot Named Entity Recognition Dataset\n", "abstract": " Recently, considerable literature has grown up around the theme of few-shot named entity recognition (NER), but little published benchmark data specifically focused on the practical and challenging task. Current approaches collect existing supervised NER datasets and re-organize them to the few-shot setting for empirical study. These strategies conventionally aim to recognize coarse-grained entity types with few examples, while in practice, most unseen entity types are fine-grained. In this paper, we present Few-NERD, a large-scale human-annotated few-shot NER dataset with a hierarchy of 8 coarse-grained and 66 fine-grained entity types. Few-NERD consists of 188,238 sentences from Wikipedia, 4,601,160 words are included and each is annotated as context or a part of a two-level entity type. To the best of our knowledge, this is the first few-shot NER dataset and the largest human-crafted NER dataset. We construct benchmark tasks with different emphases to comprehensively assess the generalization capability of models. Extensive empirical results and analysis show that Few-NERD is challenging and the problem requires further research. We make Few-NERD public at https://ningding97.github.io/fewnerd/.", "num_citations": "7\n", "authors": ["1187"]}
{"title": "Deep Learning in Knowledge Graph\n", "abstract": " Knowledge Graph (KG) is a fundamental resource for human-like commonsense reasoning and natural language understanding, which contains rich knowledge about the world\u2019s entities, entities\u2019 attributes, and semantic relations between different entities. Recent years have witnessed the remarkable success of deep learning techniques in KG. In this chapter, we introduce three broad categories of deep learning-based KG techniques: (1) knowledge representation learning techniques which embed entities and relations in a KG into a dense, low-dimensional, and real-valued semantic space; (2) neural relation extraction techniques which extract facts/relations from text, which can then be used to construct/complete KG; (3) deep learning-based entity linking techniques which bridge Knowledge Graph with textual data, which can facilitate many different tasks.", "num_citations": "7\n", "authors": ["1187"]}
{"title": "The Semantic Network Model of Creativity: Analysis of Online Social Media Data\n", "abstract": " The central hypothesis of Semantic Network Model of Creativity is that creative people, who are exposed to more information that are both novel and useful, will have more interconnections between event schemas in their associations. The networks of event schemas in creative people\u2019s minds were expected to be wider and denser than those in less creative people\u2019s minds. Based on this theory, data from Chinese online social media, also known as \u201cWeibo microblogging,\u201d were analyzed. Each user\u2019s score consisted of the metric of coverage, which represented the spread of the network, as well as the metric of density, which represented the interconnections among nodes in the network. The results showed that occupations had a significant effect on people\u2019s creativity score. Academic scholars and writers in general had higher scores compared to other groups, such as entertainment celebrities and sport stars\u00a0\u2026", "num_citations": "7\n", "authors": ["1187"]}
{"title": "PPT: Pre-trained Prompt Tuning for Few-shot Learning\n", "abstract": " Prompts for pre-trained language models (PLMs) have shown remarkable performance by bridging the gap between pre-training tasks and various downstream tasks. Among these methods, prompt tuning, which freezes PLMs and only tunes soft prompts, provides an efficient and effective solution for adapting large-scale PLMs to downstream tasks. However, prompt tuning is yet to be fully explored. In our pilot experiments, we find that prompt tuning performs comparably with conventional full-model fine-tuning when downstream data are sufficient, whereas it performs much worse under few-shot learning settings, which may hinder the application of prompt tuning in practice. We attribute this low performance to the manner of initializing soft prompts. Therefore, in this work, we propose to pre-train prompts by adding soft prompts into the pre-training stage to obtain a better initialization. We name this Pre-trained Prompt Tuning framework \"PPT\". To ensure the generalization of PPT, we formulate similar classification tasks into a unified task form and pre-train soft prompts for this unified task. Extensive experiments show that tuning pre-trained prompts for downstream tasks can reach or even outperform full-model fine-tuning under both full-data and few-shot settings. Our approach is effective and efficient for using large-scale PLMs in practice.", "num_citations": "6\n", "authors": ["1187"]}
{"title": "Partially-Aligned Data-to-Text Generation with Distant Supervision\n", "abstract": " The Data-to-Text task aims to generate human-readable text for describing some given structured data enabling more interpretability. However, the typical generation task is confined to a few particular domains since it requires well-aligned data which is difficult and expensive to obtain. Using partially-aligned data is an alternative way of solving the dataset scarcity problem. This kind of data is much easier to obtain since it can be produced automatically. However, using this kind of data induces the over-generation problem posing difficulties for existing models, which tends to add unrelated excerpts during the generation procedure. In order to effectively utilize automatically annotated partially-aligned datasets, we extend the traditional generation task to a refined task called Partially-Aligned Data-to-Text Generation (PADTG) which is more practical since it utilizes automatically annotated data for training and thus considerably expands the application domains. To tackle this new task, we propose a novel distant supervision generation framework. It firstly estimates the input data's supportiveness for each target word with an estimator and then applies a supportiveness adaptor and a rebalanced beam search to harness the over-generation problem in the training and generation phases respectively. We also contribute a partially-aligned dataset (The data and source code of this paper can be obtained from https://github.com/fuzihaofzh/distant_supervision_nlg by sampling sentences from Wikipedia and automatically extracting corresponding KB triples for each sentence from Wikidata. The experimental results show that our framework outperforms all\u00a0\u2026", "num_citations": "6\n", "authors": ["1187"]}
{"title": "RHINE: Relation Structure-Aware Heterogeneous Information Network Embedding\n", "abstract": " Heterogeneous information network (HIN) embedding aims to learn the low-dimensional representations of nodes while preserving structures and semantics in HINs. Although most existing methods consider heterogeneous relations and achieve promising performance, they usually employ one single model for all relations without distinction, which inevitably restricts the capability of HIN embedding. In this paper, we argue that heterogeneous relations have different structural characteristics, and propose a novel Relation structure-aware HIN Embedding model, called RHINE. By exploring four real-world networks with thorough analysis, we present two structure-related measures which consistently distinguish heterogeneous relations into two categories: Affiliation Relations (ARs) and Interaction Relations (IRs). To respect the distinctive structural characteristics of relations, in RHINE, we propose different models\u00a0\u2026", "num_citations": "6\n", "authors": ["1187"]}
{"title": "\u5927\u6570\u636e\u667a\u80fd\n", "abstract": " \u6b63\u51fa\u7248\u793e: \u7535\u5b50\u5de5\u4e1a\u51fa\u7248\u793e\u672c\u4e66\u662f\u4e00\u672c\u4ecb\u7ecd\u5927\u6570\u636e\u667a\u80fd\u5206\u6790\u7684\u79d1\u666e\u4e66\u7c4d, \u65e8\u5728\u8ba9\u66f4\u591a\u7684\u4eba\u4e86\u89e3\u548c\u5b66\u4e60\u4e92\u8054\u7f51\u65f6\u4ee3\u7684\u673a\u5668\u5b66\u4e60\u548c\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6280\u672f, \u4ee5\u671f\u8ba9\u5927\u6570\u636e\u6280\u672f\u66f4\u597d\u5730\u4e3a\u6211\u4eec\u7684\u751f\u4ea7\u548c\u751f\u6d3b\u670d\u52a1.", "num_citations": "6\n", "authors": ["1187"]}
{"title": "Prompt-Learning for Fine-Grained Entity Typing\n", "abstract": " As an effective approach to tune pre-trained language models (PLMs) for specific tasks, prompt-learning has recently attracted much attention from researchers. By using \\textit{cloze}-style language prompts to stimulate the versatile knowledge of PLMs, prompt-learning can achieve promising results on a series of NLP tasks, such as natural language inference, sentiment classification, and knowledge probing. In this work, we investigate the application of prompt-learning on fine-grained entity typing in fully supervised, few-shot and zero-shot scenarios. We first develop a simple and effective prompt-learning pipeline by constructing entity-oriented verbalizers and templates and conducting masked language modeling. Further, to tackle the zero-shot regime, we propose a self-supervised strategy that carries out distribution-level optimization in prompt-learning to automatically summarize the information of entity types. Extensive experiments on three fine-grained entity typing benchmarks (with up to 86 classes) under fully supervised, few-shot and zero-shot settings show that prompt-learning methods significantly outperform fine-tuning baselines, especially when the training data is insufficient.", "num_citations": "5\n", "authors": ["1187"]}
{"title": "Crossmodal Language Grounding, Learning, and Teaching.\n", "abstract": " The human brain as one of the most complex dynamic systems enables us to communicate and externalise information by natural language. Despite extensive research, human-like communication with interactive robots is not yet possible, because we have not yet fully understood the mechanistic characteristics of the crossmodal binding between language, actions, and visual sensation that enable humans to acquire and use natural language. In this position paper we present visionand action-embodied language learning research as part of a project investigating multi-modal learning. Our research endeavour includes to develop a) a cortical neural-network model that learns to ground language into crossmodal embodied perception and b) a knowledge-based teaching framework to bootstrap and scale-up the language acquisition to a level of language development in children of age up to two years. We embed this approach of internally grounding embodied experience and externally teaching abstract experience into the developmental robotics paradigm, by means of developing and employing a neurorobot that is capable of multisensory perception and interaction. The proposed research contributes to designing neuroscientific experiments on discovering crossmodal integration particularly in language processing and to constructing future robotic companions capable of natural communication.", "num_citations": "5\n", "authors": ["1187"]}
{"title": "City flow: Prototype exploration for visualizing urban traffic conversations\n", "abstract": " The paper presents City Flow, an urban traffic visualization prototype based on real post streams captured from Sina Weibo, the most popular social networking site in China. With the increasingly pervasive use of online social networks in China, these new channels attract more users than conventional social media. People use them to seek comments and points exchanges about urgent event and popular news. However, the high noise level precludes conscious or objective standpoints. Especially regarding urban traffic, the challenge is to synthesize some general characteristics of cities rather than to follow a particular event. City Flow captures a mass of data from the conversations about traffic issues through Sina Weibo and generates visualizations of general patterns in addition to including detailed insights. This paper describes the interface design, interaction modes, and data capturing methods. It also\u00a0\u2026", "num_citations": "5\n", "authors": ["1187"]}
{"title": "Few-Shot Conversational Dense Retrieval\n", "abstract": " Dense retrieval (DR) has the potential to resolve the query understanding challenge in conversational search by matching in the learned embedding space. However, this adaptation is challenging due to DR models' extra needs for supervision signals and the long-tail nature of conversational search. In this paper, we present a Conversational Dense Retrieval system, ConvDR, that learns contextualized embeddings for multi-turn conversational queries and retrieves documents solely using embedding dot products. In addition, we grant ConvDR few-shot ability using a teacher-student framework, where we employ an ad hoc dense retriever as the teacher, inherit its document encodings, and learn a student query encoder to mimic the teacher embeddings on oracle reformulated queries. Our experiments on TREC CAsT and OR-QuAC demonstrate ConvDR's effectiveness in both few-shot and fully-supervised settings. It outperforms previous systems that operate in the sparse word space, matches the retrieval accuracy of oracle query reformulations, and is also more efficient thanks to its simplicity. Our analyses reveal that the advantages of ConvDR come from its ability to capture informative context while ignoring the unrelated context in previous conversation rounds. This makes ConvDR more effective as conversations evolve while previous systems may get confused by the increased noise from previous turns. Our code is publicly available at https://github.com/thunlp/ConvDR.", "num_citations": "4\n", "authors": ["1187"]}
{"title": "Enhancing Transformer with Sememe Knowledge\n", "abstract": " While large-scale pretraining has achieved great success in many NLP tasks, it has not been fully studied whether external linguistic knowledge can improve data-driven models. In this work, we introduce sememe knowledge into Transformer and propose three sememe-enhanced Transformer models. Sememes, by linguistic definition, are the minimum semantic units of language, which can well represent implicit semantic meanings behind words. Our experiments demonstrate that introducing sememe knowledge into Transformer can consistently improve language modeling and downstream tasks. The adversarial test further demonstrates that sememe knowledge can substantially improve model robustness.", "num_citations": "4\n", "authors": ["1187"]}
{"title": "Parallel generation of topics from documents\n", "abstract": " Methods, systems, and apparatus, including computer programs encoded on a computer storage medium, for enhanced parallel latent Dirichlet allocation (PLDA+). A PLDA+ system is a system of multiple processors that are configured to generate topics from multiple documents. The multiple processors are designated as two types: document processors and matrix processors. The documents are distributed among the document processors. Generated topics are distributed among the matrix processors. Tasks performed on the document processors and matrix processors are segregated into two types of tasks: computation-bound tasks and communication-bound tasks. Computation-bound tasks are CPU intensive tasks; communication-bound tasks are network intensive tasks. Data placement and pipeline strategies are employed such that the computation-bound tasks and the communication-bound tasks are\u00a0\u2026", "num_citations": "4\n", "authors": ["1187"]}
{"title": "Creating reflections in public emotion visualization: Prototype exploration on traffic theme\n", "abstract": " The paper presents a visualization prototype that tackles the issue of public emotion. It emphasizes the exploration process in the aim of creating reflections for the viewer to observe the whole social context as well as individual perspectives. Urban traffic conversation on SNS (Social Networking Site) is our current interest. The prototype we present is made on real post streams captured from Chinese most popular microblog Sina Weibo. After experimenting on two online prototypes we determine a visualizing flow to lead the viewer going through the insights from macro to micro view in three interaction frameworks: City Sentiment, Related Topics, and Post Content. Through showing the visualizing flow with interaction mode, data analysis, and prototype construction, the paper ends with discussing about design considerations in creating reflections on public traffic emotion in visualization prototype.", "num_citations": "4\n", "authors": ["1187"]}
{"title": "Meta Adaptive Neural Ranking with Contrastive Synthetic Supervision\n", "abstract": " Neural Information Retrieval (Neu-IR) models have shown their effectiveness and thrive from end-to-end training with massive high-quality relevance labels. Nevertheless, relevance labels at such quantity are luxury and unavailable in many ranking scenarios, for example, in biomedical search. This paper improves Neu-IR in such few-shot search scenarios by meta-adaptively training neural rankers with synthetic weak supervision. We first leverage contrastive query generation (ContrastQG) to synthesize more informative queries as in-domain weak relevance labels, and then filter them with meta adaptive learning to rank (MetaLTR) to better generalize neural rankers to the target few-shot domain. Experiments on three different search domains: web, news, and biomedical, demonstrate significantly improved few-shot accuracy of neural rankers with our weak supervision framework. The code of this paper will be open-sourced.", "num_citations": "3\n", "authors": ["1187"]}
{"title": "On Modeling Sense Relatedness in Multi-prototype Word Embedding\n", "abstract": " To enhance the expression ability of distributional word representation learning model, many researchers tend to induce word senses through clustering, and learn multiple embedding vectors for each word, namely multi-prototype word embedding model. However, most related work ignores the relatedness among word senses which actually plays an important role. In this paper, we propose a novel approach to capture word sense relatedness in multi-prototype word embedding model. Particularly, we differentiate the original sense and extended senses of a word by introducing their global occurrence information and model their relatedness through the local textual context information. Based on the idea of fuzzy clustering, we introduce a random process to integrate these two types of senses and design two non-parametric methods for word sense induction. To make our model more scalable and efficient, we use an online joint learning framework extended from the Skip-gram model. The experimental results demonstrate that our model outperforms both conventional single-prototype embedding models and other multi-prototype embedding models, and achieves more stable performance when trained on smaller data.", "num_citations": "3\n", "authors": ["1187"]}
{"title": "Category-sensitive ranking for text\n", "abstract": " Provided are methods, systems and apparatus which include computer program products, for generating topic models for text summarization In one aspect, a method includes receiving a first document of text that is associated with one or more category labels and that includes one or more sequences of one or more words, determining a category label that represents a first category associated with the first document, sampling the one or more sequences to determine a topic and a co-occurrence relationship between the topic and the category label, where a topic represents a subdivision within a category, sampling the one or more sequences to determine a co-occurrence relationship between a sequence in the first document and the topic, and generating a category-topic model that represents the co-occurrence relationships.", "num_citations": "3\n", "authors": ["1187"]}
{"title": "Growing related words from seed via user behaviors: a re-ranking based approach\n", "abstract": " Motivated by Google Sets, we study the problem of growing related words from a single seed word by leveraging user behaviors hiding in user records of Chinese input method. Our proposed method is motivated by the observation that the more frequently two words cooccur in user records, the more related they are. First, we utilize user behaviors to generate candidate words. Then, we utilize search engine to enrich candidate words with adequate semantic features. Finally, we reorder candidate words according to their semantic relatedness to the seed word. Experimental results on a Chinese input method dataset show that our method gains better performance.", "num_citations": "3\n", "authors": ["1187"]}
{"title": "CLEVE: Contrastive Pre-training for Event Extraction\n", "abstract": " Event extraction (EE) has considerably benefited from pre-trained language models (PLMs) by fine-tuning. However, existing pre-training methods have not involved modeling event characteristics, resulting in the developed EE models cannot take full advantage of large-scale unsupervised data. To this end, we propose CLEVE, a contrastive pre-training framework for EE to better learn event knowledge from large unsupervised data and their semantic structures (e.g. AMR) obtained with automatic parsers. CLEVE contains a text encoder to learn event semantics and a graph encoder to learn event structures respectively. Specifically, the text encoder learns event semantic representations by self-supervised contrastive learning to represent the words of the same events closer than those unrelated words; the graph encoder learns event structure representations by graph contrastive pre-training on parsed event-related semantic structures. The two complementary representations then work together to improve both the conventional supervised EE and the unsupervised \"liberal\" EE, which requires jointly extracting events and discovering event schemata without any annotated data. Experiments on ACE 2005 and MAVEN datasets show that CLEVE achieves significant improvements, especially in the challenging unsupervised setting. The source code and pre-trained checkpoints can be obtained from https://github.com/THU-KEG/CLEVE.", "num_citations": "2\n", "authors": ["1187"]}
{"title": "Dynamic Anticipation and Completion for Multi-Hop Reasoning over Sparse Knowledge Graph\n", "abstract": " Multi-hop reasoning has been widely studied in recent years to seek an effective and interpretable method for knowledge graph (KG) completion. Most previous reasoning methods are designed for dense KGs with enough paths between entities, but cannot work well on those sparse KGs that only contain sparse paths for reasoning. On the one hand, sparse KGs contain less information, which makes it difficult for the model to choose correct paths. On the other hand, the lack of evidential paths to target entities also makes the reasoning process difficult. To solve these problems, we propose a multi-hop reasoning model named DacKGR over sparse KGs, by applying novel dynamic anticipation and completion strategies: (1) The anticipation strategy utilizes the latent prediction of embedding-based models to make our model perform more potential path search over sparse KGs. (2) Based on the anticipation information, the completion strategy dynamically adds edges as additional actions during the path search, which further alleviates the sparseness problem of KGs. The experimental results on five datasets sampled from Freebase, NELL and Wikidata show that our method outperforms state-of-the-art baselines. Our codes and datasets can be obtained from https://github.com/THU-KEG/DacKGR", "num_citations": "2\n", "authors": ["1187"]}
{"title": "Representation Learning for the Semantic Web\n", "abstract": " Representation learning for the Semantic Web - MADOC Website der UB | Impressum | Datenschutzerkl\u00e4rung | Drucken | English Clear Cookie - decide language by browser settings MADOC Universit\u00e4t Mannheim Startseite St\u00f6bern Volltexte Universit\u00e4tsbibliographie Statistik \u00dcber MADOC Hilfe Kontakt Login Erweiterte Suche Representation learning for the Semantic Web Paulheim, Heiko ; Tresp, Volker ; Liu, Zhiyuan DOI: https://doi.org/10.1016/j.websem.URL: https://www.sciencedirect.com/science/article/abs/... Dokumenttyp: Zeitschriftenartikel Erscheinungsjahr: 2020 Titel einer Zeitschrift oder einer Reihe: Journal of Web Semantics Band/Volume: 61/62 Seitenbereich: Article 100570 Ort der Ver\u00f6ffentlichung: Amsterdam [ua] Verlag: Elsevier ISSN: 1570-8268 Sprache der Ver\u00f6ffentlichung: Englisch Einrichtung: Fakult\u00e4t f\u00fcr Wirtschaftsinformatik und Wirtschaftsmathematik > Web Data Mining (Paulheim 2018-) \u2026", "num_citations": "2\n", "authors": ["1187"]}
{"title": "\u793e\u4ea4\u5a92\u4f53\u5e73\u53f0\u8c23\u8a00\u7684\u65e9\u671f\u81ea\u52a8\u68c0\u6d4b\n", "abstract": " \u5728\u793e\u4ea4\u5a92\u4f53\u670d\u52a1\u8fc5\u901f\u53d1\u5c55\u4e0e\u666e\u53ca\u7684\u4eca\u5929, \u8c23\u8a00\u4f20\u64ad\u4ee5\u524d\u6240\u672a\u6709\u7684\u8fc5\u731b\u4e4b\u52bf\u5bf9\u4eba\u7c7b\u793e\u4f1a\u4ea7\u751f\u7740\u5de8\u5927\u7684\u5f71\u54cd. \u540c\u65f6, \u4eba\u5de5\u667a\u80fd\u6280\u672f\u7684\u5f02\u519b\u7a81\u8d77, \u4e5f\u4e3a\u793e\u4ea4\u5a92\u4f53\u5e73\u53f0\u7684\u8c23\u8a00\u81ea\u52a8\u68c0\u6d4b\u63d0\u4f9b\u4e86\u53ef\u80fd. \u8c23\u8a00\u68c0\u6d4b\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u662f, \u901a\u8fc7\u5b66\u4e60\u67d0\u6761\u793e\u4ea4\u5a92\u4f53\u4fe1\u606f\u7684\u6240\u6709\u8f6c\u53d1\u6216\u8bc4\u8bba\u7684\u8bed\u4e49\u8868\u793a, \u6765\u9884\u6d4b\u8be5\u6761\u793e\u4ea4\u5a92\u4f53\u4fe1\u606f\u662f\u5426\u4e3a\u8c23\u8a00. \u7136\u800c, \u662f\u5426\u80fd\u5728\u8c23\u8a00\u5f15\u8d77\u4e25\u91cd\u7684\u793e\u4f1a\u5f71\u54cd\u4e4b\u524d\u5c3d\u65e9\u6709\u6548\u505a\u51fa\u5224\u65ad (\u8c23\u8a00\u65e9\u671f\u68c0\u6d4b) \u81f3\u5173\u91cd\u8981, \u8fd9\u4e00\u95ee\u9898\u5728\u4ee5\u5f80\u7684\u7814\u7a76\u4e2d\u5c1a\u672a\u5f97\u5230\u5f88\u597d\u7684\u89e3\u51b3. \u672c\u6587\u603b\u7ed3\u4e86\u73b0\u6709\u793e\u4ea4\u5a92\u4f53\u5e73\u53f0\u8c23\u8a00\u81ea\u52a8\u68c0\u6d4b\u7684\u4e3b\u8981\u6280\u672f\u8def\u7ebf, \u5e76\u63a2\u8ba8\u4e86\u8fdb\u884c\u8c23\u8a00\u65e9\u671f\u68c0\u6d4b\u7684\u53ef\u80fd\u6027.", "num_citations": "2\n", "authors": ["1187"]}
{"title": "Errata: Distant Supervision for Relation Extraction with Matrix Completion\n", "abstract": " The essence of distantly supervised relation extraction is that it is an incomplete multi-label classification problem with sparse and noisy features. To tackle the sparsity and noise challenges, we propose solving the classification problem using matrix completion on factorized matrix of minimized rank. We formulate relation classification as completing the unknown labels of testing items (entity pairs) in a sparse matrix that concatenates training and testing textual features with training labels. Our algorithmic framework is based on the assumption that the rank of item-by-feature and item-by-label joint matrix is low. We apply two optimization models to recover the underlying low-rank matrix leveraging the sparsity of feature-label matrix. The matrix completion problem is then solved by the fixed point continuation (FPC) algorithm, which can find the global optimum. Experiments on two widely used datasets with different dimensions of textual features demonstrate that our low-rank matrix completion approach significantly outperforms the baseline and the state-of-the-art methods.", "num_citations": "2\n", "authors": ["1187"]}
{"title": "Capturing Global Informativeness in Open Domain Keyphrase Extraction\n", "abstract": " Open-domain KeyPhrase Extraction (KPE) aims to extract keyphrases from documents without domain or quality restrictions, e.g., web pages with variant domains and qualities. Recently, neural methods have shown promising results in many KPE tasks due to their powerful capacity for modeling contextual semantics of the given documents. However, we empirically show that most neural KPE methods prefer to extract keyphrases with good phraseness, such as short and entity-style n-grams, instead of globally informative keyphrases from open-domain documents. This paper presents JointKPE, an open-domain KPE architecture built on pre-trained language models, which can capture both local phraseness and global informativeness when extracting keyphrases. JointKPE learns to rank keyphrases by estimating their informativeness in the entire document and is jointly trained on the keyphrase\u00a0\u2026", "num_citations": "1\n", "authors": ["1187"]}
{"title": "\u65b0\u51a0\u75ab\u60c5\u76f8\u5173\u793e\u4ea4\u5a92\u4f53\u8c23\u8a00\u4f20\u64ad\u91cf\u5316\u5206\u6790\n", "abstract": " \u65b0\u51a0\u80ba\u708e\u75ab\u60c5\u7684\u7206\u53d1\u4f34\u968f\u7740\u5927\u91cf\u7684\u8c23\u8a00\u5728\u793e\u4ea4\u5a92\u4f53\u5e73\u53f0\u4f20\u64ad, \u5bf9\u7f51\u7edc\u79e9\u5e8f\u548c\u793e\u4f1a\u7a33\u5b9a\u4ea7\u751f\u4e86\u4e0d\u826f\u5f71\u54cd. \u5df2\u6709\u7684\u75ab\u60c5\u76f8\u5173\u793e\u4ea4\u5a92\u4f53\u8c23\u8a00\u4f20\u64ad\u91cf\u5316\u5206\u6790\u7814\u7a76\u4ec5\u5bf9\u8c23\u8a00\u5185\u5bb9\u7b49\u5355\u4e00\u4f20\u64ad\u8981\u7d20\u5c55\u5f00\u5206\u6790, \u800c\u5ffd\u7565\u4e86\u6784\u6210\u4fe1\u606f\u4f20\u64ad\u7684\u5176\u4ed6\u57fa\u7840\u8981\u7d20, \u5305\u62ec\u4f20\u64ad\u8005, \u53d7\u4f17\u4ee5\u53ca\u4f20\u64ad\u6548\u679c\u7b49. \u540c\u65f6, \u8fd9\u4e9b\u7814\u7a76\u7684\u8c23\u8a00\u6570\u636e\u4e0e\u771f\u5b9e\u7684\u793e\u4ea4\u5a92\u4f53\u8c23\u8a00\u6570\u636e\u4e5f\u5b58\u5728\u5206\u5e03\u504f\u5dee\u548c\u4fe1\u606f\u7f3a\u5931. \u56e0\u6b64, \u57fa\u4e8e\u65b0\u6d6a\u5fae\u535a\u5e73\u53f0\u5bf9\u65b0\u51a0\u75ab\u60c5\u76f8\u5173\u793e\u4ea4\u5a92\u4f53\u8c23\u8a00\u7684\u4f20\u64ad\u5c55\u5f00\u66f4\u52a0\u5168\u9762\u7684\u91cf\u5316\u5206\u6790. \u5177\u4f53\u800c\u8a00, \u9996\u5148\u5bf9\u8c23\u8a00\u4f20\u64ad\u5185\u5bb9\u8fdb\u884c\u5206\u6790, \u5305\u62ec\u5176\u4e3b\u9898\u5206\u6790, \u6d89\u53ca\u5730\u533a\u5206\u6790, \u4e8b\u4ef6\u503e\u5411\u6027\u5206\u6790\u4ee5\u53ca\u60c5\u611f\u5206\u6790; \u8fdb\u4e00\u6b65\u5bf9\u8c23\u8a00\u53c2\u4e0e\u7528\u6237\u8fdb\u884c\u5206\u6790, \u5c06\u53c2\u4e0e\u7528\u6237\u5206\u4e3a 3 \u7c7b: \u9020\u8c23\u8005, \u4f20\u8c23\u8005\u548c\u8f9f\u8c23\u8005, \u5e76\u5206\u522b\u5bf9\u5176\u57fa\u7840\u5c5e\u6027, \u5173\u6ce8\u4e3b\u9898, \u4e2a\u4f53\u60c5\u7eea\u4ee5\u53ca\u81ea\u7f51\u7edc\u5c5e\u6027\u8fdb\u884c\u63a2\u7a76; \u6700\u540e\u5bf9\u8c23\u8a00\u5f15\u53d1\u8206\u60c5\u8fdb\u884c\u5206\u6790, \u63a2\u7a76\u5176\u60c5\u611f\u7684\u6574\u4f53\u5206\u5e03, \u4e0e\u4e3b\u9898, \u5173\u952e\u8bcd\u548c\u5730\u533a\u7684\u5173\u7cfb, \u4ee5\u53ca\u60c5\u611f\u7684\u6f14\u53d8\u89c4\u5f8b. \u8be5\u7814\u7a76\u9996\u6b21\u4ece\u4fe1\u606f\u4f20\u64ad\u7684\u5404\u4e2a\u57fa\u7840\u8981\u7d20\u5c42\u9762\u5bf9\u75ab\u60c5\u76f8\u5173\u7684\u793e\u4ea4\u5a92\u4f53\u8c23\u8a00\u4f20\u64ad\u5c55\u5f00\u91cf\u5316\u5206\u6790, \u4e0d\u4ec5\u5bf9\u65b0\u51a0\u80ba\u708e\u75ab\u60c5\u76f8\u5173\u8c23\u8a00\u4f20\u64ad\u6709\u4e86\u66f4\u5168\u9762\u6df1\u523b\u7684\u8ba4\u8bc6, \u540c\u65f6\u5bf9\u7a81\u53d1\u516c\u5171\u4e8b\u4ef6\u7684\u8c23\u8a00\u7814\u7a76\u548c\u8c23\u8a00\u6cbb\u7406\u4e5f\u5177\u6709\u5341\u5206\u91cd\u8981\u7684\u4ef7\u503c.", "num_citations": "1\n", "authors": ["1187"]}
{"title": "Is Multi-Hop Reasoning Really Explainable? Towards Benchmarking Reasoning Interpretability\n", "abstract": " Multi-hop reasoning has been widely studied in recent years to obtain more interpretable link prediction. However, we find in experiments that many paths given by these models are actually unreasonable, while little works have been done on interpretability evaluation for them. In this paper, we propose a unified framework to quantitatively evaluate the interpretability of multi-hop reasoning models so as to advance their development. In specific, we define three metrics including path recall, local interpretability, and global interpretability for evaluation, and design an approximate strategy to calculate them using the interpretability scores of rules. Furthermore, we manually annotate all possible rules and establish a Benchmark to detect the Interpretability of Multi-hop Reasoning (BIMR). In experiments, we run nine baselines on our benchmark. The experimental results show that the interpretability of current multi-hop reasoning models is less satisfactory and is still far from the upper bound given by our benchmark. Moreover, the rule-based models outperform the multi-hop reasoning models in terms of performance and interpretability, which points to a direction for future research, i.e., we should investigate how to better incorporate rule information into the multi-hop reasoning model. Our codes and datasets can be obtained from https://github.com/THU-KEG/BIMR.", "num_citations": "1\n", "authors": ["1187"]}
{"title": "Neural Gibbs Sampling for Joint Event Argument Extraction\n", "abstract": " Event Argument Extraction (EAE) aims at predicting event argument roles of entities in text, which is a crucial subtask and bottleneck of event extraction. Existing EAE methods either extract each event argument roles independently or sequentially, which cannot adequately model the joint probability distribution among event arguments and their roles. In this paper, we propose a Bayesian model named Neural Gibbs Sampling (NGS) to jointly extract event arguments. Specifically, we train two neural networks to model the prior distribution and conditional distribution over event arguments respectively and then use Gibbs sampling to approximate the joint distribution with the learned distributions. For overcoming the shortcoming of the high complexity of the original Gibbs sampling algorithm, we further apply simulated annealing to efficiently estimate the joint probability distribution over event arguments and make predictions. We conduct experiments on the two widely-used benchmark datasets ACE 2005 and TAC KBP 2016. The Experimental results show that our NGS model can achieve comparable results to existing state-of-the-art EAE methods. The source code can be obtained from https://github. com/THU-KEG/NGS.", "num_citations": "1\n", "authors": ["1187"]}
{"title": "Learning to embed sentences using attentive recursive trees\n", "abstract": " Sentence embedding is an effective feature representation for most deep learning-based NLP tasks. One prevailing line of methods is using recursive latent tree-structured networks to embed sentences with task-specific structures. However, existing models have no explicit mechanism to emphasize taskinformative words in the tree structure. To this end, we propose an Attentive Recursive Tree model (AR-Tree), where the words are dynamically located according to their importance in the task. Specifically, we construct the latent tree for a sentence in a proposed important-first strategy, and place more attentive words nearer to the root; thus, AR-Tree can inherently emphasize important words during the bottomup composition of the sentence embedding. We propose an end-to-end reinforced training strategy for AR-Tree, which is demonstrated to consistently outperform, or be at least comparable to, the state-of-the-art sentence embedding methods on three sentence understanding tasks.", "num_citations": "1\n", "authors": ["1187"]}