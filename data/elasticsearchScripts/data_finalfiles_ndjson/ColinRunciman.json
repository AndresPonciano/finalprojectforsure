{"title": "Haskell and XML: Generic combinators or type-based translation?\n", "abstract": " We present two complementary approaches to writing XML document-processing applications in a functional language. In the first approach, the generic tree structure of XML documents is used as the basis for the design of a library of combinators for generic processing: selection, generation, and transformation of XML trees. The second approach is to use a type-translation framework for treating XML document type definitions (DTDs) as declarations of algebraic data types, and a derivation of the corresponding functions for reading and writing documents as typed values in Haskell.", "num_citations": "272\n", "authors": ["1894"]}
{"title": "Smallcheck and lazy smallcheck: automatic exhaustive testing for small values\n", "abstract": " This paper describes two Haskell libraries for property-based testing. Following the lead of QuickCheck, these testing libraries SmallCheck and Lazy SmallCheck also use type-based generators to obtain test-sets of finite values for which properties are checked, and report any counter-examples found. But instead of using a sample of randomly generated values they test properties for all values up to some limiting depth, progressively increasing this limit. The paper explains the design and implementation of both libraries and evaluates them in comparison with each other and with QuickCheck.", "num_citations": "221\n", "authors": ["1894"]}
{"title": "Heap profiling of lazy functional programs\n", "abstract": " We describe the design, implementation and use of a new kind of profiling tool that yields valuable information about the memory use of lazy functional programs. The tool has two parts: a modified functional language implementation which generates profiling information during the execution of programs, and a separate program which converts this information to graphical form. With the aid of profile graphs, one can make alterations to a functional program which dramatically reduce its space consumption. We demonstrate this in the case of a genuine example - the first to which the tool was applied - for which the results are strikingly successful.", "num_citations": "106\n", "authors": ["1894"]}
{"title": "Retrieving reusable software components by polymorphic type\n", "abstract": " Polymorphic types are labels classifying both (a) degned components in a library and@) contexts of free variables in partially written programs. We propose to help programmers make better use of software libraries by providing a system that, given (b). identifies candidates from (a) with matching types. Assuming at first that matching means unifying (ie having a common instance) we discuss efficient ways of implementing such a retrieval system, and also indicate its likely effectiveness based on a quantitative study of currently available libraries. Later we introduce the applicative inamnce relation between types* which captures some iUttlitiOnS about generalisation/specialisation~ and discuss its use as the basis of a more flexible system.", "num_citations": "106\n", "authors": ["1894"]}
{"title": "Retrieving re-usable software components by polymorphic type\n", "abstract": " Polymorphic types are labels classifying both (a) degned components in a library and@) contexts of free variables in partially written programs. We propose to help programmers make better use of software libraries by providing a system that, given (b). identifies candidates from (a) with matching types. Assuming at first that matching means unifying (ie having a common instance) we discuss efficient ways of implementing such a retrieval system, and also indicate its likely effectiveness based on a quantitative study of currently available libraries. Later we introduce the applicative inamnce relation between types* which captures some iUttlitiOnS about generalisation/specialisation~ and discuss its use as the basis of a more flexible system.", "num_citations": "106\n", "authors": ["1894"]}
{"title": "Multiple-view tracing for Haskell: a new Hat\n", "abstract": " Different tracing systems for Haskell give different views of a program at work. In practice, several views are complementary and can productively be used together. Until now each system has generated its own trace, containing only the information needed for its particular view. Here we present the design of a trace that can serve several views. The trace is generated and written to file as the computation proceeds. We have implemented both the generation of the trace and several different viewers.", "num_citations": "92\n", "authors": ["1894"]}
{"title": "Uniform boilerplate and list processing\n", "abstract": " Generic traversals over recursive data structures are often referred to as boilerplate code. The definitions of functions involving such traversals may repeat very similar patterns, but with variations for different data types and different functionality. Libraries of operations abstracting away boilerplate code typically rely on elaborate types to make operations generic. The motivating observation for this paper is that most traversals have value-specific behaviour for just one type. We present the design of a new library exploiting this assumption. Our library allows concise expression of traversals with competitive performance.", "num_citations": "91\n", "authors": ["1894"]}
{"title": "Tracing lazy functional computations using redex trails\n", "abstract": " We describe the design and implementation of a system for tracing computations in a lazy functional language. The basis of our tracing method is a program transformation carried out by the compiler: transformed programs compute the same values as the original, but embedded in functional data structures that also include redex trails showing how the values were obtained. A special-purpose display program enables detailed but selective exploration of the redex trails, with cross-links to the source program.", "num_citations": "89\n", "authors": ["1894"]}
{"title": "Freja, hat and hood-a comparative evaluation of three systems for tracing and debugging lazy functional programs\n", "abstract": " In this paper we compare three systems for tracing and debugging Haskell programs: Freja, Hat and Hood. We evaluate their usefulness in practice by applying them to a number of moderately complex programs in which errors had deliberately been introduced. We identify the strengths and weaknesses of each system and then form ideas on how the systems can be improved further.", "num_citations": "84\n", "authors": ["1894"]}
{"title": "Lag, drag, void and use\u2014heap profiling and space-efficient compilation revisited\n", "abstract": " The context for this paper is functional computation by graph reduction. Our overall aim is more efficient use of memory. The specific topic is the detection of dormant cells in the live graph --- those retained in heap memory though not actually playing a useful role in computation. We describe a profiler that can identify heap consumption by such 'useless' cells. Unlike heap profilers based on traversals of the live heap, this profiler works by examining cells postmortem. The new profiler has revealed a surprisingly large proportion of 'useless' cells, even in some programs that previously seemed space-efficient such as the boot-strapping Haskell compiler nhc.", "num_citations": "79\n", "authors": ["1894"]}
{"title": "Haskell program coverage\n", "abstract": " We describe the design, implementation and use of HPC, a tool-kit to record and display Haskell Program Coverage. HPC includes tools that instrument Haskell programs to record program coverage, run instrumented programs, and display information derived from coverage data in various ways.", "num_citations": "60\n", "authors": ["1894"]}
{"title": "Specifying pointer structures by graph reduction\n", "abstract": " Graph reduction specifications (GRSs) are a\u00a0powerful new method for specifying classes of pointer data structures (shapes). They cover important shapes, like various forms of balanced trees, that cannot be handled by existing methods.               This paper formally defines GRSs as graph reduction systems with a signature restriction and an accepting graph. We are mainly interested in PGRSs \u2014 polynomially-terminating GRSs whose graph languages are closed under reduction and have a\u00a0polynomial membership test.               We investigate the power of the PGRS framework by presenting example specifications and by considering its language closure properties: PGRS languages are closed under intersection; not closed under union (unless we drop the closedness restriction and exclude languages with the empty graph); and not closed under complement.               Our practical investigation presents\u00a0\u2026", "num_citations": "60\n", "authors": ["1894"]}
{"title": "A supercompiler for core Haskell\n", "abstract": " Haskell is a functional language, with features such as higher order functions and lazy evaluation, which allow succinct programs. These high-level features present many challenges for optimising compilers. We report practical experiments using novel variants of supercompilation, with special attention to let bindings and the generalisation technique.", "num_citations": "56\n", "authors": ["1894"]}
{"title": "Linearity and laziness\n", "abstract": " A criticism often levelled at functional languages is that they do not cope elegantly or efficiently with problems involving changes of state. In a recent paper [26], Wadler has proposed a new approach to these problems. His proposal involves the use of a type system based on the linear logic of Girard [7]. This allows the programmer to specify the \u201cnatural\u201d imperative operations without at the same time sacrificing the crucial property of referential transparency.             In this paper we investigate the practicality of Wadler's approach, describing the design and implementation of a variant of Lazy ML [2]. A small example program shows how imperative operations can be used in a referentially transparent way, and at the same time it highlights some of the problems with the approach. Our implementation is based on a variant of the G-machine [15, 1]. We give some benchmark figures to compare the performance of\u00a0\u2026", "num_citations": "51\n", "authors": ["1894"]}
{"title": "Transforming Haskell for tracing\n", "abstract": " Hat is a programmer\u2019s tool for generating a trace of a computation of a Haskell 98 program and viewing such a trace in various different ways. Applications include program comprehension and debugging. A new version of Hat uses a stand-alone program transformation to produce self-tracing Haskell programs. The transformation is small and works with any Haskell 98 compiler that implements the standard foreign function interface. We present general techniques for building compiler independent tools similar to Hat based on program transformation. We also point out which features of Haskell 98 caused us particular grief.", "num_citations": "43\n", "authors": ["1894"]}
{"title": "The Reduceron Reconfigured and Reevaluated\n", "abstract": " A new version of a special-purpose processor for running lazy functional programs is presented. This processor \u2013 the Reduceron \u2013 exploits parallel memories and dynamic analyses to increase evaluation speed, and is implemented using reconfigurable hardware. Compared to a more conventional functional language implementation targeting a standard RISC processor running on the same reconfigurable hardware, the Reduceron offers a significant improvement in run-time performance.", "num_citations": "39\n", "authors": ["1894"]}
{"title": "Complete and partial redex trails of functional computations\n", "abstract": " Redex trails are histories of functional computations by graph reduction; their main application is fault-tracing. A prototype implementation of a tracer based on redex trails [8] demonstrated the promise of the technique, but was limited in two respects: (1) trails did not record every reduction, only those constructing a new value; (2) even so computing trails was very expensive, particularly in terms of the memory space they occupied. In this paper, we address both problems: complete redex trails provide a full computational record; partial versions of these trails exclude all but selected details, greatly reducing memory costs. We include results of experiments tracing several applications, including a compiler.", "num_citations": "36\n", "authors": ["1894"]}
{"title": "Lazy assertions\n", "abstract": " Assertions test expected properties of run-time values without disrupting the normal working of a program. So in a lazy functional language assertions should be lazy \u2013 not forcing evaluation, but only examining what is evaluated by other parts of the program. We explore the subtle semantics of lazy assertions and describe sequential and concurrent variants of a method for checking lazy assertions. All variants are implemented in Haskell.", "num_citations": "35\n", "authors": ["1894"]}
{"title": "Equal opportunity interactive systems\n", "abstract": " One view of interactive computer systems is that the user, having problems to solve, supplies the \u201cgivens\u201d of these problems to the machine, which in response supplies as output the \u201cunknowns\u201d. Reassigning or discarding these labels \u201cgivens\u201d and \u201cunknown\u201d is a time-honoured heuristic for problem-solving. Also, people seem to prefer interpretations without such labels for fast interactive systems, and mere speed in systems that do embody fixed distinctions between input and output often contributes little towards ease of use\u2014it may only serve to emphasize a frustrating mechanical dumbness. We therefore apply the same heuristic to the design of interactive computer systems, noting that a number of existing successful interactive system styles can be viewed as the outcome of this approach.", "num_citations": "35\n", "authors": ["1894"]}
{"title": "Gadgets: Lazy functional components for graphical user interfaces\n", "abstract": " We describe a process extension to a lazy functional programming system, intended for applications with graphical user interfaces (GUIs). In the extended language, dynamically-created processes communicate by asynchronous message passing. We illustrate the use of the language, including as an extended example a simple board game in which squares are implemented as concurrent processes. We also describe a window manager, itself implemented in the extended functional language.", "num_citations": "34\n", "authors": ["1894"]}
{"title": "Profiling parallel functional computations (without parallel machines)\n", "abstract": " Pick up a textbook on functional programming. Somewhere amidst the introductory remarks the author will almost certainly claim that functional languages are well suited for programming parallel computers. But are they? Not, it would seem, without the use of annotations to indicate where parallel evaluation should be performed. Such annotations have at least two advantages over a laissez-faire approach to parallelism. Firstly, they make it easier to reason about the program\u2019s behaviour when it is run on a parallel machine. The annotated program describes both what is to be computed and how it is to be done, while maintaining a clean separation between the two aspects. Secondly, annotations mean that the compiler does not have to tackle the difficult problem of determining a suitable parallel evaluation strategy for the program. That remains the programmer\u2019s responsibility.", "num_citations": "33\n", "authors": ["1894"]}
{"title": "Not all patterns, but enough: an automatic verifier for partial but sufficient pattern matching\n", "abstract": " We describe an automated analysis of Haskell 98 programs to check statically that, despite the possible use of partial (or non-exhaustive) pattern matching, no pattern-match failure can occur. Our method is an iterative backward analysis using a novel form of pattern-constraint to represent sets of data values. The analysis is defined for a core first-order language to which Haskell 98 programs are reduced. Our analysis tool has been successfully applied to a range of programs, and our techniques seem to scale well. Throughout the paper, methods are represented much as we have implemented them in practice, again in Haskell.", "num_citations": "30\n", "authors": ["1894"]}
{"title": "Checking the shape safety of pointer manipulations\n", "abstract": " We present a\u00a0new algorithm for checking the shape-safety of pointer manipulation programs. In our model, an abstract, data-less pointer structure is a\u00a0graph. A\u00a0shape is a\u00a0language of graphs. A\u00a0pointer manipulation program is modelled abstractly as a\u00a0set of graph rewrite rules over such graphs where each rule corresponds to a\u00a0pointer manipulation step. Each rule is annotated with the intended shape of its domain and range and our algorithm checks these annotations.               We formally define the algorithm and apply it to a\u00a0binary search tree insertion program. Shape-safety is undecidable in general, but our method is more widely applicable than previous checkers, in particular, it can check programs that temporarily violate a\u00a0shape by the introduction of intermediate shape definitions.", "num_citations": "30\n", "authors": ["1894"]}
{"title": "The Reduceron: Widening the von Neumann Bottleneck for Graph Reduction Using an FPGA\n", "abstract": " For the memory intensive task of graph reduction, modern PCs are limited not by processor speed, but by the rate that data can travel between processor and memory. This limitation is known as the von Neumann bottleneck. We explore the effect of widening this bottleneck using a special-purpose graph reduction machine with wide, parallel memories. Our prototype machine \u2013 the Reduceron \u2013 is implemented using an FPGA, and is based on a simple template-instantiation evaluator. Running at only 91.5MHz on an FPGA, the Reduceron is faster than mature bytecode implementations of Haskell running on a 2.8GHz PC.", "num_citations": "29\n", "authors": ["1894"]}
{"title": "A static checker for safe pattern matching in Haskell\n", "abstract": " A Haskell program may fail at runtime with a pattern-match error if the program has any incomplete (non-exhaustive) patterns in de\ufb01nitions or case alternatives. This paper describes a static checker that allows non-exhaustive patterns to exist, yet ensures that a pattern-match error does not occur. It describes a constraint language that can be used to reason about pattern matches, along with mechanisms to propagate these constraints between program components.", "num_citations": "28\n", "authors": ["1894"]}
{"title": "Interaction models and the principled design of interactive systems\n", "abstract": " System design should be controlled by sound engineering principles. We discuss issues concerned with the derivation and formalisation of such principles that may be employed in the construction of specifications of interactive systems and in their validation. We present the state of a research method for the design of interactive systems, which is currently being used and is indeed incorporating user engineering principles into the design process.             Our present discussion focuses on two distinct problems: (1) the derivation of appropriate and effective principles of interaction behaviour; (2) how appropriate formulations of principles may be applied to the design process using mathematical models of interactive behaviour. We also report on the application of these experimental techniques to a realistic example.", "num_citations": "28\n", "authors": ["1894"]}
{"title": "Losing functions without gaining data: another look at defunctionalisation\n", "abstract": " We describe a transformation which takes a higher-order program, and produces an equivalent first-order program. Unlike Reynolds-style defunctionalisation, it does not introduce any new data types, and the results are more amenable to subsequent analysis operations. We can use our method to improve the results of existing analysis operations, including strictness analysis, pattern-match safety and termination checking. Our transformation is implemented, and works on a Core language to which Haskell programs can be reduced. Our method cannot always succeed in removing all functional values, but in practice is remarkably successful.", "num_citations": "27\n", "authors": ["1894"]}
{"title": "New dimensions in heap profiling\n", "abstract": " First-generation heap profilers for lazy functional languages have proved to be effective tools for locating some kinds of space faults, but in other cases they cannot provide sufficient information to solve the problem. This paper describes the design, implementation and use of a new profiler that goes beyond the two-dimensional \u2018who produces what\u2019 view of heap cells to provide information about their more dynamic and structural attributes. Specifically, the new profiler can distinguish between cells according to their eventual lifetime, or on the basis of the closure retainers by virtue of which they remain part of the live heap. A bootstrapping Haskell compiler (nhc) hosts the implementation: among examples of the profiler's use we include self-application to nhc. Another example is the original heap-profiling case study clausify, which now consumes even less memory and is much faster.", "num_citations": "27\n", "authors": ["1894"]}
{"title": "Extending a functional programming system for embedded applications\n", "abstract": " Functional languages do not usually mesh well with embedded applications because of the need for special I/O device\u2010handling. By introducing a process model to a language, however, it becomes possible to express register\u2010level device operations and interrupts in a modular manner. This paper describes such a model, its implementation by extension to the Gofer programming system, and examples of its use. Performance results indicate that even this prototype interpretive system is adequate for small applications. The major gain of using a functional language is the ease with which abstraction can be layered over low\u2010level detail, improving both the readability of code and its tractability.", "num_citations": "26\n", "authors": ["1894"]}
{"title": "Functional Languages and Graphical User Interfaces: A Review and a Case Study\n", "abstract": " At rst sight, I/O in a pure functional language is not as straightforward as in imperative languages. For some years work has been going on to alleviate these problems, and there are now a number of di erent approaches. The purpose of this report is twofold| rstly we shall review the problems encountered in performing I/O in a functional language and look at some of the ways these might be conquered, and secondly we shall look at some more recent solutions to I/O which encompass graphical interfaces.", "num_citations": "25\n", "authors": ["1894"]}
{"title": "Heap profiling for space efficiency\n", "abstract": " Excessive requirements for memory space have in the past hindered or even prevented otherwise attractive applications of functional programming. Although this could be blamed in part on space-hungry implementation methods, in most cases it would have been possible to cut memory requirements very significantly by making a few changes to the source program. But there were no tools to help programmers make appropriate changes. Usage of heap memory was reported only as a total volume of allocations; there was no way to investigate how different parts of a program made demands on heap memory--something which may be far from apparent in the source of a sizable program making use of lazy evaluation and higher-order functions. Finding the appropriate place to make a space-saving change could be very difficult. In the last few years, there has been a renewed effort to provide appropriate profiling\u00a0\u2026", "num_citations": "23\n", "authors": ["1894"]}
{"title": "Expert system guides CAD for automatic assembly\n", "abstract": " A knowledge\u2010based system is under development to provide component designers at CAD workstations with estimated costs and proposed design modifications for automated manufacture.", "num_citations": "22\n", "authors": ["1894"]}
{"title": "Heap profiling of a lazy functional compiler\n", "abstract": " A significant problem with lazy functional programs is that they often demand a great deal of space. Multi-megabyte workstations are now commonplace, but serious users of functional programming systems have to equip even these machines with additional memory. The essence of laziness is to delay evaluation rather than compute values that may not be needed; and once values are computed to retain them if they may be needed again. This policy might save time, but it can easily lead to space faults: the accumulation or retention of large structures in memory, in ways that the programmer is unaware of, or does not fully understand, let alone intend. Hence profiling tools, by which programmers can obtain information about memory use in terms of the source program, are potentially of great value.", "num_citations": "20\n", "authors": ["1894"]}
{"title": "User programs: a way to match computer systems and human cognition\n", "abstract": " This memory system is mainly concerned with the long-term retention of information. Psychologically, the model is underspecified: it is not Figure 2: details of the attentional system resource control mechanisms inference mechanisms working store intended to capture long-term learning, that is the accretion of information into the memory system or the migration of information between its components. However, we recognise that temporarily activated information within the memory system is a form of transitory storage (an effective extension of the working store), and that such information may have been recently added to the memory system. The action system is responsible for detailed control of output via the musculature. It has no conceptual planning abilities, rather its activity is determined in two ways. It can receive abstract commands from the attentional system, such as\" log on\", requiring a complex sequence of actions; it can also be triggered directly by external input received via the recognition component of the memory system (both input and resulting action plan can be complex---a skilled user might detect and correct a spelling mistake while engrossed in a conversation). The translation from command to action sequence relies heavily upon procedural schemas (action plans) held in the memory system, and includes provision for the automatic processing of external information and feedback. Thus the task of logging on requires the cognitive system to recognise both internal and external feedback from key presses, and perhaps also visual prompts from the display, information which the action system will use as triggers for further action\u00a0\u2026", "num_citations": "20\n", "authors": ["1894"]}
{"title": "Huge data but small programs: Visualization design via multiple embedded DSLs\n", "abstract": " Although applications of functional programming are diverse, most examples deal with modest amounts of data \u2013 no more than a few megabytes. This paper describes how Haskell has been used to address a challenging astrophysics visualization problem, where the complete uncompressed dataset is nearly a terabyte. Our solution makes extensive use of three novel domain-specific languages: to specify data resources, to abstract over rendering operations, and most significantly, to design the desired visualization. The result is a powerful framework for time-varying multi-field visualization. This approach represents a significant departure from standard practices in the visualization field, and has application well beyond the original problem. That our solution consists of less than 4.5K lines of code is itself a notable result. This paper motivates and describes the overall architecture of our solution, and\u00a0\u2026", "num_citations": "19\n", "authors": ["1894"]}
{"title": "Finding inputs that reach a target expression\n", "abstract": " We present an automated program analysis, called Reach, to compute program inputs that cause evaluation of explicitly-marked target expressions. Reach has a range of applications including property refutation, assertion breaking, program crashing, program covering, program understanding, and the development of customised data generators. Reach is based on lazy narrowing, a symbolic evaluation strategy from functional-logic programming. We use Reach to analyse a range of programs, and find it to be a useful tool with clear performance benefits over a method based on exhaustive input generation. We also explore different methods for bounding the search space, the selective use of breadth-first search to find the first solution quickly, and techniques to avoid evaluation that is unnecessary to reach a target.", "num_citations": "19\n", "authors": ["1894"]}
{"title": "Transformation in a non-strict language: An approach to instantiation\n", "abstract": " A problem arises when the usual rules of fold/unfold transformation are applied in a non-strict programming system. Case analysis by instantiation may alter strictness characteristics of the function being transformed, and hence alter the behaviour of programs. Although such behavioural changes can in general be quite subtle, they are all too apparent if the program is interactive, since I/O interleaving is closely tied to strictness properties. A two-phase solution to the problem is proposed. It comprises a suitable form of strictness analysis to determine whether a proposed instantiation is safe, and a procedure to re-formulate troublesome definitions so that, in effect, case analysis is shifted to a nearby safe context.", "num_citations": "19\n", "authors": ["1894"]}
{"title": "Advances in lazy smallcheck\n", "abstract": " A property-based testing library enables users to perform lightweight verification of software. This paper presents improvements to the Lazy SmallCheck property-based testing library. Users can now test properties that quantify over first-order functional values and nest universal and existential quantifiers in properties. When a property fails, Lazy SmallCheck now accurately expresses the partiality of the counterexample. These improvements are demonstrated through several practical examples.", "num_citations": "18\n", "authors": ["1894"]}
{"title": "A model for comparing the space usage of lazy evaluators\n", "abstract": " Identifying the source of space faults in functional programs is hard. The problem is compounded as space usage can vary enormously from one implementation to another. We use a term-graph rewriting model to describe evaluators with explicit space usage. Given descriptions for two evaluators E1 and E2, if E1 never has asymptotically worse space usage than E2, we can use a bisimulation-like proof method to prove it. Conversely, if E1 is leakier than E2, we characterise a class of computations that expose the difference between them.", "num_citations": "18\n", "authors": ["1894"]}
{"title": "An incremental garbage collector for embedded real-time systems\n", "abstract": " Embedded real-time systems often have a single processor and small memory. This combination runs contrary to frequent architectural assumptions in the context of functional programming\u2013parallel processing and/or large memory. The dichotomy is especially a problem when it comes to memory management, for an incremental garbage collection scheme is needed to achieve real-time, but present algorithms require either an extra processor or additional memory\u2013the very thing an embedded system cannot afford. We present an incremental garbage collector of the mark-sweep type, which minimises structural overhead without sacrificing performance. It uses a very small stack for marking the live heap, with \u2018safety bits\u2019 to ensure that stack overflow does not hinder collection. We report on how the algorithm performs and discuss its correctness.", "num_citations": "18\n", "authors": ["1894"]}
{"title": "A functional-logic library for wired\n", "abstract": " We develop a Haskell library for functional-logic programming, motivated by the implementation of Wired, a relational embedded domain-specific language for describing and analysing digital circuits at the VLSI-layout level. Compared to a previous library for logic programming by Claessen and Ljungl\u00f6f, we support residuation, easier creation of logical data types, and pattern matching. We discuss other applications of our library, including test-data generation, and various extensions, including lazy narrowing.", "num_citations": "17\n", "authors": ["1894"]}
{"title": "Fine-grained visualization pipelines and lazy functional languages\n", "abstract": " The pipeline model in visualization has evolved from a conceptual model of data processing into a widely used architecture for implementing visualization systems. In the process, a number of capabilities have been introduced, including streaming of data in chunks, distributed pipelines, and demand-driven processing. Visualization systems have invariably built on stateful programming technologies, and these capabilities have had to be implemented explicitly within the lower layers of a complex hierarchy of services. The good news for developers is that applications built on top of this hierarchy can access these capabilities without concern for how they are implemented. The bad news is that by freezing capabilities into low-level services expressive power and flexibility is lost. In this paper we express visualization systems in a programming language that more naturally supports this kind of processing model. Lazy\u00a0\u2026", "num_citations": "17\n", "authors": ["1894"]}
{"title": "An Incremental, Exploratory and Transformational Environment for Lazy Functional Programming\n", "abstract": " Most programming environments for functional languages offer a single tool used to evaluate programs \u2013 either a batch compiler or an interpreter with a read-eval-print loop. This paper presents a programming environment that supports not only evaluation, but also a range of other programming activities including transformation. The environment is designed to encourage working in an incremental and exploratory style, avoiding constraints on the order in which things must be done yet guarenteeing security. What has already been done towards the development of a program automatically persists, as does information about what has yet to be done. For instance, new laws can be introduced as conjectures and used in program transformation, but full details of proof obligations and dependencies are maintained.The paper outlines the functional language supported by the environment, and uses an extended\u00a0\u2026", "num_citations": "17\n", "authors": ["1894"]}
{"title": "Lambdas in the liftshaft\u2014functional programming and an embedded architecture\n", "abstract": " Embedded computer systems seem to be the antithesis of functional language systems. Embedded systems are small, stand-alone, and are often forced to accept inelegant design compromises due to hardware cost. They run continuously and are reactive, that is, their primary goal is to monitor sensors and control effecters, using observed external events to trigger state-changing control actions. Yet this paper describes how functional abstraction can tame the inelegance of embedded systems. Architectural compromises can be made in device drivers, programmed within the functional language, but a function-level interface is presented to the application programmer. Examples are given from a liftshaft case study.", "num_citations": "16\n", "authors": ["1894"]}
{"title": "The bits between the lambdas: binary data in a lazy functional language\n", "abstract": " For the programmer, storage media are usually assumed to have a minimum atomic unit of transfer of one byte. However, sometimes it is useful to have an even finer storage granularity of one bit, for instance in order to compress data.This paper describes an API in the lazy functional language Haskell for treating storage media as arbitrary-length streams of bits without byte-alignment constraints. So far as possible, storage media are treated uniformly. In particular, bit-stream memory and binary files share the same API -- a new and useful abstraction over memory management and file management. This uniformity of access leads to a novel technique for lazy random-access to files in a purely functional manner. We also describe a technique for automatically deriving compressed binary representations of user-defined data structures, whose operations provide both in-heap data compression and convenient high\u00a0\u2026", "num_citations": "13\n", "authors": ["1894"]}
{"title": "Lazy functional components for graphical user interfaces\n", "abstract": " The subject of this Thesis is the programming of applications that use a graphical user interface (GUI) in a lazy functional language. In graphical user interfaces, many possible paths of interaction are open to the user at once. The central idea of this Thesis is to add concurrent processes to a lazy functional language, allowing GUI programs to be expressed more naturally. An extended review includes an in-depth case study comparing two of the most advanced previous solutions. An extension to a lazy functional language facilitates the de nition of concurrent processes that communicate through typed channels. Interface objects are made more re-usable by parameterising them on the communication channels used. An implementation of concurrent processes in the Gofer interpreter ensures static type-checking of process communication and composition. Unlike previous systems, this Thesis also formulates a screen manager within the functional setting. The validity of the approach is demonstrated through a series of application programs. These include the case-study used to measure two earlier systems, a board game, and a simulator for programs written for another system.", "num_citations": "13\n", "authors": ["1894"]}
{"title": "FitSpec: refining property sets for functional testing\n", "abstract": " This paper presents FitSpec, a tool providing automated assistance in the task of refining sets of test properties for Haskell functions. FitSpec tests mutant variations of functions under test against a given property set, recording any surviving mutants that pass all tests. The number of surviving mutants and any smallest survivor are presented to the user. A surviving mutant indicates incompleteness of the property set, prompting the user to amend a property or to add a new one, making the property set stronger. Based on the same test results, FitSpec also provides conjectures in the form of equivalences and implications between property subsets. These conjectures help the user to identify minimal core subsets of properties and so to reduce the cost of future property-based testing.", "num_citations": "12\n", "authors": ["1894"]}
{"title": "What about the natural numbers?\n", "abstract": " A prime concern in the design of any general purpose programming language should be the ease and safety of working with natural numbers, particularly in conjunction with discrete data structures. This theme of commitment to the naturals as the basic numeric data type is explored in the context of a lazy functional language.", "num_citations": "12\n", "authors": ["1894"]}
{"title": "Performance polymorphism\n", "abstract": " In an interactive functional programming environment with a Milner-style polymorphic type system (Milner 1978), a modification to one definition may imply changes in the types of other definitions. A polymorphic typechecker must carry out some re-typechecking to determine all of these changes. This paper presents a new typechecking algorithm which performs fine-grained re-typechecking based on analysis of individual type constraints. The new algorithm is compared with that of Nikhil (Nikhil 1985), which performs re-typechecking of entire definitions.", "num_citations": "12\n", "authors": ["1894"]}
{"title": "Standard libraries for the Haskell 98 programming language\n", "abstract": " Standard Libraries for the Haskell 98 Programming Language - ORA - Oxford University Research Archive Logos Header links Search History Bookmarks 0 New Search Deposit Help Footer links Deposit Agreements Disclaimer Privacy Policy Cookies Accessibility Statement Take-down Policy Copyright API Contact Skip to main NEW SEARCH Deposit HELP 0 Back to Search CONTACT Name Email Comment Send message Actions Authors Bibliographic Details Terms of Use Stats Export BibTeX EndNote RefWorks General item icon General item Standard Libraries for the Haskell 98 Programming Language Actions Email \u00d7 Send the bibliographic details of this record to your email address. Your Email Please enter the email address that the record information will be sent to. Your message (optional) Please add any additional information to be included within the email. Send Cite \u00d7 APA Style [editor], PJ, Simon, [\u2026", "num_citations": "11\n", "authors": ["1894"]}
{"title": "Semantic errors - diagnosis and repair\n", "abstract": " In compiler technology, semantic error handling has too often used ad hoc techniques and terse messages of fixed content. In contrast, we present general models of diagnosis and repair of semantic errors and illustrate their use in the York Ada Workbench Compiler. The diagnostic model focuses upon the problem of selecting suitable information from which the programmer can locate and correct an error; following a brief discussion of an approach in which filters are applied to reports, and we give a more extensive account of a new approach using an interactive diagnostic interpreter. The repair model is based upon the idea of plastic nodes which are introduced into the compiler's internal representation of a program to replace normal nodes which are missing or erroneous.", "num_citations": "11\n", "authors": ["1894"]}
{"title": "A reference interpreter for the graph programming language GP 2\n", "abstract": " GP 2 is an experimental programming language for computing by graph transformation. An initial interpreter for GP 2, written in the functional language Haskell, provides a concise and simply structured reference implementation. Despite its simplicity, the performance of the interpreter is sufficient for the comparative investigation of a range of test programs. It also provides a platform for the development of more sophisticated implementations.", "num_citations": "10\n", "authors": ["1894"]}
{"title": "Lazy Generation of Canonical Test Programs\n", "abstract": " Property-based testing can be a highly effective form of lightweight verification, but it relies critically on the method used to generate test cases. If we wish to test properties of compilers and related tools we need a generator for source programs as test cases.             We describe experiments generating functional programs in a core first-order language with algebraic data types. Candidate programs are generated freely over a syntactic representation with positional names. Static conditions for program validity and canonical representatives of large equivalence classes are defined separately. The technique is used to investigate the correctness properties of a program optimisation and two language implementations.", "num_citations": "10\n", "authors": ["1894"]}
{"title": "Supercompilation and the Reduceron\n", "abstract": " Reduction of an expression range 0 10={Instantiate function body (1 cycle)}(\u2264) 0 10 [range# 1, range# 2] 0 10={Primitive application (1 cycle)} True [range# 1, range# 2] 0 10={Constructor reduction (0 cycle)} range# 2 [range# 1, range# 2] 0 10={Instantiate function body (2 cycles)} Cons 0 (range ((+) 0 1) 10)", "num_citations": "10\n", "authors": ["1894"]}
{"title": "A space semantics for core Haskell\n", "abstract": " Haskell currently lacks a standard operational semantics. We argue that such a semantics should be provided to enable reasoning about operational properties of programs, to ensure that implementations guarantee certain space and time behaviour and to help determine the source of space faults. We present a small-step deterministic semantics for the sequential evaluation of Core Haskell programs and show that it is an accurate model of asymptotic space and time usage. The semantics is a formalisation of a graphical notation so it provides a useful mental model as well as a precise mathematical notation. We discuss its implications for education, programming and implementation. The basic semantics is extended with a monadic IO mechanism so that all the space under the control of an implementation is included.", "num_citations": "10\n", "authors": ["1894"]}
{"title": "Heap compression and binary I/O in Haskell\n", "abstract": " Two new facilities for Haskell are described: compression of data values in memory, and a new scheme for binary I/O. These facilities, although they can be used individually, can also be combined because they use the same binary representations for values. Heap compression in memory is valuable because it enables programs to run on smaller machines, or conversely allows programs to store more data in the same amount of memory. Binary I/O is valuable because it makes the le storage and retrieval of heap data structures smooth and painless. The combination of heap compression and binary I/O allows data transfer to be both fast and space-e cient.", "num_citations": "10\n", "authors": ["1894"]}
{"title": "Type-checked message-passing between functional processes\n", "abstract": " Karlsson introduced the notion of communicating functional processes. It relied on a non-deterministic function, used within the functional program, to implement message-passing between processes. Stoye described a sorting office for inter-process messages which removed the need for non-determinism in the functional language. Turner added a limited form of static type-checking on message traffic passing through the sorting office. This paper describes the use of constructor classes to improve the static type-checking model, with example applications in Embedded Gofer.", "num_citations": "10\n", "authors": ["1894"]}
{"title": "Problems and proposals for time and space profiling of functional programs\n", "abstract": " Many functional programs are quick and concise to express but large and slow to run. A few critical parts of the program may account for much of the time and space used: replacing these with something cheaper, or applying them less often, significantly improves overall performance of the program. The repetition of this refinement process, starting from a deliberately artless prototype, is a popular strategy for software development. But its effectiveness depends on the ability to identify the \u201ccritical parts\u201d, and this is far from easy in a lazy functional language. Commenting on the experiences of a colleague developing a large application with their LML system, Augustsson and Johnsson note that                  After long and painful searching in the program he finally found three places where full laziness was lost, each of which accounted for an order of magnitude in time (and it is not guaranteed that all such places\u00a0\u2026", "num_citations": "10\n", "authors": ["1894"]}
{"title": "Speculate: discovering conditional equations and inequalities about black-box functions by reasoning from test results\n", "abstract": " This paper presents Speculate, a tool that automatically conjectures laws involving conditional equations and inequalities about Haskell functions. Speculate enumerates expressions involving a given collection of Haskell functions, testing to separate those expressions into apparent equivalence classes. Expressions in the same equivalence class are used to conjecture equations. Representative expressions of different equivalence classes are used to conjecture conditional equations and inequalities. Speculate uses lightweight equational reasoning based on term rewriting to discard redundant laws and to avoid needless testing. Several applications demonstrate the effectiveness of Speculate.", "num_citations": "9\n", "authors": ["1894"]}
{"title": "Widening the representation bottleneck: a functional implementation of relational programming\n", "abstract": " Relational programming is a generalisation of functional programming that includes aspects of logic programming. We describe a relational language, Drusil. la, that retains the lazy, polymorphic and higher-order aspects of functional languages and the flexible handling of non-determinism and search based computation of logic languages. As a result it offers certain economy of expression not found in functional or logic programming. However a complex implementation, using a combination of polymorphic type inference and automatic program transformation to select appropriate representations, is needed to support this language. representation. For example, consider MacLennan\u2019s word frequency program shown in Figure 1. Two domain restriction operators must be defined\u2014one for restricting an intentional relation(restrict), and one for restricting an extensional relation(->). Even then, both operators require the\u00a0\u2026", "num_citations": "9\n", "authors": ["1894"]}
{"title": "Inductive benchmarking for purely functional data structures\n", "abstract": " Every designer of a new data structure wants to know how well it performs in comparison with others. But finding, coding and testing applications as benchmarks can be tedious and time-consuming. Besides, how a benchmark uses a data structure may considerably affect its apparent efficiency, so the choice of applications may bias the results. We address these problems by developing a tool for inductive benchmarking. This tool, Auburn, can generate benchmarks across a wide distribution of uses. We precisely define \u2018the use of a data structure\u2019, upon which we build the core algorithms of Auburn: how to generate a benchmark from a description of use, and how to extract a description of use from an application. We then apply inductive classification techniques to obtain decision trees for the choice between competing data structures. We test Auburn by benchmarking several implementations of three common\u00a0\u2026", "num_citations": "8\n", "authors": ["1894"]}
{"title": "Techniques for simplifying the visualization of graph reduction\n", "abstract": " Space and time problems still occasionally dog the functional programmer, despite increasingly efficient implementations and the recent spate of useful profiling tools. There is a need for a model of program reduction that relates directly to the user\u2019s code and has a simple graphical representation. Na\u00efve graph reduction provides this. We address the problems of displaying a series of program graphs which may be long, and the elements of which may be large and complex. We offer a scheme for compacting an individual display by creating a quotient graph through defining equivalence classes, and a similar scheme for reducing the number of graphs to show. A metalanguage to allow the user to define compaction rules gives the model flexibility. A prototype system exists in a Haskell implementation.", "num_citations": "8\n", "authors": ["1894"]}
{"title": "A relational programming system with inferred representations\n", "abstract": " Relational programming was originally proposed by MacLennan [4, 5, 6]. He advocated a language based on binary relations and operators for combining and manipulating relations. Such operators form a relational algebra--a set of combining forms for relations which generulise a language like FP from functions to relations. MacLenna~ designed a relational language and built an interpreter for it [1] but his implementation was achieved at the expense of an explicit compromise to relational abstraction. He split relational programming into two worlds--an intensional world (relations represented by many-to-one functions) and an extensional world (relations represented by association lists). The relational operators were similarly segregated into two groups--one applicable to intensional, the other to extensional relations. This representation divide cor~ siderably inhibited the freedom of programmer expression.The\u00a0\u2026", "num_citations": "8\n", "authors": ["1894"]}
{"title": "Improving implicit parallelism\n", "abstract": " Using static analysis techniques compilers for lazy functional languages can be used to identify parts of a program that can be legitimately evaluated in parallel and ensure that those expressions are executed concurrently with the main thread of execution. These techniques can produce improvements in the runtime performance of a program, but are limited by the static analyses\u2019 poor prediction of runtime performance. This paper outlines the development of a system that uses iterative profile-directed improvement in addition to well-studied static analysis techniques. This allows us to achieve higher performance gains than through static analysis alone.", "num_citations": "7\n", "authors": ["1894"]}
{"title": "Deja Fu: a concurrency testing library for Haskell\n", "abstract": " Systematic concurrency testing (SCT) is an approach to testing potentially nondeterministic concurrent programs. SCT avoids potentially unrepeatable results that may arise from unit testing concurrent programs. It seems to have received little attention from Haskell programmers. This paper introduces a generalisation of Haskell's concurrency abstraction in the form of typeclasses, and a library for testing concurrent programs. A number of examples are provided, some of which come from pre-existing packages.", "num_citations": "7\n", "authors": ["1894"]}
{"title": "Two-pass heap profiling: A matter of life and death\n", "abstract": " A heap profile is a chart showing the contents of heap memory throughout a computation. Contents are depicted abstractly by showing how much space is occupied by memory cells in each of several classes. A good heap profiler can use a variety of attributes of memory cells to define a classification. Effective profiling usually involves a combination of attributes. The ideal profiler gives full support for combination in two ways. First, a section of the heap of interest to the programmer can be specified by constraining the values of any combination of cell attributes. Secondly, no matter what attributes are used to specify such a section, a heap profile can be obtained for that section only, and any other attribute can be used to define the classification. Achieving this ideal is not simple for some combinations of attributes. A heap profile is derived by interpolation of a series of censuses of heap contents at different\u00a0\u2026", "num_citations": "7\n", "authors": ["1894"]}
{"title": "Lag, drag and post-mortem heap profiling\n", "abstract": " The context for this paper is functional computation by graph reduction. Our overall aim is more efficient use of memory. The specific topic is the detection of dormant cells in the live graph---those retained in heap memory though not actually playing a useful role in computation. We describe a profiler that can identify heap consumption by suchuseless' cells. Unlike heap profilers based on traversals of the live heap, this profiler works by examining cells post-mortem. Early experience with applications confirms our suspicions about space-efficiency: as usual, it is even worse than expected! 1 Introduction A typical computation by graph reduction involves a large and changing population of heap-memory cells. Taking a census of this population at regular intervals can be very instructive, both for functional programmers and for functionallanguage implementors. A heap profiler [RW93] records population counts for different classes of cells (eg. representing different values, creat...", "num_citations": "7\n", "authors": ["1894"]}
{"title": "A lightweight hat: Simple type-preserving instrumentation for self-tracing lazy functional programs\n", "abstract": " Existing methods for generating a detailed trace of a computation of a lazy functional program are complex. These complications limit the use of tracing in practice. However, such a detailed trace is desirable for understanding and debugging a lazy functional program. Here we present a lightweight method that instruments a program to generate such a trace, namely the augmented redex trail introduced by the Haskell tracer Hat. The new method is a major step towards an omniscient debugger for real-world Haskell programs.", "num_citations": "6\n", "authors": ["1894"]}
{"title": "The space usage problem: An evaluation kit for graph-reduction semantics\n", "abstract": " We describe a software tool for specifying operational semantics as a term-graph reduction system. The semantics are guaranteed to model the asymptotic space and time usage of an implementation accurately yet are abstract enough to support reasoning at the program level.Term graphs make all the aspects of reduction relating to space usage explicit as they naturally encode size and address information. The semantics are constrained to be small-step, deterministic rules and each rewrite step uses bounded resources. The result is a system suitable for describing and analysing the space behaviour of functional programming languages.", "num_citations": "6\n", "authors": ["1894"]}
{"title": "An interactive approach to profiling parallel functional programs\n", "abstract": " The full details of a parallel computation can be very complex. To understand and improve performance one useful resource is a log-file recording all the computational events that change the number or status of parallel tasks. Raw log-files are not easy reading for the programmer, so profiling tools present only graphical summary charts. These charts sometimes show just what the programmer needs to know, but they often become too crowded, or fail to show enough, or both. Moreover, the charts are static so the programmer has little control over what is displayed. In this paper we discuss a tool that combines the advantages of graphical representation of computations with a query interface. The programmer interactively extracts specific information about the computation. Results of queries can be displayed in a graphical form; and parameters of subsequent queries can be specified by pointing within a past\u00a0\u2026", "num_citations": "6\n", "authors": ["1894"]}
{"title": "LZW text compression in Haskell\n", "abstract": " Functional programming is largely untested in the industrial environment. This paper summarises the results of a study into the suitability of Haskell in the area of text compression, an area with definite commercial interest. Our program initially performs very poorly in comparison with a version written in C. Experiments reveal the cause of this to be the large disparity in the relative speed of I/O and bit-level operations and also a space leak inherent in the Haskell definition.", "num_citations": "6\n", "authors": ["1894"]}
{"title": "From abstract models to functional prototypes\n", "abstract": " From abstract models to functional prototypes | Formal methods in human-computer interaction ACM Digital Library home ACM home Google, Inc. (search) Advanced Search Browse About Sign in Register Advanced Search Journals Magazines Proceedings Books SIGs Conferences People More Search ACM Digital Library SearchSearch Advanced Search Browse Browse Digital Library Collections More HomeBrowse by TitleBooksFormal methods in human-computer interactionFrom abstract models to functional prototypes chapter From abstract models to functional prototypes Share on Author: Colin Runciman View Profile Authors Info & Affiliations Publication: Formal methods in human-computer interactionApril 1990 Pages 201\u2013232 weeks0 Get Citation Alerts New Citation Alert added! This alert has been successfully added \u2026", "num_citations": "6\n", "authors": ["1894"]}
{"title": "Relative specification and transformational re-use of functional programs\n", "abstract": " Arelative specification is a collection of laws relating the behaviour of a required new program to that of one or more existing programs. A two stage method for transforming such relative specifications into effective functional programs is described and illustrated. Theinversion stage re-arranges the specifying laws to obtain a collection ofpartial definitions for each unknown function, typically involving non-deterministic operators. The subsequentfusion stage combines each set of partial definitions into a single complete definition, thereby eliminating non-deterministic operators.", "num_citations": "6\n", "authors": ["1894"]}
{"title": "Modula and a vision laboratory\n", "abstract": " A short description of Modula, the high-level language for real-time parallel programming, concentrates on its distinctive features as compared with Pascal; in particular the processs, signal and three types of module are considered.VRW, a vision laboratory control program written in Modula is introduced. Its complete module and process structure is presented in support of the argument that Modula allows a most attractive program architecture which matches that of the laboratory and -the experimental control problem. Detailed fragments of VRW are presented to illustrate the capabilities of Modula with special attention to device handling.Further benefits of the Modula discipline such as the inherent confidence possible in solutions and the merits of the module as a unit for software construction are discussed. In examining means of control over the use of machine-store, scalar types and, more particularly, the timing\u00a0\u2026", "num_citations": "6\n", "authors": ["1894"]}
{"title": "Multi-cultural visualization: how functional programming can enrich visualization (and vice versa)\n", "abstract": " The past two decades have seen visualization flourish as a research field in its own right, with advances on the computational challenges of faster algorithms, new techniques for datasets too large for in-core processing, and advances in understanding the perceptual and cognitive processes recruited by visualization systems, and through this, how to improve the representation of data. However, progress within visualization has sometimes proceeded in parallel with that in other branches of computer science, and there is a danger that when novel solutions ossify into `accepted practice' the field can easily overlook significant advances elsewhere in the community. In this paper we describe recent advances in the design and implementation of pure functional programming languages that, significantly, contain important insights into questions raised by the recent NIH/NSF report on Visualization Challenges. We argue and demonstrate that modern functional languages combine high-level mathematically-based specifications of visualization techniques, concise implementation of algorithms through fine-grained composition, support for writing correct programs through strong type checking, and a different kind of modularity inherent in the abstractive power of these languages. And to cap it off, we have initial evidence that in some cases functional implementations are faster than their imperative counterparts.", "num_citations": "5\n", "authors": ["1894"]}
{"title": "Automated benchmarking of functional data structures\n", "abstract": " Despite a lot of recent interest in purely functional data structures, for example [Ada93, Oka95, BO96, Oka96, OB97, Erw97], few have been benchmarked. Of these, even fewer have their performance qualified by how they are used. But how a data structure is used can significantly affect performance. This paper makes three original contributions. (1) We present an algorithm for generating a benchmark according to a given use of data structure. (2) We compare use of an automated tool based on this algorithm, with the traditional technique of hand-picked benchmarks, by benchmarking six implementations of random-access list using both methods. (3) We use the results of this benchmarking to present a decision tree for the choice of random-access list implementation, according to how the list will be used.", "num_citations": "5\n", "authors": ["1894"]}
{"title": "Auburn: A kit for benchmarking functional data structures\n", "abstract": " Benchmarking competing implementations of a data structure can be both tricky and time consuming. The efficiency of an implementation may depend critically on how it is used. This problem is compounded by persistence. All purely functional data structures are persistent. We present a kit that can generate benchmarks for a given data structure. A benchmark is made from a description of how it should use an implementation of the data structure. The kit will improve the speed, ease and power of the process of benchmarking functional data structures.", "num_citations": "5\n", "authors": ["1894"]}
{"title": "Lazy wheel sieves and spirals of primes\n", "abstract": " The popular method of enumerating the primes is the Sieve of Eratosthenes. It can be programmed very neatly in a lazy functional language, but runs rather slowly. A little-known alternative method is the Wheel Sieve, originally formulated as a fast imperative algorithm for obtaining all primes up to a given limit, assuming destructive access to a bit-array. This article describes functional variants of the wheel sieve that enumerate all primes as a lazy list.", "num_citations": "5\n", "authors": ["1894"]}
{"title": "TIP in Haskell\u2014another exercise in functional programming\n", "abstract": " Several years ago, Peyton Jones [3] tested some of the claims made for functional programming by re-implementing a well-known parser generator (YACC) in a lazy functional language (SASL) and comparing the result with the original imperative implementation. His conclusions were positive so far as the expressive power of functional programming was concerned \u2014 laziness and higher-order functions both proved valuable \u2014 but he bemoaned SASL\u2019s lack of type-checking, abstract data types and modules, and also the difficulties of correcting errors and optimising performance in the presence of lazy evaluation.", "num_citations": "5\n", "authors": ["1894"]}
{"title": "Extrapolate: generalizing counterexamples of functional test properties\n", "abstract": " This paper presents a new tool called Extrapolate that automatically generalizes counterexamples found by property-based testing in Haskell. Example applications show that generalized counterexamples can inform the programmer more fully and more immediately what characterises failures. Extrapolate is able to produce more general results than similar tools. Although it is intrinsically unsound, as reported generalizations are based on testing, it works well for examples drawn from previous published work in this area.", "num_citations": "4\n", "authors": ["1894"]}
{"title": "Faster production of redex trails: The Hat G-Machine\n", "abstract": " The Hat system provides a method for tracing a lazy functional program. However Hat works by transforming the source program into a much larger self-tracing variant that runs between 15 and 100 times slower and uses several times as much memory. We show how equivalent traces can be generated much more efficiently by modifying the underlying abstract machine. Our approach shows that it is only necessary to add a few extra machine instructions and change the interpretation of a few others in order to generate Hat traces efficiently.", "num_citations": "4\n", "authors": ["1894"]}
{"title": "Automated generalisation of function definitions\n", "abstract": " We address the problem of finding the common generalisation of a set of Haskell function definitions so that each function can be defined by partial application of the generalisation. By analogy with unification, which derives the most general common specialisation of two terms, we aim to infer the least general common generalisation. This problem has a unique solution in a first-order setting, but not in a higher-order language. We define a smallest minimal common generalisation which is unique and consider how it might be used for automated program improvement. The same function can have many definitions; we risk over-generalisation if equality is not recognised. A normalising rewrite system is used before generalisation, so many equivalent definitions become identical. The generalisation system we describe has been implemented in Haskell.", "num_citations": "4\n", "authors": ["1894"]}
{"title": "Cheap remarks about concurrent programs\n", "abstract": " We present CoCo, the Concurrency Commentator, a tool that recovers a declarative view of concurrent Haskell functions operating on some shared state. This declarative view is presented as a collection of automatically discovered properties. These properties are about refinement and equivalence of effects, rather than equality of final results. The tool is based on testing in a dynamically pruned search-space, rather than static analysis or theorem proving. Case studies about concurrent stacks and semaphores demonstrate how use of CoCo can inform understanding of program behaviour.", "num_citations": "3\n", "authors": ["1894"]}
{"title": "Weaving Parallel Threads\n", "abstract": " As the speed of processors is starting to plateau, chip manufacturers are instead looking to multi-core architectures for increased performance. The ubiquity of multi-core hardware has made parallelism an important tool in writing performant programs. Unfortunately, parallel programming is still considered an advanced technique and most programs are written as sequential programs.                 We propose that we lift this burden from the programmer and allow the compiler to automatically determine which parts of a program can be executed in parallel. Historically, most attempts at auto-parallelism depended on static analysis alone. While static analysis is often able to find safe parallelism, it is difficult to determine worthwhile parallelism. This is known as the granularity problem. Our work shows that we can use static analysis in conjunction with search techniques by having the compiler execute the\u00a0\u2026", "num_citations": "3\n", "authors": ["1894"]}
{"title": "Experience report: visualizing data through functional pipelines\n", "abstract": " Scientific visualization is the transformation of data into images. The pipeline model is a widely-used implementation strategy. This term refers not only to linear chains of processing stages, but more generally to demand-driven networks of components. Apparent parallels with functional programming are more than superficial: e.g. some pipelines support streams of data, and a limited form of lazy evaluation. Yet almost all visualization systems are implemented in imperative languages. We challenge this position. Using Haskell, we have reconstructed several fundamental visualization techniques, with encouraging results both in terms of novel insight and performance. In this paper we set the context for our modest rebellion, report some of our results, and reflect on the lessons that we have learned.", "num_citations": "3\n", "authors": ["1894"]}
{"title": "Using trace data to diagnose non-termination errors\n", "abstract": " This paper discusses black-hatand hat-nonterm, two tools for locating and diagnosing non-termination errors in Haskell programs. Both of these tools give a small trace which is intended to illuminate the cause of the non-termination error. black-hat analyses programs which contain black holes, a particularly restricted kind of non-termination error, while hatnonterm applies the approach used in black-hat to more general non-terminating programs. This paper discusses the traces generated by black-hat and hat-nonterm, as well as the approach used to generate these traces.", "num_citations": "3\n", "authors": ["1894"]}
{"title": "Astro-gofer: Parallel functional programming with co-ordinating processes\n", "abstract": " This paper investigates the addition of operations for explicit parallelism to the lazy functional language, Gofer. For this purpose, we choose to use a co-ordination language, derived from Linda, based on logically shared associative memories, called spaces.             We describe the use of Gofer type classes in building an interface to spaces, which highlights and extends the traditional associative access mechanism. We then show how processes can be added to Gofer, and how we can combine spaces and processes to produce a parallel functional language, which we have called Astro-Gofer.", "num_citations": "3\n", "authors": ["1894"]}
{"title": "Scarcely variabled programming and Pascal\n", "abstract": " Commonly, the variable is among the very first concepts taught to novice Pasca l [7] programmers. This is undesirable and, it will be argued, unnecessary.Work of, for example, Bauer [1] and Griffiths [3] on program development considers the early expression of program solutions as variableless schemes, an d the application of formal transforms to these schemes, introducing variables in a controlled manner to obtain improved equivalent solutions. Typically, their early solutions are built of recursive functions and their transforms eliminate explicit recursion in favour of iterative constructs.", "num_citations": "3\n", "authors": ["1894"]}
{"title": "Expressible sharing for functional circuit description\n", "abstract": " We consider the design of a circuit description library for a pure functional language where circuits are defined as functions and connected together by writing applicative expressions. Just like the return value of any other function in a functional program, the output of a circuit can be bound to a variable and referred to many times. Usually it is the programmer\u2019s intention that such references represent sharing of a circuit\u2019s output, implying a fanout structure. However, in a pure functional language, references are transparent and the circuit description library can only view finite graph-shaped circuit structures as infinitely expanded regular trees. To overcome this problem, we introduce expressible sharing, a technique in which the programmer expresses the fork-points present in a circuit in the same way that they express other circuit components like and-gates and or-gates. We define a library for circuit\u00a0\u2026", "num_citations": "2\n", "authors": ["1894"]}
{"title": "The accepting power of unary string logic programs\n", "abstract": " The set of programs written in a small subset of pure Prolog called US is shown to accept exactly the class of regular languages. The language US contains only unary predicates and unary function symbols. Also, a subset of US called RUS is shown to be equivalent to US in its ability in accepting the class of regular languages. Every clause in RUS contains at most one function symbol in the head and at most one literal with no function symbol in the body. The result is very close to a theorem of Matos (TCS April 1997) but our proof is quite different. Though US and RUS have the same accepting power, their conciseness of expression is dramatically different: if we try to write an RUS program equivalent to a US program, the number of predicates in the RUS program could be O (2 2 N) where N is the sum of the number of predicates and the number of functors in the US program.", "num_citations": "2\n", "authors": ["1894"]}
{"title": "Performance Monitoring\n", "abstract": " The full details of a parallel computation can be very complex. For example, many run-time systems employ a number of strategies to ensure best use of available hardware. Most parallel functional languages hide this information from the programmer, providing very simple annotations to control the parallelism in a program [476], [288], [573], [104]. Some, such as Hudak\u2019s para-functional programming [288], allow the programmer to describe the mapping of tasks onto processors [404], [432], [331], [560], whilst the more recent GUM system [573] only requires the programmer to annotate possible expressions that can be evaluated in parallel.", "num_citations": "2\n", "authors": ["1894"]}
{"title": "A virtual terminal\n", "abstract": " A virtual terminal | Applications of functional programming ACM Digital Library home ACM home Google, Inc. (search) Advanced Search Browse About Sign in Register Advanced Search Journals Magazines Proceedings Books SIGs Conferences People More Search ACM Digital Library SearchSearch Advanced Search Browse Browse Digital Library Collections More HomeBrowse by TitleBooksApplications of functional programmingA virtual terminal chapter A virtual terminal Share on Author: Colin Runciman View Profile Authors Info & Affiliations Publication: Applications of functional programmingFebruary 1996 Pages 60\u201373 0citation 0 Downloads Metrics Total Citations0 Total Downloads0 Last 12 Months0 Last 6 weeks0 Get Citation Alerts New Citation Alert added! This alert has been successfully added and will be sent to: You will be notified whenever a record that you have chosen has been cited. To manage \u2026", "num_citations": "2\n", "authors": ["1894"]}
{"title": "Formalised development of software by machine assisted transformation\n", "abstract": " 1. Introduction: the Transformational Approach We advocate a transformational approach to the formalised development of software, starting from exact specifications of required programs. The earlier process of capturing specifications is a demanding one, so methods used at that stage of formalisation are important: but they are not our concern.We also assume an effective or constructive specification, though this may be the result of recasting a prior non-constructive specification. Again, we recognise that the process of re-casting is important: it is not our main concern at present, though we have done some work on this problem. 6 So our specifications are, in fact, initial prototypes. They may be extremely expensive to run, because in composing them clarity and simplicity of expression have been put before all other concerns. Even so, they might be evaluated against requirements other than efficiency. They might\u00a0\u2026", "num_citations": "2\n", "authors": ["1894"]}
{"title": "Systematic testing for distributed systems\n", "abstract": " We propose a method of applying systematic concurrency testing (SCT) to distributed systems. SCT enables us to verify assertions for many possible executions of a concurrent program, varying according to scheduling decisions. Distributed systems are similar to concurrent programs in the message-passing style, but messages sent over the network may be lost, re-ordered, or duplicated. Prior approaches avoid these problems by assuming that communications are reliable. Our approach does not have this limitation, and allows a variety of network failure cases to be tested.", "num_citations": "1\n", "authors": ["1894"]}
{"title": "Weaving parallel threads\n", "abstract": " As the speed of processors is starting to plateau, chip manufacturers are instead looking to multi-core architectures for increased performance. The ubiquity of multi-core hardware has made parallelism an important tool in writing performant programs. Unfortunately, parallel programming is still considered an advanced technique and most programs are written as sequential programs. We propose that we lift this burden from the programmer and allow the compiler to automatically determine which parts of a program can be executed in parallel. Historically, most attempts at auto-parallelism depended on static analysis alone. While static analysis is often able to find safe parallelism, it is difficult to determine worthwhile parallelism. This is known as the granularity problem. Our work shows that we can use static analysis in conjunction with search techniques by having the compiler execute the program and then alter the amount of parallelism based on execution speed. We do this by annotating the program with parallel annotations and using search to determine which annotations to enable.", "num_citations": "1\n", "authors": ["1894"]}
{"title": "Mathematical foundations for generic surfacing\n", "abstract": " \u2018Marching Cubes\u2019 is but the most well known member of a large family of algorithms that approximate n-dimensional surfaces through substitope approximations with a range of cell geometries. This poster reports on work in which a novel mathematical view of cells is being used to tease out the common basis of these algorithms and capture this as a single instance of a generic programming scheme, from which any member of this family can then be instantiated automatically and on demand. By relating cells and vertex colouring to symmetric and permutation groups, work by Hege [4] and Banks et al.[3] identified an intriguing link between visualization algorithms and results from algebraic combinatorics. We are building on this work in two ways. First, we utilise a combinatorial construction that provides a simple, expressive language for cell geometry and symmetries. Second, using advances in generic\u00a0\u2026", "num_citations": "1\n", "authors": ["1894"]}
{"title": "Exploring datatype usage space\n", "abstract": " Quantifying the use of a data structure makes benchmarking data structures easier and more reliable. We explore di erent ways of quantifying datatype usage. We present a basic solution and examine three extensions to this solution.", "num_citations": "1\n", "authors": ["1894"]}
{"title": "A quasi-parallel evaluator\n", "abstract": " A quasi-parallel evaluator | Applications of functional programming ACM Digital Library home ACM home Google, Inc. (search) Advanced Search Browse About Sign in Register Advanced Search Journals Magazines Proceedings Books SIGs Conferences People More Search ACM Digital Library SearchSearch Advanced Search Browse Browse Digital Library Collections More HomeBrowse by TitleBooksApplications of functional programmingA quasi-parallel evaluator chapter A quasi-parallel evaluator Share on Authors: Colin Runciman View Profile , David Wakeling View Profile Authors Info & Affiliations Publication: Applications of functional programmingFebruary 1996 Pages 161\u2013176 0citation 0 Downloads Metrics Total Citations0 Total Downloads0 Last 12 Months0 Last 6 weeks0 Get Citation Alerts New Citation Alert added! This alert has been successfully added and will be sent to: You will be notified \u2026", "num_citations": "1\n", "authors": ["1894"]}
{"title": "Functional Counterparts of some Logic Programming Techniques\n", "abstract": " Drawing on experience of translating a Prolog program into Haskell, a range of correspondences between logic and functional programs are discussed. Despite the differences of underlying paradigms, in many cases we can find close counterparts between the two programming schemes.", "num_citations": "1\n", "authors": ["1894"]}
{"title": "The display, browing and filtering of graph trees\n", "abstract": " We are writing an interpreter for a little lazy functional language. Implementation is by graph reduction 6]. The user is allowed to view, and explore, the program graph at every reduction step, or at less frequent intervals on request. The aim is to gain insight into the process of lazy graph reduction, where the order of reduction is not always intuitive. There are two main objectives: to explore the extent of sharing, and to be able to identify areas of ine ciency. The potential size and complexity of the graphs pose problems for display. This paper presents and discusses novel solutions to these problems.", "num_citations": "1\n", "authors": ["1894"]}
{"title": "Perfect hash functions made parallel-Lazy functional programming on a distributed multiprocessor\n", "abstract": " A programming technique for efficient parallel search is described. The authors study a search problem for which a heuristic preprocess makes sequential execution feasible. Two key questions are addressed. (1) How can this algorithm, optimized for sequential execution, be programmed in parallel to produce significant speedup? (2) how can this be done in a purely functional language without compromising either conciseness or referential transparency? The authors describe programming techniques for efficient parallel search in a lazy and pure functional language. These techniques are applied to an illustrative example. Results of execution on a real parallel machine are given.< >", "num_citations": "1\n", "authors": ["1894"]}
{"title": "Some software tools used in the development of the prototype York Ada compiler\n", "abstract": " Among the various software tools developed in the early stages of the York Ada compiler project are two families of tools described here. Syntax-driven generators, distinctive in their inclusion of a mechanism for syntax abstraction and their mode of adaptability, were used to produce a variety of compiler components; interface checkers, automatically generated from specifications of the information intended to pass between components, were used to help isolate and repair faulty components.", "num_citations": "1\n", "authors": ["1894"]}