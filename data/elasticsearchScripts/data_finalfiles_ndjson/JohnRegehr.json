{"title": "Finding and understanding bugs in C compilers\n", "abstract": " Compilers should be correct. To improve the quality of C compilers, we created Csmith, a randomized test-case generation tool, and spent three years using it to find compiler bugs. During this period we reported more than 325 previously unknown bugs to compiler developers. Every compiler we tested was found to crash and also to silently generate wrong code when presented with valid input. In this paper we present our compiler-testing tool and the results of our bug-hunting study. Our first contribution is to advance the state of the art in compiler testing. Unlike previous tools, Csmith generates programs that cover a large subset of C while avoiding the undefined and unspecified behaviors that would destroy its ability to automatically find wrong-code bugs. Our second contribution is a collection of qualitative and quantitative results about the bugs we have found in open-source C compilers.", "num_citations": "855\n", "authors": ["1047"]}
{"title": "Test-case reduction for C compiler bugs\n", "abstract": " To report a compiler bug, one must often find a small test case that triggers the bug. The existing approach to automated test-case reduction, delta debugging, works by removing substrings of the original input; the result is a concatenation of substrings that delta cannot remove. We have found this approach less than ideal for reducing C programs because it typically yields test cases that are too large or even invalid (relying on undefined behavior). To obtain small and valid test cases consistently, we designed and implemented three new, domain-specific test-case reducers. The best of these is based on a novel framework in which a generic fixpoint computation invokes modular transformations that perform reduction operations. This reducer produces outputs that are, on average, more than 25 times smaller than those produced by our other reducers or by the existing reducer that is most commonly used by\u00a0\u2026", "num_citations": "249\n", "authors": ["1047"]}
{"title": "Understanding Integer Overflow in C/C++\n", "abstract": " Integer overflow bugs in C and C++ programs are difficult to track down and may lead to fatal errors or exploitable vulnerabilities. Although a number of tools for finding these bugs exist, the situation is complicated because not all overflows are bugs. Better tools need to be constructed, but a thorough understanding of the issues behind these errors does not yet exist. We developed IOC, a dynamic checking tool for integer overflows, and used it to conduct the first detailed empirical study of the prevalence and patterns of occurrence of integer overflows in C and C++ code. Our results show that intentional uses of wraparound behaviors are more common than is widely believed; for example, there are over 200 distinct locations in the SPEC CINT2000 benchmarks where overflow occurs. Although many overflows are intentional, a large number of accidental overflows also occur. Orthogonal to programmers' intent\u00a0\u2026", "num_citations": "208\n", "authors": ["1047"]}
{"title": "HLS: A framework for composing soft real-time schedulers\n", "abstract": " Hierarchical CPU scheduling has emerged as a way to (1) support applications with diverse scheduling requirements in open systems, and (2) provide load isolation between applications, users, and other resource principals. Most existing work on hierarchical scheduling has focused on systems that provide a fixed scheduling model: the schedulers in part or all of the hierarchy are specified in advance. In this paper we describe a system of guarantees that permits a general hierarchy of soft real-time schedulers one that contains arbitrary scheduling algorithms at all points within the hierarchy - to be analyzed. This analysis results in deterministic guarantees for threads at the leaves of the hierarchy. We also describe the design, implementation, and performance evaluation of a system for supporting such a hierarchy in the Windows 2000 kernel. Finally, we show that complex scheduling behaviors can be created\u00a0\u2026", "num_citations": "203\n", "authors": ["1047"]}
{"title": "Efficient memory safety for TinyOS\n", "abstract": " Reliable sensor network software is difficult to create: applications are concurrent and distributed, hardware-based memory protection is unavailable, and severe resource constraints necessitate the use of unsafe, low-level languages. Our work improves this situation by providing efficient memory and type safety for TinyOS 2 applications running on the Mica2, MicaZ, and TelosB platforms. Safe execution ensures that array and pointer errors are caught before they can corrupt RAM. Our contributions include showing that aggressive optimizations can make safe execution practical in terms of resource usage; developing a technique for efficiently enforcing safety under interrupt-driven concurrency; extending the nesC language and compiler to support safety annotations; finding previously unknown bugs in TinyOS; and, finally, showing that safety can be exploited to increase the availability of sensor networks\u00a0\u2026", "num_citations": "161\n", "authors": ["1047"]}
{"title": "Scheduling tasks with mixed preemption relations for robustness to timing faults\n", "abstract": " This paper introduces and shows how to schedule two novel scheduling abstractions that overcome limitations of existing work on preemption threshold scheduling. The abstractions are task clusters, groups of tasks that are mutually non-preemptible by design, and task barriers, which partition the task set into subsets that must be mapped to different threads. Barriers prevent the preemption threshold logic that runs multiple design-time tasks in the same runtime thread from violating architectural constraints, e.g. by merging an interrupt handler and a user-level thread. We show that the preemption threshold logic for mapping tasks to as few threads as possible can rule out the schedules with the highest critical scaling factors - these schedules are the least likely to miss deadlines under timing faults. We have developed a framework for robust CPU scheduling and three novel algorithms: an optimal algorithm for\u00a0\u2026", "num_citations": "133\n", "authors": ["1047"]}
{"title": "Intent fuzzer: crafting intents of death\n", "abstract": " We present a fuzzing framework for Intents: the core IPC mechanism for intra-and inter-app communication in Android. Since intents lie at a trust boundary between apps, their correctness is important and thorough testing is warranted. The key challenge is to balance the tension between generating intents that applications expect, permitting deep penetration into application logic, and generating intents that trigger interesting bugs that have not been previously uncovered. Our work strikes this balance using a novel combination of static analysis and random test-case generation. Our intent fuzzer crashed dozens of Google core and top Google Play apps, resulting in app restarts or even in a complete OS reboot.", "num_citations": "130\n", "authors": ["1047"]}
{"title": "Provably correct peephole optimizations with alive\n", "abstract": " Compilers should not miscompile. Our work addresses problems in developing peephole optimizations that perform local rewriting to improve the efficiency of LLVM code. These optimizations are individually difficult to get right, particularly in the presence of undefined behavior; taken together they represent a persistent source of bugs. This paper presents Alive, a domain-specific language for writing optimizations and for automatically either proving them correct or else generating counterexamples. Furthermore, Alive can be automatically translated into C++ code that is suitable for inclusion in an LLVM optimization pass. Alive is based on an attempt to balance usability and formal methods; for example, it captures---but largely hides---the detailed semantics of three different kinds of undefined behavior in LLVM. We have translated more than 300 LLVM optimizations into Alive and, in the process, found that eight of\u00a0\u2026", "num_citations": "120\n", "authors": ["1047"]}
{"title": "T-Check: bug finding for sensor networks\n", "abstract": " Sensor nodes are resource poor and failure-prone. Sensor networks are composed of many such nodes that are often hard to physically reach and that are connected by unreliable wireless links. Together, these factors make sensor network debugging into a challenging activity, and in fact it is not uncommon for a deployed sensornet to encounter sporadic faults that are effectively impossible to locate, reproduce, and fix.", "num_citations": "119\n", "authors": ["1047"]}
{"title": "Random testing of interrupt-driven software\n", "abstract": " Interrupt-driven embedded software is hard to thoroughly test since it usually contains a very large number of executable paths. Developers can test more of these paths using random interrupt testing---firing random interrupt handlers at random times. Unfortunately, na\u00efve application of random testing to interrupt-driven software does not work: some randomly generated interrupt schedules violate system semantics, causing spurious failures. The contribution of this paper is the design, implementation, and experimental evaluation of RID, a restricted interrupt discipline that hardens embedded software with respect to unexpected interrupts, making it possible to perform random interrupt testing and also protecting it from spurious interrupts after deployment. We evaluate RID by implementing it in TinyOS and then using random interrupt testing to find bugs and also to drive applications toward their worst-case stack depths.", "num_citations": "116\n", "authors": ["1047"]}
{"title": "Deriving abstract transfer functions for analyzing embedded software\n", "abstract": " This paper addresses the problem of creating abstract transfer functions supporting dataflow analyses. Writing these functions by hand is problematic: transfer functions are difficult to understand, difficult to make precise, and difficult to debug. Bugs in transfer functions are particularly serious since they defeat the soundness of any program analysis running on top of them. Furthermore, implementing transfer functions by hand is wasteful because the resulting code is often difficult to reuse in new analyzers and to analyze new languages. We have developed algorithms and tools for deriving transfer functions for the bitwise and unsigned interval abstract domains. The interval domain is standard; in the bitwise domain, values are vectors of three-valued bits. For both domains, important challenges are to derive transfer functions that are sound in the presence of integer overflow, and to derive precise transfer functions\u00a0\u2026", "num_citations": "112\n", "authors": ["1047"]}
{"title": "Eliminating stack overflow by abstract interpretation\n", "abstract": " An important correctness criterion for software running on embedded microcontrollers is stack safety: a guarantee that the call stack does not overflow. Our first contribution is a method for statically guaranteeing stack safety of interrupt-driven embedded software using an approach based on context-sensitive dataflow analysis of object code. We have implemented a prototype stack analysis tool that targets software for Atmel AVR microcontrollers and tested it on embedded applications compiled from up to 30,000 lines of C. We experimentally validate the accuracy of the tool, which runs in under 10 sec on the largest programs that we tested. The second contribution of this paper is the development of two novel ways to reduce stack memory requirements of embedded software.", "num_citations": "109\n", "authors": ["1047"]}
{"title": "Swarm Testing\n", "abstract": " Swarm testing is a novel and inexpensive way to improve the diversity of test cases generated during random testing. Increased diversity leads to improved coverage and fault detection. In swarm testing, the usual practice of potentially including all features in every test case is abandoned. Rather, a large \u201cswarm\u201d of randomly generated configurations, each of which omits some features, is used, with configurations receiving equal resources. We have identified two mechanisms by which feature omission leads to better exploration of a system\u2019s state space. First, some features actively prevent the system from executing interesting behaviors; eg,\u201cpop\u201d calls may prevent a stack data structure from executing a bug in its overflow detection logic. Second, even when there is no active suppression of behaviors, test features compete for space in each test, limiting the depth to which logic driven by features can be explored\u00a0\u2026", "num_citations": "105\n", "authors": ["1047"]}
{"title": "Volatiles are miscompiled, and what to do about it\n", "abstract": " C's volatile qualifier is intended to provide a reliable link between operations at the source-code level and operations at the memory-system level. We tested thirteen production-quality C compilers and, for each, found situations in which the compiler generated incorrect code for accessing volatile variables. This result is disturbing because it implies that embedded software and operating systems---both typically coded in C, both being bases for many mission-critical and safety-critical applications, and both relying on the correct translation of volatiles---may be being miscompiled.", "num_citations": "98\n", "authors": ["1047"]}
{"title": "Preventing interrupt overload\n", "abstract": " Performance guarantees can be given to tasks in an embedded system by ensuring that access to each shared resource is mediated by an appropriate scheduler. However, almost all previous work on CPU scheduling has focused on thread-level scheduling, resulting in systems that are vulnerable to a lower-level form of overload that occurs when too many interrupts arrive. This paper describes three new techniques, two software-based and one hardware-based, for creating systems that delay or drop excessive interrupt requests before they can overload a processor. Our interrupt schedulers bound both the amount of work performed in interrupt context and its granularity, making it possible to provide strong progress guarantees to thread-level processing. We show that our solutions work and are efficient when implemented on embedded processors. We have also taken a description for a microprocessor in VHDL, modified it to include logic that prevents interrupt overload, synthesized the processor, and verified that it works using simulation. By allowing developers to avoid making assumptions about the worst-case interrupt rates of peripherals, our work fills an important gap in the chain of reasoning leading to a validated system. These techniques cannot replace careful system design, but they do provide a lastditch safety guarantee in the presence of a serious malfunction.", "num_citations": "95\n", "authors": ["1047"]}
{"title": "Evolving real-time systems using hierarchical scheduling and concurrency analysis\n", "abstract": " We have developed a new way to look at real-time and embedded software: as a collection of execution environments created by a hierarchy of schedulers. Common schedulers include those than run interrupts, bottom-half handlers, threads, and events. We have created algorithms for deriving response times, scheduling overheads, and blocking terms for tasks in systems containing multiple execution environments. We have also created task scheduler logic, a formalism that permits checking systems for race conditions and other errors. Concurrency analysis of low-level software is challenging because there are typically several kinds of locks, such as thread mutexes and disabling interrupts, and groups of cooperating tasks may need to acquire some, all or none of the available types of locks to create correct software. Our high-level goal is to create systems that are evolvable: they are easier to modify in\u00a0\u2026", "num_citations": "84\n", "authors": ["1047"]}
{"title": "ARMor: fully verified software fault isolation\n", "abstract": " We have designed and implemented ARMor, a system that uses software fault isolation (SFI) to sandbox application code running on small embedded processors. Sandboxing can be used to protect components such as the RTOS and critical control loops from other, less-trusted components. ARMor guarantees memory safety and control flow integrity; it works by rewriting a binary to put a check in front of every potentially dangerous operation. We formally and automatically verify that an ARMored application respects the SFI safety properties using the HOL theorem prover. Thus, ARMor provides strong isolation guarantees and has an exceptionally small trusted computing base-there is no trusted compiler, binary rewriter, verifier, or operating system.", "num_citations": "77\n", "authors": ["1047"]}
{"title": "Surviving sensor network software faults\n", "abstract": " We describe Neutron, a version of the TinyOS operating system that efficiently recovers from memory safety bugs. Where existing schemes reboot an entire node on an error, Neutron's compiler and runtime extensions divide programs into recovery units and reboot only the faulting unit. The TinyOS kernel itself is a recovery unit: a kernel safety violation appears to applications as the processor being unavailable for 10-20 milliseconds.", "num_citations": "74\n", "authors": ["1047"]}
{"title": "Testing static analyzers with randomly generated programs\n", "abstract": " Static analyzers should be correct. We used the random C-program generator Csmith, initially intended to test C compilers, to test parts of the Frama-C static analysis platform. Although Frama-C was already relatively mature at that point, fifty bugs were found and fixed during the process, in the front-end (AST elaboration and type-checking) and in the value analysis, constant propagation and slicing plug-ins. Several bugs were also found in Csmith, even though it had been extensively tested and had been used to find numerous bugs in compilers.", "num_citations": "69\n", "authors": ["1047"]}
{"title": "Interface contracts for TinyOS\n", "abstract": " TinyOS applications are built with software components that communicate through narrow interfaces. Since components enable finegrained code reuse, this approach has been successful in creating applications that make very efficient use of the limited code and data memory on sensor network nodes. However, the other important benefit of components-rapid application development through black-box reuse-remains largely unrealized because in many cases interfaces have implied usage constraints that can be the source of frustrating program errors. Developers are commonly forced to read the source code for components, partially defeating the purpose of using components in the first place. Our research helps solve these problems by allowing developers to explicitly specify and enforce component interface contracts. Due to the extensive reuse of the most common interfaces, implementing contracts for a\u00a0\u2026", "num_citations": "69\n", "authors": ["1047"]}
{"title": "Using hierarchical scheduling to support soft real-time applications in general-purpose operating systems\n", "abstract": " The thesis of this dissertation is that extending a general-purpose operating system with a general, heterogeneous scheduling hierarchy is feasible and useful. A hierarchy of schedulers generalizes the role of CPU schedulers by allowing them to schedule other schedulers in addition to scheduling threads. A general, heterogeneous scheduling hierarchy is one that allows arbitrary (or nearly arbitrary) scheduling algorithms throughout the hierarchy. In contrast, most of the previous work on hierarchical scheduling has imposed restrictions on the schedulers used in part or all of the hierarchy.", "num_citations": "69\n", "authors": ["1047"]}
{"title": "Interrupt Verification via Thread Verification\n", "abstract": " Most of the research effort towards verification of concurrent software has focused on multithreaded code. On the other hand, concurrency in low-end embedded systems is predominantly based on interrupts. Low-end embedded systems are ubiquitous in safety-critical applications such as those supporting transportation and medical automation; their verification is important. Although interrupts are superficially similar to threads, there are subtle semantic differences between the two abstractions. This paper compares and contrasts threads and interrupts from the point of view of verifying the absence of race conditions. We identify a small set of extensions that permit thread verification tools to also verify interrupt-driven software, and we present examples of source-to-source transformations that turn interrupt-driven code into semantically equivalent thread-based code that can be checked by a thread verifier. Finally\u00a0\u2026", "num_citations": "67\n", "authors": ["1047"]}
{"title": "Dynamic CPU management for real-time, middleware-based systems\n", "abstract": " Many real-world distributed, real-time, embedded (ORE) systems, such as multiagent military applications, are built using commercially available operating systems, middleware, and collections of pre-existing software. The complexity of these systems makes it difficult to ensure that they maintain high quality of service (QOS). At design time, the challenge is to introduce coordinated QOS controls into multiple software elements in a non-invasive manner. At run time, the system must adapt dynamically to maintain high QOS in the face of both expected events, such as application mode changes, and unexpected events, such as resource demands from other applications. We describe the design and implementation of a CPU broker for these types of ORE systems. The CPU broker mediates between multiple real-time tasks and the facilities of a real-time operating system: using feedback and other inputs, it adjusts\u00a0\u2026", "num_citations": "61\n", "authors": ["1047"]}
{"title": "Inferring scheduling behavior with hourglass\n", "abstract": " Although computer programs explicitly represent data values, time values are usually implicit. This makes it difficult to analyze and debug real-time programs whose correctness depends partially on the time at which results are computed. This paper shows how to use Hourglass, an instrumented, synthetic real-time application, to make inferences about what is happening on a computer at millisecond and microsecond granularities. These inferences are possible because Hourglass records a very fine-grained map of when each of its threads runs, and because Hourglass supports a variety of thread execution models that model the properties and requirements of non-synthetic real-time applications. We conclude that between measurements and inferences, surprisingly detailed knowledge about scheduling behavior can be obtained without modifying, or even explicitly interacting with, the operating system kernel.", "num_citations": "61\n", "authors": ["1047"]}
{"title": "Augmented CPU reservations: Towards predictable execution on general-purpose operating systems\n", "abstract": " One problem with performing soft real-time computations on general-purpose operating systems is that these OSs may spend significant amounts of time in the kernel instead of performing work on behalf of the application that is nominally scheduled: the OS effectively steals time from the running application. Stolen time can be a significant obstacle to predictable program execution on real-time versions of Linux and Windows 2000, where it can cause applications to miss essentially all of their deadlines. We propose augmented CPU reservations, a novel mechanism for using fine-grained accounting information about the amount of stolen time to help the scheduler allow applications to meet their deadlines. We have designed and implemented Rez-C and Rez-FB, two schedulers that provide augmented reservations, and we have tested them in Windows 2000, showing that they can increase the predictability of\u00a0\u2026", "num_citations": "58\n", "authors": ["1047"]}
{"title": "Eliminating stack overflow by abstract interpretation\n", "abstract": " An important correctness criterion for software running on embedded microcontrollers is stack safety: a guarantee that the call stack does not overflow. We address two aspects of the problem of creating stack-safe embedded software that also makes efficient use of memory: statically bounding worst-case stack depth, and automatically reducing stack memory requirements. Our first contribution is a method for statically guaranteeing stack safety by performing whole-program analysis, using an approach based on context-sensitive abstract interpretation of machine code. Abstract interpretation permits our analysis to accurately model when interrupts are enabled and disabled, which is essential for accurately bounding the stack depth of typical embedded systems. We have implemented a stack analysis tool that targets Atmel AVR microcontrollers, and tested it on embedded applications compiled from up to 30\u00a0\u2026", "num_citations": "57\n", "authors": ["1047"]}
{"title": "HOIST: A system for automatically deriving static analyzers for embedded systems\n", "abstract": " Embedded software must meet conflicting requirements such as be-ing highly reliable, running on resource-constrained platforms, and being developed rapidly. Static program analysis can help meet all of these goals. People developing analyzers for embedded object code face a difficult problem: writing an abstract version of each instruction in the target architecture(s). This is currently done by hand, resulting in abstract operations that are both buggy and im-precise. We have developed Hoist: a novel system that solves these problems by automatically constructing abstract operations using a microprocessor (or simulator) as its own specification. With almost no input from a human, Hoist generates a collection of C func-tions that are ready to be linked into an abstract interpreter. We demonstrate that Hoist generates abstract operations that are cor-rect, having been extensively tested, sufficiently fast, and substan\u00a0\u2026", "num_citations": "55\n", "authors": ["1047"]}
{"title": "Cause reduction for quick testing\n", "abstract": " In random testing, it is often desirable to produce a \"quick test\" -- an extremely inexpensive test suite that can serve as a frequently applied regression and allow the benefits of random testing to be obtained even in very slow or over-subscribed test environments. Delta debugging is an algorithm that, given a failing test case, produces a smaller test case that also fails, and typically executes much more quickly. Delta debugging of random tests can produce effective regression suites for previously detected faults, but such suites often have little power for detecting new faults, and in some cases provide poor code coverage. This paper proposes extending delta debugging by simplifying tests with respect to code coverage, an instance of a generalization of delta debugging we call cause reduction. We show that test suites reduced in this fashion can provide very effective quick tests for real-world programs. For Mozilla's\u00a0\u2026", "num_citations": "49\n", "authors": ["1047"]}
{"title": "The problems you're having may not be the problems you think you're having: Results from a latency study of Windows NT\n", "abstract": " This paper is intended to catalyze discussions on two intertwined systems topics. First, it presents early results from a latency study of Windows NT that identifies some specific causes of long thread scheduling latencies, many of which delay the dispatching of runnable threads for tens of milliseconds. Reasons for these delays, including technical, methodological, and economic are presented and possible solutions are discussed. Secondly, and equally importantly, it is intended to serve as a cautionary tale against believing one's own intuition about the causes of poor system performance. We went into this study believing we understood a number of the causes of these delays, with our beliefs informed more by conventional wisdom and hunches than data. In nearly all cases the reasons we discovered via instrumentation and measurement surprised us. In fact, some directly contradicted \"facts\" we thought we \"knew\".", "num_citations": "46\n", "authors": ["1047"]}
{"title": "Precise garbage collection for C\n", "abstract": " Magpie is a source-to-source transformation for C programs that enables precise garbage collection, where precise means that integers are not confused with pointers, and the liveness of a pointer is apparent at the source level. Precise GC is primarily useful for long-running programs and programs that interact with untrusted components. In particular, we have successfully deployed precise GC in the C implementation of a language run-time system that was originally designed to use conservative GC. We also report on our experience in transforming parts of the Linux kernel to use precise GC instead of manual memory management.", "num_citations": "45\n", "authors": ["1047"]}
{"title": "Cause reduction: delta debugging, even without bugs\n", "abstract": " What is a test case for? Sometimes, to expose a fault. Tests can also exercise code, use memory or time, or produce desired output. Given a desired effect, a test case can be seen as a cause, and its components divided into essential (required for effect) and accidental. Delta debugging is used for removing accidents from failing test cases, producing smaller test cases that are easier to understand. This paper extends delta debugging by simplifying test cases with respect to arbitrary effects, a generalization called cause reduction. Suites produced by cause reduction provide effective quick tests for real\u2010world programs. For Mozilla's JavaScript engine, the reduced suite is possibly more effective for finding faults. The effectiveness of reduction\u2010based suites persists through changes to the software, improving coverage by over 500 branches for versions up to 4\u2009months later. Cause reduction has other applications\u00a0\u2026", "num_citations": "42\n", "authors": ["1047"]}
{"title": "Providing predictable scheduling of programs using repeating precomputed schedules on discretely scheduled and/or multiprocessor operating systems\n", "abstract": " The present invention provides providing predictable scheduling of programs using repeating precomputed schedules on discretely scheduled and/or multiprocessor operating systems. In one embodiment, a scheduler accesses an activity scheduling graph. The activity scheduling graph is comprised of nodes each representing a recurring execution interval, and has one root, one or more leaves, and at least one path from the root to each leaf. Each node is on at least one path from the root to a leaf, and the number of times the execution interval represented by each node occurs during the traversal of the graph is equal to the number of paths from the root to a leaf that the node is on. Each node has associated with it an execution interval length, and is adapted to being dedicated to executing the threads of a single activity. There may be one scheduling graph for each processor, or a scheduling graph may traverse\u00a0\u2026", "num_citations": "41\n", "authors": ["1047"]}
{"title": "Edicts: implementing features with flexible binding times\n", "abstract": " In a software product line, the binding time of a feature is the time at which one decides to include or exclude a feature from a product. Typical binding site implementations are intended to support a single binding time only, eg, compile time or run time. Sometimes, however, a product line must support features with variable binding times. For instance, a product line may need to include both embedded system configurations, in which features are selected and optimized early, and desktop configurations, in which client programs choose features on demand.", "num_citations": "38\n", "authors": ["1047"]}
{"title": "Static and dynamic structure in design patterns\n", "abstract": " Design patterns are a valuable mechanism for emphasizing structure, capturing design expertise, and facilitating restructuring of software systems. Patterns are typically applied in the context of an object-oriented language and are implemented so that the pattern participants correspond to object instances that are created and connected at run-time. This paper describes a complementary realization of design patterns, in which many pattern participants correspond to statically instantiated and connected components. Our approach separates the static parts of the software design from the dynamic parts of the system behavior. This separation makes the software design more amenable to analysis, thus enabling more effective and domain-specific detection of system design errors, prediction of run-time behavior, and more effective optimization. This technique is applicable to imperative, functional, and object-oriented\u00a0\u2026", "num_citations": "35\n", "authors": ["1047"]}
{"title": "Operating system support for multimedia: The programming model matters\n", "abstract": " Multimedia is an increasingly important part of the mix of applications that users run on personal computers and workstations. The requirements placed on a multimedia operating system are demanding and often conflicting: untrusted, independently written soft real-time applications must be able to coexist without interfering with each other. This must be accomplished while requiring as little extra effort as possible from application developers, and the resulting system must be usable and understandable by end users even when application resource requirements exceed system capacity. This article analyzes the goals of multimedia schedulers and provides a taxonomy of techniques used to achieve them; representative schedulers are classified and characterized in terms of the things that they make easy and hard, including the associated programming tasks. This is done to support our principal contribution: an analysis of usability issues and tradeoffs in multimedia scheduling for both application developers and end users.", "num_citations": "28\n", "authors": ["1047"]}
{"title": "Safe and structured use of interrupts in real-time and embedded software\n", "abstract": " While developing embedded and real-time systems, it is usually necessary to write code that handles interrupts, or code the interacts with interrupt handlers. Interrupts are indispensable because they use hardware support to reduce both the latency and overhead of event detection, when compared to polling. Furthermore, modern embedded processor cores can save energy by shutting down when idle, leaving only simple circuitry such as timers running. For example, a TelosB [13] sensor network node drains a pair of AA batteries in a few days by running the processor continuously, but can run a useful application for months if the duty cycle is kept appropriately low.Interrupts have some inherent drawbacks from a software engineering point of view. First, they are relatively non-portable across compilers and hardware platforms. Second, they lend themselves to a variety of severe software errors that are difficult to track down since they manifest only rarely. These problems give interrupts a bad reputation for leading to flaky software: a significant problem where the software is part of a highly-available or safety-critical system.", "num_citations": "27\n", "authors": ["1047"]}
{"title": "Pluggable abstract domains for analyzing embedded software\n", "abstract": " Many abstract value domains such as intervals, bitwise, constants, and value-sets have been developed to support dataflow analysis. Different domains offer alternative tradeoffs between analysis speed and precision. Furthermore, some domains are a better match for certain kinds of code than others. This paper presents the design and implementation of cXprop, an analysis and transformation tool for C that implements\" conditional X propagation,\" a generalization of the well-known conditional constant propagation algorithm where X is an abstract value domain supplied by the user. cXprop is interprocedural, context-insensitive, and achieves reasonable precision on pointer-rich codes. We have applied cXprop to sensor network programs running on TinyOS, in order to reduce code size through interprocedural dead code elimination, and to find limited-bitwidth global variables. Our analysis of global variables is\u00a0\u2026", "num_citations": "27\n", "authors": ["1047"]}
{"title": "Issues in using commodity operating systems for time-dependent tasks: experiences from a study of Windows NT\n", "abstract": " This paper presents a snapshot of early results from a study of Windows NT aimed at understanding and improving its limitations when used for time-dependent tasks, such as those that arise for audio and video processing.Clearly there are time scales for which it can achieve effectively perfect reliability, such as the onesecond deadlines present in the Tiger Video Filesystem. Other time scales, such as reliable sub-millisecond scheduling of periodic tasks in user space, are clearly out of reach. Yet, there is an interesting middle ground between these time scales in which deadlines may be met, but will not always be. This study focuses on system and application behaviors in this region with the short-term goals of understanding and improving the real-time responsiveness of applications using Windows NT 5.0 and a longer-term goal of prototyping and recommending possible scheduling and resource management enhancements to future Microsoft systems products. Finally, while this paper primarily contains examples and results from Windows NT, we believe that the kinds of limitations and artifacts identified may also apply to other commodity systems such as the many UNIX variants. Indeed, this paper is primarily intended to provide a starting point for fruitful discussions along these lines at the workshop and not as a record of completed work.", "num_citations": "26\n", "authors": ["1047"]}
{"title": "Souper: A synthesizing superoptimizer\n", "abstract": " If we can automatically derive compiler optimizations, we might be able to sidestep some of the substantial engineering challenges involved in creating and maintaining a high-quality compiler. We developed Souper, a synthesizing superoptimizer, to see how far these ideas might be pushed in the context of LLVM. Along the way, we discovered that Souper's intermediate representation was sufficiently similar to the one in Microsoft Visual C++ that we applied Souper to that compiler as well. Shipping, or about-to-ship, versions of both compilers contain optimizations suggested by Souper but implemented by hand. Alternately, when Souper is used as a fully automated optimization pass it compiles a Clang compiler binary that is about 3 MB (4.4%) smaller than the one compiled by LLVM.", "num_citations": "25\n", "authors": ["1047"]}
{"title": "Two case studies in predictable application scheduling using Rialto/NT\n", "abstract": " This paper analyzes the results of two case studies in applying the Rialto/NT scheduler to real Windows 2000 applications. The first study is of a soft modem-a modem whose signal processing work is performed on the host CPU, rather than on a dedicated signal processing chip. The second is of an audio player application. Both of these are frequently used real-time applications-ones running on systems that were not designed to support predictable real-time execution. To function correctly, both applications require that ongoing computations be performed in a timely manner. In both cases, we first measured an original version designed to run on Windows 2000, and then modified the application to take advantage of ongoing CPU reservations provided by the Rialto/NT scheduler. We report on the benefits and problems observed when using reservations in these real-world scenarios. In both cases, we found that a\u00a0\u2026", "num_citations": "22\n", "authors": ["1047"]}
{"title": "Eliminating the call stack to save RAM\n", "abstract": " Most programming languages support a call stack in the programming model and also in the runtime system. We show that for applications targeting low-power embedded microcontrollers (MCUs), RAM usage can be significantly decreased by partially or completely eliminating the runtime callstack. We present flattening, a transformation that absorbs a function into its caller, replacing function invocations and returns with jumps. Unlike inlining, flattening does not duplicate the bodies of functions that have multiple callsites. Applied aggressively, flattening results in stack elimination. Flattening is most useful in conjunction with a lifting transformation that moves global variables into a local scope.", "num_citations": "21\n", "authors": ["1047"]}
{"title": "Abstractions for practical virtual machine replay\n", "abstract": " Efficient deterministic replay of whole operating systems is feasible and useful, so why isn't replay a default part of the software stack? While implementing deterministic replay is hard, we argue that the main reason is the lack of general abstractions for understanding and addressing the significant engineering challenges involved in the development of a replay engine for a modern VMM. We present a design blueprint---a set of abstractions, general principles, and low-level implementation details---for efficient deterministic replay in a modern hypervisor. We build and evaluate our architecture in Xen, a full-featured hypervisor. Our architecture can be readily followed and adopted, enabling replay as a ubiquitous part of a modern virtualization stack.", "num_citations": "19\n", "authors": ["1047"]}
{"title": "Help, help, i'm being suppressed! The significance of suppressors in software testing\n", "abstract": " Test features are basic compositional units used to describe what a test does (and does not) involve. For example, in API-based testing, the most obvious features are function calls; in grammar-based testing, the obvious features are the elements of the grammar. The relationship between features as abstractions of tests and produced behaviors of the tested program is surprisingly poorly understood. This paper shows how large-scale random testing modified to use diverse feature sets can uncover causal relationships between what a test contains and what the program being tested does. We introduce a general notion of observable behaviors as targets, where a target can be a detected fault, an executed branch or statement, or a complex coverage entity such as a state, predicate-valuation, or program path. While it is obvious that targets have triggers - features without which they cannot be hit by a test - the notion\u00a0\u2026", "num_citations": "19\n", "authors": ["1047"]}
{"title": "Some guidelines for proportional share CPU scheduling in general-purpose operating systems\n", "abstract": " Our premise is that since there already exists a large, mature body of literature on real-time scheduling in general-purpose operating systems, it is time to spend more effort deciding which of these algorithms should be used and when, and less effort on generating new algorithms. In this paper we focus on proportional share schedulers. We introduce the notion of pessimism\u2014the proportion of overreservation required for an application to meet real-time deadlines when scheduled by proportional share schedulers that have bounded allocation error. We study the implications of pessimism and its effect on the selection of scheduling algorithm and scheduling quantum size, and also the interaction of quantum size and context switch overhead. Finally, we examine the implications of these tradeoffs for the design of applications and schedulers.", "num_citations": "18\n", "authors": ["1047"]}
{"title": "Practical verification of peephole optimizations with Alive\n", "abstract": " Compilers should not miscompile. Peephole optimizations, which perform local rewriting of the input program to improve the efficiency of generated code, are a persistent source of compiler bugs. We created Alive, a domain-specific language for writing optimizations and for automatically either proving them correct or else generating counterexamples. Furthermore, Alive can be automatically translated into C++ code that is suitable for inclusion in an LLVM optimization pass. Alive is based on an attempt to balance usability and formal methods; for example, it captures---but largely hides---the detailed semantics of the various kinds of undefined behavior. Alive has found numerous bugs in the LLVM compiler and is being used by LLVM developers.", "num_citations": "16\n", "authors": ["1047"]}
{"title": "Efficient type and memory safety for tiny embedded systems\n", "abstract": " We report our experience in implementing type and memory safety in an efficient manner for sensor network nodes running TinyOS: tiny embedded systems running legacy, C-like code. A compiler for a safe language must often insert dynamic checks into the programs it produces; these generally make programs both larger and slower. In this paper, we describe our novel compiler toolchain, which uses a family of techniques to minimize or avoid these run-time costs. Our results show that safety can in fact be implemented cheaply on low-end 8-bit microcontrollers.", "num_citations": "15\n", "authors": ["1047"]}
{"title": "Random testing for C and C++ compilers with YARPGen\n", "abstract": " Compilers should not crash and they should not miscompile applications. Random testing is an effective method for finding compiler bugs that have escaped other kinds of testing. This paper presents Yet Another Random Program Generator (YARPGen), a random test-case generator for C and C++ that we used to find and report more than 220 bugs in GCC, LLVM, and the Intel\u00ae C++ Compiler. Our research contributions include a method for generating expressive programs that avoid undefined behavior without using dynamic checks, and generation policies, a mechanism for increasing diversity of generated code and for triggering more optimizations. Generation policies decrease the testing time to find hard-to-trigger compiler bugs and, for the kinds of scalar optimizations YARPGen was designed to stress-test, increase the number of times these optimizations are applied by the compiler by an average of 20\u00a0\u2026", "num_citations": "14\n", "authors": ["1047"]}
{"title": "The case for hierarchical schedulers with performance guarantees\n", "abstract": " Audio and video applications, process control, agile manufacturing and even defense systems are using commodity hardware and operating systems to run combinations of real-time and non-real-time tasks. We propose an architecture that will allow a general-purpose operating system to schedule conventional and real-time tasks with diverse requirements, to provide flexible load isolation between applications, users, and accounting domains, and to enforce high-level policies about the allocation of CPU time. This is accomplished by implementing a dynamic, hierarchical scheduling infrastructure. The infrastructure is integrated with a resource manager that provides a level of indirection between resource requests and the scheduling hierarchy. A scheduling infrastructure separates scheduler code from the rest of the operating system. To demonstrate the utility of our architecture, we describe its application to three existing real-time schedulers. For each of the three, we show added flexibility while retaining the original scheduling guarantees.", "num_citations": "14\n", "authors": ["1047"]}
{"title": "An isotach implementation for Myrinet\n", "abstract": " An isotach network provides strong guarantees about message delivery order. We show that an isotach network can be implemented e ciently entirely in software, using commercial othe-shelf hardware. This report describes that e ort. Parts of this implementation could be performed much more e ciently in hardware; we are currently developing custom hardware components to do this. The all-software version then serves several purposes: to develop a working isotach system rapidly, for use as a platform for development of higher level software to nd potential problems in the hardware design to pre-test software components of the system so that they will not have to be debugged at the same time as hardware to achieve as much performance as possible without hardware acceleration", "num_citations": "14\n", "authors": ["1047"]}
{"title": "Offline compression for on-chip RAM\n", "abstract": " We present offline RAM compression, an automated source-to-source transformation that reduces a program's data size. Statically allocated scalars, pointers, structures, and arrays are encoded and packed based on the results of a whole-program analysis in the value set and pointer set domains. We target embedded software written in C that relies heavily on static memory allocation and runs on Harvard-architecture microcontrollers supporting just a few KB of on-chip RAM. On a collection of embedded applications for AVR microcontrollers, our transformation reduces RAM usage by an average of 12%, in addition to a 10% reduction through a dead-data elimination pass that is also driven by our whole-program analysis, for a total RAM savings of 22%. We also developeda technique for giving developers access to a flexible spectrum of tradeoffs between RAM consumption, ROM consumption, and CPU efficiency\u00a0\u2026", "num_citations": "13\n", "authors": ["1047"]}
{"title": "The case for using middleware to manage diverse soft real-time schedulers\n", "abstract": " Although a number of general-purpose operating systems have been extended with soft real-time schedulers and have the potential to support coexisting, independently developed real-time applications, this potential is currently largely unexploited by common applications. This is because the provided scheduling functionality is low-level and depends on parameters that are difficult to estimate, and because different semantics are provided by different schedulers. The cost/benefit ratio of real-time support in general-purpose operating systems is too high for most users and application developers to tolerate. The contribution of this paper is the design of the CPU Resource Manager (CRM): a middleware application that manages processor allocation in a QoS-enabled general-purpose operating system by (1) providing a level of indirection between applications and the scheduling subsystem,(2) automatically\u00a0\u2026", "num_citations": "13\n", "authors": ["1047"]}
{"title": "Advanced adaptive application (A3) environment: initial experience\n", "abstract": " In this paper, we describe the prevention-focused and adaptive middleware mechanisms implemented as part of the Advanced Adaptive Applications (A3) Environment that we are developing as a near-application and application-focused cyber-defense technology under the DARPA Clean-slate design of Resilient, Adaptive, Secure Hosts (CRASH) program.", "num_citations": "9\n", "authors": ["1047"]}
{"title": "Hierarchical Loadable Schedulers\n", "abstract": " The processors in workstations, personal computers, and servers are becoming increasingly powerful, enabling them to run new kinds of applications, and to simultaneously run combinations of applications that were previously infeasible. However, fast hardware is not enough-the operating system must effectively manage system resources such as processor time, memory, and I/O bandwidth. The proposed work focuses on management of processor time, the effectiveness of which is an important factor in overall system performance [14].Each processor in a system can only perform one task at a time; the scheduler in the operating system decides which task that is. The schedulers in general-purpose operating systems are designed to provide fast response time for interactive applications and high throughput for non-interactive ones. Unfortunately, these schedulers have very little knowledge of applications' actual CPU scheduling needs, causing them to poorly schedule several classes of applications. Even multi-gigahertz processors will be of little use if the operating system does not run the right applications at the right times.", "num_citations": "8\n", "authors": ["1047"]}
{"title": "Dataflow-based pruning for speeding up superoptimization\n", "abstract": " Superoptimization is a compilation strategy that uses search to improve code quality, rather than relying on a canned sequence of transformations, as traditional optimizing compilers do. This search can be seen as a program synthesis problem: from unoptimized code serving as a specification, the synthesis procedure attempts to create a more efficient implementation. An important family of synthesis algorithms works by enumerating candidates and then successively checking if each refines the specification, using an SMT solver. The contribution of this paper is a pruning technique which reduces the enumerative search space using fast dataflow-based techniques to discard synthesis candidates that contain symbolic constants and uninstantiated instructions. We demonstrate the effectiveness of this technique by improving the runtime of an enumerative synthesis procedure in the Souper superoptimizer for the\u00a0\u2026", "num_citations": "7\n", "authors": ["1047"]}
{"title": "Testing static analyses for precision and soundness\n", "abstract": " Static analyses compute properties of programs that are true in all executions, and compilers use these properties to justify optimizations such as dead code elimination. Each static analysis in a compiler should be as precise as possible while remaining sound and being sufficiently fast. Unsound static analyses typically lead to miscompilations, whereas imprecisions typically lead to missed optimizations. Neither kind of bug is easy to track down.", "num_citations": "7\n", "authors": ["1047"]}
{"title": "Lock inference for systems software\n", "abstract": " We have developed task scheduler logic (TSL) to automate reasoning about scheduling and concurrency in systems software. TSL can detect race conditions and other errors as well as supporting lock inference: the derivation of an appropriate lock implementation for each critical section in a system. Lock inference solves a number of problems in creating flexible, reliable, and efficient systems software. TSL is based on a notion of asymmetrical preemption relations and it exploits the hierarchical inheritance of scheduling properties that is common in systems software.", "num_citations": "5\n", "authors": ["1047"]}
{"title": "Predictable Scheduling for Digital Audio\n", "abstract": " This paper presents results from applying the Rialto/NT scheduler to some real Windows 2000 application scenarios. We report on two aspects of this work. First, we studied the reliability of an audio player application and the middleware and kernel components running beneath it in order to assess its reliability under various concurrent application loads. Then we added CPU Reservations to portions of the workload in order to determine if doing so would increase playback reliability under workloads in which problems were previously seen. We report on the benefits and problems observed when using reservations in these real-world scenarios. We also describe the methodologies we used to analyze the real-time behavior of the operating system and applications, including the use of instrumented kernels to produce execution traces. Finally, we describe several improvements in the Rialto/NT implementation that have been made since the system was originally described.", "num_citations": "5\n", "authors": ["1047"]}
{"title": "Thread verification vs. interrupt verification\n", "abstract": " Interrupts are superficially similar to threads, but there are subtle semantic differences between the two abstractions. This paper compares and contrasts threads and interrupts from the point of view of verifying the absence of race conditions. We identify a small set of extensions that permit thread verification tools to also verify interrupt-driven software, and we present examples of source-to-source transformations that turn interrupt-driven code into semantically equivalent thread-based code that can be checked by a thread verifier.", "num_citations": "4\n", "authors": ["1047"]}
{"title": "Task/Scheduler Logic: Reasoning about Concurrency in Component-Based Systems Software\n", "abstract": " Although component-based software development promises increased reuse and faster development time, it has proven difficult to build component-based systems software. One obstacle is that the concurrency structure in systems software tends to be complex. First, instead of a single scheduler, there is a hierarchy of schedulers: the processor schedules interrupts, the OS schedules software interrupts and threads, and threads run event loops. This gives rise to many different execution environments, each with its own restrictions on actions that can be taken by code running in it. Second, the preemption relationships between these execution environments are often asymmetric: an interrupt handler can preempt a thread but not vice versa. This results in an asymmetric pattern of locking where low priority code must protect against high priority code but not vice versa. This situation is rare in other application domains but common in systems software.", "num_citations": "4\n", "authors": ["1047"]}
{"title": "Composable execution environments\n", "abstract": " Many problems in developing and maintaining systems software stem from inappropriate choices of execution environments: sets of rules and conventions for structuring code. We have developed composable execution environments (CEE), a new way to compose systems software that is based on two main capabilities: the hierarchical composition of execution environments, and the late binding of design requirements to implementation constructs.", "num_citations": "4\n", "authors": ["1047"]}
{"title": "The Problems You're Having May Not Be The Problems You Think You're Having\n", "abstract": " This paper presents a snapshot of early results from a study of Windows NT aimed at understanding and improving its limitations when used for real-time tasks, such as those that arise for audio, video, and industrial control applications. It also examines the roles of intuition and conventional wisdom versus instrumentation and measurement in investigating latency behaviors.", "num_citations": "4\n", "authors": ["1047"]}
{"title": "Atomicity and visibility in tiny embedded systems\n", "abstract": " Visibility is a property of a programming language's memory model that determines when values stored by one concurrent computation become visible to other computations. Our work exploits the insight that in nesC, a C-like language with explicit atomicity, the traditional way of ensuring timely visibility---volatile variables---can be entirely avoided. This is advantageous because the volatile qualifier is a notorious source of programming errors and misunderstandings. Furthermore, the volatile qualifier hurts performance by inhibiting many more optimizations than are necessary to ensure visibility. In this paper we extend the semantics of nesC's atomic statements to include a visibility guarantee, we show two ways that these semantics can be implemented, and we also show that our better implementation has no drawbacks in terms of resource usage.", "num_citations": "3\n", "authors": ["1047"]}
{"title": "Hierarchical schedulers, performance guarantees, and resource management\n", "abstract": " An attractive approach to scheduling applications with diverse CPU scheduling requirements is to use different schedulers for different applications. For example: real-time schedulers allow applications to perform computations before deadlines, time-sharing schedulers provide high throughput for compute-bound processes and fast response time for interactive applications, and gang schedulers and cluster coschedulers permit tightly-coupled parallel applications to achieve high performance in the presence of multiprogramming. Furthermore, individual members of these broad classes of algorithms make tradeoffs that may or may not be appropriate for a given situation. In order to take advantage of these diverse algorithms, we permit schedulers to be arranged in a hierarchy-a root scheduler gives CPU time to the schedulers below it in the hierarchy and so on until an application thread is scheduled by a leaf scheduler. This architecture has a number of advantages:", "num_citations": "3\n", "authors": ["1047"]}
{"title": "Research for practice: vigorous public debates in academic computer science\n", "abstract": " Expert-curated guides to the best of CS research.", "num_citations": "2\n", "authors": ["1047"]}
{"title": "Undefined behavior in 2017\n", "abstract": " Undefined Behavior in 2017 Page 1 Undefined Behavior in 2017 John Regehr University of Utah, USA Page 2 Today: \u2022 What is undefined behavior (UB)? \u2022 Why does it exist? \u2022 What are the consequences of UB in C and C++? \u2022 Modern UB detection and mitigation Page 3 sqrt(-1) = ? \u2022 i \u2022 NaN \u2022 An arbitrary value \u2022 Throw an exception \u2022 Abort the program \u2022 Undefined behavior Page 4 \u2022 Undefined behavior (UB) is a design choice \u2022 UB is the most efficient alternative because it imposes the fewest requirements \u2013 \u201c\u2026 behavior, upon use of a nonportable or erroneous program construct or of erroneous data, for which this International Standard imposes no requirements\u201d Page 5 C and C++ have lots of UB \u2022 To avoid overhead \u2022 To avoid compiler complexity \u2022 To provide maximal compatibility across implementations and targets Page 6 According to Appendix J of the standard, C11 has 199 undefined behaviors \u2022 But this list isn\u2026", "num_citations": "2\n", "authors": ["1047"]}
{"title": "A Practical Logic Framework for Verifying Safety Properties of Executables\n", "abstract": " We present a novel program logic, Lf, which is designed on top of a Hoare logic, but is simpler, more flexible and more scalable. Based on Lf, we develop a framework for automatically verifying safety properties of executables. It utilizes a whole-program interprocedural abstract interpretation to automatically discover the specifications needed by Lf to prove a program judgment. We implemented Lf and the framework in the HOL theorem prover.", "num_citations": "2\n", "authors": ["1047"]}
{"title": "How to rapidly prototype a realtime scheduler\n", "abstract": " Implementing a new scheduling algorithm in an OS kernel is often an important step in scheduling research because it permits evaluation of the algorithm\u2019s performance on real workloads. However, developing a new scheduler is not a trivial task because it requires sophisticated programming skills and a deep knowledge of kernel internals. In this paper we show how to use the HLS scheduling framework to develop new schedulers in a user-level simulator, where advanced debugging tools can be used to achieve a high level of robustness before the scheduler is converted to a loadable kernel module simply by recompiling it. Besides facilitating debugging and porting, the HLS abstraction has the benefit of bringing the programming model very close to what, in our experience, scheduler developers want.", "num_citations": "2\n", "authors": ["1047"]}