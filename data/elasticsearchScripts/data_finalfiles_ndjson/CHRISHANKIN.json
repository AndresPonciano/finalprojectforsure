{"title": "Abstract interpretation of declarative languages\n", "abstract": " Abstract Interpretation of Declarative Languages | Guide books ACM Digital Library home ACM home Google, Inc. (search) Advanced Search Browse About Sign in Register Advanced Search Journals Magazines Proceedings Books SIGs Conferences People More Search ACM Digital Library SearchSearch Advanced Search Browse Browse Digital Library Collections More HomeBrowse by TitleBooksAbstract Interpretation of Declarative Languages ABSTRACT No abstract available. Index Terms 1.Abstract Interpretation of Declarative Languages 1.General and reference 1.Document types 1.Reference works 2.Software and its engineering 1.Software notations and tools 1.Compilers 1.Interpreters Comments Login options Check if you have access through your login credentials or your institution to get full access on this article. Sign in Full Access Get this Publication Information Contributors Published in Guide books \u2026", "num_citations": "447\n", "authors": ["279"]}
{"title": "Decision support approaches for cyber security investment\n", "abstract": " When investing in cyber security resources, information security managers have to follow effective decision-making strategies. We refer to this as the cyber security investment challenge.In this paper, we consider three possible decision support methodologies for security managers to tackle this challenge. We consider methods based on game theory, combinatorial optimisation, and a hybrid of the two. Our modelling starts by building a framework where we can investigate the effectiveness of a cyber security control regarding the protection of different assets seen as targets in presence of commodity threats. As game theory captures the interaction between the endogenous organisation's and attackers' decisions, we consider a 2-person control game between the security manager who has to choose among different implementation levels of a cyber security control, and a commodity attacker who chooses among\u00a0\u2026", "num_citations": "227\n", "authors": ["279"]}
{"title": "Strictness analysis for higher-order functions\n", "abstract": " Abstract interpretation is a compile-time technique which is used to gain information about a program that may then be used to optimise the execution of the program. A particular use of abstract interpretation is the strictness analysis of functional programs. This provides the key to the exploitation of parallelism in the evaluation of programs written in functional languages. In a language that has lazy semantics, the main potential for parallelism arises in the evaluation of operands of strict operators. A function is strict in an argument if its value is undefined whenever the argument is undefined. If we can use strictness analysis to detect which arguments a function is strict in, we then know that these arguments can be safely evaluated in parallel because this will not affect the lazy semantics. Experimental results suggest that this leads to significant speed-ups.Mycroft was the first person to apply abstract interpretation to\u00a0\u2026", "num_citations": "223\n", "authors": ["279"]}
{"title": "Efficient field-sensitive pointer analysis of C\n", "abstract": " The subject of this article is flow- and context-insensitive pointer analysis. We present a novel approach for precisely modelling struct variables and indirect function calls. Our method emphasises efficiency and simplicity and is based on a simple language of set constraints. We obtain an O(v4) bound on the time needed to solve a set of constraints from this language, where v is the number of constraint variables. This gives, for the first time, some insight into the hardness of performing field-sensitive pointer analysis of C. Furthermore, we experimentally evaluate the time versus precision trade-off for our method by comparing against the field-insensitive equivalent. Our benchmark suite consists of 11 common C programs ranging in size from 15,000 to 200,000 lines of code. Our results indicate the field-sensitive analysis is more expensive to compute, but yields significantly better precision. In addition, our technique\u00a0\u2026", "num_citations": "202\n", "authors": ["279"]}
{"title": "A framework for security analysis of mobile wireless networks\n", "abstract": " We present a framework for specification and security analysis of communication protocols for mobile wireless networks. This setting introduces new challenges which are not being addressed by classical protocol analysis techniques. The main complication stems from the fact that the actions of intermediate nodes and their connectivity can no longer be abstracted into a single unstructured adversarial environment as they form an inherent part of the system's security. In order to model this scenario faithfully, we present a broadcast calculus which makes a clear distinction between the protocol processes and the network's connectivity graph, which may change independently from protocol actions. We identify a property characterising an important aspect of security in this setting and express it using behavioural equivalences of the calculus. We complement this approach with a control flow analysis which enables us\u00a0\u2026", "num_citations": "195\n", "authors": ["279"]}
{"title": "Approximate non-interference\n", "abstract": " We address the problem of characterising the security of a program against unauthorised information flows. Classical approaches are based on non-interference models which depend ultimately on the notion of process equivalence. In these models confidentiality is an absolute property stating the absence of any illegal information flow. We present a model in which the notion of non-interference is approximated in the sense that it allows for some exactly quantified leakage of information. This is characterised via a notion of process similarity which replaces the indistinguishability of processes by a quantitative measure of their behavioural difference. Such a quantity is related to the number of statistical tests needed to distinguish two behaviours. We also present two semantics-based analyses of approximate non-interference and we show that one is a correct abstraction of the other.", "num_citations": "187\n", "authors": ["279"]}
{"title": "Lambda calculi: a guide for computer scientists\n", "abstract": " The (lambda)-calculus lies at the very foundations of computer science. Besides its historical role in computability theory it has had significant influence on programming language design and implementation, denotational semantics, and domain theory. The book emphasizes the proof theory for the type-free (lambda)-calculus. The first six chapters concern this calculus and cover the basic theory, reduction, models, computability, and the relationship between the (lambda)-calculus and combinatory logic. Chapter 7 presents a variety of typed calculi; first the simply typed (lambda)-calculus, then Milner-style polymorphism and, finally, the polymorphic (lambda)-calculus. Chapter 8 concerns three variants of the type-free (lambda)-calculus that have recently appeared in the research literature: the lazy (lambda)-calculus, the concurrent (gamma)-calculus and the (lambda)(sigma)-calculus. The final chapter contains references and a guide to further reading. There are exercises throughout. In contrast to earlier books on these topics, which were written by logicians, the book is written from a computer science perspective and emphasizes the practical relevance of many of the key theoretical ideas. The book is intended as a course text for final year undergraduates or first year graduate students in computer science. Research students should find it a useful introduction to more specialist literature.", "num_citations": "114\n", "authors": ["279"]}
{"title": "The theory of strictness analysis for higher order functions\n", "abstract": " Abstract interpretation is a compile-time technique which is used to gain information about a program that may then be used to optimise the execution of the program. A particular use of abstract interpretation is in strictness analysis of functional programs. This provides the key to the exploitation of parallelism in the evaluation of programs written in functional languages. In a language that has lazy semantics, the main potential for parallelism arises in the evaluation of operands of strict operators. A function is strict in an argument if its value is undefined whenever the argument is undefined. If we can use strictness analysis to detect which arguments a function is strict in, we then know that these arguments can be safely evaluated in parallel because this will not affect the lazy semantics. Experimental results suggest that this leads to significant speed-ups.             Mycroft was the first person to apply abstract\u00a0\u2026", "num_citations": "109\n", "authors": ["279"]}
{"title": "Cybersecurity and healthcare: how safe are we?\n", "abstract": " Rising cybersecurity threats to healthcare require policy makers to tackle fragmented governance, to develop and implement security standards, and to help organisations to improve their resilience, say Guy Martin and colleagues", "num_citations": "102\n", "authors": ["279"]}
{"title": "A calculus of Gamma programs\n", "abstract": " Gamma is a minimal language based on conditional multiset rewriting. The virtues of this paradigm in terms of systematic program construction and design of programs for highly parallel machines have been demonstrated in previous papers. We introduce here sequential and parallel operators for combining Gamma programs and we study their properties. The main focus of the paper is to give conditions under which sequential composition can be transformed into parallel composition and vice versa. Such transformations are especially valuable for adapting Gamma programs for execution on a particular target architecture.", "num_citations": "95\n", "authors": ["279"]}
{"title": "Principles of functional programming\n", "abstract": " Since 1977, when John Backus gave his Turing Award Lecture [1], there has been an increasing interest in functional (or applicative) programming. Unfortunately, there have been few textbooks written on the subject, with the notable exception of Henderson's book [2]. The book under review, which is designed as an intermediate undergraduate text, therefore addresses an important need. The language used for examples is SUGAR, a syntactically sugared variant of the Lambda-calculus, but another functional language such as SASL, KRC, or HOPE could be used for the lab portion of the course. The first chapter briefly contrasts functional and imperative programming and discusses functional decomposition. Chapter 2 introduces SUGAR. The next part of the book, consisting of Chapters 3 through 6, covers a wide variety of topics classified as \u201cMathematical Foundations and Implementation.\u201d These topics include\u00a0\u2026", "num_citations": "93\n", "authors": ["279"]}
{"title": "Fast multi-scale detection of relevant communities in large-scale networks\n", "abstract": " Nowadays, networks are almost ubiquitous. In the past decade, community detection received an increasing interest as a way to uncover the structure of networks by grouping nodes into communities more densely connected internally than externally. Yet most of the effective methods available do not consider the potential levels of organization, or scales, a network may encompass and are therefore limited. In this paper, we present a method compatible with global and local criteria that enables fast multi-scale community detection on large networks. The method is derived in two algorithms, one for each type of criterion, and implemented with six known criteria. Uncovering communities at various scales is a computationally expensive task. Therefore, this work puts a strong emphasis on the reduction of computational complexity. Some heuristics are introduced for speed-up purposes. Experiments demonstrate\u00a0\u2026", "num_citations": "87\n", "authors": ["279"]}
{"title": "Coordination Languages and Models First International Conference, COORDINATION'96 Cesena, Italy, April 15\u201317, 1996 Proceedings\n", "abstract": " This book constitutes the refereed proceedings of the First International Conference on Coordination Languages and Models, COORDINATION'96, held in Cesena, Italy in April 1996. Over the last few years, a new class of models, formalisms, and mechanisms for describing concurrent and distributed computations has emerged. A characteristic feature of these coordination languages and models is that they are based on (generative) communication via a shared data space. The 21 revised full papers presented were selected from a total of 78 submissions; also included are three invited papers and 10 posters. All in all, these papers report the state of the art in this young and active area of research and development.", "num_citations": "84\n", "authors": ["279"]}
{"title": "Game theory meets information security management\n", "abstract": " This work addresses the challenge \u201chow do we make better security decisions?\u201d and it develops techniques to support human decision making and algorithms which enable well-founded cyber security decisions to be made. In this paper we propose a game theoretic model which optimally allocates cyber security resources such as administrators\u2019 time across different tasks. We first model the interactions between an omnipresent attacker and a team of system administrators seen as the defender, and we have derived the mixed Nash Equilibria (NE) in such games. We have formulated general-sum games that represent our cyber security environment, and we have proven that the defender\u2019s Nash strategy is also minimax. This result guarantees that independently from the attacker\u2019s strategy the defender\u2019s solution is optimal. We also propose Singular Value Decomposition (SVD) as an efficient technique to\u00a0\u2026", "num_citations": "77\n", "authors": ["279"]}
{"title": "Probabilistic \u03bb-calculus and quantitative program analysis\n", "abstract": " We show how the framework of probabilistic abstract interpretation can be applied to statically analyse a probabilistic version of the \u03bb-calculus. The resulting analysis allows for a more speculative use of its outcomes based on the consideration of statistically defined quantities. After introducing a linear operator based semantics for our probabilistic \u03bb-calculus \u039b p , and reviewing the framework of abstract interpretation and strictness analysis, we demonstrate our technique by constructing a probabilistic (first-order) strictness analysis for \u039b p .", "num_citations": "68\n", "authors": ["279"]}
{"title": "An introduction to abstract interpretation\n", "abstract": " A significant proportion of the code in most modern production compilers is devoted to the optimization of generated code. All too often the run-time behaviour of the optimized program is inconsistent with the pre-optimization behaviour; in other words, the optimization has affected the semantics of the program as well as the pragmatics. This problem normally arises because insufficient rigour has been applied to the correctness proof of the optimization. For programming languages with defined mathematical semantics there is a growing set of tools that provides a basis for semantically correct transformation; one such tool is abstract interpretation. This book is mainly, but not exclusively, devoted to the presentation of various uses of abstract interpretation in the compile-time analysis of declarative programming languages. Declarative languages are being increasingly used as the basis for rapid prototyping systems\u00a0\u2026", "num_citations": "68\n", "authors": ["279"]}
{"title": "Coordination programming: mechanisms, models and semantics\n", "abstract": " Coordination, considered abstractly, is an ubiquitous notion in computer science: for example, programming languages coordinate elementary instructions; operating systems coordinate accesses to hardware resources; database transaction schedulers coordinate accesses to shared data; etc. All these situations have some common features, which can be identified at the abstract level as \u201ccoordination mechanisms\u201d. This book focuses on a class of coordination models where multiple pieces of software coordinate their activities through some shared dataspace. The book has three parts. Part 1 presents the main coordination models studied in this book (Gamma, LO, TAO, LambdaN). Part 2 focuses on various semantics aspects of coordination, applied mainly to Gamma. Part 3 presents actual implementations of coordination models and an application.", "num_citations": "65\n", "authors": ["279"]}
{"title": "Real-time processing of social media with SENTINEL: A syndromic surveillance system incorporating deep learning for health classification\n", "abstract": " Interest in real-time syndromic surveillance based on social media data has greatly increased in recent years. The ability to detect disease outbreaks earlier than traditional methods would be highly useful for public health officials. This paper describes a software system which is built upon recent developments in machine learning and data processing to achieve this goal. The system is built from reusable modules integrated into data processing pipelines that are easily deployable and configurable. It applies deep learning to the problem of classifying health-related tweets and is able to do so with high accuracy. It has the capability to detect illness outbreaks from Twitter data and then to build up and display information about these outbreaks, including relevant news articles, to provide situational awareness. It also provides nowcasting functionality of current disease levels from previous clinical data combined with\u00a0\u2026", "num_citations": "62\n", "authors": ["279"]}
{"title": "Information flow for Algol-like languages\n", "abstract": " In this paper we present an approach to information flow analysis for a family of languages. We start with a simple imperative language. We present an information flow analysis using a flow logic. The paper contains detailed correctness proofs for this analysis. We next extend the analysis to a restricted form of Idealised Algol, a call-by-value higher-order extension of the simple imperative language (the key restriction being the lack of recursion). The paper concludes with a discussion of further extensions, including a probabilistic extension of Idealised Algol.", "num_citations": "61\n", "authors": ["279"]}
{"title": "Coordination languages for parallel programming\n", "abstract": " A number of interesting models have been proposed and used to support coordination languages and systems. In this introductory paper, we first present a number of important concepts that form a context for classification and comparison of various coordination models and languages, and their applications. Next, we review three models and their associated languages, representing three different approaches to coordination. We illustrate the application of each model and language by using it to solve the classical dining philosophers problem. This paper ends with an overview of the rest of the papers that appear in this special issue.", "num_citations": "60\n", "authors": ["279"]}
{"title": "Quantitative relations and approximate process equivalences\n", "abstract": " We introduce a characterisation of probabilistic transition systems (PTS) in terms of linear operators on some suitably defined vector space representing the set of states. Various notions of process equivalences can then be re-formulated as abstract linear operators related to the concrete PTS semantics via a probabilistic abstract interpretation. These process equivalences can be turned into corresponding approximate notions by identifying processes whose abstract operators \u201cdiffer\u201d by a given quantity, which can be calculated as the norm of the difference operator. We argue that this number can be given a statistical interpretation in terms of the tests needed to distinguish two behaviours.", "num_citations": "58\n", "authors": ["279"]}
{"title": "Online cycle detection and difference propagation for pointer analysis\n", "abstract": " We present and evaluate a number of techniques to improve the execution time of interprocedural pointer analysis in the context of large C programs. The analysis is formulated as a graph of set constraints and solved using a worklist algorithm. Indirections lead to new constraints being added during this process. We present a new algorithm for online cycle detection, and a difference propagation technique which records changes in a variable's solution. The effectiveness of these and other methods are evaluated experimentally using nine common 'C' programs ranging between 1000 to 55000 lines of code.", "num_citations": "57\n", "authors": ["279"]}
{"title": "Deriving algorithms from type inference systems: Application to strictness analysis\n", "abstract": " The role of non-standard type inference in static program analysis has been much studied recently. Early work emphasised the efficiency of type inference algorithms and paid little attention to the correctness of the inference system. Recently more powerful inference systems have been investigated but the connection with efficient inference algorithms has been obscured. The contribution of this paper is twofold: first we show how to transform a program logic into an algorithm and, second, we introduce the notion of lazy types and show how to derive an efficient algorithm or strictness analysis.", "num_citations": "57\n", "authors": ["279"]}
{"title": "Cybersecurity games and investments: A decision support approach\n", "abstract": " In this paper we investigate how to optimally invest in cybersecurity controls. We are particularly interested in examining cases where the organization suffers from an underinvestment problem or inefficient spending on cybersecurity. To this end, we first model the cybersecurity environment of an organization. We then model non-cooperative cybersecurity control-games between the defender which abstracts all defense mechanisms of the organization and the attacker which can exploit different vulnerabilities at different network locations. To implement our methodology we use the SANS Top 20 Critical Security Controls and the 2011 CWE/SANS top 25 most dangerous software errors. Based on the profile of an organization, which forms its preferences in terms of indirect costs, its concerns about different kinds of threats and the importance of the assets given their associated risks we derive the Nash\u00a0\u2026", "num_citations": "49\n", "authors": ["279"]}
{"title": "Measuring the confinement of probabilistic systems\n", "abstract": " In this paper we lay the semantic basis for a quantitative security analysis of probabilistic systems by introducing notions of approximate confinement based on various process equivalences. We re-cast the operational semantics classically expressed via probabilistic transition systems (PTS) in terms of linear operators and we present a technique for defining approximate semantics as probabilistic abstract interpretations of the PTS semantics. An operator norm is then used to quantify this approximation. This provides a quantitative measure \u025b of the indistinguishability of two processes and therefore of their confinement. In this security setting a statistical interpretation is then given of the quantity \u025b which relates it to the number of tests needed to breach the security of the system.", "num_citations": "48\n", "authors": ["279"]}
{"title": "A parallel programming style and its algebra of programs\n", "abstract": " We present a set of primitive program schemes, which together with just two basic combining forms provide a suprisingly expressive parallel programming language. The primitive program schemes (called tropes) take the form of parameterised conditional rewrite rules, and the computational model is a variant of the Gamma style, in which computation proceeds by nondeterministic local rewriting of a global multiset.             We consider a number of examples which illustrate the use of tropes and we study the algebraic properties of the sequential and parallel combining forms. Using the examples we illustrate the application of these properties in the verification of some simple program transformations.", "num_citations": "48\n", "authors": ["279"]}
{"title": "Non-deterministic games and program analysis: An application to security\n", "abstract": " We present a unifying framework for using game semantics as a basis for program analysis. Also, we present a case study of the techniques. The unifying framework presents games-based program analysis as an abstract interpretation of an appropriate games category in the category of non-deterministic games. The case study concerns an application to security.", "num_citations": "47\n", "authors": ["279"]}
{"title": "Finding fixed points in finite lattices\n", "abstract": " Recently there has been much interest in the abstract interpretation of declarative languages. Abstract interpretation is a semantics-based approach to program analysis that uses compile time evaluation of programs using simplified value domains. This gives information about the run-time properties of programs and provides the basis for significant performance improvements. A particular example of abstract interpretation is strictness analysis which allows the detection of the parameters in which a function is strict; these parameters may be passed by value without compromising the termination properties of the program.             The central, most complex task of an abstract interpreter is finding the fixpoints of recursive functions in the abstract value space. An elegant algorithm, the frontiers algorithm, has been proposed by Simon Peyton-Jones and Chris Clack that performs very well for the strictness analysis\u00a0\u2026", "num_citations": "46\n", "authors": ["279"]}
{"title": "Fixed points and frontiers: a new perspective\n", "abstract": " Abstract interpretation is the collective name for a family of semantics-based techniques for compile-time analysis of programs. One of the most costly operations in automating such analyses is the computation of fixed points. The frontiers algorithm is an elegant method, invented by Chris Clack and Simon Peyton Jones, which addresses this issue.In this article we present a new approach to the frontiers algorithm based on the insight that frontiers represent upper and lower subsets of a function's argument domain. This insight leads to a new formulation of the frontiers algorithm for higher-order functions, which is considerably more concise than previous versions.We go on to argue that for many functions, especially in the higher-order case, finding fixed points is an intractable problem unless the sizes of the abstract domains are reduced. We show how the semantic machinery of abstract interpretation allows us to\u00a0\u2026", "num_citations": "45\n", "authors": ["279"]}
{"title": "Online cycle detection and difference propagation: Applications to pointer analysis\n", "abstract": " This paper presents and evaluates a number of techniques to improve the execution time of interprocedural pointer analysis in the context of C programs. The analysis is formulated as a graph of set constraints and solved using a worklist algorithm. Indirections lead to new constraints being added during this procedure. The solution process can be simplified by identifying cycles, and we present a novel online algorithm for doing this. We also present a difference propagation scheme which avoids redundant work by tracking changes to each solution set. The effectiveness of these and other methods are shown in an experimental study over 12 common \u2018C\u2019 programs ranging between 1000 to 150,000 lines of code.", "num_citations": "43\n", "authors": ["279"]}
{"title": "Multi-scale community detection using stability as optimisation criterion in a greedy algorithm.\n", "abstract": " Whether biological, social or technical, many real systems are represented as networks whose structure can be very informative regarding the original system\u2019s organisation. In this respect the field of community detection has received a lot of attention in the past decade. Most of the approaches rely on the notion of modularity to assess the quality of a partition and use this measure as an optimisation criterion. Recently stability was introduced as a new partition quality measure encompassing former partition quality measures such as modularity. The work presented here assesses stability as an optimisation criterion in a greedy approach similar to modularity optimisation techniques and enables multi-scale analysis using Markov time as resolution parameter. The method is validated and compared with other popular approaches against synthetic and various real data networks and the results show that the method enables accurate multi-scale network analysis.", "num_citations": "41\n", "authors": ["279"]}
{"title": "Probabilistic klaim\n", "abstract": " We introduce a\u00a0probabilistic extension of KLAIM, where the behaviour of networks and individual nodes is determined by a\u00a0probabilistic scheduler for processes and probabilistic allocation environments which describe the logical neighbourhood of each node. The resulting language has two variants which are modelled respectively as discrete and continuous time Markov processes. We suggest that Poisson processes are a\u00a0natural probabilistic model for the coordination of discrete processes asynchronously communicating in continuous time and we use them to define the operational semantics of the continuous time variant. This framework allows for the implementation of networks with independent clocks on each site.", "num_citations": "35\n", "authors": ["279"]}
{"title": "A new approach to control flow analysis\n", "abstract": " We develop a control flow analysis algorithm for PCF based on game semantics. The analysis is closely related to Shivers' 0-CFA analysis and the algorithm is shown to be cubic. The game semantics basis for the algorithm means that it can be naturally extended to handle strict languages and languages with imperative features. These extensions are discussed in the paper. We sketch the correctness proof for the algorithm. We also illustrate an algorithm for computing k-limited CFA.", "num_citations": "35\n", "authors": ["279"]}
{"title": "WannaCry\u2014a year on\n", "abstract": " The disruption from last year\u2019s WannaCry malware attack affected 60 NHS trusts, 595 general practices, and thousands of patients. 1 The costs of the cybersecurity incident are not known. Worryingly, all 200 NHS hospitals inspected by the Care Quality Commission since the attack have fallen short of the UK government\u2019s Cyber Essentials Plus certification, a basic set of minimum organisational security standards. 2 3This sobering finding not only highlights the poor security and resilience in the NHS but also suggests that little real progress has been made in the past year. As we continue to rely evermore on technology, effective cybersecurity should be a fundamental part of the healthcare culture. Any breach, loss, or corruption of patient data can paralyse a hospital, harm individuals, and erode patients\u2019 trust in healthcare systems that are regularly under threat as they are a rich source of data and present a soft\u00a0\u2026", "num_citations": "31\n", "authors": ["279"]}
{"title": "The early bird catches the term: combining twitter and news data for event detection and situational awareness\n", "abstract": " Twitter updates now represent an enormous stream of information originating from a wide variety of formal and informal sources, much of which is relevant to real-world events. They can therefore be highly useful for event detection and situational awareness applications. In this paper we apply customised filtering techniques to existing bio-surveillance algorithms to detect localised spikes in Twitter activity, showing that these correspond to real events with a high level of confidence. We then develop a methodology to automatically summarise these events, both by providing the tweets which best describe the event and by linking to highly relevant news articles. This news linkage is accomplished by identifying terms occurring more frequently in the event tweets than in a baseline of activity for the area concerned, and using these to search for news. We apply our methods to outbreaks of illness and events strongly affecting sentiment and are able to detect events verifiable by third party sources and produce high quality summaries. This study demonstrates linking event detection from Twitter with relevant online news to provide situational awareness. This builds on the existing studies that focus on Twitter alone, showing that integrating information from multiple online sources can produce useful analysis.", "num_citations": "30\n", "authors": ["279"]}
{"title": "Ranking twitter influence by combining network centrality and influence observables in an evolutionary model\n", "abstract": " Influential agents in networks play a pivotal role in information diffusion. Influence may rise or fall quickly over time and thus capturing this evolution of influence is of benefit to a varied number of application domains such as digital marketing, counter-terrorism or policing. In this paper, we investigate the influence of users in programming communities on Twitter.         We propose a new model for capturing both time-invariant influence and also temporal influence. The unified model is a combination of network topological methods and observation of influence-relevant events in the network. We provide an application of Hidden Markov Models (HMM) for capturing this effect on the network. There are many possible combinations of influence factors, hence we required a ground-truth for model configuration. We performed a primary survey of our population users to elicit their views on influential users. The survey\u00a0\u2026", "num_citations": "30\n", "authors": ["279"]}
{"title": "Two formal approaches for approximating noninterference properties\n", "abstract": " The formalisation of security properties for computer systems raises the problem of overcoming also in a formal setting the classical view according to which confidentiality is an absolute property stating the complete absence of any unauthorised disclosure of information. In this paper, we present two formal models in which the notion of noninterference, which is at the basis of a large variety of security properties defined in the recent literature, is approximated. To this aim, the definition of indistinguishability of process behaviour is replaced by a similarity notion, which introduces a quantitative measure \u03b5 of the behavioural difference among processes. The first model relies on a programming paradigm called Probabilistic Concurrent Constraint Programming, while the second one is presented in the setting of a probabilistic process algebra. In both models, appropriate notions of distance provide information\u00a0\u2026", "num_citations": "30\n", "authors": ["279"]}
{"title": "Generalised flowcharts and games\n", "abstract": " We introduce a generalization of the classical notion of flowchart for languages with higher order and object-oriented features. These general flowcharts are obtained by an abstraction of the game semantics for Idealized Algol and as such rely on a solid mathematical basis. We demonstrate how charts may be used as the basis for data flow analysis.", "num_citations": "28\n", "authors": ["279"]}
{"title": "Static analysis of routing protocols for ad-hoc networks\n", "abstract": " An ad-hoc network is a collection of wireless nodes which adhere to a communication principle without fixed infrastructure or a centralised control component. Instead, nodes have to rely on each other in order to forward packages to others beyond direct transmission range. Developing reliable routing protocols for these networks is a hard task, especially when studied in an untrusted environment. But new protocols are predominantly validated by an interpretation of simulation results, thus lacking a rigid formal analysis necessary for ensuring security. We present a step in the direction of formal protocol verification in this setting by proposing a quantitative extension to the process calculus CBS that allows us to model the behaviour of ad-hoc networks. We apply our development to the DSR routing protocol and develop a static analysis defined by a flow logic in order to track the messages sent in a network. The analysis is then used to evaluate the sensitivity of the network to the presence of an attacker trying to disrupt communication.", "num_citations": "27\n", "authors": ["279"]}
{"title": "A type-based framework for program analysis\n", "abstract": " In this paper we present a general framework for type-based analyses of functional programs. Our framework is a generalisation of our earlier work on strictness analysis and was inspired by Burn's logical framework. The framework is parameterised by a set of types to represent properties and interpretations for constants in the language. To construct a new analysis, the user needs only to supply a model for the types (which properties they denote) and sound rules for the constants. We identify the local properties that must be proven to guarantee the correctness of a specific analysis and algorithm. We illustrate the approach by recasting Hunt and Sand's binding time analysis in our framework. Furthermore we report on experimental results suggesting that our generic inference algorithm can provide the basis for an efficient program analyser.", "num_citations": "27\n", "authors": ["279"]}
{"title": "Effective cybersecurity is fundamental to patient safety\n", "abstract": " The global WannaCry ransomware attack has had a disproportionate effect on the UK healthcare sector, highlighting the poor state of cybersecurity in the NHS and the failure to recognise it as a fundamental matter for patient safety.WannaCry is trojan malware designed to extort money by holding files to ransom. It exploits a known vulnerability in the Windows operating system that was initially identified and patched by Microsoft in March 2017, with a further patch released after the event. 1 2 The attack is widely reported to have used system exploits released by the hacker group the Shadow Brokers but originating from the US National Security Agency. Once a computer is infected the malware creates encrypted copies of files before deleting the originals; the only way to retrieve affected data is to pay the bitcoin ransom. Computer systems across more than 150 countries have been infected, and only the fortuitous\u00a0\u2026", "num_citations": "26\n", "authors": ["279"]}
{"title": "Reversible combinatory logic\n", "abstract": " The -calculus that maintains irreversibility. Recently, reversible computational models have been studied mainly in the context of quantum computation, as (without measurements) quantum physics is inherently reversible. However, reversibility also fundamentally changes the semantical framework in which classical computation has to be investigated. We describe an implementation of classical combinatory logic in a reversible calculus for which we present an algebraic model based on a generalisation of the notion of a group.", "num_citations": "26\n", "authors": ["279"]}
{"title": "DEFENDER: detecting and forecasting epidemics using novel data-analytics for enhanced response\n", "abstract": " In recent years social and news media have increasingly been used to explain patterns in disease activity and progression. Social media data, principally from the Twitter network, has been shown to correlate well with official disease case counts. This fact has been exploited to provide advance warning of outbreak detection, forecasting of disease levels and the ability to predict the likelihood of individuals developing symptoms. In this paper we introduce DEFENDER, a software system that integrates data from social and news media and incorporates algorithms for outbreak detection, situational awareness and forecasting. As part of this system we have developed a technique for creating a location network for any country or region based purely on Twitter data. We also present a disease nowcasting (forecasting the current but still unknown level) approach which leverages counts from multiple symptoms, which was found to improve the nowcasting accuracy by 37 percent over a model that used only previous case data. Finally we attempt to forecast future levels of symptom activity based on observed user movement on Twitter, finding a moderate gain of 5 percent over a time series forecasting model.", "num_citations": "25\n", "authors": ["279"]}
{"title": "Formal security analysis for ad-hoc networks\n", "abstract": " In ad-hoc networks, autonomous wireless nodes can communicate by forwarding messages for each other. For routing protocols in this setting, it is known that a malicious node can perform a variety of attacks just by not behaving according to the specification. Whilst secure versions of routing protocols are under development, little effort has been made to formalise the scenario similarly to developments in the realm of traditional security protocols for secrecy and authentication. We present a broadcast process calculus suitable to describe the behaviour of protocols which require a local memory component for every node. By adding annotations for the origin of messages, we are able to formalise a vital security property in this context, called store authorisation. Furthermore, we describe a static analysis for the detection of violations of this property. For a model of the AODV protocol in our calculus, we are then able to\u00a0\u2026", "num_citations": "24\n", "authors": ["279"]}
{"title": "Quantitative static analysis of distributed systems\n", "abstract": " We introduce a quantitative approach to the analysis of distributed systems which relies on a linear operator based network semantics. A typical problem in a distributed setting is how information propagates through a network, and a typical qualitative analysis is concerned with establishing whether some information will eventually be transmitted from one node to another node in the network. The quantitative approach we present allows us to obtain additional information such as an estimation of the probability that some data is transmitted within a given interval of time. We formalise situations like this using a probabilistic version of a process calculus which is the core of KLAIM, a language for distributed and mobile computing based on interactions through distributed tuple spaces. The analysis we present exploits techniques based on Probabilistic Abstract Interpretation and is characterised by compositional\u00a0\u2026", "num_citations": "24\n", "authors": ["279"]}
{"title": "Multi-scale community detection using stability optimisation within greedy algorithms\n", "abstract": " Many real systems can be represented as networks whose analysis can be very informative regarding the original system's organisation. In the past decade community detection received a lot of attention and is now an active field of research. Recently stability was introduced as a new measure for partition quality. This work investigates stability as an optimisation criterion that exploits a Markov process view of networks to enable multi-scale community detection. Several heuristics and variations of an algorithm optimising stability are presented as well as an application to overlapping communities. Experiments show that the method enables accurate multi-scale network analysis.", "num_citations": "23\n", "authors": ["279"]}
{"title": "Quantifying timing leaks and cost optimisation\n", "abstract": " We develop a new notion of security against timing attacks where the attacker is able to simultaneously observe the execution time of a program and the probability of the values of low variables. We then show how to measure the security of a program with respect to this notion via a computable estimate of the timing leakage and use this estimate for cost optimisation.", "num_citations": "23\n", "authors": ["279"]}
{"title": "Discovery of anomalous behaviour in temporal networks\n", "abstract": " In this work we consider the problem of detecting anomalous behaviour and present a novel approach that allows \u2018behaviour\u2019 to be classified as either to be normal or abnormal by checking the p-value associated with the occurrence of the behaviour which is modelled following a binomial distribution within a discrete time model. We investigate the problem of detecting anomalous behaviour by looking at how communication evolves over time in a social network graph. Under the assumption that some nodes of the network could be labelled qualitatively, we present a novel approach that allows us to infer a subset of nodes of the social network which might share the same qualitative connotation. In other words, assuming one or more members belong to some criminal organisation, we wish to investigate how many other persons belong to the same organisation. We have tested our method in two datasets\u00a0\u2026", "num_citations": "22\n", "authors": ["279"]}
{"title": "A safe approach to parallel combinator reduction\n", "abstract": " In this paper we present the results of two pieces of work which, when combined, allow us to go from a program text in a functional language to a parallel implementation of that program. We present techniques for discovering sources of parallelism in a program at compile time, and then show how this parallelism is naturally mapped into a parallel combinator set that we will define.             To discover sources of parallelism in a program, we use abstract interpretation. Abstract interpretation is a compile-time technique which is used to gain information about a program that may then be used to optimise the execution of the program. A particular use of abstract interpretation is in strictness analysis of functional programs. In a language that has lazy semantics, the main potential for parallelism arises in the evaluation of operands of strict operators. A function is strict in an argument if its value is undefined whenever\u00a0\u2026", "num_citations": "22\n", "authors": ["279"]}
{"title": "Multi-scale community detection using stability optimisation\n", "abstract": " Many real systems can be represented as networks whose analysis can be very informative regarding the original system\u2019s organisation. In the past decade, community detection received a lot of attention and is now a very active field of research. Recently, stability was introduced as a new measure for partition quality. This work investigates stability as an optimisation criterion that exploits a Markov process view of networks to enable multi-scale community detection. Several heuristics and variations of an algorithm optimising stability are presented as well as an application to overlapping communities. Experiments show that the method enables accurate multi-scale network analysis.", "num_citations": "21\n", "authors": ["279"]}
{"title": "Continuous-time probabilistic KLAIM\n", "abstract": " The design of languages supporting network programming is a necessary step towards the formalisation of distributed and mobile computing. The existence of an abstract semantic framework constitutes the basis for a formal analysis of such systems. The KLAIM paradigm [5] provides such a semantic framework by introducing basic concepts and primitives addressing the key aspects of the coordination of interacting located processes. We extend this basic paradigm with probabilistic constructs with the aim of introducing a semantic basis for a quantitative analysis of networks. A quantitative analysis allows in general for the consideration of more \u201crealistic\u201d situations. For example, a probabilistic analysis allows for establishing the security of a system up to a", "num_citations": "19\n", "authors": ["279"]}
{"title": "Approximate fixed points in abstract interpretation\n", "abstract": " Much of the earlier development of abstract interpretation, and its application to imperative programming languages, has concerned techniques for finding fixed points in large (often infinite) lattices. The standard approach in the abstract interpretation of functional languages has been to work with small, finite lattices and this supposedly circumvents the need for such techniques. However, practical experience has shown that, in the presence of higher order functions, the lattices soon become too large (although still finite) for the fixed-point finding problem to be tractable. This paper develops some approximation techniques which were first proposed by Hunt and shows how these techniques relate to the earlier use of widening and narrowing operations by the Cousots.", "num_citations": "19\n", "authors": ["279"]}
{"title": "Lazy type inference and program analysis\n", "abstract": " Approaches to static analysis based on nonstandard type systems have received considerable interest recently. Most work has concentrated on the relationship between such analyses and abstract interpretation. In this paper, we focus on the problem of producing efficient algorithms from such type-based analyses. The key idea is the introduction of laziness into type inference. We present the basic notions in the context of a higher-order strictness analysis of list-processing functions. We also present a general framework for program analysis based on these ideas. We conclude with some experimental results.", "num_citations": "18\n", "authors": ["279"]}
{"title": "Cobweb\u2014A combinator reduction architecture\n", "abstract": " The work reported in this paper represents the convergence of ideas stemming from two areas of research. In the hardware field there has been significant interest in developing the techniques of Wafer Scale Integration to provide large assemblies of tightly-coupled simple processors that can act cooperatively in the execution of a task. In the software field there has been a growing awareness that declarative languages lead to higher programmer productivity and offer more potential for parallel evaluation than the traditional, imperative languages.             This paper describes the first machine in a family called COBWEB. The common denominator of the family is that all of the machines are targetted to supporting functional languages and all execute some form of combinator code. The first machine employs normal order reduction to evaluate pure functional programs. The next machine will use a parallel\u00a0\u2026", "num_citations": "18\n", "authors": ["279"]}
{"title": "A proposal for the JCVMLe operational semantics\n", "abstract": " We present the operational semantics of the JCVMLe language, a language that models the Java Card Virtual machine Language. We use the instruction set and the program structures proposed in [4]. We define a smallstep relation between program configurations, including rules for exception handling and subroutines. We also include the basic structures needed to model object ownership and the Java Card firewall. New versions of this document should contain:", "num_citations": "17\n", "authors": ["279"]}
{"title": "Cobweb-2: structured specification of a wafer-scale supercomputer\n", "abstract": " Using an informal presentation of the specification techniques being used in COBWEB-2's design, this paper has described the machine and the principles of its operation.             An on-wafer communications network with a fault-tolerant packet routing mechanism has been described. These techniques. Cartesian routing with convex wrapping, are a substantial improvement on Catt's spiral (used in COBWEB-1). The use of the network to increase \u201charvest\u201d without requiring very high yield processing elements provides the key to feasible wafer scale integration.", "num_citations": "17\n", "authors": ["279"]}
{"title": "The data flow programming language CAJOLE-an informal introduction\n", "abstract": " Data Flow represents a radical departure from the conventional approach to computer organisation. Rather than a centralised control unit that controls the sequencing of operations, the scheduling of instructions for execution is controlled by the flow of data between operations. In this way the control is distributed and several operations may be executed concurrently. This new approach has major implications for computer architecture and several \"data flow\" machines have been proposed in the literature.In this paper we present a low level graphical programming language that can be used to represent data flow programs. We also introduce a high level textual language, CAJOLE, that reflects the data flow philosophy. The notations used are simple and elegant in contrast to \"fat and flabby\" Von Neumann languages.With the advent of fast and cheap microprocessors, we are likely to see a vast increase in the\u00a0\u2026", "num_citations": "17\n", "authors": ["279"]}
{"title": "CPS-MT: a real-time cyber-physical system monitoring tool for security research\n", "abstract": " Monitoring systems are essential to understand and control the behaviour of systems and networks. Cyber-physical systems (CPS) are particularly delicate under that perspective since they involve real-time constraints and physical phenomena that are not usually considered in common IT solutions. Therefore, there is a need for publicly available monitoring tools able to contemplate these aspects. In this poster/demo, we present our initiative, called CPS-MT, towards a versatile, real-time CPS monitoring tool, with a particular focus on security research. We first present its architecture and main components, followed by a MiniCPS-based case study. We also describe a performance analysis and preliminary results. During the demo, we will discuss CPS-MT's capabilities and limitations for security applications.", "num_citations": "15\n", "authors": ["279"]}
{"title": "Reasoning about Gamma programs\n", "abstract": " We describe an axiomatic semantics for the parallel programming language Gamma based on a framework developed by Brookes. This framework is an extension of Floyd-Hoare logic that allows compositional reasoning about the interleaved execution of Gamma programs. We present a proof system for deriving valid assertions about Gamma programs and examine its practical use through an example.", "num_citations": "15\n", "authors": ["279"]}
{"title": "Measuring cyber-physical security in industrial control systems via minimum-effort attack strategies\n", "abstract": " In recent years, Industrial Control Systems (ICS) have become increasingly exposed to a wide range of cyber-physical attacks, having massive destructive consequences. Security metrics are therefore essential to assess and improve their security posture. In this paper, we present a novel ICS security metric based on AND/OR graphs and hypergraphs which is able to efficiently identify the set of critical ICS components and security measures that should be compromised, with minimum cost (effort) for an attacker, in order to disrupt the operation of vital ICS assets. Our tool, META4ICS (pronounced as metaphorics), leverages state-of-the-art methods from the field of logical satisfiability optimisation and MAX-SAT techniques in order to achieve efficient computation times. In addition, we present a case study where we have used our system to analyse the security posture of a realistic Water Transport Network\u00a0(WTN).", "num_citations": "14\n", "authors": ["279"]}
{"title": "Adversarial machine learning beyond the image domain\n", "abstract": " Machine learning systems have had enormous success in a wide range of fields from computer vision, natural language processing, and anomaly detection. However, such systems are vulnerable to attackers who can cause deliberate misclassification by introducing small perturbations. With machine learning systems being proposed for cyber attack detection such attackers are cause for serious concern. Despite this the vast majority of adversarial machine learning security research is focused on the image domain. This work gives a brief overview of adversarial machine learning and machine learning used in cyber attack detection and suggests key differences between the traditional image domain of adversarial machine learning and the cyber domain. Finally we show an adversarial machine learning attack on an industrial control system.", "num_citations": "14\n", "authors": ["279"]}
{"title": "Using machine learning to infer reasoning provenance from user interaction log data: based on the data/frame theory of sensemaking\n", "abstract": " The reconstruction of analysts\u2019 reasoning processes (reasoning provenance) during complex sensemaking tasks can support reflection and decision making. One potential approach to such reconstruction is to automatically infer reasoning from low-level user interaction logs. We explore a novel method for doing this using machine learning. Two user studies were conducted in which participants performed similar intelligence analysis tasks. In one study, participants used a standard web browser and word processor; in the other, they used a system called INVISQUE (Interactive Visual Search and Query Environment). Interaction logs were manually coded for cognitive actions based on captured think-aloud protocol and posttask interviews based on Klein, Phillips, Rall, and Pelusos\u2019s data/frame model of sensemaking as a conceptual framework. This analysis was then used to train an interaction frame mapper\u00a0\u2026", "num_citations": "14\n", "authors": ["279"]}
{"title": "Defense-in-depth vs. critical component defense for industrial control systems\n", "abstract": " Originally designed as self-contained and isolated networks, IndustrialControl Systems (ICS) have evolved to become increasingly interconnected with IT systems and other wider networks and services, which enables cyber attacks to sabotage the normal operation of ICS. This paper proposes a simulation of attackers and defenders, who have limited resources that must be applied to either advancing the technology they have available to them or attempting to attack (defend) the system. The objective is to identify the appropriate deployment of specific defensive strategy, such as Defense-in-depth and Critical Component Defense. The problem is represented as a strategic competitive optimisation problem, which is solved using a coevolutionary Particle Swarm Optimisation problem. Through the development of optimal defense strategies, it is possible to identify when each specific defensive strategies is most appropriate; where the optimal defensive strategy depends on the kind of attacker the system is expecting and the structure of the network.", "num_citations": "14\n", "authors": ["279"]}
{"title": "Probabilistic confinement in a declarative framework\n", "abstract": " We show how to formulate and analyse some security notions in the context of declarative programming. We concentrate on a particular class of security properties, namely the so-called confinement properties. Our reference language is concurrent constraint programming. We use a probabilistic version of this language (PCCP) to highlight via simple program examples the difference between probabilistic and nondeterministic confinement. The different role played by variables in imperative and constraint programming hinders a direct translation of the notion of confinement into our declarative setting. Therefore, we introduce the notion of identity confinement which is more appropriate for constraint languages. Finally, we present an approximating probabilistic semantics which can be used as a base for the analysis of confinement properties, and show its correctness with respect to the operational semantics of PCCP.", "num_citations": "14\n", "authors": ["279"]}
{"title": "A safe approach to parallel combinator reduction\n", "abstract": " In this paper we present the results of two pieces of work which, when combined, allow us to take a program text in a functional language and produce a parallel implementation of that program. We present techniques for discovering sources of parallelism in a program at compile time, and then show how this parallelism is naturally mapped into a parallel combinator set that we will define.To discover sources of parallelism in a program, we use abstract interpretation. Abstract interpretation is a compile-time technique which is used to gain information about a program that may then be used to optimise the execution of the program. A particular use of abstract interpretation is in strictness analysis of functional programs. In a language that has lazy semantics, the main potential for parallelism arises in the evaluation of operands of strict operators. A function is strict in an argument if its value is undefined whenever the\u00a0\u2026", "num_citations": "14\n", "authors": ["279"]}
{"title": "Safety of strictness analysis via term graph rewriting\n", "abstract": " A safe abstraction is presented for a restricted form of term graph rewriting. This abstraction can be seen as a formalisation of the rewrite system employed by the strictness analyser in the Concurrent Clean compiler. Programs written in a core functional language are interpreted as graph rewriting systems using a form of equational term graph rewriting due to Ariola and Arvind. Abstract graphs are defined by extending the signature of ordinary graphs and it is shown how to extend a rewriting system on ordinary graphs to one on abstract graphs. An abstraction relation between abstract graphs is used to define a notion of safety with respect to a variant of Ariola and Arvind\u2019s direct approximation semantics, and this notion of safety is shown to be adequate for strictness analysis. Abstract reduction is defined as the union of the extended rewrite system with additional \u2018heuristic\u2019 reductions and shown to be safe.", "num_citations": "13\n", "authors": ["279"]}
{"title": "A program logic for Gamma\n", "abstract": " We take a systematic approach to the construction of a program logic for Gamma, by applying Abramsky's domain theory in logical form to a denotational semantics of the language. Starting from a resumption semantics of Gamma, we are able to derive both the formulae and the proof system of the transition assertion logic previously proposed by Errington, Hankin and Jensen. The general theory enables us to prove soundness of the logic, although completeness fails because the resumption semantics of Gamma is not fully abstract. At the end of the paper we discuss the possibilities for obtaining a complete logic.", "num_citations": "13\n", "authors": ["279"]}
{"title": "Intrusion detection for industrial control systems: Evaluation analysis and adversarial attacks\n", "abstract": " Neural networks are increasingly used in security applications for intrusion detection on industrial control systems. In this work we examine two areas that must be considered for their effective use. Firstly, is their vulnerability to adversarial attacks when used in a time series setting. Secondly, is potential over-estimation of performance arising from data leakage artefacts. To investigate these areas we implement a long short-term memory (LSTM) based intrusion detection system (IDS) which effectively detects cyber-physical attacks on a water treatment testbed representing a strong baseline IDS. For investigating adversarial attacks we model two different white box attackers. The first attacker is able to manipulate sensor readings on a subset of the Secure Water Treatment (SWaT) system. By creating a stream of adversarial data the attacker is able to hide the cyber-physical attacks from the IDS. For the cyber-physical attacks which are detected by the IDS, the attacker required on average 2.48 out of 12 total sensors to be compromised for the cyber-physical attacks to be hidden from the IDS. The second attacker model we explore is an  bounded attacker who can send fake readings to the IDS, but to remain imperceptible, limits their perturbations to the smallest  value needed. Additionally, we examine data leakage problems arising from tuning for  score on the whole SWaT attack set and propose a method to tune detection parameters that does not utilise any attack data. If attack after-effects are accounted for then our new parameter tuning method achieved an  score of 0.8110.0103.", "num_citations": "12\n", "authors": ["279"]}
{"title": "Fast multi-scale detection of overlapping communities using local criteria\n", "abstract": " Many systems can be described using graphs, or networks. Detecting communities in these networks can provide information about the underlying structure and functioning of the original systems. Yet this detection is a complex task and a large amount of work was dedicated to it in the past decade. One important feature is that communities can be found at several scales, or levels of resolution, indicating several levels of organisation. Therefore a graph may have several community structures. Also networks tend to be large and hence require efficient processing. In this work, we present a new algorithm with linear complexity for the fast detection of communities across scales using a local criterion. We exploit the local aspect of the criterion to enable parallel computation and improve the execution speed further. The algorithm is tested against very large generated multi-scale networks and experiments\u00a0\u2026", "num_citations": "12\n", "authors": ["279"]}
{"title": "A systematic approach to probabilistic pointer analysis\n", "abstract": " We present a formal framework for syntax directed probabilistic program analysis. Our focus is on probabilistic pointer analysis. We show how to obtain probabilistic points-to matrices and their relational counterparts in a systematic way via Probabilistic Abstract Interpretation (PAI). The analysis is based on a non-standard semantics for a simple imperative language which corresponds to a Discrete-Time Markov Chain (DTMC). The generator of this DTMC is constructed by composing (via tensor product) the probabilistic control flow of the program and the data updates of the different variables at individual program points. The dimensionality of the concrete semantics is in general prohibitively large but abstraction (via PAI) allows for a drastic (exponential) reduction of size.", "num_citations": "12\n", "authors": ["279"]}
{"title": "Tempus fugit: How to plug it\n", "abstract": " Secret or private information may be leaked to an external attacker through the timing behaviour of the system running the untrusted code. After introducing a formalisation of this situation in terms of a confinement property, we present an algorithm which is able to transform the system into one that is computationally equivalent to the given system but free of timing leaks.", "num_citations": "12\n", "authors": ["279"]}
{"title": "Probabilistic Linda-based coordination languages\n", "abstract": " Coordination languages are intended to simplify the development of complex software systems by separating the coordination aspects of an application from its computational aspects. Coordination refers to the ways the independent active pieces of a program (e.g. a process, a task, a thread, etc.) communicate and synchronise with each other. We review various approaches to introducing probabilistic or stochastic features in coordination languages. The main objective of such a study is to develop a semantic basis for a quantitative analysis of systems of interconnected or interacting components, which allows us to address not only the functional (qualitative) aspects of a system behaviour but also its non-functional aspects, typically considered in the realm of performance modelling and evaluation.", "num_citations": "12\n", "authors": ["279"]}
{"title": "Fast multi-scale community detection based on local criteria within a multi-threaded algorithm\n", "abstract": " Many systems can be described using graphs, or networks. Detecting communities in these networks can provide information about the underlying structure and functioning of the original systems. Yet this detection is a complex task and a large amount of work was dedicated to it in the past decade. One important feature is that communities can be found at several scales, or levels of resolution, indicating several levels of organisations. Therefore solutions to the community structure may not be unique. Also networks tend to be large and hence require efficient processing. In this work, we present a new algorithm for the fast detection of communities across scales using a local criterion. We exploit the local aspect of the criterion to enable parallel computation and improve the algorithm's efficiency further. The algorithm is tested against large generated multi-scale networks and experiments demonstrate its efficiency and accuracy.", "num_citations": "11\n", "authors": ["279"]}
{"title": "Refining multiset transformers\n", "abstract": " Gamma is a minimal language based on local multiset rewriting with an elegant chemical reaction metaphor. The virtues of this paradigm in terms of systematic program construction and design of parallel programs have been argued in previous papers. Gamma can also be seen as a notation for coordinating independent programs in a larger application. In this paper, we study a notion of refinement for programs involving parallel and sequential composition operators, and derive a number of programming laws. The calculus thus obtained is applied in the development of a generic \u201cpipelining\u201d transformation, which enables certain sequential compositions to be refined into parallel compositions.", "num_citations": "11\n", "authors": ["279"]}
{"title": "Identifying security-critical cyber-physical components in industrial control systems\n", "abstract": " In recent years, Industrial Control Systems (ICS) have become an appealing target for cyber attacks, having massive destructive consequences. Security metrics are therefore essential to assess their security posture. In this paper, we present a novel ICS security metric based on AND/OR graphs that represent cyber-physical dependencies among network components. Our metric is able to efficiently identify sets of critical cyber-physical components, with minimal cost for an attacker, such that if compromised, the system would enter into a non-operational state. We address this problem by efficiently transforming the input AND/OR graph-based model into a weighted logical formula that is then used to build and solve a Weighted Partial MAX-SAT problem. Our tool, META4ICS, leverages state-of-the-art techniques from the field of logical satisfiability optimisation in order to achieve efficient computation times. Our experimental results indicate that the proposed security metric can efficiently scale to networks with thousands of nodes and be computed in seconds. In addition, we present a case study where we have used our system to analyse the security posture of a realistic water transport network. We discuss our findings on the plant as well as further security applications of our metric.", "num_citations": "10\n", "authors": ["279"]}
{"title": "A reference architecture for IIoT and industrial control systems testbeds\n", "abstract": " Conducting cyber security research within live operational technology and industrial Internet of Things environments is, understandably, not practical and as such research needs to be undertaken within non-live mimics or testbeds. However, testbeds and especially those which are built using real-world infrastructure are expensive to develop and maintain. Moreover, such testbeds tend to be representative of a single industry vertical (often based upon the skill set or research focus) and built in isolation. In this paper we present a reference architecture, developed whilst designing and building the Bristol Cyber Security Group ICS/IIoT testbed for critical national infrastructure security research.", "num_citations": "10\n", "authors": ["279"]}
{"title": "Effective defence against zero-day exploits using bayesian networks\n", "abstract": " Industrial Control Systems (ICS) play a crucial role in controlling industrial processes. Unlike conventional IT systems or networks, cyber attacks against ICS can cause destructive physical damage. Zero-day exploits (i.e. unknown exploits) have demonstrated their essential contributions to causing such damage by Stuxnet. In this work, we investigate the possibility of improving the tolerance of a system against zero-day attacks by defending against known weaknesses of the system. We first propose a metric to measure the system tolerance against zero-day attacks, which is the minimum effort required by zero-day exploits to compromise a system. We then apply this metric to evaluate different defensive plans to decide the most effective one in maximising the system tolerance against zero-day attacks. A case study about ICS security management is demonstrated in this paper.", "num_citations": "10\n", "authors": ["279"]}
{"title": "Probabilistic semantics and program analysis\n", "abstract": " The aims of these lecture notes are two-fold: (i) we investigate the relation between the operational semantics of probabilistic programming languages and Discrete Time Markov Chains (DTMCs), and (ii) we present a framework for probabilistic program analysis which is inspired by the classical Abstract Interpretation framework by Cousot & Cousot and which we introduced as Probabilistic Abstract Interpretation (PAI) in [1]. The link between programming languages and DTMCs is the construction of a so-called Linear Operator semantics (LOS) in a syntax-directed or compositional way. The main element in this construction is the use of tensor product to combine information about different aspects of a program. Although this inevitably results in a combinatorial explosion of the size of the semantics of program, the PAI approach allows us to keep some control and to obtain reasonably sized abstract models.", "num_citations": "10\n", "authors": ["279"]}
{"title": "Program analysis tools\n", "abstract": " This paper introduces a special section on program analysis tools. We start by de ning what we mean by\\program analysis\". We identify and brie y describe three approaches to program analysis. We then introduce the three contributed papers of the section.", "num_citations": "10\n", "authors": ["279"]}
{"title": "Efficient numerical frameworks for multi-objective cyber security planning\n", "abstract": " We consider the problem of optimal investment in cyber-security by an enterprise. Optimality is measured with respect to the overall (1)\u00a0monetary cost of implementation, (2)\u00a0negative side-effects of cyber-security controls (indirect costs), and (3)\u00a0mitigation of the cyber-security risk. We consider \u201cpassive\u201d and \u201creactive\u201d threats, the former representing the case where attack attempts are independent of the defender\u2019s plan, the latter, where attackers can adapt and react to an implemented cyber-security defense. Moreover, we model in three different ways the combined effect of multiple cyber-security controls, depending on their degree of complementarity and correlation. We also consider multi-stage attacks and the potential correlations in the success of different stages. First, we formalize the problem as a non-linear multi-objective integer programming. We then convert them into Mixed Integer Linear\u00a0\u2026", "num_citations": "9\n", "authors": ["279"]}
{"title": "Multi-scale community detection using stability optimisation within greedy algorithms\n", "abstract": " Many real systems can be represented as networks whose analysis can be very informative regarding the original system\u2019s organisation. In the past decade community detection received a lot of attention and is now an active field of research. Recently stability was introduced as a new measure for partition quality. This work investigates stability as an optimisation criterion that exploits a Markov process view of networks to enable multi-scale community detection. Several heuristics and variations of an algorithm optimising stability are presented as well as an application to overlapping communities. Experiments show that the method enables accurate multi-scale network analysis.", "num_citations": "9\n", "authors": ["279"]}
{"title": "Probabilistic timing covert channels: to close or not to close?\n", "abstract": " We develop a new notion of security against timing attacks where the attacker is able to simultaneously observe the execution time of a program and the probability of the values of low variables. We then propose an algorithm which computes an estimate of the security of a program with respect to this notion in terms of timing leakage and show how to use this estimate for cost optimisation.", "num_citations": "9\n", "authors": ["279"]}
{"title": "Abstract interpretation for worst and average case analysis\n", "abstract": " We review Wilhelm\u2019s work on WCET for hard real-time applications and also recent work on analysis of soft-real time systems using probabilistic methods. We then present Probabilistic Abstract Interpretation (PAI) as a quantitative variation of the classical approach; PAI aims to provide close approximations \u2013 this should be contrasted to the safe approximations studied in the standard setting. We discuss the relation between PAI and classical Abstract Interpretation as well as average case analysis.", "num_citations": "9\n", "authors": ["279"]}
{"title": "Lazy type inference for the strictness analysis of lists\n", "abstract": " We present a type inference system for the strictness analysis of lists and we show that it can be used as the basis for an efficient algorithm. The algorithm is as accurate as the usual abstract interpretation technique. One distinctive advantage of this approach is that it is not necessary to impose an abstract domain of a particular depth prior to the analysis: the lazy type algorithm will instead explore the part of a potentially infinite domain required to prove the strictness property.", "num_citations": "9\n", "authors": ["279"]}
{"title": "Analysing approximate confinement under uniform attacks\n", "abstract": " We are concerned to give certain guarantees about the security of a system. We identify two kinds of attack: the internally scheduled attack (exemplified by Trojan Horse attacks) and externally scheduled attacks (exemplified by timing attacks). In this paper we focus on the latter. We present a semantic framework for studying such attacks in the context of PCCP, a simple process algebra with a constraint store. We show that a measure of the efficacy of an attacker can be determined by considering its observable behaviour over the \u201d average\u201d store of the system (for some number of steps). We show how to construct an analysis to determine the average store using the technique of probabilistic abstract interpretation.", "num_citations": "8\n", "authors": ["279"]}
{"title": "Position statements on strategic directions for research on programming languages\n", "abstract": " One of the working groups at the workshop on Strategic Directions in Computing Research was concerned with Programming Languages. We summarise the strategic directions identified by the group and we introduce 16 of the position statements that formed the basis for the report. These statements can then be found at the following pages.", "num_citations": "8\n", "authors": ["279"]}
{"title": "Static Analysis of Value-Passing Process Calculi.\n", "abstract": " Standard approaches to providing a semantics for value passing process calculi involve an expansion into a basic calculus which only involves pure synchronisation. When the semantics is intended to provide a basis for static analysis of properties of the values, this expansion leads to an unacceptable loss of information. In this paper we present a model of value passing process calculi based on synchronisation trees which gives a direct treatment of value passing. We present an abstract interpretation which discovers the same information about programs as Schreiber's flow\u00b7 analysis.", "num_citations": "8\n", "authors": ["279"]}
{"title": "Improving ICS cyber resilience through optimal diversification of network resources\n", "abstract": " Network diversity has been widely recognized as an effective defense strategy to mitigate the spread of malware. Optimally diversifying network resources can improve the resilience of a network against malware propagation. This work proposes an efficient method to compute such an optimal deployment, in the context of upgrading a legacy Industrial Control System with modern IT infrastructure. Our approach can tolerate various constraints when searching for an optimal diversification, such as outdated products and strict configuration policies. We explicitly measure the vulnerability similarity of products based on the CVE/NVD, to estimate the infection rate of malware between products. A Stuxnet-inspired case demonstrates our optimal diversification in practice, particularly when constrained by various requirements. We then measure the improved resilience of the diversified network in terms of a well-defined diversity metric and Mean-time-to-compromise (MTTC), to verify the effectiveness of our approach. We further evaluate three factors affecting the performance of the optimization, such as the network structure, the variety of products and constraints. Finally, we show the competitive scalability of our approach in finding optimal solutions within a couple of seconds to minutes for networks of large scales (up to 10,000 hosts) and high densities (up to 240,000 edges).", "num_citations": "7\n", "authors": ["279"]}
{"title": "Game theory and industrial control systems\n", "abstract": " Post-Stuxnet, the last couple of years has seen an increasing awareness of cyber threats to industrial control systems (ICS). We will review why these threats have become more prominent. We will explore the differences between Enterprise IT security and cyber security of ICS. Game Theory has been used to provide decision support in cyber security for a number of years. Recently, we have developed a hybrid approach using game theory and classical optimisation to produce decision support tools to help system administrators optimise their investment in cyber defence. We will describe how our game theoretic work might be used to provide novel approaches to protecting ICS against cyber attacks.", "num_citations": "7\n", "authors": ["279"]}
{"title": "Flow analysis: games and nets\n", "abstract": " This paper presents a graph-based formulation of control- flow analysis using results from game semantics and proof-nets. Control- flow analysis aims to give a conservative prediction of the flow of control in a program. In our analysis, terms are represented by proof-nets and control-flow analysis amounts to the analysis of computation paths in the proof-net. We focus on a context free analysis known in the literature as 0-CFA, and develop an algorithm for the analysis. The algorithm for 0- CFA performs dynamic transitive closure of a graph that is based on the judgement associated with the proof-net. Correctness of the algorithm relies on the correspondence between proof-nets and certain kinds of strategies in game semantics.", "num_citations": "7\n", "authors": ["279"]}
{"title": "Strategic directions in research on programming languages\n", "abstract": " A good programming language is a conceptual universe for thinking about programming. ALAN PERLIS NATO Conference on Software Engineering Techniques Rome, 1969 Programming language research covers a broad spectrum from systems work to theory. On the theoretical side, programming language research has its roots in the work of Sch\u00f6nfinkel, Curry, Kleene, and Church in the 1920s and 1930s. On the practical side, it has been influenced by developments in hardware from the the von Neumann, architecture of the 1950s to today\u2019s heterogeneous distributed networks. In addition, developments in software engineering with its requirements of feasibility, reliability, and performance have had a great impact on the area. A feature of programming language research that gives it much of its excitement is that individual researchers have the oppor-", "num_citations": "7\n", "authors": ["279"]}
{"title": "A congruence for Gamma programs\n", "abstract": " This paper defines a congruence relation on Gamma programs. Based on this congruence relation, laws for transforming programs are derived. We define an axiomatic semantics for Gamma based on Brookes' transition assertions. The definition of the congruence is in terms of provable satisfiability of such assertions. We consider the relationship between our congruence and other orderings that have been proposed in the literature.", "num_citations": "7\n", "authors": ["279"]}
{"title": "MaxSAT Evaluation 2019--Benchmark: Identifying Security-Critical Cyber-Physical Components in Weighted AND/OR Graphs\n", "abstract": " This paper presents a MaxSAT benchmark focused on identifying critical nodes in AND/OR graphs. We use AND/OR graphs to model Industrial Control Systems (ICS) as they are able to semantically grasp intricate logical interdependencies among ICS components. However, identifying critical nodes in AND/OR graphs is an NP-complete problem. We address this problem by efficiently transforming the input AND/OR graph-based model into a weighted logical formula that is then used to build and solve a Weighted Partial MaxSAT problem. The benchmark includes 80 cases with AND/OR graphs of different size and composition as well as the optimal cost and solution for each case.", "num_citations": "6\n", "authors": ["279"]}
{"title": "Modelling cost-effectiveness of defenses in industrial control systems\n", "abstract": " Industrial Control Systems (ICS) play a critical role in controlling industrial processes. Wide use of modern IT technologies enables cyber attacks to disrupt the operation of ICS. Advanced Persistent Threats (APT) are the most threatening attacks to ICS due to their long persistence and destructive cyber-physical effects to ICS. This paper considers a simulation of attackers and defenders of an ICS, where the defender must consider the cost-effectiveness of implementing defensive measures within the system in order to create an optimal defense. The aim is to identify the appropriate deployment of a specific defensive strategy, such as defense-in-depth or critical component defense. The problem is represented as a strategic competitive optimisation problem, which is solved using a co-evolutionary particle swarm optimisation algorithm. Through the development of optimal defense strategy, it is possible to\u00a0\u2026", "num_citations": "6\n", "authors": ["279"]}
{"title": "Fast multi-scale detection of relevant communities\n", "abstract": " Nowadays, networks are almost ubiquitous. In the past decade, community detection received an increasing interest as a way to uncover the structure of networks by grouping nodes into communities more densely connected internally than externally. Yet most of the effective methods available do not consider the potential levels of organisation, or scales, a network may encompass and are therefore limited. In this paper we present a method compatible with global and local criteria that enables fast multi-scale community detection. The method is derived in two algorithms, one for each type of criterion, and implemented with 6 known criteria. Uncovering communities at various scales is a computationally expensive task. Therefore this work puts a strong emphasis on the reduction of computational complexity. Some heuristics are introduced for speed-up purposes. Experiments demonstrate the efficiency and accuracy of our method with respect to each algorithm and criterion by testing them against large generated multi-scale networks. This study also offers a comparison between criteria and between the global and local approaches.", "num_citations": "6\n", "authors": ["279"]}
{"title": "Possibilistic information flow is safe for probabilistic non-interference\n", "abstract": " ) David Clark 1, Chris Hankin 1, Sebastian Hunt 2, and Rajagopal Nagarajan 1 1 Introduction Recently there has been considerable interest in analysing programs for the existence of probabilistic covert channels. Sabelfeld and Sands [6] have given a semantic characterisation of probabilistic non-interference and used it to give elegant correctness proofs for type-based systems proposed by Volpano and Smith [7]. Our thesis in this paper is that one can use a possibilistic information ow analysis to check for probabilistic interference| the absence of information ow implies the absence of interference. We apply our techniques to the analysis of a Probabilistic Idealised Algol. Our analysis is based on a control ow analysis which is used to construct an analogue of the the higher-order owcharts of Malacaria and Hankin [3]; the information ows can be read fairly directly from the owchart. In the next section we introduce the syntax and semantics of the programming language....", "num_citations": "6\n", "authors": ["279"]}
{"title": "Programming Languages and Systems 7th European Symposium on Programming, ESOP'98 Held as Part of the Joint European Conferences on Theory and Practice of Software, ETAPS'98\u00a0\u2026\n", "abstract": " This volume presents 17 revised full papers selected from a total of 59 submissions; also included is one invited paper. Among the issues addressed are software specification and verification, programming paradigms, semantics for formal development and implementation, program analysis, program transformation, etc.", "num_citations": "6\n", "authors": ["279"]}
{"title": "Gamma and the logic of transition traces\n", "abstract": " Gamma is a language of conditional multiset rewrites, which can be seen either as a parallel programming language or as a specification language for parallel algorithms. Taking the latter view, we are interested in program refinement and formal techniques for reasoning about it. In the present paper we apply Abramsky's framework of domain theory in logical form, to systematically develop a program logic for Gamma from a denotational semantics. Our semantics is a domain-theoretic reformulation of the transition trace semantics, which was defined for Gamma by Sands and based on earlier work by Brookes. We obtain a logic and proof system which is sound for our chosen notion of operational approximation or refinement.", "num_citations": "6\n", "authors": ["279"]}
{"title": "Approximate fixed points in abstract interpretation\n", "abstract": " Much of the earlier development of abstract interpretation, and its application to imperative programming languages, has concerned techniques for finding fixed points in large (often infinite) lattices. The standard approach in the abstract interpretation of functional languages has been to work with small, finite lattices and this supposedly circumvents the need for such techniques. However, practical experience has shown that, in the presence of higher-order functions, the lattices soon become too large (although still finite) for the fixed point finding problem to be tractable. This paper develops some approximation techniques which were first proposed by Hunt and shows how these techniques relate to the earlier use of widening and narrowing operations by the Cousots.", "num_citations": "6\n", "authors": ["279"]}
{"title": "Abstract reduction: Towards a theory via abstract interpretation\n", "abstract": " Abstract reduction | Term graph rewriting ACM Digital Library home ACM home Google, Inc. (search) Advanced Search Browse About Sign in Register Advanced Search Journals Magazines Proceedings Books SIGs Conferences People More Search ACM Digital Library SearchSearch Advanced Search Browse Browse Digital Library Collections More HomeBrowse by TitleBooksTerm graph rewriting: theory and practiceAbstract reduction: towards a theory via abstract interpretation chapter Abstract reduction: towards a theory via abstract interpretation Share on Authors: MCJD Van Eekelen profile image Marko van Eekelen View Profile , \u00c9ric Goubault profile image Eric Goubault View Profile , Chris L Hankin profile image Chris Hankin View Profile , Eric GJMH Nocker profile image Eric N\u00f6cker View Profile Authors Info & Affiliations Publication: Term graph rewriting: theory and practiceJanuary 1993 Pages 117\u2013129 10 .\u2026", "num_citations": "6\n", "authors": ["279"]}
{"title": "Parallel object-oriented descriptions of graph reduction machines\n", "abstract": " Abstract machine descriptions of parallel computer architectures must capture communications and concurrency characteristics at a high level. Current design techniques and notations are weak in this respect. We present a layered method for refinement of a requirements specification through to a detailed systems architecture design.             This paper concentrates on the two highest layers, the logical model, which is a requirements statement, and the systems architecture, which specifies logical processes and explicit communications. While requirements are expressed in a language that matches the problem domain, we suggest that a parallel object-oriented notation is most appropriate for the systems architecture layer. Refinements within this layer reflect implementation details (eg. structure sharing and distribution of work among processing elements). We introduce a parallel object-oriented notation\u00a0\u2026", "num_citations": "6\n", "authors": ["279"]}
{"title": "On probabilistic techniques for data flow analysis\n", "abstract": " We present a semantics-based technique for analysing probabilistic properties of imperative programs. This consists in a probabilistic version of classical data flow analysis. We apply this technique to pWhile programs, i.e programs written in a probabilistic version of a simple While language. As a first step we introduce a syntax based definition of a linear operator semantics (LOS) which is equivalent to the standard structural operational semantics of While. The LOS of a pWhile program can be seen as the generator of a Discrete Time Markov Chain and plays a similar role as a collecting or trace semantics for classical While. Probabilistic Abstract Interpretation techniques are then employed in order to define data flow analyses for properties like Parity and Live Variables.", "num_citations": "5\n", "authors": ["279"]}
{"title": "Measuring the confinement of concurrent probabilistic systems\n", "abstract": " In previous work we have studied the notion of approximate confinement based on I/O observables. In this paper, we generalise this work to consider probabilistic bisimulation (both exact and approximate) in the context of probabilistic transition systems. We also show the relationship between probabilistic bisimulation and probabilistic abstract interpretation; this result parallels Schmidt's result in the classical (nonprobabilistic) setting where he relates bisimulation and Galois connections.", "num_citations": "5\n", "authors": ["279"]}
{"title": "Theory and Formal Methods of Computing 94: Proceedings of the Second Imperial College Workshop\n", "abstract": " The focus of this workshop was the development of mathematically-based techniques of formal specification of system behaviour, and the systematic development of implementations. The aim is to produce correct, efficient implementations in a reliable fashion. Topics covered at the workshop include category theory, logic, domain theory, semantics, concurrency, specification and verification. The papers published here range from the purely theoretical to practical applications.", "num_citations": "5\n", "authors": ["279"]}
{"title": "A lattice for the abstract interpretation of term graph rewriting systems\n", "abstract": " A lattice for the abstract interpretation of term graph rewriting systems | Term graph rewriting ACM Digital Library home ACM home Google, Inc. (search) Advanced Search Browse About Sign in Register Advanced Search Journals Magazines Proceedings Books SIGs Conferences People More Search ACM Digital Library SearchSearch Advanced Search Browse Browse Digital Library Collections More HomeBrowse by TitleBooksTerm graph rewriting: theory and practiceA lattice for the abstract interpretation of term graph rewriting systems chapter A lattice for the abstract interpretation of term graph rewriting systems Share on Authors: E. Goubault View Profile , CL Hankin View Profile Authors Info & Affiliations Publication: Term graph rewriting: theory and practiceJanuary 1993 Pages 131\u2013140 0citation 0 Downloads Metrics Total Citations0 Total Downloads0 Last 12 Months0 Last 6 weeks0 Get Citation Alerts New ! be \u2026", "num_citations": "5\n", "authors": ["279"]}
{"title": "An operational semantics for Paragon: A design notation for parallel architectures\n", "abstract": " The need to design and verify architectures to support parallel implementations of declarative languages has led to the development of a novel language, called Paragon, which bridges the gap between the top-level specification of the abstract machine, and its detailed implementation in terms of parallel processes and message passing.               The central technical contributions in this paper are:                                        \u2022 The introduction and specification of Paragon, a parallel object-oriented language based on graph rewriting and message passing principles.                                                           \u2022 An illustration of the approach at work in the design of a parallel supercombinator graph reduction machine.                                                           \u2022 A sketch proof that this design meets the requirements statement.", "num_citations": "5\n", "authors": ["279"]}
{"title": "Static analysis of term graph rewriting systems\n", "abstract": " In this paper we present a framework for the abstract interpretation of term graph rewriting systems. The framework is based on the approach taken by the Cousots for flowchart programs. We give an example of the use of the framework by presenting an interpretation which performs a form of type inference.", "num_citations": "5\n", "authors": ["279"]}
{"title": "Parallel object-oriented descriptions of graph reduction machines\n", "abstract": " Abstract machine descriptions of parallel computer architectures must capture communications and concurrency characteristics at a high level. Current design techniques and notations are weak in this respect. We present a layered method for refinement of a requirements specification through to a detailed systems architecture design.This paper concentrates on the two highest layers, the logical model, which is a requirements statement, and the systems architecture, which specifies logical processes and explicit communications. While requirements are expressed in a language that matches the problem domain, we suggest that a parallel object-oriented notation is most appropriate for the systems architecture layer. Refinements within this layer reflect implementation details (e.g. structure sharing and distribution of work among processing elements). We introduce a parallel object-oriented notation based on\u00a0\u2026", "num_citations": "5\n", "authors": ["279"]}
{"title": "Fault Tree Analysis: Identifying Maximum Probability Minimal Cut Sets with MaxSAT\n", "abstract": " In this paper, we present a novel MaxSAT-based technique to compute Maximum Probability Minimal Cut Sets (MPMCSs) in fault trees. We model the MPMCS problem as a Weighted Partial MaxSAT problem and solve it using a parallel SAT-solving architecture. The results obtained with our open source tool indicate that the approach is effective and efficient.", "num_citations": "4\n", "authors": ["279"]}
{"title": "Comparing decision support approaches for cyber security investment\n", "abstract": " When investing in cyber security resources, information security managers have to follow effective decision-making strategies. We refer to this as the cyber security investment challenge. In this paper, we consider three possible decision-support methodologies for security managers to tackle this challenge. We consider methods based on game theory, combinatorial optimisation and a hybrid of the two. Our modelling starts by building a framework where we can investigate the effectiveness of a cyber security control regarding the protection of different assets seen as targets in presence of commodity threats. In terms of game theory we consider a 2-person control game between the security manager who has to choose among different implementation levels of a cyber security control, and a commodity attacker who chooses among different targets to attack. The pure game theoretical methodology consists of a large game including all controls and all threats. In the hybrid methodology the game solutions of individual control-games along with their direct costs (e.g. financial) are combined with a knapsack algorithm to derive an optimal investment strategy. The combinatorial optimisation technique consists of a multi-objective multiple choice knapsack based strategy. We compare these approaches on a case study that was built on SANS top critical controls. The main achievements of this work is to highlight the weaknesses and strengths of different investment methodologies for cyber security, the benefit of their interaction, and the impact that indirect costs have on cyber security investment.", "num_citations": "4\n", "authors": ["279"]}
{"title": "On a probabilistic chemical abstract machine and the expressiveness of linda languages\n", "abstract": " The Chemical Abstract Machine (CHAM) of Berry and Boudol provides a commonly accepted, uniform framework for describing the operational semantics of various process calculi and languages, such as for example CCS, the \u03c0 calculus and coordination languages like Linda. In its original form the CHAM is purely non-deterministic and thus only describes what reactions are possible but not how long it will take (in the average) before a certain reaction takes place or its probability. Such quantitative information is however often vital for \u201creal world\u201d applications such as systems biology or performance analysis. We propose a probabilistic version of the CHAM. We then define a linear operator semantics for the probabilistic CHAM which exploits a tensor product representation for distributions over possible solutions. Based on this we propose a novel approach towards comparing the expressive power of\u00a0\u2026", "num_citations": "4\n", "authors": ["279"]}
{"title": "A lattice of abstract graphs\n", "abstract": " This paper concerns the abstract interpretation of Term Graph Rewriting systems. We introduce a new lattice of abstract graphs; this improves over previous work in that the ordering is defined directly on the graph structure rather than via unravelling into trees. We use the lattice in sketching the correctness of N\u00f6cker's abstract reduction algorithm; an efficient strictness analysis used in the Concurrent Clean system.", "num_citations": "4\n", "authors": ["279"]}
{"title": "Structured data flow programming\n", "abstract": " Although both structured programming (particularly top-down stepwise refinement) [4,7,17] and a data flow view of parallel processing [1,8,11] have been with us now for many years, no attempt has been made to unite the two fields: This paper describes an experiment that unites the two areas in both a textual and a graphical framework. Two packages are described. One produces a Cajole program [13] from input consisting of high-level function definitions together with refinements; and the other produces machine code for a data flow machine from a two dimensional graphical description of a programming problem. An example of each of the systems is given.", "num_citations": "4\n", "authors": ["279"]}
{"title": "Payoffs, intensionality and abstraction in games\n", "abstract": " We discuss some fundamental concepts in Game Theory: the concept of payoffs and the relation between rational solutions to games like Nash equilibrium and real world behaviour. We sketch some connections between Game Theory and Game Semantics by exploring some possible uses of Game Semantics strategies enriched with payoffs. Finally we discuss potential contributions of Abstract Interpretation to Game Theory in addressing the state explosion problem of game models of real world systems.", "num_citations": "3\n", "authors": ["279"]}
{"title": "Program analysis probably counts\n", "abstract": " Semantics-based program analysis uses an abstract semantics of programs/systems to statically determine run-time properties. Classic examples from compiler technology include analyses to support constant propagation and constant folding transformations and estimation of pointer values to prevent buffer overruns. More recent examples include the estimation of information flows (to enforce security constraints) and estimation of non-functional properties such as timing (to determine worst case execution times in hard real-time applications). The classical approaches are based on semantics involving discrete mathematics. Paralleling trends in model-checking, there have been recent moves towards using probabilistic and quantitative methods in program analysis. In this paper we start by reviewing both classical and probabilistic/quantitative approaches to program analysis. We shall provide a comparison of the\u00a0\u2026", "num_citations": "3\n", "authors": ["279"]}
{"title": "Program analysis games\n", "abstract": " Programming Language Semantics has been an active area of research in Computer Science since the 1960s. However, the work has had disappointingly little impact on Computing practice. No commercially successful language has been designed using formal semantics and some even attempt to make a virtue out of the lack of a formal basis. Semantics is seen as something of a black art that is beyond the grasp of the typical programmer.This grim picture has serious long-term implications for the funding of research in this area. One successful application of the work on semantics has been its use in the development of semantics-based program analysis techniques [8]. The work on abstract interpretation, control flow analysis and type (and effect) systems is based on semantics and has already demonstrated significant economic and social benefits. The techniques have notably been used in verifying mission\u00a0\u2026", "num_citations": "3\n", "authors": ["279"]}
{"title": "Theoretical aspects of coordination languages\n", "abstract": " The emergence of high bandwidth network technology has fuelled the development of distributed computing and concurrent programming. Coordination languages are a new class of programming languages which offer a solution to the problem of managing the interaction among concurrent programs. Gelemter and Carrier0 coined the term Coordination in the following slogan:", "num_citations": "3\n", "authors": ["279"]}
{"title": "Applicative Languages and Data Flow\n", "abstract": " Applicative Languages and Data Flow - ePrints Soton The University of Southampton Courses University life Research Business Global About Visit Alumni Departments News Events Contact \u00d7 Search the Site Search Filter your search: All Courses Projects Staff University of Southampton Institutional Repository Search Advanced Search Policies & Help Latest Download Statistics Browse by Year Browse by Divisions LeftRight Applicative Languages and Data Flow Till, DR, Hankin, CL and Glaser, H. (1987) Applicative Languages and Data Flow. Eisenbach, S. (ed.) Functional Programming: Languages, Tools and Architectures. pp. 128-140 . Record type: Conference or Workshop Item (Other) Full text not available from this repository. More information Published date: 1987 Venue - Dates: Functional Programming: Languages, Tools and Architectures, 1987-01-01 Organisations: Web & Internet Science Identifiers Local \u2026", "num_citations": "3\n", "authors": ["279"]}
{"title": "Trustworthy Inter-connected Cyber-Physical Systems\n", "abstract": " In this paper we identify some of the particular challenges that are encountered when trying to secure cyber-physical systems. We describe three of our current activities: the architecture of a system for monitoring cyber-physical systems; a new approach to modelling dependencies in such systems which leads to a measurement of the security of the system \u2013 interpreted as the least effort that an attacker has to expend to compromise the operation; and an approach to optimising the diversity of products used in a system with a view to slowing the propagation of malware. We conclude by discussing how these different threads of work contribute to meeting the challenges and identify possible avenues for future development, as well as providing some pointers to other work.", "num_citations": "2\n", "authors": ["279"]}
{"title": "MaxSAT Evaluation 2020--Benchmark: Identifying Maximum Probability Minimal Cut Sets in Fault Trees\n", "abstract": " This paper presents a MaxSAT benchmark focused on the identification of Maximum Probability Minimal Cut Sets (MPMCSs) in fault trees. We address the MPMCS problem by transforming the input fault tree into a weighted logical formula that is then used to build and solve a Weighted Partial MaxSAT problem. The benchmark includes 80 cases with fault trees of different size and composition as well as the optimal cost and solution for each case.", "num_citations": "2\n", "authors": ["279"]}
{"title": "Deep latent defence\n", "abstract": " Deep learning methods have shown state of the art performance in a range of tasks from computer vision to natural language processing. However, it is well known that such systems are vulnerable to attackers who craft inputs in order to cause misclassification. The level of perturbation an attacker needs to introduce in order to cause such a misclassification can be extremely small, and often imperceptible. This is of significant security concern, particularly where misclassification can cause harm to humans. We thus propose Deep Latent Defence, an architecture which seeks to combine adversarial training with a detection system. At its core Deep Latent Defence has a adversarially trained neural network. A series of encoders take the intermediate layer representation of data as it passes though the network and project it to a latent space which we use for detecting adversarial samples via a -nn classifier. We present results using both grey and white box attackers, as well as an adaptive  bounded attack which was constructed specifically to try and evade our defence. We find that even under the strongest attacker model that we have investigated our defence is able to offer significant defensive benefits.", "num_citations": "2\n", "authors": ["279"]}
{"title": "Welcome to the Europe region special section\n", "abstract": " The computing community throughout the European Region is championing many enterprising industry, academic, and government initiatives to further develop the field and ensure a workforce prepared to take it on.The articles in this special section, written by some of the leading voices in the region, tell stories of informatics and ICT innovations, Web science in Europe, the EuroHPC, future research directions planned for this vibrant part of the world, and much more.", "num_citations": "2\n", "authors": ["279"]}
{"title": "Open Testbeds for CNI\n", "abstract": " 1 Introduction: Rationale/Justification Industrial Control Systems (ICS), composed of combinations of hardware, software and ICT networks, orchestrate the myriad of functions needed to execute complex tasks such as the delivery of utility services and the operation of intricate and disparate manufacturing processes. ICS are examples of cyber-physical systems\u2013digital systems that affect and are affected by, physical processes\u2013whose use is growing through developments in smart-city technologies and the rapid emergence of the Internet of Things. Such systems are increasing in importance as techno-social components of the Critical National Infrastructure (CNI) of the future and as they extend their scope, becoming ubiquitous, accessible and transformative to wider society and the economy, the need to understand their security characteristics also increases.To date the Research Institute in Trustworthy Industrial Control Systems (RITICS) activity has focussed on identifying existing technical and practical problems that surround the development of secure and trustworthy ICS. In order to develop realisable solutions to these problems RITICS has conducted a research programme that includes work in:\u2022 Theory and analysis", "num_citations": "2\n", "authors": ["279"]}
{"title": "Semantics, Logics, and Calculi: Essays Dedicated to Hanne Riis Nielson and Flemming Nielson on the Occasion of Their 60th Birthdays\n", "abstract": " This Festschrift volume is published in honor of Hanne Riis Nielson and Flemming Nielson on the occasion of their 60th birthdays in 2014 and 2015, respectively. The papers included in this volume deal with the wide area of calculi, semantics, and analysis. The book features contributions from colleagues, who have worked together with Hanne and Flemming through their scientific life and are dedicated to them and to their work. The papers were presented at a colloquium at the Technical University of Denmark in January 2016.", "num_citations": "2\n", "authors": ["279"]}
{"title": "A Model-based Approach to Interdependency between Safety and Security in ICS\n", "abstract": " Wide use of modern ICT technologies brings not only communication efficiency, but also security vulnerabilities into industrial control systems. Traditional physically-isolated systems are now required to take cyber security into consideration, which might also lead to system failures. However, integrating security and safety analysis has always been a challenging issue and the various interdependencies between them make it even more difficult, because they might mutually enhance, or undermine. The paper proposes an integrating framework to (i) formalise the desired and undesired properties to be safe(unsafe) or secure(insecure), including the dependencies between them, (ii) evaluate if a query state reaches a safe(unsafe) or secure(insecure) state, and further quantify how safe or secure the state is. In this way,we can accurately capture the benign and harmful relations between safety and security, particularly detecting and measuring conflicting impacts on them. Finally, this framework is implemented by answer set programming to enable automatic evaluation, which is demonstrated by a case study on pipeline transportation.", "num_citations": "2\n", "authors": ["279"]}
{"title": "\u03bb-calculus and Quantitative Program Analysis\n", "abstract": " In this paper we show how the framework of probabilistic abstract interpretation can be applied to statically analyse a probabilistic \u03bb-calculus. We start by reviewing the classical framework of abstract interpretation. We choose to use (first-order) strictness analysis as our running example. We present the definition of probabilistic abstract interpretation and use it to construct a probabilistic strictness analysis.", "num_citations": "2\n", "authors": ["279"]}
{"title": "Coordination Languages and Models: First International Conference, COORDINATION'96, Cesena, Italy, April 15-17, 1996. Proceedings.\n", "abstract": " This book constitutes the refereed proceedings of the First International Conference on Coordination Languages and Models, COORDINATION'96, held in Cesena, Italy in April 1996. Over the last few years, a new class of models, formalisms, and mechanisms for describing concurrent and distributed computations has emerged. A characteristic feature of these coordination languages and models is that they are based on (generative) communication via a shared data space. The 21 revised full papers presented were selected from a total of 78 submissions; also included are three invited papers and 10 posters. All in all, these papers report the state of the art in this young and active area of research and development.", "num_citations": "2\n", "authors": ["279"]}
{"title": "Lambda Calculi: A Guide for the Perplexed\n", "abstract": " The lambda-calculus lies at the very foundation of computer science. Besides its historical role in computability theory it has had significant influence on programming language design and implementation, denotational semantics and domain theory. The book emphasizes the proof theory for the type-free lambda-calculus. The first six chapters concern this calculus and cover the basic theory, reduction, models, computability, and the relationship between the lambda-calculus and combinatory logic. Chapter 7 presents a variety of typed calculi; first the simply typed lambda-calculus, then Milner-style polymorphism and, finally the polymporphic lambda-calculus. Chapter 8 concerns three variants of the type-free lambda-calculus that have recently appeared in the research literature: the lazy lambda-calculus, the concurrent y-calculus and the lamdba omega-calculus. The final chapter contains references and a guide to further reading. There are exercises throughout. In contrast to earlier books on these topics, which were written by logicians, the book is written from a computer science perspective and emphasizes the practical relevance of many of the key theoretical ideas. The book is intended as a course text for final year undergraduates or first year graduate students in computer science. Research students should find it a useful introduction to more specialist literature.", "num_citations": "2\n", "authors": ["279"]}
{"title": "Graph rewriting systems and abstract interpretation\n", "abstract": " Graph rewriting systems are the generalisation of term rewriting systems from (finite) trees to graphs. They provide the basis for an abstract treatment of graph reduction, a well-established technique for the implementation of declarative languages. Given this last observation, it is sensible to develop tools for the compile-time analysis of graph rewrite programs; it is to be expected that opportunities for optimisation which are difficult to detect in the source program might be exposed at this level. This paper summarises the work that we have done on semantics-based static analysis of programs represented by Term Graph Rewriting Systems.", "num_citations": "2\n", "authors": ["279"]}
{"title": "Abstract Interpretation of Term Graph Rewriting Systems\n", "abstract": " In this paper we present a framework for the abstract interpretation of term graph rewriting systems. The framework is based on the approach taken by the Cousots for flowchart programs. We give an example of the use of the framework by presenting an interpretation which performs a form of type inference.", "num_citations": "2\n", "authors": ["279"]}
{"title": "Principes de Programmation Fonctionelle\n", "abstract": " Principes de Programmation Fonctionelle - ePrints Soton The University of Southampton Courses University life Research Business Global About Visit Alumni Departments News Events Contact \u00d7 Search the Site Search Filter your search: All Courses Projects Staff University of Southampton Institutional Repository Search Advanced Search Policies & Help Latest Download Statistics Browse by Year Browse by Divisions LeftRight Principes de Programmation Fonctionelle Glaser, H., Hankin, CL and Till, D. (1987) Principes de Programmation Fonctionelle , Masson Record type: Book Full text not available from this repository. More information Published date: 1987 Additional Information: Address: French Translation Organisations: Web & Internet Science Identifiers Local EPrints ID: 253116 URI: http://eprints.soton.ac.uk/id/eprint/253116 PURE UUID: 7b8ad106-e5f5-41af-862f-23523a223954 Catalogue record Date : 03 \u2026", "num_citations": "2\n", "authors": ["279"]}
{"title": "Scalable Approach to Enhancing ICS Resilience by Network Diversity\n", "abstract": " Network diversity has been widely recognized as an effective defense strategy to mitigate the spread of malware. Optimally diversifying network resources can improve the resilience of a network against malware propagation. This work proposes a scalable method to compute such an optimal deployment, in the context of upgrading a legacy Industrial Control System with modern IT infrastructure. Our approach can tolerate various constraints when searching for optimal diversification, such as outdated products and strict configuration policies. We explicitly measure the vulnerability similarity of products based on the CVE/NVD, to estimate the infection rate of malware between products. A Stuxnet-inspired case demonstrates our optimal diversification in practice, particularly when constrained by various requirements. We then measure the improved resilience of the diversified network in terms of a well-defined\u00a0\u2026", "num_citations": "1\n", "authors": ["279"]}
{"title": "CyRes--Avoiding Catastrophic Failure in Connected and Autonomous Vehicles\n", "abstract": " Existing approaches to cyber security and regulation in the automotive sector cannot achieve the quality of outcome necessary to ensure the safe mass deployment of advanced vehicle technologies and smart mobility systems. Without sustainable resilience hard-fought public trust will evaporate, derailing emerging global initiatives to improve the efficiency, safety and environmental impact of future transport. This paper introduces an operational cyber resilience methodology, CyRes, that is suitable for standardisation. The CyRes methodology itself is capable of being tested in court or by publicly appointed regulators. It is designed so that operators understand what evidence should be produced by it and are able to measure the quality of that evidence. The evidence produced is capable of being tested in court or by publicly appointed regulators. Thus, the real-world system to which the CyRes methodology has been applied is capable of operating at all times and in all places with a legally and socially acceptable value of negative consequence.", "num_citations": "1\n", "authors": ["279"]}
{"title": "Assessing cyber-physical security in industrial control systems\n", "abstract": " Over the last years, Industrial Control Systems (ICS) have become increasingly exposed to a wide range of cyber-physical threats. Efficient models and techniques able to capture their complex structure and identify critical cyber-physical components are therefore essential. AND/OR graphs have proven very useful in this context as they are able to semantically grasp intricate logical interdependencies among ICS components. However, identifying critical nodes in AND/OR graphs is an NP-complete problem. In addition, ICS settings normally involve various cyber and physical security measures that simultaneously protect multiple ICS components in overlapping manners, which makes this problem even harder. In this paper, we present an extended security metric based on AND/OR hypergraphs which efficiently identifies the set of critical ICS components and security measures that should be compromised, with minimum cost (effort) for an attacker, in order to disrupt the operation of vital ICS assets. Our approach relies on MAX-SAT techniques, which we have incorporated in META4ICS, a Java-based security metric analyser for ICS. We also provide a thorough performance evaluation that shows the feasibility of our method. Finally, we illustrate our methodology through a case study in which we analyse the security posture of a realistic Water Transport Network (WTN).", "num_citations": "1\n", "authors": ["279"]}
{"title": "Future identities: changing identities in the UK: the next 10 years. Final project report\n", "abstract": " Future identities : changing identities in the UK : the next 10 years. Final project report - Digital Education Resource Archive (DERA) Digital Education Resource Archive (DERA) Digital Education Resource Archive (DERA) Home About Browse Future identities : changing identities in the UK : the next 10 years. Final project report Hankin, Chris, UK Parliament Government Office for Science, corp creators. (2013) Future identities : changing identities in the UK : the next 10 years. Final project report. [img] Preview Text 13-523-future-identities-changing-identities-report_Redacted.pdf - Published Version Available under License Open Government Licence. Download (1MB) | Preview [img] Text 13-523-future-identities-changing-identities-report.pdf - Published Version Restricted to Repository staff only Available under License Open Government Licence. Download (1MB) Item Type: Document from Web Variant Title: future :\u2026", "num_citations": "1\n", "authors": ["279"]}
{"title": "Time-based Interference and Probabilistic Padding\n", "abstract": " Non-interference is widely studied in the security community as a property which formally specifies the absence of illegal information flows which may disclose some confidential data to unauthorised users. Often the secret data are encoded in the timing behaviour of a program; this produces covert channels which are typically eliminated via program transformation techniques or avoided by an appropriate design of the program eg excluding branching on conditions involving private variables.Transformation techniques have been considered before in the setting of simple programming languages [1], and language with objects and exceptions [2]. In this work we consider the general setting of probabilistic process algebra; in this setting we express the confinement of a probabilistic system against timing covert channels by means of noninterference. Based on the equivalence between this notion and the indistinguishability of processes wrt a given semantics [10], we specify our confinement property in terms of bisimulation semantics: two processes are confined against timing attacks whenever they are probabilistic bisimilar. We concentrate on a particular notion of probabilistic processes, namely finite Markov chains, which correspond to a special class of probabilistic transition systems (PTS) as defined in [6]: a finite Markov chain is just a PTS with a finite set of states, an empty set of labels and a tree-structured transition graph. For these processes bisimilarity corresponds to lumpability [7]. We present an algorithm for transforming two (possibly non-bisimilar) processes into processes which are bisimilar, thus eliminating timing covert channels. The\u00a0\u2026", "num_citations": "1\n", "authors": ["279"]}
{"title": "Coordination languages\n", "abstract": " Coordination Languages Page 1 Introduction Linda JavaSpaces KLAIM AspectK Example Programs Conclusions Coordination Languages Chris Hankin c.hankin@imperial.ac.uk Department of Computing, Imperial College London BCS Advanced Programming Group Lecture, December 2008 Chris Hankin Imperial College London Coordination Languages Page 2 Introduction Linda JavaSpaces KLAIM AspectK Example Programs Conclusions Outline 1 Introduction 2 Linda 3 JavaSpaces 4 KLAIM 5 AspectK 6 Example Programs 7 Conclusions Chris Hankin Imperial College London Coordination Languages Page 3 Introduction Linda JavaSpaces KLAIM AspectK Example Programs Conclusions Outline 1 Introduction 2 Linda 3 JavaSpaces 4 KLAIM 5 AspectK 6 Example Programs 7 Conclusions Chris Hankin Imperial College London Coordination Languages Page 4 Introduction Linda JavaSpaces KLAIM AspectK -\u2026", "num_citations": "1\n", "authors": ["279"]}