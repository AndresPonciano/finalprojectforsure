{"title": "A survey of the use of crowdsourcing in software engineering\n", "abstract": " The term \u2018crowdsourcing\u2019 was initially introduced in 2006 to describe an emerging distributed problem-solving model by online workers. Since then it has been widely studied and practiced to support software engineering. In this paper we provide a comprehensive survey of the use of crowdsourcing in software engineering, seeking to cover all literature on this topic. We first review the definitions of crowdsourcing and derive our definition of Crowdsourcing Software Engineering together with its taxonomy. Then we summarise industrial crowdsourcing practice in software engineering and corresponding case studies. We further analyse the software engineering domains, tasks and applications for crowdsourcing and the platforms and stakeholders involved in realising Crowdsourced Software Engineering solutions. We conclude by exposing trends, open issues and opportunities for future research on Crowdsourced\u00a0\u2026", "num_citations": "386\n", "authors": ["321"]}
{"title": "Sapienz: Multi-objective automated testing for android applications\n", "abstract": " We introduce Sapienz, an approach to Android testing that uses multi-objective search-based testing to automatically explore and optimise test sequences, minimising length, while simultaneously maximising coverage and fault revelation. Sapienz combines random fuzzing, systematic and search-based exploration, exploiting seeding and multi-level instrumentation. Sapienz significantly outperforms (with large effect size) both the state-of-the-art technique Dynodroid and the widely-used tool, Android Monkey, in 7/10 experiments for coverage, 7/10 for fault detection and 10/10 for fault-revealing sequence length. When applied to the top 1,000 Google Play apps, Sapienz found 558 unique, previously unknown crashes. So far we have managed to make contact with the developers of 27 crashing apps. Of these, 14 have confirmed that the crashes are caused by real faults. Of those 14, six already have developer\u00a0\u2026", "num_citations": "379\n", "authors": ["321"]}
{"title": "A study of equivalent and stubborn mutation operators using human analysis of equivalence\n", "abstract": " Though mutation testing has been widely studied for more than thirty years, the prevalence and properties of equivalent mutants remain largely unknown. We report on the causes and prevalence of equivalent mutants and their relationship to stubborn mutants (those that remain undetected by a high quality test suite, yet are non-equivalent). Our results, based on manual analysis of 1,230 mutants from 18 programs, reveal a highly uneven distribution of equivalence and stubbornness. For example, the ABS class and half UOI class generate many equivalent and almost no stubborn mutants, while the LCR class generates many stubborn and few equivalent mutants. We conclude that previous test effectiveness studies based on fault seeding could be skewed, while developers of mutation testing tools should prioritise those operators that we found generate disproportionately many stubborn (and few equivalent\u00a0\u2026", "num_citations": "155\n", "authors": ["321"]}
{"title": "Automated software transplantation\n", "abstract": " Automated transplantation would open many exciting avenues for software development: suppose we could autotransplant code from one system into another, entirely unrelated, system. This paper introduces a theory, an algorithm, and a tool that achieve this. Leveraging lightweight annotation, program analysis identifies an organ (interesting behavior to transplant); testing validates that the organ exhibits the desired behavior during its extraction and after its implantation into a host. While we do not claim automated transplantation is now a solved problem, our results are encouraging: we report that in 12 of 15 experiments, involving 5 donors and 3 hosts (all popular real-world systems), we successfully autotransplanted new functionality and passed all regression tests. Autotransplantation is also already useful: in 26 hours computation time we successfully autotransplanted the H. 264 video encoding functionality\u00a0\u2026", "num_citations": "122\n", "authors": ["321"]}
{"title": "Developer recommendation for crowdsourced software development tasks\n", "abstract": " Crowdsourced software development utilises an open call format to attract geographically distributed developers to accomplish various types of software development tasks. Although the open call format enables wide task accessibility, potential developers must choose from a dauntingly large set of task options (usually more than one hundred available tasks on TopCoder each day). Inappropriate developer-task matching may lower the quality of the software deliverables. In this paper, we employ content-based recommendation techniques to automatically match tasks and developers. The approach learns particular interests from registration history and mines winner history to favour appropriate developers. We measure the performance of our approach by defining accuracy and diversity metrics. We evaluate our recommendation approach by introducing 4 machine learners on 3,094 historical tasks from\u00a0\u2026", "num_citations": "106\n", "authors": ["321"]}
{"title": "Deploying search based software engineering with Sapienz at Facebook\n", "abstract": " We describe the deployment of the Sapienz Search Based Software Engineering (SBSE) testing system. Sapienz has been deployed in production at Facebook since September 2017 to design test cases, localise and triage crashes to developers and to monitor their fixes. Since then, running in fully continuous integration within Facebook\u2019s production development process, Sapienz has been testing Facebook\u2019s Android app, which consists of millions of lines of code and is used daily by hundreds of millions of people around the globe.                 We continue to build on the Sapienz infrastructure, extending it to provide other software engineering services, applying it to other apps and platforms, and hope this will yield further industrial interest in and uptake of SBSE (and hybridisations of SBSE) as a result.", "num_citations": "47\n", "authors": ["321"]}
{"title": "An empirical comparison of combinatorial testing, random testing and adaptive random testing\n", "abstract": " We present an empirical comparison of three test generation techniques, namely, Combinatorial Testing (CT), Random Testing (RT) and Adaptive Random Testing (ART), under different test scenarios. This is the first study in the literature to account for the (more realistic) testing setting in which the tester may not have complete information about the parameters and constraints that pertain to the system, and to account for the challenge posed by faults (in terms of failure rate). Our study was conducted on nine real-world programs under a total of 1683 test scenarios (combinations of available parameter and constraint information and failure rate). The results show significant differences in the techniques' fault detection ability when faults are hard to detect (failure rates are relatively low). CT performs best overall; no worse than any other in 98 percent of scenarios studied. ART enhances RT, and is comparable to CT in\u00a0\u2026", "num_citations": "27\n", "authors": ["321"]}
{"title": "Automated transplantation of call graph and layout features into Kate\n", "abstract": " We report the automated transplantation of two features currently missing from Kate: call graph generation and automatic layout for C programs, which have been requested by users on the Kate development forum. Our approach uses a lightweight annotation system with Search Based techniques augmented by static analysis for automated transplantation. The results are promising: on average, our tool requires 101\u00a0min of standard desktop machine time to transplant the call graph feature, and 31\u00a0min to transplant the layout feature. We repeated each experiment 20 times and validated the resulting transplants using unit, regression and acceptance test suites. In 34 of 40 experiments conducted our search-based autotransplantation tool,      Scalpel, was able to successfully transplant the new functionality, passing all tests.", "num_citations": "23\n", "authors": ["321"]}
{"title": "KD-ART: Should we intensify or diversify tests to kill mutants?\n", "abstract": " Context: Adaptive Random Testing (ART) spreads test cases evenly over the input domain. Yet once a fault is found, decisions must be made to diversify or intensify subsequent inputs. Diversification employs a wide range of tests to increase the chances of finding new faults. Intensification selects test inputs similar to those previously shown to be successful.Objective: Explore the trade-off between diversification and intensification to kill mutants.Method: We augment Adaptive Random Testing (ART) to estimate the Kernel Density (KD\u2013ART) of input values found to kill mutants. KD\u2013ART was first proposed at the 10th International Workshop on Mutation Analysis. We now extend this work to handle real world non numeric applications. Specifically we incorporate a technique to support programs with input parameters that have composite data types (such as arrays and structs).Results: Intensification is the most effective\u00a0\u2026", "num_citations": "16\n", "authors": ["321"]}
{"title": "Hyperheuristic search for sbst\n", "abstract": " This paper argues that incorporating hyper heuristic techniques into existing SBST approaches could help to increase their applicability and generality. We propose a general two layer selective hyper heuristic approach for SBST and provide an example of its use for Combinatorial Interaction Testing (CIT).", "num_citations": "13\n", "authors": ["321"]}
{"title": "Comparative analysis of constraint handling techniques for constrained combinatorial testing\n", "abstract": " Constraints depict the dependency relationships between parameters in a software system under test. Because almost all systems are constrained in some way, techniques that adequately cater for constraints have become a crucial factor for adoption, deployment and exploitation of Combinatorial Testing (CT). Currently, despite a variety of different constraint handling techniques available, the relationship between these techniques and the generation algorithms that use them remains unknown, yielding an important gap and pressing concern in the literature of constrained combination testing. In this paper, we present a comparative empirical study to investigate the impact of four common constraint handling techniques on the performance of six representative (greedy and search-based) test suite generation algorithms. The results reveal that the Verify technique implemented with the Minimal Forbidden Tuple\u00a0\u2026", "num_citations": "9\n", "authors": ["321"]}
{"title": "Kernel density adaptive random testing\n", "abstract": " Mutation analysis is used to assess the effectiveness of a test data generation technique at finding faults. Once a mutant is killed, decisions must be made whether to diversify or intensify the subsequent test inputs. Diversification employs a wide range of test inputs with the aim of increasing the chances of killing new mutants. By contrast, intensification selects test inputs which are similar to those previously shown to be successful, taking advantage of overlaps in the conditions under which mutants can be killed. This paper explores the trade-off between diversification and intensification by augmenting Adaptive Random Testing (ART) to estimate the Kernel Density (KD-ART) of input values which are found to kill mutants. The results suggest that intensification is typically more effective at finding faults than diversification. KD-ART (intensify) achieves 7.24% higher mutation score on average than KD-ART (diversify\u00a0\u2026", "num_citations": "7\n", "authors": ["321"]}
{"title": "Prem: Prestige network enhanced developer-task matching for crowdsourced software development\n", "abstract": " Many software organizations are turning to employ crowdsourcing to augment their software production. For current practice of crowdsourcing, it is common to see a mass number of tasks posted on software crowdsourcing platforms, with little guidance for task selection. Considering that crowd developers may vary greatly in expertise, inappropriate developer-task matching will harm the quality of the deliverables. It is also not time-efficient for developers to discover their most appropriate tasks from vast open call requests. We propose an approach called PREM, aiming to appropriately match between developers and tasks. PREM automatically learns from the developers\u2019 historical task data. In addition to task preference, PREM considers the competition nature of crowdsourcing by constructing developers\u2019 prestige network. This differs our approach from previous developer recommendation methods that are based on task and/or individual features. Experiments are conducted on 3 TopCoder datasets with 9,191 tasks in total. Our experimental results show that reasonable accuracies are achievable (63%, 46%, 36% for the 3 datasets respectively, when matching 5 developers to each task) and the constructed prestige network can help improve the matching results.", "num_citations": "6\n", "authors": ["321"]}
{"title": "Regression test case prioritisation for Guava\n", "abstract": " We present a three objective formulation of regression test prioritisation. Our formulation involves the well-known, and widely-used objectives of Average Percentage of Statement Coverage (APSC) and Effective Execution Time (EET). However, we additionally include the Average Percentage of Change Coverage (APCC), which has not previously been used in search-based regression test optimisation. We apply our approach to prioritise the base and the collection package of the Guava project, which contains over 26,815 test cases. Our results demonstrate the value of search-based test case prioritisation: the sequences we find require only 0.2\u00a0% of the 26,815 test cases and only 0.45\u00a0% of their effective execution time. However, we find solutions that achieve more than 99.9\u00a0% of both regression testing objectives; covering both changed code and existing code. We also investigate the tension between\u00a0\u2026", "num_citations": "6\n", "authors": ["321"]}
{"title": "Introduction to the special issue on Mutation Testing\n", "abstract": " It is our pleasure to introduce this special issue on Mutation Testing. The special issue contains nine papers, including four extended versions of papers presented at the 7th International Workshop on Mutation Analysis and five new submissions. We have divided the special issue into three broad areas based on the topics covered. The first area focuses on the techniques for making mutation testing more efficient and practical; the second area revisits some fundamental questions about mutants, whilst the third area presents some advanced applications of mutation testing for model-based testing.Mutation Testing has been proven to be an effective way to measure the quality of a test suite in terms of its ability to detect faults [1]. The history of mutation testing can be traced back to 1971 in a publication by Richard Lipton [2] as well as in publications from the late 1970s by DeMillo et al.[3] and Hamlet [4]. In Mutation\u00a0\u2026", "num_citations": "4\n", "authors": ["321"]}
{"title": "Milu: A higher order mutation testing tool\n", "abstract": " Milu: A Higher Order Mutation Testing Tool Page 1 Milu: A Higher Order Mutation Testing Tool Yue Jia University College London Joint work with Mark Harman and William Langdon Page 2 Agenda Why Higher Order Mutation Testing? Search for interesting HOMs Milu mutation testing tool Scalability and Extendability Performance Study Page 3 Mutation Testing First Order Mutants : A single change Simple faults / FOMs Higher Order Mutant : Multiple changes Multiple faults / HOMs Page 4 Mutation Testing Subset of First Order Mutants are used No Higher Order Mutants at all! Page 5 Mutation Testing Subset of First Order Mutants are used No Higher Order Mutants at all! Page 6 Higher Order Mutation Testing The space of all mutants (first and higher order) is a search space, We should apply search based optimisation techniques to find mutants that are fit for purpose. Page 7 Higher Order Mutation Testing Search for /\u2026", "num_citations": "3\n", "authors": ["321"]}
{"title": "The GISMOE Architecture\u2711\n", "abstract": " The GISMOE research agenda is concerned with optimising programs for non-functional properties such as speed, size, throughput, power consumption and bandwidth can be demanding. GISMOE sets out a vision for a new kind of software development environment inspired by recent results from Search Based Software Engineering (SBSE). Details of the GISMOE research agenda are provided in the extended keynote paper for the 27th IEEE/ACM International Conference on Automated Software Engineering (ASE 2012). This talk overview is a brief introduction to the approach and a description of the talk about the GISMOE agenda at the 2nd Chinese SBSE workshop in Dalian, 8th and 9th June 2013.", "num_citations": "1\n", "authors": ["321"]}