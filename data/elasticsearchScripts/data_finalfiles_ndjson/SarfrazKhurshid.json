{"title": "Improving bug localization using structured information retrieval\n", "abstract": " Locating bugs is important, difficult, and expensive, particularly for large-scale systems. To address this, natural language information retrieval techniques are increasingly being used to suggest potential faulty source files given bug reports. While these techniques are very scalable, in practice their effectiveness remains low in accurately localizing bugs to a small number of files. Our key insight is that structured information retrieval based on code constructs, such as class and method names, enables more accurate bug localization. We present BLUiR, which embodies this insight, requires only the source code and bug reports, and takes advantage of bug similarity data if available. We build BLUiR on a proven, open source IR toolkit that anyone can use. Our work provides a thorough grounding of IR-based bug localization research in fundamental IR theoretical and empirical knowledge and practice. We evaluate\u00a0\u2026", "num_citations": "353\n", "authors": ["1818"]}
{"title": "DeepRoad: GAN-based metamorphic testing and input validation framework for autonomous driving systems\n", "abstract": " While Deep Neural Networks (DNNs) have established the fundamentals of image-based autonomous driving systems, they may exhibit erroneous behaviors and cause fatal accidents. To address the safety issues in autonomous driving systems, a recent set of testing techniques have been designed to automatically generate artificial driving scenes to enrich test suite, e.g., generating new input images transformed from the original ones. However, these techniques are insufficient due to two limitations: first, many such synthetic images often lack diversity of driving scenes, and hence compromise the resulting efficacy and reliability. Second, for machine-learning-based systems, a mismatch between training and application domain can dramatically degrade system accuracy, such that it is necessary to validate inputs for improving system robustness. In this paper, we propose DeepRoad, an unsupervised DNN\u00a0\u2026", "num_citations": "299\n", "authors": ["1818"]}
{"title": "Reducing combinatorics in testing product lines\n", "abstract": " A Software Product Line (SPL) is a family of programs where each program is defined by a unique combination of features. Testing or checking properties of an SPL is hard as it may require the examination of a combinatorial number of programs. In reality, however, features are often irrelevant for a given test-they augment, but do not change, existing behavior, making many feature combinations unnecessary as far as testing is concerned. In this paper we show how to reduce the amount of effort in testing an SPL. We represent an SPL in a form where conventional static program analysis techniques can be applied to find irrelevant features for a test. We use this information to reduce the combinatorial number of SPL programs to examine.", "num_citations": "137\n", "authors": ["1818"]}
{"title": "Specification-based program repair using SAT\n", "abstract": " Removing bugs in programs \u2013 even when location of faulty statements is known \u2013 is tedious and error-prone, particularly because of the increased likelihood of introducing new bugs as a result of fixing known bugs. We present an automated approach for generating likely bug fixes using behavioral specifications. Our key insight is to replace a faulty statement that has deterministic behavior with one that has nondeterministic behavior, and to use the specification constraints to prune the ensuing nondeterminism and repair the faulty statement. As an enabling technology, we use the SAT-based Alloy tool-set to describe specification constraints as well as for solving them. Initial experiments show the effectiveness of our approach in repairing programs that manipulate structurally complex data. We believe specification-based automated debugging using SAT holds much promise.", "num_citations": "125\n", "authors": ["1818"]}
{"title": "An information retrieval approach for regression test prioritization based on program changes\n", "abstract": " Regression testing is widely used in practice for validating program changes. However, running large regression suites can be costly. Researchers have developed several techniques for prioritizing tests such that the higher-priority tests have a higher likelihood of finding bugs. A vast majority of these techniques are based on dynamic analysis, which can be precise but can also have significant overhead (e.g., for program instrumentation and test-coverage collection). We introduce a new approach, REPiR, to address the problem of regression test prioritization by reducing it to a standard Information Retrieval problem such that the differences between two program versions form the query and the tests constitute the document collection. REPiR does not require any dynamic profiling or static program analysis. As an enabling technology we leverage the open-source IR toolkit Indri. An empirical evaluation using eight\u00a0\u2026", "num_citations": "120\n", "authors": ["1818"]}
{"title": "Incremental test generation for software product lines\n", "abstract": " Recent advances in mechanical techniques for systematic testing have increased our ability to automatically find subtle bugs, and hence, to deploy more dependable software. This paper builds on one such systematic technique, scope-bounded testing, to develop a novel specification-based approach for efficiently generating tests for products in a software product line. Given properties of features as first-order logic formulas in Alloy, our approach uses SAT-based analysis to automatically generate test inputs for each product in a product line. To ensure soundness of generation, we introduce an automatic technique for mapping a formula that specifies a feature into a transformation that defines incremental refinement of test suites. Our experimental results using different data structure product lines show that an incremental approach can provide an order of magnitude speedup over conventional techniques. We\u00a0\u2026", "num_citations": "111\n", "authors": ["1818"]}
{"title": "Memoized symbolic execution\n", "abstract": " This paper introduces memoized symbolic execution (Memoise), a new approach for more efficient application of forward symbolic execution, which is a well-studied technique for systematic exploration of program behaviors based on bounded execution paths. Our key insight is that application of symbolic execution often requires several successive runs of the technique on largely similar underlying problems, eg, running it once to check a program to find a bug, fixing the bug, and running it again to check the modified program. Memoise introduces a trie-based data structure that stores the key elements of a run of symbolic execution. Maintenance of the trie during successive runs allows re-use of previously computed results of symbolic execution without the need for re-computing them as is traditionally done. Experiments using our prototype implementation of Memoise show the benefits it holds in various\u00a0\u2026", "num_citations": "109\n", "authors": ["1818"]}
{"title": "Localizing failure-inducing program edits based on spectrum information\n", "abstract": " Keeping evolving systems fault free is hard. Change impact analysis is a well-studied methodology for finding faults in evolving systems. For example, in order to help developers identify failure-inducing edits, Chianti extracts program edits as atomic changes between different program versions, selects affected tests, and determines a subset of those changes that might induce test failures. However, identifying real regression faults is challenging for developers since the number of affecting changes related to each test failure may still be too large for manual inspection. This paper presents a novel approach FAULTTRACER which ranks program edits in order to reduce developers' effort in manually inspecting all affecting changes. FAULTTRACER adapts spectrum-based fault localization techniques and applies them in tandem with an enhanced change impact analysis that uses Extended Call Graphs to identify\u00a0\u2026", "num_citations": "109\n", "authors": ["1818"]}
{"title": "Software assurance by bounded exhaustive testing\n", "abstract": " The contribution of this paper is an experiment that shows the potential value of a combination of selective reverse engineering to formal specifications and bounded exhaustive testing to improve the assurance levels of complex software. A key problem is to scale up test input generation so that meaningful results can be obtained. We present an approach, using Alloy and TestEra for test input generation, which we evaluate by experimental application to the Galileo dynamic fault tree analysis tool.", "num_citations": "96\n", "authors": ["1818"]}
{"title": "Towards practical program repair with on-demand candidate generation\n", "abstract": " Effective program repair techniques, which modify faulty programs to fix them with respect to given test suites, can substantially reduce the cost of manual debugging. A common repair approach is to iteratively first generate candidate programs with possible bug fixes and then validate them against the given tests until a candidate that passes all the tests is found. While this approach is conceptually simple, due to the potentially high number of candidates that need to first be generated and then be compiled and tested, existing repair techniques that embody this approach have relatively low effectiveness, especially for faults at a fine granularity.", "num_citations": "92\n", "authors": ["1818"]}
{"title": "Query-aware test generation using a relational constraint solver\n", "abstract": " We present a novel approach for black-box testing of database management systems (DBMS) using the Alloy tool-set. Given a database schema and an SQL query as inputs, our approach first formulates Alloy models for both inputs, and then using the Alloy Analyzer, it generates (1) input data to populate test databases, and (2) the expected result of executing the given query on the generated data. The Alloy Analyzer results form a complete test suite (input/oracle) for verifying the execution result of a DBMS query processor. By incorporating both the schema and the query during the analysis, our approach performs query-aware data generation where executing the query on the generated data produces meaningful non-empty results. We developed a prototype tool, ADUSA, and used it to evaluate our approach. Experimental results show the ability of our approach to detect bugs in both open-source as well as\u00a0\u2026", "num_citations": "87\n", "authors": ["1818"]}
{"title": "Boosting spectrum-based fault localization using pagerank\n", "abstract": " Manual debugging is notoriously tedious and time consuming. Therefore, various automated fault localization techniques have been proposed to help with manual debugging. Among the existing fault localization techniques, spectrum-based fault localization (SBFL) is one of the most widely studied techniques due to being lightweight. A focus of existing SBFL techniques is to consider how to differentiate program source code entities (ie, one dimension in program spectra); indeed, this focus is aligned with the ultimate goal of finding the faulty lines of code. Our key insight is to enhance existing SBFL techniques by additionally considering how to differentiate tests (ie, the other dimension in program spectra), which, to the best of our knowledge, has not been studied in prior work.", "num_citations": "79\n", "authors": ["1818"]}
{"title": "Assertion-based repair of complex data structures\n", "abstract": " Programmers have long used assertions to characterize properties of code. An assertion violation signals a corruption in the programstate. At such a state, it is standard to terminate the program, debug it if possible, and re-execute it. We propose a new view: instead of terminating the program, use the violated assertion as a basis of repairing the state of the program and let it continue.", "num_citations": "74\n", "authors": ["1818"]}
{"title": "Abstracting symbolic execution with string analysis\n", "abstract": " Forward symbolic execution is a technique for program analysis that explores the execution paths of a program by maintaining a symbolic representation of the program state. Traditionally, applications of this technique have focused on symbolically representing only primitive data types, while more recent extensions have expanded to reference types. We demonstrate the ability to symbolically execute a commonly used library class, specifically a string, at an abstract level. By abstracting away the implementation details of strings using finite-state automata, symbolic execution can scale to more complex programs. This technique can be applied to programs which generate complicated strings, such as SQL database queries.", "num_citations": "73\n", "authors": ["1818"]}
{"title": "Perceptions on the state of the art in verification and validation in cyber-physical systems\n", "abstract": " It is widely held that debugging cyber-physical systems (CPS) is challenging; many strongly held beliefs exist regarding how CPS are currently debugged and tested and the suitability of various techniques. For instance, dissenting opinions exist as to whether formal methods (including static analysis, theorem proving, and model checking) are appropriate in CPS verification and validation. Simulation tools and simulation-based testing are also often considered insufficient for CPS. Many \u201cexperts\u201d posit that high-level programming languages (e.g., Java or C#) are not applicable to CPS due to their inability to address (significant) resource constraints at a high level of abstraction. To date, empirical studies investigating these questions have not been done. In this paper, we qualitatively and quantitatively analyze why debugging CPS remains challenging and either dispel or confirm these strongly held beliefs along the\u00a0\u2026", "num_citations": "72\n", "authors": ["1818"]}
{"title": "Shared execution for efficiently testing product lines\n", "abstract": " A software product line (SPL) is a family of related programs, each of which is uniquely defined by a combination of features. Testing an SPL requires running each of its programs, which may be computationally expensive as the number of programs in an SPL is potentially exponential in the number of features. It is also wasteful since instructions common to many programs must be repeatedly executed, rather than just once. To reduce this waste, we propose the idea of shared execution, which runs instructions just once for a set of programs until a variable read yields multiple values, causing execution to branch for each value until a common execution point that allows shared execution to resume. Experiments show that shared execution can be faster than conventionally running each program from start to finish, despite its overhead.", "num_citations": "67\n", "authors": ["1818"]}
{"title": "Event listener analysis and symbolic execution for testing GUI applications\n", "abstract": " Graphical User Interfaces (GUIs) are composed of virtual objects, widgets, which respond to events triggered by user actions. Therefore, test inputs for GUIs are event sequences that mimic user interaction. The nature of these sequences and the values for certain widgets, such as textboxes, causes a two-dimensional combinatorial explosion. In this paper we present Barad, a GUI testing framework that uniformly addresses event-flow and data-flow in GUI applications generating tests in the form of event sequences and data inputs. Barad tackles the two-dimensional combinatorial explosion by pruning regions of the event and data input space. For event sequence generation we consider only events with registered event listeners, thus pruning regions of the event input space. We introduce symbolic widgets which allow us to obtain an executable symbolic version of the GUI. By symbolically executing the\u00a0\u2026", "num_citations": "67\n", "authors": ["1818"]}
{"title": "Juzi\n", "abstract": " This paper describes Juzi, a tool for automatic repair of complex data structures. Juzi takes a Java class representing the data structure as well as a predicate method that specifies the structural integrity constraints as inputs. Juzi instruments its inputs and generates a new Java class which behaves similarly to the original class, yet automatically repairs itself when the structural integrity constraints are violated. Juzi implements a novel repair algorithm. Given a structure that violates its integrity constraints, Juzi performs a systematic search based on symbolic execution to repair the structure, i.e., mutate it such that the resulting structure satisfies the given constraints. Experiments on structures ranging from library classes to standalone applications, show that Juzi repairs complex structures while enabling programs to recover from erroneous executions caused by data structure corruptions.", "num_citations": "67\n", "authors": ["1818"]}
{"title": "Exploring the design of an intentional naming scheme with an automatic constraint analyzer\n", "abstract": " Lightweight formal modeling and automatic analysis were used to explore the design of the intentional naming system (INS), a new scheme for resource discovery in a dynamic networked environment. We constructed a model of INS in Alloy a lightweight relational notation, and analyzed it with the Alloy Constraint Analyzer, a fully automatic simulation and checking tool. In doing so, we exposed several serious flaws in both the algorithm of INS and the underlying naming semantics. We were able to characterize the conditions under which the existing INS scheme works correctly, and evaluate proposed fixes.", "num_citations": "67\n", "authors": ["1818"]}
{"title": "Testing software product lines using incremental test generation\n", "abstract": " We present a novel specification-based approach for generating tests for products in a software product line. Given properties of features as first-order logic formulas, our approach uses SAT-based analysis to automatically generate test inputs for each product in a product line. To ensure soundness of generation, we introduce an automatic technique for mapping a formula that specifies a feature into a transformation that defines incremental refinement of test suites. Our experimental results using different data structure product lines show that incremental approach can provide an order of magnitude speed-up over conventional techniques.", "num_citations": "65\n", "authors": ["1818"]}
{"title": "Test generation for graphical user interfaces based on symbolic execution\n", "abstract": " While Graphical User Interfaces (GUIs) have become ubiquitous, testing them remains largely ad-hoc. Since the state of a GUI is defined by a sequence of events on the GUI's widgets, a test input for a GUI is such an event sequence. Due to the combinatorial nature of the sequences, testing a GUI thoroughly is problematic and time-consuming. Moreover, the wide range of possible values for certain GUI widgets, such as a textbox, compounds the problem.", "num_citations": "58\n", "authors": ["1818"]}
{"title": "Software assurance by bounded exhaustive testing\n", "abstract": " Bounded exhaustive testing (BET) is a verification technique in which software is automatically tested for all valid inputs up to specified size bounds. A particularly interesting case of BET arises in the context of systems that take structurally complex inputs. Early research suggests that the BET approach can reveal faults in small systems with inputs of low structural complexity, but its potential utility for larger systems with more complex input structures remains unclear. We set out to test its utility on one such system. We used Alloy and TestEra to generate inputs to test the Galileo dynamic fault tree analysis tool, for which we already had both a formal specification of the input space and a test oracle. An initial attempt to generate inputs using a straightforward translation of our specification to Alloy did not work well. The generator failed to generate inputs to meaningful bounds. We developed an approach in which we\u00a0\u2026", "num_citations": "58\n", "authors": ["1818"]}
{"title": "Scaling symbolic execution using ranged analysis\n", "abstract": " This paper introduces a novel approach to scale symbolic execution --- a program analysis technique for systematic exploration of bounded execution paths---for test input generation. While the foundations of symbolic execution were developed over three decades ago, recent years have seen a real resurgence of the technique, specifically for systematic bug finding. However, scaling symbolic execution remains a primary technical challenge due to the inherent complexity of the path-based exploration that lies at core of the technique. Our key insight is that the state of the analysis can be represented highly compactly: a test input is all that is needed to effectively encode the state of a symbolic execution run. We present ranged symbolic execution, which embodies this insight and uses two test inputs to define a range, i.e., the beginning and end, for a symbolic execution run. As an application of our approach, we\u00a0\u2026", "num_citations": "57\n", "authors": ["1818"]}
{"title": "An empirical study of long lived bugs\n", "abstract": " Bug fixing is a crucial part of software development and maintenance. A large number of bugs often indicate poor software quality since buggy behavior not only causes failures that may be costly but also has a detrimental effect on the user's overall experience with the software product. The impact of long lived bugs can be even more critical since experiencing the same bug version after version can be particularly frustrating for user. While there are many studies that investigate factors affecting bug fixing time for entire bug repositories, to the best of our knowledge, none of these studies investigates the extent and reasons of long lived bugs. In this paper, we analyzed long lived bugs from five different perspectives: their proportion, severity, assignment, reasons, as well as the nature of fixes. Our study on four open-source projects shows that there are a considerable number of long lived bugs in each system and\u00a0\u2026", "num_citations": "54\n", "authors": ["1818"]}
{"title": "A case for automated debugging using data structure repair\n", "abstract": " Automated debugging is becoming increasingly important as the size and complexity of software increases. This paper makes a case for using constraint-based data structure repair, a recently developed technique for fault recovery, as a basis for automated debugging. Data structure repair uses given structural integrity constraints for key data structures to monitor their correctness during the execution of a program. If a constraint violation is detected, repair performs mutations on the data structures, i.e., corrupt program state, and transforms it into another state, which satisfies the desired constraints. The primary goal of data structure repair is to transform an erroneous state into an acceptable state. Therefore, the mutations performed by repair actions provide a basis of debugging faults in code (assuming the errors are due to bugs). A key challenge to embodying this insight into a mechanical technique arises due to\u00a0\u2026", "num_citations": "54\n", "authors": ["1818"]}
{"title": "Repairing structurally complex data\n", "abstract": " We present a novel algorithm for repairing structurally complex data. Given an assertion that represents desired structural integrity constraints and a structure that violates them, the algorithm performs repair actions that mutate the given structure to generate a new structure that satisfies the constraints. Assertions are written as imperative predicates, which can express rich structural properties. Since these properties can be arbitrarily complex, our algorithm is sound but not complete, and it may not terminate in certain cases. Experimental results with our prototype implementation, Juzi, show that it is feasible to efficiently repair a variety of complex data structures that are routinely used in library code. Juzi can often repair structures comprising of over a hundred objects (even when majority of the objects have some corrupted field) in less than one second. Our algorithm is based on systematic backtracking but\u00a0\u2026", "num_citations": "54\n", "authors": ["1818"]}
{"title": "ParSym: Parallel symbolic execution\n", "abstract": " Scaling software analysis techniques based on source-code, such as symbolic execution and data flow analyses, remains a challenging problem for systematically checking software systems. The increasing availability of clusters of commodity machines provides novel opportunities to scale these techniques using parallel algorithms. This paper presents ParSym, a novel parallel algorithm for scaling symbolic execution using a parallel implementation. In every iteration ParSym explores multiple branches of a path condition in parallel by distributing them among available workers resulting in an efficient parallel version of symbolic execution. Experimental results show that symbolic execution is highly scalable using parallel algorithms: using 512 processors, more than two orders of magnitude speedup are observed.", "num_citations": "53\n", "authors": ["1818"]}
{"title": "Faulttracer: a change impact and regression fault analysis tool for evolving java programs\n", "abstract": " Keeping evolving software fault-free is hard. In our previous work, we proposed FaultTracer, a change impact and regression fault analysis tool for evolving programs. It takes the old and new versions of a program and a regression test suite as inputs, and then identifies affected tests---a subset of tests relevant to the program differences between the two versions and affecting changes---a subset of atomic changes relevant to each affected test. It adapts spectrum-based fault localization techniques and applies them in tandem with an enhanced change impact analysis to identify and rank failure-inducing program edits. We have shown that FaultTracer, compared to existing techniques (eg, Chianti), achieves improvement in selecting influenced tests, determining suspicious failure-inducing edits, and ranking failure-inducing program edits. In this paper, we show the design, implementation, and demonstration of our\u00a0\u2026", "num_citations": "48\n", "authors": ["1818"]}
{"title": "Generalizing symbolic execution to library classes\n", "abstract": " Forward symbolic execution is a program analysis technique that allows using symbolic inputs to explore program executions. The traditional applications of this technique have focused on programs that manipulate primitive data types, such as integer or boolean. Recent extensions have shown how to handle reference types at their representation level. The extensions have favorably been backed by advances in constraint solving technology, and together they have made symbolic execution applicable, at least in theory, to a large class of programs. In practice, however, the increased potential for applications has created significant issues with scalability of symbolic execution to programs of non-trivial size---the ensuing path conditions rapidly become unfeasibly complex. We present Dianju, a new technique that aims to address the scalability of symbolic execution. The fundamental idea in Dianju is to perform\u00a0\u2026", "num_citations": "48\n", "authors": ["1818"]}
{"title": "Automated SQL query generation for systematic testing of database engines\n", "abstract": " We present a novel approach for generating syntactically and semantically correct SQL queries as inputs for testing relational databases. We leverage the SAT-based Alloy tool-set to reduce the problem of generating valid SQL queries into a SAT problem. Our approach translates SQL query constraints into Alloy models, which enable it to generate valid queries that cannot be automatically generated using conventional grammar-based generators.", "num_citations": "46\n", "authors": ["1818"]}
{"title": "Symbolic execution for deep neural networks\n", "abstract": " Deep Neural Networks (DNN) are increasingly used in a variety of applications, many of them with substantial safety and security concerns. This paper introduces DeepCheck, a new approach for validating DNNs based on core ideas from program analysis, specifically from symbolic execution. The idea is to translate a DNN into an imperative program, thereby enabling program analysis to assist with DNN validation. A basic translation however creates programs that are very complex to analyze. DeepCheck introduces novel techniques for lightweight symbolic analysis of DNNs and applies them in the context of image classification to address two challenging problems in DNN analysis: 1) identification of important pixels (for attribution and adversarial generation); and 2) creation of 1-pixel and 2-pixel attacks. Experimental results using the MNIST data-set show that DeepCheck's lightweight symbolic analysis provides a valuable tool for DNN validation.", "num_citations": "43\n", "authors": ["1818"]}
{"title": "On the effectiveness of information retrieval based bug localization for c programs\n", "abstract": " Localizing bugs is important, difficult, and expensive, especially for large software projects. To address this problem, information retrieval (IR) based bug localization has increasingly been used to suggest potential buggy files given a bug report. To date, researchers have proposed a number of IR techniques for bug localization and empirically evaluated them to understand their effectiveness. However, virtually all of the evaluations have been limited to the projects written in object-oriented programming languages, particularly Java. Therefore, the effectiveness of these techniques for other widely used languages such as C is still unknown. In this paper, we create a benchmark dataset consisting of more than 7,500 bug reports from five popular C projects and rigorously evaluate our recently introduced IR-based bug localization tool using this dataset. Our results indicate that although the IR-relevant properties of C\u00a0\u2026", "num_citations": "43\n", "authors": ["1818"]}
{"title": "Understanding the triaging and fixing processes of long lived bugs\n", "abstract": " ContextBug fixing is an integral part of software development and maintenance. A large number of bugs often indicate poor software quality, since buggy behavior not only causes failures that may be costly but also has a detrimental effect on the user\u2019s overall experience with the software product. The impact of long lived bugs can be even more critical since experiencing the same bug version after version can be particularly frustrating for user. While there are many studies that investigate factors affecting bug fixing time for entire bug repositories, to the best of our knowledge, none of these studies investigates the extent and reasons of long lived bugs.ObjectiveIn this paper, we investigate the triaging and fixing processes of long lived bugs so that we can identify the reasons for delay and improve the overall bug fixing process.MethodologyWe mine the bug repositories of popular open source projects, and analyze\u00a0\u2026", "num_citations": "39\n", "authors": ["1818"]}
{"title": "Generating structurally complex tests from declarative constraints\n", "abstract": " This dissertation describes a method for systematic constraint-based test generation for programs that take as inputs structurally complex data, presents an automated SAT-based framework for testing such programs, and provides evidence on the feasibility of using this approach to generate high quality test suites and find bugs in non-trivial programs. The framework tests a program systematically on all nonisomorphic inputs (within a given bound on the input size). Test inputs are automatically generated from a given input constraint that characterizes allowed program inputs. In unit testing of object-oriented programs, for example, an input constraint corresponds to the representation invariant; the test inputs are then objects on which to invoke a method under test. Input constraints may additionally describe test purposes and test selection criteria. Constraints are expressed in a simple (first-order) relational logic and solved by translating them into propositional formulas that are handed to an off-the-shelf SAT solver. Solutions found by the SAT solver are lifted back to the relational domain and reified as tests. The TestEra tool implements this framework for testing Java programs. Ex-periments on generating several complex structures indicate the feasibility of using off-the-shelf SAT solvers for systematic generation of nonisomorphic structures. The tool also uncovered previously unknown errors in several applications including an intentional naming scheme for dynamic networks and a fault-tree analysis system developed for NASA.", "num_citations": "39\n", "authors": ["1818"]}
{"title": "Compositional symbolic execution with memoized replay\n", "abstract": " Symbolic execution is a powerful, systematic analysis that has received much visibility in the last decade. Scalability however remains a major challenge for symbolic execution. Compositional analysis is a well-known general purpose methodology for increasing scalability. This paper introduces a new approach for compositional symbolic execution. Our key insight is that we can summarize each analyzed method as a memoization tree that captures the crucial elements of symbolic execution, and leverage these memoization trees to efficiently replay the symbolic execution of the corresponding methods with respect to their calling contexts. Memoization trees offer a natural way to compose in the presence of heap operations, which cannot be dealt with by previous work that uses logical formulas as summaries for compositional symbolic execution. Our approach also enables efficient target oriented symbolic\u00a0\u2026", "num_citations": "35\n", "authors": ["1818"]}
{"title": "Constraint prioritization for efficient analysis of declarative models\n", "abstract": " The declarative modeling language Alloy and its automatic analyzer provide an effective tool-set for building designs of systems and checking their properties. The Alloy Analyzer performs bounded exhaustive analysis using off-the-shelf SAT solvers. The analyzer\u2019s performance hinges on the complexity of the models and so far, its feasibility has been shown only within limited bounds. We present a novel optimization technique that defines program slicing for declarative models and enables efficient analyses exploiting partial solutions. We present an algorithm that computes transient slices for Alloy models by partitioning them into a base and a derived slice. A satisfying solution to the base slice is systematically extended to generate a solution for the entire model, while unsatisfiability of the base implies unsatisfiability of the entire model.               By generating slices, our approach enables constraint\u00a0\u2026", "num_citations": "35\n", "authors": ["1818"]}
{"title": "PKorat: Parallel generation of structurally complex test inputs\n", "abstract": " Constraint solving lies at the heart of several specification-based approaches to automated testing. Korat is a previously developed algorithm for solving constraints in Java programs. Given a Java predicate that represents the desired constraints and a bound on the input size, Korat systematically explores the bounded input space of the predicate and enumerates inputs that satisfy the constraint. Korat search is largely sequential: it considers one candidate input in each iteration and it prunes the search space based on the candidates considered. This paper presents PKorat, a new parallel algorithm that parallelizes the Korat search. PKorat explores the same state space as Korat but considers several candidates in each iteration. These candidates are distributed among parallel workers resulting in an efficient parallel version of Korat. Experimental results using complex structural constraints from a variety of subject\u00a0\u2026", "num_citations": "32\n", "authors": ["1818"]}
{"title": "Studying the influence of standard compiler optimizations on symbolic execution\n", "abstract": " Systematic testing plays a vital role in increasing software reliability. A particularly effective and popular approach for systematic testing is symbolic execution, which analyzes a large number of program behaviors using symbolic inputs. Even though symbolic execution is among the most studied analyses during the last decade, scaling it to real-world applications remains a key challenge. This paper studies how a class of semantics-preserving program transformations, namely compiler optimizations, which are designed to enhance performance of standard program execution (using concrete inputs), influence traditional symbolic execution. As an enabling technology, the study uses KLEE, a well-known symbolic execution engine based on the LLVM compiler infrastructure, and focuses on 33 optimization flags of LLVM. Our specific research questions include: (1) how different optimizations influence the\u00a0\u2026", "num_citations": "31\n", "authors": ["1818"]}
{"title": "Constraint-based program debugging using data structure repair\n", "abstract": " Developers have used data structure repair over the last few decades as an effective means to recover on-the-fly from errors in program state. Traditional repair techniques were based on dedicated repair routines, whereas more recent techniques have used invariants that describe desired structural properties as the basis for repair. All repair techniques are designed with one primary goal: run-time error recovery. However, the actions that any such technique performs to repair an erroneous program state are meant to produce the effect of the actions of a (hypothetical) correct program. The key insight in this paper is that repair actions on the program state can guide debugging of code (when the erroneous program execution is due to a fault in the program and not an external event).This paper presents an approach that abstracts concrete repair actions that a routine performs to repair an erroneous state into a\u00a0\u2026", "num_citations": "30\n", "authors": ["1818"]}
{"title": "Starc: static analysis for efficient repair of complex data\n", "abstract": " Data structure corruptions are insidious bugs that reduce the reliability of software systems. Constraint-based datastructure repair promises to help programs recover from potentially crippling corruption errors. Prior work repairs a variety of relatively small data structures, usually with hundreds of nodes.", "num_citations": "30\n", "authors": ["1818"]}
{"title": "Software fault localization using feature selection\n", "abstract": " Manually locating and fixing faults can be tedious and hard. Recent years have seen much progress in automated techniques for fault localization. A particularly promising approach is to analyze passing and failing runs to compute how likely each statement is to be faulty. Techniques based on this approach have so far largely focused on either using statistical analysis or similarity based algorithms, which have a natural application in evaluating such runs. We present a novel approach to fault localization using feature selection techniques from machine learning. Our insight is that each additional failing or passing run can provide significantly diverse amount of information, which can help localize faults in code--the statements with maximum feature diversity information can point to most suspicious lines of code. Experimental results show that our approach outperforms state-of-the-art approaches for localizing faults\u00a0\u2026", "num_citations": "29\n", "authors": ["1818"]}
{"title": "On the state of the art in verification and validation in cyber physical systems\n", "abstract": " It is widely held that debugging cyber-physical systems (CPS) is challenging. However, few empirical studies quantitatively and qualitatively capture the state of the art and the state of the practice in debugging CPS and analyze what major research gaps remain. This paper presents an empirical study of verification and validation in CPS through three complementary methods: a structured on-line survey of CPS developers and researchers, semi-structured interviews with professional CPS developers from various backgrounds, and a qualitative analysis of state of the art in research related to CPS testing. We find that traditional verification and validation methodologies are not sufficient for cyber-physical systems, and we identify several potential avenues for future work. Our key findings include:(i) many CPS developers do not use traditional verification and validation methodologies and rely heavily on trial and error;(ii) simulation alone is not enough to capture dangerous bugs in CPS;(iii) it is widely acknowledged that the main challenges in CPS debugging are related to models of software systems, models of physics, and integration of cyber and physics models. These findings aid in identifying research directions to address the identified key challenges in CPS verification and validation.", "num_citations": "27\n", "authors": ["1818"]}
{"title": "Systematic testing of database engines using a relational constraint solver\n", "abstract": " We describe an automated approach for systematic black-box testing of database management systems (DBMS) using a relational constraint solver. We reduce the problem of automated database testing into generating three artifacts: (1) SQL queries for testing, (2) meaningful input data to populate test databases, and (3) expected results of executing the queries on the generated data. We leverage our previous work on ADUSA and the Automated SQL Query Generator to form high-quality test suites for testing DBMS engines. This paper presents a detailed description of our framework for Automated SQL Query Generation using the Alloy tool-set, and experimental results of testing database engines using our framework. We show how the main SQL grammar constraints can be solved by translating them to Alloy constraints to generate semantically and syntactically correct SQL queries. We also present\u00a0\u2026", "num_citations": "26\n", "authors": ["1818"]}
{"title": "CRN++: Molecular programming language\n", "abstract": " Synthetic biology is a rapidly emerging research area, with expected wide-ranging impact in biology, nanofabrication, and medicine. A key technical challenge lies in embedding computation in molecular contexts where electronic micro-controllers cannot be inserted. This necessitates effective representation of computation using molecular components. While previous work established the Turing-completeness of chemical reactions, defining representations that are faithful, efficient, and practical remains challenging . This paper introduces CRN++, a new language for programming deterministic (mass-action) chemical kinetics to perform computation. We present its syntax and semantics, and build a compiler translating CRN++ programs into chemical reactions, thereby laying the foundation of a comprehensive framework for molecular programming. Our language addresses the key challenge of embedding\u00a0\u2026", "num_citations": "25\n", "authors": ["1818"]}
{"title": "Evaluation of semantic interference detection in parallel changes: an exploratory experiment\n", "abstract": " Parallel developments are becoming increasingly prevalent in the building and evolution of large-scale software systems. Our previous studies of a large industrial project showed that there was a linear correlation between the degree of parallelism and the likelihood of defects in the changes. To further study the relationship between parallel changes and faults, we have designed and implemented an algorithm to detect \"direct\" semantic interference between parallel changes. To evaluate the analyzer's effectiveness in fault prediction, we designed an experiment in the context of an industrial project. We first mine the change and version management repositories to find sample versions sets of different degrees of parallelism. We investigate the interference between the versions with our analyzer. We then mine the change and version repositories to find out what faults were discovered subsequent to the analyzed\u00a0\u2026", "num_citations": "25\n", "authors": ["1818"]}
{"title": "EdSketch: Execution-driven sketching for Java\n", "abstract": " Sketching is a synthesis approach that allows users to provide high-level insights into a synthesis problem and let synthesis tools complete low-level details. Users write sketches\u2014partial programs that have \u201choles\u201d and provide test assertions as the correctness criteria. The sketching techniques fill the holes with code fragments such that the complete program satisfies all test assertions. Traditional techniques translate the sketching problem to propositional satisfiability formulas and leverage SAT solvers to generate programs with the desired functionality. While effective for a range of small well-defined domains, such translation-based approaches have a key limitation when applying to real applications: They require either translating all relevant libraries that are invoked directly or indirectly by the given sketch or creating models of those libraries, which requires much manual effort. This paper introduces\u00a0\u2026", "num_citations": "24\n", "authors": ["1818"]}
{"title": "An incremental approach to scope-bounded checking using a lightweight formal method\n", "abstract": " We present a novel approach to optimize scope-bounded checking programs using a relational constraint solver. Given a program and its correctness specification, the traditional approach translates a bounded code segment of the entire program into a declarative formula and uses a constraint solver to search for any correctness violations. Scalability is a key issue with such approaches since for non-trivial programs the formulas are complex and represent a heavy workload that can choke the solvers. Our insight is that bounded code segments, which can be viewed as a set of (possible) execution paths, naturally lend to incremental checking through a partitioning of the set, where each partition represents a sub-set of paths. The partitions can be checked independently, and thus the problem of scope-bounded checking for the given program reduces to several sub-problems, where each sub-problem\u00a0\u2026", "num_citations": "24\n", "authors": ["1818"]}
{"title": "Efficient symbolic execution of strings for validating web applications\n", "abstract": " Symbolic execution is a popular technique used in formal verification of software and hardware systems. In this paper we examine three different ways of performing symbolic execution for the purpose of formal model checking, on web application software implemented with the Java programming language. We evaluate the different techniques on real industrial applications and compare them on issues of performance, implementation ease, and ease-of-use. There are some special characteristics of web applications like extensive use of string inputs that need to be tackled before traditional symbolic execution techniques become feasible. We provide details of how we have solved those issues.", "num_citations": "24\n", "authors": ["1818"]}
{"title": "Whispec: White-box testing of libraries using declarative specifications\n", "abstract": " We present a novel framework, Whispec, for white-box testing of methods that manipulate structurally complex data, such as those that pervade library classes. Given method preconditions as declarative constraints, our framework systematically generates test inputs for the methods to maximize their code coverage. The constraints are written in Alloy, a first-order language based on relations. To test a method, given its precondition constraint, we first solve that constraint using the Alloy Analyzer and translate a solution into a test input. Next, we execute the method on that input and build the path condition for the resulting execution path. Then, we run the analyzer on a conjunction of the precondition and a new path condition that represents a previously unexplored path. The solution is translated to a new test input, which triggers the next round of test generation. The iterative execution of Whispec can systematically\u00a0\u2026", "num_citations": "24\n", "authors": ["1818"]}
{"title": "An empirical study of boosting spectrum-based fault localization via pagerank\n", "abstract": " Manual debugging is notoriously tedious and time-consuming. Therefore, various automated fault localization techniques have been proposed to help with manual debugging. Among the existing fault localization techniques, spectrum-based fault localization (SBFL) is one of the most widely studied techniques due to being lightweight. The focus of the existing SBFL techniques is to consider how to differentiate program entities (i.e., one dimension in program spectra); indeed, this focus is aligned with the ultimate goal of finding the faulty lines of code. Our key insight is to enhance the existing SBFL techniques by additionally considering how to differentiate tests (i.e., the other dimension in program spectra), which, to the best of our knowledge, has not been studied in prior work. We present our basic approach, PRFL, a lightweight technique that boosts SBFL by differentiating tests using PageRank algorithm\u00a0\u2026", "num_citations": "23\n", "authors": ["1818"]}
{"title": "Specification-based test repair using a lightweight formal method\n", "abstract": " When a program evolves, its test suite must be modified to reflect changes in requirements or to account for new feature additions. This problem of modifying tests as a program evolves is termed test repair. Existing approaches either assume that updated implementation is correct, or assume that most test repairs require simply fixing compilation errors caused by refactoring of previously tested implementation. This paper focuses on the problem of repairing semantically broken or outdated tests by leveraging specifications. Our technique, Spectr, employs a lightweight formal method to perform specification-based repair. Specifically, Spectr supports the Alloy language for writing specifications and uses its SAT-based analyzer for repairing JUnit tests. Since Spectr utilizes specifications, it works even when the specification is modified but the change has not yet been implemented in code\u2013in such a case, S\u00a0\u2026", "num_citations": "23\n", "authors": ["1818"]}
{"title": "Are these bugs really\" normal\"?\n", "abstract": " Understanding the severity of reported bugs is important in both research and practice. In particular, a number of recently proposed mining-based software engineering techniques predict bug severity, bug report quality, and bug-fix time, according to this information. Many bug tracking systems provide a field \"severity\" offering options such as \"severe\", \"normal\", and \"minor\", with \"normal\" as the default. However, there is a widespread perception that for many bug reports the label \"normal\" may not reflect the actual severity, because reporters may overlook setting the severity or may not feel confident enough to do so. In many cases, researchers ignore \"normal\" bug reports, and thus overlook a large percentage of the reports provided. On the other hand, treating them all together risks mixing reports that have very diverse properties. In this study, we investigate the extent to which \"normal\" bug reports actually have the\u00a0\u2026", "num_citations": "22\n", "authors": ["1818"]}
{"title": "Verification of multi-agent negotiations using the alloy analyzer\n", "abstract": " Multi-agent systems provide an increasingly popular solution in problem domains that require management of uncertainty and a high degree of adaptability. Robustness is a key design criterion in building multi-agent systems. We present a novel approach for the design of robust multi-agent systems. Our approach constructs a model of the design of a multi-agent system in Alloy, a declarative language based on relations, and checks the properties of the model using the Alloy Analyzer, a fully automatic analysis tool for Alloy models. While several prior techniques exist for checking properties of multi-agent systems, the novelty of our work is that we can check properties of coordination and interaction, as well as properties of complex data structures that the agents may internally be manipulating or even sharing. This is the first application of Alloy to checking properties of multi-agent systems. Such unified\u00a0\u2026", "num_citations": "21\n", "authors": ["1818"]}
{"title": "Generating representation invariants of structurally complex data\n", "abstract": " Generating likely invariants using dynamic analyses is becoming an increasingly effective technique in software checking methodologies. This paper presents Deryaft, a novel algorithm for generating likely representation invariants of structurally complex data. Given a small set of concrete structures, Deryaft analyzes their key characteristics to formulate local and global properties that the structures exhibit. For effective formulation of structural invariants, Deryaft focuses on graph properties, including reachability, and views the program heap as an edge-labeled graph.               Deryaft outputs a Java predicate that represents the invariants; the predicate takes an input structure and returns true if and only if it satisfies the invariants. The invariants generated by Deryaft directly enable automation of various existing frameworks, such as the Korat test generation framework and the Juzi data structure repair\u00a0\u2026", "num_citations": "20\n", "authors": ["1818"]}
{"title": "Ranger: Parallel analysis of alloy models by range partitioning\n", "abstract": " We present a novel approach for parallel analysis of models written in Alloy, a declarative extension of first-order logic based on relations. The Alloy language is supported by the fully automatic Alloy Analyzer, which translates models into propositional formulas and uses off-the-shelf SAT technology to solve them. Our key insight is that the underlying constraint satisfaction problem can be split into subproblems of lesser complexity by using ranges of candidate solutions, which partition the space of all candidate solutions. Conceptually, we define a total ordering among the candidate solutions, split this space of candidates into ranges, and let independent SAT searches take place within these ranges' endpoints. Our tool, Ranger, embodies our insight. Experimental evaluation shows that Ranger provides substantial speedups (in several cases, superlinear ones) for a variety of hard-to-solve Alloy models, and that\u00a0\u2026", "num_citations": "18\n", "authors": ["1818"]}
{"title": "Memoise: a tool for memoized symbolic execution\n", "abstract": " This tool paper presents a tool for performing memoized symbolic execution (Memoise), an approach we developed in previous work for more efficient application of symbolic execution. The key idea in Memoise is to allow re-use of symbolic execution results across different runs of symbolic execution without having to re-compute previously computed results as done in earlier approaches. Specifically, Memoise builds a trie-based data structure to record path exploration information during a run of symbolic execution, optimizes the trie for the next run, and re-uses the resulting trie during the next run. Our tool optimizes symbolic execution in three standard scenarios where it is commonly applied: iterative deepening, regression analysis, and heuristic search. Our tool Memoise builds on the Symbolic PathFinder framework to provide more efficient symbolic execution of Java programs and is available online for\u00a0\u2026", "num_citations": "18\n", "authors": ["1818"]}
{"title": "Annotations for alloy: Automated incremental analysis using domain specific solvers\n", "abstract": " Alloy is a declarative modeling language based on first-order logic with sets and relations. Alloy problems are analyzed fully automatically by the Alloy Analyzer. The analyzer translates a problem for given bounds to a propositional formula for which it searches a satisfying assignment via an off-the-shelf propositional satisfiability (SAT) solver. Hence, the performed analysis is a bounded exhaustive search and increasing the bounds leads to a combinatorial explosion.               We increase the efficiency of the Alloy Analyzer by performing incremental analysis via domain specific solvers. We introduce annotations that define data types, operations on these data types, and bindings from data types to domain specific solvers. This meta-data is utilized to automatically partition a problem into sub-problems and opportunistically solve independent sub-problems in parallel using dedicated constraint solvers. We\u00a0\u2026", "num_citations": "18\n", "authors": ["1818"]}
{"title": "Kato: A program slicing tool for declarative specifications\n", "abstract": " This paper presents Kato, a tool that implements a novel class of optimizations that are inspired by program slicing for imperative languages but are applicable to analyzable declarative languages, such as Alloy. Kato implements a novel algorithm for slicing declarative models written in Alloy and leverages its relational engine KodKodfor analysis. Given an Alloy model, Kato identifies a slice representing the model's core: a satisfying instance for the core can systematically be extended into a satisfying instance for the entire model, while unsatisfiability of the core implies unsatisfiability of the entire model. The experimental results show that for a variety of subject models Kato's slicing algorithm enables an order of magnitude speed-up over Alloy's default translation to SAT.", "num_citations": "18\n", "authors": ["1818"]}
{"title": "Sketchfix: A tool for automated program repair approach using lazy candidate generation\n", "abstract": " Manually locating and removing bugs in faulty program is often tedious and error-prone. A common automated program repair approach called generate-and-validate (G&V) iteratively creates candidate fixes, compiles them, and runs these candidates against the given tests. This approach can be costly due to a large number of re-compilations and re-executions of the program. To tackle this limitation, recent work introduced the SketchFix approach that tightly integrates the generation and validation phases, and utilizes runtime behaviors to substantially prune a large amount of repair candidates. This tool paper describes our Java implementation of SketchFix, which is an open-source library that we released on Github. Our experimental evaluation using Defects4J benchmark shows that SketchFix can significantly reduce the number of re-compilations and re-executions compared to other approaches and work\u00a0\u2026", "num_citations": "17\n", "authors": ["1818"]}
{"title": "A specification-based approach to testing software product lines\n", "abstract": " This paper presents a specification-based approach for systematic testing of products from a software product line. Our approach uses specifications given as formulas in Alloy, a first-order logic based on relations. Alloy formulas can be checked for satisfiability using the Alloy Analyzer. The fully automatic analyzer, given an Alloy formula and a scope, ie, a bound on the universe of discourse, searches for an instance, ie, a valuation to the relations in the formula such that it evaluates to true. The analyzer translates an Alloy formula (for the given scope) to a propositional formula and finds an instance using an off-the-shelf SAT solver. The use of an enumerating solver enables systematic test generation.", "num_citations": "17\n", "authors": ["1818"]}
{"title": "Automated model repair for alloy\n", "abstract": " Automated program repair is an active research area. However, existing research focuses mostly on imperative code, e.g. in Java. In this paper, we study the problem of repairing declarative models in Alloy - a first order relational logic with transitive closure. We introduce ARepair, the first technique for repairing Alloy models. ARepair follows the spirit of traditional automated program repair techniques. Specifically, ARepair takes as input a faulty Alloy model and a test suite that contains some failing test, and outputs a repaired model that is correct with respect to the given tests. ARepair integrates ideas from mutation testing and program synthesis to provide an effective solution for repairing Alloy models. The experimental results show that ARepair can fix 28 out of 38 real-world faulty models we collected.", "num_citations": "16\n", "authors": ["1818"]}
{"title": "Aunit: A test automation tool for alloy\n", "abstract": " Software models help improve the reliability of software systems: models can convey requirements, and can analyze design and implementation properties. A key strength of Alloy, a commonly used first-order modeling language, is the Alloy Analyzer tool-set. The Analyzer allows users to execute commands over models by leveraging a fully automatic SAT-based analysis engine. However, prior to the introduction of AUnit - a testing framework for Alloy - users had to rely on ad-hoc practices to validate their models. In this paper, we present our efforts to establish a formal testing environment in the Alloy Analyzer by creating an AUnit extension. We present additional grammar to support test case creation, as well as the details for executing test suites, calculating test suite coverage, and automatically generating test suites. The tool is available as a stand-alone executable at the following URL (https://sites.google.com\u00a0\u2026", "num_citations": "16\n", "authors": ["1818"]}
{"title": "FaultTracer: a spectrum\u2010based approach to localizing failure\u2010inducing program edits\n", "abstract": " Detecting faults in evolving systems is important. Change impact analysis has been shown to be effective for finding faults during software evolution. For example, Chianti represents program edits as atomic changes, selects affected tests, and determines a subset of affecting changes that might have caused test failures. However, the number of affecting changes related to each test failure in practice may still be overwhelming for manual inspection. In this paper, we present a novel approach, FaultTracer, which ranks program edits according to their suspiciousness to reduce developer effort in manually inspecting affecting changes. FaultTracer adapts spectrum\u2010based fault localization techniques, which assume the statements that are primarily executed by failed tests are more suspicious, and applies them in tandem with an enhanced change impact analysis to identify failure\u2010inducing edits more precisely. We\u00a0\u2026", "num_citations": "16\n", "authors": ["1818"]}
{"title": "An empirical study of structural constraint solving techniques\n", "abstract": " Structural constraint solving allows finding object graphs that satisfy given constraints, thereby enabling software reliability tasks, such as systematic testing and error recovery. Since enumerating all possible object graphs is prohibitively expensive, researchers have proposed a number of techniques for reducing the number of potential object graphs to consider as candidate solutions. These techniques analyze the structural constraints to prune from search object graphs that cannot satisfy the constraints. Although, analytical and empirical evaluations of individual techniques have been done, comparative studies of different kinds of techniques are rare in the literature. We performed an experiment to evaluate the relative strengths and weaknesses of some key structural constraint solving techniques. The experiment considered four techniques using: a model checker, a SAT solver, a symbolic execution\u00a0\u2026", "num_citations": "16\n", "authors": ["1818"]}
{"title": "Symbolic execution for attribution and attack synthesis in neural networks\n", "abstract": " This paper introduces DeepCheck, a new approach for validating Deep Neural Networks (DNNs) based on core ideas from program analysis, specifically from symbolic execution. DeepCheck implements techniques for lightweight symbolic analysis of DNNs and applies them in the context of image classification to address two challenging problems: 1) identification of important pixels (for attribution and adversarial generation); and 2) creation of adversarial attacks. Experimental results using the MNIST data-set show that DeepCheck's lightweight symbolic analysis provides a valuable tool for DNN validation.", "num_citations": "15\n", "authors": ["1818"]}
{"title": "Staged symbolic execution\n", "abstract": " Recent advances in constraint solving technology and raw computation power have led to a substantial increase in the effectiveness of techniques based on symbolic execution for systematic bug finding. However, scaling symbolic execution remains a challenging problem.", "num_citations": "15\n", "authors": ["1818"]}
{"title": "A case for white-box testing using declarative specifications poster abstract\n", "abstract": " Software testing, the most commonly used technique for validating the quality of software, is a labor intensive process, and typically accounts for about half the total cost of software development and maintenance. Automating testing not only reduces the cost of producing software but also increases the reliability of modern software. White-box testing and black-box testing are two commonly used techniques that have complementary strengths. White-box testing uses the internal structures (such as control flow or data flow) of programs. Black-box uses an external interface. Automated approaches to black-box testing make extensive use of specifications, e.g., to specify test inputs or test oracles. In unit testing of object-oriented code, preconditions, which define constraints on legal method inputs, and postconditions, which define expected behavior and outputs, form an integral part of specifications.", "num_citations": "15\n", "authors": ["1818"]}
{"title": "Efficiently generating structurally complex inputs with thousands of objects\n", "abstract": " We present Shekoosh, a novel framework for constraint-based generation of structurally complex inputs of large sizes. Given a Java predicate that represents the desired structural integrity constraints, Shekoosh systematically explores the input space of the predicate and generates inputs that satisfy the given constraints. While the problem of generating an input that satisfies all the given constraints is hard, generating a structure at random, which may not satisfy the constraints but has a desired number of objects is straightforward. Indeed, a structure generated at random is highly unlikely to satisfy any of the desired constraints. However, it can be repaired to transform it so that it satisfies all the desired constraints.               Experiments show that Shekoosh can efficiently generate structures that are up to 100 times larger than those possible with previous algorithms, including those that are based on a\u00a0\u2026", "num_citations": "15\n", "authors": ["1818"]}
{"title": "Design and validation of a general security model with the alloy analyzer\n", "abstract": " We define secure communication to require message integrity, confidentiality, authentication and non-repudiation. This high-level definition forms the basis for many widely accepted definitions of secure communication. In order to understand how security constrains the design of our secure connectors, we have created new logical formulas that define these security properties. Our novel definitions use first-order epistemic and modal logics to precisely describe the constituent properties of secure communications. Our definitions should be applicable to describe security in the general case. We subsequently codified our logical formulas into the Alloy language and executed them using the Alloy Analyzer to validate that our models are correct. This paper presents the definition of our security model, our Alloy implementation, and the results of our validation efforts.", "num_citations": "15\n", "authors": ["1818"]}
{"title": "MuAlloy: a mutation testing framework for alloy\n", "abstract": " Creating models of software systems and analyzing the models helps develop more reliable systems. A well-known software modeling tool-set is embodied by the declarative language Alloy and its automatic SAT-based analyzer. Recent work introduced a novel approach to testing Alloy models to validate their correctness in the spirit of traditional software testing: AUnit defined the foundations of testing (unit tests, test execution, and model coverage) for Alloy, and MuAlloy defined mutation testing (mutation operators, mutant generation, and equivalent mutant checking) for Alloy. This tool paper describes our Java implementation of MuAlloy, which is a command-line tool that we released as an open-source project on GitHub. Our experimental results show that MuAlloy is efficient and practical. The demo video for MuAlloy can be found at https://youtu.be/3lvnQKiLcLE.", "num_citations": "14\n", "authors": ["1818"]}
{"title": "Bounded exhaustive test input generation from hybrid invariants\n", "abstract": " We present a novel technique for producing bounded exhaustive test suites from hybrid invariants, i.e., invariants that are expressed imperatively, declaratively, or as a combination of declarative and imperative predicates. Hybrid specifications are processed using known mechanisms for the imperative and declarative parts, but combined in a way that enables us to exploit information from the declarative side, such as tight bounds computed from the declarative specification, to improve the search both on the imperative and declarative sides. Moreover, our technique automatically evaluates different possible ways of processing the imperative side, and the alternative settings (imperative or declarative) for parts of the invariant available both declaratively and imperatively, to decide the most convenient invariant configuration with respect to efficiency in test generation. This is achieved by transcoping, i.e., by\u00a0\u2026", "num_citations": "14\n", "authors": ["1818"]}
{"title": "A novel framework for locating software faults using latent divergences\n", "abstract": " Fault localization, i.e., identifying erroneous lines of code in a buggy program, is a tedious process, which often requires considerable manual effort and is costly. Recent years have seen much progress in techniques for automated fault localization, specifically using program spectra \u2013 executions of failed and passed test runs provide a basis for isolating the faults. Despite the progress, fault localization in large programs remains a challenging problem, because even inspecting a small fraction of the lines of code in a large problem can require substantial manual effort. This paper presents a novel framework for fault localization based on latent divergences \u2013 an effective method for feature selection in machine learning. Our insight is that the problem of fault localization can be reduced to the problem of feature selection, where lines of code correspond to features. We also present an experimental evaluation\u00a0\u2026", "num_citations": "14\n", "authors": ["1818"]}
{"title": "Deryaft\n", "abstract": " Deryaft is a tool for generating likely representation invariants of structurally complex data. Given a small set of concrete structures, Deryaft analyzes their key characteristics to formulate local and global properties that the structures exhibit. For effective formulation of structural invariants, Deryaft focuses on graph properties, including reachability, and views the program heap as an edge-labeled graph. Deryaft outputs a Java predicate that represents the invariants; the predicate takes an input structure and returns true if and only if it satisfies the invariants.", "num_citations": "14\n", "authors": ["1818"]}
{"title": "Bounded exhaustive test-input generation on GPUs\n", "abstract": " Bounded exhaustive testing is an effective methodology for detecting bugs in a wide range of applications. A well-known approach for bounded exhaustive testing is Korat. It generates all test inputs, up to a given small size, based on a formal specification that is written as an executable predicate and characterizes properties of desired inputs. Korat uses the predicate's executions on candidate inputs to implement a backtracking search based on pruning to systematically explore the space of all possible inputs and generate only those that satisfy the specification.   This paper presents a novel approach for speeding up test generation for bounded exhaustive testing using Korat. The novelty of our approach is two-fold. One, we introduce a new technique for writing the specification predicate based on an abstract representation of candidate inputs, so that the predicate executes directly on these abstract structures and\u00a0\u2026", "num_citations": "13\n", "authors": ["1818"]}
{"title": "Context-sensitive relevancy analysis for efficient symbolic execution\n", "abstract": " Symbolic execution is a flexible and powerful, but computationally expensive technique to detect dynamic behaviors of a program. In this paper, we present a context-sensitive relevancy analysis algorithm based on weighted pushdown model checking, which pinpoints memory locations in the program where symbolic values can flow into. This information is then utilized by a code instrumenter to transform only relevant parts of the program with symbolic constructs, to help improve the efficiency of symbolic execution of Java programs. Our technique is evaluated on a generalized symbolic execution engine that is developed upon Java Path Finder with checking safety properties of Java applications. Our experiments indicate that this technique can effectively improve the performance of the symbolic execution engine with respect to the approach that blindly instruments the whole program.", "num_citations": "13\n", "authors": ["1818"]}
{"title": "Natural language processing and program analysis for supporting todo comments as software evolves\n", "abstract": " Natural language elements (eg, API comments, todo comments) form a substantial part of software repositories. While developers routinely use many natural language elements (eg, todo comments) for communication, the semantic content of these elements is often neglected by software engineering techniques and tools. Additionally, as software evolves and development teams re-organize, these natural language elements are frequently forgotten, or just become outdated, imprecise and irrelevant. We envision several techniques, which combine natural language processing and program analysis, to help developers maintain their todo comments. Specifically, we propose techniques to synthesize code from comments, make comments executable, answer questions in comments, improve comment quality, and detect dangling comments.", "num_citations": "12\n", "authors": ["1818"]}
{"title": "Combinatorial generation of structurally complex test inputs for commercial software applications\n", "abstract": " Despite recent progress in automated test generation research, significant challenges remain for applying these techniques on large-scale software systems. These systems under test often require structurally complex test inputs within a large input domain. It is challenging to automatically generate a reasonable number of tests that are both legal and behaviorally-diverse to exercise these systems. Constraint-based test generation is an effective approach for generating structurally complex inputs for systematic testing. While this approach can typically generate large numbers of tests, it has limited scalability\u2013tests generated are usually only up to a small bound on input size. Combinatorial test generation, eg, pair-wise testing, is a more scalable approach but is challenging to apply on commercial software systems that require complex input structures that cannot be formed by using arbitrary combinations. This\u00a0\u2026", "num_citations": "12\n", "authors": ["1818"]}
{"title": "Ranged model checking\n", "abstract": " We introduce ranged model checking, a novel technique for more effective checking of Java programs using the Java PathFinder (JPF) model checker. Our key insight is that the order in which JPF makes non-deterministic choices denes a total ordering of execution paths it explores in the program it checks. Thus, two in-order paths define a range for restricting the model checking run by defining a start point and an end point for JPF's exploration. Moreover, a given set of paths can be linearly ordered to define consecutive, (essentially) non-overlapping ranges that partition the exploration space and can be explored separately. While restricting the run of a model checker is a well-known technique in model checking, the key novelty of our work is conceptually to restrict the run using vertical boundaries rather than the traditional approach of using a horizontal boundary, i.e., the search depth bound. Initial results using\u00a0\u2026", "num_citations": "12\n", "authors": ["1818"]}
{"title": "Eliminating products to test in a software product line\n", "abstract": " A Software Product Line (SPL) is a family of programs where each program is defined by a unique combination of features. Developing a set of programs with commonalities and variabilities in this way can significantly reduce both the time and cost of software development. However, as the number of programs may be exponential in the number of features, testing an SPL, the phase to which the majority of software development is dedicated, becomes especially challenging [12].", "num_citations": "12\n", "authors": ["1818"]}
{"title": "Optimizing incremental scope-bounded checking with data-flow analysis\n", "abstract": " We present a novel approach to optimize incremental scope-bounded checking of programs using a relational constraint solver. Given a program and its correctness specification, scope-bounded checking encodes control-flow and data-flow of bounded code segments into declarative formulas and uses constraint solvers to search for correctness violations. For non-trivial programs, the formulas are often complex and represent a heavy workload that can choke the solvers. To scale scope-bounded checking, our previous work introduced an incremental approach that uses the program's control-flow as a basis of partitioning the program and generating several sub-formulas, which represent simpler problem instances for the underlying solvers. This paper introduces a new approach that uses the program's dataflow, specifically variable-definitions, as a basis for incremental checking. Experimental results show that\u00a0\u2026", "num_citations": "11\n", "authors": ["1818"]}
{"title": "SCA: a semantic conflict analyzer for parallel changes\n", "abstract": " Parallel changes are becoming increasingly prevalent in the development of large scale software system. To further study the relationship between parallel changes and faults, we have designed and implemented a semantic conflict analyzer (SCA) to detect semantic interference between parallel changes. SCA combines data dependency analysis and program slicing. Data dependency analysis can disclose the semantic structure of the program. And program slicing can identify which semantic structures are impacted by a change. By comparing the overlap between impacts of two changes, SCA can detect if there are semantic interference between the two changes. An experiment with an industrial project shows that SCA can detect a significant portion of the faults in highly parallel changes. SCA is effective in predicting faults (based on\" direct\" semantic interference detection) in changes made within a short time\u00a0\u2026", "num_citations": "11\n", "authors": ["1818"]}
{"title": "Sequential circuits for relational analysis\n", "abstract": " The alloy tool-set has been gaining popularity as an alternative to traditional manual testing and checking for design correctness. Alloy uses a first-order relational logic for modeling designs. The alloy analyzer translates alloy formulas for a given scope, i.e., a bound on the universe of discourse, to Boolean formulas in conjunctive normal form (CNF), which are subsequently checked using prepositional satisfiability solvers. We present SERA, a novel algorithm that compiles a relational logic formula for a given scope to a sequential circuit. There are two key advantages of sequential circuits: they form a more succinct representation than CNF formulas, sometimes by several orders of magnitude. Also sequential circuits are amenable to a range of powerful automatic analysis techniques that have no counterparts for CNF formulas. Our experiments show that SERA, used in conjunction with a sequential circuit analyzer\u00a0\u2026", "num_citations": "11\n", "authors": ["1818"]}
{"title": "Testing an intentional naming scheme using genetic algorithms\n", "abstract": " Various attempts have been made to use genetic algorithms (GAs) for software testing, a problem that consumes a large amount of time and effort in software development. We demonstrate the use of GAs in automating testing of complex data structures and methods for manipulating them, which to our knowledge has not been successfully displayed before on non-trivial software structures. We evaluate the effectiveness of our GA-based test suite generation technique by applying it to test the design and implementation of the Intentional Naming System (INS), a new scheme for resource discovery and service location in a dynamic networked environment. Our analysis using GAs reveals serious problems with both the design of INS and its inventors\u2019 implementation.", "num_citations": "11\n", "authors": ["1818"]}
{"title": "Optimizing parallel Korat using invalid ranges\n", "abstract": " Constraint-based input generation enables systematic testing for effective bug finding, but requires exploration of very large spaces of candidate inputs. This paper introduces a novel approach to optimize input generation using Korat\u2013a solver for constraints written as imperative predicates in Java\u2013when Korat is executed more than once for the same constraint solving problem. Our key insight is that in certain application scenarios the Korat search over the same state space and constraint is repeated across separate runs of Korat, and an earlier run can be summarized to optimize a later run. We introduce invalid ranges to represent parts of the exploration space that do not contain any valid inputs but must be explicitly explored by Korat. Our approach directly prunes these parts in a future run of Korat over the same search problem. We develop our approach for two settings: a sequential setting where the Korat\u00a0\u2026", "num_citations": "10\n", "authors": ["1818"]}
{"title": "A sketching-based approach for debugging using test cases\n", "abstract": " Manually locating and removing bugs in faulty code is often tedious and error-prone. Despite much progress in automated debugging, developing effective debugging techniques remains a challenge. This paper introduces a novel approach that uses a well-known program synthesis technique to automate debugging. As inputs, our approach takes a program and a test suite (with some passing and some failing tests), similar to various other recent techniques. Our key insight is to reduce the problem of finding a fix to the problem of program sketching. We translate the faulty program into a sketch of the correct program, and use off-the-shelf sketching technology to create a program that is correct with respect to the given test cases. The experimental evaluation using a suite of small, yet complex programs shows that our prototype embodiment of our approach is more effective than previous state-of-the-art.", "num_citations": "10\n", "authors": ["1818"]}
{"title": "Efficiently running test suites using abstract undo operations\n", "abstract": " The last decade has seen many advances in test input generation, specifically using systematic approaches that can enumerate many tests. While such approaches have enhanced our ability to find bugs in programs, running large numbers of tests remains a time consuming and expensive task, especially for tests that execute operations on external resources, such as a file system or a network. This paper presents a novel technique for optimizing execution of suites of tests, where several tests in a suite may contain common initial execution -- a property often exhibited by systematically generated suites, e.g., those for bounded exhaustive testing. Our insight is that we can cluster execution of such tests by defining abstract-level undo operations, which allow a common execution segment to be performed once, and its result to be shared across the tests, which then perform the rest of their operations. We present our\u00a0\u2026", "num_citations": "10\n", "authors": ["1818"]}
{"title": "Testsage: Regression test selection for large-scale web service testing\n", "abstract": " Regression testing is an important but expensive activity in software development. Among various types of tests, web service tests are usually one of the most expensive (due to network communications) but widely adopted types of tests in commercial software development. Regression test selection (RTS) aims to reduce the number of tests which need to be retested by only running tests that are affected by code changes. Although a large number of RTS techniques have been proposed in the past few decades, these techniques have not been adopted on large-scale web service testing. This is because most existing RTS techniques either require direct code dependency between tests and code under test or cannot be applied on large scale systems with enough efficiency. In this paper, we present a novel RTS technique, TestSage, that performs RTS for web service tests on large scale commercial software. With\u00a0\u2026", "num_citations": "9\n", "authors": ["1818"]}
{"title": "Approximate transformations as mutation operators\n", "abstract": " Mutation testing is a well-established approach for evaluating test-suite quality by modifying code using syntax-changing (and potentially semantics-changing) transformations, called mutation operators. This paper proposes approximate transformations as new mutation operators that can give novel insights about the code and tests. Approximate transformations are semantics-changing transformations used in the emerging area of approximate computing, but so far they were not evaluated for mutation testing. We found that approximate transformations can be effective mutation operators. We compared three approximate transformations with a set of conventional mutation operators from the literature, on nine open-source Java subjects. The results showed that approximate transformations change program behavior differently from conventional mutation operators. Our analysis uncovered code patterns in which\u00a0\u2026", "num_citations": "9\n", "authors": ["1818"]}
{"title": "A family of generalized entropies and its application to software fault localization\n", "abstract": " Fault localization is the process of locating faulty lines of code in a buggy program. This paper presents a novel approach to automate fault localization by combining feature selection (a fundamental concept in machine learning) with mutual information (a fundamental concept in information theory). Specifically, we present a family of generalized entropies for computing generalized mutual information, which enables feature selection. The family generalizes well-known entropies, such as Shannon and Renyi entropies, and lays the foundation of a uniform entropy-based technique for fault localization. We perform an experimental evaluation of our approach using the Siemens suite of subject programs. Experimental results show that while using mutual information based on generalized entropies allows more accurate fault localization that traditional techniques, the specific entropies used do not have a significant\u00a0\u2026", "num_citations": "9\n", "authors": ["1818"]}
{"title": "Semantic impact and faults in source code changes: An empirical study\n", "abstract": " Changes to source code have become a critical factor in fault predictions. Text or syntactic approaches have been widely used. Textual analysis focuses on changed text fragments while syntactic analysis focuses on changed syntactic entities. Although both of them have demonstrated their advantages in experimental results, they only study code fragments modified during changes. Because of semantic dependencies within programs, we believe that code fragments impacted by changes are also helpful. Given a source code change, we identify its impact by program slicing along the variable def-use chains. To evaluate the effectiveness of change impacts in fault detection and prediction, we compare impacted code with changed code according to size and fault density. Our experiment on the change history of a successful industrial project shows that: fault density in changed and impacted fragments are higher\u00a0\u2026", "num_citations": "9\n", "authors": ["1818"]}
{"title": "EdSynth: Synthesizing API sequences with conditionals and loops\n", "abstract": " Good API design enables many clients to effectively use the core functionality implemented by the APIs. For real-world applications however, correctly using the APIs and identifying what methods to use and how to invoke them appropriately can be challenging. Researchers have developed a number of API synthesis approaches that enable a semantically rich form of API completion where the client provides a description of desired functionality, e.g., in the form of test suites, and the automatic tools create method sequences using the desired APIs based on the given correctness criteria (e.g., all given tests pass). However, existing API synthesis approaches are largely limited to creating single basic blocks of code and do not readily handle multiple blocks in the presence of loops (or recursion) and complex test executions. A key issue with handling multiple blocks is the very large space of possible method\u00a0\u2026", "num_citations": "8\n", "authors": ["1818"]}
{"title": "Barad\u2013a GUI testing framework based on symbolic execution\n", "abstract": " While Graphical User Interfaces (GUIs) have become ubiquitous, testing them remains largely adhoc. Since the state of a GUI is modified by events on the GUI widgets, a useful approach is to consider test input for a GUI as an event sequence. Due to the combinatorial nature of these sequences, testing a GUI thoroughly is problematic and time-consuming. Moreover, the possible values for certain GUI widgets, such as a textbox, are also combinatorial compounding the problem. This paper presents Barad, a novel GUI testing framework based on symbolic execution. Barad addresses uniformly event-flow as well as data-flow in GUI applications: generating tests in the form of event sequences and data inputs. We generate test cases as chains of event listener method invocations and map these chains to event sequences that force the execution of those invocations. Since listeners for some events in the GUI are not present, this approach prunes significant regions of the event input space. We introduce symbolic widgets as a higher level of abstraction, which enables symbolic execution of GUI applications. We obtain data inputs through executing symbolically the generated test cases (chains of event listener method invocations). Barad generates significantly fewer tests compared to traditional GUI testing techniques, while improving branch and statement coverage.", "num_citations": "8\n", "authors": ["1818"]}
{"title": "An automated approach for writing Alloy specifications using instances\n", "abstract": " We present aDeryaft, a novel technique for automating the writing of specifications in Alloy-a first-order relational logic with transitive closure. Alloy is particularly suitable for specifying structural properties of software, and has steadily been gaining popularity due to the rapid feedback that its SAT-based analyzer provides fully automatically. Alloy users however, still have to manually write specifications in a declarative language and use a paradigm that is different from the commonly used imperative programming paradigm. aDeryaft assists Alloy users in writing their specifications by providing a novel specification-writing approach, which is particularly tailored to users, such as engineers or practitioners in industry, who may not have much prior experience or proficiency in Alloy or similar logics. The user constructs by hand a few small concrete instances that represent the constraints of the software structure they\u00a0\u2026", "num_citations": "8\n", "authors": ["1818"]}
{"title": "Symbolic execution for importance analysis and adversarial generation in neural networks\n", "abstract": " Deep Neural Networks (DNN) are increasingly used in a variety of applications, many of them with serious safety and security concerns. This paper describes DeepCheck, a new approach for validating DNNs based on core ideas from program analysis, specifically from symbolic execution. DeepCheck implements novel techniques for lightweight symbolic analysis of DNNs and applies them to address two challenging problems in DNN analysis: 1) identification of important input features and 2) leveraging those features to create adversarial inputs. Experimental results with an MNIST image classification network and a sentiment network for textual data show that DeepCheck promises to be a valuable tool for DNN analysis.", "num_citations": "7\n", "authors": ["1818"]}
{"title": "Mutation testing meets approximate computing\n", "abstract": " One of the most widely studied techniques in software testing research is mutation testing - a technique for evaluating the quality of test suites. Despite over four decades of academic advances in this technique, mutation testing has not found its way to mainstream development. The key issue with mutation testing is its high computational cost: it requires running the test suite against not just the program under test but against typically thousands of mutants, i.e., syntactic variants, of the program. Our key insight is that exciting advances in the upcoming, yet unrelated, area of approximate computing allow us to define a principled approach that provides the benefits of traditional mutation testing at a fraction of its usually large cost. This paper introduces the idea of a novel approach, named Approximut, that blends the power of mutation testing with the practicality of approximate computing. To demonstrate the potential of\u00a0\u2026", "num_citations": "7\n", "authors": ["1818"]}
{"title": "Non-semantics-preserving transformations for higher-coverage test generation using symbolic execution\n", "abstract": " Symbolic execution is a well-studied method that has a number of useful applications, including generation of high-quality test suites that find many bugs. However, scaling it to real-world applications is a significant challenge, as it depends on the often expensive process of solving constraints on program inputs. Our insight is that when the goal of symbolic execution is test generation, non-semantics-preserving program transformations can reduce the cost of symbolic execution and the tests generated for the transformed programs can still serve as quality suites for the original program. We present five such transformations based on a few different program simplification heuristics that are designed to lower the cost of symbolic execution for input generation. As enabling technology we use the KLEE symbolic execution engine and the LLVM compiler infrastructure. We evaluate our transformations using a suite of\u00a0\u2026", "num_citations": "7\n", "authors": ["1818"]}
{"title": "Deep molecular programming: a natural implementation of binary-weight ReLU neural networks\n", "abstract": " Embedding computation in molecular contexts incompatible with traditional electronics is expected to have wide ranging impact in synthetic biology, medicine, nanofabrication and other fields. A key remaining challenge lies in developing programming paradigms for molecular computation that are well-aligned with the underlying chemical hardware and do not attempt to shoehorn ill-fitting electronics paradigms. We discover a surprisingly tight connection between a popular class of neural networks (binary-weight ReLU aka BinaryConnect) and a class of coupled chemical reactions that are absolutely robust to reaction rates. The robustness of rate-independent chemical computation makes it a promising target for bioengineering implementation. We show how a BinaryConnect neural network trained in silico using well-founded deep learning optimization techniques, can be compiled to an equivalent chemical reaction network, providing a novel molecular programming paradigm. We illustrate such translation on the paradigmatic IRIS and MNIST datasets. Toward intended applications of chemical computation, we further use our method to generate a chemical reaction network that can discriminate between different virus types based on gene expression levels. Our work sets the stage for rich knowledge transfer between neural network and molecular programming communities.", "num_citations": "6\n", "authors": ["1818"]}
{"title": "Using test ranges to improve symbolic execution\n", "abstract": " Symbolic execution is a powerful systematic technique for checking programs, which has received a lot of research attention during the last decade. In practice however, the technique remains hard to scale. This paper introduces SynergiSE, a novel approach to improve symbolic execution by tackling a key bottleneck to its wider adoption: costly and incomplete constraint solving. To mitigate the cost, SynergiSE introduces a succinct encoding of constraint solving results, thereby enabling symbolic execution to be distributed among different workers while sharing and re-using constraint solving results among them without having to communicate databases of constraint solving results. To mitigate the incompleteness, SynergiSE introduces an integration of complementary approaches for testing, e.g., search-based test generation, with symbolic execution, thereby enabling symbolic execution and other\u00a0\u2026", "num_citations": "6\n", "authors": ["1818"]}
{"title": "Korat-API: a framework to enhance korat to better support testing and reliability techniques\n", "abstract": " Logical constraints play an important role in software testing and reliability. For example, constraints written by users allow automating test case generation and systematic bug finding, and constraints computed using data-flow of a program allow studying its reliability. The key to practical usefulness of constraints is the effectiveness and efficiency of constraint solvers that determine constraint feasibility and produce solutions.", "num_citations": "6\n", "authors": ["1818"]}
{"title": "A synergistic approach for distributed symbolic execution using test ranges\n", "abstract": " Symbolic execution is a systematic program analysis technique that has received a lot of attention in the research community. However, scaling symbolic execution continues to pose a major challenge. This paper introduces Synergise, a novel two-fold integration approach. One, it integrates distributed analysis and constraint re-use to enhance symbolic execution using feasible ranges, which allow sharing of constraint solving results among different workers without communicating or sharing potentially large constraint databases (as required traditionally). Two, it integrates complementary techniques for test input generation, e.g., search-based generation and symbolic execution, for creating higher quality tests using unexplored ranges, which allows symbolic execution to re-use tests created by another technique for effective distribution of exploration of previously unexplored paths.", "num_citations": "6\n", "authors": ["1818"]}
{"title": "Improving constraint-based test input generation using Korat\n", "abstract": " Korat is an existing technique for test input generation using imperative constraints that describe properties of desired inputs written as Java predicates, termed RepOk methods, which are executable checks for those properties. Korat efficiently prunes the space of candidate inputs for the RepOk method by executing it on candidate inputs and monitoring the object fields that RepOk accesses in deciding if the properties are satisfied. While Korat generates inputs effectively, its correctness and efficiency rely on two assumptions about the RepOk methods. For correctness, Korat assumes the RepOk methods do not use the Java reflection API for field accesses; the use of reflection renders Korat unable to enumerate all desired inputs. For efficiency, Korat assumes the RepOk methods do not make unnecessary field accesses, which can reduce the effectiveness of Korat\u2019s pruning. Our thesis addresses both these limitations. To support reflection, we build on the core Korat to enhance it such that it can monitor field accesses based on reflection. To assist the users with writing RepOk\u2019s, we introduce a static analysis tool that detects potential places where the input RepOk may be edited to enhance performance of Korat. We also present experimental results using a suite of standard data structure subjects.", "num_citations": "6\n", "authors": ["1818"]}
{"title": "Scaling symbolic execution using staged analysis\n", "abstract": " Recent advances in constraint solving technology and raw computation power have led to a substantial increase in the effectiveness of techniques based on symbolic execution for systematic bug finding. However, scaling symbolic execution remains a challenging problem. We present a novel approach to increase the efficiency of symbolic execution for systematic testing of object-oriented programs. Our insight is that we can apply symbolic execution in stages, rather than the traditional approach of applying it all at once, to compute abstract symbolic inputs that can later be shared across different methods to test them systematically. For example, a class invariant can provide the basis of generating abstract symbolic tests that are then used to symbolically execute several methods that require their inputs to satisfy the invariant. We present an experimental evaluation to compare our approach against KLEE\u00a0\u2026", "num_citations": "6\n", "authors": ["1818"]}
{"title": "Symbolic execution of Alloy models\n", "abstract": " Symbolic execution is a technique for systematic exploration of program behaviors using symbolic inputs, which characterize classes of concrete inputs. Symbolic execution is traditionally performed on imperative programs, such as those in C/C++ or Java. This paper presents a novel approach to symbolic execution for declarative programs, specifically those written in Alloy \u2013 a first-order, declarative language based on relations. Unlike imperative programs that describe how to perform computation to conform to desired behavioral properties, declarative programs describe what the desired properties are, without enforcing a specific method for computation. Thus, symbolic execution does not directly apply to declarative programs the way it applies to imperative programs. Our insight is that we can leverage the fully automatic, SAT-based analysis of the Alloy Analyzer to enable symbolic execution of Alloy\u00a0\u2026", "num_citations": "6\n", "authors": ["1818"]}
{"title": "Pythia: Automatic generation of counterexamples for ACL2 using Alloy\n", "abstract": " A key research problem in automated theorem proving is generating examples and counterexamples to guide the discovery of proofs. We present Pythia, a framework that connects ACL2 with the SAT-based Alloy Analyzer, a tool for solving formulas in first-order logic with transitive closure using bounded exhaustive checking. Pythia takes as inputs an Alloy model of the ACL2 type system together with an ACL2 formula and automatically generates examples of ACL2 objects that satisfy the model\u2019s constraints. Pythia then produces an ACL2 script that evaluates the formula on the generated objects to search for counterexamples. We test Pythia on a set of classic ACL2 non-theorems and find that it effectively discovers counterexamples to such formulas. Based on our experiments, we suggest making ACL2 more novice-friendly by adding the option to test every formula on a set of basic examples before attempting the proof.", "num_citations": "6\n", "authors": ["1818"]}
{"title": "Program slicing for declarative models\n", "abstract": " The declarative modeling language Alloy and its automatic analyzer provide an effective tool-set for building designs of systems and checking their properties. The Alloy Analyzer performs bounded exhaustive analysis using off-the-shelf SAT solvers. The analyzer's performance hinges on the complexity of the models and so far, its feasibility has been shown only within small bounds. With the growing popularity of analyzable declarative modeling languages, in general, and Alloy, in particular, it is imperative to develop new techniques that allow the underlying solvers to scale to real systems.We present Kato, a novel technique that defines program slicing for declarative models and enables efficient analyses using existing analyzers, such as the Alloy Analyzer. Given a declarative model, Kato identifies a slice, which represents the model's core: a satisfying solution to the slice can be systematically extended to\u00a0\u2026", "num_citations": "6\n", "authors": ["1818"]}
{"title": "TestMC: testing model counters using differential and metamorphic testing\n", "abstract": " Model counting is the problem for finding the number of solutions to a formula over a bounded universe. This is a classic problem in computer science that has seen many recent advances in techniques and tools that tackle it. These advances have led to applications of model counting in many domains, eg, quantitative program analysis, reliability, and security. Given the sheer complexity of the underlying problem, today's model counters employ sophisticated algorithms and heuristics, which result in complex tools that must be heavily optimized. Therefore, establishing the correctness of implementations of model counters necessitates rigorous testing. This experience paper presents an empirical study on testing industrial strength model counters by applying the principles of differential and metamorphic testing together with bounded exhaustive input generation and input minimization. We embody these principles\u00a0\u2026", "num_citations": "5\n", "authors": ["1818"]}
{"title": "A Study of Symmetry Breaking Predicates and Model Counting\n", "abstract": " Propositional model counting is a classic problem that has recently witnessed many technical advances and novel applications. While the basic model counting problem requires computing the number of all solutions to the given formula, in some important application scenarios, the desired count is not of all solutions, but instead, of all unique solutions up to isomorphism. In such a scenario, the user herself must try to either use the full count that the model counter returns to compute the count up to isomorphism, or ensure that the input formula to the model counter adequately captures the symmetry breaking predicates so it can directly report the count she desires. We study the use of CNF-level and domain-level symmetry breaking predicates in the context of the state-of-the-art in model counting, specifically the leading approximate model counter ApproxMC and the recently introduced exact model counter ProjMC. As benchmarks, we use a range of problems, including structurally complex specifications of software systems and constraint satisfaction problems. The results show that while it is sometimes feasible to compute the model counts up to isomorphism using the full counts that are computed by the model counters, doing so suffers from poor scalability. The addition of symmetry breaking predicates substantially assists model counters. Domain-specific predicates are particularly useful, and in many cases can provide full symmetry breaking to enable highly efficient model counting up to isomorphism. We hope our study motivates new research on designing model counters that directly account for symmetries to facilitate further applications of\u00a0\u2026", "num_citations": "5\n", "authors": ["1818"]}
{"title": "A study of learning data structure invariants using off-the-shelf tools\n", "abstract": " Data structure invariants play a key role in checking correctness of code, e.g., a model checker can use an invariant, e.g., acyclicity of a binary tree, that is written in the form of an assertion to search for program executions that violate it, e.g., erroneously introduce a cycle in the structure. Traditionally, the properties are written manually by the users. However, writing them manually can itself be error-prone, which can lead to false alarms or missed bugs. This paper presents a controlled experiment on applying a suite of off-the-shelf machine learning (ML) tools to learn properties of dynamically allocated data structures that reside on the program heap. Specifically, we use 10 data structure subjects, and systematically create training and test data for 6 ML methods, which include decision trees, support vector machines, and neural networks, for binary classification, e.g., to classify input structures as valid binary\u00a0\u2026", "num_citations": "5\n", "authors": ["1818"]}
{"title": "Arepair: a repair framework for alloy\n", "abstract": " Researchers have proposed many automated program repair techniques for imperative languages, e.g. Java. However, little work has been done to repair programs written in declarative languages, e.g. Alloy. We proposed ARepair, the first automated program repair technique for faulty Alloy models. ARepair takes as input a faulty Alloy model and a set of tests that capture the desired model properties, and produces a fixed model that passes all tests. ARepair uses tests written for the recently introduced AUnit framework, which provides a notion of unit testing for Alloy models. In this paper, we describes our Java implementation of ARepair, which is a command-line tool, released as an open-source project on GitHub. Our experimental results show that ARepair is able to fix 28 out of 38 real-world faulty models we collected. The demo video for ARepair can be found at https://youtu.be/436drvWvbEU.", "num_citations": "5\n", "authors": ["1818"]}
{"title": "Learning to optimize the alloy analyzer\n", "abstract": " Constraint-solving is an expensive phase for scenario finding tools. It has been widely observed that there is no single \"dominant\" SAT solver that always wins in every case; instead, the performance of different solvers varies by cases. Some SAT solvers perform particularly well for certain tasks while other solvers perform well for other tasks. In this paper, we propose an approach that uses machine learning techniques to automatically select a SAT solver for one of the widely used scenario finding tools, i.e. Alloy Analyzer, based on the features extracted from a given model. The goal is to choose the best SAT solver for a given model to minimize the expensive constraint solving time. We extract features from three different levels, i.e. the Alloy source code level, the Kodkod formula level and the boolean formula level. The experimental results show that our portfolio approach outperforms the best SAT solver by 30\u00a0\u2026", "num_citations": "5\n", "authors": ["1818"]}
{"title": "Incremental analysis of evolving alloy models\n", "abstract": " Alloy is a well-known tool-set for building and analyzing software designs and models. Alloy\u2019s key strengths are its intuitive notation based on relational logic, and its powerful analysis engine backed by propositional satisfiability (SAT) solvers to help users find subtle design flaws. However, scaling the analysis to the designs of real-world systems remains an important technical challenge. This paper introduces a new approach, iAlloy, for more efficient analysis of Alloy models. Our key insight is that users often make small and frequent changes and repeatedly run the analyzer when developing Alloy models, and the development cost can be reduced with the incremental analysis over these changes. iAlloy is based on two techniques\u2013a static technique based on a lightweight impact analysis and a dynamic technique based on solution re-use\u2013which in many cases helps avoid potential costly SAT solving. Experimental results show that iAlloy significantly outperforms Alloy analyzer in the analysis of evolving Alloy models with more than 50% reduction in SAT solver calls on average, and up to 7x speedup.", "num_citations": "5\n", "authors": ["1818"]}
{"title": "The comKorat Tool: Unified Combinatorial and Constraint-Based Generation of Structurally Complex Tests\n", "abstract": " This tool paper presents comKorat, which unifies constraint-based generation of structurally complex tests with combinatorial testing. Constraint-based test generation is an effective approach for generating structurally complex inputs for systematic testing. While this approach can typically generate large numbers of tests, it has limited scalability \u2013 tests generated are usually only up\u00a0to a small bound on input size. Combinatorial test generation, e.g., pair-wise testing, is a more scalable approach but is challenging to apply on commercial software systems that require complex input structures that cannot be formed by using arbitrary combinations. The comKorat tool integrates Korat and ACTS test generators to generate test suites for large scale commercial systems. This paper presents a case-study of applying comKorat on a software application developed at Yahoo!. The experimental results show that\u00a0\u2026", "num_citations": "5\n", "authors": ["1818"]}
{"title": "Text-Based Intelligent Content Filtering on Social Platforms\n", "abstract": " Social platforms have become one of the popular mediums of information sharing and communication over the Internet today. People share all types of contents such as text, images, audio and video using these social platforms. Though information gained using these social platforms can be very useful for people around the globe, some of the user generated contents are very negative as they contain abusive, racial, offensive and insulting material. Thus, there is a need for an effective online content filtering technique which blocks these negative contents while not disturbing the access of users to rest of the contents available on these sites. Current techniques simply filter on the basis of URLs blocking and keyword matching or either rely on a large database of pre-classified web addresses. The problem is how to intelligently filter the negative contents, rather than filtering entire websites using their URLs or\u00a0\u2026", "num_citations": "5\n", "authors": ["1818"]}
{"title": "Dynamic shape analysis using spectral graph properties\n", "abstract": " Dynamically allocated data structures pervade imperative and object-oriented programs. Automated analysis and testing of such programs requires reasoning about their data structures. The structures often have complex structural properties, such as a cyclicity of the object graph rooted at a given pointer. Such properties pose a challenge for automated reasoning. Shape analysis is a class of techniques that address reasoning about such programs. Traditionally, shape analysis is performed using static analysis of the program code. More recently, dynamic techniques for shape analysis have been developed, which inspect program states to identify properties of data structures. This paper presents a novel dynamic technique, which adapts well-studied results from graph theory to determine the shape of the program's key data structures. Specifically, spectral graph theory, a field that studies the properties of a graph\u00a0\u2026", "num_citations": "5\n", "authors": ["1818"]}
{"title": "Mixed constraints for test input generation-an initial exploration\n", "abstract": " The use of specifications provides an effective technique to automate testing. A form of specification that automates generation of test inputs is logical constraints that define properties of desired inputs. Recent advances in constraint solving technology have made the use of constraints particularly attractive. However, manually writing constraints to define complex inputs to real-world programs can pose a significant burden on the user and restrict their wider use. We envision a novel approach to facilitate the use of constraints: to provide a mixed notation for writing the properties. Our key insight is that different properties can lend to easier formulation using different programming paradigms. Thus, a notation that supports more than one paradigm, e.g., declarative and imperative paradigms, can enable achieving a sweet-spot in minimizing the manual effort required in constraint formulation. Moreover, solving such\u00a0\u2026", "num_citations": "5\n", "authors": ["1818"]}
{"title": "A framework for writing trigger-action todo comments in executable format\n", "abstract": " Natural language elements, eg, todo comments, are frequently used to communicate among developers and to describe tasks that need to be performed (actions) when specific conditions hold on artifacts related to the code repository (triggers), eg, from the Apache Struts project:\u201cremove expectedJDK15 and if () after switching to Java 1.6\u201d. As projects evolve, development processes change, and development teams reorganize, these comments, because of their informal nature, frequently become irrelevant or forgotten. We present the first framework, dubbed TrigIt, to specify trigger-action todo comments in executable format. Thus, actions are executed automatically when triggers evaluate to true. TrigIt specifications are written in the host language (eg, Java) and are evaluated as part of the build process. The triggers are specified as query statements over abstract syntax trees, abstract representation of build\u00a0\u2026", "num_citations": "4\n", "authors": ["1818"]}
{"title": "Extension-aware automated testing based on imperative predicates\n", "abstract": " Bounded exhaustive testing (BET) techniques have been shown to be effective for detecting faults in software. BET techniques based on imperative predicates, enumerate all test inputs up to the given bounds such that each test input satisfies the properties encoded by the predicate. The search space is bounded by the user, who specifies the number of objects of each type and the list of values for each field of each type. To optimize the search, existing techniques detect isomorphic instances and record accessed fields during the execution of a predicate. However, these optimizations are extension-unaware, i.e., they do not speed up the search when the predicate is modified, say due to a fix or additional properties. We present a technique, named iGen, that speeds up test generation when imperative predicates are extended. iGen memoizes intermediate results of a test generation and reuses the results in a\u00a0\u2026", "num_citations": "4\n", "authors": ["1818"]}
{"title": "Certified symbolic execution\n", "abstract": " We propose a certification approach for checking the analysis results produced by symbolic execution. Given a program P under test, an analysis producer performs symbolic execution on P and creates a certificate C that represents the results of symbolic execution. The analysis consumer checks the validity of C with respect to P using efficient symbolic re-execution of P. The certificates are simple to create and easy to validate. Each certificate is a list of witnesses that include: test inputs that validate path feasibility without requiring any constraint solving; and infeasibility summaries that provide hints on how to efficiently establish path infeasibility. To account for incompleteness in symbolic execution (due to incompleteness of the backend solver), the certificate also contains an incompleteness summary. Our approach deploys constraint slicing and other heuristics as performance optimizations\u00a0\u2026", "num_citations": "4\n", "authors": ["1818"]}
{"title": "Using Frankencerts for Automated Adversarial Testing of Certificate Validation\n", "abstract": " Modern network security rests on the Secure Sock-ets Layer (SSL) and Transport Layer Security (TLS) protocols. Distributed systems, mobile and desktop applications, embedded devices, and all of secure Web rely on SSL/TLS for protection against network attacks. This protection critically depends on whether SSL/TLS clients correctly validate X. 509 certificates presented by servers during the SSL/TLS handshake protocol. We design, implement, and apply the first methodology for large-scale testing of certificate validation logic in SSL/TLS implementations. Our first ingredient is \u201cfrankencerts,\u201d synthetic certificates that are randomly mutated from parts of real cer-tificates and thus include unusual combinations of extensions and constraints. Our second ingredient is differential testing: if one SSL/TLS implementation accepts a certificate while another rejects the same certificate, we use the discrepancy as an oracle for finding flaws in individual implementations. Differential testing with frankencerts uncovered 208 dis-crepancies between popular SSL/TLS implementations such as OpenSSL, NSS, CyaSSL, GnuTLS, PolarSSL, MatrixSSL, etc. Many of them are caused by serious security vulnerabilities. For example, any server with a valid X. 509 version 1 certificate can act as a rogue certificate authority and issue fake certificates for any domain, enabling man-in-the-middle attacks against MatrixSSL and GnuTLS. Several implementations also accept certificate authorities created by unauthorized issuers, as well as certificates not intended for server authentication. We also found serious vulnerabilities in how users are warned about certificate\u00a0\u2026", "num_citations": "4\n", "authors": ["1818"]}
{"title": "Sequential circuits for program analysis\n", "abstract": " A number of researchers have proposed the use of Boolean satisfiability solvers for verifying C programs. They encode correctness checks as Boolean formulas using finitization: loops and recursion are bounded, as is the size of the input instances. The SAT approach has been shown to find subtle bugs with reasonable resources. However, it does not scale well; in particular, it lacks the ability to handle larger bounds. We present SEBAC, which can handle the same class of programs as the SAT approach, and scales to bounds that are orders of magnitude higher. The key difference between SEBAC and SAT techniques is SEBAC's use of imperative Boolean sequential circuits, which are Boolean formulas with memory elements instead of the Boolean formulas which are stateless", "num_citations": "4\n", "authors": ["1818"]}
{"title": "Symbolic execution for gui testing\n", "abstract": " A Graphical User Interface (GUI) is an abstraction providing users with a more natural way of interacting with computers. It consists of objects like buttons, text boxes, toolbars etc. The communication between users and GUIs is event driven. Users can modify the state of a GUI and trigger events that lead to the execution of different code fragments. Hence, in order to test a GUI one should execute event sequences simulating user behaviors. While the state of some GUI widgets is limited to a small number of values (the value of a radio button), others have a wide range of possible states (the value of a text box). Such widgets are used for data input from the user in the form of text (alphabetic or numeric). Since program execution may depend on the user input, it is a challenge to select suitable values in a way that allows thorough testing. We propose symbolic execution for obtaining these inputs. During symbolic execution, each branch of the program is visited and the constraints for control variables are resolved determining if it is reachable or not. Thus, by symbolically executing code that depends on user input, we can obtain values that ensure visiting each reachable branch in the program.", "num_citations": "4\n", "authors": ["1818"]}
{"title": "A progress bar for the JPF search using program executions\n", "abstract": " Software model checkers, such as JPF, are routinely used to explore executions of programs that have very large state spaces. Sometimes the exploration can take a significant amount of time before a bug is found or the checking is complete, in which case the user must patiently wait, possibly for quite some time, to learn the result of checking. A progress bar that accurately shows the status of the search provides the user useful feedback about the time expected for the search to complete. This paper introduces JPFBar, a novel technique to estimate the percentage of work done by the JPF search by computing weights for the execution paths it explores and summing up the weights. JPF-Bar is embodied into a listener that prints a progress bar during JPF execution. An experimental evaluation using a variety of Java subjects shows that JPFBar provides accurate information about the search\u2019s progress and fares well in comparison with a state-based progress estimator that is part of the standard JPF distribution. We implement JPFBar as a JPF listener and it is available at https://github. com/kaiyuanw/JPFBar.", "num_citations": "3\n", "authors": ["1818"]}
{"title": "Repairing intricate faults in code using machine learning and path exploration\n", "abstract": " Debugging remains costly and tedious, especially for code that performs intricate operations that are conceptually complex to reason about. We present MLR, a novel approach for repairing faults in such operations, specifically in the context of complex data structures. Our focus is on faults in conditional statements. Our insight is that an integrated approach based on machine learning and systematic path exploration can provide effective repairs. MLR mines the data-spectra of the passing and failing executions of conditional branches to prune the search space for repair and generate patches that are likely valid beyond the existing test-suite. We apply MLR to repair faults in small but complex data structure subjects to demonstrate its efficacy. Experimental results show that MLR has the potential to repair this fault class more effectively than state-of-the-art repair tools.", "num_citations": "3\n", "authors": ["1818"]}
{"title": "Improving dynamic analysis with data flow analysis\n", "abstract": " Many challenges in software quality can be tackled with dynamic analysis. However, these techniques are often limited in their efficiency or scalability as they are often applied uniformly to an entire program. In this thesis, we show that dynamic program analysis can be made significantly more efficient and scalable by first performing a static data flow analysis so that the dynamic analysis can be selectively applied only to important parts of the program. We apply this general principle to the design and implementation of two different systems, one for runtime security policy enforcement and the other for software test input generation.   For runtime security policy enforcement, we enforce user-defined policies using a dynamic data flow analysis that is more general and flexible than previous systems. Our system uses the user-defined policy to drive a static data flow analysis that identifies and instruments only the statements that may be involved in a security vulnerability, often eliminating the need to track most objects and greatly reducing the overhead. For taint analysis on a set of five server programs, the slowdown is only 0.65%, two orders of magnitude lower than previous taint tracking systems. Our system also has negligible overhead on file disclosure vulnerabilities, a problem that taint tracking cannot handle.   For software test case generation, we introduce the idea of targeted testing, which focuses testing effort on select parts of the program instead of treating all program paths equally. Our \u201cBullseye\u201d system uses a static analysis performed with respect to user-defined \u201cinteresting points\u201d to steer the search down certain paths, thereby finding\u00a0\u2026", "num_citations": "3\n", "authors": ["1818"]}
{"title": "Validation of a security model with the Alloy analyzer\n", "abstract": " We define secure communication to require message integrity, confidentiality, authentication and non-repudiation. This high-level definition forms the basis for many widely accepted definitions of secure communication. In order to understand how security constrains the design of our secure connectors, we have created new logical formulas that define these security properties. Our novel definitions use first-order epistemic and modal logics to precisely describe the constituent properties of secure communications. Our definitions should be applicable to describe security in the general case. We subsequently codified our logical formulas into the Alloy language and executed them using the Alloy Analyzer to validate that our models are correct. This paper presents the definition of our security model, our Alloy implementation, and the results of our validation efforts.", "num_citations": "3\n", "authors": ["1818"]}
{"title": "Sequential encoding for relational analysis\n", "abstract": " We present SERA, a novel algorithm for compiling a class of finitized relational logic formulas into sequential circuits. The compiled sequential structures use much fewer variables than traditional approaches that compile to SAT, and allow us to iteratively apply powerful reduction, abstraction, and decision algorithms from transformation-based verification (TBV). Our SERA prototype leverages TBV to analyze formulas written in Alloy, a first-order language with transitive closure that is based on relations. The experimental results show that SERA can check formulas for scopes\u2013bounds on universe of discourse\u2013that are an order of magnitude higher than those feasible with existing combinational approaches such as the Alloy Analyzer.", "num_citations": "3\n", "authors": ["1818"]}
{"title": "Quantifying the exploration of the Korat solver for imperative constraints\n", "abstract": " Tools that explore very large state spaces to nd bugs, e.g., when model checking, or to nd solutions, e.g., when constraint solving, can take a considerable amount of time before the search termi- nates, and the user may not get useful feedback on the state of the search during that time. Our focus is a tool that solves im- perative constraints to provide automated test input generation for systematic testing. Speci cally, we introduce a technique to quantify the exploration of Korat, a well-known tool that explores the bounded space of all candidate inputs and enumerates desired inputs that satisfy given constraints. Our technique quanti es the size of the input space as it is explored by the Korat search, and provides the user exact information on the size of the remaining input space. In addition, it allows studying key characteristics of the search, such as the distribution of solutions as the search nds them. We implement the\u00a0\u2026", "num_citations": "2\n", "authors": ["1818"]}
{"title": "Predictive constraint solving and analysis\n", "abstract": " We introduce a new idea for enhancing constraint solving engines that drive many analysis and synthesis techniques that are powerful but have high complexity. Our insight is that in many cases the engines are run repeatedly against input constraints that encode problems that are related but of increasing complexity, and domain-specific knowledge can reduce the complexity. Moreover, even for one formula the engine may perform multiple expensive tasks with commonalities that can be estimated and exploited. We believe these relationships lay a foundation for making the engines more effective and scalable. We illustrate the viability of our idea in the context of a well-known solver for imperative constraints, and discuss how the idea generalizes to more general purpose methods.", "num_citations": "2\n", "authors": ["1818"]}
{"title": "A study of the learnability of relational properties: model counting meets machine learning (MCML)\n", "abstract": " This paper introduces the MCML approach for empirically studying the learnability of relational properties that can be expressed in the well-known software design language Alloy. A key novelty of MCML is quantification of the performance of and semantic differences among trained machine learning (ML) models, specifically decision trees, with respect to entire (bounded) input spaces, and not just for given training and test datasets (as is the common practice). MCML reduces the quantification problems to the classic complexity theory problem of model counting, and employs state-of-the-art model counters. The results show that relatively simple ML models can achieve surprisingly high performance (accuracy and F1-score) when evaluated in the common setting of using training and test datasets--even when the training dataset is much smaller than the test dataset--indicating the seeming simplicity of learning\u00a0\u2026", "num_citations": "2\n", "authors": ["1818"]}
{"title": "JPR: Replaying JPF traces using standard JVM\n", "abstract": " Java PathFinder (JPF) is a backtrackable Java Virtual Machine (JVM), which is implemented in Java and runs on a standard JVM (e.g., Oracle HotSpot). Thus, a JPF developer can use off-the- shelf Java debuggers (e.g., jdb) when debugging code that makes up JPF. JPF explores all non-deterministic executions of a given target program and monitors for property violations. To facilitate debugging of the target program, JPF can capture and replay the execution trace that leads to a property violation. While the deterministic replay is invaluable, the replay with JPF does not allow the developer to attach an off-the-shelf Java debugger to the target program (e.g., step through the application code, set breakpoints, etc.). We present a technique, dubbed JPR, to improve the debugging experience of the JPF captured traces by migrating the JPF traces to a new format that can be executed using the standard JVM. JPR\u00a0\u2026", "num_citations": "2\n", "authors": ["1818"]}
{"title": "Sketch4J: Execution-Driven Sketching for Java\n", "abstract": " Sketching is a relatively recent approach to program synthesis, which has shown much promise. e key idea in sketching is to allow users to write partial programs that have \u201choles\u201d and provide test harnesses or reference implementations, and let synthesis tools create program fragments that ll the holes such that the resulting complete program has the desired functionality. Traditional solutions to the sketching problem perform a translation to SAT and employ CEGIS. While e ective for a range of programs, when applied to real applications, such translation-based approaches have a key limitation: they require either translating all relevant libraries that are invoked directly or indirectly by the given sketch\u2013which can lead to impractical SAT problems\u2013or creating models of those libraries\u2013which can require much manual e ort. is paper introduces execution-driven sketching, a novel approach for synthesis of Java programs using a backtracking search that is commonly employed in so ware model checkers. e key novelty of our work is to introduce e ective pruning strategies to e ciently explore the actual program behaviors in presence of libraries and to provide a practical solution to sketching small parts of real-world applications, which may use complex constructs of modern languages, such as re ection or native calls. Our tool S 4J embodies our approach in two forms: a stateful search based on the Java PathFinder model checker; and a stateless search based on re-execution inspired by the VeriSo model checker. Experimental results show that S 4J\u2019s performance compares well with the well-known SAT-based Sketch system for a range of small but\u00a0\u2026", "num_citations": "2\n", "authors": ["1818"]}
{"title": "Brace: Assertion-driven development of cyber-physical systems applications\n", "abstract": " Developing cyber-physical systems (CPS) is challenging because correctness depends on both logical and physical states, which are difficult to observe collectively. Developers must repeatedly rerun the system, often in different physical environments, while observing its behavior. The developers then tweak the hardware and software until the entire system appears to meet some minimum requirements. This process is tedious, error-prone, and lacks rigor. In addition, there are always underlying and often unstated assumptions about the physical environment that are subject to variance; these assumptions should be captured early and explicitly in the development process. To address these issues, we present Brace, a framework that allows developers to explicitly specify both physical and logical assumptions and expected behaviors. Brace then enables run-time checking of these combined physical and logical specifications, provided in the form of assertions, using the physical environment in which a CPS application is running. Brace uses physics models and temporal semantics to guide CPS developers in creating appropriate assertions and to check specified assertions for inconsistencies with the physical world. This paper presents our initial investigation into the requirements and semantics of such assertions, which we call cyber-physical assertions, and the realization of cyber-physical assertions within the Brace framework. We discuss our experience implementing and using Brace with a variety of sensors.", "num_citations": "2\n", "authors": ["1818"]}
{"title": "Localization of faults in software programs using Bernoulli divergences\n", "abstract": " Software testing and debugging play a vital role in developing reliable software. A crucial part of debugging is fault localization - the process of identifying the locations of bugs, i.e., lines of code that are faulty due to a human error. For real systems, fault localization can be costly, requiring much human time and effort. To address this code, researchers have proposed a number of useful techniques for fault localization. However, effective and accurate fault localization remains an elusive goal at present. This paper presents a novel approach, which is based on Bernoulli divergences - a family of divergences that use Bernoulli random variables - to automate fault localization. Thus, our approach takes concepts from information theory and machine learning and applies them to software engineering. Initial experimental results a suite of programs show this approach for fault localization holds promise.", "num_citations": "2\n", "authors": ["1818"]}
{"title": "A case for Alloy annotations for efficient incremental analysis via domain specific solvers\n", "abstract": " Alloy is a declarative modelling language based on first-order logic with sets and relations. Alloy formulas are checked for satisfiability by the fully automatic Alloy Analyzer. The analyzer, given an Alloy formula and a scope, i.e. a bound on the universe of discourse, searches for an instance i.e. a valuation to the sets and relations in the formula, such that it evaluates to true. The analyzer translates the Alloy problem to a propositional formula for which it searches a satisfying assignment via an off-the-shelf propositional satisfiability (SAT) solver. The SAT solver performs an exhaustive search and increasing the scope leads to the combinatorial explosion problem. We envision annotations, a meta-data facility used in imperative languages, as a means of augmenting Alloy models to enable more efficient analysis by specifying the priority, i.e. order of solving, of a given constraint and the slover to be used. This additional\u00a0\u2026", "num_citations": "2\n", "authors": ["1818"]}
{"title": "Alloy annotations for efficient incremental analysis via domain specific solvers\n", "abstract": " Alloy is a declarative modelling language based on first-order logic with sets and relations. Alloy formulas are checked for satisfiability by the fully automatic Alloy Analyzer. The analyzer, given an Alloy formula and a scope, ie a bound on the universe of discourse, searches for an instance ie a valuation to the sets and relations in the formula, such that it evaluates to true. The analyzer translates the Alloy problem to a propositional formula for which it searches a satisfying assignment via an offthe-shelf propositional satisfiability (SAT) solver. The SAT solver performs an exhaustive search and increasing the scope leads to the combinatorial explosion problem. We envision annotations, a meta-data facility used in imperative languages, as a means of augmenting Alloy models to enable more efficient analysis by specifying the priority, ie order of solving, of a given constraint and the slover to be used. This additional information would enable using the solutions to a particular constraint as partial solutions to the next in case constraint priority is specified and using a specific solver for reasoning about a given constraint in case a constraint solver is specified.", "num_citations": "2\n", "authors": ["1818"]}
{"title": "Abstract State Machines, Alloy, B and Z: Second International Conference, ABZ 2010, Orford, QC, Canada, February 22-25, 2010, Proceedings\n", "abstract": " This book constitutes the proceedings of the Second International Conference on Abstract State Machines, B and Z, which took place in Orford, QC, Canada, in February 2010. The 26 full papers presented were carefully reviewed and selected from 60 submissions. The book also contains two invited talks and abstracts of 18 short papers which address work in progress, industrial experience reports and tool descriptions. The papers cover recent advances in four equally rigorous methods for software and hardware development: abstract state machines (ASM), Alloy, B and Z. They share a common conceptual framework, centered around the notions of state and operation, and promote mathematical precision in the modeling, verification and construction of highly dependable systems.", "num_citations": "2\n", "authors": ["1818"]}
{"title": "A Case for GUI Testing Using Symbolic Execution Poster Abstract\n", "abstract": " A Graphical User Interface (GUI) consists of virtual objects (widgets) that are more intuitive to use, for example buttons, edit boxes, etc. While GUIs have become ubiquitous, testing them remains largely ad- hoc. In contrast with console applications where there is only one point of interaction (the command line), GUIs provide multiple points each of which might have different states. This structure makes GUI testing especially challenging because of its large input space.", "num_citations": "2\n", "authors": ["1818"]}
{"title": "Detecting Semantic Interference in Parallel Changes: An Exploratory Case Study\n", "abstract": " Parallel changes are becoming increasingly prevalent in the development of large scale software system. To further study the relationship between parallel changes and faults, we have designed and implemented an algorithm to detect semantic interference between parallel changes. To evaluate the effectiveness and efficiency of this analyzer, we designed an exploratory case study in the context of an industrial project. We first mine the change and version management repositories to find sample versions sets of different degrees of parallelism. We investigate the interference between the versions with our analyzer. We then mine the change and version repositories to find out what faults were discovered subsequent to the analyzed interfering versions. We use the match rate between semantic interference and faults to evaluate the effectiveness of the semantic interference detection tool. We also evaluate its efficiency by the lapse for finding (an average of 150 days) and fixing the faults associated with those samples. The case study shows that the analyzer is most effective in detecting non-pointer variable interference in adaptive changes with a high degree of parallelism. Further, the analyzer is both efficient (averaging less than two minutes) and scalable (requiring only the local context).", "num_citations": "2\n", "authors": ["1818"]}
{"title": "Correcting a naming architecture using lightweight constraint analysis\n", "abstract": " We used lightweight formal modeling and automatic analysis to explore and correct the design and implementation of the Intentional Naming System (INS). INS is a new scheme for resource discovery and service location in dynamic networks. We constructed a model of INS in Alloy, a lightweight relational notation, and analyzed it with the Alloy Analyzer (AA), a fully automatic simulation and checking tool. In doing so, we exposed several serious flaws in both the algorithms of INS and its underlying naming semantics. To correct the flaws in the INS algorithms, we developed a formal specification for naming in Alloy and refined it to specify the correctness criteria for INS algorithms. We accordingly modified the INS algorithms and verified their correctness using AA. Finally, we mapped these changes onto the original INS implementation, thereby correcting both the original design and implementation of the naming architecture of INS.", "num_citations": "2\n", "authors": ["1818"]}
{"title": "A study of learning likely data structure properties using machine learning models\n", "abstract": " Data structure properties are important for many testing and analysis tasks. For example, model checkers use these properties to find program faults. These properties are often written manually which can be error prone and lead to false alarms. This paper presents the results of controlled experiments performed using existing machine learning (ML) models on various data structures. These data structures are dynamic and reside on the program heap. We use ten data structure subjects and ten ML models to evaluate the learnability of data structure properties. The study reveals five key findings. One, most of the ML models perform well in learning data structure properties, but some of the ML models such as quadratic discriminant analysis and Gaussian naive Bayes are not suitable for learning data structure properties. Two, most of the ML models have high performance even when trained on just 1% of\u00a0\u2026", "num_citations": "1\n", "authors": ["1818"]}
{"title": "CRNs Exposed: A Method for the Systematic Exploration of Chemical Reaction Networks\n", "abstract": " Formal methods have enabled breakthroughs in many fields, such as in hardware verification, machine learning and biological systems. The key object of interest in systems biology, synthetic biology, and molecular programming is chemical reaction networks (CRNs) which formalizes coupled chemical reactions in a well-mixed solution. CRNs are pivotal for our understanding of biological regulatory and metabolic networks, as well as for programming engineered molecular behavior. Although it is clear that small CRNs are capable of complex dynamics and computational behavior, it remains difficult to explore the space of CRNs in search for desired functionality. We use Alloy, a tool for expressing structural constraints and behavior in software systems, to enumerate CRNs with declaratively specified properties. We show how this framework can enumerate CRNs with a variety of structural constraints including biologically motivated catalytic networks and metabolic networks, and seesaw networks motivated by DNA nanotechnology. We also use the framework to explore analog function computation in rate-independent CRNs. By computing the desired output value with stoichiometry rather than with reaction rates (in the sense that X\u2192 Y+ Y computes multiplication by 2), such CRNs are completely robust to the choice of reaction rates or rate law. We find the smallest CRNs computing the max, minmax, abs and ReLU (rectified linear unit) functions in a natural subclass of rate-independent CRNs where rate-independence follows from structural network properties.", "num_citations": "1\n", "authors": ["1818"]}
{"title": "CRNs exposed: Systematic exploration of chemical reaction networks\n", "abstract": " Formal methods have enabled breakthroughs in many fields, such as in hardware verification, machine learning and biological systems. The key object of interest in systems biology, synthetic biology, and molecular programming is chemical reaction networks (CRNs) which formalizes coupled chemical reactions in a well-mixed solution. CRNs are pivotal for our understanding of biological regulatory and metabolic networks, as well as for programming engineered molecular behavior. Although it is clear that small CRNs are capable of complex dynamics and computational behavior, it remains difficult to explore the space of CRNs in search for desired functionality. We use Alloy, a tool for expressing structural constraints and behavior in software systems, to enumerate CRNs with declaratively specified properties. We show how this framework can enumerate CRNs with a variety of structural constraints including biologically motivated catalytic networks and metabolic networks, and seesaw networks motivated by DNA nanotechnology. We also use the framework to explore analog function computation in rate-independent CRNs. By computing the desired output value with stoichiometry rather than with reaction rates (in the sense that  computes multiplication by ), such CRNs are completely robust to the choice of reaction rates or rate law. We find the smallest CRNs computing the max, minmax, abs and ReLU (rectified linear unit) functions in a natural subclass of rate-independent CRNs where rate-independence follows from structural network properties.", "num_citations": "1\n", "authors": ["1818"]}
{"title": "A synergistic approach to improving symbolic execution using test ranges\n", "abstract": " Symbolic execution is a systematic technique for checking programs, which has received a lot of research attention during the last decade. In principle, symbolic execution provides a powerful analysis for bug finding. In practice however, the technique remains computationally expensive and hard to scale. This paper introduces SynergiSE, a novel synergistic approach to improving symbolic execution by tackling a key bottleneck to its wider adoption: costly and incomplete constraint solving. To mitigate the cost, SynergiSE introduces a succinct encoding of constraint solving results, thereby enabling symbolic execution to be distributed among different workers while sharing and reusing constraint solving results among them without having to communicate databases of constraint solving results. To mitigate the incompleteness, SynergiSE introduces an integration of complementary approaches for testing, e.g\u00a0\u2026", "num_citations": "1\n", "authors": ["1818"]}
{"title": "Learning guided enumerative synthesis for superoptimization\n", "abstract": " The field of program synthesis has seen substantial recent progress in new ideas, e.g., program sketching and synthesis modulo pruning, and applications, e.g., in program repair and superoptimization, which is our focus in this paper. The goal of superoptimization is to generate a program which is functionally equivalent to the given program but is optimal with respect to some desired criteria. We develop a learning-based approach to guide the exploration of the space of candidate programs to parts of the space where an optimal solution likely exists. We introduce the techniques of bulk and sequence orderings which enable this directed search. We integrate these machine learning techniques with an enumerative superoptimizer and experimentally evaluate our framework using a suite of subjects. Our findings demonstrate that machine learning techniques can play a useful role in reducing the amount of\u00a0\u2026", "num_citations": "1\n", "authors": ["1818"]}
{"title": "Executable trigger-action comments\n", "abstract": " Natural language elements, e.g., todo comments, are frequently used to communicate among the developers and to describe tasks that need to be performed (actions) when specific conditions hold in the code repository (triggers). As projects evolve, development processes change, and development teams reorganize, these comments, because of their informal nature, frequently become irrelevant or forgotten. We present the first technique, dubbed TrigIt, to specify triggeraction todo comments as executable statements. Thus, actions are executed automatically when triggers evaluate to true. TrigIt specifications are written in the host language (e.g., Java) and are evaluated as part of the build process. The triggers are specified as query statements over abstract syntax trees and abstract representation of build configuration scripts, and the actions are specified as code transformation steps. We implemented TrigIt for the Java programming language and migrated 20 existing trigger-action comments from 8 popular open-source projects. We evaluate the cost of using TrigIt in terms of the number of tokens in the executable comments and the time overhead introduced in the build process.", "num_citations": "1\n", "authors": ["1818"]}
{"title": "Towards Exhaustive Testing of Websites using JPF\n", "abstract": " In this paper, we present a framework for exhaustive test input generation and execution of tests for websites using JPF, and Java libraries such as Selenium and JUnit. Specifically, we utilize the core functionality of jpf-nhandler, a JPF extension, to enable use of useful Java libraries such as GSON in the JPF environment. The paper further describes iterating through webpages successively and generating test inputs systematically for each page. It also presents the experimental results of running our framework on a small example website and a real-world website", "num_citations": "1\n", "authors": ["1818"]}
{"title": "Predicate detection for parallel computations\n", "abstract": " One of the fundamental problems in runtime verification of parallel program is to check if a predicate could become true in any global state of the system. The problem is challenging because of the nondeterministic process or thread scheduling of the system. Predicate detection alleviates this problem by analyzing the computation of the program and predicting whether the predicate could become true by exercising an alternative process schedule. The technique was first introduced by Cooper et al. and Garg et al. for distributed debugging. Later, jPredictor applies this technique for concurrent debugging. We improve the technique of predicate detection in three ways. The first part of this dissertation presents the first online-and-parallel predicate detector for general-purpose predicate detection, named ParaMount. ParaMount partitions the set of consistent global states and each subset can be enumerated in parallel using existing sequential enumeration algorithms. Our experimental results show that ParaMount speeds up the existing sequential algorithms by a factor of 6 with 8 threads. Moreover, Paramount can run along with the execution of users\u2019 program and hence it is applicable even to non-terminating programs. The second part develops a fast enumeration algorithm, named QuickLex, for consistent global states. In comparison with the original lexical algorithm (Lex), QuickLex uses an additional O(n2) space to reduce the time complexity from O(n2) to O(n\u00b7\u2206(P)), where n is the number of processes or threads in the computation and \u2206(P) is the maximal number of incoming edges of any event. The third part introduces Loset \u2014 a new model\u00a0\u2026", "num_citations": "1\n", "authors": ["1818"]}
{"title": "A case for using data-flow analysis to optimize incremental scope-bounded checking\n", "abstract": " In software verification, scope-bounded checking of programs has become an effective technique for finding subtle bugs. Given bounds (that are iteratively relaxed) on input size and length of execution paths, a program and its correctness specifications are translated into a formula, which is solved using off-the-shelf solvers \u2013 a solution to the formula is a counterexample to the correctness specification.", "num_citations": "1\n", "authors": ["1818"]}
{"title": "Integrating semantic interference detection into version management systems\n", "abstract": " Global software developments intensify parallel changes. Although parallel changes can improve performance, their interferences contribute to faults. Current Software Configuration Management (SCM) systems can detect the interference between changes at textual level. However, our empirical study shows that, compared with textual interference, semantic approach is more effective and efficient in detecting interference in highdegree parallel changes. We propose to integrate semantic interference checking into SCM system. Semantic interferences detected during check in can alert developers to potential faults.", "num_citations": "1\n", "authors": ["1818"]}
{"title": "Understanding Semantic Impact of Source Code Changes: an Empirical Study\n", "abstract": " Since source code is the ultimate definition of the behavior of a software product, changes to source code become the critical factor in understanding behavioral changes and predicting faults. In studies on source code changes, text or syntactic approaches have been widely used. Textual analysis focuses on changed text fragments while syntactic analysis focuses on changed syntactic entities. Although both of them have demonstrated their advantages in experimental results, they have only focused on changed code. Because of semantic dependencies within programs, we believe that code impacted by changes is also helpful. Given a source code change, we identify its impact by program slicing along the variable def-use chains. To evaluate the effectiveness of change impacts in fault detection and prediction, we compare impacted code with changed code according to size and fault density. Our experiment on the change history of a successful industrial project shows that: for large changes, their impacts have relative small size and high fault density; while for small changes, change themselves have relative small size and high fault density. Our study suggests that: 1) change impacts are complementary to change themselves in detecting or predicting faults; 2) within the impact of a large change, a high degree of interference between impacts of different changed lines contributes to the high fault density; 3) high fault density in impacts aggravates the danger of large changes.", "num_citations": "1\n", "authors": ["1818"]}
{"title": "Global Optimization of Compositional Systems\n", "abstract": " Embedded systems typically consist of a composition of a set of hardware and software IP modules. Each module is heavily optimized by itself. However, when these modules are composed together, significant additional opportunities for optimizations are introduced because only a subset of the entire functionality is actually used. We propose COSE-a technique to jointly optimize such designs. We use symbolic execution to compute invariants in each component of the design. We propagate these invariants as constraints to other modules using global flow analysis of the composition of the design. This captures optimizations that go beyond, and are qualitatively different than, those achievable by compiler optimization techniques such as common subexpression elimination, which are localized. We again employ static analysis techniques to perform optimizations subject to these constraints. We implemented COSE\u00a0\u2026", "num_citations": "1\n", "authors": ["1818"]}
{"title": "Mining Software Repositories for Rigorous Empirical Evaluation: A Position Paper\n", "abstract": " In the software tools studies, most of the evaluations were done within artificial contexts at labs. Although this approach can give instant feedback with low cost, the mock contexts are quite different from the real context and industrial developers are still confused with the experiment results. Our study provides a significant and low cost method to evaluate software tools with near real contexts.", "num_citations": "1\n", "authors": ["1818"]}
{"title": "Predicting Faults from Direct Semantic Interference: An Evaluative Experiment\n", "abstract": " Parallel developments are becoming increasingly prevalent in the building and evolution of large-scale software systems. Our previous studies of a large industrial project showed that there was a linear correlation between the degree of parallelism and the likelihood of defects in the changes. To further study the relationship between parallel changes and faults, we have designed and implemented an algorithm to detect \u201cdirect\u201d semantic interference between parallel changes. To evaluate the analyzer\u2019s effectiveness in fault prediction, we designed an experiment in the context of an industrial project. We first mine the change and version management repositories to find sample versions sets of different degrees of parallelism. We investigate the interference between the versions with our analyzer. We then mine the change and version repositories to find out what faults were discovered subsequent to the analyzed interfering versions. We use the match rate between semantic interference and faults to evaluate the effectiveness of the analyzer in predicting faults. Our contributions in this evaluative empirical study are twofold.. First, we evaluate the semantic interference analyzer and show that it is effective in predicting faults (based on \u201cdirect\u201d semantic interference detection) in changes made within a short time period. Second, the design of our experiment is itself a significant contribution and exemplifies how to mine software repositories rather than use artificial cases for rigorous experimental evaluations.", "num_citations": "1\n", "authors": ["1818"]}
{"title": "Verification of Cooperative Multi-Agent Negotiation with Alloy Analyzer\n", "abstract": " Multi-agent systems provide an increasingly popular solution in problem domains that require management of uncertainty and high degree of adaptability. Robustness is a key design criteria in building multi-agent systems. We present a novel approach for the design of robust multi-agent systems. Our approach constructs a model of the design of a multi-agent system in Alloy, a declarative language based on relations, and checks the properties of the model using the Alloy Analyzer, a fully automatic analysis tool for Alloy models. While several prior techniques exist for checking properties of multi-agent systems, the novelty of our work is that we can check properties of coordination and interaction, as well as properties of complex data structures that the agents may internally be manipulating or even sharing. The suggested work is the first application of Alloy to checking properties of multi-agent systems. Such unified analysis has not been possible before.", "num_citations": "1\n", "authors": ["1818"]}
{"title": "Applying a Sequential Circuit Solver to Alloy\n", "abstract": " Alloy uses a first-order relational logic for modeling designs. The Alloy Analyzer translates Alloy formulas for a given scope, ie, a bound on the universe of discourse, to Boolean formulas in conjunctive normal form (CNF), and subsequently checks them using propositional satisfiability solvers. We present SERA, a novel algorithm that compiles a relational logic formula for a given scope to a sequential circuit. There are two key advantages of sequential circuits: they form a more succinct representation than CNF formulas, sometimes by several orders of magnitude. Also sequential circuits are amenable to a range of powerful automatic analysis techniques that have no counterparts for CNF formulas. Our experiments show that SERA, in conjunction with a sequential circuit analyzer, can check formulas for scopes that are an order of magnitude higher than those feasible with the Alloy Analyzer.", "num_citations": "1\n", "authors": ["1818"]}
{"title": "The Alloy modelling language and analyzer\n", "abstract": " Alloy is a software modelling language that can describe complex structures and their evolution, and which is amenable to fully automatic analysis. The Alloy Analyzer is a tool for analyzing models written in Alloy. It can generate instances of invariants, simulate the execution of operations (even those defined implicitly), and can check user-specified properties of a model. Alloy and its analyzer have been used primarily to explore abstract software designs; some recent case studies are described as a separate project (see \u201cCase Studies in Modeling and Analysis of Software\u201d). Its use in analyzing code for conformance to a specification and as an automatic test case generator (see \u201cMulSaw: Automated Checking of Code Conformance\u201d) are being investigated in ongoing research projects.", "num_citations": "1\n", "authors": ["1818"]}