{"title": "The landscape of parallel computing research: A view from berkeley\n", "abstract": " The recent switch to parallel microprocessors is a milestone in the history of computing. Industry has laid out a roadmap for multicore designs that preserves the programming paradigm of the past via binary compatibility and cache coherence. Conventional wisdom is now to double the number of cores on a chip with each silicon generation.A multidisciplinary group of Berkeley researchers met nearly two years to discuss this change. Our view is that this evolutionary approach to parallel hardware and software may work from 2 or 8 processor systems, but is likely to face diminishing returns as 16 and 32 processor systems are realized, just as returns fell with greater instruction-level parallelism.", "num_citations": "2912\n", "authors": ["1693"]}
{"title": "Mining specifications\n", "abstract": " Program verification is a promising approach to improving program quality, because it can search all possible program executions for specific errors. However, the need to formally describe correct behavior or errors is a major barrier to the widespread adoption of program verification, since programmers historically have been reluctant to write formal specifications. Automating the process of formulating specifications would remove a barrier to program verification and enhance its practicality.This paper describes specification mining, a machine learning approach to discovering formal specifications of the protocols that code must obey when interacting with an application program interface or abstract data type. Starting from the assumption that a working program is well enough debugged to reveal strong hints of correct protocols, our tool infers a specification by observing program execution and concisely\u00a0\u2026", "num_citations": "909\n", "authors": ["1693"]}
{"title": "Jungloid mining: helping to navigate the API jungle\n", "abstract": " Reuse of existing code from class libraries and frameworks is often difficult because APIs are complex and the client code required to use the APIs can be hard to write. We observed that a common scenario is that the programmer knows what type of object he needs, but does not know how to write the code to get the object.In order to help programmers write API client code more easily, we developed techniques for synthesizing jungloid code fragments automatically given a simple query that describes that desired code in terms of input and output types. A jungloid is simply a unary expression; jungloids are simple, enabling synthesis, but are also versatile, covering many coding problems, and composable, combining to form more complex code fragments. We synthesize jungloids using both API method signatures and jungloids mined from a corpus of sample client programs.We implemented a tool, prospector\u00a0\u2026", "num_citations": "566\n", "authors": ["1693"]}
{"title": "A\" flight data recorder\" for enabling full-system multiprocessor deterministic replay\n", "abstract": " Debuggers have been proven indispensable in improving software reliability. Unfortunately, on most real-life software, debuggers fail to deliver their most essential feature---a faithful replay of the execution. The reason is non-determinism caused by multithreading and non-repeatable inputs. A common solution to faithful replay has been to record the non-deterministic execution. Existing recorders, however, either work only for datarace-free programs or have prohibitive overhead. As a step towards powerful debugging, we develop a practical low-overhead hardware recorder for cachecoherent multiprocessors, called Flight Data Recorder (FDR). Like an aircraft flight data recorder, FDR continuously records the execution, even on deployed systems, logging the execution for post-mortem analysis. FDR is practical because it piggybacks on the cache coherence hardware and logs nearly the minimal threadordering\u00a0\u2026", "num_citations": "505\n", "authors": ["1693"]}
{"title": "Refinement-based context-sensitive points-to analysis for Java\n", "abstract": " We present a scalable and precise context-sensitive points-to analysis with three key properties: (1) filtering out of unrealizable paths, (2) a context-sensitive heap abstraction, and (3) a context-sensitive call graph. Previous work [21] has shown that all three properties are important for precisely analyzing large programs, e.g., to show safety of downcasts. Existing analyses typically give up one or more of the properties for scalability. We have developed a refinement-based analysis that succeeds by simultaneously refining handling of method calls and heap accesses, allowing the analysis to precisely analyze important code while entirely skipping irrelevant code. The analysis is demanddriven and client-driven, facilitating refinement specific to each queried variable and increasing scalability. In our experimental evaluation, our analysis proved the safety of 61% more casts than one of the most precise existing\u00a0\u2026", "num_citations": "367\n", "authors": ["1693"]}
{"title": "Focusing processor policies via critical-path prediction\n", "abstract": " Although some instructions hurt performance more than others, current processors typically apply scheduling and speculation as if each instruction was equally costly. Instruction cost can be naturally expressed through the critical path: if we could predict it at run-time, egalitarian policies could be replaced with cost-sensitive strategies that will grow increasingly effective as processors become more parallel. This paper introduces a hardware predictor of instruction criticality and uses it to improve performance. The predictor is both effective and simple in its hardware implementation. The effectiveness at improving performance stems from using a dependence-graph model of the microarchitectural critical path that identifies execution bottlenecks by incorporating both data and machine-specific dependences. The simplicity stems from a token-passing algorithm that computes the critical path without actually building the\u00a0\u2026", "num_citations": "344\n", "authors": ["1693"]}
{"title": "Programming with models: writing statistical algorithms for general model structures with NIMBLE\n", "abstract": " We describe NIMBLE, a system for programming statistical algorithms for general model structures within R. NIMBLE is designed to meet three challenges: flexible model specification, a language for programming algorithms that can use different models, and a balance between high-level programmability and execution efficiency. For model specification, NIMBLE extends the BUGS language and creates model objects, which can manipulate variables, calculate log probability values, generate simulations, and query the relationships among variables. For algorithm programming, NIMBLE provides functions that operate with model objects using two stages of evaluation. The first stage allows specialization of a function to a particular model and/or nodes, such as creating a Metropolis-Hastings sampler for a particular block of nodes. The second stage allows repeated execution of computations using the results of the\u00a0\u2026", "num_citations": "324\n", "authors": ["1693"]}
{"title": "Programming by sketching for bit-streaming programs\n", "abstract": " This paper introduces the concept of programming with sketches, an approach for the rapid development of high-performance applications. This approach allows a programmer to write clean and portable reference code, and then obtain a high-quality implementation by simply sketching the outlines of the desired implementation. Subsequently, a compiler automatically fills in the missing details while also ensuring that a completed sketch is faithful to the input reference code. In this paper, we develop StreamBit as a sketching methodology for the important class of bit-streaming programs (eg, coding and cryptography). A sketch is a partial specification of the implementation, and as such, it affords several benefits to programmer in terms of productivity and code robustness. First, a sketch is easier to write compared to a complete implementation. Second, sketching allows the programmer to focus on exploiting\u00a0\u2026", "num_citations": "307\n", "authors": ["1693"]}
{"title": "Demand-driven points-to analysis for Java\n", "abstract": " We present a points-to analysis technique suitable for environments with small time and memory budgets, such as just-in-time (JIT) compilers and interactive development environments (IDEs). Our technique is demand-driven, performing only the work necessary to answer each query (a request for a variable's points-to information) issued by a client. In cases where even the demand-driven approach exceeds the time budget for a query, we employ early termination, i.e., stopping the analysis prematurely and returning an over-approximated result to the client. Our technique improves on previous demand-driven points-to analysis algorithms [17, 33] by achieving much higher precision under small time budgets and early termination.We formulate Andersen's analysis [5] for Java as a CFL-reachability problem [33]. This formulation shows that Andersen's analysis for Java is a balanced-parentheses problem, an insight\u00a0\u2026", "num_citations": "214\n", "authors": ["1693"]}
{"title": "A serializability violation detector for shared-memory server programs\n", "abstract": " We aim to improve reliability of multithreaded programs by proposing a dynamic detector that detects potentially erroneous program executions and their causes. We design and evaluate a Serializability Violation Detector (SVD) that has two unique goals: (I) triggering automatic recovery from erroneous executions using backward error recovery (BER), or simply alerting users that a software error may have occurred; and (II) helping debug programs by revealing causes of error symptoms.Two properties of SVD help in achieving these goals. First, to detect only erroneous executions, SVD checks serializability of atomic regions, which are code regions that need to be executed atomically. Second, to improve usability, SVD does not require a priori annotations of atomic regions; instead, SVD approximates them using a heuristic. Experimental results on three widely-used multithreaded server programs show that SVD\u00a0\u2026", "num_citations": "213\n", "authors": ["1693"]}
{"title": "Sketching concurrent data structures\n", "abstract": " We describe PSketch, a program synthesizer that helps programmers implement concurrent data structures. The system is based on the concept of sketching, a form of synthesis that allows programmers to express their insight about an implementation as a partial program: a sketch. The synthesizer automatically completes the sketch to produce an implementation that matches a given correctness criteria.", "num_citations": "206\n", "authors": ["1693"]}
{"title": "A lightweight symbolic virtual machine for solver-aided host languages\n", "abstract": " Solver-aided domain-specific languages (SDSLs) are an emerging class of computer-aided programming systems. They ease the construction of programs by using satisfiability solvers to automate tasks such as verification, debugging, synthesis, and non-deterministic execution. But reducing programming tasks to satisfiability problems involves translating programs to logical constraints, which is an engineering challenge even for domain-specific languages. We have previously shown that translation to constraints can be avoided if SDSLs are implemented by (traditional) embedding into a host language that is itself solver-aided. This paper describes how to implement a symbolic virtual machine (SVM) for such a host language. Our symbolic virtual machine is lightweight because it compiles to constraints only a small subset of the host's constructs, while allowing SDSL designers to use the entire language\u00a0\u2026", "num_citations": "200\n", "authors": ["1693"]}
{"title": "Growing solver-aided languages with Rosette\n", "abstract": " SAT and SMT solvers have automated a spectrum of programming tasks, including program synthesis, code checking, bug localization, program repair, and programming with oracles. In principle, we obtain all these benefits by translating the program (once) to a constraint system understood by the solver. In practice, however, compiling a language to logical formulas is a tricky process, complicated by having to map the solution back to the program level and extend the language with new solver-aided constructs, such as symbolic holes used in synthesis.", "num_citations": "173\n", "authors": ["1693"]}
{"title": "Refining data flow information using infeasible paths\n", "abstract": " Experimental evidence indicates that large programs exhibit significant amount of branch correlation amenable to compile-time detection. Branch correlation gives rise to infeasible paths, which in turn make data flow information overly conservative. For example, def-use pairs that always span infeasible paths cannot be tested by any program input, preventing 100% def-use testing coverage. We present an algorithm for identifying infeasible program paths and a data flow analysis technique that improves the precision of traditional def-use pair analysis by incorporating the information about infeasible paths into the analysis. Infeasible paths are computed using branch correlation analysis, which can be performed either intra- or inter-procedurally. The efficiency of our technique is achieved through demand-driven formulation of both the infeasible paths detection and the def-use pair analysis. Our experiments\u00a0\u2026", "num_citations": "165\n", "authors": ["1693"]}
{"title": "Complete removal of redundant expressions\n", "abstract": " Partial redundancy elimination (PRE), the most important component of global optimizers, generalizes the removal of common subexpressions and loop-invariant computations. Because existing PRE implementations are based on code motion, they fail to completely remove the redundancies. In fact, we observed that 73% of loop-invariant statements cannot be eliminated from loops by code motion alone. In dynamic terms, traditional PRE eliminates only half of redundancies that are strictly partial. To achieve a complete PRE, control flow restructuring must be applied. However, the resulting code duplication may cause code size explosion.This paper focuses on achieving a complete PRE while incurring an acceptable code growth. First, we present an algorithm for complete removal of partial redundancies, based on the integration of code motion and control flow restructuring. In contrast to existing complete\u00a0\u2026", "num_citations": "160\n", "authors": ["1693"]}
{"title": "Slack: Maximizing performance under technological constraints\n", "abstract": " Many emerging processor microarchitectures seek to manage technological constraints (e.g., wire, delay, power, and circuit complexity) by resorting to non-uniform designs that provide resources at multiple quality levels (e.g., fast/slow bypass paths, multi-speed functional units, and grid architectures). In such designs, the constraint problem becomes a control problem, and the challenge becomes designing a control policy that mitigates the performance penalty of the non-uniformity. Given the increasing importance of non-uniform control policies, we believe it is appropriate to examine them, in their own right. To this end, we develop slack for use in creating control policies that match program execution behavior to machine design. Intuitively, the slack of a dynamic instruction i is the number of cycles i can be delayed with no effect on execution time. This property makes slack a natural candidate for hiding non\u00a0\u2026", "num_citations": "155\n", "authors": ["1693"]}
{"title": "An efficient profile-analysis framework for data-layout optimizations\n", "abstract": " Data-layout optimizations rearrange fields within objects, objects within objects, and objects within the heap, with the goal of increasing spatial locality. While the importance of data-layout optimizations has been growing, their deployment has been limited, partly because they lack a unifying framework. We propose a parameterizable framework for data-layout optimization of general-purpose applications. Acknowledging that finding an optimal layout is not only NP-hard, but also poorly approximable, our framework finds a good layout by searching the space of possible layouts, with the help of profile feedback. The search process iteratively prototypes candidate data layouts, evaluating them by \"simulating\" the program on a representative trace of memory accesses. To make the search process practical, we develop space-reduction heuristics and optimize the expensive simulation via memoization. Equipped with\u00a0\u2026", "num_citations": "155\n", "authors": ["1693"]}
{"title": "Fast and parallel webpage layout\n", "abstract": " The web browser is a CPU-intensive program. Especially on mobile devices, webpages load too slowly, expending significant time in processing a document's appearance. Due to power constraints, most hardware-driven speedups will come in the form of parallel architectures. This is also true of mobile devices such as phones and e-books. In this paper, we introduce new algorithms for CSS selector matching, layout solving, and font rendering, which represent key components for a fast layout engine. Evaluation on popular sites shows speedups as high as 80x. We also formulate the layout problem with attribute grammars, enabling us to not only parallelize our algorithm but prove that it computes in O (log) time and without reflow.", "num_citations": "148\n", "authors": ["1693"]}
{"title": "Synthesizing highly expressive SQL queries from input-output examples\n", "abstract": " SQL is the de facto language for manipulating relational data. Though powerful, many users find it difficult to write SQL queries due to highly expressive constructs.", "num_citations": "146\n", "authors": ["1693"]}
{"title": "A regulated transitive reduction (RTR) for longer memory race recording\n", "abstract": " Now at VMware. Multithreaded deterministic replay has important applications in cyclic debugging, fault tolerance and intrusion analysis. Memory race recording is a key technology for multithreaded deterministic replay. In this paper, we considerably improve our previous always-on Flight Data Recorder (FDR) in four ways: \u2022Longer recording by reducing the log size growth rate to approximately one byte per thousand dynamic instructions. \u2022Lower hardware cost by reducing the cost to 24 KB per processor core. \u2022Simpler design by modifying only the cache coherence protocol, but not the cache. \u2022Broader applicability by supporting both Sequential Consistency (SC) and Total Store Order (TSO) memory consistency models (existing recorders support only SC).These improvements stem from several ideas: (1) a Regulated Transitive Reduction (RTR) recording algorithm that creates stricter and vectorizable\u00a0\u2026", "num_citations": "138\n", "authors": ["1693"]}
{"title": "Interprocedural conditional branch elimination\n", "abstract": " The existence of statically detectable correlation among conditional branches enables their elimination, an optimization that has a number of benefits. This paper presents techniques to determine whether an interprocedural execution path leading to a conditional branch exists along which the branch outcome is known at compile time, and then to eliminate the branch along this path through code restructuring. The technique consists of a demand driven interprocedural analysis that determines whether a specific branch outcome is correlated with prior statements or branch outcomes. The optimization is performed using a code restructuring algorithm that replicates code to separate out the paths with correlation. When the correlated path is affected by a procedure call, the restructuring is based on procedure entry splitting and exit splitting. The entry splitting transformation creates multiple entries to a procedure, and\u00a0\u2026", "num_citations": "129\n", "authors": ["1693"]}
{"title": "Debugging temporal specifications with concept analysis\n", "abstract": " Program verification tools (such as model checkers and static analyzers) can find many errors in programs. These tools need formal specifications of correct program behavior, but writing a correct specification is difficult, just as writing a correct program is difficult. Thus, just as we need methods for debugging programs, we need methods for debugging specifications.This paper describes a novel method for debugging formal, temporal specifications. Our method exploits the short program execution traces that program verification tools generate from specification violations and that specification miners extract from programs. Manually examining these traces is a straightforward way to debug a specification, but this method is tedious and error-prone because there may be hundreds or thousands of traces to inspect. Our method uses concept analysis to automatically group the traces into highly similar clusters. By\u00a0\u2026", "num_citations": "123\n", "authors": ["1693"]}
{"title": "Parallelizing the web browser\n", "abstract": " We argue that the transition from laptops to handheld computers will happen only if we rethink the design of web browsers. Web browsers are an indispensable part of the end-user software stack but they are too inefficient for handhelds. While the laptop reused the software stack of its desktop ancestor, solid-state device trends suggest that today\u2019s browser designs will not become sufficiently (1) responsive and (2) energy-efficient. We argue that browser improvements must go beyond JavaScript JIT compilation and discuss how parallelism may help achieve these two goals. Motivated by a future browser-based application, we describe the preliminary design of our parallel browser, its work-efficient parallel algorithms, and an actor-based scripting language.", "num_citations": "105\n", "authors": ["1693"]}
{"title": "Load-reuse analysis: Design and evaluation\n", "abstract": " Load-reuse analysis finds instructions that repeatedly access the same memory location. This location can be promoted to a register, eliminating redundant loads by reusing the results of prior memory accesses. This paper develops a load-reuse analysis and designs a method for evaluating its precision. In designing the analysis, we aspire for completeness---the goal of exposing all reuse that can be harvested by a subsequent program transformation. For register promotion, a suitable transformation is partial redundancy elimination (PRE). To approach the ideal goal of PRE-completeness, the load-reuse analysis is phrased as a data-flow problem on a program representation that is path-sensitive, as it detects reuse even when it originates in a different instruction along each control flow path. Furthermore, the analysis is comprehensive, as it treats scalar, array and pointer-based loads uniformly. In evaluating the\u00a0\u2026", "num_citations": "91\n", "authors": ["1693"]}
{"title": "Scaling up superoptimization\n", "abstract": " Developing a code optimizer is challenging, especially for new, idiosyncratic ISAs. Superoptimization can, in principle, discover machine-specific optimizations automatically by searching the space of all instruction sequences. If we can increase the size of code fragments a superoptimizer can optimize, we will be able to discover more optimizations. We develop LENS, a search algorithm that increases the size of code a superoptimizer can synthesize by rapidly pruning away invalid candidate programs. Pruning is achieved by selectively refining the abstraction under which candidates are considered equivalent, only in the promising part of the candidate space. LENS also uses a bidirectional search strategy to prune the candidate space from both forward and backward directions. These pruning strategies allow LENS to solve twice as many benchmarks as existing enumerative search algorithms, while LENS is\u00a0\u2026", "num_citations": "87\n", "authors": ["1693"]}
{"title": "Partial dead code elimination using slicing transformations\n", "abstract": " We present an approach for optimizing programs that uncovers additional opportunities for optimization of a statement by predicating the statement. In this paper predication algorithms for achieving partial dead code elimination (PDE) are presented. The process of predication embeds a statement in a control flow structure such that the statement is executed only if the execution follows a path along which the value computed by the statement is live. The control flow restructuring performed to achieve predication is expressed through slicing transformations. This approach achieves PDE that is not realizable by existing algorithms. We prove that our algorithm never increases the operation count along any path, and that for acyclic code all partially dead statements are eliminated. The slicing transformation that achieves predication introduces into the program additional conditional branches. These branches are\u00a0\u2026", "num_citations": "85\n", "authors": ["1693"]}
{"title": "DITTO: Automatic incrementalization of data structure invariant checks (in Java)\n", "abstract": " We present DITTO, an automatic incrementalizer for dynamic, side-effect-free data structure invariant checks. Incrementalization speeds up the execution of a check by reusing its previous executions, checking the invariant anew only the changed parts of the data structure. DITTO exploits properties specific to the domain of invariant checks to automate and simplify the process without restricting what mutations the program can perform. Our incrementalizer works for modern imperative languages such as Java and C#. It can incrementalize,for example, verification of red-black tree properties and the consistency of the hash code in a hash table bucket. Our source-to-source implementation for Java is automatic, portable, and efficient. DITTO provides speedups on data structures with as few as 100 elements; on larger data structures, its speedups are characteristic of non-automatic incrementalizers: roughly 5-fold at 5\u00a0\u2026", "num_citations": "84\n", "authors": ["1693"]}
{"title": "Using interaction costs for microarchitectural bottleneck analysis\n", "abstract": " Attacking bottlenecks in modern processors is difficult because many microarchitectural events overlap with each other. This parallelism makes it difficult to both: (a) assign a cost to an event (e.g., to one of two overlapping cache misses); and (b) assign blame for each cycle (e.g., for a cycle where many, overlapping resources are active). This paper introduces a new model for understanding event costs to facilitate processor design and optimization. First, we observe that everything in a machine (instructions, hardware structures, events) can interact in only one of two ways (in parallel or serially). We quantify these interactions by defining interaction cost, which can be zero (independent, no interaction), positive (parallel), or negative (serial). Second, we illustrate the value of using interaction costs in processor design and optimization. Finally, we propose performance-monitoring hardware for measuring interaction\u00a0\u2026", "num_citations": "81\n", "authors": ["1693"]}
{"title": "Chlorophyll: Synthesis-aided compiler for low-power spatial architectures\n", "abstract": " We developed Chlorophyll, a synthesis-aided programming model and compiler for the GreenArrays GA144, an extremely minimalist low-power spatial architecture that requires partitioning the program into fragments of no more than 256 instructions and 64 words of data. This processor is 100-times more energy efficient than its competitors, but currently can only be programmed using a low-level stack-based language. The Chlorophyll programming model allows programmers to provide human insight by specifying partial partitioning of data and computation. The Chlorophyll compiler relies on synthesis, sidestepping the need to develop classical optimizations, which may be challenging given the unusual architecture. To scale synthesis to real problems, we decompose the compilation into smaller synthesis subproblems---partitioning, layout, and code generation. We show that the synthesized programs are no\u00a0\u2026", "num_citations": "68\n", "authors": ["1693"]}
{"title": "Rapid profiling via stratified sampling\n", "abstract": " Sophisticated binary translators and dynamic optimizers demand a program profiler with low overhead, high accuracy, and the ability to collect a variety of profile types. A profiling scheme that achieves these goals is proposed. Conceptually, the hardware compresses a stream of profile data by counting identical events; the compressed profile dam is passed to software for analysis. Compressing the high-bandwidth event stream greatly reduces software overhead. Because optimizations can tolerate some profiling errors, we allow the stream compressor to be lossy, thereby enabling a low-cost sampling-based hardware design. Because the hardware compressor is insensitive to the event content, it supports various profile types and can process multiple types simultaneously. Basic components of our framework are periodic and random samplers, counters, and hash functions. These components are composed to\u00a0\u2026", "num_citations": "64\n", "authors": ["1693"]}
{"title": "Floem: A programming system for NIC-accelerated network applications\n", "abstract": " Developing server applications that offload computation and data to a NIC accelerator is laborious because one has to explore the design space of decisions about data placement and caching; partitioning of code and its parallelism; and communication strategies between program components across devices.", "num_citations": "62\n", "authors": ["1693"]}
{"title": "Rousillon: Scraping distributed hierarchical web data\n", "abstract": " Programming by Demonstration (PBD) promises to enable data scientists to collect web data. However, in formative interviews with social scientists, we learned that current PBD tools are insufficient for many real-world web scraping tasks. The missing piece is the capability to collect hierarchically-structured data from across many different webpages. We present Rousillon, a programming system for writing complex web automation scripts by demonstration. Users demonstrate how to collect the first row of a'universal table'view of a hierarchical dataset to teach Rousillon how to collect all rows. To offer this new demonstration model, we developed novel relation selection and generalization algorithms. In a within-subject user study on 15 computer scientists, users can write hierarchical web scrapers 8 times more quickly with Rousillon than with traditional programming.", "num_citations": "49\n", "authors": ["1693"]}
{"title": "Runtime specialization with optimistic heap analysis\n", "abstract": " We describe a highly practical program specializer for Java programs. The specializer is powerful, because it specializes optimistically, using (potentially transient) constants in the heap; it is precise, because it specializes using data structures that are only partially invariant; it is deployable, because it is hidden in a JIT compiler and does not require any user annotations or offline preprocessing; it is simple, because it uses existing JIT compiler ingredients; and it is fast, because it specializes programs in under 1s.These properties are the result of (1) a new algorithm for selecting specializable code fragments, based on a notion of influence; (2) a precise store profile for identifying constant heap locations; and (3) an efficient invalidation mechanism for monitoring optimistic assumptions about heap constants. Our implementation of the specializer in the Jikes RVM has low overhead, selects specialization points that\u00a0\u2026", "num_citations": "49\n", "authors": ["1693"]}
{"title": "Complete removal of redundant expressions\n", "abstract": " BackgroundInvented in 1979, Partial Redundancy Elimination (PRE) received renewed interest in the 90\u2019s, thanks to multiple industrial efforts crafting optimizing compilers for upcoming VLIW processors, and also thanks to new efficient PRE algorithms, most notably the Lazy Code Motion [24]. Although PRE was ready for industrial implementation, some of its potential was left unexploited, due to the limited transformational power of code motion underlying the standard PRE. We became interested in developing complete PRE when our experiments revealed that standard PRE failed to eliminate about 70% of loop invariant expressions\u2014a disappointing result given that PRE was being implemented as a generalization of loop-invariant code motion. At the time, it was already known that redundant expressions could be removed completely, in fact with a simple algorithm that restructured the control flow graph via path duplication [30]. In practice, however, the resulting code growth limited the use of restructuring to controlled ad hoc transformations like do-until conversion, which in some cases improved the standard PRE, but were inadequate in general. The question that initiated our research thus was how to integrate code motion with restructuring in a PRE algorithm that would resort to restructuring only when necessary, ie, when the (preferred) code motion fails. The result, we hoped, would subsume ad hoc transformations by performing custom restructuring needed to enable code motion. Besides restructuring, a promising technique for improving standard PRE was control flow speculation. First applied in instruction scheduling, this profile\u00a0\u2026", "num_citations": "47\n", "authors": ["1693"]}
{"title": "Algorithmic program synthesis: introduction\n", "abstract": " Program synthesis is a process of producing an executable program from a specification. Algorithmic synthesis produces the program automatically, without an intervention from an expert. While classical compilation falls under the definition of algorithmic program synthesis, with the source program being the specification, the synthesis literature is typically concerned with producing programs that cannot be (easily) obtained with the deterministic transformations of a compiler. To this end, synthesis algorithms often perform a search, either in a space of candidate programs or in a space of transformations that might be composed to transform the specification into a desired program. In this introduction to the special journal issue, we survey the history of algorithmic program synthesis and introduce the contributed articles. We divide the field into reactive synthesis, which is concerned with automata-theoretic\u00a0\u2026", "num_citations": "43\n", "authors": ["1693"]}
{"title": "Interaction cost and shotgun profiling\n", "abstract": " We observe that the challenges software optimizers and microarchitects face every day boil down to a single problem: bottleneck analysis. A bottleneck is any event or resource that contributes to execution time, such as a critical cache miss or window stall. Tasks such as tuning processors for energy efficiency and finding the right loads to prefetch all require measuring the performance costs of bottlenecks.In the past, simple event counts were enough to find the important bottlenecks. Today, the parallelism of modern processors makes such analysis much more difficult, rendering traditional performance counters less useful. If two microarchitectural events (such as a fetch stall and a cache miss) occur in the same cycle, which event should we blame for the cycle? What cost should we assign to each event? In this paper, we introduce a new model for understanding event costs to facilitate processor design and\u00a0\u2026", "num_citations": "42\n", "authors": ["1693"]}
{"title": "Register pressure sensitive redundancy elimination\n", "abstract": " Redundancy elimination optimizations avoid repeated computation of the same value by computing the value once, saving it in a temporary, and reusing the value from the temporary when it is needed again. Examples of redundancy elimination optimizations include common subexpression elimination, loop invariant code motion and partial redundancy elimination. We demonstrate that the introduction of temporaries to save computed values can result in a significant increase in register pressure. An increase in register pressure may in turn trigger generation of spill code which can more than offset the gains derived from redundancy elimination. While current techniques minimize increases in register pressure, to avoid spill code generation it is instead necessary to ensure that register pressure does not exceed the number of available registers.               In this paper we develop a redundancy\u00a0\u2026", "num_citations": "39\n", "authors": ["1693"]}
{"title": "Parallel schedule synthesis for attribute grammars\n", "abstract": " We examine how to synthesize a parallel schedule of structured traversals over trees. In our system, programs are declaratively specified as attribute grammars. Our synthesizer automatically, correctly, and quickly schedules the attribute grammar as a composition of parallel tree traversals. Our downstream compiler optimizes for GPUs and multicore CPUs.", "num_citations": "37\n", "authors": ["1693"]}
{"title": "Sampling invariants from frequency distributions\n", "abstract": " We present a new SMT-based, probabilistic, syntax-guided method to discover numerical inductive invariants. The core idea is to initialize frequency distributions from the program's source code, then repeatedly sample lemmas from those distributions, and terminate when the conjunction of learned lemmas becomes a safe invariant. The sampling process gets further optimized by priority distributions fine-tuned after each positive and negative sample. The stochastic nature of this approach admits simple, asynchronous parallelization. We implemented and evaluated this approach in a tool called FreqHorn which shows competitive performance on well-known linear and some non-linear programs.", "num_citations": "35\n", "authors": ["1693"]}
{"title": "Gradual synthesis for static parallelization of single-pass array-processing programs\n", "abstract": " Parallelizing of software improves its effectiveness and productivity. To guarantee correctness, the parallel and serial versions of the same code must be formally verified to be equivalent. We present a novel approach, called GRASSP, that automatically synthesizes parallel single-pass array-processing programs by treating the given serial versions as specifications. Given arbitrary segmentation of the input array, GRASSP synthesizes a code to determine a new segmentation of the array that allows computing partial results for each segment and merging them. In contrast to other parallelizers, GRASSP gradually considers several parallelization scenarios and certifies the results using constrained Horn solving. For several classes of programs, we show that such parallelization can be performed efficiently. The C++ translations of the GRASSP solutions sped performance by up to 5X relative to serial code on an 8\u00a0\u2026", "num_citations": "28\n", "authors": ["1693"]}
{"title": "The landscape of parallel computing research: A view from Berkeley, 2006\n", "abstract": " The path to petascale computing will be paved with new system architectures featuring hundreds of thousands of manycore processors. Such systems will require scientists to completely rethink programming models. A frequently cited white paper called \u201cThe Landscape of Parallel Computing Research: A View from Berkeley,\u201d addressed the challenge of finding ways to make it easy to write programs that run efficiently on manycore systems.", "num_citations": "27\n", "authors": ["1693"]}
{"title": "Interactive query synthesis from input-output examples\n", "abstract": " This demo showcases Scythe, a novel query-by-example system that can synthesize expressive SQL queries from input-output examples. Scythe is designed to help end-users program SQL and explore data simply using input-output examples. From a web-browser, users can obtain SQL queries with Scythe in an automated, interactive fashion: from a provided example, Scythe synthesizes SQL queries and resolves ambiguities via conversations with the users.", "num_citations": "26\n", "authors": ["1693"]}
{"title": "Swift: Compiled inference for probabilistic programming languages\n", "abstract": " A probabilistic program defines a probability measure over its semantic structures. One common goal of probabilistic programming languages (PPLs) is to compute posterior probabilities for arbitrary models and queries, given observed evidence, using a generic inference engine. Most PPL inference engines---even the compiled ones---incur significant runtime interpretation overhead, especially for contingent and open-universe models. This paper describes Swift, a compiler for the BLOG PPL. Swift-generated code incorporates optimizations that eliminate interpretation overhead, maintain dynamic dependencies efficiently, and handle memory management for possible worlds of varying sizes. Experiments comparing Swift with other PPL engines on a variety of inference problems demonstrate speedups ranging from 12x to 326x.", "num_citations": "26\n", "authors": ["1693"]}
{"title": "Programming by manipulation for layout.\n", "abstract": " We present Programming by Manipulation, a new programming methodology for specifying the layout of data visualizations, targeted at non-programmers. We address the two central sources of bugs that arise when programming with constraints: ambiguities and conflicts (inconsistencies). We rule out conflicts by design and exploit ambiguity to explore possible layout designs. Our users design layouts by highlighting undesirable aspects of a current design, effectively breaking spurious constraints and introducing ambiguity by giving some elements freedom to move or resize. Subsequently, the tool indicates how the ambiguity can be removed, by computing how the free elements can be fixed with available constraints. To support this workflow, our tool computes the ambiguity and summarizes it visually. We evaluate our work with two user-studies demonstrating that both non-programmers and programmers can effectively use our prototype. Our results suggest that our tool is 5-times more productive than direct programming with constraints.", "num_citations": "26\n", "authors": ["1693"]}
{"title": "Characterizing coarse-grained reuse of computation\n", "abstract": " Value locality is the phenomenon that a small number of values occur repeatedly in the same register or memory location. Non-speculativereuse of computation [21] is one of the methods that has been proposed to exploit value locality. However, reuse becomes profitable only when multiple instructions are reused simultaneously. Identifying suitable chains of reusable instructions requires a global view of the program and is therefore difficult to be accomplished with hardware alone.This paper investigates the properties of reuse in the context of a dynamic optimization setting. We focus on characterizing the available computation reuse in programs at coarse granularities, and in determining the relative applicability of specialization and memoization, two commonly used techniques for exploiting coarse-grained reuse. Our study suggests that a reuse technique based on an online optimization system is feasible due to the following reasons. First, programs contain many large regions: on average, regions of 16 or more dynamic instructions represent 54% of all reuse, or 26% of all dynamic instructions. Second, large regions are stable over time and hence can likely be identified cost-effectively with a dynamic optimizer. Third, we observed that many reuse opportunities cannot be exploited with memoization, the currently used reuse technique, but must be supplemented with specialization, a technique that requires run-time code generation.", "num_citations": "26\n", "authors": ["1693"]}
{"title": "Adaptive loop transformations for scientific programs\n", "abstract": " To facilitate the efficient execution of scientific programs, parallelizing compilers apply a wide range of loop transformations. In some situations it may not be possible to determine the applicability and usefulness of the transformations at compile time. We develop adaptive versions of various loop transformations which lend themselves to efficient application at run time. instead of explicitly applying a transformation at compile time, we generate adaptive code which is able to behave like the transformed code, if so desired, at run time. Therefore an adaptive program can be viewed as expressing multiple ways of executing a program. The selection of the particular execution is based upon run time information. Adaptive programs offer a more practical and powerful alternative to multiversion programs.", "num_citations": "26\n", "authors": ["1693"]}
{"title": "Path-sensitive, value-flow optimizations of programs\n", "abstract": " Current compiler optimizers are conservative and inflexible. As a result, even \u201chighly optimized\u201d programs execute half of their instructions redundantly, only to recompute previously computed values. Ideally, these values should be remembered and later reused, removing recomputations.", "num_citations": "25\n", "authors": ["1693"]}
{"title": "Array data flow analysis for load-store optimizations in fine-grain architectures\n", "abstract": " The performence of scientific programs on modern processors can be significantly degraded by memory references that frequently arise due to load and store operations associated with array references. We have developed techniques for optimally allocating registers to array elements whose values are repeatedly referenced over one or more loop iterations. The resulting placement of loads and stores is optimal in that number of loads and stores encoutered along each path through the loop is minimal for the given program branching structure. To place load, store, and register-to-register shift operations without introducing fully/partially redundant and dead memory operations, a detailed value flow analysis of array references is required. We present an analysis framework to efficiently solve various data flow problems required by array load-store optimizations. The framework determines the collective\u00a0\u2026", "num_citations": "25\n", "authors": ["1693"]}
{"title": "Accelerating syntax-guided invariant synthesis\n", "abstract": " We present a fast algorithm for syntax-guided synthesis of inductive invariants which combines enumerative learning with inductive-subset extraction, leverages counterexamples-to-induction and interpolation-based bounded proofs. It is a variant of a recently proposed probabilistic method, called FreqHorn, which is however less dependent on heuristics than its predecessor. We present an evaluation of the new algorithm on a large set of benchmarks and show that it exhibits a more predictable behavior than its predecessor, and it is competitive to the state-of-the-art invariant synthesizers based on Property Directed Reachability.", "num_citations": "23\n", "authors": ["1693"]}
{"title": "Swizzle inventor: data movement synthesis for GPU kernels\n", "abstract": " Utilizing memory and register bandwidth in modern architectures may require swizzles---non-trivial mappings of data and computations onto hardware resources---such as shuffles. We develop Swizzle Inventor to help programmers implement swizzle programs, by writing program sketches that omit swizzles and delegating their creation to an automatic synthesizer. Our synthesis algorithm scales to real-world programs, allowing us to invent new GPU kernels for stencil computations, matrix transposition, and a finite field multiplication algorithm (used in cryptographic applications). The synthesized 2D convolution and finite field multiplication kernels are on average 1.5--3.2 x and 1.1--1.7 x faster, respectively, than expert-optimized CUDA kernels.", "num_citations": "19\n", "authors": ["1693"]}
{"title": "Concurrency concerns in rich internet applications\n", "abstract": " Linguistic constructs for concurrent programming, such as atomic regions, typically intend to orchestrate low-level code and strive to support generalpurpose programming. We show that concurrency concerns (1) exist also at a higher-level of software design and (2) are somewhat domain-specific in nature. This observation motivates domain-specific high-level abstractions. Specifically, we look at Rich Internet Applications, exemplified by Google Maps and the Web version of Microsoft Outlook, which are written using scripting frameworks such as AJAX and Flash. These frameworks provide event-driven programming models that are single-threaded and non-preemptive. While these models avoid the need for low-level synchronization, they are inadequate for preventing concurrency problems at the higher level of the program. We identify three fundamental sources of concurrency in these applications: asynchronous server data exchange, script execution during loading, and animation. We present preliminary ideas for suitable abstractions that would simplify correctness validation and expose parallelism.", "num_citations": "16\n", "authors": ["1693"]}
{"title": "Synthesizing signaling pathways from temporal phosphoproteomic data\n", "abstract": " We present a method for automatically discovering signaling pathways from time-resolved phosphoproteomic data. The Temporal Pathway Synthesizer (TPS) algorithm uses constraint-solving techniques first developed in the context of formal verification to explore paths in an interaction network. It systematically eliminates all candidate structures for a signaling pathway where a protein is activated or inactivated before its upstream regulators. The algorithm can model more than one hundred thousand dynamic phosphosites and can discover pathway members that are not differentially phosphorylated. By analyzing temporal data, TPS defines signaling cascades without needing to experimentally perturb individual proteins. It recovers known pathways and proposes pathway connections when applied to the human epidermal growth factor and yeast osmotic stress responses. Independent kinase mutant studies\u00a0\u2026", "num_citations": "15\n", "authors": ["1693"]}
{"title": "Greenthumb: Superoptimizer construction framework\n", "abstract": " Developing an optimizing compiler backend remains a laborious process, especially for nontraditional ISAs that have been appearing recently. Superoptimization sidesteps the need for many code transformations by searching for the most optimal instruction sequence semantically equivalent to the original code fragment. Even though superoptimization discovers the best machine-specific code optimizations, it has yet to become widely-used. We propose GreenThumb, an extensible framework that reduces the cost of constructing superoptimizers and provides a fast search algorithm that can be reused for any ISA, exploiting the unique strengths of enumerative, stochastic, and symbolic (SAT-solver-based) search algorithms. To extend GreenThumb to a new ISA, it is only necessary to implement an emulator for the ISA and provide some ISA-specific search utility functions.", "num_citations": "14\n", "authors": ["1693"]}
{"title": "Array data flow analysis for load-store optimizations in superscalar architectures\n", "abstract": " The performance of scientific programs on superscalar processors can be significantly degraded by memory references that frequently arise due to load and store operations associated with array references. Therefore, register allocation techniques have been developed for allocating registers to array elements whose values are repeatedly referenced over one or more loop iterations. To place load, store, and register-to-register shift operations without introducing fully/partially redundant and dead memory operations, a detailed value flow analysis of array references is required. We present an analysis framework to efficiently solve various data flow problems required by array load-store optimizations. The framework determines the collective behavior of recurrent references spread over multiple loop iterations. We also demonstrate how our algorithms can be adapted for various fine-grain architectures.", "num_citations": "14\n", "authors": ["1693"]}
{"title": "Skip blocks: reusing execution history to accelerate web scripts\n", "abstract": " With more and more web scripting languages on offer, programmers have access to increasing language support for web scraping tasks. However, in our experiences collaborating with data scientists, we learned that two issues still plague long-running scraping scripts: i) When a network or website goes down mid-scrape, recovery sometimes requires restarting from the beginning, which users find frustratingly slow. ii) Websites do not offer atomic snapshots of their databases; they update their content so frequently that output data is cluttered with slight variations of the same information \u2014 e.g., a tweet from profile 1 that is retweeted on profile 2 and scraped from both profiles, once with 52 responses then later with 53 responses.   We introduce the skip block, a language construct that addresses both of these disparate problems. Programmers write lightweight annotations to indicate when the current object can be\u00a0\u2026", "num_citations": "12\n", "authors": ["1693"]}
{"title": "Superconductor: A language for big data visualization\n", "abstract": " Increases in data availability is the force behind many recent innovations in society. However, visualization technology for exploring data is not keeping up. Designers must choose between scale and interactivity. They want big displays because of the ability to show an entire data set. However, viewers get lost in a sea of noise, so they also want interactivity. Performance constraints limit interactions to operating on a small data slice. We present SUPERCONDUCTOR: a high-level visualization language for interacting with large data sets. It has three design goals that we distilled from popular visualization languages:\u2022 Scale. Visualizations should support thousands or even millions of data points. For example, Matlab and Circos1 are used for static visualizations of large data sets.\u2022 Interactivity. Interactions should be within 100ms and animation should achieve 30fps. For example, JavaScript and its libraries such as D3 [2] are used for animating data and orchestrating interactions with users.", "num_citations": "12\n", "authors": ["1693"]}
{"title": "Synthesis of layout engines from relational constraints\n", "abstract": " We present an algorithm for synthesizing efficient document layout engines from compact relational specifications. These specifications are compact in that a single specification can produce multiple engines, each for a distinct layout situation, i.e., a different combination of known vs. unknown attributes. Technically, our specifications are relational attribute grammars, while our engines are functional attribute grammars. By synthesizing functions from relational constraints, we obviate the need for constraint solving at runtime, because functional attribute grammars can be easily evaluated according to a fixed schedule, sidestepping the backtracking search performed by constraint solvers. Our experiments show that we can generate layout engines for non-trivial data visualizations, and that our synthesized engines are between 39- and 200-times faster than general-purpose constraint solvers. Relational\u00a0\u2026", "num_citations": "10\n", "authors": ["1693"]}
{"title": "Iterative search for reconfigurable accelerator blocks with a compiler in the loop\n", "abstract": " Domain-specific reconfigurable accelerators achieve high performance and energy efficiency by using specialized processing elements (PEs) instead of general-purpose alternatives. However, the process of designing, selecting, and refining the reconfigurable PEs that compose the accelerator fabric has remained a manual and difficult task. This paper presents reconfigurable accelerator design using iterative search for hardware (RADISH) which is a full-stack framework for automatically identifying and generating PEs from an application corpus. RADISH uses a genetic algorithm to iteratively search for and refine the proposed PEs with a compiler-in-the-loop to guide the search. We show that RADISH-generated PEs can generalize to both larger instances of the same application as well as other previously unseen applications within the same domain. We evaluate a coarse-grain reconfigurable array (CGRA\u00a0\u2026", "num_citations": "9\n", "authors": ["1693"]}
{"title": "Speeding up symbolic reasoning for relational queries\n", "abstract": " The ability to reason about relational queries plays an important role across many types of database applications, such as test data generation, query equivalence checking, and computer-assisted query authoring. Unfortunately, symbolic reasoning about relational queries can be challenging because relational tables are multisets (bags) of tuples, and the underlying languages, such as SQL, can introduce complex computation among tuples.  We propose a space refinement algorithm that soundly reduces the space of tables such applications need to consider. The refinement procedure, independent of the specific dataset application, uses the abstract semantics of the query language to exploit the provenance of tuples in the query output to prune the search space. We implemented the refinement algorithm and evaluated it on SQL using three reasoning tasks: bounded query equivalence checking, test generation\u00a0\u2026", "num_citations": "9\n", "authors": ["1693"]}
{"title": "Synthesizing Programs with Constraint Solvers.\n", "abstract": " Classical synthesis derives programs from a specification. We show an alternative approach where programs are obtained through search in a space of candidate programs. Searching for a program that meets a specification frees us from having to develop a sufficiently complete set of derivation rules, a task that is more challenging than merely describing the syntactic shape of the desired program. To make the search for a program efficient, we exploit symbolic constraint solving, lifted to synthesis from the setting of program verification.We start by describing the interface to the synthesizer, which the programmer uses to specify the space of candidate programs \ud835\udc43 as well as the desired correctness condition \ud835\udf19. The space \ud835\udc43 is defined by a program template whose missing expressions are described with a grammar. The correctness condition is a multi-modal specification, given as a combination of assertions, input/output pairs, and traces.", "num_citations": "9\n", "authors": ["1693"]}
{"title": "Synthesizing hardware from sketches\n", "abstract": " This paper proposes to adapt sketching, a software synthesis technique, to hardware development. In sketching, the designer develops an incomplete hardware description, providing the \"insight\" into the design. The synthesizer completes the design to match an executable specification. This style of synthesis liberates the designer from tedious and error-prone details-such as timing delays, wiring in combinational circuits and initialization of lookup tables-while allowing him to control low-level aspects of the design. The main benefit will be a reduction of the time-to-market without impairing system performance.", "num_citations": "9\n", "authors": ["1693"]}
{"title": "A hardware memory race recorder for deterministic replay\n", "abstract": " As hardware vendors transition to multicore chips, software vendors face increased software reliability challenges. To effectively debug software in this new world, developers must be able to replay executions that exhibit a bug so that they can zero in on concurrency bugs - especially intermittent ones. Such deterministic replay also aids fault detection and recovery, intrusion detection, and the like. Deterministic replay requires both a recorder and a replayer. The recorder logs information during multithreaded program execution that is sufficient to enable deterministic replay. In the postmortem analysis, the replayer uses the logged information, together with the program binary, to faithfully replay the original execution. The replay will always exercise the same bugs and produce the same outputs. Clearly, an investment in modest chip resources has great potential to ease the challenges of debugging the multithreaded\u00a0\u2026", "num_citations": "9\n", "authors": ["1693"]}
{"title": "Verifying and improving Halide\u2019s term rewriting system with program synthesis\n", "abstract": " Halide is a domain-specific language for high-performance image processing and tensor computations, widely adopted in industry. Internally, the Halide compiler relies on a term rewriting system to prove properties of code required for efficient and correct compilation. This rewrite system is a collection of handwritten transformation rules that incrementally rewrite expressions into simpler forms; the system requires high performance in both time and memory usage to keep compile times low, while operating over the undecidable theory of integers. In this work, we apply formal techniques to prove the correctness of existing rewrite rules and provide a guarantee of termination. Then, we build an automatic program synthesis system in order to craft new, provably correct rules from failure cases where the compiler was unable to prove properties. We identify and fix 4 incorrect rules as well as 8 rules which could give rise\u00a0\u2026", "num_citations": "8\n", "authors": ["1693"]}
{"title": "Jungloid mining: Helping to navigate the API jungle\n", "abstract": " Reuse of existing code from class libraries and frameworks is often difficult because APIs are complex and the client code required to use the APIs can be hard to write. We observed that a common scenario is that the programmer knows what type of object he needs, but does not know how to write the code to get the object. In order to help programmers write API client code more easily, we developed techniques for synthesizing jungloid code fragments automatically given a simple query that describes that desired code in terms of input and output types. A jungloid is simply a unary expression; jungloids are simple, enabling synthesis, but are also versatile, covering many coding problems, and composable, combining to form more complex code fragments. We synthesize jungloids using both API method signatures and jungloids mined from a corpus of sample client programs. We implemented a tool, PROSPECTOR, based on these techniques. PROSPECTOR is integrated with the Eclipse IDE code assistance feature, and it infers queries from context so there is no need for the programmer to write queries. We tested PROSPECTOR on a set of real programming problems involving APIs; PROSPECTOR found the desired solution for 18 of 20 problems. We also evaluated PROSPECTOR in a user study, finding that programmers solved programming problems more quickly and with more reuse when using PROSPECTOR than without PROSPECTOR.", "num_citations": "8\n", "authors": ["1693"]}
{"title": "Bonsai: synthesis-based reasoning for type systems\n", "abstract": " When designing a type system, we may want to mechanically check the design to guide its further development. We describe algorithms that perform symbolic reasoning about executable models of type systems. The algorithms support three queries. First, they check type soundness and synthesize a counterexample program if such a soundness bug is found. Second, they compare two versions of a type system, synthesizing a program accepted by one but rejected by the other. Third, they minimize the size of synthesized counterexample programs.  These algorithms symbolically evaluate typecheckers and interpreters, producing formulas that characterize the set of programs that fail or succeed in the typechecker and the interpreter. However, symbolically evaluating interpreters poses efficiency challenges, which are caused by having to merge execution paths of the various possible input programs. Our main\u00a0\u2026", "num_citations": "7\n", "authors": ["1693"]}
{"title": "Quicksilver: Automatic synthesis of relational queries\n", "abstract": " Relational data has become so widespread that even end-users such as secretaries and teachers frequently interact with them. However, finding the right query to retrieve the necessary data from complex databases can be very difficult for end-users. Many of these users can be seen voicing their confusion on several Excel help forums, receiving little help and waiting days for responses from experts. In this paper, we present Quicksilver, a programming-by-demonstration solution that derives queries from user inputs. It is designed to be easy and intuitive for users who are not familiar with database theory. We present Quicksilver\u2019s interface designs and synthesis algorithms. We conclude with a user study designed to evaluate Quicksilver\u2019s performance.", "num_citations": "7\n", "authors": ["1693"]}
{"title": "Toward synthesizing executable models in biology\n", "abstract": " Over the last decade, executable models of biological behaviors have repeatedly provided new scientific discoveries, uncovered novel insights, and directed new experimental avenues. These models are computer programs whose execution mechanistically simulates aspects of the cell\u2019s behaviors. If the observed behavior of the program agrees with the observed biological behavior, then the program explains the phenomena. This approach has proven beneficial for gaining new biological insights and directing new experimental avenues. One advantage of this approach is that techniques for analysis of computer programs can be applied to the analysis of executable models. For example, one can confirm that a model agrees with experiments for all possible executions of the model (corresponding to all environmental conditions), even if there are a huge number of executions. Various formal methods have been adapted for this context, for example, model checking or symbolic analysis of state spaces. To avoid manual construction of executable models, one can apply synthesis, a method to produce programs automatically from high-level specifications. In the context of biological modelling, synthesis would correspond to extracting executable models from experimental data. We survey recent results about the usage of the techniques underlying synthesis of computer programs for the inference of biological models from experimental data. We describe synthesis of biological models from curated mutation experiment data, inferring network connectivity models from phosphoproteomic data, and synthesis of Boolean networks from gene\u00a0\u2026", "num_citations": "6\n", "authors": ["1693"]}
{"title": "Interaction cost: For when event counts just don't add up\n", "abstract": " Most performance analysis tasks boil down to finding bottlenecks. In the context of this article, a bottleneck is any event (for example, branch mispredict, window stall, or arithmetic-logic unit (ALU) operation) that limits performance. Bottleneck analysis is critical to an architect's work, whether the goal is tuning processors for energy efficiency, improving the effectiveness of optimizations, or designing a more balanced processor. Interaction cost helps to improve processor performance and decrease power consumption by identifying when designers can choose among a set of optimizations and when it's necessary to perform them all", "num_citations": "5\n", "authors": ["1693"]}
{"title": "Optimal Placement of Load-Store Operations for Array Accesses in Loops,\"\n", "abstract": " The performance of scientic programs on superscalar processors can be signicantly degraded by memory references that can frequently arise due to load and store operations associated with array references. Therefore, register allocation techniques have been developed for allocating registers to array elements whose values are repeatedly referenced over one or more loop iterations. However, straightforward techniques for the placement of load, store and register-to-register shift operations can result in the introduction of fully/partially redundant and dead operations.In this paper we present techniques for determining the placement of loads, stores, and register-to-register shift operations at points such that placement of fully/partially redundant and dead operations is avoided. We avoid redundant and dead operations that are exposed by examining a single loop iteration as well as multiple loop iterations. The\u00a0\u2026", "num_citations": "5\n", "authors": ["1693"]}
{"title": "Domain-specific symbolic compilation\n", "abstract": " A symbolic compiler translates a program to symbolic constraints, automatically reducing model checking and synthesis to constraint solving. We show that new applications of constraint solving require domain-specific encodings that yield the required orders of magnitude improvements in solver efficiency. Unfortunately, these encodings cannot be obtained with today's symbolic compilation. We introduce symbolic languages that encapsulate domain-specific encodings under abstractions that behave as their non-symbolic counterparts: client code using the abstractions can be tested and debugged on concrete inputs. When client code is symbolically compiled, the resulting constraints use domain-specific encodings. We demonstrate the idea on the first fully symbolic checker of type systems; a program partitioner; and a parallelizer of tree computations. In each of these case studies, symbolic languages improved on classical symbolic compilers by orders of magnitude.", "num_citations": "4\n", "authors": ["1693"]}
{"title": "Program synthesis: opportunities for the next decade\n", "abstract": " Program synthesis is the contemporary answer to automatic programming. It innovates in two ways: First, it replaces batch automation with interactivity, assisting the programmer in refining the understanding of the programming problem. Second, it produces programs using search in a candidate space rather than by derivation from a specification. Searching for an acceptable program means that we can accommodate incomplete specifications, such as examples. Additionally, search makes synthesis applicable to domains that lack correct-by-construction derivation rules, such as hardware design, education, end-user programming, and systems biology. The future of synthesis rests on four challenges, each presenting an opportunity to develop novel abstractions for \"programming with search.\" Larger scope: today, we synthesize small, flat programs; synthesis of large software will need constructs for modularity and\u00a0\u2026", "num_citations": "4\n", "authors": ["1693"]}
{"title": "Fireiron: A Data-Movement-Aware Scheduling Language for GPUs\n", "abstract": " High GPU performance can only be achieved if a kernel efficiently uses the multi-layered compute and memory hierarchies. For example, accelerators such as NVIDIA? s Tensor Cores require specific mappings of threads to data that must be considered in data movements to and from registers. Current compilers struggle to match the performance of vendor libraries like cu BLAS, which are developed by experts in assembly. This manual low-level coding is time-consuming and complicates to unlock the full GPU potential, preventing experimentation to achieve even higher performance. In this paper we introduce Fireiron, a scheduling language aimed at performance experts. Fireiron provides high-level abstractions for expressing GPU optimizations that are unavailable to compilers today and which so far must be written in assembly. Our innovation is that both computations and data movements are first class\u00a0\u2026", "num_citations": "3\n", "authors": ["1693"]}