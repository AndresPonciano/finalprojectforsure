{"title": "Modeling and analysis of communicating systems\n", "abstract": " Rigorous theory and real-world applications for modeling and analysis of the behavior of complex communicating computer systems Complex communicating computer systems\u2014computers connected by data networks and in constant communication with their environments\u2014do not always behave as expected. This book introduces behavioral modeling, a rigorous approach to behavioral specification and verification of concurrent and distributed systems. It is among the very few techniques capable of modeling systems interaction at a level of abstraction sufficient for the interaction to be understood and analyzed. Offering both a mathematically grounded theory and real-world applications, the book is suitable for classroom use and as a reference for system architects. The book covers the foundation of behavioral modeling using process algebra, transition systems, abstract data types, and modal logics. Exercises and examples augment the theoretical discussion. The book introduces a modeling language, mCRL2, that enables concise descriptions of even the most intricate distributed algorithms and protocols. Using behavioral axioms and such proof methods as confluence, cones, and foci, readers will learn how to prove such algorithms equal to their specifications. Specifications in mCRL2 can be simulated, visualized, or verified against their requirements. An extensive mCRL2 toolset for mechanically verifying the requirements is freely available online; this toolset has been successfully used to design and analyze industrial software that ranges from healthcare applications to particle accelerators at CERN. Appendixes offer material on\u00a0\u2026", "num_citations": "212\n", "authors": ["1520"]}
{"title": "SOS formats and meta-theory: 20 years after\n", "abstract": " In 1981 Structural Operational Semantics (SOS) was introduced as a systematic way to define operational semantics of programming languages by a set of rules of a certain shape [G.D. Plotkin, A structural approach to operational semantics, Technical Report DAIMI FN-19, Computer Science Department, Aarhus University, Aarhus, Denmark, September 1981]. Subsequently, the format of SOS rules became the object of study. Using so-called Transition System Specifications (TSS\u2019s) several authors syntactically restricted the format of rules and showed several useful properties about the semantics induced by any TSS adhering to the format. This has resulted in a line of research proposing several syntactical rule formats and associated meta-theorems. Properties that are guaranteed by such rule formats range from well-definedness of the operational semantics and compositionality of behavioral equivalences to\u00a0\u2026", "num_citations": "96\n", "authors": ["1520"]}
{"title": "CPP2XMI: reverse engineering of UML class, sequence, and activity diagrams from C++ source code\n", "abstract": " In most cases, reverse engineering is used to retrieve missing design documentation from the source code in the form of an abstract (e.g., UML) model. In the context of this work, reverse engineering is used as a part of the verification and validation chain of software systems, where the static structure and the dynamic behavior of a system are derived from the source code and represented in XML Metadata Interchange (XMI) format. The obtained model is further analyzed for such characteristics as soundness and complexity of the system. XMI is a standard that enables us to express objects using Extensible Markup Language (XML). XMI can be used to represent objects from UML model in XML. In this paper, we describe a reverse engineering tool, CPP2XMI, which allows extracting UML class, sequence, and activity diagrams in XMI format from C++ source code, and its position in the toolset for software system\u00a0\u2026", "num_citations": "72\n", "authors": ["1520"]}
{"title": "Formal modeling of evolving self-adaptive systems\n", "abstract": " In this paper, we present a formal model, named PobSAM (Policy-based Self-Adaptive Model), for developing and modeling self-adaptive evolving systems. In this model, policies are used as a mechanism to direct and adapt the behavior of self-adaptive systems. A PobSAM model is a collection of autonomous managers and managed actors. The managed actors are dedicated to the functional behavior while the autonomous managers govern the behavior of managed actors by enforcing suitable policies. A manager has a set of configurations including two types of policies: governing policies and adaptation policies. To adapt the system behavior in response to the changes, the managers switch among different configurations. We employ the combination of an algebraic formalism and an actor-based model to specify this model formally. Managed actors are expressed by an actor model. Managers are modeled as\u00a0\u2026", "num_citations": "56\n", "authors": ["1520"]}
{"title": "Notions of bisimulation and congruence formats for SOS with data\n", "abstract": " While studying the specification of the operational semantics of different programming languages and formalisms, one can observe the following three facts. First, Plotkin\u2019s style of Structural Operational Semantics has become a standard in defining operational semantics. Second, congruence with respect to some notion of bisimilarity is an interesting property for such languages and it is essential in reasoning. Third, there are numerous languages that contain an explicit data part in the state of the operational semantics. The first two facts have resulted in a line of research exploring syntactic formats of operational rules to derive the desired congruence property for free. However, the third point (in combination with the first two) is not sufficiently addressed and there is no standard congruence format for operational semantics with an explicit data state. In this article, we address this problem by studying the implications\u00a0\u2026", "num_citations": "54\n", "authors": ["1520"]}
{"title": "Symmetry and partial order reduction techniques in model checking Rebeca\n", "abstract": " Rebeca is an actor-based language with formal semantics which is suitable for modeling concurrent and distributed systems and protocols. Due to its object model, partial order and symmetry detection and reduction techniques can be efficiently applied to dynamic Rebeca models. We present two approaches for detecting symmetry in Rebeca models: One that detects symmetry in the topology of inter-connections among objects and another one which exploits specific data structures to reflect internal symmetry in the internal structure of an object. The former approach is novel in that it does not require any input from the modeler and can deal with the dynamic changes of topology. This approach is potentially applicable to a wide range of modeling languages for distributed and reactive systems. We have also developed a model checking tool that implements all of the above-mentioned techniques. The\u00a0\u2026", "num_citations": "41\n", "authors": ["1520"]}
{"title": "Formal semantics and analysis of component connectors in Reo\n", "abstract": " We present an operational semantics for a component composition language called Reo. Reo connectors exogenously compose and coordinate the interactions among individual components that comprise a complex system, into a coherent collaboration. The formal semantics we present here paves the way for a rigorous study of the behavior of component composition mechanisms. To demonstrate the feasibility of such a rigorous approach, we give a faithful translation of Reo semantics into the Maude term rewriting language. This translation allows us to exploit the rewriting engine and the model-checking module in the Maude tool-set to symbolically run and model-check the behavior of Reo connectors.", "num_citations": "40\n", "authors": ["1520"]}
{"title": "Operational and epistemic approaches to protocol analysis: Bridging the gap\n", "abstract": " Operational models of protocols, on one hand, are readable and conveniently match their implementation, at a certain abstraction level. Epistemic models, on the other hand, are appropriate for specifying knowledge-related properties such as anonymity. These two approaches to specification and analysis have so far developed in parallel and one has either to define ad hoc correctness criteria for the operational model or use complicated epistemic models to specify the operational behavior. We work towards bridging this gap by proposing a combined framework which allows modeling the behavior of a protocol in a process language with an operational semantics and supports reasoning about properties expressed in a rich logic with temporal and epistemic operators.", "num_citations": "39\n", "authors": ["1520"]}
{"title": "PobSAM: policy-based managing of actors in self-adaptive systems\n", "abstract": " In this paper, we present a formal model, named PobSAM (Policy-based Self-Adaptive Model), for developing and modeling self-adaptive systems. In this model, policies are used as a mechanism to direct and adapt the behavior of self-adaptive systems. A PobSAM model consists of a set of self-managed modules(SMM). An SMM is a collection of autonomous managers and managed actors. Managed actors are dedicated to functional behavior while autonomous managers govern the behavior of managed actors by enforcing suitable policies. To adapt SMM behavior in response to changes, policies governing an SMM are adjusted, i.e., dynamic policies are used to govern and adapt system behavior. We employ the combination of an algebraic formalism and an actor-based model to specify this model formally. Managers are modeled as meta-actors whose policies are described using an algebra. Managed actors\u00a0\u2026", "num_citations": "36\n", "authors": ["1520"]}
{"title": "Modelling and analysis of communicating systems\n", "abstract": " Robin Milner observed in 1973 that the primary task of computers appeared to be interacting with their environment, yet the theory of programs and programming at that time seemed to ignore this fact completely [39, 40]. As a consequence, he set out working on his seminal book [41, 43] in which he developed the CCS, the Calculus of Communicating Systems. At the same time two other main process algebras were developed, namely ACP (Algebra of Communicating Processes,[6]) and CSP (Communicating Sequential Processes,[29, 30]).Interesting as they were, these process algebras were too bare to be used for the description of actual systems, mainly because they lacked a proper integration of data. In order to solve this, process algebraic specification languages have been designed (most notably LOTOS [32] and PSF [38]) which contained both data and processes. A problem with these languages was that they were too complex to act as a basic carrier for the development of behavioural analysis techniques. We designed an intermediate language, namely mCRL2 (and its direct predecessor \u00b5CRL [23, 20]) as a stripped down process specification language or an extended process algebra. It contains exactly those ingredients needed for a complete behavioural specification, and its (relative) simplicity allows to concentrate on proof and analysis techniques for process behaviour. Throughout the years many of these techniques have been developed. To mention a few: the Recursive Specification Principle, Invariants, Confluence, Cones and Foci, Abstract Interpretation and Coordinate Transformations, Boolean Equation Systems, Proof by\u00a0\u2026", "num_citations": "34\n", "authors": ["1520"]}
{"title": "A rule format for associativity\n", "abstract": " We propose a rule format that guarantees associativity of binary operators with respect to all notions of behavioral equivalence that are defined in terms of (im)possibility of transitions, e.g., the notions below strong bisimilarity in van Glabbeek\u2019s spectrum. The initial format is a subset of the De Simone format. We show that all trivial generalizations of our format are bound for failure. We further extend the format in a few directions and illustrate its application to several formalisms in the literature. A subset of the format is studied to obtain associativity with respect to graph isomorphism.", "num_citations": "32\n", "authors": ["1520"]}
{"title": "A syntactic commutativity format for SOS\n", "abstract": " Considering operators defined using Structural Operational Semantics (SOS), commutativity axioms are intuitive properties that hold for many of them. Proving this intuition is usually a laborious task, requiring several pages of boring and standard proof. To save this effort, we propose a syntactic SOS format which guarantees commutativity for a set of composition operators.", "num_citations": "27\n", "authors": ["1520"]}
{"title": "Input-output conformance testing based on featured transition systems\n", "abstract": " We extend the theory of input-output conformance testing to the setting of software product lines. In particular, we allow for input-output featured transition systems to be used as the basis for generating test suites and test cases. We introduce refinement operators both at the level of models and at the level of test suites that allow for projecting them into a specific product configuration (or a product sub-line). We show that the two sorts of refinement are consistent and lead to the same set of test-cases.", "num_citations": "26\n", "authors": ["1520"]}
{"title": "Notions of conformance testing for cyber-physical systems: Overview and roadmap\n", "abstract": " We review and compare three notions of conformance testing for cyber-physical systems. We begin with a review of their underlying semantic models and present conformance-preserving translations between them. We identify the differences in the underlying semantic models and the various design decisions that lead to these substantially different notions of conformance testing. Learning from this exercise, we reflect upon the challenges in designing an\" ideal\" notion of conformance for cyber-physical systems and sketch a roadmap of future research in this domain.", "num_citations": "25\n", "authors": ["1520"]}
{"title": "SOS for higher order processes\n", "abstract": " We lay the foundations for a Structural Operational Semantics (SOS) framework for higher order processes. Then, we propose a number of extensions to Bernstein\u2019s promoted tyft/tyxt format which aims at proving congruence of strong bisimilarity for higher order processes. The extended format is called promoted PANTH. This format is easier to apply and strictly more expressive than the promoted tyft/tyxt format. Furthermore, we propose and prove a congruence format for a notion of higher order bisimilarity arising naturally from our SOS framework. To illustrate our formats, we apply them to Thomsen\u2019s Calculus of Higher Order Communicating Systems (CHOCS).", "num_citations": "25\n", "authors": ["1520"]}
{"title": "Congruence for structural congruences\n", "abstract": " Structural congruences have been used to define the semantics and to capture inherent properties of language constructs. They have been used as an addendum to transition system specifications in Plotkin\u2019s style of Structural Operational Semantics (SOS). However, there has been little theoretical work on establishing a formal link between these two semantic specification frameworks. In this paper, we give an interpretation of structural congruences inside the transition system specification framework. This way, we extend a number of well-behavedness meta-theorems for SOS (such as well-definedness of the semantics and congruence of bisimilarity) to the extended setting with structural congruences.", "num_citations": "25\n", "authors": ["1520"]}
{"title": "Congruence for SOS with data\n", "abstract": " While studying the specification of the operational semantics of different programming languages and formalisms, one can observe the following three facts. Firstly, Plotkin's style of structured operational semantics (SOS) has become a standard in defining operational semantics. Secondly, congruence with respect to some notion of bisimilarity is an interesting property for such languages and it is essential in reasoning about them. Thirdly, there are numerous languages that contain an explicit data part in the state of the operational semantics. The first two facts have resulted in a line of research exploring syntactic formats of operational rules to derive the desired congruence property for free. However, the third point (in combination with the first two) is not sufficiently addressed and there is no standard congruence format for operational semantics with an explicit data state. In this paper, we address this problem by\u00a0\u2026", "num_citations": "25\n", "authors": ["1520"]}
{"title": "A hierarchy of SOS rule formats\n", "abstract": " In 1981 Structural Operational Semantics (SOS) was introduced as a systematic way to define operational semantics of programming languages by a set of rules of a certain shape [G.D. Plotkin. A structural approach to operational semantics. Technical Report DAIMI FN-19, Computer Science Department, Aarhus University, Aarhus, Denmark, September 1981. Also published in: Journal of Logic and Algebraic Programming 60\u201361 (2004) 17\u2013140]. Subsequently, the format of SOS rules became the object of study. Using so-called Transition System Specifications (TSS's) several authors syntactically restricted the format of rules and showed several useful properties about the semantics induced by any TSS adhering to the format. This has resulted in a line of research proposing several syntactical rule formats and associated meta-theorems. Properties that are guaranteed by such rule formats range from well\u00a0\u2026", "num_citations": "23\n", "authors": ["1520"]}
{"title": "Efficient symmetry reduction for an actor-based model\n", "abstract": " Symmetry reduction is a promising technique for combatting state space explosion in model checking. The problem of finding the equivalence classes, i.e., the so-called orbits, of states under symmetry is a difficult problem known to be as hard as graph isomorphism. In this paper, we show how we can automatically find the orbits in an actor-based model, called Rebeca, without enforcing any restriction on the modeler. The proposed algorithm solves the orbit problem for Rebeca models in polynomial time. As a result, the simple actor-based Rebeca language can be utilized efficiently for modeling and verification of systems, without involving the modeler with the details of the verification technique implemented.", "num_citations": "23\n", "authors": ["1520"]}
{"title": "Spinal test suites for software product lines\n", "abstract": " A major challenge in testing software product lines is efficiency. In particular, testing a product line should take less effort than testing each and every product individually. We address this issue in the context of input-output conformance testing, which is a formal theory of model-based testing. We extend the notion of conformance testing on input-output featured transition systems with the novel concept of spinal test suites. We show how this concept dispenses with retesting the common behavior among different, but similar, products of a software product line.", "num_citations": "21\n", "authors": ["1520"]}
{"title": "Orthogonal extensions in structural operational semantics\n", "abstract": " In this paper, we give novel and more liberal notions of operational and equational conservativity for language extensions. We motivate these notions by showing their practical application in existing formalisms. Based on our notions, we formulate and prove meta-theorems that establish conservative extensions for languages defined using Structural Operational Semantics (SOS).", "num_citations": "21\n", "authors": ["1520"]}
{"title": "Compositional Verification of an Object-Based Model for Reactive Systems\n", "abstract": " this paper, we focus on safety properties only, verification of progress properties may also be considered. Induction methods and special optimization algorithms for model checking of the Alecs will also be needed in the future", "num_citations": "21\n", "authors": ["1520"]}
{"title": "A tool prototype for model-based testing of cyber-physical systems\n", "abstract": " We report on a tool prototype for model-based testing of cyber-physical systems. Our starting point is a hybrid-system model specified in a domain-specific language called Acumen. Our prototype tool is implemented in Matlab and covers three stages of model-based testing, namely, test-case generation, test-case execution, and conformance analysis. We have applied our implementation to a number of typical examples of cyber-physical systems in order to analyze its applicability. In this paper, we report on the result of applying the prototype tool on a DC-DC boost converter.", "num_citations": "20\n", "authors": ["1520"]}
{"title": "Structuring structural operational semantics\n", "abstract": " Defining a formal (ie, mathematical) semantics for computer languages is the first step towards developing rigorous techniques for reasoning about computerprograms and specifications in such a language. Structural Operational Semantics (SOS), introduced by Plotkin in 1981, has become a popular technique for defining formal semantics. In this thesis, we first review the basic concepts of SOS and the existing meta-results. Subsequently, we enhance the state of the art in this field by offering the following contributions:\u2022 developing a syntactic format guaranteeing a language construct to be commutative;\u2022 extending the existing congruence and well-definedness meta-results to the setting with equational specifications;\u2022 defining a more liberal notion of operational conservativity, called orthogonality, and formulating meta-theorems for it;\u2022 prototyping a framework for checking the premises of congruence and conservativity meta-theorems and animating programs according to their SOS specification;\u2022 defining notions of bisimulation with data and formulating syntactic rule formats guaranteeing congruence for these notions;\u2022 proposing syntactic rule formats for guaranteeing congruence of strong bisimilarity and higher-order bisimilarity in the setting of higher order processes.", "num_citations": "19\n", "authors": ["1520"]}
{"title": "Sound conformance testing for cyber-physical systems: Theory and implementation\n", "abstract": " Conformance testing is a formal and structured approach to verifying system correctness. We propose a conformance testing algorithm for cyber-physical systems, based on the notion of hybrid conformance by Abbas and Fainekos. We show how the dynamics of system specification and the sampling rate play an essential role in making sound verdicts. We specify and prove error bounds that lead to sound test-suites for a given specification and a given sampling rate. We use reachability analysis to find such bounds and implement the proposed approach using the CORA toolbox in Matlab. We apply the implemented approach on a case study from the automotive domain.", "num_citations": "18\n", "authors": ["1520"]}
{"title": "Validated test models for software product lines: Featured finite state machines\n", "abstract": " Variants of the finite state machine (FSM) model have been extensively used to describe the behaviour of reactive systems. In particular, several model-based testing techniques have been developed to support test case generation and test case executions from FSMs. Most such techniques require several validation properties to hold for the underlying test models. In this paper, we propose an extension of the FSM test model for software product lines (SPLs), named featured finite state machine (FFSM). As the first step towards using FFSMs as test models, we define feature-oriented variants of basic test model validation criteria. We show how the high-level validation properties coincide with the necessary properties on the product FSMs. Moreover, we provide a mechanised tool prototype for checking the feature-oriented properties using satisfiability modulo theory (SMT) solver tools. We investigate the\u00a0\u2026", "num_citations": "17\n", "authors": ["1520"]}
{"title": "Conformance testing of cyber-physical systems: A comparative study\n", "abstract": " For systematic and automatic testing of cyber-physical systems, in which a set of test cases is generated based on a formal specification, a number of notions of conformance testing have been proposed. In this paper, we review two existing theories of conformance testing for cyber-physical systems and compare them. We point out their fundamental differences, and prove under which assumptions they coincide.", "num_citations": "17\n", "authors": ["1520"]}
{"title": "Nominal SOS\n", "abstract": " Plotkin\u02bcs style of Structural Operational Semantics (SOS) has become a de facto standard in giving operational semantics to formalisms and process calculi. In many such formalisms and calculi, the concepts of names, variables and binders are essential ingredients. In this paper, we propose a formal framework for dealing with names in SOS. The framework is based on the Nominal Logic of Gabbay and Pitts and hence is called Nominal SOS. We define nominal bisimilarity, an adaptation of the notion of bisimilarity that is aware of binding. We provide evidence of the expressiveness of the framework by formulating the early \u03c0-calculus and Abramsky\u02bcs lazy \u03bb-calculus within Nominal SOS. For both calculi we establish the operational correspondence with the original calculi. Moreover, in the context of the \u03c0-calculus, we prove that nominal bisimilarity coincides with Sangiorgi\u02bcs open bisimilarity and in the context of\u00a0\u2026", "num_citations": "17\n", "authors": ["1520"]}
{"title": "Synchronizing asynchronous conformance testing\n", "abstract": " We present several theorems and their proofs which enable using synchronous testing techniques such as input output conformance testing (ioco ) in order to test implementations only accessible through asynchronous communication channels. These theorems define when the synchronous test-cases are sufficient for checking all aspects of conformance that are observable by asynchronous interaction with the implementation under test.", "num_citations": "17\n", "authors": ["1520"]}
{"title": "Sound test-suites for cyber-physical systems\n", "abstract": " Conformance testing is a formal and structured approach to verifying system correctness. We propose a conformance testing algorithm for cyber-physical systems, based on the notion of hybrid conformance by Abbas and Fainekos. We show how the dynamics of system specification and the sampling rate play an essential role in making sound verdicts. We specify and prove error bounds that lead to sound test-suites for a given specification and a given sampling rate.", "num_citations": "16\n", "authors": ["1520"]}
{"title": "Towards model-based testing of electronic funds transfer systems\n", "abstract": " We report on our first experience with applying model-based testing techniques to an operational Electronic Funds Transfer (EFT) switch. The goal is to test the conformance of the EFT switch to the standard flows described by the ISO 8583 standard. To this end, we first make a formalization of the transaction flows specified in the ISO 8583 standard in terms of a Labeled Transition System (LTS). This formalization paves the way for model-based testing based on the formal notion of Input-Output Conformance (IOCO) testing. We adopt and augment IOCO testing for our particular application domain. We develop a prototype implementation and apply our proposed techniques in practice. We discuss the encouraging obtained results and the observed shortcomings of the present approach. We outline a roadmap to remedy the shortcomings and enhance the test results.", "num_citations": "16\n", "authors": ["1520"]}
{"title": "Model-based testing of cyber-physical systems\n", "abstract": " Cyber-physical systems (CPSs) are the result of the integration of connected computer systems with the physical world. They feature complex interactions that go beyond traditional communication schemes and protocols in computer systems. One distinguished feature of such complex interactions is the tight coupling between discrete and continuous interactions, captured by hybrid system models.Due to the complexity of CPSs, providing rigorous and model-based analysis methods and tools for verifying correctness of such systems is of the utmost importance. Model-based testing (MBT) is one such verification technique that can be used for checking the conformance of an implementation of a system to its specification (model).In this chapter, we first review the main concepts and techniques in MBT. Subsequently, we review the most common modeling formalisms for CPSs, with focus on hybrid system models\u00a0\u2026", "num_citations": "15\n", "authors": ["1520"]}
{"title": "Semantics and expressiveness of ordered SOS\n", "abstract": " Structured Operational Semantics (SOS) is a popular method for defining semantics by means of transition rules. An important feature of SOS rules is negative premises, which are crucial in the definitions of such phenomena as priority mechanisms and time-outs. However, the inclusion of negative premises in SOS rules also introduces doubts as to the preferred meaning of SOS specifications.Orderings on SOS rules were proposed by Phillips and Ulidowski as an alternative to negative premises. Apart from the definition of the semantics of positive GSOS rules with orderings, the meaning of more general types of SOS rules with orderings has not been studied hitherto. This paper presents several candidates for the meaning of general SOS rules with orderings and discusses their conformance to our intuition for such rules.We take two general frameworks (rule formats) for SOS with negative premises and SOS with\u00a0\u2026", "num_citations": "15\n", "authors": ["1520"]}
{"title": "Extending HSI test generation method for software product lines\n", "abstract": " Featured Finite State Machines (FFSMs) were proposed as a modeling formalism that represents the abstract behavior of an entire software product line (SPL). Several model-based testing techniques have been developed to support test case generation for SPL specifications, but none support the full fault coverage criterion for SPLs at the family-wide level. In this paper, we propose an extension of the Harmonized State Identifiers (HSI) method, an FSM-based testing method supporting full fault coverage. By extending the HSI method for FFSMs, we are able to generate a single configurable test suite for groups of SPL products that can be instantiated using feature constraints. We implement a graphical tool named ConFTGen to guide the design, validation, derivation and test case generation for state, transition and full fault coverage of FFSMs. Experimental results indicate a reduction of approximately 50\u00a0\u2026", "num_citations": "14\n", "authors": ["1520"]}
{"title": "Input\u2013output conformance testing for software product lines\n", "abstract": " We extend the theory of input\u2013output conformance (IOCO) testing to accommodate behavioral models of software product lines (SPLs). We present the notions of residual and spinal testing. These notions allow for structuring the test process for SPLs by taking variability into account and extracting separate test suites for common and specific features of an SPL. The introduced notions of residual and spinal test suites allow for focusing on the newly introduced behavior and avoiding unnecessary re-test of the old one. Residual test suites are very conservative in that they require retesting the old behavior that can reach to new behavior. However, spinal test suites more aggressively prune the old tests and only focus on those test sequences that are necessary in reaching the new behavior. We show that residual testing is complete but does not usually lead to much reduction in the test-suite. In contrast, spinal testing\u00a0\u2026", "num_citations": "14\n", "authors": ["1520"]}
{"title": "Prototyping SOS meta-theory in Maude\n", "abstract": " We present a prototype implementation of SOS meta-theory in the Maude term rewriting language. The prototype defines the basic concepts of SOS meta-theory (e.g., transition formulae, deduction rules and transition system specifications) in Maude. Besides the basic definitions, we implement methods for checking the premises of some SOS meta-theorems (e.g., GSOS congruence meta-theorem) in this framework. Furthermore, we define a generic strategy for animating programs and models for semantic specifications in our meta-language. The general goal of this line of research is to develop a general-purpose tool that assists language designers by checking useful properties about the language under definition and by providing a rapid prototyping environment for scrutinizing the actual behavior of programs according to the defined semantics.", "num_citations": "14\n", "authors": ["1520"]}
{"title": "Integrating model-based and constraint-based testing using SpecExplorer\n", "abstract": " We report on our experience with model-based testing using SpecExplorer within the Flat X-Ray Detection (FXD) Department of Philips Healthcare. Our initial experiments showed a practical obstacle in combining traditional functional testing techniques with model-based testing using SpecExplorer. We overcome this obstacle by specifying the constraints on our data domain in a spreadsheet and interfacing SpecExplorer with a constraint solver in order to generate concrete test data for the behavioral specifications. We report on some empirical results obtained from our experiments.", "num_citations": "13\n", "authors": ["1520"]}
{"title": "Property-based testing of quantum programs in Q#\n", "abstract": " Property-based testing is a structured method for automated testing using program specifications. We report on the design and implementation of what is to our knowledge the first property-based framework for quantum programs. We review various aspects of our design concerning property-specification, test-case generation, and test result analysis. We also provide an overview of the implementation and its way of working. Finally, we present the result of applying our framework to some examples.", "num_citations": "11\n", "authors": ["1520"]}
{"title": "Early fault detection in DSLs using SMT solving and automated debugging\n", "abstract": " In the context of Domain Specific Languages (DSLs), we study ways to detect faults early in the software development cycle. We propose techniques that validate a wide range of properties, classified into basic and advanced. Basic validation includes syntax checking, reference checking and type checking. Advanced validation concerns domain specific properties related to the semantics of the DSL. For verification, we mechanically translate the DSL instance and the advanced properties into Satisfiability Modulo Theory (SMT) problems, and solve these problems using an SMT solver. For user feedback, we extend the verification with automated debugging, which pinpoints the causes of the violated properties and traces them back to the syntactic constructs of the DSL. We illustrate this integration of techniques using an industrial case on collision prevention for medical imaging equipment.", "num_citations": "11\n", "authors": ["1520"]}
{"title": "Temporal logic falsification of cyber-physical systems: An input-signal-space optimization approach\n", "abstract": " Temporal logic falsification is a promising approach to model-based testing of cyber-physical systems. It starts off with a formalized system requirement specified as a Metric Temporal Logic (MTL) property. Subsequently, test input signals are generated in order to stimulate the system and produce an output signal. Finally, output signals of the system under test are compared to those prescribed by the property to falsify the property by means of a counterexample. To find such a counterexample, Markov Chain Monte-Carlo (MCMC) methods are used to construct an optimization problem to steer the test input generations to those input areas that maximize the probability of falsifying the property. In this paper, we identify two practical issues in the above-mentioned falsification process. Firstly, a fixed time domain of the input-signal space is assumed in this process, which restricts the frequency content of the (generated\u00a0\u2026", "num_citations": "10\n", "authors": ["1520"]}
{"title": "(De-) composing causality in labeled transition systems\n", "abstract": " In this paper we introduce a notion of counterfactual causality in the Halpern and Pearl sense that is compositional with respect to the interleaving of transition systems. The formal framework for reasoning on what caused the violation of a safety property is established in the context of labeled transition systems and Hennessy Milner logic. The compositionality results are devised for non-communicating systems.", "num_citations": "10\n", "authors": ["1520"]}
{"title": "Synchrony and asynchrony in conformance testing\n", "abstract": " We present and compare different notions of conformance testing based on labeled transition systems. We formulate and prove several theorems which enable using synchronous conformance testing techniques such as input\u2013output conformance testing (ioco\u00a0) in order to test implementations only accessible through asynchronous communication channels. These theorems define when the synchronous test cases are sufficient for checking all aspects of conformance that are observable by asynchronous interaction with the implementation under test.", "num_citations": "10\n", "authors": ["1520"]}
{"title": "Decomposability in input output conformance testing\n", "abstract": " We study the problem of deriving a specification for a third-party component, based on the specification of the system and the environment in which the component is supposed to reside. Particularly, we are interested in using component specifications for conformance testing of black-box components, using the theory of input-output conformance (ioco) testing. We propose and prove sufficient criteria for decompositionality, i.e., that components conforming to the derived specification will always compose to produce a correct system with respect to the system specification. We also study the criteria for strong decomposability, by which we can ensure that only those components conforming to the derived specification can lead to a correct system.", "num_citations": "10\n", "authors": ["1520"]}
{"title": "Mechanized extraction of topology anti-patterns in wireless networks\n", "abstract": " Exhaustive and mechanized formal verification of wireless networks is hampered by the huge number of possible topologies and the large size of the actual networks. However, the generic communication structure in such networks allows for reducing the root causes of faults to faulty (sub-)topologies, called anti-patterns, of small size. We propose techniques to find such anti-patterns using a combination of model-checking and automated debugging. We apply the proposed technique on two well-known protocols for wireless sensor networks and show that the techniques indeed find the root causes in terms of canonical topologies featuring the fault.", "num_citations": "10\n", "authors": ["1520"]}
{"title": "Model checking Verilog descriptions of cell libraries\n", "abstract": " We present a formal semantics for a subset of Verilog, commonly used to describe cell libraries, in terms of transition systems. Such transition systems can serve as input to symbolic model checking, for example equivalence checking with a transistor netlist description. We implement our formal semantics as an encoding from the subset of Verilog to the input language of the SMV model-checker. Experiments show that this approach is able to verify complete cell libraries.", "num_citations": "10\n", "authors": ["1520"]}
{"title": "Process algebraic verification of SystemC codes\n", "abstract": " SystemC is an IEEE standard system-level language used in hardware/software co-design and has been widely adopted in the industry. This paper describes a formal approach to verifying SystemC codes by providing a mapping to the process algebra mCRL2. The outstanding advantages of mCRL2 are the support for different data types and a powerful tool-set for model reduction and analysis. A tool is implemented to automatically perform the proposed mapping. This translation enabled us to exploit process-algebraic verification techniques to analyze a number of case-studies, including the formal analysis of a single-cycle and a pipelined MIPS processor specified in SystemC.", "num_citations": "9\n", "authors": ["1520"]}
{"title": "Robustness of equations under operational extensions\n", "abstract": " Sound behavioral equations on open terms may become unsound after conservative extensions of the underlying operational semantics. Providing criteria under which such equations are preserved is extremely useful; in particular, it can avoid the need to repeat proofs when extending the specified language. This paper investigates preservation of sound equations for several notions of bisimilarity on open terms: closed-instance (ci-)bisimilarity and formal-hypothesis (fh-)bisimilarity, both due to Robert de Simone, and hypothesis-preserving (hp-)bisimilarity, due to Arend Rensink. For both fh-bisimilarity and hp-bisimilarity, we prove that arbitrary sound equations on open terms are preserved by all disjoint extensions which do not add labels. We also define slight variations of fh- and hp-bisimilarity such that all sound equations are preserved by arbitrary disjoint extensions. Finally, we give two sets of syntactic criteria (on equations, resp. operational extensions) and prove each of them to be sufficient for preserving ci-bisimilarity.", "num_citations": "8\n", "authors": ["1520"]}
{"title": "Formal Analysis of Non-Determinism in Verilog Cell Library Simulation Models\n", "abstract": " Cell libraries often contain a simulation model in a system design language, such as Verilog. These languages usually involve non-determinism, which in turn, poses a challenge to their validation. Simulators often resolve such problems by using certain rules to make the specification deterministic. This however is not justified by the behavior of the hardware that is to be modeled. Hence, simulation might not be able to detect certain errors. In this paper we develop a technique to prove whether non-determinism does not affect the behavior of the simulation model, or whether there exists a situation in which the simulation model might produce different results. To make our technique efficient, we show that the global property of equal behavior for all possible evaluations is equivalent to checking only a certain local property.", "num_citations": "8\n", "authors": ["1520"]}
{"title": "The meaning of ordered SOS\n", "abstract": " Structured Operational Semantics (SOS) is a popular method for defining semantics by means of deduction rules. An important feature of deduction rules, or simply SOS rules, are negative premises, which are crucial in the definitions of such phenomena as priority mechanisms and time-outs. Orderings on SOS rules were proposed by Phillips and Ulidowski as an alternative to negative premises. The meaning of general types of SOS rules with orderings has not been studied hitherto. This paper presents satisfactory ways of giving a meaning to general SOS rules with orderings. We also give semantics-preserving transformations between the two paradigms, namely, SOS with negative premises and SOS with orderings.", "num_citations": "8\n", "authors": ["1520"]}
{"title": "Making nondeterminism explicit in Z\n", "abstract": " Specification of system requirements is often involved with ambiguity and nondeterminism. Formal methods tend to mitigate ambiguity but nondeterminism remains as an inherent part of specification. This is due to the abstraction from real world details that causes a formal specification to define several results as a correct solution to a problem. Hence, a support for nondeterminism should be foreseen in formal methods.In this paper, after studying different types of nondeterminism, some basic notations, namely multi-and power-schema, are added to Z formal language, to help explicit specification of nondeterminate constructs. Afterwards, a transformation is defined to generate nondeterministic semantics from specification in the same language. The results of adding the proposed notations is discussed from program development point of view.", "num_citations": "8\n", "authors": ["1520"]}
{"title": "Automatic consequence analysis of automotive standards (AUTO-CAAS)\n", "abstract": " This paper provides some background and the roadmap of the AUTO-CAAS project, which is a 3-year project financed by the Swedish Knowledge Foundation and is ongoing as a joint project among three academic and industrial partners. The aim of the project is to exploit the formal models of the AUTOSAR standard, developed by the industrial partner of the project Quviq AB, in order to predict possible future failures in concrete implementations of components. To this end, the deviations from the formal specification will be exploited to generate test-cases that can push concrete components to the corners where such deviation will result in observable failures. The same information will also be used in the diagnosis of otherwise detected failures in order to pinpoint their root causes.", "num_citations": "7\n", "authors": ["1520"]}
{"title": "On the complexity of input output conformance testing\n", "abstract": " Input-output conformance (ioco) testing is a well-known approach to model-based testing. In this paper, we study the complexity of checking ioco. We show that the problem of checking ioco is PSPACE-complete. To provide a more efficient algorithm, we propose a more restricted setting for checking ioco, namely with deterministic models and show that in this restricted setting ioco checking can be performed in polynomial time.", "num_citations": "7\n", "authors": ["1520"]}
{"title": "Algebraic meta-theory of processes with data\n", "abstract": " There exists a rich literature of rule formats guaranteeing different algebraic properties for formalisms with a Structural Operational Semantics. Moreover, there exist a few approaches for automatically deriving axiomatizations characterizing strong bisimilarity of processes. To our knowledge, this literature has never been extended to the setting with data (e.g. to model storage and memory). We show how the rule formats for algebraic properties can be exploited in a generic manner in the setting with data. Moreover, we introduce a new approach for deriving sound and ground-complete axiom schemata for a notion of bisimilarity with data, called stateless bisimilarity, based on intuitive auxiliary function symbols for handling the store component. We do restrict, however, the axiomatization to the setting where the store component is only given in terms of constants.", "num_citations": "7\n", "authors": ["1520"]}
{"title": "A congruence rule format with universal quantification\n", "abstract": " We investigate the addition of universal quantification to the meta-theory of Structural Operational Semantics (SOS). We study the syntax and semantics of SOS rules extended with universal quantification and propose a congruence rule format for strong bisimilarity that supports this new feature.", "num_citations": "7\n", "authors": ["1520"]}
{"title": "Multi-objective search for effective testing of cyber-physical systems\n", "abstract": " We propose a multi-objective strategy for finding effective inputs for fault detection in Cyber Physical Systems (CPSs). The main goal is to provide input signals for a system in such a way that they maximise the distance between the system\u2019s output and an ideal target, thus leading the system towards a fault; this is based on Genetic Algorithm and Simulated Annealing heuristics. Additionally, we take into consideration the discrete locations (of hybrid system models) and a notion of input diversity to increase coverage. We implement our strategy and present an empirical analysis to estimate its effectiveness.", "num_citations": "6\n", "authors": ["1520"]}
{"title": "Hierarchical featured state machines\n", "abstract": " Variants of the Finite State Machine (FSM) model have been extensively used to describe the behavior of reactive systems. In particular, several model-based testing techniques have been developed to support test case generation from FSMs and test case execution. Most of such techniques require several validation properties to hold for the underlying test models. Featured Finite State Machine (FFSM) is an extension of the FSM model proposed in our earlier publication that represents the abstract behavior of an entire Software Product Line (SPL). By validating an FFSM, we validate all valid products configurations of the SPL looking forward configurable test suites. However, modeling a large SPL using flat FFSMs may lead to a huge and hard-to-maintain specification. In this paper, we propose an extension of the FFSM model, named Hierarchical Featured State Machine (HFSM). Inspired by Statecharts and\u00a0\u2026", "num_citations": "6\n", "authors": ["1520"]}
{"title": "Reducing the concretization effort in fsm-based testing of software product lines\n", "abstract": " To test a Software Product Line (SPL), the test artifacts and the techniques must be extended to support variability. In general, when new SPL products are developed, more tests are generated to cover new or modified features. A dominant source of extra effort for such tests is the concretization of newly generated tests. Thus, minimizing the amount of new nonconcretized tests required to perform conformance testing on new products reduces the overall test effort. In this paper, we propose a test reuse strategy for conformance testing of SPL products that aims at reducing test effort. We use incremental test generation methods based on finite state machines (FSMs) to maximize test reuse. We combine these methods with a selection algorithm used to identify non-redundant concretized tests. We illustrate our strategy using examples and a case study with an embedded mobile SPL. The results indicate that our\u00a0\u2026", "num_citations": "6\n", "authors": ["1520"]}
{"title": "State distribution policy for distributed model checking of actor models\n", "abstract": " Model checking temporal properties is often reduced to finding accepting cycles in B\u00fcchi automata. A key ingredient for an effective distributed model checking technique is a distribution policy that does not split the potential accepting cycles of the corresponding automaton among several nodes. In this paper, we introduce a distribution policy to reduce the number of split cycles. This policy is based on the call dependency graph, obtained from the message passing skeleton of the model. We prove theoretical results about the correspondence between the cycles of the call dependency graph and the cycles of the concrete state space and provide empirical data obtained from applying our distribution policy in state space generation and reachability analysis. We take Rebeca, an imperative interpretation of actors, as our modeling language and implement the introduced policy in its distributed state space generator. Our technique can be applied to other message-driven actor-based models where concurrent objects or services are units of concurrency.", "num_citations": "6\n", "authors": ["1520"]}
{"title": "Specification, simulation, and verification of component connectors in Reo\n", "abstract": " Coordination and composition of components is an essential concern in component-based software engineering. In this paper, we present an operational semantics for a component composition language called Reo. Reo connectors exogenously compose and coordinate the interactions among individual components, that unawarely comprise a complex system, into a coherent collaboration. The formal semantics we present here paves the way for studying the behavior of component composition mechanisms rigorously. To demonstrate the feasibility of such a rigorous approach, we give a faithful translation of Reo semantics into the Maude term rewriting language. This translation allows us to exploit the rewriting engine and the modelchecking module in the Maude tool-set to symbolically run and model-check the behavior of Reo connectors.", "num_citations": "6\n", "authors": ["1520"]}
{"title": "Learning to reuse: Adaptive model learning for evolving systems\n", "abstract": " Software systems undergo several changes along their life-cycle and hence, their models may become outdated. To tackle this issue, we propose an efficient algorithm for adaptive learning, called  () that improves upon the state of the art by exploring observation tables on-the-fly to discard redundant prefixes and deprecated suffixes. Using 18 versions of the OpenSSL toolkit, we compare our proposed algorithm along with three adaptive algorithms. For the existing algorithms in the literature, our experiments indicate a strong positive correlation between number of membership queries and temporal distance between versions and; for our algorithm, we found a weak positive correlation between membership queries and temporal distance, as well, a significantly lower number of membership queries. These findings indicate that, compared to the state-of-the-art algorithms, our  algorithm is\u00a0\u2026", "num_citations": "5\n", "authors": ["1520"]}
{"title": "Learning from difference: an automated approach for learning family models from software product lines\n", "abstract": " Substantial effort has been spent on extending specification notations and their associated reasoning techniques to software product lines (SPLs). Family-based analysis techniques operate on a single artifact, referred to as a family model, that is annotated with variability constraints. This modeling approach paves the way for efficient model-based testing and model checking for SPLs. Albeit reasonably efficient, the creation and maintenance of family models tend to be time consuming and error-prone, especially if there are crosscutting features. To tackle this issue, we introduce FFSM Diff, a fully automated technique to learn featured finite state machines (FFSM), a family-based formalism that unifies Mealy Machines from SPLs into a single representation. Our technique incorporates variability to compare and merge Mealy machines and annotate states and transitions with feature constraints. We evaluate our\u00a0\u2026", "num_citations": "5\n", "authors": ["1520"]}
{"title": "Cyber Physical Systems. Design, Modeling, and Evaluation: 5th International Workshop, CyPhy 2015, Amsterdam, The Netherlands, October 8, 2015, Proceedings\n", "abstract": " URN: urn: nbn: se: hh: diva-30173 DOI: 10.1007/978-3-319-25141-7 Scopus ID: 2-s2. 0-84952683654 ISBN: 978-3-319-25140-0 ISBN: 978-3-319-25141-7 OAI: oai: DiVA. org: hh-30173 DiVA, id: diva2: 894429", "num_citations": "5\n", "authors": ["1520"]}
{"title": "Statistical properties of high frequency acoustic fluctuations induced by rough sea surface\n", "abstract": " Probability Density Function (PDF) of the arrival angle and arrival time of high-frequency sound forward scattering from the rough sea surface are predicted by combining the ray-based and surface-based models. Empirical ocean wave spectrum and empirical model of the sea surface slope are used to model the surface-wave model. We show that environmental parameters and geometrical conditions influence the statistical properties of a wave forward-scattered from the sea surface. To show the validity of the approach, the results are compared with an experimental model. We observe that the proposed model is in good agreement with the expermentally obtained model.", "num_citations": "5\n", "authors": ["1520"]}
{"title": "Formal specification and analysis of accelerated heartbeat protocols\n", "abstract": " We present a formal analysis of all different variations of accelerated heartbeat protocols presented in [MG Gouda and TM McGuire, Accelerated Heartbeat Protocols, Proc. of ICDCS'98]. We formalize the specification of the protocols both in a process-algebraic and in an automata-theoretic formalism. Then, we formulate some natural functional requirements on the above-mentioned protocols and formalize these requirements. Using model-checking techniques, we verify these requirement on each and every version. We report counter-examples witnessing that the formulated requirements are not satisfied. We propose solutions for different versions of the protocol and model check the fixed versions; the model checking results indicate that the fixed versions indeed satisfy the requirements.", "num_citations": "5\n", "authors": ["1520"]}
{"title": "Causality in the semantics of Esterel: Revisited\n", "abstract": " We re-examine the challenges concerning causality in the semantics of Esterel and show that they pertain to the known issues in the semantics of Structured Operational Semantics with negative premises. We show that the solutions offered for the semantics of SOS also provide answers to the semantic challenges of Esterel and that they satisfy the intuitive requirements set by the language designers.", "num_citations": "5\n", "authors": ["1520"]}
{"title": "Timing the untimed: Terminating successfully while being conservative\n", "abstract": " There have been several timed extensions of ACP-style process algebras with successful termination. None of them, to our knowledge, are equationally conservative (ground-)extensions of ACP with successful termination. Here, we point out some design decisions which were the possible causes of this misfortune and by taking different decisions, we propose a spectrum of timed process algebras ordered by equational conservativity ordering.", "num_citations": "5\n", "authors": ["1520"]}
{"title": "Towards an approximate conformance relation for hybrid I/O automata\n", "abstract": " Several notions of conformance have been proposed for checking the behavior of cyber-physical systems against their hybrid systems models. In this paper, we explore the initial idea of a notion of approximate conformance that allows for comparison of both observable discrete actions and (sampled) continuous trajectories. As such, this notion will consolidate two earlier notions, namely the notion of Hybrid Input-Output Conformance (HIOCO) by M. van Osch and the notion of Hybrid Conformance by H. Abbas and G.E. Fainekos. We prove that our proposed notion of conformance satisfies a semi-transitivity property, which makes it suitable for a step-wise proof of conformance or refinement.", "num_citations": "4\n", "authors": ["1520"]}
{"title": "A Pre-congruence Format for XY-simulation\n", "abstract": " XY-simulation is a generalization of bisimulation that is parameterized with two subsets of actions. XY-simulation is known in the literature under different names such as modal refinement, partial bisimulation, and alternating simulation. In this paper, we propose a pre-congruence rule format for XY-simulation. The format allows for checking compositionality of XY-simulation for an arbitrary language with structural operational semantics, by performing very simple checks on the syntactic shape of the rules. We apply our format to derive concrete compositionality results for different notions of behavioral pre-order with respect to different process calculi in the literature.", "num_citations": "4\n", "authors": ["1520"]}
{"title": "Simulation of hybrid systems from natural-language requirements\n", "abstract": " Cyber-physical systems are characterised by a massive and tight interaction between computer systems and physical components. Hybrid systems provide an abstraction for modelling cyber-physical systems by featuring the integration of discrete and continuous behavioural aspects. Simulation is an important tool for validating hybrid system models, which are often too complex to be treated using other validation and verification techniques. Motivated by the industrial need for such tools, we propose a strategy (h-NAT2TEST) for simulation of hybrid systems from natural-language requirements. Using the proposed approach, one writes the system specification using a controlled natural language, from which an informal semantics is automatically inferred based on the case grammar theory. Then, a formal representation is built considering a model of hybrid data-flow reactive systems (h-DFRS). Finally, in order to\u00a0\u2026", "num_citations": "3\n", "authors": ["1520"]}
{"title": "A process for sound conformance testing of cyber-physical systems\n", "abstract": " We present a process for sound conformance testing of cyber-physical systems, which involves functional but also non-functional aspects. The process starts with a hybrid model of cyber-physical systems in which the correct behavior of the system (at its interface level) is specified. Such a model captures both discrete behavior and evolution of continuous dynamics of the system in time. Since conformance testing inherently involves comparing continuous dynamics, the key parameters of the process are (1) the conformance bounds defining when two signals are sufficiently close to each other, and (2) the permitted error margin in the conformance analysis introduced by sampling of continuous signals. The final parameter of this process is (3) finding (and adjusting) the sampling rate of the dynamic behavior. In the specified process, we provide different alternatives for fixing the error margin of the conformance testing\u00a0\u2026", "num_citations": "3\n", "authors": ["1520"]}
{"title": "Cyber Physical Systems. Design, Modeling, and Evaluation: 5th International Workshop, CyPhy 2015, Amsterdam, the Netherlands, October 8, 2015, Proceedings\n", "abstract": " This book constitutes the proceedings of the 5th International Workshop on Design, Modeling, and Evaluation of Cyber Physical Systems, CyPhy 2015, held as part of ESWeek 2015, in Amsterdam, The Netherlands, in October 2015. The 10 papers presented in this volume were carefully reviewed and selected from 13 submissions. They broadly interpret, from a diverse set of disciplines, the modeling, simulation, and evaluation of cyber-physical systems.", "num_citations": "3\n", "authors": ["1520"]}
{"title": "Two logical characterizations for input-output conformance\n", "abstract": " Input-output conformance (ioco) is a behavioral relation on input-output labeled transition systems. In this note, we propose two logical characterizations of the ioco relation in the style of Hennessy and Milner. The first one is an implicit characterization at the level of suspension automata, which are extensively used by the tools that establish the ioco relation, while the other is an explicit characterization at the level of input-output labeled transition systems.", "num_citations": "3\n", "authors": ["1520"]}
{"title": "Modular semantics for transition system specifications with negative premises\n", "abstract": " Transition rules with negative premises are needed in the structural operational semantics of programming and specification constructs such as priority and interrupt, as well as in timed extensions of specification languages. The well-known proof-theoretic semantics for transition system specifications involving such rules is based on well-supported proofs for closed transitions. Dealing with open formulae by considering all closed instances is inherently non-modular \u2013 proofs are not necessarily preserved by disjoint extensions of the transition system specification.               Here, we conservatively extend the notion of well-supported proof to open transition rules. We prove that the resulting semantics is modular, consistent, and closed under instantiation. Our results provide the foundations for modular notions of bisimulation such that equivalence can be proved with reference only to the relevant rules, without\u00a0\u2026", "num_citations": "3\n", "authors": ["1520"]}
{"title": "Interpreted systems semantics for process algebra with identity annotations\n", "abstract": " Process algebras have been developed as formalisms for specifying the behavioral aspects of protocols. Interpreted systems have been proposed as a semantic model for multi-agent communication. In this paper, we connect these two formalisms by defining an interpreted systems semantics for a generic process algebraic formalism. This allows us to translate and compare the vast body of knowledge and results for each of the two formalisms to the other and perform epistemic reasoning, e.g., using model-checking tools for interpreted systems, on process algebraic specifications. Based on our translation we formulate and prove some results about the interpreted systems generated by process algebraic specifications.", "num_citations": "3\n", "authors": ["1520"]}
{"title": "RDR: A language for restricted delegation and revocation\n", "abstract": " We add two statements for restricted delegation and revocation (of earlier delegations) to a security-typed programming language and study their influence on the formal semantics of the extended language. We discuss different alternatives for the presented semantics and point out their consequences. We present a type system for the sublanguage language containing restricted delegation.", "num_citations": "3\n", "authors": ["1520"]}
{"title": "Formal analysis of SystemC designs in process algebra\n", "abstract": " SystemC is an IEEE standard system-level language used in hardware/software co-design and has been widely adopted in the industry. This paper describes a formal approach to verifying SystemC designs by providing a mapping to the process algebra mCRL2. Our mapping formalizes both the simulation semantics as well as exhaustive state-space exploration of SystemC designs. By exploiting the existing reduction techniques of mCRL2 and also its model-checking tools, we efficiently locate the race conditions in a system and resolve them. A tool is implemented to automatically perform the proposed mapping. This mapping and the implemented tool enabled us to exploit process-algebraic verification techniques to analyze a number of case-studies, including the formal analysis of a single-cycle and a pipelined MIPS processor specified in SystemC.", "num_citations": "3\n", "authors": ["1520"]}
{"title": "Order-independence of vector-based transition systems\n", "abstract": " Semantics of many specification languages, particularly those used in the domain of hardware, is described in terms of vector-based transition systems. In such a transition system, each macro-step transition is labeled by a vector of inputs. When performing a macro-step, several inputs may potentially change. Each macro-step can thus be decomposed in a number of micro-steps, taking one input change at a time into account. This is akin to an interleaving semantics, where a concurrent step is represented by an interleaving of its constituting components. We present criteria on vector-based transition systems, which guarantee that the next state computation is independent of the order in which these micro-steps are executed. If our criteria are satisfied by the semantic definition of a certain specification, then its state-space generation or exploration algorithm needs to only consider one representative among all\u00a0\u2026", "num_citations": "3\n", "authors": ["1520"]}
{"title": "Restricted delegation and revocation in language-based security: (position paper)\n", "abstract": " In this paper, we introduce a notion of restricted revocable delegation and study its consequences in language-based security. In particular, we add this notion by means of delegate and revoke commands to a simple imperative programming language. We then define an operational semantics for our programming language, in the Natural Semantics style of Gilles Kahn. We briefly discuss our initial ideas about the security properties of the semantics, which are extensions of existing variations of the renowned non-interference property, eg, in the context of delimited information release.", "num_citations": "3\n", "authors": ["1520"]}
{"title": "A framework for performance evaluation and functional verification in stochastic process algebras\n", "abstract": " Despite its relatively short history, a wealth of formalisms exist for algebraic specification of stochastic systems. The goal of this paper is to give such formalisms a unifying framework for performance evaluation and functional verification. To this end, we propose an approach enabling a provably sound transformation from some existing stochastic process algebras, eg, PEPA and MTIPP, to a generic form in the mCRL2 language. This way, we resolve the semantic differences among different stochastic process algebras themselves, on one hand, and between stochastic process algebras and classic ones, such as mCRL2, on the other hand. From the generic form, one can generate a state space and perform various functional and performance-related analyses, as we illustrate in this paper.", "num_citations": "3\n", "authors": ["1520"]}
{"title": "Sarir: A rebeca to mCRL2 translator\n", "abstract": " We describe a translation from Rebeca, an actor-based language, to mCRL2, a process algebra enhanced with data types. The main motivation is to exploit the verification tools and theories developed for mCRL2 in Rebeca. The mapping is applied to several case-studies including the tree identify phase of the IEEE 1394 standard. The results of the experiment show that the minimization tools of mCRL2 can be very effective and the outcome of the present translation outperforms that of the translation to the input language of the Spin model-checker.", "num_citations": "3\n", "authors": ["1520"]}
{"title": "Nondeterminism in set theoretic formal specifications: A constructive approach\n", "abstract": " Formal methods serve as mathematical tools for specication and verication of software applications. Formal specication is often used with an abstraction from real world details. As a result, nondeterminism is introduced in formal specications and it should be supported in above mentioned methods.In this paper, a basic notation to specify nondeterminacy is added to a formal language, called CZ and a transformation to generate nondeterministic semantics is dened in the same language. Afterwards, a concurrent database management scheduler is specied with the notation and the transformation is applied to it.", "num_citations": "3\n", "authors": ["1520"]}
{"title": "Logical characterisation of hybrid conformance\n", "abstract": " Logical characterisation of a behavioural equivalence relation precisely specifies the set of formulae that are preserved and reflected by the relation. Such characterisations have been studied extensively for exact semantics on discrete models such as bisimulations for labelled transition systems and Kripke structures, but to a much lesser extent for approximate relations, in particular in the context of hybrid systems. We present what is to our knowledge the first characterisation result for approximate notions of hybrid refinement and hybrid conformance involving tolerance thresholds in both time and value. Since the notion of conformance in this setting is approximate, any characterisation will unavoidably involve a notion of relaxation, denoting how the specification formulae should be relaxed in order to hold for the implementation. We also show that an existing relaxation scheme on Metric Temporal Logic used for preservation results in this setting is not tight enough for providing a characterisation of neither hybrid conformance nor refinement. The characterisation result, while interesting in its own right, paves the way to more applied research, as our notion of hybrid conformance underlies a formal model-based technique for the verification of cyber-physical systems.", "num_citations": "2\n", "authors": ["1520"]}
{"title": "Gray-box conformance testing for symbolic reactive state machines\n", "abstract": " Model-based testing (MBT) is typically a black-box testing technique. Therefore, generated test suites may leave some untested gaps in a given implementation under test (IUT). We propose an approach to use the structural and behavioural information exploited from the implementation domain\u00a0to generate effective and efficient test suites. Our approach considers both specification models and implementation models, and generates an enriched test model which is used to automatically generate test suites. We show that the proposed approach is sound and exhaustive and cover both the specification and the implementation. We examine the applicability and the effectiveness of our approach by applying it to a well-known example from the railway domain.", "num_citations": "2\n", "authors": ["1520"]}
{"title": "Topics in Theoretical Computer Science\n", "abstract": " Welcome to the proceedings of the First IFIP International Conference on Topics in Theoretical Computer Science (TTCS 2015)! This volume contains the revised papers presented at TTCS 2015. The conference was held during August 26\u201328, 2015, at the Institute for Research in Fundamental Sciences (IPM), Tehran, Iran.For this first edition of TTCS, we received 48 submissions from 12 different countries. An international Program Committee comprising 45 leading scientists from 14 countries reviewed the papers thoroughly providing on average five review reports for each paper. We ended up accepting 11 submissions, which translates into less than 23% of all submissions. This means that the process was highly selective and only very high quality papers were accepted.", "num_citations": "2\n", "authors": ["1520"]}
{"title": "Product line process theory\n", "abstract": " Software product lines (SPLs) facilitate reuse and customization in software development by genuinely addressing the concept of variability. Product Line Calculus of Communicating Systems (PL-CCS) is a process calculus for behavioral modeling of SPLs, in which variability can be explicitly modeled by a binary variant operator. In this paper, we study different notions of behavioral equivalence for PL-CCS, based on Park and Milner's strong bisimilarity. These notions enable reasoning about the behavior of SPLs at different levels of abstraction. We study the compositionality property of these notions and the mutual relationship among them. We further show how the strengths of these notions can be consolidated in an equational reasoning method. Finally, we designate the notions of behavioral equivalence that are characterized by the property specification language for PL-CCS, called multi-valued modal \u03bc\u00a0\u2026", "num_citations": "2\n", "authors": ["1520"]}
{"title": "Reconciling operational and epistemic approaches to the formal analysis of crypto-based security protocols\n", "abstract": " We propose a unifying framework for formal specification and verification of both epistemic and behavioral aspects of security protocols. The main novelty of the proposed framework is the explicit support for cryptographic constructs, which is among the most essential ingredients of security protocols. Due to this feature, the indistinguishability relation for the epistemic constructs gets a dynamic semantics by taking the communicated keys and cryptographic terms in the operational specification into account.", "num_citations": "2\n", "authors": ["1520"]}
{"title": "Checking and deriving module paths in Verilog cell library descriptions\n", "abstract": " Module paths are often used to specify the delays of cells in a Verilog cell library description, which define the propagation delay for an event from an input to an output. Specifying such paths manually is an error prone task; a forgotten path is interpreted as a zero delay, which can cause further flaws in the subsequent design steps. Moreover, one can specify superfluous module paths, i.e., module paths that can never occur in any practical run of the model and hence, make excessive restrictions on the subsequent design decision. This paper presents a method to check whether the given module paths are reflected in the functional implementation. Complementing this check, we also present a method to derive module paths from a functional description of a cell.", "num_citations": "2\n", "authors": ["1520"]}
{"title": "Decision Table-Based Testing\n", "abstract": " Decision Table-Based Testing Page 1 Decision Tables Classification Trees Conclusions Decision Table-Based Testing Mohammad Mousavi Eindhoven University of Technology, The Netherlands Software Testing, 2013 Mousavi: Decision Table-Based Testing Page 2 Decision Tables Classification Trees Conclusions Outline Decision Tables Classification Trees Conclusions Mousavi: Decision Table-Based Testing Page 3 Decision Tables Classification Trees Conclusions Idea \u25b6 Goal: Summarize the logic of the program (`a la Karnaugh maps) \u25b6 Find a few conditions on input determining the output behavior need not be independent relaxing the independence assumption in all previous techniques \u25b6 Determine the output actions for each combination of condition evaluations \u25b6 also called: cause-effect graph testing, or tableau testing Mousavi: Decision Table-Based Testing Page 4 Decision Tables Classification \u2026", "num_citations": "2\n", "authors": ["1520"]}
{"title": "On well-foundedness and expressiveness of promoted tyft: being promoted makes a difference\n", "abstract": " In this paper, we solve two open problems posed by Karen L. Bernstein regarding her promoted tyft format for structured operational semantics. We show that, unlike formats with closed terms as labels, such as the tyft format, the well-foundedness assumption cannot be dropped for the promoted tyft format while preserving the congruence result. We also show that the well-founded promoted tyft format is incomparable to the tyft format with closed terms as labels, i.e., there are transition relations that can be specified by the promoted tyft format but not by the tyft format, and vice versa.", "num_citations": "2\n", "authors": ["1520"]}
{"title": "Learning by sampling: learning behavioral family models from software product lines\n", "abstract": " Family-based behavioral analysis operates on a single specification artifact, referred to as family model, annotated with feature constraints to express behavioral variability in terms of conditional states and transitions. Family-based behavioral modeling paves the way for efficient model-based analysis of software product lines. Family-based behavioral model learning incorporates feature model analysis and model learning principles to efficiently unify product models into a family model and integrate the behavior of various products into a behavioral family model. Albeit reasonably effective, the exhaustive analysis of product lines is often infeasible due to the potentially exponential number of valid configurations. In this paper, we first present a family-based behavioral model learning techniques, called FFSM Diff. Subsequently, we report on our experience on learning family models by employing product sampling\u00a0\u2026", "num_citations": "1\n", "authors": ["1520"]}
{"title": "Topics in Theoretical Computer Science\n", "abstract": " Welcome to the Second IFIP International Conference on Topics in Theoretical Computer Science (TTCS 2017), held during September 12\u201314, 2017, at the School of Computer Science, Institute for Research in Fundamental Sciences (IPM), Tehran, Iran. This volume contains the papers accepted for presentation at TTCS 2017. For this edition of TTCS, we received 20 submissions from 10 different countries. An international Program Committee comprising 32 leading scientists from 13 countries reviewed the papers thoroughly providing on average four review reports for each paper. We accepted eight submissions, which translates into 40% of all submissions. This means that the process was selective and only high-quality papers were accepted. The program also includes four invited talks by the following world-renowned computer scientists:", "num_citations": "1\n", "authors": ["1520"]}
{"title": "AUTO-CAAS: Model-Based Fault Prediction and Diagnosis of Automotive Software\n", "abstract": " AUTO-CAAS: Model-Based Fault Prediction and Diagnosis of Automotive Software Page 1 . hh.se . . AUTO-CAAS: Model-Based Fault Prediction and Diagnosis of Automotive Software Wojciech Mostowski Halmstad University, Sweden AstaZero Researchers Day 2016 . Page 2 . hh.se . 2 . Outline 1 Project overview 2 Consortium 3 Model-based testing of AUTOSAR 4 Fault model learning 5 Status & next steps . Page 3 . hh.se . 3 . Motivation Automotive Open System Architecture \u2013 AUTOSAR To enable pluggable components and multiple vendors Room for interpretation and optimisation Intentional and inadvertent specification loopholes Specific implementations differ (from each other and from the specification) Results in non-conformant components Can lead to potentially serious problems in the software Research question \u2013 find the consequences . Page 4 . hh.se . 4 . Goals In the context of the AUTOSAR standard\u2026", "num_citations": "1\n", "authors": ["1520"]}
{"title": "Preface: Special section on foundations of coordination languages and software architectures (selected papers from FOCLASA'10)\n", "abstract": " Editorial: Preface: Special section on foundations of coordination languages and software architectures (selected papers from FOCLASA'10): Science of Computer Programming: Vol 80, No null ACM Digital Library home ACM home Google, Inc. (search) Advanced Search Browse About Sign in Register Advanced Search Journals Magazines Proceedings Books SIGs Conferences People More Search ACM Digital Library SearchSearch Advanced Search Science of Computer Programming Periodical Home Latest Issue Archive Authors Affiliations Award Winners More HomeBrowse by TitlePeriodicalsScience of Computer ProgrammingVol. Editorial: Preface: Special section on foundations of coordination languages and software architectures (selected papers from FOCLASA'10) article Editorial: Preface: Special section on foundations of coordination languages and software architectures (selected papers from \u2026", "num_citations": "1\n", "authors": ["1520"]}
{"title": "Basic manipulation of processes\n", "abstract": " This chapter contains sections titled: 9.1 Derivation rules for equations, 9.2 Derivation rules for formulas, 9.3 The sum operator, 9.4 The sum elimination lemma, 9.5 Induction for constructor sorts, 9.6 Recursive specification principle, 9.7 Koomen's fair abstraction rule, 9.8 Parallel expansion, 9.9 Historical notes", "num_citations": "1\n", "authors": ["1520"]}
{"title": "Decomposability in formal conformance testing\n", "abstract": " We study the problem of deriving a specification for a third-party component, based on the specification of the system and the environment in which the component is supposed to reside. Particularly, we are interested in using component specifications for conformance testing of black-box components, using the theory of input-output conformance (ioco) testing. We propose and prove sufficient criteria for decompositionality, i.e., that components conforming to the derived specification will always compose to produce a correct system with respect to the system specification. We also study the criteria for strong decomposability, by which we can ensure that only those components conforming to the derived specification can lead to a correct system.", "num_citations": "1\n", "authors": ["1520"]}
{"title": "Software specification\n", "abstract": " Software is a novel type of man-made product and its \u201cengineering\u201d is a relatively young discipline. Making software starts from requirements analysis, ie, eliciting and specifying the goals of the stake-holders and learning about the domain in which and about which the software is going to be developed. The requirements have to be put down in terms of models or specifications. These specifications provide a firm basis for both stake-holders and software developers. They will be massaged and transformed into an architectural description of the system and further into the detailed design and finally into the implementation. In other words, all artifacts throughout the process of software development evolve from the abstract model resulting from requirements analysis and are, themselves, models at different levels of abstraction. Software specification is about such models of software: how to write them, what they mean and when to use them. It is also about methods for analyzing models: comparison of models of the same kind, checking consistency among models of different kinds and reasoning about properties of models. Before we go into more details about our choice for models and their corresponding techniques, we would like to underscore the relevance of the notion of model in software engineering. In all mature engineering disciplines, before building a complex structure models of such structure are made. Many calculations and experiments are made on the produced models and only when the model is proven to meet the requirements, they are going to be turned into the real constructions. Software is by no means less complex than\u00a0\u2026", "num_citations": "1\n", "authors": ["1520"]}
{"title": "Formal verification of unreliable failure detectors in partially synchronous systems\n", "abstract": " We formally verify four algorithms proposed in [M. Larrea, S. Ar\u00e9valo and A. Fern\u00e1ndez, Efficient Algorithms to Implement Unreliable Failure Detectors in Partially Synchronous Systems, 1999]. Each algorithm is specified as a network of timed automata and is verified with respect to completeness and accuracy properties. Using the model-checking tool UP-PAAL, we detect and report the occurrences of deadlock (for all algorithms) between each pair of non-faulty nodes due to buffer overflow in communication channels with arbitrarily large buffers and we propose a solution. Moreover, we use one of the algorithms as a measure to compare three model-checking tools, namely, UPPAAL, mCRL2 and FDR2.", "num_citations": "1\n", "authors": ["1520"]}
{"title": "Long-run order-independence of vector-based transition systems\n", "abstract": " Semantics of many specification languages, particularly those used in the domain of hardware, is described in terms of vector-based transition systems. In such transition systems, each macro-step transition is labeled by a vector of inputs in which several inputs may change simultaneously. Each macro-step can thus be decomposed into a number of micro-steps, considering one input change at a time. This is akin to an interleaving semantics, where a concurrent step is represented by an interleaving of its constituting components. In this paper, the authors present abstract criteria on vector-based transition systems, which guarantee the next state computation to be independent of the execution order of micro-steps. If these abstract criteria are satisfied, then state-space generation or exploration algorithms only need to consider one representative among all possible permutations of micro-steps. For most practical\u00a0\u2026", "num_citations": "1\n", "authors": ["1520"]}
{"title": "Symbolic power analysis of cell libraries\n", "abstract": " Cell libraries are collections of logic cores (cells) used to construct larger chip designs; hence, any reduction in their power consumption may have a major impact in the power consumption of larger designs. The power consumption of a cell is often determined by triggering it with all possible input values in all possible orders at each state. In this paper, we first present a technique to measure the power consumption of a cell more efficiently by reducing the number of input orders that have to be checked. This is based on symbolic techniques and analyzes the number of (weighted) wire chargings taking place. Additionally, we present a technique that computes for a cell all orders that lead to the same state, but differ in their power consumption. Such an analysis is used to select the orders that minimize the required power, without affecting functionality, by inserting sufficient delays. Both techniques have been\u00a0\u2026", "num_citations": "1\n", "authors": ["1520"]}
{"title": "Robustness of behavioral equivalence on open terms\n", "abstract": " Sound behavioral equations on open terms may become unsound after conservative extensions of the underlying operational semantics. Providing criteria under which such equations are preserved is extremely useful; in particular, it can avoid the need to repeat proofs when extending the specified language.This paper investigates preservation of sound equations for several notions of bisimilarity on open terms: closed-instance (ci-) bisimilarity and formal-hypothesis (fh-) bisimilarity, both due to Robert de Simone, and hypothesis-preserving (hp-) bisimilarity, due to Arend Rensink. For both fh-bisimilarity and hp-bisimilarity, we prove that arbitrary sound equations on open terms are preserved by all disjoint extensions which do not add labels. We also define slight variations of fh-and hp-bisimilarity such that all sound equations are preserved by arbitrary disjoint extensions. Finally, we give two sets of syntactic criteria (on equations, resp. operational extensions) and prove each of them to be sufficient for preserving ci-bisimilarity.", "num_citations": "1\n", "authors": ["1520"]}
{"title": "Efficient Verification of Verilog Cell Libraries\n", "abstract": " Functional descriptions of cells in a subset of Verilog, called VeriCell and consisting of Ternary Constants T={0, 1, X} Variables, eg, ck, d,... Built-in primitives, eg, not, and,... User Defined Primitives (UDPs) A module instantiating a number of primitives, thereby defining the cell Example (D Flip-Flop with Active Low Enable) module dff_enb (q, d, ck, enb); output q; input d, ck, enb; not (en, enb); dff_en (q, d, ck, en); endmodule", "num_citations": "1\n", "authors": ["1520"]}
{"title": "Analytical software design: introduction and industrial experience report\n", "abstract": " Analytical Software Design (ASD) is a design approach that combines formal and empirical methods for developing mathematically verified software systems. Unlike conventional design methods, the design phase is extended with more formal techniques, so that flaws are detected earlier, thereby reducing the time needed for coding, testing, and integration. In this paper, we demonstrate formal details and concepts behind the ASD approach, report about our experience with applying ASD in industrial control applications within Philips Healthcare, and discuss results and findings gathered during this work as well as some commonly faced issues and their practical solutions.", "num_citations": "1\n", "authors": ["1520"]}
{"title": "Reconstruction and verification of group membership protocols\n", "abstract": " In this paper, we present a process-algebraic specification of group membership protocols specified in [Y. Amir, D. Dolev, S. Kramer and D. Malki, Membership Algorithms for Multicast Communication Groups, Springer-Verlag, 1992]. In order to formalise the protocol and its properties we disambiguate the informal specification provided by the paper. This requires trying different possible interpretations in the formal model and checking the consistency of the assumption and formally verifying the correctness properties. We thus present a formal reconstruction of the membership algorithms and model-check our reconstruction.", "num_citations": "1\n", "authors": ["1520"]}
{"title": "Towards sos meta-theory for language-based security\n", "abstract": " SOS meta-theory has been very successful in defining meta-theorems using which one can prove useful properties about language constructs. These meta-theorems can save pages of standard proof thanks to their generic and language-independent formulation. Security properties of language constructs look like promising candidates to be turned into SOS meta-theorems and there has already been an attempt in this direction in the context of process calculi security.In this paper, we give an exploratory account of this issue in the context of language-based security. To do this, we give a superficial overview of information-flow security and in particular, non-interference as a central notion in this field. Then, we point out some interesting links between non-interference and our recent work on notions of bisimulation with data. Finally, some ideas regarding SOS meta-theorems for these notions are presented.", "num_citations": "1\n", "authors": ["1520"]}
{"title": "Structural congruences and structural operational semantics\n", "abstract": " Structural congruences have been used to define the semantics and to capture inherent properties of language constructs. They have been used as an addendum to transition system specifications in Plotkin\u2019s style of Structural Operational Semantics (SOS). However, there has been little theoretical work on establishing a formal link between theses two semantic specification frameworks. In this paper, we try to fill this gap by accommodating structural congruences inside transition system specifications. The Contributions of this paper can be summarized as follows:", "num_citations": "1\n", "authors": ["1520"]}