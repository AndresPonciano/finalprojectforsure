{"title": "A touring machine: Prototyping 3D mobile augmented reality systems for exploring the urban environment\n", "abstract": " We describe a prototype system that combines the overlaid 3D graphics of augmented reality with the untethered freedom of mobile computing. The goal is to explore how these two technologies might together make possible wearable computer systems that can support users in their everyday interactions with the world. We introduce an application that presents information about our university's campus, using a head-tracked, see-through, head-worn, 3D display, and an untracked, opaque, hand-held, 2D display with stylus and trackpad. We provide an illustrated explanation of how our prototype is used, and describe our rationale behind designing its software infrastructure and selecting the hardware on which it runs.", "num_citations": "1649\n", "authors": ["1194"]}
{"title": "Mobile augmented reality\n", "abstract": " As computers increase in power and decrease in size, new mobile, wearable, and pervasive computing applications are rapidly becoming feasible, providing people access to online resources always and everywhere. This new flexibility makes possible new kind of applications that exploit the person's surrounding context. Augmented reality (AR) presents a particularly powerful user interface (UI) to context-aware computing environments. AR systems integrate virtual information into a person's physical environment so that he or she will perceive that information as existing in their surroundings. Mobile augmented reality systems (MARS) provide this service without constraining the individual\u2019s whereabouts to a specially equipped area. Ideally, they work virtually anywhere, adding a palpable layer of information to any environment whenever desired. By doing so, they hold the potential to revolutionize the way in which information is presented to people. Computer-presented material is directly integrated with the real world surrounding the freely roaming person, who can interact with it to display related information, to pose and resolve queries, and to collaborate with other people. The world becomes the user interface.This chapter provides a detailed introduction to mobile AR technology with in-depth reviews of important topics, such as wearable display and computing hardware, tracking, registration, user interaction, heterogeneous UIs, collaboration, and UI management for situated computing. As part of this introduction, we define what we mean by augmented reality, give a brief overview of the history of the field in general, and review some\u00a0\u2026", "num_citations": "599\n", "authors": ["1194"]}
{"title": "Evaluation of interest point detectors and feature descriptors for visual tracking\n", "abstract": " Applications for real-time visual tracking can be found in many areas, including visual odometry and augmented reality. Interest point detection and feature description form the basis of feature-based tracking, and a variety of algorithms for these tasks have been proposed. In this work, we present (1) a carefully designed dataset of video sequences of planar textures with ground truth, which includes various geometric changes, lighting conditions, and levels of motion blur, and which may serve as a testbed for a variety of tracking-related problems, and (2) a comprehensive quantitative evaluation of detector-descriptor-based visual camera tracking based on this testbed. We evaluate the impact of individual algorithm parameters, compare algorithms for both detection and description in isolation, as well as all detector-descriptor combinations as a tracking solution. In contrast to existing evaluations, which aim\u00a0\u2026", "num_citations": "525\n", "authors": ["1194"]}
{"title": "Augmented reality: principles and practice\n", "abstract": " Today\u2019s Comprehensive and Authoritative Guide to Augmented Reality By overlaying computer-generated information on the real world, augmented reality (AR) amplifies human perception and cognition in remarkable ways. Working in this fast-growing field requires knowledge of multiple disciplines, including computer vision, computer graphics, and human-computer interaction. Augmented Reality: Principles and Practice integrates all this knowledge into a single-source reference, presenting today\u2019s most significant work with scrupulous accuracy. Pioneering researchers Dieter Schmalstieg and Tobias H\u00f6llerer carefully balance principles and practice, illuminating AR from technical, methodological, and user perspectives. Coverage includes Displays: head-mounted, handheld, projective, auditory, and haptic Tracking/sensing, including physical principles, sensor fusion, and real-time computer vision Calibration/registration, ensuring repeatable, accurate, coherent behavior Seamless blending of real and virtual objects Visualization to enhance intuitive understanding Interaction\u2013from situated browsing to full 3D interaction Modeling new geometric content Authoring AR presentations and databases Architecting AR systems with real-time, multimedia, and distributed elements This guide is indispensable for anyone interested in AR, including developers, engineers, students, instructors, researchers, and serious hobbyists.", "num_citations": "478\n", "authors": ["1194"]}
{"title": "View management for virtual and augmented reality\n", "abstract": " We describe a view-management component for interactive 3D user interfaces. By view management, we mean maintaining visual constraints on the projections of objects on the view plane, such as locating related objects near each other, or preventing objects from occluding each other. Our view-management component accomplishes this by modifying selected object properties, including position, size, and transparency, which are tagged to indicate their constraints. For example, some objects may have geometric properties that are determined entirely by a physical simulation and which cannot be modified, while other objects may be annotations whose position and size are flexible. We introduce algorithms that use upright rectangular extents to represent on the view plane a dynamic and efficient approximation of the occupied space containing the projections of visible portions of 3D objects, as well as the\u00a0\u2026", "num_citations": "450\n", "authors": ["1194"]}
{"title": "Handy AR: Markerless inspection of augmented reality objects using fingertip tracking\n", "abstract": " We present markerless camera tracking and user interface methodology for readily inspecting augmented reality (AR) objects in wearable computing applications. Instead of a marker, we use the human hand as a distinctive pattern that almost all wearable computer users have readily available. We present a robust real-time algorithm that recognizes fingertips to reconstruct the six-degree-of-freedom camera pose relative to the user's outstretched hand. A hand pose model is constructed in a one-time calibration step by measuring the fingertip positions in presence of ground-truth scale information. Through frame-by-frame reconstruction of the camera pose relative to the hand, we can stabilize 3D graphics annotations on top of the hand, allowing the user to inspect such virtual objects conveniently from different viewing angles in AR. We evaluate our approach with regard to speed and accuracy, and compare it to\u00a0\u2026", "num_citations": "349\n", "authors": ["1194"]}
{"title": "Information filtering for mobile augmented reality\n", "abstract": " Augmented reality is a potentially powerful paradigm for annotating the (real) environment with computer-generated material. These benefits will be even greater when augmented reality systems become mobile and wearable. However, to minimize the problem of clutter and to maximize the effectiveness of the display, algorithms must be developed to select only the most important information for the user. In this paper, we describe a region-based information filtering algorithm. The algorithm takes account of the state of the user (location and intent) and the state of individual objects about which information can be presented. It can dynamically respond to changes in the environment and the user's state. We also describe how simple temporal, distance and angle cues can be used to refine the transitions between different information sets.", "num_citations": "323\n", "authors": ["1194"]}
{"title": "Situated documentaries: Embedding multimedia presentations in the real world\n", "abstract": " We describe an experimental wearable augmented reality system that enables users to experience hypermedia presentations that are integrated with the actual outdoor locations to which they are relevant. Our mobile prototype uses a tracked see-through head-worn display to overlay 3D graphics, imagery, and sound on top of the real world, and presents additional, coordinated material on a hand-held pen computer. We have used these facilities to create several situated documentaries that tell the stories of events that took place on our campus. We describe the software and hardware that underly our prototype system and explain the user interface that we have developed for it.", "num_citations": "318\n", "authors": ["1194"]}
{"title": "TasteWeights: a visual interactive hybrid recommender system\n", "abstract": " This paper presents an interactive hybrid recommendation system that generates item predictions from multiple social and semantic web resources, such as Wikipedia, Facebook, and Twitter. The system employs hybrid techniques from traditional recommender system literature, in addition to a novel interactive interface which serves to explain the recommendation process and elicit preferences from the end user. We present an evaluation that compares different interactive and non-interactive hybrid strategies for computing recommendations across diverse social and semantic web APIs. Results of the study indicate that explanation and interaction with a visual representation of the hybrid system increase user satisfaction and relevance of predicted content.", "num_citations": "282\n", "authors": ["1194"]}
{"title": "Topicnets: Visual analysis of large text corpora with topic modeling\n", "abstract": " We present TopicNets, a Web-based system for visual and interactive analysis of large sets of documents using statistical topic models. A range of visualization types and control mechanisms to support knowledge discovery are presented. These include corpus- and document-specific views, iterative topic modeling, search, and visual filtering. Drill-down functionality is provided to allow analysts to visualize individual document sections and their relations within the global topic space. Analysts can search across a dataset through a set of expansion techniques on selected document and topic nodes. Furthermore, analysts can select relevant subsets of documents and perform real-time topic modeling on these subsets to interactively visualize topics at various levels of granularity, allowing for a better understanding of the documents. A discussion of the design and implementation choices for each visual analysis\u00a0\u2026", "num_citations": "212\n", "authors": ["1194"]}
{"title": "Enveloping users and computers in a collaborative 3D augmented reality\n", "abstract": " We present EMMIE (Environment Management for Multiuser Information Environments), a prototype experimental user interface to a collaborative augmented environment. Users share a 3D virtual space and manipulate virtual objects that represent information to be discussed. We refer to EMMIE as a hybrid user interface because it combines a variety of different technologies and techniques, including virtual elements such as 3D widgets, and physical objects such as tracked displays and input devices. See-through head-worn displays overlay the virtual environment on the physical environment, visualizing the pervasive \"virtual ether\" within which all interaction occurs. Our prototype includes additional 2D and 3D displays, ranging from palm-sized to wall-sized, allowing the most appropriate one to be used for any task. Objects can be moved among displays (including across dimensionalities) through drag-and\u00a0\u2026", "num_citations": "211\n", "authors": ["1194"]}
{"title": "User interface management techniques for collaborative mobile augmented reality\n", "abstract": " Mobile augmented reality systems (MARS) have the potential to revolutionize the way in which information is provided to users. Virtual information can be directly integrated with the real world surrounding the mobile user, who can interact with it to display related information, to pose and resolve queries, and to collaborate with other users. However, we believe that the benefits of MARS will only be achieved if the user interface (UI) is actively managed so as to maximize the relevance and minimize the confusion of the virtual material relative to the real world. This article addresses some of the steps involved in this process, focusing on the design and layout of the mobile user's overlaid virtual environment.The augmented view of the user's surroundings presents an interface to context-dependent operations, many of which are related to the objects in view\u2014the augmented world is the user interface. We present three\u00a0\u2026", "num_citations": "185\n", "authors": ["1194"]}
{"title": "World-stabilized annotations and virtual scene navigation for remote collaboration\n", "abstract": " We present a system that supports an augmented shared visual space for live mobile remote collaboration on physical tasks. The remote user can explore the scene independently of the local user's current camera position and can communicate via spatial annotations that are immediately visible to the local user in augmented reality. Our system operates on off-the-shelf hardware and uses real-time visual tracking and modeling, thus not requiring any preparation or instrumentation of the environment. It creates a synergy between video conferencing and remote scene exploration under a unique coherent interface. To evaluate the collaboration with our system, we conducted an extensive outdoor user study with 60 participants comparing our system with two baseline interfaces. Our results indicate an overwhelming user preference (80%) for our system, a high level of usability, as well as performance benefits\u00a0\u2026", "num_citations": "178\n", "authors": ["1194"]}
{"title": "Resolving multiple occluded layers in augmented reality\n", "abstract": " A useful function of augmented reality (AR) systems is their ability to visualize occluded infrastructure directly in a user's view of the environment. This is especially important for our application context, which utilizes mobile AR for navigation and other operations in an urban environment. A key problem in the AR field is how to best depict occluded objects in such a way that the viewer can correctly infer the depth relationships between different physical and virtual objects. Showing a single occluded object with no depth context presents an ambiguous picture to the user. But showing all occluded objects in the environments leads to the \"Superman's X-ray vision\" problem, in which the user sees too much information to make sense of the depth relationships of objects. Our efforts differ qualitatively from previous work in AR occlusion, because our application domain involves far-field occluded objects, which are tens of\u00a0\u2026", "num_citations": "171\n", "authors": ["1194"]}
{"title": "Annotation in outdoor augmented reality\n", "abstract": " Annotation, the process of adding extra virtual information to an object, is one of the most common uses for augmented reality. Although annotation is widely used in augmented reality, there is no general agreed-upon definition of what precisely constitutes an annotation in this context. In this paper, we propose a taxonomy of annotation, describing what constitutes an annotation and outlining different dimensions along which annotation can vary. Using this taxonomy we also highlight what styles of annotation are used in different types of applications and areas where further work needs to be done to improve annotation.Through our taxonomy we found two primary categories into which annotations in current applications fall. Some annotations present information that is directly related to the object they are annotating, while others are only indirectly related to the object that is being annotated. We also found that\u00a0\u2026", "num_citations": "152\n", "authors": ["1194"]}
{"title": "Peerchooser: visual interactive recommendation\n", "abstract": " Collaborative filtering (CF) has been successfully deployed over the years to compute predictions on items based on a user's correlation with a set of peers. The black-box nature of most CF applications leave the user wondering how the system arrived at its recommendation. This note introduces PeerChooser, a collaborative recommender system with an interactive graphical explanation interface. Users are provided with a visual explanation of the CF process and opportunity to manipulate their neighborhood at varying levels of granularity to reflect aspects of their current requirements. In this manner we overcome the problem of redundant profile information in CF systems, in addition to providing an explanation interface. Our layout algorithm produces an exact, noiseless graph representation of the underlying correlations between users. PeerChooser's prediction component uses this graph directly to yield the\u00a0\u2026", "num_citations": "150\n", "authors": ["1194"]}
{"title": "Interactive tools for virtual x-ray vision in mobile augmented reality\n", "abstract": " This paper presents a set of interactive tools designed to give users virtual x-ray vision. These tools address a common problem in depicting occluded infrastructure: either too much information is displayed, confusing users, or too little information is displayed, depriving users of important depth cues. Four tools are presented: the tunnel tool and room selector tool directly augment the user's view of the environment, allowing them to explore the scene in direct, first person view. The room in miniature tool allows the user to select and interact with a room from a third person perspective, allowing users to view the contents of the room from points of view that would normally be difficult or impossible to achieve. The room slicer tool aids users in exploring volumetric data displayed within the room in miniature tool. Used together, the tools presented in this paper can be used to achieve the virtual x-ray vision effect. We test\u00a0\u2026", "num_citations": "141\n", "authors": ["1194"]}
{"title": "I\u2019m feeling LoCo: A Location Based Context Aware Recommendation System\n", "abstract": " Research in ubiquitous location recommendation systems has focused on automatically inferring a user\u2019s preferences while little attention has been devoted to the recommendation algorithms. Location recommendation systems with a focus on recommendation algorithms generally require the user to complete complicated and time consuming surveys and rarely consider the user\u2019s current context. The purpose of this investigation is to design a more complete ubiquitous location based recommendation algorithm that by inferring user\u2019s preferences and considering time geography and similarity measurements automatically, betters the user experience. Our system learns user preferences by mining a person\u2019s social network profile. The physical constraints are delimited by a user\u2019s location, and form of transportation, which is automatically detected through the use of a decision tree followed by a discrete\u00a0\u2026", "num_citations": "135\n", "authors": ["1194"]}
{"title": "Smallworlds: Visualizing social recommendations\n", "abstract": " We present SmallWorlds, a visual interactive graph\u2010based interface that allows users to specify, refine and build item\u2010preference profiles in a variety of domains. The interface facilitates expressions of taste through simple graph interactions and these preferences are used to compute personalized, fully transparent item recommendations for a target user. Predictions are based on a collaborative analysis of preference data from a user's direct peer group on a social network. We find that in addition to receiving transparent and accurate item recommendations, users also learn a wealth of information about the preferences of their peers through interaction with our visualization. Such information is not easily discoverable in traditional text based interfaces. A detailed analysis of our design choices for visual layout, interaction and prediction techniques is presented. Our evaluations discuss results from a user study in\u00a0\u2026", "num_citations": "130\n", "authors": ["1194"]}
{"title": "ARWin-a desktop augmented reality window manager\n", "abstract": " We present ARWin, a single user 3D augmented reality desktop. We explain our design considerations and system architecture and discuss a variety of applications and interaction techniques designed to take advantage of this new platform.", "num_citations": "129\n", "authors": ["1194"]}
{"title": "Credibility in context: An analysis of feature distributions in twitter\n", "abstract": " Twitter is a major forum for rapid dissemination of user-provided content in real time. As such, a large proportion of the information it contains is not particularly relevant to many users and in fact is perceived as unwanted 'noise' by many. There has been increased research interest in predicting whether tweets are relevant, newsworthy or credible, using a variety of models and methods. In this paper, we focus on an analysis that highlights the utility of the individual features in Twitter such as hash tags, retweets and mentions for predicting credibility. We first describe a context-based evaluation of the utility of a set of features for predicting manually provided credibility assessments on a corpus of microblog tweets. This is followed by an evaluation of the distribution/presence of each feature across 8 diverse crawls of tweet data. Last, an analysis of feature distribution across dyadic pairs of tweets and retweet chains of\u00a0\u2026", "num_citations": "124\n", "authors": ["1194"]}
{"title": "Botivist: Calling volunteers to action using online bots\n", "abstract": " To help activists call new volunteers to action, we present Botivist: a platform that uses Twitter bots to find potential volunteers and request contributions. By leveraging different Twitter accounts, Botivist employs different strategies to encourage participation. We explore how people respond to bots calling them to action using a test case about corruption in Latin America. Our results show that the majority of volunteers (80\\%) who responded to Botivist's calls to action contributed relevant proposals to address the assigned social problem. Different strategies produced differences in the quantity and relevance of contributions. Some strategies that work well offline and face-to-face appeared to hinder people's participation when used by an online bot. We analyze user behavior in response to being approached by bots with an activist purpose. We also provide strong evidence for the value of this type of civic media, and\u00a0\u2026", "num_citations": "123\n", "authors": ["1194"]}
{"title": "Integrating the physical environment into mobile remote collaboration\n", "abstract": " We describe a framework and prototype implementation for unobtrusive mobile remote collaboration on tasks that involve the physical environment. Our system uses the Augmented Reality paradigm and model-free, markerless visual tracking to facilitate decoupled, live updated views of the environment and world-stabilized annotations while supporting a moving camera and unknown, unprepared environments. In order to evaluate our concept and prototype, we conducted a user study with 48 participants in which a remote expert instructed a local user to operate a mock-up airplane cockpit. Users performed significantly better with our prototype (40.8 tasks completed on average) as well as with static annotations (37.3) than without annotations (28.9). 79% of the users preferred our prototype despite noticeably imperfect tracking.", "num_citations": "121\n", "authors": ["1194"]}
{"title": "Modeling topic specific credibility on twitter\n", "abstract": " This paper presents and evaluates three computational models for recommending credible topic-specific information in Twitter. The first model focuses on credibility at the user level, harnessing various dynamics of information flow in the underlying social graph to compute a rating. The second model applies a content-based strategy to compute a finer-grained credibility score for individual tweets. Lastly, we discuss a third model which combines facets from both models in a hybrid method, using both averaging and filtering hybrid strategies. To evaluate our novel credibility models, we perform an evaluation on 7 topic specific data sets mined from the Twitter streaming API, with specific focus on a data set of 37K users who tweeted about the topic\" Libya\". Results show that the social model outperfoms hybrid and content-based prediction models in terms of predictive accuracy over a set of manually collected\u00a0\u2026", "num_citations": "118\n", "authors": ["1194"]}
{"title": "Hybrid feature tracking and user interaction for markerless augmented reality\n", "abstract": " We describe a novel markerless camera tracking approach and user interaction methodology for augmented reality (AR) on unprepared tabletop environments. We propose a real-time system architecture that combines two types of feature tracking methods. Distinctive image features of the scene are detected and tracked frame- to-frame by computing optical flow. In order to achieve real-time performance, multiple operations are processed in a multi-threaded manner for capturing a video frame, tracking features using optical flow, detecting distinctive invariant features, and rendering an output frame. We also introduce a user interaction for establishing a global coordinate system and for locating virtual objects in the AR environment. A user's bare hand is used for the user interface by estimating a camera pose relative to the user's outstretched hand. We evaluate the speed and accuracy of our hybrid feature tracking\u00a0\u2026", "num_citations": "117\n", "authors": ["1194"]}
{"title": "In touch with the remote world: Remote collaboration with augmented reality drawings and virtual navigation\n", "abstract": " Augmented reality annotations and virtual scene navigation add new dimensions to remote collaboration. In this paper, we present a touchscreen interface for creating freehand drawings as world-stabilized annotations and for virtually navigating a scene reconstructed live in 3D, all in the context of live remote collaboration. Two main focuses of this work are (1) automatically inferring depth for 2D drawings in 3D space, for which we evaluate four possible alternatives, and (2) gesture-based virtual navigation designed specifically to incorporate constraints arising from partially modeled remote scenes. We evaluate these elements via qualitative user studies, which in addition provide insights regarding the design of individual visual feedback elements and the need to visualize the direction of drawings.", "num_citations": "116\n", "authors": ["1194"]}
{"title": "Multithreaded hybrid feature tracking for markerless augmented reality\n", "abstract": " We describe a novel markerless camera tracking approach and user interaction methodology for augmented reality (AR) on unprepared tabletop environments. We propose a real-time system architecture that combines two types of feature tracking. Distinctive image features of the scene are detected and tracked frame-to-frame by computing optical flow. In order to achieve real-time performance, multiple operations are processed in a synchronized multi-threaded manner: capturing a video frame, tracking features using optical flow, detecting distinctive invariant features, and rendering an output frame. We also introduce user interaction methodology for establishing a global coordinate system and for placing virtual objects in the AR environment by tracking a user's outstretched hand and estimating a camera pose relative to it. We evaluate the speed and accuracy of our hybrid feature tracking approach, and\u00a0\u2026", "num_citations": "112\n", "authors": ["1194"]}
{"title": "An annotated situation-awareness aid for augmented reality\n", "abstract": " We present a situation-awareness aid for augmented reality systems based on an annotated\" world in miniature.\" Our aid is designed to provide users with an overview of their environment that allows them to select and inquire about the objects that it contains. Two key capabilities are discussed that are intended to address the needs of mobile users. The aid's position, scale, and orientation are controlled by a novel approach that allows the user to inspect the aid without the need for manual interaction. As the user alternates their attention between the physical world and virtual aid, popup annotations associated with selected objects can move freely between the objects' representations in the two models.", "num_citations": "112\n", "authors": ["1194"]}
{"title": "Optimizing the viewing graph for structure-from-motion\n", "abstract": " The viewing graph represents a set of views that are related by pairwise relative geometries. In the context of Structure-from-Motion (SfM), the viewing graph is the input to the incremental or global estimation pipeline. Much effort has been put towards developing robust algorithms to overcome potentially inaccurate relative geometries in the viewing graph during SfM. In this paper, we take a fundamentally different approach to SfM and instead focus on improving the quality of the viewing graph before applying SfM. Our main contribution is a novel optimization that improves the quality of the relative geometries in the viewing graph by enforcing loop consistency constraints with the epipolar point transfer. We show that this optimization greatly improves the accuracy of relative poses in the viewing graph and removes the need for filtering steps or robust algorithms typically used in global SfM methods. In addition, the optimized viewing graph can be used to efficiently calibrate cameras at scale. We combine our viewing graph optimization and focal length calibration into a global SfM pipeline that is more efficient than existing approaches. To our knowledge, ours is the first global SfM pipeline capable of handling uncalibrated image sets.", "num_citations": "103\n", "authors": ["1194"]}
{"title": "Level of detail interfaces\n", "abstract": " We present the level of detail interface based on the marriage of level of detail geometry and an adaptable user interface. Level of detail interfaces allow applications to paramaterize their display of data and interface widgets with respect to distance from the camera, to best take advantage of diminished screen space in a 3D environment.", "num_citations": "103\n", "authors": ["1194"]}
{"title": "Vision-based interfaces for mobility\n", "abstract": " Vision-based user interfaces are a feasible and advantageous modality for wearable computers. To substantiate this claim, we present a robust real-time hand gesture recognition system that is capable of being the sole input provider for a demonstration application. It achieves usability and interactivity even when both the head-worn camera and the object of interest are in motion. We describe a set of general gesture-based interaction styles and explore their characteristics in terms of task suitability and the computer vision algorithms required for their recognition. Preliminary evaluation of our prototype system leads to the conclusion that vision-based interfaces have achieved the maturity necessary to help overcome some limitations of more traditional mobile user interfaces.", "num_citations": "102\n", "authors": ["1194"]}
{"title": "Methods, apparatus and data structures for providing a user interface which facilitates decision making\n", "abstract": " A user interface for facilitating a decision making process, such as planning a trip. A unified view of various types of information related to an event may be provided. The unified view may be presented in a simulated three-dimensional environment having different types of information depicted on different windows. Different types of information related to a common event may be visually linked. A window showing a articular type of information may be brought into a focus view for closer inspection by a user. Alternative decisions may be depicted to permit vagueness or uncertainty, particularly at early iterations in the decision making process.", "num_citations": "96\n", "authors": ["1194"]}
{"title": "iVisDesigner: Expressive interactive design of information visualizations\n", "abstract": " We present the design, implementation and evaluation of iVisDesigner, a web-based system that enables users to design information visualizations for complex datasets interactively, without the need for textual programming. Our system achieves high interactive expressiveness through conceptual modularity, covering a broad information visualization design space. iVisDesigner supports the interactive design of interactive visualizations, such as provisioning for responsive graph layouts and different types of brushing and linking interactions. We present the system design and implementation, exemplify it through a variety of illustrative visualization designs and discuss its limitations. A performance analysis and an informal user study are presented to evaluate the system.", "num_citations": "95\n", "authors": ["1194"]}
{"title": "Stereoscopic highlighting: 2d graph visualization on stereo displays\n", "abstract": " In this paper we present a new technique and prototype graph visualization system, stereoscopic highlighting, to help answer accessibility and adjacency queries when interacting with a node-link diagram. Our technique utilizes stereoscopic depth to highlight regions of interest in a 2D graph by projecting these parts onto a plane closer to the viewpoint of the user. This technique aims to isolate and magnify specific portions of the graph that need to be explored in detail without resorting to other highlighting techniques like color or motion, which can then be reserved to encode other data attributes. This mechanism of stereoscopic highlighting also enables focus+context views by juxtaposing a detailed image of a region of interest with the overall graph, which is visualized at a further depth with correspondingly less detail. In order to validate our technique, we ran a controlled experiment with 16 subjects comparing\u00a0\u2026", "num_citations": "87\n", "authors": ["1194"]}
{"title": "Negotiation for automated generation of temporal multimedia presentations\n", "abstract": " Creating high-quality multimedia presentations requires much skill, time, and effort. This is particularly true when temporal media, such as speech and animation, are involved. We describe the design and implementation of a knowledge-based system that generates customized temporal multimedia presentations. We provide art overview of the system\u2019s architecture, and explain how speech, written text, and graphics are generated and coordinated. Our emphasis is on how temporal media are coordinated by the system through amulti-stage negotiation process. In negotiation, media-specific generation components interact with a novel coordination component that solves temporal constraints provided by the generators. We illustrate our work with a set of examples generated by the system in a testbed application intended to update hospital caregivers on the status of patients who have undergone a cardiac bypass\u00a0\u2026", "num_citations": "87\n", "authors": ["1194"]}
{"title": "The effects of visual realism on search tasks in mixed reality simulation\n", "abstract": " In this paper, we investigate the validity of Mixed Reality (MR) Simulation by conducting an experiment studying the effects of the visual realism of the simulated environment on various search tasks in Augmented Reality (AR). MR Simulation is a practical approach to conducting controlled and repeatable user experiments in MR, including AR. This approach uses a high-fidelity Virtual Reality (VR) display system to simulate a wide range of equal or lower fidelity displays from the MR continuum, for the express purpose of conducting user experiments. For the experiment, we created three virtual models of a real-world location, each with a different perceived level of visual realism. We designed and executed an AR experiment using the real-world location and repeated the experiment within VR using the three virtual models we created. The experiment looked into how fast users could search for both physical and\u00a0\u2026", "num_citations": "86\n", "authors": ["1194"]}
{"title": "Wide-area scene mapping for mobile visual tracking\n", "abstract": " We propose a system for easily preparing arbitrary wide-area environments for subsequent real-time tracking with a handheld device. Our system evaluation shows that minimal user effort is required to initialize a camera tracking session in an unprepared environment. We combine panoramas captured using a handheld omnidirectional camera from several viewpoints to create a point cloud model. After the offline modeling step, live camera pose tracking is initialized by feature point matching, and continuously updated by aligning the point cloud model to the camera image. Given a reconstruction made with less than five minutes of video, we achieve below 25 cm translational error and 0.5 degrees rotational error for over 80% of images tested. In contrast to camera-based simultaneous localization and mapping (SLAM) systems, our methods are suitable for handheld use in large outdoor spaces.", "num_citations": "85\n", "authors": ["1194"]}
{"title": "Multimodal Interaction with a Wearable Augmented Reality System\n", "abstract": " An augmented reality system enhances a mobile user's situational awareness and provides new visualization functionality. The custom-built multimodal interface provides access to information encountered in urban environments. In this article, we detail our experiences with various input devices and modalities and discuss their advantages and drawbacks in the context of interaction tasks in mobile computing. We show how we integrated the input channels to use the modalities beneficially and how this enhances the interface's overall usability", "num_citations": "83\n", "authors": ["1194"]}
{"title": "Pictorial depth cues for outdoor augmented reality\n", "abstract": " This paper presents and evaluates a set of pictorial depth cues for far-field outdoor mobile augmented reality (AR). We examine the problem of accurately placing virtual annotations at physical target points from a static point of view. While it is easy to line up annotations with a target point's projection in the view plane, finding the correct distance for the annotation is difficult if the target point is not represented in an environment model. We have found that AR depth cues, such as vertical and horizontal shadow planes, a small top-down map, or color encodings of relative depth, have a positive impact on a user's ability to align a 3D cursor with physical objects at various distances. These cues aid the user's depth perception and estimation by providing information about the 3D cursor's distance and its relationship in 3-space to any features that may already have been annotated. We conducted a user study that\u00a0\u2026", "num_citations": "78\n", "authors": ["1194"]}
{"title": "Chartaccent: Annotation for data-driven storytelling\n", "abstract": " Annotation plays an important role in conveying key points in visual data-driven storytelling; it helps presenters explain and emphasize core messages and specific data. However, the visualization research community has a limited understanding of annotation and its role in data-driven storytelling, and existing charting software provides limited support for creating annotations. In this paper, we characterize a design space of chart annotations, one informed by a survey of 106 annotated charts published by six prominent news graphics desks. Using this design space, we designed and developed ChartAccent, a tool that allows people to quickly and easily augment charts via a palette of annotation interactions that generate manual and data-driven annotations. We also report on a study in which participants reproduced a series of annotated charts using ChartAccent, beginning with unadorned versions of the same\u00a0\u2026", "num_citations": "75\n", "authors": ["1194"]}
{"title": "Simulation of augmented reality systems in purely virtual environments\n", "abstract": " We propose the use of virtual environments to simulate augmented reality (AR) systems for the purposes of experimentation and usability evaluation. This method allows complete control in the AR environment, providing many advantages over testing with true AR systems. We also discuss some of the limitations to the simulation approach. We have demonstrated the use of such a simulation in a proof of concept experiment controlling the levels of registration error in the AR scenario. In this experiment, we used the simulation method to investigate the effects of registration error on task performance for a generic task involving precise motor control for AR object manipulation. Isolating jitter and latency errors, we provide empirical evidence of the relationship between accurate registration and task performance.", "num_citations": "71\n", "authors": ["1194"]}
{"title": "A hand-held AR magic lens with user-perspective rendering\n", "abstract": " In this paper we present a user study evaluating the benefits of geometrically correct user-perspective rendering using an Augmented Reality (AR) magic lens. In simulation we compared a user-perspective magic lens against the common device-perspective magic lens on both phone-sized and tablet-sized displays. Our results indicate that a tablet-sized display allows for significantly faster performance of a selection task and that a user-perspective lens has benefits over a device-perspective lens for a selection task. Based on these promising results, we created a proof-of-concept prototype, engineered with current off-the-shelf devices and software. To our knowledge, this is the first geometrically correct user-perspective magic lens.", "num_citations": "69\n", "authors": ["1194"]}
{"title": "The interactive fogscreen\n", "abstract": " The FogScreen is an immaterial projection screen that consists of air and a little humidity, and enables high-quality projected images in thin air. Objects and images appear to float in mid-air, and touching or walking through them enhances the impression, as the screen feels just like air. One nice feature is the possibility to project different images on each side without having them interfere with each other.", "num_citations": "68\n", "authors": ["1194"]}
{"title": "Wearing it out: First steps toward mobile augmented reality systems\n", "abstract": " Over the past decade, there has been a ground swell of activity in two elds of user interface research: augmented reality and wearable computing. Augmented reality [1] refers to the creation of virtual environments that supplement, rather than replace, the real world with additional information. This is accomplished through the use of\\see-through\" displays that enrich the user's view of the world by overlaying visual, auditory, and even haptic, material on what she experiences. Visual augmented reality systems typically, but not exclusively, employ head-tracked, head-worn displays. These either use half-silvered mirror beam splitters to re ect small computer displays, optically combining them with a view of the real world, or use opaque displays fed by electronics that merge imagery captured by head-worn cameras with synthesized graphics. Wearable computing moves computers o the desktop and onto the user's body, made possible through the miniaturization of computers, peripherals, and networking technology.(While we prefer this general de nition implied by the", "num_citations": "68\n", "authors": ["1194"]}
{"title": "The allosphere: Immersive multimedia for scientific discovery and artistic exploration\n", "abstract": " The AlloSphere is a spherical space in which immersive, virtual environments allow users to explore large-scale data sets through multimodal, interactive media.", "num_citations": "66\n", "authors": ["1194"]}
{"title": "Interactive perspective cut-away views for general 3d scenes\n", "abstract": " We present a technique that allows a user to look beyond occluding objects in arbitrary 3D graphics scenes. In order to control this form of virtual x-ray vision, the user interactively cuts holes into the occluding geometry. The user can rapidly define a cutout shape or choose a standard shape and sweep it over the occluding wall segments to reveal what lies behind them. Holes are rendered in the correct 3D perspective as if they were actually cut into the obstructing geometry, including border regions that give the cutout shape physical depth, simulating penetration of a physical wall that possesses some generic thickness.", "num_citations": "66\n", "authors": ["1194"]}
{"title": "Effects of information availability on command-and-control decision making: performance, trust, and situation awareness\n", "abstract": " Objective:We investigated how increases in task-relevant information affect human decision-making performance, situation awareness (SA), and trust in a simulated command-and-control (C2) environment.Background:Increased information is often associated with an improvement of SA and decision-making performance in networked organizations. However, previous research suggests that increasing information without considering the task relevance and the presentation can impair performance.Method:We used a simulated C2 task across two experiments. Experiment 1 varied the information volume provided to individual participants and measured the speed and accuracy of decision making for task performance. Experiment 2 varied information volume and information reliability provided to two participants acting in different roles and assessed decision-making performance, SA, and trust between the paired\u00a0\u2026", "num_citations": "64\n", "authors": ["1194"]}
{"title": "Bridging the gaps: Hybrid tracking for adaptive mobile augmented reality\n", "abstract": " Tracking accuracy in a location-aware mobile system can change dynamically as a function of the user's location and other variables specific to the tracking technologies used. This is especially problematic for mobile augmented reality systems, which ideally require extremely precise position tracking for the user's head, but which may not always be able to achieve that level of accuracy. While it is possible to ignore variable positional accuracy in an augmented reality user interface, this can make for a confusing system; for example, when accuracy is low, virtual objects that are nominally registered with real ones may be too far off to be of use.             To address this problem, we describe an experimental mobile augmented reality system that: (1) employs multiple position-tracking technologies, including ones that apply heuristics based on environmental knowledge; (2) coordinates these concurrently monitored\u00a0\u2026", "num_citations": "64\n", "authors": ["1194"]}
{"title": "Steps toward accommodating variable position tracking accuracy in a mobile augmented reality system\n", "abstract": " The position-tracking accuracy of a location-aware mobile system can change dynamically as a function of the user\u2019s location and other variables specific to the tracker technology used. This is especially problematic for mobile augmented reality systems, which ideally require extremely precise position tracking for the user\u2019s head, but which may not always be able to achieve the necessary level of accuracy. While it is possible to ignore variable positional accuracy in an augmented reality user interface, this can make for a confusing system; for example, when accuracy is low, virtual objects that are nominally registered with real ones may be too far off to be of use.To address this problem, we describe the early stages of an experimental mobile augmented reality system that adapts its user interface automatically to accommodate changes in tracking accuracy. Our system employs different technologies for tracking a user\u2019s position, resulting in a wide variation in positional accuracy: an indoor ultrasonic tracker and an outdoor real-time kinematic GPS system. For areas outside the range of both, we introduce a dead-reckoning approach that combines a pedometer and orientation tracker with environmental knowledge expressed in spatial maps and accessibility graphs. We present preliminary results from this approach in the context of a navigational guidance system that helps users to orient themselves in an unfamiliar environment. Our system uses inferencing and path planning to guide users toward targets that they choose.", "num_citations": "63\n", "authors": ["1194"]}
{"title": "The allosphere: a large-scale immersive surround-view instrument\n", "abstract": " We present the design of the Allosphere and initial experiences from its ongoing implementation. The UCSB Allosphere is a novel large-scale instrument for immersive visualization and simulation, which in its full realization will be one of the world's largest immersive environments. The three-story high cubical space comprises an anechoic chamber with a spherical display screen, ten meters in diameter, surrounding from one to thirty users standing on a bridge structure. The Allosphere is differentiated from conventional virtual reality environments by its size and focus on collaborative experiences, its seamless surround-view capabilities and its focus on multiple sensory modalities and interaction. The Allosphere is being equipped with high-resolution active stereo projectors, a complete 3D sound system with hundreds of speakers, and interaction technology. In this paper we will give an overview of the purpose of\u00a0\u2026", "num_citations": "62\n", "authors": ["1194"]}
{"title": "The role of latency in the validity of AR simulation\n", "abstract": " It is extremely challenging to run controlled studies comparing multiple Augmented Reality (AR) systems. We use an AR simulation approach, in which a Virtual Reality (VR) system is used to simulate multiple AR systems. To investigate the validity of this approach, in our first experiment we carefully replicated a well-known study by Ellis et al. using our simulator, obtaining comparable results. We include a discussion on general issues we encountered with replicating a prior study. In our second experiment further exploring the validity of AR simulation, we investigated the effects of simulator latency on the results from experiments conducted in an AR simulator. We found simulator latency to have a significant effect on 3D tracing, however there was no interaction between simulator latency and artificial latency. Based on the results from these two experiments, we conclude that simulator latency is not inconsequential\u00a0\u2026", "num_citations": "59\n", "authors": ["1194"]}
{"title": "Live tracking and mapping from both general and rotation-only camera motion\n", "abstract": " We present an approach to real-time tracking and mapping that supports any type of camera motion in 3D environments, that is, general (parallax-inducing) as well as rotation-only (degenerate) motions. Our approach effectively generalizes both a panorama mapping and tracking system and a keyframe-based Simultaneous Localization and Mapping (SLAM) system, behaving like one or the other depending on the camera movement. It seamlessly switches between the two and is thus able to track and map through arbitrary sequences of general and rotation-only camera movements. Key elements of our approach are to design each system component such that it is compatible with both panoramic data and Structure-from-Motion data, and the use of the `Geometric Robust Information Criterion' to decide whether the transformation between a given pair of frames can best be modeled with an essential matrix E, or\u00a0\u2026", "num_citations": "58\n", "authors": ["1194"]}
{"title": "Envisor: Online environment map construction for mixed reality\n", "abstract": " One of the main goals of anywhere augmentation is the development of automatic algorithms for scene acquisition in augmented reality systems. In this paper, we present Envisor, a system for online construction of environment maps in new locations. To accomplish this, Envisor uses vision-based frame to frame and landmark orientation tracking for long-term, drift-free registration. For additional robustness, a gyroscope/compass orientation unit can optionally be used for hybrid tracking. The tracked video is then projected into a cubemap frame by frame. Feedback is presented to the user to help avoid gaps in the cubemap, while any remaining gaps are filled by texture diffusion. The resulting environment map can be used for a variety of applications, including shading of virtual geometry and remote presence.", "num_citations": "58\n", "authors": ["1194"]}
{"title": "Theia: A fast and scalable structure-from-motion library\n", "abstract": " In this paper, we have presented a comprehensive multi-view geometry library, Theia, that focuses on large-scale SfM. In addition to state-of-the-art scalable SfM pipelines, the library provides numerous tools that are useful for students, researchers, and industry experts in the field of multi-view geometry. Theia contains clean code that is well documented (with code comments and the website) and easy to extend. The modular design allows for users to easily implement and experiment with new algorithms within our current pipeline without having to implement a full end-to-end SfM pipeline themselves. Theia has already gathered a large number of diverse users from universities, startups, and industry and we hope to continue to gather users and active contributors from the open-source community.", "num_citations": "57\n", "authors": ["1194"]}
{"title": "Understanding information credibility on twitter\n", "abstract": " Increased popularity of microblogs in recent years brings about a need for better mechanisms to extract credible or otherwise useful information from noisy and large data. While there are a great number of studies that introduce methods to find credible data, there is no accepted credibility benchmark. As a result, it is hard to compare different studies and generalize from their findings. In this paper, we argue for a methodology for making such studies more useful to the research community. First, the underlying ground truth values of credibility must be reliable. The specific constructs used to define credibility must be carefully defined. Secondly, the underlying network context must be quantified and documented. To illustrate these two points, we conduct a unique credibility study of two different data sets on the same topic, but with different network characteristics. We also conduct two different user surveys, and\u00a0\u2026", "num_citations": "55\n", "authors": ["1194"]}
{"title": "Real-time hand interaction for augmented reality on mobile phones\n", "abstract": " Over the past few years, Augmented Reality has become widely popular in the form of smart phone applications, however most smart phone-based AR applications are limited in user interaction and do not support gesture-based direct manipulation of the augmented scene. In this paper, we introduce a new AR interaction methodology, employing users' hands and fingers to interact with the virtual (and possibly physical) objects that appear on the mobile phone screen. The goal of this project was to support different types of interaction (selection, transformation, and fine-grain control of an input value) while keeping the methodology for hand detection as simple as possible to maintain good performance on smart phones. We evaluated our methods in user studies, collecting task performance data and user impressions about this direct way of interacting with augmented scenes through mobile phones.", "num_citations": "55\n", "authors": ["1194"]}
{"title": "Evaluating wide-field-of-view augmented reality with mixed reality simulation\n", "abstract": " Full-surround augmented reality, with augmentations spanning the entire human field of view and beyond, is an under-explored topic since there is currently no hardware that can support it. As current AR displays only support relatively small fields of view, most AR applications to-date employ relatively small point-based annotations of the physical world. Anticipating a change in AR capabilities, we experiment with wide-field-of-view annotations that link elements far apart in the visual field. We have built a system that uses full-surround virtual reality to simulate augmented reality with different field of views, with and without tracking artifacts. We conducted a study comparing user performance on five different task groups within an information-seeking scenario, comparing two different fields of view and presence and absence of tracking artifacts. A constrained field of view significantly increased task completion time\u00a0\u2026", "num_citations": "54\n", "authors": ["1194"]}
{"title": "Methods, apparatus and data structures for providing a user interface which facilitates decision making\n", "abstract": " A user interface for facilitating a decision making process, such as planning a trip. A unified view of various types of information related to an event may be provided. The unified view may be presented in a simulated three-dimensional environment having different types of information depicted on different windows. Different types of information related to a common event may be visually linked. A window showing a particular type of information may be brought into a focus view for closer inspection by a user. Alternative decisions may be depicted to permit vagueness or uncertainty, particularly at early iterations in the decision making process.", "num_citations": "54\n", "authors": ["1194"]}
{"title": "Coarse, inexpensive, infrared tracking for wearable computing\n", "abstract": " We present a novel, inexpensive, coarse tracking system that determines a person\u2019s approximate 2D location and 1D head orientation in an indoor environment. While this coarse tracking cannot support precise registration of overlaid material, it can be used to drive user interfaces that can adapt to the quality of tracking available. Our approach uses a set of strong infrared beacons, each of which broadcasts a unique ID. The beacons are deployed in the environment such that their zones of influence strategically overlap, partitioning the area of coverage into a set of uniquely identifiable fragments. We use a compound, omnidirectional infrared receiver, composed of a set of individual, directional infrared receivers, to infer 2D position (parallel to the ground plane) and 1D orientation (azimuth), employing a Kalman-filter\u2013based architecture for smoothing and data integration with other tracking systems available. To test our ideas, we have applied them to a prototype head tracker, and present results from our tests.", "num_citations": "54\n", "authors": ["1194"]}
{"title": "Arbis pictus: A study of vocabulary learning with augmented reality\n", "abstract": " We conducted a fundamental user study to assess potential benefits of AR technology for immersive vocabulary learning. With the idea that AR systems will soon be able to label real-world objects in any language in real time, our within-subjects (N=52) lab-based study explores the effect of such an AR vocabulary prompter on participants learning nouns in an unfamiliar foreign language, compared to a traditional flashcard-based learning approach. Our results show that the immersive AR experience of learning with virtual labels on real-world objects is both more effective and more enjoyable for the majority of participants, compared to flashcards. Specifically, when participants learned through augmented reality, they scored significantly better on both same-day and 4-day delayed productive recall tests than when they learned using the flashcard method. We believe this result is an indication of the strong potential\u00a0\u2026", "num_citations": "53\n", "authors": ["1194"]}
{"title": "A novel walk-through 3D display\n", "abstract": " We present a novel walk-through 3D display based on the patented FogScreen, an \"immaterial\" indoor 2D projection screen, which enables high-quality projected images in free space. We extend the basic 2D FogScreen setup in three major ways. First, we use head tracking to provide correct perspective rendering for a single user. Second, we add support for multiple types of stereoscopic imagery. Third, we present the front and back views of the graphics content on the two sides of the FogScreen, so that the viewer can cross the screen to see the content from the back. The result is a wall-sized, immaterial display that creates an engaging 3D visual.", "num_citations": "53\n", "authors": ["1194"]}
{"title": "Information at a glance [augmented reality user interfaces]\n", "abstract": " What if we could visualize and interact with information directly in the context of our surroundings? Our research group is exploring how augmented reality (AR) could someday make this possible. AR integrates a complementary virtual world with the physical world-for example, by using head-tracked see-through head-worn displays to overlay graphics on what we see. Instead of looking back and forth between the real world and a PDA, we look directly at the real world and the virtual information overlaid on it. At the heart of this approach is context-aware computing, computing systems that are sensitive to the context in which they operate, ranging from human relationships to physical location. For example, information might be tied to specific locations within a global, Earth-centered, coordinate system. How can we design effective mobile AR user interfaces? We've been trying to answer this question in part by\u00a0\u2026", "num_citations": "53\n", "authors": ["1194"]}
{"title": "The City of Sights: Design, construction, and measurement of an Augmented Reality stage set\n", "abstract": " We describe the design and implementation of a physical and virtual model of an imaginary urban scene-the \u201cCity of Sights\u201d- that can serve as a backdrop or \u201cstage\u201d for a variety of Augmented Reality (AR) research. We argue that the AR research community would benefit from such a standard model dataset which can be used for evaluation of such AR topics as tracking systems, modeling, spatial AR, rendering tests, collaborative AR and user interface design. By openly sharing the digital blueprints and assembly instructions for our models, we allow the proposed set to be physically replicable by anyone and permit customization and experimental changes to the stage design which enable comprehensive exploration of algorithms and methods. Furthermore we provide an accompanying rich dataset consisting of video sequences under varying conditions with ground truth camera pose. We employed three different\u00a0\u2026", "num_citations": "52\n", "authors": ["1194"]}
{"title": "\u201cAnywhere Augmentation\u201d: Towards Mobile Augmented Reality in Unprepared Environments\n", "abstract": " We introduce the term \u201cAnywhere Augmentation\u201d to refer to the idea of linking location-specific computing services with the physical world, making them readily and directly available in any situation and location. This chapter presents a novel approach to \u201cAnywhere Augmentation\u201d based on efficient human input for wearable computing and augmented reality (AR). Current mobile and wearable computing technologies, as found in many industrial and governmental service applications, do not routinely integrate the services they provide with the physical world. Major limitations in the computer\u2019s general scene understanding abilities and the infeasibility of instrumenting the whole globe with a unified sensing and computing environment prevent progress in this area. Alternative approaches must be considered.               We present a mobile augmented reality system for outdoor annotation of the real world. To\u00a0\u2026", "num_citations": "52\n", "authors": ["1194"]}
{"title": "Using aerial photographs for improved mobile ar annotation\n", "abstract": " We present a mobile augmented reality system for outdoor annotation of the real world. To reduce user burden, we use aerial photographs in addition to the wearable system's usual data sources (position, orientation, camera and user input). This allows the user to accurately annotate 3D features with only a few simple interactions from a single position by aligning features in both their first-person viewpoint and in the aerial view. We examine three types of aerial photograph features - corners, edges, and regions - that are suitable for a wide variety of useful mobile augmented reality applications, and are easily visible on aerial photographs. By using aerial photographs in combination with wearable augmented reality, we are able to achieve much higher accuracy 3D annotation positions than was previously possible from a single user location.", "num_citations": "52\n", "authors": ["1194"]}
{"title": "Efficient and robust radiance transfer for probeless photorealistic augmented reality\n", "abstract": " Photorealistic Augmented Reality (AR) requires knowledge of the scene geometry and environment lighting to compute photometric registration. Recent work has introduced probeless photometric registration, where environment lighting is estimated directly from observations of reflections in the scene rather than through an invasive probe such as a reflective ball. However, computing the dense radiance transfer of a dynamically changing scene is computationally challenging. In this work, we present an improved radiance transfer sampling approach, which combines adaptive sampling in image and visibility space with robust caching of radiance transfer to yield real time framerates for photorealistic AR scenes with dynamically changing scene geometry and environment lighting.", "num_citations": "50\n", "authors": ["1194"]}
{"title": "Groundcam: A tracking modality for mobile mixed reality\n", "abstract": " Anywhere augmentation pursues the goal of lowering the initial investment of time and money necessary to participate in mixed reality work, bridging the gap between researchers in the field and regular computer users. Our paper contributes to this goal by introducing the GroundCam, a cheap tracking modality with no significant setup necessary. By itself, the GroundCam provides high frequency, high resolution relative position information similar to an inertial navigation system, but with significantly less drift. When coupled with a wide area tracking modality via a complementary Kalman filter, the hybrid tracker becomes a powerful base for indoor and outdoor mobile mixed reality work", "num_citations": "48\n", "authors": ["1194"]}
{"title": "System and method for view management in three dimensional space\n", "abstract": " A method for managing a display space for a 3D environment is provided. A 3D scene having at least one scene object is displayed and the visible surfaces of the scene objects are represented as visible space in a 2D view plane representation. Controllable objects that are to be placed in the scene are defined by parameters such as size, placement priority, proximity relationships and the like. The available space for placing controllable objects, which can include empty space and low priority background and foreground regions, is determined for each controllable object. The placement for controllable objects in the 3D space is then determined in accordance with at least placement parameter and one of the visible space and available space of the view-plane representation such that view management objectives, such as not occluding important scene objects, are accomplished.", "num_citations": "46\n", "authors": ["1194"]}
{"title": "Medium containing information gathered from material including a source and interface for graphically displaying the information\n", "abstract": " An electronic medium is a new form of publication for a source material, such as a book. The medium includes information about features of the source material and features of secondary information related to the source material. The medium can be used with a visualization system. With the system, a user is provided with tools that respond to the user's needs and requests at a level of the collection, rather than just with a single work.", "num_citations": "44\n", "authors": ["1194"]}
{"title": "Immersive full-surround multi-user system design\n", "abstract": " This paper describes our research in full-surround, multimodal, multi-user, immersive instrument design in a large VR instrument. The three-story instrument, designed for large-scale, multimodal representation of complex and potentially high-dimensional information, specifically focuses on multi-user participation by facilitating interdisciplinary teams of co-located researchers in exploring complex information through interactive visual and aural displays in a full-surround, immersive environment. We recently achieved several milestones in the instrument's design that improves multi-user participation when exploring complex data representations and scientific simulations. These milestones include affordances for \u201censemble-style\u201d interaction allowing groups of participants to see, hear, and explore data as a team using our multi-user tracking and interaction systems; separate visual display modes for rectangular\u00a0\u2026", "num_citations": "42\n", "authors": ["1194"]}
{"title": "LinkedVis: exploring social and semantic career recommendations\n", "abstract": " This paper presents LinkedVis, an interactive visual recommender system that combines social and semantic knowledge to produce career recommendations based on the LinkedIn API. A collaborative (social) approach is employed to identify professionals with similar career paths and produce personalized recommendations of both companies and roles. To unify semantically identical but lexically distinct entities and arrive at better user models, we employ lightweight natural language processing and entity resolution using semantic information from a variety of end-points on the web. Elements from the underlying recommendation algorithm are exposed through an interactive interface that allows users to manipulate different aspects of the algorithm and the data it operates on, allowing users to explore a variety of\" what-if\" scenarios around their current profile. We evaluate LinkedVis through leave-one-out\u00a0\u2026", "num_citations": "42\n", "authors": ["1194"]}
{"title": "Fast annotation and modeling with a single-point laser range finder\n", "abstract": " This paper presents methodology for integrating a small, single-point laser range finder into a wearable augmented reality system. We first present a way of creating object-aligned annotations with very little user effort. Second, we describe techniques to segment and pop-up foreground objects. Finally, we introduce a method using the laser range finder to incrementally build 3D panoramas from a fixed observerpsilas location. To build a 3D panorama semi-automatically, we track the systempsilas orientation and use the sparse range data acquired as the user looks around in conjunction with real-time image processing to construct geometry around the userpsilas position. Using full 3D panoramic geometry, it is possible for new virtual objects to be placed in the scene with proper lighting and occlusion by real world objects, which increases the expressivity of the AR experience.", "num_citations": "42\n", "authors": ["1194"]}
{"title": "Depth-fused 3D imagery on an immaterial display\n", "abstract": " We present an immaterial display that uses a generalized form of depth-fused 3D (DFD) rendering to create unencumbered 3D visuals. To accomplish this result, we demonstrate a DFD display simulator that extends the established depth-fused 3D principle by using screens in arbitrary configurations and from arbitrary viewpoints. The feasibility of the generalized DFD effect is established with a user study using the simulator. Based on these results, we developed a prototype display using one or two immaterial screens to create an unencumbered 3D visual that users can penetrate, examining the potential for direct walk-through and reach-through manipulation of the 3D scene. We evaluate the prototype system in formative and summative user studies and report the tolerance thresholds discovered for both tracking and projector errors.", "num_citations": "42\n", "authors": ["1194"]}
{"title": "A cost-effective usability evaluation progression for novel interactive systems\n", "abstract": " This paper reports on user interface design and evaluation for a mobile, outdoor, augmented reality (AR) application. This novel system, called the battlefield augmented reality system (BARS), supports information presentation and entry for situation awareness in an urban war fighting setting. To our knowledge, this is the first time extensive use of usability engineering has been systematically applied to development of a real-world AR system. Our BARS team has applied a cost-effective progression of usability engineering activities from the very beginning of BARS development. We discuss how we first applied cycles of structured expert evaluations to BARS user interface development, employing user interface mockups representing occluded (non-visible) objects. Then we discuss how results of these evaluations informed our subsequent user-based statistical evaluations and formative evaluations, and present\u00a0\u2026", "num_citations": "41\n", "authors": ["1194"]}
{"title": "gdls: A scalable solution to the generalized pose and scale problem\n", "abstract": " In this work, we present a scalable least-squares solution for computing a seven degree-of-freedom similarity transform. Our method utilizes the generalized camera model to compute relative rotation, translation, and scale from four or more 2D-3D correspondences. In particular, structure and motion estimations from monocular cameras lack scale without specific calibration. As such, our methods have applications in loop closure in visual odometry and registering multiple structure from motion reconstructions where scale must be recovered. We formulate the generalized pose and scale problem as a minimization of a least squares cost function and solve this minimization without iterations or initialization. Additionally, we obtain all minima of the cost function. The order of the polynomial system that we solve is independent of the number of points, allowing our overall approach to scale favorably. We\u00a0\u2026", "num_citations": "38\n", "authors": ["1194"]}
{"title": "Efficient computation of absolute pose for gravity-aware augmented reality\n", "abstract": " We propose a novel formulation for determining the absolute pose of a single or multi-camera system given a known vertical direction. The vertical direction may be easily obtained by detecting the vertical vanishing points with computer vision techniques, or with the aid of IMU sensor measurements from a smartphone. Our solver is general and able to compute absolute camera pose from two 2D-3D correspondences for single or multi-camera systems. We run several synthetic experiments that demonstrate our algorithm's improved robustness to image and IMU noise compared to the current state of the art. Additionally, we run an image localization experiment that demonstrates the accuracy of our algorithm in real-world scenarios. Finally, we show that our algorithm provides increased performance for real-time model-based tracking compared to solvers that do not utilize the vertical direction and show our\u00a0\u2026", "num_citations": "37\n", "authors": ["1194"]}
{"title": "Systems and methods for augmented reality-based remote collaboration\n", "abstract": " Various embodiments each include at least one of systems, methods, devices, and software for an augmented shared visual space for live mobile remote collaboration on physical tasks. One or more participants in location A can explore a scene in location B independently of one or more local participants current camera position in location B, and can communicate via spatial annotations that are immediately visible to all other participants in augmented reality.", "num_citations": "36\n", "authors": ["1194"]}
{"title": "Evaluating effectiveness in virtual environments with MR simulation\n", "abstract": " Both virtual reality (VR) and augmented reality (AR) systems have achieved some success and offer further potential to be used in military training. However, the use of high-end VR and AR remains costly and cumbersome, and the most advanced technologies are not widely deployed in actual training systems. Decision makers need evidence for the effectiveness of such systems in order to justify their use. In particular, it is important to know which display systems (eg, head-mounted display, CAVE) will provide the best cost/benefit ratio for training, and what display characteristics (eg, field of view, stereoscopy) are most critical in determining the effectiveness of a VR or AR training system.The answers to these questions depend on an understanding of the effects of display parameters on task performance and training transfer. Obtaining this knowledge requires empirical studies, but such studies pose significant challenges. Direct comparisons of different displays do not produce generalizable results because the displays differ in many ways. AR studies face the additional issues of unreliable hardware that lacks desirable features and a lack of control of the real-world environment.", "num_citations": "36\n", "authors": ["1194"]}
{"title": "WiGis: A Framework for Scalable Web-Based Interactive Graph Visualizations\n", "abstract": " Traditional network visualization tools inherently suffer from scalability problems, particularly when such tools are interactive and web-based. In this paper we introduce WiGis \u2013Web-based Interactive Graph Visualizations. WiGis exemplify a fully web-based framework for visualizing large-scale graphs natively in a user\u2019s browser at interactive frame rates with no discernible associated startup costs. We demonstrate fast, interactive graph animations for up to hundreds of thousands of nodes in a browser through the use of asynchronous data and image transfer. Empirical evaluations show that our system outperforms traditional web-based graph visualization tools by at least an order of magnitude in terms of scalability, while maintaining fast, high-quality interaction.", "num_citations": "36\n", "authors": ["1194"]}
{"title": "Interpreting 2d gesture annotations in 3d augmented reality\n", "abstract": " A 2D gesture annotation provides a simple way to annotate the physical world in augmented reality for a range of applications such as remote collaboration. When rendered from novel viewpoints, these annotations have previously only worked with statically positioned cameras or planar scenes. However, if the camera moves and is observing an arbitrary environment, 2D gesture annotations can easily lose their meaning when shown from novel viewpoints due to perspective effects. In this paper, we present a new approach towards solving this problem by using a gesture enhanced annotation interpretation. By first classifying which type of gesture the user drew, we show that it is possible to render the 2D annotations in 3D in a way that conforms more to the original intention of the user than with traditional methods. We first determined a generic vocabulary of important 2D gestures for an augmented reality\u00a0\u2026", "num_citations": "34\n", "authors": ["1194"]}
{"title": "Getting the message? A study of explanation interfaces for microblog data analysis\n", "abstract": " In many of today's online applications that facilitate data exploration, results from information filters such as recommender systems are displayed alongside traditional search tools. However, the effect of prediction algorithms on users who are performing open-ended data exploration tasks through a search interface is not well understood. This paper describes a study of three interface variations of a tool for analyzing commuter traffic anomalies in the San Francisco Bay Area. The system supports novel interaction between a prediction algorithm and a human analyst, and is designed to explore the boundaries, limitations and synergies of both. The degree of explanation of underlying data and algorithmic process was varied experimentally across each interface. The experiment (N= 197) was performed to assess the impact of algorithm transparency/explanation on data analysis tasks in terms of search success\u00a0\u2026", "num_citations": "34\n", "authors": ["1194"]}
{"title": "Large scale sfm with the distributed camera model\n", "abstract": " We introduce the distributed camera model, a novel model for Structure-from-Motion (SfM). This model describes image observations in terms of light rays with ray origins and directions rather than pixels. As such, the proposed model is capable of describing a single camera or multiple cameras simultaneously as the collection of all light rays observed. We show how the distributed camera model is a generalization of the standard camera model and we describe a general formulation and solution to the absolute camera pose problem that works for standard or distributed cameras. The proposed method computes a solution that is up to 8 times more efficient and robust to rotation singularities in comparison with gDLS[21]. Finally, this method is used in an novel large-scale incremental SfM pipeline where distributed cameras are accurately and robustly merged together. This pipeline is a direct generalization of\u00a0\u2026", "num_citations": "33\n", "authors": ["1194"]}
{"title": "A replication study testing the validity of ar simulation in vr for controlled experiments\n", "abstract": " It is extremely challenging to run controlled studies comparing multiple augmented reality (AR) systems. We use an ldquoAR simulationrdquo approach, in which a virtual reality (VR) system is used to simulate multiple AR systems. In order to validate this approach, we carefully replicated a well-known study by Ellis et al. using our simulator, obtaining comparable results.", "num_citations": "33\n", "authors": ["1194"]}
{"title": "Environment mapping with automatic motion model selection\n", "abstract": " Various embodiments each include at least one of systems, methods, devices, and software for environment mapping with automatic motion model selection. One embodiment in the form of a method includes receiving a video frame captured by a camera device into memory and estimating a type of motion from a previously received video frame held in memory to the received video frame. When the type of motion is the same as motion type of a current keyframe group held in memory, the method includes adding the received video frame to the current keyframe group. Conversely, when the type of motion is not the same motion type of the current keyframe group held in memory, the method includes creating a new keyframe group in memory and adding the received video frame to the new keyframe group.", "num_citations": "32\n", "authors": ["1194"]}
{"title": "Spatio-temporal detection of divided attention in reading applications using EEG and eye tracking\n", "abstract": " Reading is central to learning and communicating, however, divided attention in the form of distraction may be present in learning environments, resulting in a limited understanding of the reading material. This paper presents a novel system that can spatio-temporally detect divided attention in users during two different reading applications: typical document reading and speed reading. Eye tracking and electroencephalography (EEG) monitor the user during reading and provide a classifier with data to decide the user's attention state. The multimodal data informs the system where the user was distracted spatially in the user interface and when the user was distracted. Classification was evaluated with two exploratory experiments. The first experiment was designed to divide the user's attention with a multitasking scenario. The second experiment was designed to divide the users attention by simulating a real-world\u00a0\u2026", "num_citations": "32\n", "authors": ["1194"]}
{"title": "SCUBA: Focus and Context for Real-Time Mesh Network Health Diagnosis\n", "abstract": " Large-scale wireless metro-mesh networks consisting of hundreds of routers and thousands of clients suffer from a plethora of performance problems. The sheer scale of such networks, the abundance of performance metrics, and the absence of effective tools can quickly overwhelm a network operators\u2019 ability to diagnose these problems. As a solution, we present SCUBA, an interactive focus and context visualization framework for metro-mesh health diagnosis. SCUBA places performance metrics into multiple tiers or contexts, and displays only the topmost context by default to reduce screen clutter and to provide a broad contextual overview of network performance. A network operator can interactively focus on problem regions and zoom to progressively reveal more detailed contexts only in the focal region. We describe SCUBA\u2019s contexts and its planar and hyperbolic views of a nearly 500 node mesh to\u00a0\u2026", "num_citations": "32\n", "authors": ["1194"]}
{"title": "Believe it or not? analyzing information credibility in microblogs\n", "abstract": " This paper identifies and evaluates key factors that influence credibility perception in microblogs. Specifically, we report on a demographic survey (N= 81) followed by a user experiment (N= 102) in order to answer the following research questions:(1) What are the important cues that contribute to information being perceived as credible? and (2) To what extent is such a quantification portable across different microblogging platforms? To answer the second question, we study two popular microblogs, Reddit and Twitter. Key results include that significant effects of individual factors can be isolated, are portable, and that metadata and image type elements are, in general, the strongest influencing factors in credibility assessments.", "num_citations": "31\n", "authors": ["1194"]}
{"title": "Behaviorism: A framework for dynamic data visualization\n", "abstract": " While a number of information visualization software frameworks exist, creating new visualizations, especially those that involve novel visualization metaphors, interaction techniques, data analysis strategies, and specialized rendering algorithms, is still often a difficult process. To facilitate the creation of novel visualizations we present a new software framework, behaviorism, which provides a wide range of flexibility when working with dynamic information on visual, temporal, and ontological levels, but at the same time providing appropriate abstractions which allow developers to create prototypes quickly which can then easily be turned into robust systems. The core of the framework is a set of three interconnected graphs, each with associated operators: a scene graph for high-performance 3D rendering, a data graph for different layers of semantically-linked heterogeneous data, and a timing graph for sophisticated\u00a0\u2026", "num_citations": "31\n", "authors": ["1194"]}
{"title": "I can do better than your AI: expertise and explanations\n", "abstract": " Intelligent assistants, such as navigation, recommender, and expert systems, are most helpful in situations where users lack domain knowledge. Despite this, recent research in cognitive psychology has revealed that lower-skilled individuals may maintain a sense of illusory superiority, which might suggest that users with the highest need for advice may be the least likely to defer judgment. Explanation interfaces-a method for persuading users to take a system's advice-are thought by many to be the solution for instilling trust, but do their effects hold for self-assured users? To address this knowledge gap, we conducted a quantitative study (N= 529) wherein participants played a binary decision-making game with help from an intelligent assistant. Participants were profiled in terms of both actual (measured) expertise and reported familiarity with the task concept. The presence of explanations, level of automation, and\u00a0\u2026", "num_citations": "30\n", "authors": ["1194"]}
{"title": "Effects of unaugmented periphery and vibrotactile feedback on proxemics with virtual humans in ar\n", "abstract": " In this paper, we investigate factors and issues related to human locomotion behavior and proxemics in the presence of a real or virtual human in augmented reality (AR). First, we discuss a unique issue with current-state optical see-through head-mounted displays, namely the mismatch between a small augmented visual field and a large unaugmented periphery, and its potential impact on locomotion behavior in close proximity of virtual content. We discuss a potential simple solution based on restricting the field of view to the central region, and we present the results of a controlled human-subject study. The study results show objective benefits for this approach in producing behaviors that more closely match those that occur when seeing a real human, but also some drawbacks in overall acceptance of the restricted field of view. Second, we discuss the limited multimodal feedback provided by virtual humans in AR\u00a0\u2026", "num_citations": "30\n", "authors": ["1194"]}
{"title": "Stardust: Accessible and transparent gpu support for information visualization rendering\n", "abstract": " Web\u2010based visualization libraries are in wide use, but performance bottlenecks occur when rendering, and especially animating, a large number of graphical marks. While GPU\u2010based rendering can drastically improve performance, that paradigm has a steep learning curve, usually requiring expertise in the computer graphics pipeline and shader programming. In addition, the recent growth of virtual and augmented reality poses a challenge for supporting multiple display environments beyond regular canvases, such as a Head Mounted Display (HMD) and Cave Automatic Virtual Environment (CAVE). In this paper, we introduce a new web\u2010based visualization library called Stardust, which provides a familiar API while leveraging GPU's processing power. Stardust also enables developers to create both 2D and 3D visualizations for diverse display environments using a uniform API. To demonstrate Stardust's\u00a0\u2026", "num_citations": "30\n", "authors": ["1194"]}
{"title": "Evaluating gesture-based augmented reality annotation\n", "abstract": " Drawing annotations with 3D hand gestures in augmented reality are useful for creating visual and spatial references in the real world, especially when these gestures can be issued from a distance. Different techniques exist for highlighting physical objects with hand-drawn circle and arrow annotations from a distance, assuming an approximate 3D scene model (e.g., as provided by the Microsoft HoloLens). However, little is known about user preference and performance of such methods for annotating real-world 3D environments. In this paper, we compare different annotation methods using the HoloLens augmented reality development platform: Surface-Drawing and Air-Drawing, with either raw but smoothed or interpreted and beautified gesture input. For the Surface-Drawing method, users control a cursor that is projected onto the world model, allowing gesture input to occur directly on the surfaces of real-world\u00a0\u2026", "num_citations": "30\n", "authors": ["1194"]}
{"title": "Implicit 3d modeling and tracking for anywhere augmentation\n", "abstract": " This paper presents an online 3D modeling and tracking methodology that uses aerial photographs for mobile augmented reality. Instead of relying on models which are created in advance, the system generates a 3D model for a real building on the fly by combining frontal and aerial views with the help of an optical sensor, an inertial sensor, a GPS unit and a few mouse clicks. A user's initial pose is estimated using an aerial photograph, which is retrieved from a database according to the user's GPS coordinates, and an inertial sensor which measures pitch. To track the user's position and orientation in real-time, feature-based tracking is carried out based on salient points on the edges and the sides of a building the user is keeping in view. We implemented camera pose estimators using both a least squares and an unscented Kalman filter (UKF) approach. The UKF approach results in more stable and reliable\u00a0\u2026", "num_citations": "30\n", "authors": ["1194"]}
{"title": "Hypothetical recommendation: A study of interactive profile manipulation behavior for recommender systems\n", "abstract": " Explanation and dynamic feedback given to a user during the recommendation process can influence user experience. Despite this, many real-world recommender systems separate profile updates and feedback, obfuscating the relationship between them. This paper studies the effects of what we call hypothetical recommendations. These are recommendations generated by low-cost, exploratory profile manipulations, or\" what-if\" scenarios. In particular, we evaluate the effects of dynamic feedback from the recommender system on profile manipulations, the resulting recommendations and the user's overall experience. Results from a user experiment (N= 129) suggest that (i) dynamic feedback improves the effectiveness of profile updates,(ii) when dynamic feedback is present, users can identify and remove items that contribute to poor recommendations,(iii) profile update tasks improve perceived accuracy of recommendations and trust in the recommender, regardless of actual recommendation accuracy.", "num_citations": "29\n", "authors": ["1194"]}
{"title": "Efficiently selecting spatially distributed keypoints for visual tracking\n", "abstract": " We describe an algorithm dubbed Suppression via Disk Covering (SDC) to efficiently select a set of strong, spatially distributed key-points, and we show that selecting keypoint in this way significantly improves visual tracking. We also describe two efficient implementation schemes for the popular Adaptive Non-Maximal Suppression algorithm, and show empirically that SDC is significantly faster while providing the same improvements with respect to tracking robustness. In our particular application, using SDC to filter the output of an inexpensive (but, by itself, less reliable) keypoint detector (FAST) results in higher tracking robustness at significantly lower total cost than using a computationally more expensive detector.", "num_citations": "29\n", "authors": ["1194"]}
{"title": "Steps toward accommodating variable position tracking accuracy in a mobile augmented reality system\n", "abstract": " The position-tracking accuracy of a location-aware mobile system can change dynamically as a function of the user\u2019s location and other variables specific to the tracker technology used. This is especially problematic for mobile augmented reality systems, which ideally require extremely precise position tracking for the user\u2019s head, but which may not always be able to achieve the necessary level of accuracy. While it is possible to ignore variable positional accuracy in an augmented reality user interface, this can make for a confusing system; for example, when accuracy is low, virtual objects that are nominally registered with real ones may be too far off to be of use.To address this problem, we describe the early stagesof an experimental mobile augmented reality system that adapts its user interface automatically to accommodate changes in tracking accuracy. Our system employs different technologies for tracking a\u00a0\u2026", "num_citations": "29\n", "authors": ["1194"]}
{"title": "Cutting through the noise: Defining ground truth in information credibility on twitter\n", "abstract": " Increased popularity of microblogs in recent years brings about a need for better mechanisms to extract credible or otherwise useful information from noisy and large data. While there are a great number of studies that introduce methods to find credible data, there is no accepted credibility benchmark. As a result, it is hard to compare different studies and generalize from their findings. In this paper, we argue for a methodology for making such studies more useful to the research community. First, the underlying ground truth values of credibility must be reliable. The specific constructs used to define credibility must be carefully identified. Secondly, the underlying network context must be quantified and documented. To illustrate these two points, we conduct a unique credibility study of two different data sets on the same topic, but with different network characteristics. We also conduct two different user surveys, and construct two additional indicators of credibility based on retweet behavior. Through a detailed statistical study, we first show that survey based methods can be extremely noisy and results may vary greatly from survey to survey. However, by combining such methods with retweet behavior, we can incorporate two signals that are noisy but uncorrelated, resulting in ground truth measures that can be predicted with high accuracy and are stable across different data sets and survey methods. Newsworthiness of tweets can be a useful frame for specific applications, but it is not necessary for achieving reliable credibility ground truth measurements. We also show that the underlying model for predicting credibility can differ depending on the\u00a0\u2026", "num_citations": "28\n", "authors": ["1194"]}
{"title": "Enhancing classroom and distance learning through augmented reality\n", "abstract": " We present a multimedia solution for easily adding virtual annotations to class lectures through the use of augmented videoconferencing and tracked physical props. In the classroom, the actions of the instructor are captured by one or more cameras. We then use a normal desktop computer to add virtual data to the camera image. Our software solution tracks the physical objects and allows for overlays of relevant information, optionally deriving information from the movement of the objects. The resulting video may be sent either to a projector or monitor (to be viewed in class) or over the Internet (to be viewed by remote students). Additionally, our solution allows students to interact with the virtual data through the augmented video, even when distributed over the Internet.", "num_citations": "28\n", "authors": ["1194"]}
{"title": "An immaterial depth-fused 3D display\n", "abstract": " We present an immaterial display that uses a generalized form of depth-fused 3D (DFD) rendering to create unencumbered 3D visuals. To accomplish this result, we demonstrate a DFD display simulator that extends the established depth-fused 3D principle by using screens in arbitrary configurations and from arbitrary viewpoints. The performance of the generalized DFD effect is established with a user study using the simulator. Based on these results, we developed a prototype display using two immaterial screens to create an unencumbered 3D visual that users can penetrate, enabling the potential for direct walk-through and reach-through manipulation of the 3D scene.", "num_citations": "27\n", "authors": ["1194"]}
{"title": "Evaluating display types for AR selection and annotation\n", "abstract": " This paper evaluates different display devices for selection or annotation tasks in augmented reality (AR). We compare three different display types - a head mounted display and two hand held displays. The first hand held display is configured as a magic lens where the user sees the augmented space directly behind the display. The second hand held display is configured to be used at waist level (as one would commonly hold a tablet computer) but the view is still of the scene in front of the user. Making a selection or annotation in AR requires two distinct tasks by the user. First, the user must find the real (or virtual) object they want to mark. Second, the user must move a cursor to the object's location. We test and compare our three representative displays with respect to both tasks. We found that using a hand held display in the magic lens configuration was faster for cursor movement than either of the other two\u00a0\u2026", "num_citations": "26\n", "authors": ["1194"]}
{"title": "Improving Keypoint Orientation Assignment.\n", "abstract": " Detection and description of local image features has proven to be a powerful paradigm for a variety of applications in computer vision. Often, this process includes an orientation assignment step to render the overall process invariant to in-plane rotation. In this paper, we review several different existing algorithms and propose two novel, efficient methods for orientation assignment. The first method exhibits a very good speedperformance trade-off; the second is capable of multiple orientations and performs comparable to SIFT\u2019s orientation assignment while being significantly cheaper. Additionally, we improve one of the existing orientation assignment methods by generalizing it. All algorithms are evaluated empirically under a variety of conditions and in combination with six keypoint detectors.", "num_citations": "25\n", "authors": ["1194"]}
{"title": "An evaluation of bimanual gestures on the microsoft hololens\n", "abstract": " We developed and evaluated two-handed gestures on the Microsoft HoloLens to manipulate augmented reality annotations through rotation and scale operations. We explore the design space of bimanual interactions on head-worn AR platforms, with the intention of dedicating two-handed gestures to rotation and scaling manipulations while reserving one-handed interactions to drawing annotations. In total, we implemented five techniques for rotation and scale manipulation gestures on the Microsoft HoloLens: three two-handed techniques, one technique for one-handed rotation and two-handed scale, and one baseline one-handed technique that represents standard HoloLens UI recommendations. Two of the bimanual interaction techniques involve axis separation for rotation whereas the third technique is fully 6DOF and modeled after the successful \u201cspindle\u201d approach from 3DUI literature. To evaluate our\u00a0\u2026", "num_citations": "24\n", "authors": ["1194"]}
{"title": "Online environment model estimation for augmented reality\n", "abstract": " Augmented reality applications often rely on a detailed environment model to support features such as annotation and occlusion. Usually, such a model is constructed offline, which restricts the generality and mobility of the AR experience. In online SLAM approaches, the fidelity of the model stays at the level of landmark feature maps. In this work we introduce a system which constructs a textured geometric model of the user's environment as it is being explored. First, 3D feature tracks are organized into roughly planar surfaces. Then, image patches in keyframes are assigned to the planes in the scene using stereo analysis. The system runs as a background process and continually updates and improves the model over time. This environment model can then be rendered into new frames to aid in several common but difficult AR tasks such as accurate real-virtual occlusion and annotation placement.", "num_citations": "24\n", "authors": ["1194"]}
{"title": "Immersive audio and music in the Allosphere\n", "abstract": " The UCSB Allosphere is a 3-story-high spherical instrument in which virtual environments and performances can be experienced in a fully immersive fashion. It is made of a perforated aluminum sphere, ten meters in diameter, suspended inside a quasi-anechoic cube. The space is now being equipped with high-resolution active stereo projectors, a 3D sound system with hundreds of speakers, and with tracking and interaction mechanisms. The Allosphere allows for the exploration of large-scale data sets in an environment that is at the same time multimodal, multimedia, multi-user, immersive, and interactive. This novel and unique instrument will be used for research into scientific visualization/auralization and data exploration, and as a research environment for behavioral and cognitive scientists. It will also serve as a research and performance space for artists exploring new forms of art. In particular, the Allosphere has been carefully designed to allow for immersive music applications. In this paper, we give an overview of the instrument, focusing on the audio subsystem. We present first results and our experiences in developing and using the Allosphere in several prototype projects.", "num_citations": "24\n", "authors": ["1194"]}
{"title": "An immaterial, dual-sided display system with 3d interaction\n", "abstract": " We present an interactive wall-sized immaterial display that introduces a number of interesting possibilities for advanced interface design. The immaterial nature of a thin sheet of fog allows users to penetrate and even walk through the screen, while its dual-sided nature allows for new possibilities in multi-user faceto- face collaboration and pseudo-3D visualization.", "num_citations": "24\n", "authors": ["1194"]}
{"title": "Gibber: Abstractions for creative multimedia programming\n", "abstract": " We describe design decisions informing the development of Gibber, an audiovisual programming environment for the browser. Our design comprises a consistent notation across modalities in addition to high-level abstractions affording intuitive declarations of multimodal mappings, unified timing constructs, and rapid, iterative reinvocations of constructors while preserving the state of audio and visual graphs. We discuss the features of our environment and the abstractions that enable them. We close by describing use cases, including live audiovisual performances and computer science education.", "num_citations": "23\n", "authors": ["1194"]}
{"title": "Automated assistants to identify and prompt action on visual news bias\n", "abstract": " Bias is a common problem in today's media, appearing frequently in text and in visual imagery. Users on social media websites such as Twitter need better methods for identifying bias. Additionally, activists--those who are motivated to effect change related to some topic, need better methods to identify and counteract bias that is contrary to their mission. With both of these use cases in mind, in this paper we propose a novel tool called UnbiasedCrowd that supports identification of, and action on bias in visual news media. In particular, it addresses the following key challenges (1) identification of bias;(2) aggregation and presentation of evidence to users;(3) enabling activists to inform the public of bias and take action by engaging people in conversation with bots. We describe a preliminary study on the Twitter platform that explores the impressions that activists had of our tool, and how people reacted and engaged\u00a0\u2026", "num_citations": "22\n", "authors": ["1194"]}
{"title": "Generative fluid profiles for interactive media arts projects\n", "abstract": " This paper presents a real-time, interactive fluid simulation and vector visualization technique that can be incorporated in media arts projects. These techniques--referred to collectively as the Fluid Automata system--have been adapted for various configurations, including mobile applications, interactive 2D and 3D projections, and multi-touch tables, and have been presented in a number of different environments--both academic and artistic--including galleries, conferences, and a virtual reality research lab. We describe specific details about the fluid simulation component, which, by changing a small number of parameters, allows users to quickly generate a vast number of\" fluid profiles\" and thus to explore a wide range of aesthetic possibilities that are easy to incorporate into media arts projects. In particular, we present this fluid simulation (and accompanying visual representation) as an example of how media\u00a0\u2026", "num_citations": "22\n", "authors": ["1194"]}
{"title": "A visual interface for social information filtering\n", "abstract": " Collaborative or \"social\" filtering has been successfully deployed over the years as a technique for analyzing large amounts of user-preference knowledge to predict interesting items for an individual user. The black-box nature of most collaborative filtering (CF) applications leave the user wondering how the system arrived at its recommendation. In this paper we introduce PeerChooser, a collaborative recommender system with an interactive interface which provides the user not only an explanation of the recommendation process, but the opportunity to manipulate a graph of their peers at varying levels of granularity, to reflect aspects of their current requirements. PeerChooser's prediction component reads directly from the graph to yield the same results as a benchmark recommendation algorithm. Users then improve on these predictions by tweaking the graph in various ways. PeerChooser compares favorably\u00a0\u2026", "num_citations": "21\n", "authors": ["1194"]}
{"title": "Heads up and camera down: A vision-based tracking modality for mobile mixed reality\n", "abstract": " Anywhere augmentation pursues the goal of lowering the initial investment of time and money necessary to participate in mixed reality work, bridging the gap between researchers in the field, and regular computer users. Our paper contributes to this goal by introducing the GroundCam, a cheap tracking modality with no significant setup necessary. By itself, the GroundCam provides high frequency and high resolution relative position information similar to an inertial navigation system but with significantly less drift. We present the design and implementation of the GroundCam, analyze the impact of several design and runtime factors on tracking accuracy and consider the implications of extending our GroundCam to different hardware configurations. Motivated by the performance analysis, we developed a hybrid tracker that couples the GroundCam with a wide area tracking modality via a complementary Kalman filter\u00a0\u2026", "num_citations": "21\n", "authors": ["1194"]}
{"title": "Interface with angels: the future of VIR and AR interfaces\n", "abstract": " Although real guardian angels aren't easy to get hold of, some of the computer technology needed for such a personal assistant is already available. Other parts exist in the form of research prototypes, but some technological breakthroughs are necessary before we can realize their potential, let alone integrate into our daily routines. Future VR and AR interfaces won't necessarily try to provide a perfect imitation of reality but instead will adapt their display mechanisms to their users' individual requirements. The emergence of these interfaces won't rely on a single technology but will depend on the advances in many areas, including computer graphics, display technology, tracking and recognition devices, natural and intuitive interactions, 3D interaction techniques, mobile and ubiquitous computing, intelligent agents, and conversational user interfaces, to name a few. The guardian angel scenario exemplifies how\u00a0\u2026", "num_citations": "21\n", "authors": ["1194"]}
{"title": "Composition for conductor and audience: new uses for mobile devices in the concert hall\n", "abstract": " Composition for Conductor and Audience is an audience interaction piece first performed for an audience of over seventy-five people in June of 2011. The audience becomes the orchestra in this composition as they control different musical variables using the touchscreen surfaces on their personal mobile devices. To the authors' knowledge this is the first concert piece for bi-directional networked interactivity on audience-owned mobile devices to ever be performed. Audience members participated using the iOS/Android application'Control', a generic solution for creating touchscreen interfaces written by the first author. Over twenty members of the audience participated in the performance, using their personal devices to match gestures made by the conductor with corresponding gestures on their mobile devices.", "num_citations": "20\n", "authors": ["1194"]}
{"title": "Mid-air display experiments to create novel user interfaces\n", "abstract": " Displays are the most visible part of most computer applications. Novel display technologies strongly influence and inspire new forms of computer use and interaction. We are particularly interested in the interplay of novel displays and interaction for ubiquitous computing or ambient media environments, as emerging display technologies may become game-changers in how we define and use computers, possibly changing the context of computing fundamentally.               We present some of our experiments and lessons learnt with a new category of displays, the \u201cimmaterial\u201d FogScreen. It can be described as a novel media platform, exhibiting some fundamental differences to and advantages over other displays. It also enables novel kinds of user interfaces and experiences. In this paper we give insights about the special properties and strengths of the FogScreen by looking at a set of successfully\u00a0\u2026", "num_citations": "20\n", "authors": ["1194"]}
{"title": "All around the map: Online spherical panorama construction\n", "abstract": " One of the main goals of anywhere augmentation is the development of automatic algorithms for scene acquisition in augmented reality systems. In this paper, we present Envisor, a system for online construction of environment maps in new locations. To accomplish this, Envisor uses vision-based frame to frame and landmark orientation tracking for long-term, drift-free registration. For additional robustness, a gyroscope/compass orientation unit can optionally be used for hybrid tracking. The tracked video is then projected into a cubemap frame by frame. Feedback is presented to the user to help avoid gaps in the cubemap, while any remaining gaps are filled by texture diffusion. The resulting environment map can be used for a variety of applications, including shading of virtual geometry and remote presence.", "num_citations": "20\n", "authors": ["1194"]}
{"title": "Virtual and augmented reality\n", "abstract": " Virtual and augmented reality technologies have entered a new near-commodity era, accompanied by massive commercial investments, but still are subject to numerous open research questions. This special issue of IEEE Computer Graphics and Applications aims at broad views to capture the state of the art, important achievements, and impact of several areas in these dynamic disciplines. It contains three original articles that consider important aspects of VR/AR technologies and outline future research opportunities.", "num_citations": "19\n", "authors": ["1194"]}
{"title": "User-perspective augmented reality magic lens from gradients\n", "abstract": " In this paper we present a new approach to creating a geometrically-correct user-perspective magic lens and a prototype device implementing the approach. Our prototype uses just standard color cameras, with no active depth sensing. We achieve this by pairing a recent gradient domain image-based rendering method with a novel semi-dense stereo matching algorithm inspired by PatchMatch. Our stereo algorithm is simple but fast and accurate within its search area. The resulting system is a real-time magic lens that displays the correct user perspective with a high-quality rendering, despite the lack of a dense disparity map.", "num_citations": "19\n", "authors": ["1194"]}
{"title": "Rapid Creation and Publication of Digital Musical Instruments.\n", "abstract": " We describe research enabling the rapid creation of digital musical instruments and their publication to the Internet. This research comprises both high-level abstractions for making continuous mappings between audio, interactive, and graphical elements, as well as a centralized database for storing and accessing instruments. Published instruments run in most devices capable of running a modern web browser. Notation of instrument design is optimized for readability and expressivity.", "num_citations": "19\n", "authors": ["1194"]}
{"title": "Visualizing and verifying directed social queries\n", "abstract": " We present a novel visualization system that automatically classifies social network data in order to support a user\u2019s directed social queries and, furthermore, that allows the user to quickly verify the accuracy of the classifications. We model a user\u2019s friends\u2019 interests in particular topics through the creation of a crowd-sourced knowledge base comprised of terms related to userspecified semantic categories. Modeling friends in terms of these topics enables precise and efficient social querying to effectively fulfill a user\u2019s information needs. That is, our system makes it possible to quickly identify friends who have a high probability of being able to answer particular questions or of having a shared interested in particular topics. Our initial investigations indicate that our model is effective at correlating friends to these topics even without sentiment or other lexical analyses. However, even the most robust system may produce results that have false positives or false negatives due to inaccurate classifications stemming from incorrect, polysemous, or sparse data. To mitigate these errors, and to allow for more fine-grained control over the selection of friends for directed social queries, an interactive visualization exposes the results of our model and enables a human-in-the-loop approach for result analysis and verification. A qualitative analysis of our verification system indicates that the transparent representation of our shared-interest modeling algorithm leads to an increased effectiveness of the model.", "num_citations": "19\n", "authors": ["1194"]}
{"title": "Relocalization using virtual keyframes for online environment map construction\n", "abstract": " The acquisition of surround-view panoramas using a single hand-held or head-worn camera relies on robust real-time camera orientation tracking. In absence of robust tracking recovery methods, the complete acquisition process has to be re-started when tracking fails. This paper presents methodology for camera orientation relocalization, using virtual keyframes for online environment map construction. Instead of relying on real keyframes from incoming video, the proposed approach enables camera orientation relocalization by employing virtual keyframes which are distributed strategically within an environment map. We discuss our insights about a suitable number and distribution of virtual keyframes, as suggested by our experiments on virtual keyframe generation and orientation relocalization. After a shading correction step, we relocalize camera orientation in real-time by comparing the current camera frame\u00a0\u2026", "num_citations": "19\n", "authors": ["1194"]}
{"title": "A sketch-based interface for photo pop-up\n", "abstract": " We present sketch-based tools for single-view modeling which allow for quick 3D mark-up of a photograph. With our interface, detailed 3D models can be produced quickly and easily. After establishing the background geometry, foreground objects can be cut out using our novel sketch-based segmentation tools. These tools make use of the stroke speed and length to help determine the user's intentions. Depth detail is added to the scene by drawing occlusion edges. Such edges play an important part in human scene understanding, and thus provide an intuitive form of input to the modeling system. Initial results and evaluation show that our methods produce good 3D results in a short amount of time and with little user effort, demonstrating the usefulness of an intelligent sketching interface for this application domain.", "num_citations": "19\n", "authors": ["1194"]}
{"title": "Initializing markerless tracking using a simple hand gesture\n", "abstract": " We introduce a technique to establish a coordinate system for augmented reality (AR) on tabletop environments. A user's hand is tracked and the fingertips on the outstretched hand are detected, providing a camera pose estimation relative to the hand. As a user places the hand on the surface of a tabletop environment, the hand's coordinate system is propagated to the environment, detecting distinctive image features in the scene. The features are tracked fast and robustly using optical flow. In this way, a new tabletop AR environment is set up without having to carry a marker or a sophisticated tracking system to the environment itself. We also demonstrate a proof-of-concept application for establishing a tabletop AR environment and recognizing a scene when detecting its features.", "num_citations": "19\n", "authors": ["1194"]}
{"title": "Viewpoint stabilization for live collaborative video augmentations\n", "abstract": " We present a method for stabilizing live video from a moving camera for the purpose of a tele-meeting, in which a participant with an AR view onto a shared canvas collaborates with a remote user. The AR view is established without markers and using no other tracking equipment than a head-worn camera. The remote user is allowed to directly annotate the local user's view in real time on a desktop or tablet PC. The planar homographies between the reference frame and the other following frames are maintained. In effect, both the local and remote participants can annotate the physical meeting space, the local AR user through physical interaction, the remote user through our stabilized video. When tracking is lost, the remote user can still continue annotating on a frozen video frame. We tested several small demo applications with this new form of transient AR collaboration that can be established easily, on a per\u00a0\u2026", "num_citations": "19\n", "authors": ["1194"]}
{"title": "Usability engineering for complex interactive systems development\n", "abstract": " Usability engineering is a cost-effective, user centered process that ensures a high level of effectiveness, efficiency, and safety in complex interactive systems. This paper presents a brief description of usability engineering activities, and discusses our experiences with leading usability engineering activities for three very different types of interactive applications a responsive workbench-based command and control application called Dragon, a wearable augmented reality application for urban warfare called Battlefield Augmented Reality System BARS, and a head-mounted hardware device, called Nomad, for dismounted soldiers. For each application, we present our approach to usability engineering, how we tailored the usability engineering process and methods to address application-specific needs, and give results.Descriptors:", "num_citations": "19\n", "authors": ["1194"]}
{"title": "Characterizing spatial distributions of astrocytes in the mammalian retina\n", "abstract": " Motivation: In addition to being involved in retinal vascular growth, astrocytes play an important role in diseases and injuries, such as glaucomatous neuro-degeneration and retinal detachment. Studying astrocytes, their morphological cell characteristics and their spatial relationships to the surrounding vasculature in the retina may elucidate their role in these conditions.                    Results: Our results show that in normal healthy retinas, the distribution of observed astrocyte cells does not follow a uniform distribution. The cells are significantly more densely packed around the blood vessels than a uniform distribution would predict. We also show that compared with the distribution of all cells, large cells are more dense in the vicinity of veins and toward the optic nerve head whereas smaller cells are often more dense in the vicinity of arteries. We hypothesize that since veinal astrocytes are known to transport\u00a0\u2026", "num_citations": "18\n", "authors": ["1194"]}
{"title": "Computing similarity transformations from only image correspondences\n", "abstract": " We propose a novel solution for computing the relative pose between two generalized cameras that includes reconciling the internal scale of the generalized cameras. This approach can be used to compute a similarity transformation between two coordinate systems, making it useful for loop closure in visual odometry and registering multiple structure from motion reconstructions together. In contrast to alternative similarity transformation methods, our approach uses 2D-2D image correspondences thus is not subject to the depth uncertainty that often arises with 3D points. We utilize a known vertical direction (which may be easily obtained from IMU data or vertical vanishing point detection) of the generalized cameras to solve the generalized relative pose and scale problem as an efficient Quadratic Eigenvalue Problem. To our knowledge, this is the first method for computing similarity transformations that does not require any 3D information. Our experiments on synthetic and real data demonstrate that this leads to improved performance compared to methods that use 3D-3D or 2D-3D correspondences, especially as the depth of the scene increases.", "num_citations": "18\n", "authors": ["1194"]}
{"title": "Optimization of target objects for natural feature tracking\n", "abstract": " This paper investigates possible physical alterations of tracking targets to obtain improved 6DoF pose detection for a camera observing the known targets. We explore the influence of several texture characteristics on the pose detection, by simulating a large number of different target objects and camera poses. Based on statistical observations, we rank the importance of characteristics such as texturedness and feature distribution for a specific implementation of a 6DoF tracking technique. These findings allow informed modification strategies for improving the tracking target objects themselves, in the common case of man-made targets, as for example used in advertising. This fundamentally differs from and complements the traditional approach of leaving the targets unchanged while trying to optimize the tracking algorithms and parameters.", "num_citations": "18\n", "authors": ["1194"]}
{"title": "Multi-view gesture annotations in image-based 3D reconstructed scenes\n", "abstract": " We present a novel 2D gesture annotation method for use in image-based 3D reconstructed scenes with applications in collaborative virtual and augmented reality. Image-based reconstructions allow users to virtually explore a remote environment using image-based rendering techniques. To collaborate with other users, either synchronously or asynchronously, simple 2D gesture annotations can be used to convey spatial information to another user. Unfortunately, prior methods are either unable to disambiguate such 2D annotations in 3D from novel viewpoints or require relatively dense reconstructions of the environment.", "num_citations": "17\n", "authors": ["1194"]}
{"title": "Model Estimation and Selection towards Unconstrained Real-Time Tracking and Mapping\n", "abstract": " We present an approach and prototype implementation to initialization-free real-time tracking and mapping that supports any type of camera motion in 3D environments, that is, parallax-inducing as well as rotation-only motions. Our approach effectively behaves like a keyframe-based Simultaneous Localization and Mapping system or a panorama tracking and mapping system, depending on the camera movement. It seamlessly switches between the two modes and is thus able to track and map through arbitrary sequences of parallax-inducing and rotation-only camera movements. The system integrates both model-based and model-free tracking, automatically choosing between the two depending on the situation, and subsequently uses the \u201cGeometric Robust Information Criterion\u201d to decide whether the current camera motion can best be represented as a parallax-inducing motion or a rotation-only motion. It\u00a0\u2026", "num_citations": "17\n", "authors": ["1194"]}
{"title": "Evaluating system capabilities and user performance in the battlefield augmented reality system\n", "abstract": " We describe a first experiment in evaluating the system capabilities of the Battlefield Augmented Reality System, an interactive system designed to present military information to dismounted warfighters. We describe not just the current experiment, but a methodology of both system evaluation and user performance measurement in the system, and show how both types of tests will be useful in system development. We summarize results in a perceptual experiment being used to inform system design, and discuss ongoing and future experiments to which the work described herein leads.Descriptors:", "num_citations": "17\n", "authors": ["1194"]}
{"title": "Relative effects of real-world and virtual-world latency on an augmented reality training task: an ar simulation experiment\n", "abstract": " In Augmented Reality (AR), virtual objects and information are overlaid onto the user\u2019s view of the physical world and can appear to become part of the real world. Accurate registration of virtual objects is a key requirement for an effective and natural AR system, but misregistration can break the illusion of virtual objects being part of the real world and disrupt immersion. End-to-end system latency severely impacts the quality of AR registration. In this research, we present a controlled study that aims at a deeper understanding of the effects of latency on virtual and real world imagery and its influences on task performance in an AR training task. We utilize an AR Simulation approach, in which an outdoor AR training task is simulated in a high-fidelity VR system. The real and augmented portions of the AR training scenarios are simulated in VR, affording us detailed control over a variety of immersion parameters and the ability to explore the effects of different types of simulated latency. We utilized a representative task inspired by outdoor AR military training systems to compare various AR system configurations, including optical see-through and video see-through setups with both matched and unmatched levels of real and virtual objects latency. Our findings indicate that users are able to perform significantly better when virtual and real-world latencies are matched (as in the case of simulated video-see-through AR with perfect augmentation-to-real-world registration). Unequal levels of latency led to reduction in performance, even when overall latency levels were lower compared to the matched case. The relative results hold up with increased overall\u00a0\u2026", "num_citations": "16\n", "authors": ["1194"]}
{"title": "Evaluating techniques for interaction at a distance\n", "abstract": " This paper presents techniques designed to facilitate interaction at a distance in mobile augmented reality and evaluates different input controls for them. The goal of the techniques is to quickly and accurately annotate distant physical objects not yet represented in the computer's model of the scene. To this end, we present a new method for judging virtual object distance. Using virtual depth cues, the user can accurately judge depth values without having to resort to near-field-only cues such as stereo vision and parallax. To place new annotations, we need to position a virtual 3D cursor accurately relative to known landmarks. We develop four techniques for controlling this cursor and conducted a user study on the relative efficiency of these techniques.", "num_citations": "16\n", "authors": ["1194"]}
{"title": "Decision-making in abstract trust games: A user interface perspective\n", "abstract": " To understand the processes involved in trust-based judgments in a computer-mediated multi-agent setting, a user interface (UI) was developed and an experiment was devised based on the Iterated Diner's Dilemma, a variation of the n-player Prisoner's Dilemma. Analysis of the experiment resulted in two major findings: (1) UI composition and information presentation have an impact on human trust and cooperation behavior, and (2) a strong positive correlation between Situation Awareness (SA) and performance is confirmed. There was a significant effect for UI levels on our main performance metric, total participant dining points., at the p=0.041 level. Also, there was a marginal effect for UI levels on participant cooperation at the p=0.084 level. Total participant dining points and SA were strongly correlated, r (92) = 0.62. Similarly, participant cooperation and SA were strongly correlated, r (92) = 0.61.", "num_citations": "15\n", "authors": ["1194"]}
{"title": "Depth compositing for augmented reality.\n", "abstract": " MethodWe restrict ourselves to the scenario where the interacting physical objects (eg hands) can be accurately described by a color histogram. In our system, color histograms are learned from video in an initialization procedure. After initialization, the histograms are updated with newly labeled pixels from subsequent frames, to provide robustness against camera movement and illumination change.The compositing algorithm consists of two steps. First, we use a color-based graph cut to separate foreground F and background B. Second, we use a depth-based graph cut to separate F from FO, foreground pixels that are occluded by the virtual object. d is the set of disparities of the virtual object, and dmax is the maximum disparity considered by the algorithm.", "num_citations": "15\n", "authors": ["1194"]}
{"title": "Advances in tangible interaction and ubiquitous virtual reality\n", "abstract": " The first article reports on context-sensitive augmented-reality research presented at the 2007 International Symposium on Ubiquitous Virtual Reality. This student-organized event explored the use of contextual information, design principles, and effective user evaluation for developing AR applications for ubiquitous computing environments. The second article reports on The International Conference on Tangible and Embedded Interaction, the first conference series worldwide to focus on tangible and embedded interaction. The conference is interdisciplinary, covering the arts, hardware design, software toolkits for prototyping, and user studies and theory development.", "num_citations": "15\n", "authors": ["1194"]}
{"title": "Experiencing audio and music in a fully immersive environment\n", "abstract": " The UCSB Allosphere is a 3-story-high spherical instrument in which virtual environments and performances can be experienced in full immersion. The space is now being equipped with high-resolution active stereo projectors, a 3D sound system with several hundred speakers, and with tracking and interaction mechanisms.                 The Allosphere is at the same time multimodal, multimedia, multi-user, immersive, and interactive. This novel and unique instrument will be used for research into scientific visualization/auralization and data exploration, and as a research environment for behavioral and cognitive scientists. It will also serve as a research and performance space for artists exploring new forms of art. In particular, the Allosphere has been carefully designed to allow for immersive music and aural applications.                 In this paper, we give an overview of the instrument, focusing on the audio\u00a0\u2026", "num_citations": "15\n", "authors": ["1194"]}
{"title": "Semi-automated placement of annotations in videos\n", "abstract": " In this paper, we present a framework for the insertion and semiautomated placement of visual annotations into video footage. Visually overlaid annotations are very common in telecasts (eg, statistics in sports broadcasts, banner advertisements), where they are strategically placed by hand, or incorporated into the video stream using elaborate camera tracking and scene modeling techniques. Such information is not commonly available for raw or unprepared videos. We look at the problem of automatically placing annotations within the space and time domains of unprepared videos using computer vision and image analysis techniques. We use measures of intensity and color uniformity, motion information, and a simple model of cluttered screen space to determine regions in the video that are of relatively minor importance to the human visual perception. We present a taxonomy of visual annotations, a subset of whose we realize as automatically placed video overlays, implemented in our prototype system named DAVid (Digital Annotations for Video). We conclude with an outlook of potential applications of these techniques in interactive television.", "num_citations": "15\n", "authors": ["1194"]}
{"title": "Structure and motion in urban environments using upright panoramas\n", "abstract": " Image-based modeling of urban environments is a key component of enabling outdoor, vision-based augmented reality applications. The images used for modeling may come from off-line efforts, or online user contributions. Panoramas have been used extensively in mapping cities and can be captured quickly by an end-user with a mobile phone. In this paper, we describe and evaluate a reconstruction pipeline for upright panoramas taken in an urban environment. We first describe how panoramas can be aligned to a common vertical orientation using vertical vanishing point detection, which we show to be robust for a range of inputs. The orientation sensors in modern cameras can also be used to correct the vertical orientation. Secondly, we introduce a pose estimation algorithm, which uses knowledge of a common vertical orientation as a simplifying constraint. This procedure is shown to reduce pose\u00a0\u2026", "num_citations": "14\n", "authors": ["1194"]}
{"title": "Interactive water streams with sphere scan conversion\n", "abstract": " Fluid simulations require efficient dynamics, surface extraction and rendering in order to achieve real time interaction. We present a novel technique for the surface extraction of stream-shaped fluid simulations represented as particles. Typical surface extraction methods for particles combine implicit function evaluation with the marching cubes algorithm. In our approach, we dynamically update vertex positions in pre-generated geometry to efficiently construct and render fluid surfaces. Cylinders are wrapped to water streams composed of particles, with simulation and polygonization on the CPU, and shadows and lighting on the GPU. While limited to stream-shaped fluids, our technique is significantly faster than marching cubes, scales well with resolution and number of particles and, unlike point-based rendering, produces true 3D polygonal surfaces.", "num_citations": "14\n", "authors": ["1194"]}
{"title": "Rendering Realistic Trees and Forests in Real Time.\n", "abstract": " Real-Time rendering of realistic trees on common graphics hardware represents a big challenge due to their inherent geometric complexity. In most cases, trees are composed of hundreds of thousands of leaves and branches with complex lighting interrelations. We present novel techniques to render and animate photorealistic trees in real-time. The described techniques are easily implemented with commonly available graphics cards, making them suitable to applications such as visual simulations and video games. Polygon counts are significantly reduced by the use of simplified textured geometry for minor branches and bill-boarded leaf textures. Fast and realistic lighting and shadowing of the leaves and surroundings enhances realism. We animate the tree branches and leaf textures using simple vertex shaders, creating a realistic effect of the tree swaying in the wind. Discrete levels of detail allow rendering of a large number of trees, making it possible to represent realistic forest scenes made of 1000-1500 trees.", "num_citations": "14\n", "authors": ["1194"]}
{"title": "A framework for generic inter-application interaction for 3D AR environments\n", "abstract": " We present a generic software framework enabling inter-application interaction in a 3D augmented reality environment. Our framework is built within a 3D AR window manager centered around ARToolkit. The user interface presents users with a simple visual mechanism to establish communications among applications in a generic way. The application interface is designed for ease of development and maximum end-user flexibility. We showcase some novel 3D applications and their interactions within the framework, demonstrating the new possibilities created by 3D interfaces. A classification is presented to categorize the applications' roles in such interactions, to help steer development. Finally, we discuss future possibilities for applications and interactions within such a framework.", "num_citations": "14\n", "authors": ["1194"]}
{"title": "Personalizing content presentation on large 3d head-up displays\n", "abstract": " Drivers' urge to access content on smartphones while driving causes a high number of fatal accidents every year. We explore 3D full-windshield size head-up displays as an opportunity to present such content in a safer manner. In particular, we look into how drivers would personalize such displays and whether it can be considered safe. Firstly, by means of an online survey we identify types of content users access on their smartphones while driving and whether users are interested in the same content on a head-up display. Secondly, we let drivers design personalized 3D layouts and assess how personalization impacts on driving safety. Thirdly, we compare personalized layouts to a one-fits-all layout concept in a 3D driving simulator study regarding safety. We found that drivers' content preferences diverge largely and that most of the personalized layouts do not respect safety sufficiently. The one-fits-all\u00a0\u2026", "num_citations": "13\n", "authors": ["1194"]}
{"title": "Trust and situation awareness in a 3-player diner's dilemma game\n", "abstract": " This paper studies the relationship between trust and Situation Awareness (SA) in a 3-Player Iterated Diner's Dilemma game. We ran an experiment in which 24 participants each played against two computer opponents for six blocks of gameplay, with different opponent strategies in each block. Based on SA theory and design principles, we developed three different user interfaces, each supporting a specific SA Level. Each SA Level is inclusive of components from the previous level(s). We assess several trust-related metrics during the study, including percentage of cooperation over time and subjective level of self-reported trust towards the opponents, and analyze the interdependencies of trust, SA, and opponent strategy. Results from the experiment reveal highest levels of cooperation at SA Level 1 overall, and a higher level of cooperation for the group of cooperation-encouraging opponent strategies at SA\u00a0\u2026", "num_citations": "13\n", "authors": ["1194"]}
{"title": "WiGipedia: A Tool for Improving Structured Data in Wikipedia\n", "abstract": " Wikipedia is emerging as the dominant global knowledge repository. Recently, large numbers of users have collaborated to produce more structured information in the so called \"info boxes''. However, editing this data requires even more care than editing standard wikitext, as one must follow arcane template syntax. This paper describes WiGipedia, a novel tool which provides an alternative to the traditional approach, by supporting editing of structured wiki data through two intuitive and interactive interfaces, facilitating user input on both tabular and graph-based representations of structured data. The tool allows users to identify and correct inconsistencies that are otherwise hidden across multiple articles. Furthermore, a novel recommendation algorithm is applied to assist users in their contribution to the wiki. The paper discusses design, implementation details, and results of a usability study in which the system\u00a0\u2026", "num_citations": "13\n", "authors": ["1194"]}
{"title": "Consigalo: multi-user face-to-face interaction on immaterial displays\n", "abstract": " In this paper, we describe and discuss interaction techniques and interfaces enabled by immaterial displays. Dual-sided projection allows casual face-to-face interaction between users, with computer-generated imagery in-between them. The immaterial display imposes minimal restrictions to the movements or communication of the users.As an example of these novel possibilities, we provide a detailed description of our Consigalo gaming system, which creates an enhanced gaming experience featuring sporadic and unencumbered interaction. Consigalo utilizes a robust 3D tracking system, which supports multiple simultaneous users on either side of the projection surface. Users manipulate graphics that are floating in mid-air with natural gestures. We have also added a responsive and adaptive sound track to further immerse the users in the interactive experience. We describe the technology used in the system, the innovative aspects compared to previous largescreen gaming systems, the gameplay and our lessons learned from designing and implementing the interactions, visuals and the auditory feedback.", "num_citations": "13\n", "authors": ["1194"]}
{"title": "Feel the globe: Enhancing the perception of immersive spherical visualizations with tangible proxies\n", "abstract": " Recent developments in the commercialization of virtual reality open up many opportunities for enhancing human interaction with three-dimensional objects and visualizations. Spherical visualizations allow for convenient exploration of certain types of data. Our tangible sphere, exactly aligned with the sphere visualizations shown in VR, implements a very natural way of interaction and utilizes senses and skills trained in the real world. In a lab study, we investigate the effects of the perception of actually holding a virtual spherical visualization in hands. As use cases, we focus on surface visualizations that benefit from or require a rounded shape. We compared the usage of two differently sized acrylic glass spheres to a related interaction technique that utilizes VR controllers as proxies. On the one hand, our work is motivated by the ability to create in VR a tangible, lightweight, handheld spherical display that can\u00a0\u2026", "num_citations": "12\n", "authors": ["1194"]}
{"title": "Inspection Mechanisms for Community-based Content Discovery in Microblogs.\n", "abstract": " This paper presents a formative evaluation of an interface for inspecting microblog content. This novel interface introduces filters by communities, and network structure, as well as ranking of tweets. It aims to improving content discovery, while maintaining content relevance and sense of user control. Participants in the US and the UK interacted with the interface in semi-structured interviews. In two iterations of the same study (n= 4, n= 8), we found that the interface gave users a sense of control. Users asked for an active selection of communities, and a more fine-grained functionality for saving individual \u2018favorite\u2019users. Users also highlighted unanticipated uses of the interface such as iteratively discovering new communities to follow, and organizing events. Informed by these studies, we propose improvements and a mock-up for an interface to be used for future larger scale experiments for exploring microblog content.", "num_citations": "12\n", "authors": ["1194"]}
{"title": "Visualizing targeted audiences\n", "abstract": " Users of social networks can be passionate about sharing their political convictions, art projects, or business ventures. They often want to direct their social interactions to certain people in order to start collaborations or to raise awareness about issues they support. However, users generally have scattered, unstructured information about the characteristics of their audiences, making it difficult for them to deliver the right messages or interactions to the right people. Existing audience-targeting tools allow people to select potential candidates based on predefined lists, but the tools provide few insights about whether or not these people would be appropriate for a specific type of communication. We introduce an online tool, Hax, to explore instead the idea of using interactive data visualizations to help people dynamically identify audiences for their different sharing efforts. We provide the results of a preliminary\u00a0\u2026", "num_citations": "12\n", "authors": ["1194"]}
{"title": "Augmented textual data viewing in 3d visualizations using tablets\n", "abstract": " Many data sets contain structural 3D components along with associated textual data. While structural data is often best visualized on large stereoscopic displays, such displays can pose problems presenting accompanying textual information. Chief among these problems is a tradeoff between size dependent legibility of text and the occlusion of structural data if text is presented at larger sizes. Our solution to this problem integrates structural data shown on a large display while users select features of the structure and view the associated textual data on personal tablets. Our solution also lends itself to collaborative browsing tasks. In our initial implementation each user can individually select structural components and view the associated text on their own tablet; thus, everyone becomes an active participant in data mining. When individual users find textual data they deem of interest to the group they can share it with\u00a0\u2026", "num_citations": "12\n", "authors": ["1194"]}
{"title": "Augmented Reality: Information for Workplace Decision-Makers, Managers, Workers and Researchers\n", "abstract": " Until recently, Augmented Reality (AR) technology has rarely been discussed outside of the computer science world. It has taken years for this technology to become closer to a stable existence, and will most likely take several more years before it will be used by average citizens. However, the technology does exist, it has been applied in several areas, and research is being done to create even more stable systems that are adaptable to various environments. For this reason, it is necessary for decision-makers in establishments where education and training, knowledge distribution, and individual and collaborative task completion are essential, to be aware of this technology, its abilities, and the possible impacts to common workspaces and workers. The purpose of this chapter is to inform decision-makers of AR\u2019s history, the completed research and current applications of AR, possible impacts to managers and workers, and the future trends of the technology.", "num_citations": "12\n", "authors": ["1194"]}
{"title": "Real-time Rendering with Wavelet-Compressed Multi-Dimensional Datasets on the GPU\n", "abstract": " We present a method for using large, high dimension and high dynamic range datasets on modern graphics hardware. Datasets are preprocessed with a discrete wavelet transform, insignificant coefficients are removed, and the resulting compressed data is stored in standard 2D texture memory. A set of drop-in shader functions allows any shader program to sample the wavelet-encoded textures without additional programming. We demonstrate our technique in three applications\u2013a terrain renderer with a 163842 sample RGB texture map, a volume renderer with a 5123 sample 3D dataset, and a complex material shader using an unapproximated BRDF dataset, sampled at 644 in RGB.", "num_citations": "12\n", "authors": ["1194"]}
{"title": "CVR-analyzer: a tool for analyzing cinematic virtual reality viewing patterns\n", "abstract": " Cinematic Virtual Reality has been increasing in popularity over the last years. Watching omnidirectional movies with head mounted displays, viewers can freely choose the direction of view, and thus the visible section of the movie. In order to explore the users' viewing behavior, methods are needed for collecting and analyzing data. We developed an analyzing tool, the CVR-Analyzer, which can be used for inspecting head pose and eye tracking data of viewers experiencing CVR movies. The visualized data are displayed on a flattened projection of the movie as flexibly controlled augmenting annotations, such as tracks or heatmaps, synchronously with the time code of the movie and allow inspecting and comparing the users' viewing behavior in different use cases.", "num_citations": "11\n", "authors": ["1194"]}
{"title": "What am I not seeing? An interactive approach to social content discovery in microblogs\n", "abstract": " In this paper, we focus on the informational and user experience benefits of user-driven topic exploration in microblog communities, such as Twitter, in an inspectable, controllable and personalized manner. To this end, we introduce \u201cHopTopics\u201d \u2013 a novel interactive tool for exploring content that is popular just beyond a user\u2019s typical information horizon in a microblog, as defined by the network of individuals that they are connected to. We present results of a user study (N=122) to evaluate HopTopics with varying complexity against a typical microblog feed in both personalized and non-personalized conditions. Results show that the HopTopics system, leveraging content from both the direct and extended network of a user, succeeds in giving users a better sense of control and transparency. Moreover, participants had a poor mental model for the degree of novel content discovered when presented with non\u00a0\u2026", "num_citations": "11\n", "authors": ["1194"]}
{"title": "An analysis of student behavior in two massive open online courses\n", "abstract": " Massive open online courses (MOOCs) have high potential for improving education worldwide, but understanding of student behavior and situations is difficult to achieve in online settings. Network analytics and visualizations can assist instructors with supporting understanding of student behavior as courses unfold. In this work, we perform a visual comparative analysis of two different MOOC courses to analyze the impacts of course structure differences and demonstrate the benefits of visual network analysis in this context. We present several insights: (1) behavior features that are best for prediction of student attrition varied with course structure, (2) a large proportion (about 35%) of students never received a reply to their original post and this was correlated with an eventual dropout, and (3) students that received a reply to their original post were twice as likely to post again. We contribute several information\u00a0\u2026", "num_citations": "11\n", "authors": ["1194"]}
{"title": "Mixed reality simulation with physical mobile display devices\n", "abstract": " This paper presents the design and implementation of a system for simulating mixed reality in setups combining mobile devices and large backdrop displays. With a mixed reality simulator, one can perform usability studies and evaluate mixed reality systems while minimizing confounding variables. This paper describes how mobile device AR design factors can be flexibly and systematically explored without sacrificing the touch and direct unobstructed manipulation of a physical personal MR display. First, we describe general principles to consider when implementing a mixed reality simulator, enumerating design factors. Then, we present our implementation which utilizes personal mobile display devices in conjunction with a large surround-view display environment. Standing in the center of the display, a user may direct a mobile device, such as a tablet or head-mounted display, to a portion of the scene, which\u00a0\u2026", "num_citations": "11\n", "authors": ["1194"]}
{"title": "Image-space correction of AR registration errors using graphics hardware\n", "abstract": " directly on top of physical objects in a video scene. Registration accuracy is a serious problem in these cases since any imprecisions are immediately apparent as virtual and physical edges and features coincide. We present a hardware-accelerated image-based post-processing technique that adjusts rendering of virtual geometry to better match edges present in images of a physical scene, reducing the visual effect of registration errors from both inaccurate tracking and oversimplified modeling. Our algorithm is easily integrable with existing AR applications, having no dependency on the underlying tracking technique. We use the advanced programmable capabilities of modern graphics hardware to achieve high performance without burdening the CPU.", "num_citations": "11\n", "authors": ["1194"]}
{"title": "POLAR: portable, optical see-through, low-cost augmented reality\n", "abstract": " We describe POLAR, a portable, optical see-through, low-cost augmented reality system, which allows a user to see annotated views of small to medium-sized physical objects in an unencumbered way. No display or tracking equipment needs to be worn. We describe the system design, including a hybrid IR/vision head-tracking solution, and present examples of simple augmented scenes. POLAR's compactness could allow it to be used as a lightweight and portable PC peripheral for providing mobile users with on-demand AR access in field work.", "num_citations": "11\n", "authors": ["1194"]}
{"title": "Real-time re-textured geometry modeling using microsoft hololens\n", "abstract": " We implemented live-textured geometry model creation with immediate coverage feedback visualizations in AR on the Microsoft HoloLens. A user walking and looking around a physical space can create a textured model of the space, ready for remote exploration and AR collaboration. Out of the box, a HoloLens builds a triangle mesh of the environment while scanning and being tracked in a new environment. The mesh contains vertices, triangles, and normals, but not color. We take the video stream from the color camera and use it to color a UV texture to be mapped to the mesh. Due to the limited graphics memory of the HoloLens, we use a fixed-size texture. Since the mesh generation dynamically changes in real time, we use an adaptive mapping scheme that evenly distributes every triangle of the dynamic mesh onto the fixed-size texture and adapts to new geometry without compromising existing color data\u00a0\u2026", "num_citations": "10\n", "authors": ["1194"]}
{"title": "Field-of-view extension for VR viewers\n", "abstract": " We present a prototype of a smartphone-based virtual reality (VR) viewer, which can cover nearly the full human field-of-view (FOV). The prototype suggests that such extensions are feasible ways to significantly expand the FOV of standard VR viewers. The concept can be employed for future VR viewers or it can even be retrofitted to some existing ones.", "num_citations": "10\n", "authors": ["1194"]}
{"title": "Extreme field-of-view for head-mounted displays\n", "abstract": " We present novel optics and head-mounted display (HMD) prototypes, which have the widest reported field-of-view (FOV), and which can cover the full human FOV or even beyond. They are based on lenses and screens which are curved around the eyes. While this is still work-in-progress, the HMD prototypes and user tests suggest a feasible approach to significantly expand the FOV of HMDs.", "num_citations": "10\n", "authors": ["1194"]}
{"title": "Structuring the space: a study on enriching node-link diagrams with visual references\n", "abstract": " Exploring large visualizations that do not fit in the screen raises orientation and navigation challenges. Structuring the space with additional visual references such as grids or contour lines provide spatial landmarks that may help viewers form a mental model of the space. However, previous studies report mixed results regarding their utility. While some evidence showed that grid and other visual embellishments improve memorability, experiments with contour lines suggest otherwise. In this work, we describe an evaluation framework to capture the impact of introducing visual references in node-link diagrams. We present the results of three controlled experiments that deepen our understanding on enriching large visualization spaces with visual structures. In particular, we provide the first tangible evidence that contour lines have significant benefits when navigating large node-link diagrams.", "num_citations": "10\n", "authors": ["1194"]}
{"title": "TopicLens: An interactive recommender system based on topical and social connections\n", "abstract": " This paper describes TopicLens, an interactive tool for exploring and recommending items within large corpora, based on both social metadata and topical associations. The system uses a hybrid visualization model that represents topics and content items side by side, allowing the user to actively explore recommendations rather than passively viewing them. The approach provides insight into the composition of relevant topics as they relate to the meta-data of underlying texts. We describe a novel approach to sorting and filtering, which can be topic or document-driven, and two novel interaction styles termed \u201cview inversion\u201d and \u201chuman-review\u201d, each of which enable novel perspectives on topic modeled sets of documents. To evaluate the system, three use cases are presented to highlight interesting insights across three different data sets using our novel recommendation interface.", "num_citations": "10\n", "authors": ["1194"]}
{"title": "Evaluating the effects of tracker reliability and field of view on a target following task in augmented reality\n", "abstract": " We examine the effect of varying levels of immersion on the performance of a target following task in augmented reality (AR) X-ray vision. We do this using virtual reality (VR) based simulation. We analyze participant performance while varying the field of view of the AR display, as well as the reliability of the head tracking sensor as our components of immersion. In low reliability conditions, we simulate sensor dropouts by disabling the augmented view of the scene for brief time periods. Our study gives insight into the effect of tracking sensor reliability, as well as the relationship between sensor reliability and field of view on user performance in a target following task in a simulated AR system.", "num_citations": "10\n", "authors": ["1194"]}
{"title": "Interactive manipulation of large graph layouts\n", "abstract": " We present two techniques for interactive graph layout manipulation which take inspiration from the fields of 3D modeling, mesh deformation, and static graph drawing. The first technique uses a multigrid method for modeling and animating large 3D meshes, the second employs ideas from a simpler mesh deformation scheme together with a basic graph searching algorithm and a user interface to control region of influence. We show how these techniques along with a set of basic graph refinement tools can be used interactively to produce informative visualizations based on graph connectivity alone, and then fine tune existing layouts to reveal insights into specific focus regions. We assume arbitrary, large, connected, undirected graphs, and draw the entire graph in 3D. Our techniques are designed to run at interactive rates on a standard desktop or laptop computer, even for graphs with hundreds of thousands of nodes. We present timing results and images of layouts generated by our techniques.", "num_citations": "10\n", "authors": ["1194"]}
{"title": "An experimental hybrid user interface for collaboration\n", "abstract": " We present EMMIE Environment Management for Multiuser Information Environments, an experimental user interface to a collaborative augmented environment. Users share a 3D virtual space and manipulate virtual objects representing information to be discussed. This approach not only allows for cooperation in a shared physical space, but also addresses telecollaboration in physically separate but virtually shared spaces. We refer to EMMIE as a hybrid user interface because it combines a variety of different technologies and techniques, including virtual elements such as 3D widgets, and physical objects such as tracked displays and input devices. See through head worn displays overlay the virtual environment on the physical environment. Our research prototype includes additional 2D and 3D displays, ranging from palmsized to wallsized, allowing the most appropriate one to be used for any task. Objects can be moved among displays including across dimensionalities through drag drop. In analogy to 2D window managers, we describe a prototype implementation of a shared 3D environment manager that is distributed across displays, machines, and operating systems. We also discuss two methods we are exploring for handling information privacy in such an environment.Descriptors:", "num_citations": "10\n", "authors": ["1194"]}
{"title": "Combining dynamic physical and virtual illumination in augmented reality\n", "abstract": " With rapidly advancing graphics hardware available, real-time graphics techniques have become significantly more capable of handling photorealistic effects. The application of these techniques to augmented reality is creating new possibilities for the seamless integration of virtual and physical objects. We demonstrate two methods for simulating light transport. The first renders virtual objects of arbitrary Phong material with light from the physical environment. The second adds virtual light onto real-time camera imagery of the physical world. Both techniques yield fully dynamic results at interactive framerates with a minimum of offline preparation.", "num_citations": "10\n", "authors": ["1194"]}
{"title": "Augmentation strategies for learning with noisy labels\n", "abstract": " Imperfect labels are ubiquitous in real-world datasets. Several recent successful methods for training deep neural networks (DNNs) robust to label noise have used two primary techniques: filtering samples based on loss during a warm-up phase to curate an initial set of cleanly labeled samples, and using the output of a network as a pseudo-label for subsequent loss calculations. In this paper, we evaluate different augmentation strategies for algorithms tackling the\"\" learning with noisy labels\"\" problem. We propose and examine multiple augmentation strategies and evaluate them using synthetic datasets based on CIFAR-10 and CIFAR-100, as well as on the real-world dataset Clothing1M. Due to several commonalities in these algorithms, we find that using one set of augmentations for loss modeling tasks and another set for learning is the most effective, improving results on the state-of-the-art and other previous methods. Furthermore, we find that applying augmentation during the warm-up period can negatively impact the loss convergence behavior of correctly versus incorrectly labeled samples. We introduce this augmentation strategy to the state-of-the-art technique and demonstrate that we can improve performance across all evaluated noise levels. In particular, we improve accuracy on the CIFAR-10 benchmark at 90% symmetric noise by more than 15% in absolute accuracy, and we also improve performance on the Clothing1M dataset.", "num_citations": "9\n", "authors": ["1194"]}
{"title": "User-perspective AR magic lens from gradient-based IBR and semi-dense stereo\n", "abstract": " We present a new approach to rendering a geometrically-correct user-perspective view for a magic lens interface, based on leveraging the gradients in the real world scene. Our approach couples a recent gradient-domain image-based rendering method with a novel semi-dense stereo matching algorithm. Our stereo algorithm borrows ideas from PatchMatch, and adapts them to semi-dense stereo. This approach is implemented in a prototype device build from off-the-shelf hardware, with no active depth sensing. Despite the limited depth data, we achieve high-quality rendering for the user-perspective magic lens.", "num_citations": "9\n", "authors": ["1194"]}
{"title": "Tag me maybe: Perceptions of public targeted sharing on facebook\n", "abstract": " Social network sites allow users to publicly tag people in their posts. These tagged posts allow users to share to both the general public and a targeted audience, dynamically assembled via notifications that alert the people mentioned. We investigate people's perceptions of this mixed sharing mode through a qualitative study with 120 participants. We found that individuals like this sharing modality as they believe it strengthens their relationships. Individuals also report using tags to have more control of Facebook's ranking algorithm, and to expose one another to novel information and people. This work helps us understand people's complex relationships with the algorithms that mediate their interactions with each another. We conclude by discussing the design implications of these findings.", "num_citations": "9\n", "authors": ["1194"]}
{"title": "Examining the equivalence of simulated and real AR on a visual following and identification task\n", "abstract": " Mixed Reality (MR) simulation, in which a Virtual Reality (VR) system is used to simulate both the real and virtual components of an Augmented Reality (AR) system, has been proposed as a method for evaluating AR systems with greater levels of experimental control. However, factors such as the latency of the MR simulator may impact the validity of experimental results obtained with MR simulation. We present a study evaluating the effects of simulator latency on the equivalence of results from an MR simulator and a real AR system. We designed an AR experiment which required the participants to visually follow a virtual pipe around a small room filled with real targets and to find and identify the targets which were intersected by the pipe. We show that, with a 95% confidence interval, the results from all three simulated AR conditions fall well within one standard deviation of the real AR case.", "num_citations": "9\n", "authors": ["1194"]}
{"title": "Interactive visualization of retinal astrocyte images\n", "abstract": " Analyzing high-resolution images of astrocytes is important in understanding the diseases, such as glaucoma and retinal detachment, to which astrocytes are known to become reactive. This is challenging because the cells are small, homogeneous, and closely packed. We propose an interactive visualization system designed for such images. Our system employs a probabilistic segmentation algorithm to help distinguish between cells. Design decisions on visualization and interactions were made based on the needs of the scientists, resulting in a visualization that shows the details of individual cells in the context of a large image mosaic. Our interactive system brings patterns of information to the surface, conveys uncertainty, and serves as a tool for astrocyte research.", "num_citations": "9\n", "authors": ["1194"]}
{"title": "A setup for evaluating detectors and descriptors for visual tracking\n", "abstract": " In many cases, visual tracking is based on detecting, describing, and then matching local features. A variety of algorithms for these steps have been proposed and used in tracking systems, leading to an increased need for independent comparisons. However, existing evaluations are geared towards object recognition and image retrieval, and their results have limited validity for real-time visual tracking. We present a setup for evaluation of detectors and descriptors which is geared towards visual tracking in terms of testbed, candidate algorithms and performance criteria. Most notably, our testbed consists of video streams with several thousand frames naturally affected by noise and motion blur.", "num_citations": "9\n", "authors": ["1194"]}
{"title": "An immaterial pseudo-3D display with 3D Interaction\n", "abstract": " Many techniques have been developed to create the impression of a 3D image floating in mid-air. These technologies all attempt to artificially recreate the depth cues we naturally perceive when viewing a real 3D object. For example, stereoscopic imaging simulates binocular disparity cues by presenting slightly different images of the same scene to the left and right eyes, which is interpreted by the brain as a single 3D image. Virtual reality applications tend to track the user\u2019s head and render different views of the 3D object depending on where the user is in relation to the object, to simulate motion parallax. 3D applications in general simulate realistic imagery with perspective and complex shading algorithms to create the impression that the virtual object is seamlessly integrated with the 3D scene. We have applied these simulated depth cues to a novel immaterial display technology [1], creating an engaging new\u00a0\u2026", "num_citations": "9\n", "authors": ["1194"]}
{"title": "A tangible spherical proxy for object manipulation in augmented reality\n", "abstract": " In this paper, we explore how a familiarly shaped object can serve as a physical proxy to manipulate virtual objects in Augmented Reality (AR) environments. Using the example of a tangible, handheld sphere, we demonstrate how irregularly shaped virtual objects can be selected, transformed, and released. After a brief description of the implementation of the tangible proxy, we present a buttonless interaction technique suited to the characteristics of the sphere. In a user study (N = 30), we compare our approach with three different controller-based methods that increasingly rely on physical buttons. As a use case, we focused on an alignment task that had to be completed in mid-air as well as on a flat surface. Results show that our concept has advantages over two of the controller-based methods regarding task completion time and user ratings. Our findings inform research on integrating tangible interaction into AR\u00a0\u2026", "num_citations": "8\n", "authors": ["1194"]}
{"title": "PanoTrace: interactive 3D modeling of surround-view panoramic images in virtual reality\n", "abstract": " Full-surround panoramic imagery can provide a viewer with a high-resolution visual impression of a pictured real or realistically rendered environment, but it does not provide as high a level of immersion as modeled 3D geometry can, when viewed with virtual reality (VR) headsets or projection-based setups. In this paper, we demonstrate that augmenting panorama images with geometrical models can be done simply in VR itself and can significantly increase the feeling of immersion a viewer experiences. We propose a novel interactive modeling tool that allows users to model geometry depicted in a surround-panoramic scene directly in VR, utilizing projection mapping of the panorama on top of the evolving geometry. The user interface is intuitive and allows novice users to produce geometry that approximates ground truth models sufficiently to enhance a user's VR viewing experience. We designed a user study\u00a0\u2026", "num_citations": "8\n", "authors": ["1194"]}
{"title": "Gesture-based augmented reality annotation\n", "abstract": " Drawing annotations with 3D hand gestures in augmented reality is useful for creating visual and spatial references in the real world, especially when these gestures can be issued from a distance. Different techniques exist for highlighting physical objects with hand-drawn annotations from a distance, assuming an approximate 3D scene model (e.g., as provided by the Microsoft HoloLens). However, little is known about user preference and performance of such methods for annotating real-world 3D environments. To explore and evaluate different 3D hand-gesture-based annotation drawing methods, we have developed an annotation drawing application using the HoloLens augmented reality development platform. The application can be used for highlighting objects at a distance and multi-user collaboration by annotating in the real world.", "num_citations": "8\n", "authors": ["1194"]}
{"title": "A compact, wide-FOV optical design for head-mounted displays\n", "abstract": " We present a new optical design for head-mounted displays (HMD) which has an exceptionally wide field of view (FOV). It can cover even the full human FOV. It is based on seamless lenses and screens curved around the eyes. The proof-of-concept prototypes are promising, and one of them far exceeds the human FOV, although the effective FOV is limited by the anatomy of the human head. The presented optical design has advantages such as compactness, light weight, low cost and super-wide FOV with high resolution. Even though this is still work-in-progress and display functionality is not yet implemented, it suggests a feasible way to significantly expand the FOV of HMDs.", "num_citations": "8\n", "authors": ["1194"]}
{"title": "Eye gaze correction with a single webcam based on eye-replacement\n", "abstract": " In traditional video conferencing systems, it is impossible for users to have eye contact when looking at the conversation partner\u2019s face displayed on the screen, due to the disparity between the locations of the camera and the screen. In this work, we implemented a gaze correction system that can automatically maintain eye contact by replacing the eyes of the user with the direct looking eyes (looking directly into the camera) captured in the initialization stage. Our real-time system has good robustness against different lighting conditions and head poses, and it provides visually convincing and natural results while relying only on a single webcam that can be positioned almost anywhere around the screen.", "num_citations": "8\n", "authors": ["1194"]}
{"title": "Spatial interaction in a multiuser immersive instrument\n", "abstract": " The AlloSphere provides multiuser spatial interaction through a curved surround screen and surround sound. Two projects illustrate how researchers employed the AlloSphere to investigate the combined use of personal-device displays and the shared display. Another two projects combined multiuser interaction with multiagent systems. These projects point to directions for future ensemble-style collaborative interaction.", "num_citations": "8\n", "authors": ["1194"]}
{"title": "Fast and scalable keypoint recognition and image retrieval using binary codes\n", "abstract": " In this paper we report an evaluation of keypoint descriptor compression using as little as 16 bits to describe a single keypoint. We use spectral hashing to compress keypoint descriptors, and match them using the Hamming distance. By indexing the keypoints in a binary tree, we can quickly recognize keypoints with a very small database, and efficiently insert new keypoints. Our tests using image datasets with perspective distortion show the method to enable fast keypoint recognition and image retrieval with a small code size, and point towards potential applications for scalable visual SLAM on mobile phones.", "num_citations": "8\n", "authors": ["1194"]}
{"title": "Sphere in hand: Exploring tangible interaction with immersive spherical visualizations\n", "abstract": " The emerging possibilities of data analysis and exploration in virtual reality raise the question of how users can be best supported during such interactions. Spherical visualizations allow for convenient exploration of certain types of data. Our tangible sphere, exactly aligned with the sphere visualizations shown in VR, implements a very natural way of interaction and utilizes senses and skills trained in the real world. This work is motivated by the prospect to create in VR a low-cost, tangible, robust, handheld spherical display that would be difficult or impossible to implement as a physical display. Our concept enables it to gain insights about the impact of a fully tangible embodiment of a virtual object on task performance, comprehension of patterns, and user behavior. After a description of the implementation we discuss the advantages and disadvantages of our approach, taking into account different handheld\u00a0\u2026", "num_citations": "7\n", "authors": ["1194"]}
{"title": "A study of dynamic information display and decision-making in abstract trust games\n", "abstract": " User interfaces that display dynamic information have the ability to influence decision makers in networked settings where many individuals collaborate. To understand how varying levels of information support affects behavior (cooperation vs. defection) in a social dilemma, a user interface (UI) was developed and an online experiment (N=901) was conducted based on the iterated Diner\u2019s Dilemma, a version of the n-player Prisoner\u2019s Dilemma.There were 3 main findings: (1) as more UI support was given, participants became more likely to retaliate against defection than they were to initiate defection; (2) participant situation awareness (SA) increased as more UI support was given but decreased in the presence of forgiving co-actors; and (3) the need for UI support to make good decisions was diminished as co-actors became more likely to exploit. These results can inform the design of information support tools for\u00a0\u2026", "num_citations": "7\n", "authors": ["1194"]}
{"title": "A Superwide-FOV Optical Design for Head-Mounted Displays.\n", "abstract": " We present a new optical design for head-mounted displays (HMD) that has an exceptionally wide field of view (FOV). It can cover even the full human FOV. It is based on seamless lenses and screens curved around the eyes. We constructed several compact and lightweight proof-of-concept prototypes of the optical design. One of them far exceeds the human FOV, although the anatomy of the human head limits the effective FOV. The presented optical design has advantages such as compactness, light weight, low cost and superwide FOV with high resolution. The prototypes are promising, and though this is still work-in-progress and display functionality is not yet implemented, it suggests a feasible way to significantly expand the FOV of HMDs.", "num_citations": "7\n", "authors": ["1194"]}
{"title": "Improved outdoor augmented reality through \u201cGlobalization\u201d\n", "abstract": " Despite the major interest in live tracking and mapping (e.g., SLAM), the field of augmented reality has yet to truly make use of the rich data provided from large-scale reconstructions generated by structure from motion. This dissertation focuses on extensible tracking and mapping for large-scale reconstructions that enables SfM and SLAM to operate cooperatively to mutually enhance the performance. We describe a multi-user, collaborative augmented reality system that will collectively extend and enhance reconstructions of urban environments at city-scales. Contrary to current outdoor augmented reality systems, this system is capable of continuous tracking through areas previously modeled as well as new, undiscovered areas. Further, we describe a new process called globalization that propagates new visual information back to the global model. Globalization allows for continuous updating of the 3D models\u00a0\u2026", "num_citations": "7\n", "authors": ["1194"]}
{"title": "Evaluating the impact of recovery density on augmented reality tracking\n", "abstract": " Natural feature tracking systems for augmented reality are highly accurate, but can suffer from lost tracking. When registration is lost, the system must be able to re-localize and recover tracking. Likewise, when a camera is new to a scene, it must be able to perform the related task of localization. Localization and re-localization can only be performed at certain points or when viewing particular objects or parts of the scene with a sufficient number and quality of recognizable features to allow for tracking recovery. We explore how the density of such recovery locations/poses influences the time it takes users to resume tracking. We focus our evaluation on two generalized techniques for localization: keyframe-based and model-based. For the keyframe-based approach we assume a constant collection rate for keyframes. We find that at practical collection rates, the task of localization to a previously acquired keyframe that\u00a0\u2026", "num_citations": "7\n", "authors": ["1194"]}
{"title": "A repository for the evaluation of image-based orientation tracking solutions\n", "abstract": " We describe an online repository we have developed for evaluating image-based orientation tracking methods. We have collected many videos which contain rotation-only camera movement under a wide variety of conditions, such as changing illumination, position, and rotation speed and direction. The dataset is useful for testing the robustness of orientation tracking systems, as well as other systems which use panoramas as a data source. In this paper we discuss the design of the repository and give examples of various uses of the imagery and other data it contains.", "num_citations": "7\n", "authors": ["1194"]}
{"title": "Implementation and evaluation of a 3D multi modal learning environment\n", "abstract": " We developed and tested a multimedia education system, called LEMMA, which allows educators without programming experience to design 3D interactive multi-modal tutorials. For the learner, our system aims to leverage advantages of interactive computer visualizations and the navigational benefits of hypermedia. LEMMA supports upcoming design principles in computer-aided education, such as modularity and reusability, facilitated through a storyboard-style authoring environment that promotes consistency among multiple media, including text, speech, 3D visualizations, and 2D and 3D interaction. We created a first tutorial about\" Rotational Rigid Body Dynamics\" and applied it in an undergraduate physics course, using an interactive reach-through screen as our medium of choice for 3D visualization and interaction. This paper introduces the system itself, the methodology the system uses and provides the\u00a0\u2026", "num_citations": "7\n", "authors": ["1194"]}
{"title": "Mobile augmented reality systems\n", "abstract": " As computers grow ever smaller and faster, the option of wearing them, rather than carrying or sitting in front of them, is rapidly becoming possible. One especially promising approach for wearable user interfaces is augmented reality. This alternative form of virtual reality augments, rather than replaces, the physical world with additional information. For example, a see-through and hear-through head-worn display can be used to overlay relevant graphics and audio on what the user normally sees and hears.", "num_citations": "7\n", "authors": ["1194"]}
{"title": "Augmented reality for construction\n", "abstract": " Recent advances in computer interface design, and the ever increasing power and miniaturization of computer hardware, have combined to make the use of augmented reality possible in demonstration testbeds for building construction, maintenance and renovation. In the spirit of the first see-through head-mounted display developed by Sutherland (Sutherland, 1968), we and other researchers (eg, Robinett, 1992; Caudell & Mizell, 1992; Bajura & Neumann, 1995) use the term augmented reality to refer to enrichment of the real world with a complementary virtual world. The augmented reality systems we are developing employ a see-through head-worn display that overlays graphics and sound on a person's naturally occurring sight and hearing. By tracking users and objects in space, these systems provide users with visual information that is tied to the physical environment. Unlike most virtual realities, whose virtual worlds replace the real world, augmented reality systems enhance the real world by superposing information onto it. The spatial tracking capabilities of our augmented reality systems distinguish them from the heads-up displays featured in some wearable computer systems (Quinn 1993, Patents 1994, Smailagic and Siewiorek 1994).As part of a program aimed at developing a variety of high-performance user interfaces, we have developed a testbed augmented reality system that addresses spaceframe construction (Webster et al. 1996). Spaceframes are typically made from a large number of components of similar size and shape (typically cylindrical struts and spherical nodes). Although the exterior dimensions of all the members\u00a0\u2026", "num_citations": "7\n", "authors": ["1194"]}
{"title": "Multimodal biometric authentication for VR/AR using EEG and eye tracking\n", "abstract": " Electroencephalogram (EEG) signals can enable an additional non-intrusive input modality especially when paired with a wearable headset (ie AR/VR). A great challenge in using EEG data for Brain-Computer Interface (BCI) algorithms is its poor generalization performance across users. Taking advantage of these inter-user differences, we investigate the potential in using this technology for user authentication\u2013similar to facial recognition in smartphones. Additionally, we evaluate this in combination with eye tracking data which is also readily available in such headsets. We develop a biometric authentication systems for each of these systems and for their fusion. We formulate a novel evaluation paradigm using publicly available EEG motor imagery and eye tracking data and demonstrate strong feasibility towards using EEG and eye tracking for authentication.", "num_citations": "6\n", "authors": ["1194"]}
{"title": "Enhanced geometric techniques for point marking in model-free augmented reality\n", "abstract": " Specifying points in three-dimensional (3D) space is an essential function in many augmented reality (AR) applications. When an environment model is not available, a straightforward solution is to perform geometric triangulation using two rays. However, na\u00efve implementations suffer from low accuracy caused by technical limitations of AR devices and human motor constraints. To overcome these issues, we designed and evaluated two enhanced geometric techniques for 3D point marking. VectorCloud uses multiple rays to reduce the effects of pointing jitter, and ImageRefinement improves the accuracy by allowing users to refine the 3D direction of the two rays. We conducted studies to understand the characteristics of these techniques in both ecologically valid outdoor settings using a mobile AR display and in more controlled setting using virtual reality simulation. Our experiments demonstrate that both\u00a0\u2026", "num_citations": "6\n", "authors": ["1194"]}
{"title": "Casual immersive viewing with smartphones\n", "abstract": " In this paper, we explore how to better integrate virtual reality viewing to a smartphone. We present novel designs for casual (short-term) immersive viewing of spatial and 3D content, such as augmented and virtual reality, with smartphones. Our goal is to create a simple and low-cost casual-viewing design which could be retrofitted and eventually be embedded into smartphones, instead of using larger spatial viewing accessories. We explore different designs and implemented several prototypes. One prototype uses thin and light near-to-eye optics with a smartphone display, thus providing the user with the functionality of a large, high-resolution virtual display.", "num_citations": "6\n", "authors": ["1194"]}
{"title": "Simulation based camera localization under a variable lighting environment\n", "abstract": " Localizing the user from a feature database of a scene is a basic and necessary step for presentation of localized augmented reality (AR) content. Commonly such a database depicts a single appearance of the scene, due to time and effort required to prepare it. However, the appearance depends on various factors, eg, the position of the sun and cloudiness. Observing the scene under different lighting conditions results in a decreased success rate and accuracy of the localization.", "num_citations": "6\n", "authors": ["1194"]}
{"title": "Anchoring 2D gesture annotations in augmented reality\n", "abstract": " Augmented reality enhanced collaboration systems often allow users to draw 2D gesture annotations onto video feeds to help collaborators to complete physical tasks. This works well for static cameras, but for movable cameras, perspective effects cause problems when trying to render 2D annotations from a new viewpoint in 3D. In this paper, we present a new approach towards solving this problem by using gesture enhanced annotations. By first classifying which type of gesture the user drew, we show that it is possible to render annotations in 3D in a way that conforms more to the original intention of the user than with traditional methods. We first determined a generic vocabulary of important 2D gestures for remote collaboration by running an Amazon Mechanical Turk study with 88 participants. Next, we designed a novel system to automatically handle the top two 2D gesture annotations - arrows and circles\u00a0\u2026", "num_citations": "6\n", "authors": ["1194"]}
{"title": "A model-based evaluation of trust and situation awareness in the diner\u2019s dilemma game\n", "abstract": " This paper describes a machine learning approach to evaluate the relationship between trust behavior and Situation Awareness (SA) in the context of a 3-player Iterated Diner\u2019s Dilemma game. Our experimental setup consisted of a set of 24 supervised studies in which participants played against computer opponents with different cooperation strategies. Three user interfaces were evaluated in the study, each corresponding to a specific level of SA (Perception, Comprehension, and Projection). Two concepts of trust are explored in the study: Empirical trust behavior was recorded as the degree of cooperation imparted by the participant in a given condition, and Self-reported trust assessments were also collected at regular intervals throughout the study. To explore the relations that exist between these two concepts of trust, and different SA Level conditions, a machine learning approach was applied to train a variety of models to accurately predict trust behavior in each condition. Our best performing algorithm was a J48 rule-based learner, which leveraged SA Level, strategy, and self-reported survey data to predict trust behavior to 76% accuracy, and 67% accuracy using only SA Level and opponent strategy. This is a relative increase of 43% and 24% respectively over a benchmark majority class predictor. These results indicate that support at a level of SA and opponent strategy are good predictors of trust behavior in the Iterated Diner\u2019s Dilemma game. Our results also show that trust-prediction models perform best when they are trained on a combination of self-reported data, and strategy/SA Level information.", "num_citations": "6\n", "authors": ["1194"]}
{"title": "A mission-centric visualization tool for cybersecurity situation awareness\n", "abstract": " We present a novel visualization tool that provides high-level situation awareness for cybersecurity scenarios. Our visualization tool is tailored to intrinsically higher-level information presentation centered around cyberdefense missions. We applied our visualizations to data logs from the 2011 UCSB International Capture The Flag competition, which was designed to mirror general cybersecurity scenarios that would be encountered by the military. In this paper, we present our system and user interface design and discuss some anomalies in the data that are highlighted through the use of our visualization tool.Descriptors:", "num_citations": "6\n", "authors": ["1194"]}
{"title": "Interactive folksonomic analytics with the Tag River visualization\n", "abstract": " Tag River is a novel visualization that presents a detailed comparative overview between user content for a particular span of time. Simultaneously it provides a trend summarization of earlier or later time spans. The summarization is displayed using vertically-adjacent polygonal regions in which the area represents some facet of quantitative information. A series of animated tag clouds are used to describe more detailed content for each user, changing over time to provide an indication of the coherence of context between time segments. The concurrent representation of both multivariate and temporal data can be cycled though programmatically or navigated interactively, allowing the user to explore time spans via filtering or zooming. Changing the view to a new time span instantly updates the tag clouds. We use color and size to represent information associated with the tags, and these aspects are updated to reflect changes in information when a new time span is selected. To facilitate these updates, we introduce a 2D packing algorithm which satisfies specified aesthetic criteria and runs at real-time frame rates. This paper describes the visualization technique in detail and presents example visualizations using datasets from social media sites.", "num_citations": "6\n", "authors": ["1194"]}
{"title": "Tangiblesphere\u2013interaction techniques for physical and virtual spherical displays\n", "abstract": " Tangible interaction is generally assumed to provide benefits compared to other interaction styles due to its physicality. We demonstrate how this physicality can be brought to VR by means of TangibleSphere\u2013a tracked, low-cost physical object that can (a) be rotated freely and (b) is overlaid with a virtual display. We present two studies, investigating performance in terms of efficiency and usability: the first study (N= 16) compares TangibleSphere to a physical spherical display regarding accuracy and task completion time. We found comparable results for both types of displays. The second study (N= 32) investigates the influence of physical rotation in more depth. We compare a pure VR condition to TangibleSphere in two conditions: one that allows actual physical rotation of the object and one that does not. Our findings show that physical rotation significantly improves accuracy and task completion time. These\u00a0\u2026", "num_citations": "5\n", "authors": ["1194"]}
{"title": "Multimodal Classification of EEG During Physical Activity\n", "abstract": " Brain Computer Interfaces (BCIs) typically utilize electroencephalography (EEG) to enable control of a computer through brain signals. However, EEG is susceptible to a large amount of noise, especially from muscle activity, making it difficult to use in ubiquitous computing environments where mobility and physicality are important features. In this work, we present a novel multimodal approach for classifying the P300 event related potential (ERP) component by coupling EEG signals with nonscalp electrodes (NSE) that measure ocular and muscle artifacts. We demonstrate the effectiveness of our approach on a new dataset where the P300 signal was evoked with participants on a stationary bike under three conditions of physical activity: rest, low-intensity, and high-intensity exercise. We show that intensity of physical activity impacts the performance of both our proposed model and existing state-of-the-art models\u00a0\u2026", "num_citations": "5\n", "authors": ["1194"]}
{"title": "Knowledge complacency and decision support systems\n", "abstract": " Decision support systems (DSS), which are often based on complex statistical, machine learning, and AI models, have increasingly become a core part of data analytics and sensemaking processes. Automation complacency - a state characterized by over-trust in intelligent systems - has the potential to result in catastrophic performance failure. An under-investigated factor in automation complacency research is the effect that DSS might have on human learning of domain concepts. In this paper, we perform a comparative analysis of two studies of users interacting with decision aids to understand how knowledge retention is affected by the competence and presentation of a DSS. Our results indicate that while humans have the opportunity to learn and internalize domain concepts while being supported by a DSS, features that make the DSS appear more competent, persuasive, or customizable may lead a user to\u00a0\u2026", "num_citations": "5\n", "authors": ["1194"]}
{"title": "Easy to please: Separating user experience from choice satisfaction\n", "abstract": " Recommender systems are evaluated based on both their ability to create a satisfying user experience and their ability to help a user make better choices. Despite this, quantitative evidence from previous research in recommender systems indicate very high correlations between user experience attitudes and choice satisfaction. This might imply invalidity in the measurement methodologies of these constructs, whereas they may not be measuring what researchers think they are measuring. To remedy this, we present a new methodology for the measurement of choice satisfaction. Part of our approach is to measure a user's\" ease of satisfaction,\" or that user's natural propensity to be satisfied, which is measured using three different approaches. An (N= 526) observational study is conducted wherein users browse a movie catalog. A factor analysis is done to assess the discriminant validity of our proposed choice\u00a0\u2026", "num_citations": "5\n", "authors": ["1194"]}
{"title": "Augmented reality\u2013principles and practice tutorial\n", "abstract": " Summary form only given. The complete presentation was not made available for publication as part of the conference proceedings. The world is becoming more complex and problem solving often requires teams of experts to work together at the same or from different locations. To support this there is a need for collaborative tools, and a variety of teleconferencing and telepresence technologies have been developed. However, most of them involve some variation of traditional video conferencing, which has limitations, such as not being able to effectively convey spatial cues or share the user's task space. This workshop will focus on how these limitations can be overcome by using Mixed Reality (MR) technology, leading to the development of radically new types of collaborative experiences. The target audience includes everyone with an interest in developing AR applications, academic and industrial\u00a0\u2026", "num_citations": "5\n", "authors": ["1194"]}
{"title": "The full story: Automatic detection of unique news content in microblogs\n", "abstract": " In recent years a large portion of news dissemination has shifted from traditional outlets to individual users on platforms such as Twitter and Facebook. Accordingly, methods for detecting newsworthy and otherwise useful information on these platforms have received a lot of research attention. In this paper, we present a novel algorithm to automatically capture core differences in newsworthy content between microblog and traditional news media streams and discuss why it is difficult to capture such information using traditional text-based search mechanisms. We describe an experiment to tune and evaluate the algorithm using a corpus of 35 million Twitter messages and 6,112 New York Times articles on a variety of topics. Finally, we describe an online user study (N=200) to evaluate user perceptions of content recommended by our algorithm. Results show significant differences in user perception of\u00a0\u2026", "num_citations": "5\n", "authors": ["1194"]}
{"title": "Interactive interfaces for complex network analysis: An information credibility perspective\n", "abstract": " This paper discusses and evaluates the impact of visualization and interaction strategies for extracting quality information from data in complex networks such as microblogs. Two different approaches to interactive visual representations of data are discussed: an interactive node-link graph and a novel approach where content is separated into interactive lists based on data properties. To assess the two approaches in terms of information credibility, the TopicNets system is compared with \u201cFluo\u201d, a novel system. An analysis scenario is performed through each system on a set of big data filtered from the Twitter message service. The exposure of content, trade-offs between algorithmic power and interaction complexity, methods for content filtering, and strategies for recommending new content are assessed for each system. Fluo is found to improve on TopicNets ability to efficiently find relevant content primarily by\u00a0\u2026", "num_citations": "5\n", "authors": ["1194"]}
{"title": "Enabling multimodal mobile interfaces for interactive musical performance\n", "abstract": " We present research that extends the scope of the mobile application Control, a prototyping environment for defining multimodal interfaces that control real-time artistic and musical performances. Control allows users to rapidly create interfaces employing a variety of modalities, including: speech recognition, computer vision, musical feature extraction, touchscreen widgets, and inertial sensor data. Information from these modalities can be transmitted wirelessly to remote applications. Interfaces are declared using JSON and can be extended with JavaScript to add complex behaviors, including the concurrent fusion of multimodal signals. By simplifying the creation of interfaces via these simple markup files, Control allows musicians and artists to make novel applications that use and combine both discrete and continuous data from the wide range of sensors available on commodity mobile devices.", "num_citations": "5\n", "authors": ["1194"]}
{"title": "Real-time Planar World Modeling for Augmented Reality\n", "abstract": " We posit that the challenge of complete visual modeling and tracking of the outdoor urban world can be made tractable by using the simplifying assumption of textured planar surfaces. While recent methods have demonstrated dense and complex reconstructions in some cases, we advocate simpler models which more efficiently and effectively capture the semantic structure of the scene. We argue that this structure is what will enable rich augmented reality interaction and annotation techniques. We describe three potential benefits of the planar modeling approach: 1) interactive mobile modeling and annotation; 2) fast and robust tracking and relocalization using oriented patches; and 3) scalable and incremental world model construction from ad-hoc user contributions. We consider several research questions and technical challenges which must be addressed to achieve mobile creation of piecewise-planar models.", "num_citations": "5\n", "authors": ["1194"]}
{"title": "Wearable 3D graphics for augmented reality: a case study of two experimental backpack computers\n", "abstract": " Graphical mobile augmented reality applications will only be effective if the wearable computers on which they run are capable of high-performance 3D graphics. However, no current commercial wearable system provides the graphical performance of even a typical modern desktop computer. In this paper, we describe two different experimental wearable graphics computers designed for prototyping augmented reality applications requiring fast 3D graphics. We explain the system requirements and design decisions that resulted in these architectures.", "num_citations": "5\n", "authors": ["1194"]}
{"title": "Telelife: the future of remote living\n", "abstract": " In recent years, everyday activities such as work and socialization have steadily shifted to more remote and virtual settings. With the COVID-19 pandemic, the switch from physical to virtual has been accelerated, which has substantially affected various aspects of our lives, including business, education, commerce, healthcare, and personal life. This rapid and large-scale switch from in-person to remote interactions has revealed that our current technologies lack functionality and are limited in their ability to recreate interpersonal interactions. To help address these limitations in the future, we introduce \"Telelife,\" a vision for the near future that depicts the potential means to improve remote living better aligned with how we interact, live and work in the physical world. Telelife encompasses novel synergies of technologies and concepts such as digital twins, virtual prototyping, and attention and context-aware user interfaces with innovative hardware that can support ultrarealistic graphics, user state detection, and more. These ideas will guide the transformation of our daily lives and routines soon, targeting the year 2035. In addition, we identify opportunities across high-impact applications in domains related to this vision of Telelife. Along with a recent survey of relevant fields such as human-computer interaction, pervasive computing, and virtual reality, the directions outlined in this paper will guide future research on remote living.", "num_citations": "4\n", "authors": ["1194"]}
{"title": "Walking and Teleportation in Wide-area Virtual Reality Experiences\n", "abstract": " Location-based or Out-of-Home Entertainment refers to experiences such as theme and amusement parks, laser tag and paintball arenas, roller and ice skating rinks, zoos and aquariums, or science centers and museums among many other family entertainment and cultural venues. More recently, location-based VR has emerged as a new category of out-of-home entertainment. These VR experiences can be likened to social entertainment options such as laser tag, where physical movement is an inherent part of the experience versus at-home VR experiences where physical movement often needs to be replaced by artificial locomotion techniques due to tracking space constraints. In this work, we present the first VR study to understand the impact of natural walking in a large physical space on presence and user preference. We compare it with teleportation in the same large space, since teleportation is the most\u00a0\u2026", "num_citations": "4\n", "authors": ["1194"]}
{"title": "Using eye tracked virtual reality to classify understanding of vocabulary in recall tasks\n", "abstract": " In recent years, augmented and virtual reality (AR/VR) have started to take a foothold in markets such as training and education. Although AR and VR have tremendous potential, current interfaces and applications are still limited in their ability to recognize context, user understanding, and intention, which can limit the options for customized individual user support and the ease of automation. This paper addresses the problem of automatically recognizing whether or not a user has an understanding of a certain term, which is directly applicable to AR/VR interfaces for language and concept learning. To do so, we first designed an interactive word recall task in VR that required non-native English speakers to assess their knowledge of English words, many of which were difficult or uncommon. Using an eye tracker integrated into the VR Display, we collected a variety of eye movement metrics that might correspond to\u00a0\u2026", "num_citations": "4\n", "authors": ["1194"]}
{"title": "Understanding node-link and matrix visualizations of networks: A large-scale online experiment\n", "abstract": " We investigated human understanding of different network visualizations in a large-scale online experiment. Three types of network visualizations were examined: node-link and two different sorting variants of matrix representations on a representative social network of either 20 or 50 nodes. Understanding of the network was quantified using task time and accuracy metrics on questions that were derived from an established task taxonomy. The sample size in our experiment was more than an order of magnitude larger (N = 600) than in previous research, leading to high statistical power and thus more precise estimation of detailed effects. Specifically, high statistical power allowed us to consider modern interaction capabilities as part of the evaluated visualizations, and to evaluate overall learning rates as well as ambient (implicit) learning. Findings indicate that participant understanding was best for the node-link\u00a0\u2026", "num_citations": "4\n", "authors": ["1194"]}
{"title": "In-situ labeling for augmented reality language learning\n", "abstract": " Augmented Reality is a promising interaction paradigm for learning applications. It has the potential to improve learning outcomes by merging educational content with spatial cues and semantically relevant objects within a learner's everyday environment. The impact of such an interface could be comparable to the method of loci, a well known memory enhancement technique used by memory champions and polyglots. However, using Augmented Reality in this manner is still impractical for a number of reasons. Scalable object recognition and consistent labeling of objects is a significant challenge, and interaction with arbitrary (unmodeled) physical objects in AR scenes has consequently not been well explored. To help address these challenges, we present a framework for in-situ object labeling and selection in Augmented Reality, with a particular focus on language learning applications. Our framework uses a\u00a0\u2026", "num_citations": "4\n", "authors": ["1194"]}
{"title": "XRCreator: interactive construction of immersive data-driven stories\n", "abstract": " Immersive data-driven storytelling, which uses interactive immersive visualizations to present insights from data, is a compelling use case for VR and AR environments. We present XRCreator, an authoring system to create immersive data-driven stories. The cross-platform nature of our React-inspired system architecture enables the collaboration among VR, AR, and web users, both in authoring and in experiencing immersive data-driven stories.", "num_citations": "4\n", "authors": ["1194"]}
{"title": "PPV: Pixel-point-volume segmentation for object referencing in collaborative augmented reality\n", "abstract": " We present a method for collaborative augmented reality (AR) that enables users from different viewpoints to interpret object references specified via 2D on-screen circling gestures. Based on a user's 2D drawing annotation, the method segments out the userselected object using an incomplete or imperfect scene model and the color image from the drawing viewpoint. Specifically, we propose a novel segmentation algorithm that utilizes both 2D and 3D scene cues, structured into a three-layer graph of pixels, 3D points, and volumes (supervoxels), solved via standard graph cut algorithms. This segmentation enables an appropriate rendering of the user's 2D annotation from other viewpoints in 3D augmented reality. Results demonstrate the superiority of the proposed method over existing methods.", "num_citations": "4\n", "authors": ["1194"]}
{"title": "Motivating crowds to volunteer neighborhood data\n", "abstract": " Organizations invest resources to gather geographical information about cities or neighborhoods. This can help governments or companies identify needed services or city improvements. However, collecting this information can be difficult and expensive. In this study we investigate ways to motivate local crowds to serve as the world's sensors and provide geographical data about their surroundings. We conduct interviews and a pilot study to understand whether we can motivate people to contribute data about their neighborhoods via games or for the greater social good of helping the neighborhood. Our results provide a glimpse of how people feel about donating neighborhood data given different motivators; they also provide insight into the amount of data people are willing to contribute. We conclude by discussing possible design implications of our findings.", "num_citations": "4\n", "authors": ["1194"]}
{"title": "Truth, lies, and data: Credibility representation in data analysis\n", "abstract": " The web has evolved in a scale free manner, with available information about different entities developing in different forms, different locations, and at massive scales. This paper addresses the cognitive limitations that information analysts typically experience as they approach the boundaries where automated analysis algorithms are sorely needed. An experiment is conducted to explore information analysts' interactions with recommendations from an automated fact-finder algorithm during the task of answering questions in a fictional humanitarian aid delivery scenario. An experiment (N=285) is performed using three increasingly complex user interfaces, with and without the presence of the automated recommendations. Results show that in the best performing group, interaction with the fact-finder recommendations was 47 percent greater than the worst performing group.", "num_citations": "4\n", "authors": ["1194"]}
{"title": "A social crowd-controlled orchestra\n", "abstract": " We present a novel social interactive system that brings music creation to the crowds. It allows anyone with a personal electronic device and regardless of their musical expertise to participate in the music an orchestra generates, while also encouraging social interactions among participants. Users can be either musicians or conductors. The latter drive the group's music production, while the others follow simple game-like instructions on their device for playing the conductor's selected songs. The primary mode of interaction for playing songs, consists of rotating one's electronic device in different ways. Our system also provides different social interaction cues that aim for novice and expert users to start conversations with each other and improve the group's performance as well as construct a community among participants. We report on a qualitative analysis, which suggests that users appreciate the system's\u00a0\u2026", "num_citations": "4\n", "authors": ["1194"]}
{"title": "Tweetprobe: A real-time microblog stream visualization framework\n", "abstract": " As the importance of social media increases in our daily life, most adopters witness its significant impact on numerous practices in different areas such as business marketing, journalism, entertainment, and social sciences. However, the enormous amount of data makes the overall content difficult to assess and comprehend for both users and information analysts, raising scalability issues. Furthermore, timely understanding of trending topics is a crucial element due to the short life characteristic of most topics in microblogs. In this paper, we present a novel data visualization approach for real-time social data stream analytics using Twitter streaming data. The visual and architectural design of the system has been implemented as a real-time visualization framework, showing the most trendy tweets, hashtags and sentiment of individual messages. The framework proposed in this paper showcases visualization of real-time message streams through different presentation methods with animation effects highlighting the nature of live information streams. Several scenarios are provided as examples of possible application of this system, including deployment as an information canvas that provides an overview of currently trending topics as a wall-sized interactive media arts installation.", "num_citations": "4\n", "authors": ["1194"]}
{"title": "Outdoor Mobile Localization from Panoramic Imagery\n", "abstract": " We describe an end-to-end system for mobile, vision-based localization and tracking in urban environments. Our system uses panoramic imagery which is processed and indexed to provide localization coverage over a large area using few capture points. We utilize a client-server model which allows for remote computation and data storage while maintaining real-time tracking performance. Previous search results are cached and re-used by the mobile client to minimize communication overhead. We evaluate the use of the system for flexible real-time camera tracking in large outdoor spaces.", "num_citations": "4\n", "authors": ["1194"]}
{"title": "Ar 2.0: Social augmented reality-social computing meets augmented reality\n", "abstract": " AJAX is a new concept of Web application development, which can help developers build more dynamic and more responsive Web applications. Grid services are software components that provide seamless access to a variety of grid resources such as computational resources and data sources. Grid service is considered to be the mainstream of future internet. In this paper, we present a perspective view about Ajax and Grid Service, then show a model how to use Ajax in web applications based on Grid Service, and use simple example to exemplify the integration. At last, we discuss the advantage and disadvantage and give our conclusion", "num_citations": "4\n", "authors": ["1194"]}
{"title": "In-depth evaluation of popular interest point detectors on video streams\n", "abstract": " We present an in-depth evaluation of popular interest point detectors, which, in contrast to existing evaluations, is targeted towards the application in visual tracking and augmented reality. In particular, candidate algorithms, testbed, and performance criteria are chosen with respect to the application of visual tracking. We evaluate the impact of individual algorithm parameters and present results in terms of repeatability, number of features detected, and computation time. We also describe our method to semiautomatically generate ground truth in detail.", "num_citations": "4\n", "authors": ["1194"]}
{"title": "Real-time rendering of realistic trees in mixed reality\n", "abstract": " Mixed reality applications put very high demands on both the visual realism and the rendering times of computer graphics elements that are to be perceived as part of the physical scene. This work presents novel techniques to render photorealistic trees in real-time mixed reality. Animation of the tree branches leads to a realistic effect of the tree swaying in the wind. To enhance the effect of blending the tree into a video texture, we present three levels of real-time filtering of the tree and its shadow, which has a great impact on the perceived realism.", "num_citations": "4\n", "authors": ["1194"]}
{"title": "A study of situated product recommendations in augmented reality\n", "abstract": " Augmented Reality interfaces increasingly utilize artificial intelligence systems to tailor content and experiences to the user. We explore the effects of one such system - a recommender system for online shopping - which allows customers to view personalized product recommendations in the physical spaces where they might be used. We describe results of a 2x3 condition exploratory study in which recommendation quality was varied across 3 user interface types. Our results highlight potential differences in user perception of the recommended objects in an AR environment. Specifically, users rate product recommendations significantly higher in AR and in a 3D browser interface, and show a significant increase in trust in the recommender system, compared to a web interface with 2D product images. Through semi-structured interviews, we gather participant feedback which suggests AR interfaces perform better\u00a0\u2026", "num_citations": "3\n", "authors": ["1194"]}
{"title": "Evaluating snapping-to-photos virtual travel interfaces for 3D reconstructed visual reality\n", "abstract": " Navigating through a virtual, 3D reconstructed scene has recently become very important in many applications. A popular approach is to virtually travel to the photos used in reconstructing the scene; such an approach may be generally termed a\" snapping-to-photos\" virtual travel interface. While previous work has either used fully constrained interfaces (always at the photos) or minimally constrained interfaces (free-flight navigation), in this paper we introduce new snapping-to-photos interfaces that lie in between these two extremes. Our snapping-to-photos interfaces snap the view to a photo in 3D based on viewpoint similarity and optionally the user's mouse cursor or finger-tap position. Experimental results, with both indoor and outdoor scene reconstructions, found that our snapping-to-photos interfaces are preferred over the baseline fully constrained-to-photos interface, that there exist differences between indoor\u00a0\u2026", "num_citations": "3\n", "authors": ["1194"]}
{"title": "Infra: structure all the way down: structured data as a visual programming language\n", "abstract": " We present Infra, a new baseline medium for representing data. With Infra, arbitrarily-complex structured data can be encoded, viewed, edited, and processed, all while remaining in an efficient non-textual form. It is suitable for the full range of information modalities, from free-form input, to compact schema-conforming structures. With its own equivalent of a text editor and text-field widget, Infra is designed to target the domain currently dominated by flat character strings while simultaneously enabling the expression of sub-structure, inter-reference, dynamic dependencies, abstraction, computation, and context (metadata).", "num_citations": "3\n", "authors": ["1194"]}
{"title": "The natural materials browser: Using a tablet interface for exploring volumetric materials science datasets\n", "abstract": " We present a novel tablet application, the Natural Materials Browser, that allows a user to interact with volumetric datasets created from a series of natural materials samples. The data samples\u2013high resolution meso-scale volumetric images of nutshells gathered via micro-computed tomography\u2013are envisioned as \u201cvirtual specimens\u201d presented many orders of magnitude larger than their characteristic length scale. The user, initially placed in the center of the volumetric dataset and facing orthogonally toward the original 2D image slices, uses an iPad tablet as a magic lens to view and navigate the data via physical rotation and multitouch gestures. The user has simultaneous access to multiple representations of the datasets from any angle or position, and an additional viewport provides real-time, spatial statistics on the current view of the currently loaded dataset. We conducted a preliminary evaluation of the application by collating cognitive walkthroughs given to domain experts in materials science. Their feedback indicated that our tablet application could potentially be an effective tool for enabling insights regarding these data samples and, more generally, that it functions as a low-cost, immersive system with which to explore volumetric datasets.", "num_citations": "3\n", "authors": ["1194"]}
{"title": "Seems Familiar: An Algorithm For Inferring Spatial Familiarity Automatically\n", "abstract": " Our level of spatial familiarity with a particular place determines our navigation and interaction around that particular space. Despite its importance, most location based services rarely consider a user's spatial familiarity when delivering their services. The lack of integration of a user\u2019s spatial familiarity in location based services may be due to scarce work in automatic detection of a user\u2019s spatial familiarity.", "num_citations": "3\n", "authors": ["1194"]}
{"title": "Robust relocalization and its evaluation for online environment map construction\n", "abstract": " The acquisition of surround-view panoramas using a single hand-held or head-worn camera relies on robust real-time camera orientation tracking and relocalization. This paper presents robust methodology and evaluation for camera orientation relocalization, using virtual keyframes for online environment map construction. In the case of tracking loss, incoming camera frames are matched against known-orientation keyframes to re-estimate camera orientation. Instead of solely using real keyframes from incoming video, the proposed approach employs virtual keyframes which are distributed strategically within completed portions of an environment map. To improve tracking speed, we introduce a new variant of our system which carries out relocalization only when tracking fails and uses inexpensive image-patch descriptors. We compare different system variants using three evaluation methods to show that the\u00a0\u2026", "num_citations": "3\n", "authors": ["1194"]}
{"title": "Analyzing Performance and Efficiency of Smoothed Particle Hydrodynamics\n", "abstract": " With the rise in performance of modern GPUs, Smoothed Particle Hydrodynamics is an increasingly attractive solution for real-time simulation of fluid flows in visual effects for film and games. Starting with simulations of 2000 particles at 20 frames per second in 2003 [4], smoothed particle hydrodynamics has now been simulated with over 242,000 particles at 4 fps using GPUs [6]. While performance has clearly increased, the terms performance and efficiency are often used interchangeably. Results of simulations, as published in graphics journals such as SIGGRAPH, are typically reported by giving the number of particles and frame rate for a particular combination of CPU and graphics hardware. This makes comparisons of algorithm implementations difficult since authors must deduce algorithm efficiency for differing hardware. The development of concrete metrics of performance and efficiency will facilitate better comparison between results. Simple metrics are presented here with an analysis of real-time simulations over the past five years.", "num_citations": "3\n", "authors": ["1194"]}
{"title": "Densification of semi-dense reconstructions for novel view generation of live scenes\n", "abstract": " In this paper, we consider the problem of rendering novel views of a live unprepared scene from video input, important to many application scenarios (such as telepresence and remote collaboration). We present an optimization approach to improving incomplete scene reconstructions captured in real time with a single moving monocular camera. We take semi-dense depth maps and convert them into a dense scene model, suitable for rendering plausible novel views of the scene using conventional image-based rendering. Our implementation densifies depth maps at the rate they are generated, and enables us to generate novel views of live scenes with no pre-capture or preprocessing. In evaluations comparing with other approaches, our method performs well even on difficult scenes, and results in higher-quality novel views.", "num_citations": "2\n", "authors": ["1194"]}
{"title": "Trust and consequences: A visual perspective\n", "abstract": " User interface (UI) composition and information presentation can impact human trust behavior. Trust is a complex concept studied by disciplines like psychology, sociology, economics, and computer science. Definitions of trust vary depending on the context, but are typically based on the core concept of \u201creliance on another person or entity\u201d. Trust is a critical concept since the presence or absence of the right level of trust can affect user behavior, and ultimately, the overall system performance. In this paper, we look across four studies to explore the relationship between UI elements and human trust behavior. Results indicate that UI composition and information presentation can impact human trust behavior. While further research is required to corroborate and generalize these results, we hope that this paper will provide a reference point for future studies by identifying UI elements that are likely to influence\u00a0\u2026", "num_citations": "2\n", "authors": ["1194"]}
{"title": "Poster: Investigating viewpoint visualizations for click & go navigation\n", "abstract": " We present an investigation of viewpoint visualizations for \u201cClick & Go\u201d 3D navigation interfaces based on a pre-populated set of viewpoints. These scenarios often occur in 3D navigation systems that are based on sets of photos and possibly an underlying 3D reconstruction. Given these photos (and the 3D reconstruction), how does one most effectively navigate through this environment? Existing systems often employ Click & Go interfaces which allow users to navigate with one click of the mouse or tap of the finger. In this work, we investigate viewpoint visualizations for such Click & Go interfaces, describing a preliminary user study and providing valuable insights into Click & Go and its viewpoint visualizations.", "num_citations": "2\n", "authors": ["1194"]}
{"title": "Say my name, say my name: User mentioning on facebook\n", "abstract": " Online social networks have become a place to broadcast personal exchanges, political opinions, and cultural artifacts. Many users tag or mention others\u2014sometimes as a tribute, sometimes to get visibility. However, the social implications of these broadcasts with user tagging have so far been understudied. In our qualitative study, we uncover how users experience broadcasts that tag them and other Facebook users. By combining surveys and semi-structured interviews, we find that users enjoy being mentioned in content, as it signals they have connected well with friends. Users who tag others strive for visibility and higher quality social interactions with their network. However, some tagged users feel a loss of control and suspect that others define their interests and social interactions. Our results provide design implications for supporting broadcasts with user tagging in social media tools and generally serve to better understand this phenomenon.", "num_citations": "2\n", "authors": ["1194"]}
{"title": "Poster: Real time hand pose recognition with depth sensors for mixed reality interfaces\n", "abstract": " We present a method for predicting articulated hand poses in real-time with a single depth camera, such as the Kinect or Xtion Pro, for the purpose of interaction in a Mixed Reality environment and for studying the effects of realistic and non-realistic articulated hand models in a Mixed Reality simulator. We demonstrate that employing a randomized decision forest for hand recognition benefits real-time applications without the typical tracking pitfalls such as reinitialization. This object recognition approach to predict hand poses results in relatively low computation, high prediction accuracy and sets the groundwork needed to utilize articulated hand movements for 3D tasks in Mixed Reality workspaces.", "num_citations": "2\n", "authors": ["1194"]}
{"title": "Directed social queries with transparent user models\n", "abstract": " The friend list of many social network users can be very large. This creates challenges when users seek to direct their social interactions to friends that share a particular interest. We present a self-organizing online tool that by incorporating ideas from user modeling and data visualization allows a person to quickly identify which friends best match a social query, enabling precise and efficient directed social interactions. To cover the different modalities in which our tool might be used, we introduce two different interactive visualizations. One view enables a human-in-the-loop approach for result analysis and verification, and, in a second view, location, social affiliations and\" personality\" data is incorporated, allowing the user to quickly consider different social and spatial factors when directing social queries. We report on a qualitative analysis, which indicates that transparency leads to an increased effectiveness of the\u00a0\u2026", "num_citations": "2\n", "authors": ["1194"]}
{"title": "Trends in mobile augmented reality\n", "abstract": " This tutorial provides a detailed introduction to mobile Augmented Reality (AR). AR is a key user-interface technology for personalized, situated information delivery, navigation, on-demand instruction and games. The widespread availability and rapid evolution of smartphones enables software-only solutions for AR where it was previously necessary to assemble custom hardware solutions. However, ergonomic and technical limitations of smartphones as a platform make this a challenging endeavor. In particular, it is necessary to design novel efficient real-time computer vision and computer graphics algorithms, and create new lightweight forms of interaction with the environment through small form-factor devices. This tutorial will present selected technical achievements in this field and highlight some examples of successful application prototypes.", "num_citations": "2\n", "authors": ["1194"]}
{"title": "Scalable Interactive Analysis of Retinal Astrocyte Networks\n", "abstract": " Retinal astrocytes are one of two types of glial cells found in the mammalian retina. In mice, these highly planar cells are located in the innermost retinal layer termed the nerve fiber layer and are robustly stained using anti-glial fibrillary acidic protein (GFAP). We sought to develop an in-depth visual analysis of the astrocyte distribution across the entire retina. Using laser scanning confocal microscopy, whole retinal datasets were captured at high resolution and subsequently assembled into seamless montages using the bio-imaging software Imago [1]. This produces very large images for quantitative and qualitative analysis. Retinal astrocytes are then segmented using a Random Walk method previously described [2]. We previously developed a system [3] to visualize segmentation results and analyze cell distributions. Continued use of the system and users\u2019 feedback revealed a critical need to create an updated system in which retinal datasets are viewed at full resolution (0.31 \u00b5m/pixel). Additionally, a need for interactive segmentation parameter choices and more comprehensive visual analysis tools was identified. The challenges for these improvements range from the size of the data to the speed of the algorithms involved. Here, we address all those challenges and report progress on the analysis tools implemented, enabling insights to be communicated both visually and quantitatively.", "num_citations": "2\n", "authors": ["1194"]}
{"title": "Dataset and evaluation of interest point detectors for visual tracking\n", "abstract": " In this report, we present an extensive dataset of 96 video streams with ground truth. It includes various geometric changes, lighting conditions, and levels of motion blur, and can be used as testbed for a variety of algorithms in the context of visual tracking. We then use this dataset for a detailed quantitative evaluation of popular interest point detectors, which, in contrast to existing evaluations, is geared towards visual tracking in all relevant factors of the evaluation design.", "num_citations": "2\n", "authors": ["1194"]}
{"title": "Creative collaborative exploration in multiple environments\n", "abstract": " We seek to support creativity in science, engineering, and design applications by building infrastructure that offers new capabilities for creative collaborative exploration of complex data in a variety of non-traditional computing environments. We describe particular novel environments and devices, including the Allosphere and the interactive Fogscreen, the software components to support collaborative interaction in mixed-media environments, and several key application scenarios that will leverage these capabilities. Our main focus is on supporting integrated visualization, sonification, and interaction capabilities in and across novel computing environments.", "num_citations": "2\n", "authors": ["1194"]}
{"title": "3DTV-Panoramic 3D Model Acquisition and its 3D Visualization on the Interactive Fogscreen\n", "abstract": " Future 3D television critically relies on mechanisms for automatically acquiring and visualizing high quality 3D content of both indoor and outdoor scenes. The envisioned goal is that a photo-realistic 3D real-time rendering from the actual and potentially arbitrary viewpoint of the beholder who is watching 3DTV becomes possible. Such scenes include movie sets in studios, e.g., for talk shows, TV series and blockbuster movies, but also outdoor scenes, e.g., buildings in a neighborhood for a car chase or cultural heritage sites for a documentary. The goal of 3D model acquisition is to provide the 3D background models where potential 3D actors can be embedded. We present both the 3D acquisition and semi-immersive 3D visualization to give an impression how a future 3D television system could be like.", "num_citations": "2\n", "authors": ["1194"]}
{"title": "INVITE: 3D-augmented interactive video teleconferencing\n", "abstract": " We propose a novel approach to mixed-reality teleconferencing that focuses on 3D augmentations of multi-camera 2D video streams. We exchange information about the geometric layout of all participating meeting sites, as well as the camera parameters for surveying these spaces. We can then correctly overlay 3D graphics on top of the video feeds, representing meeting content as well as highlight annotations and interaction tools for scene manipulation and bookkeeping of meeting contributions and decisions. Our primary contribution is a standard for the delivery and interaction with such data, which will allow for immersive meeting participation on a diverse set of devices, ranging from special-purpose 3D immersive environments to ultra-mobile platforms, such as cell phones and portable video players.", "num_citations": "2\n", "authors": ["1194"]}
{"title": "Interaction and annotation at a distance in outdoor augmented reality\n", "abstract": " The first component for successful interaction at a distance is developing a good technique for moving the user\u2019s cursor in 3-space. We needed to be able to move the cursor in all three dimensions so that when a physical object is being annotated, the virtual label is placed at the same physical location. If we did not do this, and instead simply aligned the annotation with the object in the viewing plane, then as soon as the user moves the label will no longer be in the correct location.We wanted to approach the problem from a mobile perspective, so we limited the system input to devices that were either hand-held or some other part of a wearable system. With these requirements we developed four different techniques for moving the cursor in 3-space. The first uses a Twiddler2 hand-held keyboard,", "num_citations": "2\n", "authors": ["1194"]}
{"title": "Situated Docu mentaries\n", "abstract": " Our experimental wearable augmented reality system enables users to experience hypermedia presentations that are integrated with the actual outdoor locations to which they are are relevant. The mobile prototype uses a tracked see-through head-worn display to overlay 3D graphics, imagery, and sound on top of the real world, and presents additional, coordinated material on a hand-held pen computer. We have used these facilities to create several situated documentaries that tell the stories of events that took place on our campus.", "num_citations": "2\n", "authors": ["1194"]}
{"title": "VoRTX: Volumetric 3D Reconstruction With Transformers for Voxelwise View Selection and Fusion\n", "abstract": " Recent volumetric 3D reconstruction methods can produce very accurate results, with plausible geometry even for unobserved surfaces. However, they face an undesirable trade-off when it comes to multi-view fusion. They can fuse all available view information by global averaging, thus losing fine detail, or they can heuristically cluster views for local fusion, thus restricting their ability to consider all views jointly. Our key insight is that greater detail can be retained without restricting view diversity by learning a view-fusion function conditioned on camera pose and image content. We propose to learn this multi-view fusion using a transformer. To this end, we introduce VoRTX, an end-to-end volumetric 3D reconstruction network using transformers for wide-baseline, multi-view feature fusion. Our model is occlusion-aware, leveraging the transformer architecture to predict an initial, projective scene geometry estimate. This estimate is used to avoid backprojecting image features through surfaces into occluded regions. We train our model on ScanNet and show that it produces better reconstructions than state-of-the-art methods. We also demonstrate generalization without any fine-tuning, outperforming the same state-of-the-art methods on two other datasets, TUM-RGBD and ICL-NUIM.", "num_citations": "1\n", "authors": ["1194"]}
{"title": "Improving Label Noise Robustness with Data Augmentation and Semi-Supervised Learning (Student Abstract)\n", "abstract": " Modern machine learning algorithms typically require large amounts of labeled training data to fit a reliable model. To minimize the cost of data collection, researchers often employ techniques such as crowdsourcing and web scraping. However, web data and human annotations are known to exhibit high margins of error, resulting in sizable amounts of incorrect labels. Poorly labeled training data can cause models to overfit to the noise distribution, crippling performance in real-world applications. In this work, we investigate the viability of using data augmentation in conjunction with semisupervised learning to improve the label noise robustness of image classification models. We conduct several experiments using noisy variants of the CIFAR-10 image classification dataset to benchmark our method against existing algorithms. Experimental results show that our augmentative SSL approach improves upon the state-of-the-art.", "num_citations": "1\n", "authors": ["1194"]}
{"title": "User Perception of Situated Product Recommendations in Augmented Reality\n", "abstract": " Augmented reality (AR) interfaces increasingly utilize artificial intelligence systems to tailor content and experiences to the user. We explore the effects of one such system \u2014 a recommender system for online shopping \u2014 which allows customers to view personalized product recommendations in the physical spaces where they might be used. We describe results of a  condition exploratory study in which recommendation quality was varied across three user interface types. Our results highlight potential differences in user perception of the recommended objects in an AR environment. Specifically, users rate product recommendations significantly higher in AR and in a 3D browser interface, and show a significant increase in trust in the recommender system, compared to a web interface with 2D product images. Through semi-structured interviews, we gather participant feedback which suggests AR interfaces\u00a0\u2026", "num_citations": "1\n", "authors": ["1194"]}
{"title": "Semantic Labeling and Object Registration for Augmented Reality Language Learning\n", "abstract": " We propose an Augmented Reality vocabulary learning interface in which objects in a user's environment are automatically recognized and labeled in a foreign language. Using AR for language learning in this manner is still impractical for a number of reasons. Scalable object recognition and consistent labeling of objects is still a significant challenge, and interaction with arbitrary physical objects in AR scenes has consequently not been well explored. To help address these challenges, we present a system that utilizes real-time object recognition to perform semantic labeling and object registration in Augmented Reality. We discuss its implementation, our motivations in designing it, and how it can be applied to AR language learning applications.", "num_citations": "1\n", "authors": ["1194"]}
{"title": "Hybrid orbiting-to-photos in 3D reconstructed visual reality\n", "abstract": " Virtually navigating through photos from a 3D image-based reconstruction has recently become very popular in many applications. In this paper, we consider a particular virtual travel maneuver that is important for this type of virtual navigation---orbiting to photos that can see a point-of-interest (POI). The main challenge with this particular type of orbiting is how to give appropriate feedback to the user regarding the existence and information of each photo in 3D while allowing the user to manipulate three degrees-of-freedom (DoF) for orbiting around the POI. We present a hybrid approach that combines features from two baselines---proxy plane and thumbnail approaches. Experimental results indicate that users rated our hybrid approach more favorably for several qualitative questionnaire statements, and that the hybrid approach is preferred over both baselines for outdoor scenes.", "num_citations": "1\n", "authors": ["1194"]}
{"title": "Notes on virtual and augmented reality (Keynote)\n", "abstract": " VR and AR hold enormous promises as paradigm-shifting ubiquitous technologies. The investment in these technologies by leading IT companies, as well as the buy-in and general excitement from outside investors, technologists, and content producers has never been more palpable. There are good reasons to be excited about the field. The real question will be if the technologies can add sufficient value to people's lives to establish themselves as more than just niche products. My path in this presentation will lead from a personal estimation of what matters for adoption of new technologies to important innovations we have witnessed on the road to anywhere/anytime use of immersive technologies. In recent years, one track of research in my lab has been concerned with the simulation of possible future capabilities in AR. With the goal to conduct controlled user studies evaluating technologies that are just not\u00a0\u2026", "num_citations": "1\n", "authors": ["1194"]}
{"title": "Through the Grapevine: A Comparison of News in Microblogs and Traditional Media\n", "abstract": " In recent years the greater part of news dissemination has shifted from traditional news media to individual users on microblogs such as Twitter and Reddit. Therefore, there has been increasing research effort on how to automatically detect newsworthy and otherwise useful information on these platforms.             In this paper, we present two novel algorithmic approaches\u2014content-similarity computation and graph analysis\u2014to automatically capture main differences in newsworthy content between microblogs and traditional news media.             For the content-similarity algorithm, we discuss why it is difficult to capture such unique information using traditional text-based search mechanisms. We performed an experiment to evaluate the content-similarity algorithm using a corpus of 35 million topic-specific Twitter messages and 6112 New York Times articles on a variety of topics. This is followed by an online\u00a0\u2026", "num_citations": "1\n", "authors": ["1194"]}
{"title": "A Cross-Cultural Analysis of Explanations for Product Reviews.\n", "abstract": " Cosmetic products are inherently personal. Many people rely on product reviews when choosing to purchase cosmetics. However, reviewers can have tastes that vary based on personal, demographic or cultural background. Prior work has discussed methods for generating attribute-based explanations for item ratings on cosmetic products, based on associated text-based reviews. This paper focuses on evaluating explanation interfaces for product reviews and related attributes. We present the results of a cross-cultural user study that evaluates five associated explanation interfaces for cosmetic product reviews across groups of participants from three different cultural backgrounds. We applied a 3 by 2 within subjects experimental design in a user study (N= 150) to evaluate effects of UI design and personalization on a range of user experience metrics in a cosmetics shopping scenario. Results of the study show that 1) Korean and Japanese speakers chose the most complex UI more often than English speakers. 2) older participants also preferred more options in cosmetic product selection, regardless of cultural background. 3) personalization of product ratings did not show an effect on user experience. 4) Attributebased explanations were preferred over star-ratings for all three cultures. 5) Rating propensity evaluation showed that Japanese provided significantly higher ratings than Korean or English participants, and that Females provided higher ratings than Males, regardless of background.", "num_citations": "1\n", "authors": ["1194"]}
{"title": "Augmented Reality-based Remote Collaboration\n", "abstract": " We describe a framework to support mobile remote collaboration on tasks that involve the physical environment. By building on state-of-the-art computer vision and augmented reality techniques, we enable more immersive and more direct interaction with the remote environment than what is possible with today\u2019s ubiquitous video conferencing paradigm. In addition to the general framework, we describe a prototype implementation and user study, which demonstrate significant benefits of the proposed paradigm.", "num_citations": "1\n", "authors": ["1194"]}
{"title": "Guest editors' introduction: Special section on the ieee international symposium on mixed and augmented reality (ISMAR)\n", "abstract": " THE IEEE International Symposium on Mixed and Aug-mented Reality (ISMAR) continues to be the leading venue for disseminating the latest in AR and MR research, applications, technologies, and companies, and this special section presents significantly extended versions of the three best papers from the IEEE ISMAR 2009 Proceedings. These papers place particular emphasis on two of the most important active research areas in the AR and MR field: tracking, and applications including usability studies. IEEE ISMAR 2009 had 130 submissions. ISMAR has a twotiered reviewing system consisting of a group of 12 area chairs, as well as an international program committee and outside reviewers, altogether comprising 173 AR/MR experts. The review process of each paper was organized by an area chair. It received at least four reviews from PC members. These were reported by the area chair and discussed at length during a physical meeting of the area chair committee in Orlando in July 2009. The overall acceptance rate of 19.3 percent is an indication of the significant effort expanded in selecting the best papers. A separately installed awards committee of pioneering and leading MR and AR researchers from around the globe reexamined the long papers with the five highest scores to select the Best Paper, the Best Student Paper, and an Honorable Mention.The authors of the three papers with awards were asked to submit an extended version of their conference paper, with a clear focus of additional content that expands and enhances the scientific contribution of the original conference paper. A standard IEEE Transactions on Visualization\u00a0\u2026", "num_citations": "1\n", "authors": ["1194"]}
{"title": "A framework for modeling trust in collaborative ontologies\n", "abstract": " At the heart of both social and semantic web paradigms is the support for any user to become an information provider. While this has huge benefits in terms of the scope of information available, it raises two important problems: firstly, the well researched problem of information overload, and secondly, the problem of assigning trustworthiness to a piece of information, or an information source. Given the small window of information available for us to make decisions about trust on the web, relative to real-world trust decisions, this becomes a challenging problem. This paper presents a framework for harnessing available information in the domain of collaborative/shared ontologies on the Semantic Web. Proposed by the W3C, the semantic web is a emergent standardization technology supporting structured data on the web. Ontological data is represented in many different formats, such as the Resource Description Framework (RDF), RDF Schema (RDFS) and the Web Ontology Language (OWL). These predefined formats provide formal expressions containing concepts and relationships in a knowledge domain. Ontology viewers and semantic visualizers such as Prot\u00e9g\u00e9 and JUNG are actively used to create customized knowledge bases and to support user driven exploration of ontological data.In this paper we distinguish between semantic data that is personally created, and data that has been pulled out of public database or other shared resource. Various semantic databases currently exist. some of which are integrated into a single database network, such as DbPedia for example. DBPedia is based on structured information from the Wikipedia\u00a0\u2026", "num_citations": "1\n", "authors": ["1194"]}
{"title": "Evaluation of tracking robustness in real time panorama acquisition\n", "abstract": " We present an analysis of four orientation tracking systems used for construction of environment maps. We discuss the analysis necessary to determine the robustness of tracking systems in general. Due to the difficulty inherent in collecting user evaluation data, we then propose a metric which can be used to obtain a relative estimate of these values. The proposed metric will still require a set of input videos with an associated distance to ground truth, but not an additional user evaluation.", "num_citations": "1\n", "authors": ["1194"]}
{"title": "From the Science and Technology program chairs\n", "abstract": " While ISMAR is expanding and reaching out to new communities, we proudly continue to present to you the best research papers in the field of Mixed and Augmented Reality. The technical program of ISMAR 2009, in the tradition of the proceedings of seven previous ISMAR, two ISAR, two ISMR, and two IWAR meetings, this year takes the form of a Science and Technology (S&T) track. This program is comprised of 24 papers, 28 posters, as well as an array of keynote talks, demonstrations, tutorials, workshops and the tracking competition. All of these elements of the program are the result of dedicated hard work by members of the conference committee and additional volunteers, and we would like to recognize their efforts.", "num_citations": "1\n", "authors": ["1194"]}
{"title": "Lets go out: Research in outdoor mixed and augmented reality\n", "abstract": " Lets Go Out: Research in Outdoor Mixed and Augmented Reality AR 2.0: Social Augmented Reality - Social Computing meets Augmented Page 1 229 Christian Sandor University of South Australia, Australia Itaru Kitahara University of Tsukuba, Japan Gerhard Reitmayr Graz University of Technology, Germany Steven Feiner Columbia University, USA Yuichi Ohta University of Tsukuba, Japan Tobias H\u00f6llerer University of California at Santa Barbara, USA Dieter Schmalstieg Technische Universit\u00e4t Graz, Austria Mark Billinghurst University of Canterbury/HIT Lab, New Zealand Full-Day Workshop The outdoors presents enormous challenges for mixed and augmented reality. Outdoor environments encompass extreme weather and illumination conditions, and mobile systems must deal with technological constraints, including low-resolution cameras and displays, inaccurate and fragile tracking systems, limited system \u2026", "num_citations": "1\n", "authors": ["1194"]}
{"title": "\ubaa8\ubc14\uc77c \uc99d\uac15 \ud604\uc2e4 \ubc0f \ud56d\uacf5\uc0ac\uc9c4\uc744 \uc774\uc6a9\ud55c \uac74\ubb3c\uc758 3 \ucc28\uc6d0 \ubaa8\ub378\ub9c1\n", "abstract": " \ubcf8 \ub17c\ubb38\uc5d0\uc11c\ub294 \ubaa8\ubc14\uc77c \uc99d\uac15 \ud604\uc2e4 \uc2dc\uc2a4\ud15c \ubc0f \ud56d\uacf5\uc0ac\uc9c4\uc744 \uc774\uc6a9\ud558\uc5ec \uac74\ubb3c\uc758 \ubd80\ubd84\uc801 3D \ubaa8\ub378\uc744 \uc0dd\uc131\ud558\uace0, \uc774\ub97c \ube44\ub514\uc624 \uc601\uc0c1\uacfc \ube44\uad50\ud558\uc5ec \uc0ac\uc6a9\uc790\uc758 \uc704\uce58\ub97c \uc2e4\uc2dc\uac04\uc73c\ub85c \ucd94\uc801\ud558\ub294 \ubc29\ubc95\uc744 \uc81c\uc548\ud55c\ub2e4. \uc81c\uc548\ub41c \uc2dc\uc2a4\ud15c\uc740 \ubbf8\ub9ac \uc0dd\uc131\ub41c \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud558\ub294 \ub300\uc2e0, \uc2dc\uc2a4\ud15c\ub3d9\uc791 \uc911\uc5d0 \uc0ac\uc6a9\uc790 \ubdf0\uc640 \ud56d\uacf5 \ubdf0\ub97c \uacb0\ud569\ud558\uc5ec 3D \ubaa8\ub378\uc744 \uc0dd\uc131\ud55c\ub2e4. \uc6b0\uc120 GPS\uc758 \uc704\uce58\uc5d0 \ub530\ub77c \ub370\uc774\ud130\ubca0\uc774\uc2a4\ub85c\ubd80\ud130 \uac80\uc0c9\ub41c \ud56d\uacf5\uc0ac\uc9c4\uacfc, \ud53c\uce58\ub97c \ucd94\uc815\ud558\ub294 \uad00\uc131 \uc13c\uc11c\ub97c \uc774\uc6a9\ud558\uc5ec \uc0ac\uc6a9\uc790\uc758 \ucd08\uae30 \uc790\uc138\ub97c \uacc4\uc0b0\ud55c\ub2e4. \uadf8\ub9ac\uace0 \uadf8\ub798\ud504 \ucef7\uc744 \uc774\uc6a9\ud558\uc5ec \uac74\ubb3c\uc758 \uc0c1\ub2e8\uc758 \uc5d0\uc9c0\ub97c \uac80\ucd9c\ud558\uace0, \uc81c\uc548\ub41c \ube44\uc6a9 \ud568\uc218\ub97c \ucd5c\uc18c\ud654\ud568\uc73c\ub85c\uc368 \ud558\ub2e8\uc758 \uc5d0\uc9c0\uc640 \ubaa8\ud241\uc774 \uc704\uce58\ub97c \ucc3e\ub294\ub2e4. \uc2e4\uc2dc\uac04\uc73c\ub85c \uc0ac\uc6a9\uc790\uc758 \uc790\uc138\ub97c \ucd94\uc801\ud558\uae30 \uc704\ud574, \uc0ac\uc6a9\uc790\uac00 \uad00\ucc30 \uc911\uc778 \uac74\ubb3c\uc758 \uc5d0\uc9c0 \ubc0f \ubcbd\uba74\uc5d0\uc11c\uc758 \ud2b9\uc774\uc810\uc744 \uc774\uc6a9\ud558\uc5ec \ucd94\uc801\uc744 \uc218\ud589\ud55c\ub2e4. \ubcf8 \ub17c\ubb38\uc5d0\uc11c\ub294 \ucd5c\uc18c \uc790\uc2b9 \ucd94\uc815\ubc95\uacfc \uc5b8\uc13c\ud2f0\ud2b8 \uce7c\ub9cc \ud544\ud130\ub97c \uc0ac\uc6a9\ud558\uc5ec \uce74\uba54\ub77c \uc790\uc138 \ucd94\uc815 \ubc29\ubc95\uc744 \uad6c\ud604\ud558\uace0 \ube44\uad50\ud558\uc600\ub2e4. \ub610\ud55c \ub450 \ubc29\ubc95\uc5d0 \ub300\ud55c \uc18d\ub3c4\uc640 \uc815\ud655\ub3c4\ub97c \ube44\uad50\ud558\uace0, Anywhere Augmentation \uc2dc\ub098\ub9ac\uc624\uc5d0 \ub300\ud55c \uc911\uc694\ud55c \uae30\ubcf8 \uad6c\uc131 \uc694\uc18c\ub4e4\ub85c\uc11c\u00a0\u2026", "num_citations": "1\n", "authors": ["1194"]}
{"title": "Real-time high dynamic range image generation using multiple cameras\n", "abstract": " In order to accurately acquire a real scene using a camera, it is necessary to capture an image of a high dynamic range (HDR). In particular, when a camera observes both an indoor and an outdoor scene at the same time or captures a light source directly, a dynamic range of more than 200 dB is generally required. Therefore, many methods for generating HDR images have been developed. However, most conventional methods are incapable of real-time processing because of the limitations of the equipment or high computational costs. In this paper, we propose a method for realtime generation of HDR images using multiple cameras. The remainder of this paper is structured as follows: In Section 2, we briefly review related studies on HDR generation. In Section 3, we describe our real-time HDR image generation method that makes use of multiple cameras. In Section 4, some experimental results are presented. Finally, in Section 5, we present our conclusions.", "num_citations": "1\n", "authors": ["1194"]}
{"title": "IssueBrowser: Knowledge acquisition via multimedia data\n", "abstract": " One can hardly overestimate the pervasiveness of digital multimedia in the 2008 United States presidential election. Anyone seeking to learn more about the election has at their disposal a plethora of digital media content such as online news articles, blogs,\u2018informational\u2019and public candidate videos. These data not only originate from traditional sources of media \u201ctranslated\u201d to the web but also come from an increasingly large web campaign presence of candidates themselves and supporters. Navigating through all these data can be difficult for users as the information in various media formats is scattered across the web. In addition, each of these sources of data has implicit information (such as temporal or semantic relationships) that is not readily available to a casual consumer, but which nevertheless could be of interest to someone seeking a more nuanced understanding of the data. Furthermore, these data are connected in ways that only become apparent when analyzed together. For users wishing to combine the available data into a fuller picture of a candidate and his or her position on particular issues, methods for mining the multimedia data becomes more and more important.Towards these ends, we propose \u201cIssueBrowser,\u201d a multimedia data mining application that accumulates and allows for exploration of multimedia data about the 2008 US presidential election. A novel interactive visualization installation is designed to aid users\u2019 understanding of the complex, shifting tapestry of candidates and issues involved. The analysis and unique combination of multimedia content pre-", "num_citations": "1\n", "authors": ["1194"]}
{"title": "Immersive Clients to Augmented Video Conferencing\n", "abstract": " Our augmented videoconferencing system may be used with various interaction front ends in order to perform telecollaborative tasks. We currently have implemented immersive interfaces for Invite on several platforms. Here we will discuss these implementations as well as some of the issues involved in providing immersive interaction with augmented video. Additionally, we will explore the several methods for distribution of augmented video. While augmented video introduces an interesting dimension to the traditional teleconferencing paradigm, it often requires that clients have some rendering capability. In this work, we present methods for allowing graphically weaker clients to view or improve the quality of augmented video while allowing for some limited interaction. Our additional methods avoid overly taxing the server or requiring any local rendering.", "num_citations": "1\n", "authors": ["1194"]}
{"title": "Maintaining Visibility Constraints for View Management in 3D User Interfaces\n", "abstract": " A key part of user interface design involves determining the positions and sizes of the objects that the user sees, a task often known as layout. In 2D user interfaces, the geometry of 2D objects directly determines their geometry on 2D displays, and their assignment to layers directly determines which of a set of overlapping objects obscures the others. In contrast, in 3D user interfaces, 3D objects are viewed from one or more viewpoints, and projected onto 2D view planes that are mapped to 2D displays. Even in a static 3D scene, changing the viewpoint from which objects are seen can change the positions, sizes, and visibility relationships of their projections. Thus, the positions and sizes of the objects\u2019 2D projections and their visibility relationships depend on both the objects and the viewpoints. We use the term view management to refer to the decisions that determine the spatial relationships among the projections\u00a0\u2026", "num_citations": "1\n", "authors": ["1194"]}
{"title": "Vision-based interfaces for mobility\n", "abstract": " This paper introduces fast and robust computer vision methods for hand gesture-based mobile user interfaces. In combination with other algorithms, these methods achieve usability and interactivity even when both the camera and the object of interest are in motion, such as with mobile and wearable computing settings. By means of a head-worn camera a set of applications can be controlled entirely with gestures of non-instrumented hands. We describe a set of general gesture-based interaction techniques and explore their characteristics in terms of task suitability and the computer vision algorithms required for their recognition. By doing so, we present an arsenal of mostly generic interaction methods that can be used to facilitate input to mobile applications. We developed a prototype application testbed to evaluate our gesture-based interfaces. We chose three basic tasks from an infrastructure maintenance and repair scenario to illustrate the applicability of our interface techniques.", "num_citations": "1\n", "authors": ["1194"]}
{"title": "Information at a Glance in Wearable Augmented Reality System\n", "abstract": " CiNii \u8ad6\u6587 - Information at a Glance in Wearable Augmented Reality System CiNii \u56fd\u7acb\u60c5\u5831\u5b66 \u7814\u7a76\u6240 \u5b66\u8853\u60c5\u5831\u30ca\u30d3\u30b2\u30fc\u30bf[\u30b5\u30a4\u30cb\u30a3] \u65e5\u672c\u306e\u8ad6\u6587\u3092\u3055\u304c\u3059 \u5927\u5b66\u56f3\u66f8\u9928\u306e\u672c\u3092\u3055\u304c\u3059 \u65e5\u672c\u306e\u535a\u58eb\u8ad6\u6587\u3092 \u3055\u304c\u3059 \u65b0\u898f\u767b\u9332 \u30ed\u30b0\u30a4\u30f3 English \u691c\u7d22 \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u9589\u3058\u308b \u30bf\u30a4\u30c8\u30eb \u8457\u8005\u540d \u8457\u8005 ID \u8457\u8005\u6240\u5c5e \u520a\u884c\u7269\u540d ISSN \u5dfb\u53f7\u30da\u30fc\u30b8 \u51fa\u7248\u8005 \u53c2\u8003\u6587\u732e \u51fa\u7248\u5e74 \u5e74\u304b\u3089 \u5e74\u307e\u3067 \u691c\u7d22 \u691c\u7d22 \u691c\u7d22 CiNii\u7a93\u53e3\u696d\u52d9\u306e\u518d\u958b\u306b\u3064\u3044\u3066 Information at a Glance in Wearable Augmented Reality System BELL Blaine \u88ab\u5f15\u7528\u6587\u732e: 1\u4ef6 \u8457\u8005 BELL Blaine \u53ce\u9332\u520a\u884c\u7269 The 2^<nd> CREST Workshop on Advanced Computing and Communicationg Techniques for Wearable Information Playing, 2003 The 2^<nd> CREST Workshop on Advanced Computing and Communicationg Techniques for Wearable Information Playing, 2003, 2003 \u88ab\u5f15\u7528\u6587\u732e: 1\u4ef6\u4e2d 1-1\u4ef6\u3092 \u8868\u793a 1 \u5c4b\u5916\u62e1\u5f35\u73fe\u5b9f\u611f\u306e\u305f\u3081\u306e\u7e26\u65b9\u5411\u306e\u76f4\u7dda\u30a8\u30c3\u30b8\u3092\u7528\u3044\u305f\u30d1\u30ce\u30e9\u30de\u753b\u50cf\u3068\u306e\u4f4d\u7f6e\u5408\u308f\u305b \u5150\u7389 \u771f\u543e , \u5409\u9ad8 \u2026", "num_citations": "1\n", "authors": ["1194"]}
{"title": "Visualization and 3D-Interaction for Hyperthermia Treatment Planning\n", "abstract": " In regional hyperthermia extensive numerical simulations are required for optimizing individual cancer therapy treatments. To take full advantage of such simulations a therapy planning system is needed. In this paper we discuss a new hyperthermia planning system, focussing on its visualization and 3D-interaction functionality. Our software, called HyperPlan, assists the whole treatment planning process with state-of-the-art numerics and visual controlling techniques. Its flexible design allows it to be used in both hyperthermia research and clinical practice. Introduction Planning systems are well established in conventional radiotherapy. They have also proved to be essential for hyperthermia treatment planning. In this paper we deal with regional hyperthermia that employs radiofrequency waves to heat up the tumor region non-invasively. The goal is to locally achieve temperatures higher than 41C ffi without affecting healthy tissue. Numerical simulations can help to optimize an indi...", "num_citations": "1\n", "authors": ["1194"]}
{"title": "Urban Visual Modeling\n", "abstract": " This chapter explains how to digitally capture, model, and track large outdoor spaces so that they can be used as environments for mobile-augmented reality (AR) applications. The three-dimensional (3D) visual model of the environment is used as a database for image-based pose tracking with a handheld camera-equipped tablet. Experimental analysis demonstrates that real-time localization with high accuracy can be achieved from models created using a small panoramic camera. Device positioning is a common prerequisite for many AR applications. Indoors, visual detection of flat, printed markers has proven to be a very successful method for accurate device positioning, at least for a small workspace. In larger spaces, external tracking systems allow for precise positioning by use of statically mounted cameras that observe objects moving in the space. Outdoors, however, we cannot require that the environment be covered in printed markers or surrounded by mounted cameras. The global positioning system (GPS) provides ubiquitous device tracking from satellites, but does not guarantee enough accuracy on consumer-level devices for AR applications. This chapter presents an alternative approach that treats the built environment like an existing visual marker. By detecting and tracking landmark features on the building facades, the system uses the surrounding buildings for accurate device positioning in the same way that printed markers are used indoors, except at a larger scale.Visual modeling and tracking technology is based on the relationship between points in the scene and cameras that observe them. Having images of the same\u00a0\u2026", "num_citations": "1\n", "authors": ["1194"]}
{"title": "SCUBA: Focus and Context for Real-time Mesh Network Health Diagnosis\n", "abstract": " Large-scale wireless metro-mesh networks consisting of hundreds of routers and thousands of clients suffer from a plethora of performance problems. The sheer scale of such networks, the abundance of performance metrics, and the absence of effective tools can quickly overwhelm a network operators\u2019 ability to diagnose these problems. As a solution, we present SCUBA, an interactive focus and context visualization framework for metro-mesh health diagnosis. SCUBA places performance metrics into multiple tiers or contexts, and displays only the topmost context by default to reduce screen clutter and to provide a broad contextual overview of network performance. A network operator can interactively focus on problem regions and zoom to progressively reveal more detailed contexts only in the focal region. We describe SCUBA\u2019s contexts and its planar and hyperbolic views of a nearly 500 node mesh to demonstrate how it eases and expedites health diagnosis. Further, we implement SCUBA on a 15-node testbed, demonstrate its ability to diagnose a problem within a sample scenario, and discuss its deployment challenges in a larger mesh. Our work leads to several future research directions on focus and context visualization and efficient metrics collection for fast and efficient mesh network health diagnosis1.", "num_citations": "1\n", "authors": ["1194"]}
{"title": "Wigipedia: Visual editing of semantic data in wikipedia\n", "abstract": " Wikipedia is emerging as the dominant global knowledge repository. Recently, large numbers of Wikipedia users have collaborated to produce more structured information in the online encyclopedia. For example, the information found in tables, categories and infoboxes. Infoboxes contain key-value pairs, manually appended to articles based on the unstructured text therein. The wiki contains some structured information which can be crawled by DBpedia [2], which attempts to organize wiki data into into a database of subject-predicate-object triples. By leveraging this data we generate an interface, which we call WiGipedia, embedded on every Wikipedia article as an interactive graph visualization where entities represent articles, categories and relational entities, with typed edges between them. This intelligent web interface is designed to simplify the elicitation of semantically structured information in Wikipedia (Figure 1). Our motivation is to both inform the user of interesting contextual information pertaining to the current article, and to provide a simple way to introduce and/or repair semantic relations between wiki articles. User actions result in improved accuracy and consistency of structured data spread across multiple articles. WiGipedia provides users with an intuitive interface that allows single-click Wikipedia edits without knowledge of the Wikipedia markup language, templates, etc. An online demo of the interface can be found at www. wigipedia-online. com.", "num_citations": "1\n", "authors": ["1194"]}