{"title": "A Bayesian reliability growth model for computer software\n", "abstract": " A Bayesian reliability growth model is presented which includes special features designed to reproduce special properties of the growth in reliability of an item of computer software (program). The model treats the situation where the program is sufficiently complete to work for continuous time periods between failures, and gives a repair rule for the action of the programmer at such failures. Analysis is based entirely upon the length of the periods of working between repairs and failures, and does not attempt to take account of the internal structure of the program. Methods of inference about the parameters of the model are discussed.", "num_citations": "641\n", "authors": ["453"]}
{"title": "Validation of ultra-high dependability for software-based systems\n", "abstract": " Modern society depends on computers for a number of critical tasks in which failure can have very high costs. As a consequence, high levels of dependability (reliability, safety, etc.) are required from such computers, including their software. Whenever a quantitative approach to risk is adopted, these requirements must be stated in quantitative terms, and a rigorous demonstration of their being attained is necessary. For software used in the most critical roles, such demonstrations are not usually supplied. The fact is that the dependability requirements often lie near the limit of the current state of the art, or beyond, in terms not only of the ability to satisfy them, but also, and more often, of the ability to demonstrate that they are satisfied in the individual operational products (validation). We discuss reasons why such demonstrations cannot usually be provided with the means available: reliability growth models\u00a0\u2026", "num_citations": "403\n", "authors": ["453"]}
{"title": "Evaluation of competing software reliability predictions\n", "abstract": " Different software reliability models can produce very different answers when called on to predict future reliability in a reliability growth context. Users need to know which, if any, of the competing predictions are trustworthy. Some techniques are presented which form the basis of a partial solution to this problem. Rather than attempting to decide which model is generally best, the approach adopted allows a user to decide on the most appropriate model for each application.", "num_citations": "397\n", "authors": ["453"]}
{"title": "Conceptual modeling of coincident failures in multiversion software\n", "abstract": " Work by D.E. Eckhardt and L.D. Lee (1985), shows that independently developed program versions fail dependently. The authors show that there is a precise duality between input choice and program choice in this model and consider a generalization in which different versions can be developed using diverse methodologies. The use of diverse methodologies is shown to decrease the probability of the simultaneous failure of several versions. Indeed, it is theoretically possible to obtain versions which exhibit better than independent failure behavior. The authors formalize the notion of methodological diversity by considering the sequence of decision outcomes that constitute a methodology. They show that diversity of decision implies likely diversity of behavior for the different versions developed under such forced diversity. For certain one-out-of-n systems the authors obtain an optimal method for allocating diversity\u00a0\u2026", "num_citations": "332\n", "authors": ["453"]}
{"title": "Stochastic reliability-growth: A model for fault-removal in computer-programs and hardware-designs\n", "abstract": " An assumption commonly made in early models of software reliability is that the failure rate of a program is a constant multiple of the (unknown) number of faults remaining. This implies that all faults contribute the same amount to the failure rate of the program. The assumption is challenged and an alternative proposed. The suggested model results in earlier fault-fixes having a greater effect than later ones (the faults which make the greatest contribution to the overall failure rate tend to show themselves earlier, and so are fixed earlier), and the DFR property between fault fixes (assurance about programs increases during periods of failure-free operation, as well as at fault fixes). The model is tractable and allows a variety of reliability measures to be calculated. Predictions of total execution time to achieve a target reliability, and total number of fault fixes to target reliability, are obtained. The model might also apply to\u00a0\u2026", "num_citations": "308\n", "authors": ["453"]}
{"title": "Software reliability model for modular program structure\n", "abstract": " The paper treats a modular program in which transfers of control between modules follow a semi-Markov process. Each module is failure-prone, and the different failure processes are assumed to be Poisson. The transfers of control between modules (interfaces) are themselves subject to failure. The overall failure process of the program is described, and an asymptotic Poisson process approximation is given for the case when the individual modules and interfaces are very reliable. A simple formula gives the failure rate of the overall program (and hence mean time between failures) under this limiting condition. The remainder of the paper treats the consequences of failures. Each failure results in a cost, represented by a random variable with a distribution typical of the type of failure. The quantity of interest is the total cost of running the program for a time t, and a simple approximating distribution is given for large t\u00a0\u2026", "num_citations": "301\n", "authors": ["453"]}
{"title": "Theories of software reliability: How good are they and how can they be improved?\n", "abstract": " An examination of the assumptions used in early bug-counting models of software reliability shows them to be deficient. Suggestions are made to improve modeling assumptions and examples are given of mathematical implementations. Model verification via real-life data is discussed and minimum requirements are presented. An example shows how these requirements may be satisfied in practice. It is suggested that current theories are only the first step along what threatens to be a long road.", "num_citations": "279\n", "authors": ["453"]}
{"title": "Evaluating testing methods by delivered reliability [software]\n", "abstract": " There are two main goals in testing software: (1) to achieve adequate quality (debug testing), where the objective is to probe the software for defects so that these can be removed, and (2) to assess existing quality (operational testing), where the objective is to gain confidence that the software is reliable. Debug methods tend to ignore random selection of test data from an operational profile, while for operational methods this selection is all-important. Debug methods are thought to be good at uncovering defects so that these can be repaired, but having done so they do not provide a technically defensible assessment of the reliability that results. On the other hand, operational methods provide accurate assessment, but may not be as useful for achieving reliability. This paper examines the relationship between the two testing goals, using a probabilistic analysis. We define simple models of programs and their testing\u00a0\u2026", "num_citations": "239\n", "authors": ["453"]}
{"title": "Software reliability and dependability: a roadmap\n", "abstract": " Software's increasing role creates both requirements for being able to trust it more than before, and for more people to know how much they can trust their software. A sound engineering approach requires both techniques for producing reliability and sound assessment of the achieved results. Different parts of industry and society face different challenges: the need for education and cultural changes in some areas, the adaptation of known scientific results to practical use in others, and in others still the need to confront inherently hard problems of prediction and decision-making, both to clarify the limits of current understanding and to push them back.", "num_citations": "235\n", "authors": ["453"]}
{"title": "Modeling software design diversity: a review\n", "abstract": " Design diversity has been used for many years now as a means of achieving a degree of fault tolerance in software-based systems. While there is clear evidence that the approach can be expected to deliver some increase in reliability compared to a single version, there is no agreement about the extent of this. More importantly, it remains difficult to evaluate exactly how reliable a particular diverse fault-tolerant system is. This difficulty arises because assumptions of independence of failures between different versions have been shown to be untenable: assessment of the actual level of dependence present is therefore needed, and this is difficult. In this tutorial, we survey the modeling issues here, with an emphasis upon the impact these have upon the problem of assessing the reliability of fault-tolerant systems. The intended audience is one of designers, assessors, and project managers with only a basic\u00a0\u2026", "num_citations": "205\n", "authors": ["453"]}
{"title": "Recalibrating software reliability models\n", "abstract": " There is no universally applicable software reliability growth model which can be trusted to give accurate predictions of reliability in all circumstances. A technique of analyzing predictive accuracy called the u-plot allows a user to estimate the relationship between the predicted reliability and the true reliability. It is shown how this can be used to improve reliability predictions in a very general way by a process of recalibration. Simulation results show that the technique gives improved reliability predictions in a large proportion of cases. However, a user does not need to trust the efficacy of recalibration, since the new reliability estimates produced by the technique are truly predictive and their accuracy in a particular application can be judged using the earlier methods. The generality of this approach suggests its use whenever a software reliability model is used. Indeed, although this work arose from the need to\u00a0\u2026", "num_citations": "194\n", "authors": ["453"]}
{"title": "Some conservative stopping rules for the operational testing of safety critical software\n", "abstract": " Operational testing, which aims to generate sequences of test cases with the same statistical properties as those that would be experienced in real operational use, can be used to obtain quantitative measures of the reliability of software. In the case of safety critical software it is common to demand that all known faults are removed. This means that if there is a failure during the operational testing, the offending fault must be identified and removed. Thus an operational test for safety critical software takes the form of a specified number of test cases (or a specified period of working) that must be executed failure-free. This paper addresses the problem of specifying the numbers of test cases (or time periods) required for a test, when the previous test has terminated as a result of a failure. It has been proposed that, after the obligatory fix of the offending fault, the software should be treated as if it were completely novel, and\u00a0\u2026", "num_citations": "170\n", "authors": ["453"]}
{"title": "A reliability model for systems with Markov structure\n", "abstract": " A system is considered in which switching takes place between sub\u2010systems according to a continuous parameter Markov chain. Failures may occur in Poisson processes in the sub\u2010systems, and in the transitions between sub\u2010systems. All failure processes are independent. The overall failure process is described exactly and asymptotically for highly reliable sub\u2010systems. An application to process\u2010control computer software is suggested.", "num_citations": "170\n", "authors": ["453"]}
{"title": "How to measure software reliability and how not to\n", "abstract": " The paper criticises the underlying assumptions which have been made in much early modeling of computer software reliability. The following suggestions will improve modeling. 1) Do not apply hardware techniques to software without thinking carefully. Software differs from hardware in important respects; we ignore these at our peril. In particular-2) Do not use MTTF, MTBF for software, unless certain that they exist. Even then, remember that- 3) Distributions are always more informative than moments or parameters; so try to avoid commitment to a single measure of reliability. Anyway- 4) There are better measures than MTTF. Percentiles and failure rates are more intuitively appealing than means. S) Software reliability means operational reliability. Who cares how many bugs are in a program? We should be concerned with their effect on its operation. In fact- 6) Bug identification (and elimination) should be\u00a0\u2026", "num_citations": "160\n", "authors": ["453"]}
{"title": "The risks of software\n", "abstract": " MOSt of us have experienced some kind of problem related to com puter failure: a bill mailed in er ror or a day's work destroyed by some mysterious glitch in a desktop comput er. Such nuisances, often caused by soft ware faults, or\" bugs,\" are merely incon venient when compared with the conse quences of computer failures in critical systems. Software bugs caused the se ries of large-scale outages of telephone service in the US A software problem may have prevented the Patriot missile system from tracking the Iraqi Scud missile that killed 28 American soldiers during the Gulf War. Indeed, software faults are generally more insidious and much more difficult to handle than are physical defects.The problems essentially arise from complexity, which increases the possi bility that design faults will persist and emerge in the final product. Convention al engineering has made great strides in the understanding and control\u00a0\u2026", "num_citations": "148\n", "authors": ["453"]}
{"title": "Redundancy and diversity in security\n", "abstract": " Redundancy and diversity are commonly applied principles for fault tolerance against accidental faults. Their use in security, which is attracting increasing interest, is less general and less of an accepted principle. In particular, redundancy without diversity is often argued to be useless against systematic attack, and diversity to be of dubious value. This paper discusses their roles and limits, and to what extent lessons from research on their use for reliability can be applied to security, in areas such as intrusion detection. We take a probabilistic approach to the problem, and argue its validity for security. We then discuss the various roles of redundancy and diversity for security, and show that some basic insights from probabilistic modelling in reliability and safety indeed apply to examples of design for security. We discuss the factors affecting the efficacy of redundancy and diversity, the role of \u201dindependence\u00a0\u2026", "num_citations": "146\n", "authors": ["453"]}
{"title": "Applying Bayesian belief networks to system dependability assessment\n", "abstract": " The dependability of technological systems is a growing social concern. Increasingly computer based systems are developed that carry the potential of increasing catastrophic consequences from single accidents. There have been significant research advances in assessment methods. However dependability assessment of computer systems in practice is still a very uncertain and often ad-hoc procedure. Decision making about system dependability is an uncertain affair and must account of failures in expertise and be capable of integrating different sources of evidence. A more meaningful way of reasoning about systems dependability can be achieved by rejecting current ad-hoc dependability assessment methods and replacing them with the idea of dependability argumentation. Bayesian Belief Networks (BBN\u2019s) is proposed as the most promising technology to support this kind of dependability argumentation.", "num_citations": "112\n", "authors": ["453"]}
{"title": "Assessing dependability of safety critical systems using diverse evidence\n", "abstract": " A primary objective of the DATUM (Dependability Assessment of safety critical systems Through the Unification of Measurable evidence) project was to improve the way dependability of software intensive safety-critical systems was assessed. The authors' hypothesis was that improvements were possible if multiple types of evidence could be incorporated. To achieve the objective, the authors had to investigate how to obtain improved dependability predictions given certain specific information over and above failure data alone. A framework for modelling uncertainty and combining diverse evidence was provided in such a way that it could be used to represent an entire argument about a system's dependability. The various methods and technologies for modelling uncertainty were examined in depth and a Bayesian approach was selected as the most appropriate method. To implement this approach for combining\u00a0\u2026", "num_citations": "110\n", "authors": ["453"]}
{"title": "Criteria for software reliability model comparisons\n", "abstract": " A set of criteria is proposed for the comparison of software reliability models. The intention is to provide a logically organized basis for determining the superior models and for the presentation of model characteristics. It is hoped that in the future, a software manager will be able to more easily select the model most suitable for his/her requirements from among the preferred ones.", "num_citations": "109\n", "authors": ["453"]}
{"title": "The use of multilegged arguments to increase confidence in safety claims for software-based systems: A study based on a BBN analysis of an idealized example\n", "abstract": " The work described here concerns the use of so-called multilegged arguments to support dependability claims about software-based systems. The informal justification for the use of multilegged arguments is similar to that used to support the use of multiversion software in pursuit of high reliability or safety. Just as a diverse 1-out-of-2 system might be expected to be more reliable than each of its two component versions, so might a two-legged argument be expected to give greater confidence in the correctness of a dependability claim (for example, a safety claim) than would either of the argument legs alone. Our intention here is to treat these argument structures formally, in particular, by presenting a formal probabilistic treatment of \"confidence,\" which will be used as a measure of efficacy. This will enable claims for the efficacy of the multilegged approach to be made quantitatively, answering questions such as\u00a0\u2026", "num_citations": "106\n", "authors": ["453"]}
{"title": "A Bayesian reliability model with a stochastically monotone failure rate\n", "abstract": " A reliability model is proposed with the usual continuous random variable representation of the working times of the system between failures. The model utilizes a probabilistic repair rule, via a Bayesian argument, in order to simulate decay (or improvement) of reliability in time.", "num_citations": "106\n", "authors": ["453"]}
{"title": "New ways to get accurate reliability measures (software)\n", "abstract": " Two techniques that analyze prediction accuracy and enhance predictive power of a software reliability model are presented. The u-plot technique detects systematic differences between predicted and observed failure behavior, allowing the recalibration of a software reliability model to obtain more accurate predictions. The perpetual likelihood ratio (PLR) technique compares two models' abilities to predict a particular data source so that the one that has been most accurate over a sequence of predictions can be selected. The application of these techniques is illustrated using three sets of real failure data.< >", "num_citations": "104\n", "authors": ["453"]}
{"title": "On measurement of operational security\n", "abstract": " Ideally, a measure of the security of a system should capture quantitatively the intuitive notion of \"the ability of the system to resist attack.\" That is, it should be operational, reflecting the degree to which the system can be expected to remain free of security breaches under particular conditions of operation (including attack). Instead, current security levels at best merely reflect the extensiveness of safeguards introduced during the design and development of a system. Whilst we might expect a system developed to a higher level than another to exhibit \"more secure behavior\" in operation, this cannot be guaranteed; more particularly, we cannot infer what the actual security behavior will be from knowledge of such a level. In the paper we discuss similarities between reliability and security with the intention of working toward measures of \"operational security\" similar to those that we have for reliability of systems. Very\u00a0\u2026", "num_citations": "99\n", "authors": ["453"]}
{"title": "Predictably dependable computing systems\n", "abstract": " Systems engineers are increasingly having to deal with the problem of how to make the process of designing and constructing dependable computing systems much more predictable and cost-effective. The great challenge about dependability is that it is a systems issue, since virtually all aspects of a computing system, and of the means by which it was specified, designed and constructed, can affect the system's overall dependability. This book explores links, and gaps, between topics that are often investigated separately, but whose interactions can be of considerable relevance to issues of overall system dependability. It contains material on all four of the main topics that are crucial to the successful production of dependable computing systems namely: fault prevention, fault tolerance, fault removal, and fault forecasting. Particular emphasis is placed on the problems of real-time and distributed computing systems. This book provides up to date information about the latest research on these topics from a team made up of many of Europe's leading researchers-it is based on the work of two successive major ESPRIT Basic Research Projects on Predictably Dependable Computing Systems. These projects lasted over six years in total, and each involved approximately forty researchers at any one time. The book contains a carefully edited selection from among the over two hundred published papers produced by the PDCS projects and provides a good general overview of the work of the two projects, as well as coverage of most of the projects' major research achievements.", "num_citations": "90\n", "authors": ["453"]}
{"title": "Likelihood function of a debugging model for computer software reliability\n", "abstract": " A simple model for software reliability growth, originally suggested by Jelinski & Moranda, has been widely used but suffers from difficulties associated with parameter estimation. We show that a major reason for obtaining nonsensical results from the model is its application to data sets which exhibit decreasing reliability. We present a simple, necessary and sufficient condition for the maximum likelihood estimates to be finite and suggest that this condition be tested prior to using the model.", "num_citations": "90\n", "authors": ["453"]}
{"title": "Modeling the effects of combining diverse software fault detection techniques\n", "abstract": " Considers what happens when several different fault-finding techniques are used together. The effectiveness of such multi-technique approaches depends upon a quite subtle interplay between their individual efficacies. The modeling tool we use to study this problem is closely related to earlier work on software design diversity which showed that it would be unreasonable even to expect software versions that were developed truly independently to fail independently of one another. The key idea was a \"difficulty function\" over the input space. Later work extended these ideas to introduce a notion of \"forced\" diversity. In this paper, we show that many of these results for design diversity have counterparts in diverse fault detection in a single software version. We define measures of fault-finding effectiveness and diversity, and show how these might be used to give guidance for the optimal application of different fault\u00a0\u2026", "num_citations": "85\n", "authors": ["453"]}
{"title": "The impact of diversity upon common mode failures\n", "abstract": " Recent models for the failure behaviour of systems involving redundancy and diversity have shown that common mode failures can be accounted for in terms of the variability of the failure probability of components over operational environments. Whenever such variability is present, we can expect that the overall system reliability will be less than we could have expected if the components could have been assumed to fail independently. We generalise a model of hardware redundancy due to Hughes, [Hughes, R. P., A new approach to common cause failure. Reliab. Engng, 17 (1987) 211\u2013236] and show that with forced diversity, this unwelcome result no longer applies: in fact it becomes theoretically possible to do better than would be the case under independence of failures. An example shows how the new model can be used to estimate redundant system reliability from component data.", "num_citations": "71\n", "authors": ["453"]}
{"title": "Confidence: its role in dependability cases for risk assessment\n", "abstract": " Society is increasingly requiring quantitative assessment of risk and associated dependability cases. Informally, a dependability case comprises some reasoning, based on assumptions and evidence, that supports a dependability claim at a particular level of confidence. In this paper we argue that a quantitative assessment of claim confidence is necessary for proper assessment of risk. We discuss the way in which confidence depends upon uncertainty about the underpinnings of the dependability case (truth of assumptions, correctness of reasoning, strength of evidence), and propose that probability is the appropriate measure of uncertainty. We discuss some of the obstacles to quantitative assessment of confidence (issues of composability of subsystem claims; of the multi-dimensional, multi-attribute nature of dependability claims; of the difficult role played by dependence between different kinds of evidence\u00a0\u2026", "num_citations": "63\n", "authors": ["453"]}
{"title": "Forecasting software reliability\n", "abstract": " Computer software fails because of the presence of intellectual faults, ranging from simple coding faults to fundamental design faults. In principle, such faults can be permanently removed when they are detected by failure of the software. Then the software will exhibit reliability growth. The problem considered here is the one of forecasting this growth: it includes the estimation of the current reliability of the program from the previous failure data. We begin with a brief description of the software failure process: a non-stationary stochastic process. Several of the best-known software reliability growth models are described, and examples given of their performance on real software failure data. They shown marked disagreement and thus reveal a need for methods of comparing and evaluating software reliability forecasts. Several simple techniques for conducting this evaluation are described and illustrated using\u00a0\u2026", "num_citations": "63\n", "authors": ["453"]}
{"title": "Rationale for a modified Duane model\n", "abstract": " The Duane model for reliability growth involves a rate function which is an inverse power law and has an ``infinite'' value at t = 0. The model is usually motivated entirely empirically. Here a probabilistic rationale is proposed via a reliability growth model involving the removal of design faults. This rationale results in a modified power law rate, finite at the origin. A wider class of rate functions should be investigated for NHPP models of reliability growth.", "num_citations": "61\n", "authors": ["453"]}
{"title": "Choosing a testing method to deliver reliability\n", "abstract": " Testing methods are compared in a model where program failures are detected and the software changed to eliminate them. The question considered is whether it is better to use tests that seek out failures (\" debug testing\") or to simulate usage and find failures along the way (\" operational testing'').\" Better\" is measured by the delivered reliability obtained after all test failures have been eliminated. This comparison extends previous work, where the measure was the probability of detecting a failure. The theoretical treatment of the paper is probabilistic and analytical. Revealing special cases are exhibited in which each kind of testing is superior.", "num_citations": "57\n", "authors": ["453"]}
{"title": "A conceptual model of multi-version software\n", "abstract": " Recent work by Eckhardt and Lee shows that independently developed program versions will fail dependently: specifically that simultaneous failure of several versions is greater than would be the case under true independence. We show there is a precise duality between inpuc choice and program choice in this model and consider a generalisation in which different versions may be developed using diverse methodologies.'The use of diverse methodologies is shown to decrease the probability of simultaneous failure of several versions. For certain I-out-of-n systems we obtain an optimal method for allocating diversity between versions. For 2-nut-of-3 systems there seem to be no simple optimality results which do not depend on constraints which cannot be verified in practice.", "num_citations": "54\n", "authors": ["453"]}
{"title": "Multi-Legged Arguments: The Impact of Diversity upon Confidence in Dependability Arguments.\n", "abstract": " Intellectual diversity\u2013difference\u2013has long been used in human affairs to minimise the impact of mistakes. In the past couple of decades design diversity has been used to seek dependability in software-based systems. This use of design diversity prompted the first formal studies of the efficacy of intellectual diversity. In this paper we examine diverse arguments\u2013in particular arguments to support claims about system dependability (reliability, safety). Our purpose is to see whether the probabilistic approach that has been so successful in design diversity can be applied to diversity in arguments. The work reported here is somewhat tentative and speculative.", "num_citations": "52\n", "authors": ["453"]}
{"title": "The Littlewood-Verrall model for software reliability compared with some rivals\n", "abstract": " The paper criticizes some assumptions adopted in various models of software reliability. Suggestions are made to overcome the difficulties; in particular the Littlewood-Verrall model is advocated as a plausible alternative. Some preliminary results are reported of an attempt to validate the L-V model using real data. In particular, goodness-of-fit tests are employed to compare actual times-to-failure with the distributions of times-to-failure predicted by the model. Tentative evidence is presented that the nonexponential distributions of the L-V model are superior to the simple exponential distributions of the other models.", "num_citations": "52\n", "authors": ["453"]}
{"title": "A semi-Markov model for software reliability with failure costs\n", "abstract": " Early work on probabilistic modelling of software reliability has treated the program as a black box with certain special properties and aimed to represent the occurrence of failures and/or the detection of bugs. This paper is a preliminary attempt to extend this work in two ways:(1) by incorporating some knowledge of the structure of the program; and (2) by treating consequences of failures (costs), rather than merely the failures themselves. It is assumed that the program can be partitioned into R subprograms and that dynamic control is exercised by switching among these according to a semi-Markov process in real (continuous) time. When subprogram i is occupied failures occur according to a Poisson process with rate typical of subprogram i. Associated with such a failure is a random variable Yi, representing the cost of that failure and having a distribution function G;(x). The overall failure point process can be described exactly, but is very complex. However, it can be shown to be asymptotically a Poisson process when the individual subprogram failure rates become vanishingly small. This is a generalization of the author's earlier result when the switching was assumed to be Markovian, and it provides a rationale for using standard results from reliability practice in a more widely applicable framework. The main result of the paper concerns the vector whose ith element is the total cost due to failures of subprogram i during (0, 1). The distribution of this vector is shown to be asymptotically multivariate normal, and it is shown how to obtain the mean vector and covariance matrix explicitly. The asymptotic results of the paper depend only on the first two\u00a0\u2026", "num_citations": "51\n", "authors": ["453"]}
{"title": "A reliability model for Markov structured software\n", "abstract": " A system is considered in which switching takes place between sub-systems according to a continuous parameter Markov chain. Failures may occur in Poisson processes in the sub-systems, and in the transitions between sub-systems. All failure processes are independent. The overall failure process is described exactly and asymptotically for highly reliable sub-systems. An application to process-control computer software is suggested.", "num_citations": "51\n", "authors": ["453"]}
{"title": "Limits to evaluation of software dependability\n", "abstract": " inherent uncertainty It has been said that the term software engineering is an aspiration not a description. We would like to be able to claim that we engineer software, in the same sense that we engineer an aero-engine, but most of us would agree that this is not currently an accurate description of our activities. My suspicion is that it never will be. From the point of view of this essay-ie dependability evaluation-a major difference between software and other engineering artefacts is that the former is pure design. Its unreliability is always the result of design faults, which in turn arise as a result of human intellectual failures. The unreliability of hardware systems, on the other hand, has tended until recently to be dominated by random physical failures of components-the consequences of the \u2018perversity of nature\u2019. Reliability theories have been developed over the years which have successfully allowed systems to be built to high reliability requirements, and the final system reliability to be evaluated accurately. Even for pure hardware systems, without software, however, the very success of these theories has more recently highlighted the importance of design faults in determining", "num_citations": "48\n", "authors": ["453"]}
{"title": "A Bayesian modification to the Jelinski-Moranda software reliability growth model\n", "abstract": " The Jelinski-Moranda (JM) model for software reliability growth is one of the most commonly cited (often in its guise as the \u2018Musa model\u2019). Recent studies show that the reliability estimates and predictions given by the model are often grossly inaccurate. It has been suggested that one reason for this poor performance may be the use of the maximum-likelihood method of inference. This paper describes a Bayesian version of the model and shows that it is sometimes an improvement on JM. However, both versions have a tendency to give optimistic answers, probably owing to a key, but implausible, underlying assumption common to both models. The authors conclude that the generally poor performance of the models is such that they should only be used with great caution.", "num_citations": "46\n", "authors": ["453"]}
{"title": "Software reliability: achievement and assessment\n", "abstract": " Software reliability: achievement and assessment | Guide books ACM Digital Library home ACM home Google, Inc. (search) Advanced Search Browse About Sign in Register Advanced Search Journals Magazines Proceedings Books SIGs Conferences People More Search ACM Digital Library SearchSearch Advanced Search Browse Browse Digital Library Collections More HomeBrowse by TitleBooksSoftware reliability: achievement and assessment ABSTRACT No abstract available. Index Terms 1.Software reliability: achievement and assessment 1.General and reference 1.Cross-computing tools and techniques 1.Reliability 2.Software and its engineering 1.Software organization and properties 1.Extra-functional properties 1.Software reliability Comments Login options Check if you have access through your login credentials or your institution to get full access on this article. Sign in Full Access Get this Publication :\u2026", "num_citations": "46\n", "authors": ["453"]}
{"title": "Assessing the reliability of diverse fault-tolerant software-based systems\n", "abstract": " We discuss a problem in the safety assessment of automatic control and protection systems. There is an increasing dependence on software for performing safety-critical functions, like the safety shut-down of dangerous plants. Software brings increased risk of design defects and thus systematic failures; redundancy with diversity between redundant channels is a possible defence. While diversity techniques can improve the dependability of software-based systems, they do not alleviate the difficulties of assessing whether such a system is safe enough for operation. We study this problem for a simple safety protection system consisting of two diverse channels performing the same function. The problem is evaluating its probability of failure in demand. Assuming failure independence between dangerous failures of the channels is unrealistic. One can instead use evidence from the observation of the whole system's\u00a0\u2026", "num_citations": "42\n", "authors": ["453"]}
{"title": "Assessment of the reliability of fault-tolerant software: a Bayesian approach\n", "abstract": " Fault tolerant systems based on the use of software design diversity may be able to achieve high levels of reliability more cost-effectively than other approaches, such as heroic debugging. Earlier experiments have shown multi-version software systems to be more reliable than the individual versions. However, it is also clear that the reliability benefits are much worse than would be suggested by naive assumptions of failure independence between the versions. It follows that it is necessary to assess the reliability actually achieved in a fault tolerant system. The difficulty here mainly lies in acquiring knowledge of the degree of dependence between the failures processes of the versions. The paper addresses the problem using Byesian inference. In particular, it considers the problem of choosing a prior distribution to represent the beliefs of an expert assessor. It is shown that this is not easy, and some pitfalls\u00a0\u2026", "num_citations": "42\n", "authors": ["453"]}
{"title": "Stopping rules for the operational testing of safety-critical software\n", "abstract": " It has been proposed to conduct a test of a software safety system for a nuclear reactor by subjecting it to demands that are statistically representative of those it meets in operational use. The intention behind the test is to acquire a high confidence (99%) that the probability of failure on demand is smaller than 10/sup -3/. To this end the test takes the form of executing about 5000 demands and requiring that all of these are successful. In practice if is necessary to consider what happens if the software fails the test and is repaired. We argue that the earlier failure information needs to be taken into account in devising the form of the test that the modified software needs to pass-essentially that after such failure the testing requirement might need to be more stringent (i.e. the number of tests that must be executed failure-free should increase). We examine a Bayesian approach to the problem, for this stopping rule based\u00a0\u2026", "num_citations": "42\n", "authors": ["453"]}
{"title": "Predicting software reliability\n", "abstract": " This paper surveys some aspects of the state of the art of software reliability modelling. By far the greatest effort to date has been expended on the problem of assessing and predicting the reliability growth which takes place as faults are found and fixed, so the greater part of the paper addresses this problem. We begin with a simple conceptual model of the software failure process in order to set the scene and motivate the detailed stochastic models which follow. This conceptual model suggests certain minimal characteristics which all growth models for software should possess. There are now several detailed models which aim to represent software reliability growth, but their accuracy of prediction seems to vary greatly from one application to another. As it is not possible to decide a priori which will give the most accurate answers for a particular context, the potential user is faced with a dilemma. There seems to be\u00a0\u2026", "num_citations": "42\n", "authors": ["453"]}
{"title": "A Bayesian differential debugging model for software reliability\n", "abstract": " An assumption commonly made in early models of software reliability is that the failure rate of a program is a constant multiple of the number of bugs remaining. This implies that all bugs have the same effect upon the overall failure rate. The assumption is challenged and an alternative proposed. The suggested model results in earlier bug-fixes having a greater effect than later ones (the worst bug show themselves earlier and so are fixed earlier), and the DFR properly between bug-fixes (confidence in programs increases during periods of failure-free operation, as well as at bug-fixes). The model shows a high degree of mathematical tractability, and allows a range of reliability, and allows a range of reliability measures to be calculated exactly. Predictions of total execution time to achieve a target reliability, are obtained.", "num_citations": "42\n", "authors": ["453"]}
{"title": "Techniques for prediction analysis and recalibration\n", "abstract": " The previous chapter gives a comprehensive summary of many soft-ware reliability models that have appeared in the literature. Unfortunately, no single model has emerged that can be universally recommended to a potential user. In fact, the accuracy of the reliability measures arising from the models tends to vary quite dramatically: some models sometimes give good results, some models often perform inadequately, but no model can be trusted to be accurate in all circumstances. Worse than this, it does not seem possible to identify a priori those data sets for which a particular model will be appropriate [Abde86bl.This unsatisfactory position has undoubtedly been the major factor in the poor take-up of these techniques. Users who have experienced poor results adopt a once-bitten-twice-shy approach, and are unwilling to try new techniques. It is with some trepidation that we claim that the approach presented in\u00a0\u2026", "num_citations": "39\n", "authors": ["453"]}
{"title": "A non\u2010parametric order statistics software reliability model\n", "abstract": " This paper addresses a family of probability models for the failure time process known as order statistics models. Conventional order statistics models make rather strong distributional assumptions about the detection times: typically they assume that these come from some parametric family of distributions. In this paper a new model is presented that relaxes these distributional assumptions, and\u2014in the tradition of non\u2010parametric statistics generally\u2014\u2018allows the data to speak for themselves\u2019. The accuracy of the new model is compared on some real data sets with the predictions that come from several of the better parametric reliability growth models \u00a9 1998 John Wiley & Sons, Ltd.", "num_citations": "36\n", "authors": ["453"]}
{"title": "A discussion of practices for enhancing diversity in software designs\n", "abstract": " This report discusses the practices which have been used or recommended for increasing the degree of diversity between redundant implementations of software or software-based systems. Its purpose is to give useful indications for designers, project managers and safety/reliability assessors in deciding about how great an advantage should be expected from the use of these practices, in absolute and in comparative terms. Existing knowledge does not allow one to state any strong general recommendations, but it is possible to improve on the intuitive justifications usually given for these various practices. This report clarifies the ways the various practices are conjectured to aid system reliability, the factors that should affect their efficacy, and thus, for a practitioner, the aspects of a specific project situation that need to be considered to inform decisions. Thus this report", "num_citations": "33\n", "authors": ["453"]}
{"title": "Combination of predictions obtained from different software reliability growth models\n", "abstract": " In the development of techniques for software reliability measurement and prediction, many software reliability growth models have been proposed. Application of these models to real data sources has shown that there is commonly great disagreement in predictions, while none of them has been shown to be more trustworthy than others in terms of predictive quality in all applications. Recent work has largely overcome this problem through the development of specialized techniques which analyse the accuracy of predictions from reliability models. Such techniques allow the user to choose, for future predictions for a particular data source, those models which gave the best predictions in the past, for this data.               In this paper, various methods are used to get new predictions by combining the predictions obtained from different models. For each data set, the weights used in the combination of the models\u00a0\u2026", "num_citations": "33\n", "authors": ["453"]}
{"title": "Towards operational measures of computer security: Experimentation and modelling\n", "abstract": " The two experiments described here were intended to investigate the empirical issues that arise from the probabilistic view of security assessment discussed in the previous paper. Specifically, they investigated the problems of measuring effort and reward associated with security attacks and breaches.", "num_citations": "33\n", "authors": ["453"]}
{"title": "Software reliability and metrics\n", "abstract": " The Centre for Software Reliability, CSR, was established in 1982 to provide a focus in the UK for interest in software reliability: its achievement and assessment. CSR is a strictly non-profit-making organisation which is made up of three components:(1) its Council members\u2014a group of leading UK experts in software engineering drawn from industry and academia who meet regularly, under a formal constitution, to plan and organise CSR activities;(2) its ordinary members\u2014several hundred interested professionals and academics who regularly attend the CSR seminars which take place four times per year;(3) the two University research centres, one at the City University, London and one at the University of Newcastle upon Tyne which provide infrastructure to", "num_citations": "32\n", "authors": ["453"]}
{"title": "Dependability assessment of software-based systems: state of the art\n", "abstract": " This paper presents a personal and rather selective view of the state of the art of some aspects of dependability assessment for software-based systems. This short note gives a brief outline of the issues that the author addresses.", "num_citations": "31\n", "authors": ["453"]}
{"title": "A Bayesian model that combines disparate evidence for the quantitative assessment of system dependability\n", "abstract": " For safety-critical systems, the required reliability (or safety) is often extremely high. Assessing the system, to gain confidence that the requirement has been achieved, is correspondingly hard, particularly when the system depends critically upon extensive software. In practice, such an assessment is often carried out rather informally, taking account of many different types of evidence\u2014experience of previous, similar systems; evidence of the efficacy of the development process; testing; expert judgement, etc. Ideally, the assessment would allow all such evidence to be combined into a final numerical measure of reliability in a scientifically rigorous way. In this paper we address one part of this problem: we present a means whereby our confidence in a new product can be augmented beyond what we would believe merely from testing that product, by using evidence of the high dependability in operation of\u00a0\u2026", "num_citations": "30\n", "authors": ["453"]}
{"title": "E-voting: Dependability requirements and design for dependability\n", "abstract": " Elections are increasingly dependent on computers and telecommunication systems. Such \"e-voting\" schemes create socio-technical systems (combinations of technology and human organisations) that are complex and critical, as the future of nations depends on their proper operation. Thus heated debate surrounds their adoption and the possible methods for making them demonstrably dependable. We discuss the dependability requirements for such systems, and the design issues in ensuring their satisfaction, with reference to a recent proposal that uses cryptography for fault tolerance, in order to avoid some of the perceived dangers of electronic voting. Our treatment highlights the need for considering the whole socio-technical system, and for integrating security and fault tolerance viewpoints.", "num_citations": "29\n", "authors": ["453"]}
{"title": "Design diversity: an update from research on reliability modelling\n", "abstract": " Diversity between redundant subsystems is, in various forms, a common design approach for improving system dependability. Its value in the case of software-based systems is still controversial. This paper gives an overview of reliability modelling work we carried out in recent projects on design diversity, presented in the context of previous knowledge and practice. These results provide additional insight for decisions in applying diversity and in assessing diverse-redundant systems. A general observation is that, just as diversity is a very general design approach, the models of diversity can help conceptual understanding of a range of different situations. We summarise results in the general modelling of common-mode failure, in inference from observed failure data, and in decision-making for diversity in development.", "num_citations": "29\n", "authors": ["453"]}
{"title": "On the quality of software reliability prediction\n", "abstract": " We suggest that users are interested solely in the quality of predictions which can be obtained from software reliability models. Some ways of analysing the quality of predictions are proposed and several models and inference procedures are compared on real software failure data sets. We conclude that some predictions are extremely poor: notably those arising from ML analysis of the Jelinski-Moranda model. Others seem quite good. We suggest promising areas for future work.", "num_citations": "29\n", "authors": ["453"]}
{"title": "The use of proof in diversity arguments\n", "abstract": " The limits to the reliability that can be claimed for a design-diverse fault-tolerant system are mainly determined by the dependence that must be expected in the failure behaviours of the different versions: claims for independence between version failure processes are not believable. We examine a different approach, in which a simple secondary system is used as a back-up to a more complex primary. The secondary system is sufficiently simple that claims for its perfection (with respect to design faults) are possible, but there is not complete certainty about such perfection. It is shown that assessment of the reliability of the overall fault-tolerant system in this case may take advantage of claims for independence that are more plausible than those involved in design diversity.", "num_citations": "27\n", "authors": ["453"]}
{"title": "Critical task of writing dependable software\n", "abstract": " Safety-critical software must perform as desired and should never fail. The need for dependability stems from the fact that the consequences of failure are extremely high, usually a threat to human life. To write such systems, most now agree that we must adopt rigorous techniques, rooted in mathematics.< >", "num_citations": "27\n", "authors": ["453"]}
{"title": "Evaluating software engineering standards and methods\n", "abstract": " Background The last five years have seen the introduction of a large number of standards specifically targeted at im-proving the quality of software systems or systems containing software components. At the highest level are so-called quality system standards such as the inter-national standard ISO 9001 (14) and its British equivalent BS 5750 [7], which offer generic procedural guidelines intended to be implemented as part of a company-specific policy. Next are the more specific software engineering standards, such as the US Depart-ment of Defense standard DOD-STD 2167A [12] and the American national standard ANSI/IEEE 983 [2], which cover much of the software development process. At a step down from this we find standards such as the proposed UK Ministry of Defence standard DEF-STAN 00-55 (11], which are targeted at specific application areas such as safety-critical systems. Other standards are targeted at a specific lifecycle activity-for example, ANSI/IEEE 1028 [3] for reviews and audits, or BS 5887 [8] for software testing. Finally, we are now seeing the emergence of standards, such as the CCTA standard for SSADM (10), which advocate and describe the use of specific methodologies and tools, including formal specification and development methods. Central to all these standards and methods is the notion that\" quality\" software systems can be produced only by imposing rigorous controls over the development process. One of the major reasons for the poor industrial take-up of the plethora of new software engi-neering standards and methods is that the potential user of these (the software developer, in this case) has no realistic\u00a0\u2026", "num_citations": "25\n", "authors": ["453"]}
{"title": "Tools for the analysis of the accuracy of software reliability predictions\n", "abstract": " Different software reliability models can produce very different answers when called upon to predict future reliability in a reliability growth context. Users need to know which, if any, of the competing predictions are trustworthy. Some techniques are presented which form the basis of a partial solution to this problem. In addition, it is shown that this approach can point the way towards more accurate prediction via models which learn from past behaviour.", "num_citations": "23\n", "authors": ["453"]}
{"title": "MTBF is meaningless in software reliability\n", "abstract": " MTBF is meaningless in software reliability we should be careful of defining the reliability of software in a way analogous to that of hardware. In particular, mtbf should be excluded B. LITTLEWOOD from software reliability standards. It should not be thought that these results imply a great difference Recent papers [1, 2] on reliability growth models for computer soft-between failure-time distributions in software and hardware. It can ware have resulted in time-to-next-failure distributions without moments. be shown that a distribution with moments can be approximated The lack, in particular, of mtbf has been regretted by some users of the arbitrarily closely by a moment-lessdistribution (in the sense that the models, and various techniques have been used to overcome this apparentpercentage points can bemade arbitrarily close). Here is perhaps the defect. The purpose of this note is to show that under certain assump\u00a0\u2026", "num_citations": "23\n", "authors": ["453"]}
{"title": "A critique of the Jelinski-Moranda model for software reliability\n", "abstract": " The paper discusses some problems associated with an early model for software reliability growth during debugging, first proposed by Jelinski and Moranda. It is suggested that the assumption of all faults contributing equally to the overall failure rate is overly naive and can be improved. Necessary and sufficient conditions are given for maximum likelihood (ML) estimates of the model parameters to be finite. Since other authors have shown from simulated data that ML estimates can be misleading even when finite, it is important that the goodness-of-fit tests be performed. Such a test on one set of data shows the model to perform badly; an alternative model due to Littlewood and Verrallis better.", "num_citations": "22\n", "authors": ["453"]}
{"title": "Reasoning about the reliability of multi-version, diverse real-time systems\n", "abstract": " This paper is concerned with the development of reliable real-time systems for use in high integrity applications. It advocates the use of diverse replicated channels, but does not require the dependencies between the channels to be evaluated. Rather it develops and extends the approach of Little wood and Rush by (for general systems) by investigating a two channel system in which one channel, A, is produced to a high level of reliability (i.e. has a very low failure rate), while the other, B, employs various forms of static analysis to sustain an argument that it is perfect (i.e. it will never miss a deadline). The first channel is fully functional, the second contains a more restricted computational model and contains only the critical computations. Potential dependencies between the channels (and their verification) are evaluated in terms of aleatory and epistemic uncertainty. At the aleatory level the events ''A fails\" and ''B is\u00a0\u2026", "num_citations": "21\n", "authors": ["453"]}
{"title": "The problems of assessing software reliability... when you really need to depend on it\n", "abstract": " This paper looks at the ways in which the reliability of software can be assessed and predicted. It shows that the levels of reliability that can be claimed with scientific justification are relatively modest.", "num_citations": "21\n", "authors": ["453"]}
{"title": "The role of models in managing the uncertainty of software-intensive systems\n", "abstract": " It is increasingly argued that uncertainty is an inescapable feature of the design and operational behaviour of software-intensive systems. This paper elaborates the role of models in managing such uncertainty, in relation to evidence and claims for dependability. Personal and group models are considered with regard to abstraction, consensus and corroboration. The paper focuses on the predictive property of models, arguing for the need for empirical validation of their trustworthiness through experimentation and observation. The impact on trustworthiness of human fallibility, formality of expression and expressiveness is discussed. The paper identifies two criteria for deciding the degree of trust to be placed in a model, and hence also for choosing between models, namely accuracy and informativeness. Finally, analogy and reuse are proposed as the only means by which empirical evidence can be established for\u00a0\u2026", "num_citations": "21\n", "authors": ["453"]}
{"title": "A Bayesian modification to the Jelinski-Moranda software reliability growth model\n", "abstract": " The Jelinski-Moranda (JM) model for software reliability was examined. It is suggested that a major reason for the poor results given by this model is the poor performance of the maximum likelihood method (ML) of parameter estimation. A reparameterization and Bayesian analysis, involving a slight modelling change, are proposed. It is shown that this new Bayesian-Jelinski-Moranda model (BJM) is mathematically quite tractable, and several metrics of interest to practitioners are obtained. The BJM and JM models are compared by using several sets of real software failure data collected and in all cases the BJM model gives superior reliability predictions. A change in the assumption which underlay both models to present the debugging process more accurately is discussed.", "num_citations": "21\n", "authors": ["453"]}
{"title": "The need for evidence from disparate sources to evaluate software safety\n", "abstract": " A system may fail because of its engineering hardware, computer hardware, computer software, or because of a human component. The impact on overall system dependability of hardware is well understood, provided that it is free from design faults. Furthermore, we can often engineer our systems so that the impact of these sources of unreliability is negligible.             However, some hardware failures and all software failures are due to design faults. Reliability in the presence of design faults and human operator errors is not well understood. This poses acute problems for the assessment of safety-critical systems in the presence of the effects of human errors, made during the design process or during operation. It can easily be shown that only modest reliability can be demonstrated by the direct observation of the system in test or operation. In this paper we discuss these problems in detail and consider some\u00a0\u2026", "num_citations": "19\n", "authors": ["453"]}
{"title": "Probabilistic assessment of safety-critical software: why and how?\n", "abstract": " Traditionally, safety-critical control and/or monitoring systems are assigned requirements in terms of catastrophic failure probabilities through a complex process starting with statistics on mortality, continuing through risk assessment for the determination of the requirements for a given application (eg, nuclear plant, civil airliner, subway), and culminating in the allocation of a reliability requirement for the control and/or monitoring system embedded in the application. Examples of such requirements are a failure rate less than [10. sup.-9]/h for flight control systems of civil airliners or a probability of failure on demand less than [10. sup.-5] for monitoring systems of nuclear power plants. The first of these clearly cannot be checked simply by testing the product. The second is on the boundary of what can be assured by such testing-based evaluation.For precomputer realizations of control and/or monitoring systems\u00a0\u2026", "num_citations": "19\n", "authors": ["453"]}
{"title": "The effect of testing on reliability of fault-tolerant software\n", "abstract": " Previous models have investigated the impact upon diversity - and hence upon the reliability of fault-tolerant software built from 'diverse' versions - of the variation in 'difficulty' of demands over the demand space. These models are essentially static, taking a single snapshot view of the system. In this paper, we consider a generalisation in which the individual versions are allowed to evolve - and their reliability to grow - through debugging. In particular, we examine the trade-off that occurs in testing between, on the one hand, the increasing reliability of individual versions, and on the other hand the possible diminution of diversity.", "num_citations": "18\n", "authors": ["453"]}
{"title": "Probabilistic guarantees for fault-tolerant real-time systems\n", "abstract": " Hard real-time systems are usually required to provide an absolute guarantee that all tasks will execute by their deadlines. In this paper we address fault tolerant hard realtime systems, and introduce the notion of a probabilistic guarantee. Schedulability analysis is used together with sensitivity analysis to establish the maximum fault frequency that a system can tolerate. The fault model is then used to derive a probability (likelihood) that, during the lifetime of the system, faults will not arrive faster than this maximum rate. The framework presented is a general one that can accommodate transient \u2018software\u2019faults, tolerated by recovery blocks or exception handling; or transient \u2018hardware\u2019faults dealt with by state restoration and re-execution.", "num_citations": "18\n", "authors": ["453"]}
{"title": "Software reliability modelling: achievements and limitations\n", "abstract": " An examination is made of the direct evaluation of the reliability of a software product from observation of its actual failure process during operation, using reliability growth modeling techniques described. Conclusions drawn from perfect working are examined. An assessment is also made of indirect sources of confidence in dependability. The author argues that one way forward might be to insist that certain safety-critical system should only be built if the safety case can be demonstrated to rely only on assurable levels of design dependability.<>", "num_citations": "18\n", "authors": ["453"]}
{"title": "Bayesian belief network model for the safety assessment of nuclear computer-based systems\n", "abstract": " The formalism of Bayesian Belief Networks (BBNs) is being increasingly applied to probabilistic modelling and decision problems in a widening variety of fields. This method provides the advantages of a formal probabilistic model, presented in an easily assimilated visual form, together with the ready availability of efficient computational methods and tools for exploring model consequences. Here we formulate one BBN model of a part of the safety assessment task for computer and software based nuclear systems important to safety. Our model is developed from the perspective of an independent safety assessor who is presented with the task of evaluating evidence from disparate sources: the requirement specification and verification documentation of the system licensee and of the system manufacturer; the previous reputation of the various participants in the design process; knowledge of commercial pressures;information about tools and resources used; and many other sources. Based on these multiple sources of evidence, the independent assessor is ultimately obliged to make a decision as to whether or not the system should be licensed for operation within a particular nuclear plant environment. Our BBN model is a contribution towards a formal model of this decision problem. We restrict attention to a part of this problem: the safety analysis of the Computer System Specification documentation. As with other BBN applications we see this modelling activity as having several potential benefits. It employs a rigorous formalism as a focus for examination, discussion, and criticism of arguments about safety. It obliges the modeller to be very explicit\u00a0\u2026", "num_citations": "17\n", "authors": ["453"]}
{"title": "Adaptive software reliability modeling\n", "abstract": " Software reliability models are used to estimate and predict reliability using data from past failure behaviour. Recent work suggests that the accuracy of the predictions can vary considerably from model to model for a particular data set, and from data set to data set for a particular model. Methods of analysing the quality of prediction for a particular model and a particular program have been reported earlier. In this paper we use these techniques to construct a general adaptive procedure which allows current predictions to be improved in the light of past predictive behaviour. The procedure works on a wide class of models.", "num_citations": "17\n", "authors": ["453"]}
{"title": "Bayesian belief networks for safety assessment of computer-based systems\n", "abstract": " Bayesian belief networks for safety assessment of computer-based systems | System performance evaluation ACM Digital Library home ACM home Google, Inc. (search) Advanced Search Browse About Sign in Register Advanced Search Journals Magazines Proceedings Books SIGs Conferences People More Search ACM Digital Library SearchSearch Advanced Search Browse Browse Digital Library Collections More HomeBrowse by TitleBooksSystem performance evaluation: methodologies and applicationsBayesian belief networks for safety assessment of computer-based systems chapter Bayesian belief networks for safety assessment of computer-based systems Share on Authors: Bev Littlewood View Profile , Lorenzo Strigini View Profile , David Wright View Profile , Norman Fenton View Profile , Martin Neil View Profile , P.-J. Courtois View Profile Authors Info & Affiliations Publication: System performance : \u20131! \u2026", "num_citations": "16\n", "authors": ["453"]}
{"title": "What makes a reliable program: few bugs, or a small failure rate?\n", "abstract": " It is instructive to look at some of the reasons advanced by software developers for their reluctance to use software reliability measurement tools. Here are a few common ones:", "num_citations": "16\n", "authors": ["453"]}
{"title": "A note on reliability estimation of functionally diverse systems\n", "abstract": " It has been argued that functional diversity might be a plausible means of claiming independence of failures between two versions of a system. We present a model of functional diversity, in the spirit of earlier models of diversity such as those of Eckhardt and Lee, and Hughes. In terms of the model, we show that the claims for independence between functionally diverse systems seem rather unrealistic. Instead, it seems likely that functionally diverse systems will exhibit positively correlated failures, and thus will be less reliable than an assumption of independence would suggest. The result does not, of course, suggest that functional diversity is not worthwhile; instead, it places upon the evaluator of such a system the onus to estimate the degree of dependence so as to evaluate the reliability of the system.", "num_citations": "15\n", "authors": ["453"]}
{"title": "Examination of Bayesian belief network for safety assessment of nuclear computer-based systems\n", "abstract": " We report here on a continuation of work on the Bayesian Belief Network (BBN)model described in [Fenton, Littlewood et al. 1998]. As explained in the previous deliverable, our model concerns one part of the safety assessment task for computer and software based nuclear systems. We have produced a first complete, functioning version of our BBN model by eliciting a large numerical node probability table (NPT) required for our \u2018Design Process Performance\u2019 variable. The requirement for such large numerical NPTs poses some difficult questions about how, in general, large NPTs should be elicited from domain experts. We report about the methods we have devised to support the expert in building and validating a BBN. On the one hand, we have proceeded by eliciting approximate descriptions of the expert\u2019s probabilistic beliefs, in terms of properties like stochastic orderings among distributions; on the other hand, we have explored ways of presenting to the expert visual and algebraic descriptions of relations among variables in the BBN, to assist the expert in an ongoing assessment of the validity of the BBN.", "num_citations": "15\n", "authors": ["453"]}
{"title": "Measurement for software control and assurance\n", "abstract": " For many years software engineers and managers have paid lip service to the need for quantitative methods for software product and process control. However, in practice industry has been slow to adopt such methods. This reluctance to use measurement methods has been excused, with some justification, on the grounds that many proposed metrics and models were of dubious relevance and/or inadequately validated, and methods of data collection were expensive and unreliable.Many research workers and practitioners believe that quantitative approaches are now mature enough to be more widely used. In addition, the emergence of integrated project support environments (IPSEs) provide the framework in which many of the practical difficulties of data collection, storage and analysis can be avoided. The aim of the CSR Conference on Measurement for Software Assurance and Control\u2014and this record of the proceedings\u2014are to:", "num_citations": "15\n", "authors": ["453"]}
{"title": "Advantages of open source processes for reliability: clarifying the issues\n", "abstract": " Some authors maintain that open source software processes are particularly well-suited for delivering good reliability. We discuss this kind of statement, first clarifying the different measures of reliability and of a process\u2019s ability to deliver it that can be of interest, and then proposing a way of addressing part of it via probabilistic modelling. We present a model of the reliability improvement process that results from the use of the software and the fixing of reported faults, which takes account of the effect on this process of the variety of software use patterns within the user community. We show preliminary, interesting, non intuitive results concerning the conjecture that a more diverse population of users engaged in reporting faults may give OSS processes an advantage over conventional industrial processes, in terms of fast reliability growth after release, and discuss further possible developments.", "num_citations": "14\n", "authors": ["453"]}
{"title": "Learning to Live with Uncertainty in our Software\n", "abstract": " Software measurement has been most successful in evaluating and predicting dependability, particularly reliability. There are, however severe limitations to the levels of reliability that can be assured in ways that are scientifically meaningful. The existence of these limits has serious social implications if we wish to place great dependence upon computer systems-for example in some safety-critical applications.< >", "num_citations": "14\n", "authors": ["453"]}
{"title": "Data Collection for Security Fault Forecasting-Pilot Experiment\n", "abstract": " In most contexts, it is not feasible to guarantee that a system is 100% secure. Measures and predictions of operational security of computer systems are therefore obviously of interest to any owner of a system which is a candidate for potential intruders. Such measures would allow assessment of current and future expected loss to thesystem owner due to security breaches in a given attacking environment and a given level of protection. In [Littlewood, Brocklehurst et al. 1991] a probabilistic approach to modelling operational security, analogous to that used in reliability, is suggested. It is clear that empirical data would be useful in deriving a plausible probabilistic approach to security modelling. Such data can be acquired experimentally, by allowing a group of selected people to perform security attacks on a given computer system in a controlled way. The attack process can then be monitored and relevant data recorded. This document describes such an experiment. As far as we are aware, this is the first attempt to conduct such an experiment, and our intention was more to explore general feasibility than to collect data that provides significant information for modelling. This pilot experiment did indeed give some valuable information on how future full-scale experiments of this kind should be performed and the results and recommendations for improvements to the experimental set-up are discussed here.", "num_citations": "12\n", "authors": ["453"]}
{"title": "Software reliability and safety\n", "abstract": " Software Reliability and Safety | Guide books ACM Digital Library home ACM home Google, Inc. (search) Advanced Search Browse About Sign in Register Advanced Search Journals Magazines Proceedings Books SIGs Conferences People More Search ACM Digital Library SearchSearch Advanced Search Browse Browse Digital Library Collections More HomeBrowse by TitleBooksSoftware Reliability and Safety ABSTRACT No abstract available. Comments Login options Check if you have access through your login credentials or your institution to get full access on this article. Sign in Full Access Get this Publication Information Contributors Published in Guide books cover image Software Reliability and Safety March 1991 232 pages ISBN:1851665331 Editors: B. Littlewood, D. Miller Copyright \u00a9 1991 Publisher Elsevier Science Inc. United States Publication History Published: 1 March 1991 Qualifiers book Metrics \u2026", "num_citations": "12\n", "authors": ["453"]}
{"title": "Conservative Bounds for the pfd of a 1-out-of-2 Software-Based System Based on an Assessor's Subjective Probability of\" Not Worse Than Independence\"\n", "abstract": " We consider the problem of assessing the reliability of a 1-out-of-2 software-based system, in which failures of the two channels cannot be assumed to be independent with certainty. An informal approach to this problem assesses the channel probabilities of failure on demand (pfds) conservatively, and then multiplies these together in the hope that the conservatism will be sufficient to overcome any possible dependence between the channel failures. Our intention here is to place this kind of reasoning on a formal footing. We introduce a notion of \"not worse than independence\"' and assume that an assessor has a prior belief about this, expressed as a probability. We obtain a conservative prior system pfd, and show how a conservative posterior system pfd can be obtained following the observation of a number of demands without system failure. We present some illustrative numerical examples, discuss some of the\u00a0\u2026", "num_citations": "11\n", "authors": ["453"]}
{"title": "\u201cValidation of ultra-high dependability\u2026\u201d\u201320 years on\n", "abstract": " Reuse: Copies of full items can be used for personal research or study, educational, or not-for-profit purposes without prior permission or charge. Provided that the authors, title and full bibliographic details are credited, a hyperlink and/or URL is given for the original metadata page and the content is not changed in any way.", "num_citations": "11\n", "authors": ["453"]}
{"title": "Conservative Reasoning about the Probability of Failure on Demand of a 1-out-of-2 Software-Based System in Which One Channel Is\" Possibly Perfect\"\n", "abstract": " In earlier work, [11] (henceforth LR), an analysis was presented of a 1-out-of-2 software-based system in which one channel was \u201cpossibly perfect\u201d. It was shown that, at the aleatory level, the system pfd (probability of failure on demand) could be bounded above by the product of the pfd of channel A and the pnp (probability of nonperfection) of channel B. This result was presented as a way of avoiding the well-known difficulty that for two certainly-fallible channels, failures of the two will be dependent, i.e., the system pfd cannot be expressed simply as a product of the channel pfds. A price paid in this new approach for avoiding the issue of failure dependence is that the result is conservative. Furthermore, a complete analysis requires that account be taken of epistemic uncertainty-here concerning the numeric values of the two parameters pfd A  and pnp B . Unfortunately this introduces a different difficult problem of\u00a0\u2026", "num_citations": "10\n", "authors": ["453"]}
{"title": "Choosing Between Fault-Tolerance and Increased V&V for Improving Reliability.\n", "abstract": " Fault tolerant systems based on the use of software design diversity may be able to achieve high levels of reliability more cost-effectively than other approaches, such as heroic debugging. Earlier experiments have shown that multi-version software systems are more reliable than the individual versions. However, it is also clear that the reliability benefits are much worse than would be suggested by naive assumptions of failure independence between the versions.To decide whether to use design diversity or other means for achieving the desired reliability a developer would need to know how they compare from the viewpoint of cost-effectiveness. Empirical data are insufficient for deciding this question, and expert opinions differ. We refute a recently published argument in favour of diversity and in the process show some general factors deciding whether process improvement, or debugging of the versions in a multiple-version system, will increase or decrease the statistical correlation between failures of the versions. The conclusion is that there is as yet no evidence that the choice between design diversity and other means of reliability improvement can be decided by general arguments rather than by detailed (and uncertain) special-case analysis.", "num_citations": "10\n", "authors": ["453"]}
{"title": "The use of computers in safety-critical applications\n", "abstract": " I should like to thank the Chairman and the members of the NuSAC Study Group on the Safety of Operational Computer Systems for their efforts in producing this report on a topic of considerable current interest. The Committee endorses the report and its recommendations, and commends it to all involved in the field.", "num_citations": "10\n", "authors": ["453"]}
{"title": "A non-parametric approach to software reliability prediction\n", "abstract": " The large amount of literature on software reliability assessment and prediction is essentially concerned with parametric models: the inter failure time random variables are assumed to come from parametric families of distributions. Such models involve quite strong assumptions. The motivation for the present work is to relax these assumptions and-in the tradition of non parametric statistics generally-'allow the data to speak for themselves'. We present a new non-parametric model for reliability prediction which is based upon the use of kernel density estimators and compare its accuracy on some real data sets with the predictions that come from several of the better conventional models. These initial results are encouraging: the new models seem to perform as well as the best of the earlier models.", "num_citations": "10\n", "authors": ["453"]}
{"title": "How good are software reliability predictions\n", "abstract": " CiNii \u8ad6\u6587 - How good are software reliability predictions CiNii \u56fd\u7acb\u60c5\u5831\u5b66\u7814\u7a76\u6240 \u5b66\u8853\u60c5\u5831 \u30ca\u30d3\u30b2\u30fc\u30bf[\u30b5\u30a4\u30cb\u30a3] \u65e5\u672c\u306e\u8ad6\u6587\u3092\u3055\u304c\u3059 \u5927\u5b66\u56f3\u66f8\u9928\u306e\u672c\u3092\u3055\u304c\u3059 \u65e5\u672c\u306e\u535a\u58eb\u8ad6\u6587\u3092\u3055\u304c\u3059 \u65b0\u898f \u767b\u9332 \u30ed\u30b0\u30a4\u30f3 English \u691c\u7d22 \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u9589\u3058\u308b \u30bf\u30a4\u30c8\u30eb \u8457\u8005\u540d \u8457\u8005ID \u8457\u8005 \u6240\u5c5e \u520a\u884c\u7269\u540d ISSN \u5dfb\u53f7\u30da\u30fc\u30b8 \u51fa\u7248\u8005 \u53c2\u8003\u6587\u732e \u51fa\u7248\u5e74 \u5e74\u304b\u3089 \u5e74\u307e\u3067 \u691c\u7d22 \u691c\u7d22 \u691c\u7d22 How good are software reliability predictions LITTLEWOOD B. \u88ab\u5f15\u7528\u6587\u732e: 1\u4ef6 \u8457\u8005 LITTLEWOOD B. \u53ce\u9332 \u520a\u884c\u7269 Software Reliability-Achievement and Assessment- Software Reliability-Achievement and Assessment-, 154-166, 1987 Blackwell Scientific Publications \u88ab\u5f15\u7528\u6587\u732e: 1\u4ef6\u4e2d 1-1\u4ef6\u3092 \u8868\u793a 1 \u30bd\u30d5\u30c8\u30a6\u30a7\u30a2\u4fe1\u983c\u6027\u306e\u30ce\u30f3\u30d1\u30e9\u30e1\u30c8\u30ea\u30c3\u30af\u63a8\u5b9a\u306b\u95a2\u3059\u308b\u8003\u5bdf \u6e9d\u53e3 \u771f\u592a\u90ce , \u571f\u80a5 \u6b63 \u96fb\u5b50\u60c5\u5831\u901a\u4fe1 \u5b66\u4f1a\u6280\u8853\u7814\u7a76\u5831\u544a. R, \u4fe1\u983c\u6027 109(94), 23-28, 2009-06-12 \u53c2\u8003\u6587\u732e8\u4ef6 \u88ab\u5f15\u7528\u6587\u732e1\u4ef6 Tweet \u5404\u7a2e\u30b3\u30fc\u30c9 NII\u8ad6\u6587ID(NAID) 10025423645 \u8cc7\u6599\u7a2e\u5225 \u56f3\u66f8\u306e\u4e00\u90e8 \u30c7\u30fc\u30bf\u63d0\u4f9b\u5143 CJP\u5f15\u7528 \u306b\u306b\u306b/\u2026", "num_citations": "10\n", "authors": ["453"]}
{"title": "A conceptual model of the effect of diverse methodologies on coincident failures in multi-version software\n", "abstract": " Eckhardt and Lee have shown that, in a precisely defined sense, the independent development of multi-version software cannot result in independence of failure behaviour. We have shown in earlier work that the use of diverse methodologies (forced diversity) for the development of the several versions may overcome this problem. Indeed, it is theoretically possible to obtain versions which exhibit better than independent behaviour. In this paper we try to formalise the notion of methodological diversity by considering the sequence of decision outcomes which comprises a methodology. We show that diversity of decisions implies likely diversity of behaviour for the different versions developed under such forced diversity. We define a measure of diversity of methodologies and show that there are simple orderings in the behavioural diversity resulting from the particular choices in design decisions. In particular it\u00a0\u2026", "num_citations": "10\n", "authors": ["453"]}
{"title": "Guidelines for statistical testing\n", "abstract": " This document provides an introduction to statistical testing. Statistical testing of software is here defined as testing in which the test cases are produced by a random process meant to produce different test cases with the same probabilities with which they would arise in actual use of the software. Statistical testing of software has these main advantages: for the purpose of reliability assessment and product acceptance, it supports directly estimates of reliability, and thus decisions on whether the software is ready for delivery or for use in a specific system. This feature is unique to statistical testing; for the purpose of improving the software, it tends to discover defects which would cause failures with the higher frequencies before those that would cause less frequent failures, thus focusing correction efforts in the most cost-effective way and delivering better software for a given debugging effort. Statistical testing has been reported to achieve dramatic improvements; from the point of view of costs, it facilitates the automation of the test process, thus allowing more testing at acceptable cost than manual testing would allow. This document explains the basic theory underlying statistical testing and provides guidance for its application. The material is organised to facilitate use both as an introduction for software engineers who are new to this approach to testing, and as a reference source during application. Statistical testing is applicable to practically all kinds of software, so this document is not markedly specialised for space applications, though the examples are mostly space-related and the discussion of the software lifecycle is meant to apply to common\u00a0\u2026", "num_citations": "8\n", "authors": ["453"]}
{"title": "Limits to dependability assurance-A controversy revisited\n", "abstract": " \u2022 Microsoft Windows reliability has grown from 300 hours MTBF (with 95/98) to about 3000 hours despite increased size and complexity (ie more faults)\u2022 After-the-fact estimation of failure rates, based on extensive operational experience with software in aircraft and automobiles suggest very high reliabilities can be achieved i Automobiles: Ellims has estimated that no more than 5 deaths per year (and about 300 injuries) caused by software in the UK-suggests about 0.2 x 10-6 death/injury failures per hour. Even better per system-say 10-7 i Aircraft: very few accidents have been attributed to software; Shooman claims, again, about 10-7 per hour per system i But these are after-the-fact figures", "num_citations": "7\n", "authors": ["453"]}
{"title": "N-version design Versus one Good Version\n", "abstract": " The known experiments with software diversity confirm that indeed fault-tolerant software employing diversity is\" on average\" more reliable than a single software version. Utilising design diversity when high reliability is required is, nevertheless, problematic. The cost of diverse system, which may increase substantially, is not the only obstacle. The main problem is assessing how much gain in reliability has actually been achieved. It was shown empirically and theoretically that even versions developed by not communicating development teams tend to fail simultaneously more often than they should under if their failures were independent. Quantifying the dependence between failures is no easier than assessing the reliability of the system, very difficult, because the failures of such systems are very rare and putting a reasonable confidence in the assessment results requires a very long observation.The idea that diversity may be a more cost effective way to deliver high diversity is alive and recently it was spelled out by Hatton [Hatton 1997]. His main point, which seems to be shared by many, is that despite the dependence between the failures of the independently developed software versions, for a given target of the system reliability fault-tolerant software, eg 2-out-of-3 system, may be a more cost effective solution than a single software version. The key idea is that despite the dependence between the failures of the versions, a fault-tolerant system consisting of versions of'ordinary'quality will deliver better reliability than if the efforts to develop several channels were used to produce a single'state-of-the-art'version. Hatton concludes that taking into\u00a0\u2026", "num_citations": "7\n", "authors": ["453"]}
{"title": "A note on modelling functional diversity\n", "abstract": " It has been argued that functional diversity might be a plausible means of claiming independence of failures between two versions of a system. We present a model of functional diversity, in the spirit of earlier models of diversity such as those of Eckhardt and Lee, and Hughes. In terms of the model, we show that claims for independence between functionally diverse systems seem rather unrealistic. Instead, it seems likely that functionally diverse systems will exhibit positively correlated failures, and thus will be less reliable than an assumption of independence would suggest. The result does not, of course, suggest that functional diversity is not worthwhile; instead, it places upon the evaluator of such a system the onus to estimate the degree of dependence so as to evaluate the reliability of the system.", "num_citations": "7\n", "authors": ["453"]}
{"title": "Towards operational measures of computer security\n", "abstract": " Ideally, a measure of the security of a system should capture quantitatively the intuitive notion of \u2018the ability of the system to resist attack\u2019. That is, it should be operational, reflecting the degree to which the system can be expected to remain free of security breaches under particular conditions of operation (including attack). Instead, current security levels at best merely reflect the extensiveness of safeguards introduced during the design and development of a system. Whilst we might expect a system developed to a higher level than another to exhibit \u2018more secure behaviour\u2019in operation, this cannot be guaranteed; more particularly, we cannot infer what the actual security behaviour will be from knowledge of such a level. In the paper we discuss similarities between reliability and security with the intention of working towards measures of \u2018operational security\u2019similar to those that we have for reliability of systems. Very informally, these measures could involve expressions such as the rate of occurrence of security breaches (cf rate of occurrence of failures in reliability), or the probability that a specified \u2018mission\u2019can be accomplished without a security breach (cf reliability function). This new approach is based on the analogy between system failure and security breach. A number of other analogies to support this view are introduced. We examine this duality critically, and have identified a number of important open questions that need to be answered before this quantitative approach can be taken further. The work described here is therefore somewhat tentative, and one of our major intentions is to invite discussion about the plausibility and feasibility of this\u00a0\u2026", "num_citations": "7\n", "authors": ["453"]}
{"title": "Stochastic reliability growth: A model with applications to computer software faults and hardware design faults\n", "abstract": " An assumption commonly made in early models of software reliability is that the failure rate of a program is a constant multiple of the number of faults remaining. This implies that all faults have the same effect upon the overall failure rate. The assumption is challenged and an alternative proposed. The suggested model results in earlier fault-fixes having a greater effect than later ones (the worst faults show themselves earlier and so are fixed earlier), and the DFR property between fault-fixes (confidence in programs increases during periods of failure-free operations, as well as at fault-fixes). The model shows a high degree of mathematical tractability, and allows a range of reliability measures to be calculated exactly. Predictions of total execution time to achieve a target reliability, and total number of fault-fixes to target reliability, are obtained. It is suggested that the model might also find applications in those hardware\u00a0\u2026", "num_citations": "7\n", "authors": ["453"]}
{"title": "Comments on \u2018Reliability and performance analysis for fault-tolerant programs consisting of versions with different characteristics\u2019 by Gregory Levitin [Reliability Engineering\u00a0\u2026\n", "abstract": " This paper [Levitin G. Reliability and performance analysis for fault-tolerant programs consisting of versions with different characteristics. Reliab Eng Syst Safety 2004;86:75\u201381] presents a detailed reliability and performance analysis of fault-tolerant programs. Unfortunately, the treatment is dependent upon a key assumption: \u2018Failures of versions for each component are statistically independent\u2026\u2019 (Section 2). Such an assumption is not justified.", "num_citations": "6\n", "authors": ["453"]}
{"title": "On the use of diverse arguments to increase confidence in dependability claims\n", "abstract": " We are all familiar with informal ways in which diversity is used to increase confidence. For example, if you ask somebody else to check your arithmetic you are tacitly assuming that the use of a different person (intellectual diversity) is more likely to pick up your mistakes than if you simply checked your own work. This idea of \u2018a different pair of eyes\u2019 is widespread in science and engineering. Indeed, it could be said that the whole scientific culture, with its checks and balances based on review by colleagues (and rivals), is crucially dependent on the efficacy of diversity. More formally, deliberate diversity in design is often used to gain protection from common faults that may be present in merely redundant systems. In particular, in recent years design diversity has been proposed as a means of protecting against software faults (see [1] for a recent review of work in this area). In this chapter we consider the use of diversity\u00a0\u2026", "num_citations": "6\n", "authors": ["453"]}
{"title": "Dependability of modular software in a multiuser operational environment\n", "abstract": " Effects of shared use on the dependability of modular software are evaluated in terms of a generally defined stochastic model. The total system in question consists of a community of n users who share a software system with m modules. The input aspect of the operational environment, reflecting user demands at the module level, is represented by a continuous-time, finite-state Markov process, called the operational profile. The profile's construction is based on the isolated profiles of individual users which, in the case of heterogeneous use, are pairwise-distinct processes. Moreover, in the presence of other users, an individual profile can differ from its isolated version due to \"slowdowns\" caused by sharing. This multiuser profile is then combined with a failure model which, among other things, captures \"stress\" due to shared use. A number of basic issues are then addressed and resolved in terms of closed-form\u00a0\u2026", "num_citations": "6\n", "authors": ["453"]}
{"title": "Analyses of software failure data\n", "abstract": " In this paper we present and analyse a new set of software failure data which shows the failure behaviour, over a period of four years, of a single-user work station which was installed at the City University in March 1985. The details recorded in this data collection exercise allow us to subdivide the data into various subsets of inter-failure times. A sub-collection of these are chosen for more detailed analysis.Experience of applying reliability models in the past has shown that the relative predictive performance of the models depends entirely on the context. It has been found that there is no one model that performs well over all data sets. It has also been found that for some data sets all models applied are in error. In such cases two techniques for improving predictive accuracy have been shown to be beneficial: i) recalibrating the raw model predictions and ii) using the results of trend tests to apply the models. These two techniques may be used separately or in combination. This paper is mainly devoted to the first technique but we will also show the benefit to be gained by the application of the second technique. We apply a number of reliability models to the failure data and the recalibration technique and assess the performance of the resulting prediction systems.", "num_citations": "6\n", "authors": ["453"]}
{"title": "Practical statistical evaluation of critical software\n", "abstract": " Abstract In 2010, Rolf Spiker approached one of us with a query from a client concerning advisory material in IEC 61508 on the statistical evaluation of software. We realised that there is a dearth of practical guidance for those who wish to evaluate critical software statistically. We believe statistical evaluation of software is an increasingly important assurance technique. We commence with a brief introduction to some of the simpler statistics and then consider discursively the issues which arise during evaluation.", "num_citations": "5\n", "authors": ["453"]}
{"title": "Software reliability prediction\n", "abstract": " This chapter is a recap of two or three of the author's papers on the same themes, dating back to 1973. The previous papers have been edited somewhat. The additions and clarifications are minor, but there is at least one major omission. The author describes the way input, a processor, and output interact in the presence of errors and fixes. After a brief discussion of six software reliability models\u2014four of which are due in part to the author\u2014the paper gives the results of a competition [sic] between the Jelinski-Moranda (JM) and Littlewood-Verrall (LV) models. The author then provides a way of judging the predictive quality of an estimate or, more precisely, a family of estimates of reliability. In the \u201cexamples\u201d section of the chapter, the author applies both the JM and the LV models to a well-suited data set derived from an actual realization of a software failure (or detection) process and compares the developed\u00a0\u2026", "num_citations": "5\n", "authors": ["453"]}
{"title": "Validation of a software reliability model\n", "abstract": " 4. ConclusionAlthough the results shown here relate to only one set of data, they are encouraging in the support they give to our model. In fact the quality of fit between predictions and observations is better than I expected when I embarked upon this validation exercise. This work will continue, and it is hoped to analyse more data shortly, but already there is the beginnings of a case in support of the arguments given in references [2] and [3].", "num_citations": "5\n", "authors": ["453"]}
{"title": "Comments on \u2018Evolutionary neural network modelling for software cumulative failure time prediction\u2019by Liang Tian and Afzel Noore [Reliability Engineering and System Safety 87\u00a0\u2026\n", "abstract": " This paper [Tian L, Noore A. Evolutionary neural network modelling for software cumulative failure time prediction. Reliab Eng Syst Saf 2005; 87:45\u201351] purports to present a useful means of predicting the cumulative failure time function for software reliability growth. In fact, the nature of the \u2018prediction\u2019 is too simplistic to be of use. Furthermore, the authors' claims for the accuracy of the predictions appear to be without value.", "num_citations": "4\n", "authors": ["453"]}
{"title": "Software reliability modelling\n", "abstract": " 31 Software reliability modelling Professor of Software Engineering and Director, Centre for Software Reliability, City University Bev Littlewood Contents 31.1 Introduction 31/3 31.2 The inevitability of statistical modelling for software reliability 31/4 31.3 Some software reliability models 31/6 31.4 Using software reliability growth models 31/7 31.5 Analysis of the accuracy of reliability measures and model selection 31/9 31.6 Summary and discussion 31/9 31.7 References 31/11 Introduction 31/3 31.1 Introduction This chapter is concerned with evaluation of the reliability of software. It is worth remarking right away that evaluation raises issues quite different from those involved in actually building systems with appropriate functionality, reliability, availability, maintainability, security, etc. Other chapters in this book are concerned with this question of appropriate software engineering methodologies for the achievement of\u00a0\u2026", "num_citations": "4\n", "authors": ["453"]}
{"title": "Software reliability measurement\n", "abstract": " The paper is concerned with evaluation of the reliability of software. It addresses the question of 'how to know whether it has been done successfully', with particular reference to the achievement of reliability targets.< >", "num_citations": "4\n", "authors": ["453"]}
{"title": "Random sequential addition of hard atoms to the one\u2010dimensional integer lattice\n", "abstract": " This paper considers the random sequential addition of atoms of different sizes and distinguishable types to the one\u2010dimensional integer lattice (linear chain sites). In such a situation the different processes are competing for space on the lattice, and after a sufficiently long time has elapsed the chain will consist of a linear array of different atoms, in which configuration it will remain forever, since none of the processes is reversible. We present results which enable the proportion of sites occupied by each type of atom to be calculated: (a) numerically from a set of difference equations as a function of the chain length and (b) analytically, for a simplified model and infinite chain length.", "num_citations": "4\n", "authors": ["453"]}
{"title": "Equilibrium distribution of dimers in irradiated RNA molecules\n", "abstract": " The situation where dimer formation and splitting is possible in irradiated RNA molecules is considered. The equilibrium distribution of dimers ( i.e.  the distribution after a long time) is obtained, in particular the average number of dimmers as a function of molecule length and reaction rates. The average equilibrium dimer density is obtained for long chains, and this exact result is compared with earlier approximations to the full problem.", "num_citations": "4\n", "authors": ["453"]}
{"title": "The Law Commission presumption concerning the dependability of computer evidence\n", "abstract": " (1) In any proceedings, a statement in a document produced by a computer shall not be admissible as evidence of any fact stated therein unless it is shown-(a) that there are no reasonable grounds for believing that the statement is inaccurate because of improper use of the computer;(b) that at all material times the computer was operating properly, or if not, that any respect in which it was not operating properly or was out of operation was not such as to affect the production of the document or the accuracy of its contents; and.........", "num_citations": "3\n", "authors": ["453"]}
{"title": "Conservative reasoning about epistemic uncertainty for the probability of failure on demand of a 1-out-of-2 software-based system in which one channel is\" possibly perfect\"\n", "abstract": " In earlier work, (Littlewood and Rushby 2011) (henceforth LR), an analysis was presented of a 1-out-of-2 system in which one channel was \u201cpossibly perfect\u201d. It was shown that, at the aleatory level, the system pfd could be bounded above by the product of the pfd of channel A and the pnp (probability of non-perfection)of channel B. This was presented as a way of avoiding the well-known difficulty that for two certainly-fallible channels, system pfd cannot be expressed simply as a function of the channel pfds, and in particular not as a product of these. One price paid in this new approach is that the result is conservative \u2013 perhaps greatly so. Furthermore, a complete analysis requires that account be taken of epistemic uncertainty \u2013 here concerning the numeric values of the two parameters pfdA and pnpB. This introduces some difficulties, particularly concerning the estimation of dependence between an assessor\u2019s beliefs about the parameters. The work reported here avoids these difficulties by obtaining results that require only an assessor\u2019s marginal beliefs about the individual channels, i.e. they do not require knowledge of the dependence between these beliefs", "num_citations": "3\n", "authors": ["453"]}
{"title": "Reliability prediction of a software product using testing data from other products or execution environment\n", "abstract": " For safety-critical systems, the required reliability (or safety) is often extremely high. Assessing the system, to gain con dence that the requirement has been achieved, is correspondingly hard, particularly when the system depends critically upon extensive software. In practice, such an assessment is often carried out rather informally, taking account of many di erent types of evidence| experience of previous, similar systems; evidence of the e cacy of the development process; testing; expert judgement, etc. Ideally, the assessment would allow all such evidence to be combined into a nal numerical measure of reliability in a scienti cally rigorous way. In this paper we address one part of this problem: we present a means whereby our con dence in a new product can be augmented beyond what we would believe merely from testing that product, by using evidence of the high dependability in operation of previous products. The model we propose could equally be applied to increase our con dence that a product will operate reliably in a novel environment, using evidence of its past behaviour in previous environments. We present some illustrative numerical results that seem to suggest that such experience from previous products or environments, even where very high operational dependability has been achieved, can only modestly improve our con dence in the reliability of a new product or of an existing product when transferred to a new environment.", "num_citations": "3\n", "authors": ["453"]}
{"title": "Recommendations for the probity of computer evidence\n", "abstract": " There exists widespread misunderstanding about the nature of computers and how and why they are liable to fail. The present approach to the disclosure or discovery and evaluation of evidence produced by computers in legal proceedings is unsatisfactory. The central problem is the evidential presumption that computers are reliable. This presumption is not warranted. To this end, recommendations are proposed to rectify this problem with the aim of increasing the probability of a fair trial.", "num_citations": "2\n", "authors": ["453"]}
{"title": "Assessing the dependability of Software-based systems: the importance role of confidence\n", "abstract": " \u2022 Just because large complex programs can be very reliable, it does not mean you can assume that a particular one will be\u2013even if you have successfully produced reliable software in the past, you can\u2019t assume from this that a new program will be reliable\u2013even if some software engineering processes have been successful in the past, this does not guarantee they will produce reliable software next time", "num_citations": "2\n", "authors": ["453"]}
{"title": "On diversity, and the elusiveness of independence\n", "abstract": " Diversity, as a means of avoiding mistakes, is ubiquitous in human affairs. Whenever we invite someone else to check our work, we are taking advantage of the fact that they are different from us. In particular, we expect that their different outlook may allow them to see problems that we have missed. In this talk I shall look at the uses of diversity in systems dependability engineering.               In contrast to diversity, redundancy has been used in engineering from time immemorial to obtain dependability. Mathematical theories of reliability involving redundancy of components go back over half a century. But redundancy and diversity are not the same thing.               Redundancy, involving the use of multiple copies of similar (\u2018identical\u2019) components (e.g. in parallel) can be effective in protecting against random failures of hardware. In some cases, it is reasonable to believe that failures of such components will be\u00a0\u2026", "num_citations": "2\n", "authors": ["453"]}
{"title": "Software reliability (tutorial session) basic concepts and assessment methods\n", "abstract": " Software reliability is important for many sectors of the software industry. Besides knowing how to achieve it, it is important to know the actual reliability achieved in a specific software product. Assessing the reliability of software-based systems is increasingly necessary: more users bet larger amount of money, the survival of companies and at times the lives and limbs of people on the service they expect from the software. Sound decisionmaking requires some understanding of the uncertainties thus incurred. Meanwhile, software complexity increases and progress in development tools enables more poorlytrained people to build software-based systems. The shortterm economic incentive to use off-the-shelf software, even in sensitive applications, imposes new requirements to evaluate the risk thus assumed. The pressure on vendors to guarantee some level of quality of service will thus also increase, extending from\u00a0\u2026", "num_citations": "2\n", "authors": ["453"]}
{"title": "The CSR approach to software reliability prediction\n", "abstract": " A plethora of software reliability models have been developed over the years but, in spite of extravagant claims for their efficacy, none can be trusted to give accurate results in all circumstances. This presents potential users with a serious difficulty, and has probably been the main reason why there has been such a poor take-up of these techniques. Recent work in CSR has largely overcome this problem, and it is now possible in most cases to obtain reliability measures and predictions that can be trusted. This position arises not because of the creation of better models, but of techniques for analysing predictive accuracy. Essentially, it is now possible to apply many models to a particular source of software failure data, and decide which (if any) is giving results of acceptable accuracy. The techniques which have brought about this new perspective on software reliability modelling also bring with them a bonus: they enable even more accurate predictions to be made from the availab...", "num_citations": "2\n", "authors": ["453"]}
{"title": "Reliability Modelling for Fault-Tolerant Software\n", "abstract": " The overall aims of the workshop were to evaluate the current state of the art and, through this evaluation, lay the groundwork for future research directions.", "num_citations": "2\n", "authors": ["453"]}
{"title": "Repairable Systems Reliability: Modelling, Inference, Misconceptions and their Causes\n", "abstract": " 3. Repairable Systems Reliability: Modelling, Inference, Misconceptions and their Causes. By H. Ascher and H. Feingold. New York, Marcel Dekker, 1984. 223 p. SFr 119. (Lecture Notes in Statistics, Vol. 7)", "num_citations": "2\n", "authors": ["453"]}
{"title": "Limiting proportions of hydrations in irradiated RNA molecules\n", "abstract": " The reaction kinetics of irradiated RNA molecules is analysed for the cases when dimer formation (not splitting) and hydration of uracil residues take place. After a sufficient time has elapsed, such molecules consist entirely of dimers and hydrated sites, at which time it is of interest to know the average (over an ensemble of molecules) proportion of hydrated sites. This average is obtained (i) exactly as a function of molecule length and, (ii) to any desired accuracy for infinitely long molecules.", "num_citations": "2\n", "authors": ["453"]}
{"title": "Complexity is the enemy of dependability-can diversity provide a defence?\n", "abstract": " Summary form only given. Complexity is the enemy of dependability. The author notes that if one wants to build systems that are safe and reliable, we know that we should make them as simple as possible. Unfortunately, not all complexity is the result of poor design. Sometimes complexity is necessary. In such circumstances simplicity may not be achievable. How, then, do we make our systems dependable? It is argued that one way forward is through the use of diversity. The best-known applications of diversity in computing date back a couple of decades and involve design diversity, e.g. n-version programming. This kind of diversity, in the pursuit of fault tolerance, has had a checkered history. On the one hand, several famous experiments have shown that the benefits fall far short of what might be expected if the different versions were to fail independently. On the other hand, several serious industrial applications\u00a0\u2026", "num_citations": "1\n", "authors": ["453"]}
{"title": "Combination of qualitative and quantitative sources of knowledge for risk assessment.\n", "abstract": " This paper focuses on representation of the reliability of system in the framework of possibility theory. Particularly, given a (probabilistic) quantitative knowledge pertaining to the time to failure of a system (risk function) and some qualitative knowledge about the degree of pessimism and optimism about the information supplied by this quantitative knowledge, one may construct a possibilistic reliability function of the system. The latter models the possibility that the system survives up to the current time. The proposal is motivated by the observation that the system may fail even if the risk function considers this failure as unlikely. Besides, research from cognitive science shows that probability values, particularly when human factor is involved in the system tend to be overestimated. This methodology involves implicitly the combination of the two pieces of knowledge into a more refined knowledge expressed in the possibility framework. Several ramifications of the proposal, particularly considering the common exponential lifetime distribution will be investigated. On the other hand in the absence of any qualitative knowledge the proposal is straightforwardly related to the notion of probability-possibility transformations investigated in the fuzzy literature.", "num_citations": "1\n", "authors": ["453"]}
{"title": "Fault tolerance via diversity against design faults: design principles and reliability assessment\n", "abstract": " The international Space community has become comfortable over the years with a model of operations that is based on verbal delegation of operations control from a mission control center based on Earth. This model requires near-constant telemetry regarding the status of the vehicle, as well as tasks on-board. The future long-duration exploration missions being considered will require significant changes in this operational paradigm, adjusting to situational realities, capitalizing on the evolution that has occurred in vehicle autonomous health management, and maximizing the time crewmembers can devote to exploration. NASA has created an exploration strategy aimed at multiple destinations, utilizing multiple assets during operations, and increasing both the distance to exploration objectives and the duration of exploration flights. While developing the Lunar Malapert Excursion as a reference mission, our team\u00a0\u2026", "num_citations": "1\n", "authors": ["453"]}
{"title": "Reliability analyses of workstation failure data\n", "abstract": " Experience of applying reliability models in the past has shown that the relative predictive performance of the models depends entirely on the context. It has been found that there is no one model that performs well over all data sets. It has also been found that for some data sets all models are in error. In such cases two techniques for improving predictive accuracy have been shown to be beneficial: i) recalibrating the raw model predictions andii) using the results of trend tests preliminary to applying the models. These two techniques may be used separately or in combination. This paper is mainly devoted to the first technique but we will also show the benefit to be gained by the application of the second technique. We apply a number of reliability models to some failure data collected from a workstation installed at City University, together with the recalibration technique, and assess the performance of the resulting\u00a0\u2026", "num_citations": "1\n", "authors": ["453"]}
{"title": "Evaluation of software dependability\n", "abstract": " It has been said that the term software engineering is an aspiration not a description. We would like to be able to claim that we engineer software, in the same sense that we engineer an aero-engine, but most of us would agree that this is not currently an accurate description of our activities. My suspicion is that it never will be.  From the point of view of this essay \u2013 i.e. dependability evaluation \u2013 a major difference between software and other engineering artefacts is that the former is pure design. Its unreliability is always the result of design faults, which in turn arise as a result of human intellectual failures. The unreliability of hardware systems, on the other hand, has tended until recently to be dominated by random physical failures of components \u2013 the consequences of the \u2018perversity of nature\u2019. Reliability theories have been developed over the years which have successfully allowed systems to be built to high reliability requirements, and the final system reliability to be evaluated accurately. Even for pure hardware systems, without software, however, the very success of these theories has more recently highlighted the importance of design faults in determining the overall reliability of the final product. The conventional hardware reliability theory does not address this problem at all.  In the case of software, there is no physical source of failures, and so none of the reliability theory developed for hardware is relevant. We need new theories that will allow us to achieve required dependability levels, and to evaluate the actual dependability that has been achieved, when the sources of the faults that ultimately result in failure are human intellectual failures.", "num_citations": "1\n", "authors": ["453"]}
{"title": "Why We Should Learn Not To Depend Too Much Upon Software\n", "abstract": " It is argued that there are severe limitations to the levels of quantified software dependability that can be measured with the kinds of evidence that one could reasonably expect to be available. This has serious implications for the builders of certain safety-critical systems.", "num_citations": "1\n", "authors": ["453"]}
{"title": "Limitaciones del soporte l\u00f3gico\n", "abstract": " Limitaciones del soporte l\u00f3gico - Dialnet Ayuda \u00bfEn qu\u00e9 podemos ayudarle? \u00d7 Buscar en la ayuda Buscar Consultar la ayuda \u00bfEn qu\u00e9 podemos ayudarle? \u00d7 Buscar en la ayuda Buscar Consultar la ayuda Ir al contenido Dialnet Buscar Revistas Tesis Congresos Ayuda Limitaciones del soporte l\u00f3gico Autores: Bev Littlewood, Lorenzo Strigini Localizaci\u00f3n: Investigaci\u00f3n y ciencia, ISSN 0210-136X, N\u00ba 196, 1993, p\u00e1gs. 20-26 Idioma: espa\u00f1ol Texto completo no disponible (Saber m\u00e1s ...) Fundaci\u00f3n Dialnet Acceso de usuarios registrados Imagen de identificaci\u00f3n Identificarse \u00bfOlvid\u00f3 su contrase\u00f1a? \u00bfEs nuevo? Reg\u00edstrese Ventajas de registrarse Dialnet Plus M\u00e1s informaci\u00f3n sobre Dialnet Plus Opciones de compartir Facebook Twitter Opciones de entorno Sugerencia / Errata \u00a9 2001-2021 Fundaci\u00f3n Dialnet \u00b7 Todos los derechos reservados Dialnet Plus Accesibilidad Aviso Legal Coordinado por: Fundaci\u00f3n Dialnet IB\u2026", "num_citations": "1\n", "authors": ["453"]}
{"title": "Measurement-based modelling issues\u2014the problem of assuring ultra-high dependability\n", "abstract": " It can be shown that the problem of assuring ulna-high reliability of software is very difficult. Essentially, if we need to convince ourselves that the failure rate is extremely low, it is infeasible to do this by direct observation of the failure behaviour and subsequent statistical analysis. Whilst it seems likely that we can never obtain scientifically meaningful measures of reliability at the levels sometimes required (eg 10-9 failures per hour for the A320 flight-critical control system), there is a great need to be able to improve on the present state of the art. One way forward is to incorporate into our judgements of software reliability information from other sources than direct observation of failure behaviour; experimentation is one such source of information. In this note, I will first consider some generic problems of experimentation in software engineering, and then discuss how we might use experiments to address the special\u00a0\u2026", "num_citations": "1\n", "authors": ["453"]}
{"title": "Adaptive software reliability modelling\n", "abstract": " In this chapter two methods of adapting raw predictions by learning from past mistakes are outlined. Techniques for assessing the quality of these adapted predictions are discussed with particular reference to the prequential likelihood ratio (PLR) as a means of comparing the raw predictions with the new adapted predictions. Certain problems arise with this measure for one of the adaptive procedures and these are discussed in detail. An example is shown of these procedures in use on some real software data.", "num_citations": "1\n", "authors": ["453"]}
{"title": "Equilibrium distributions of polymers on sites which are linearly arrayed\n", "abstract": " Certain long molecules can be regarded as linear arrays of chemically active sites. During chemical reaction, adjacent groups of sites may link to form dimers, trimers, etc. This paper considers the case when all such groups are unstable, and gives equilibrium distributions for the composition of the molecules as functions of the rates of formation and decay of the linked groups.", "num_citations": "1\n", "authors": ["453"]}