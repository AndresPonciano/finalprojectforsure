{"title": "Fuzzy grey relational analysis for software effort estimation\n", "abstract": " Accurate and credible software effort estimation is a challenge for academic research and software industry. From many software effort estimation models in existence, Estimation by Analogy (EA) is still one of the preferred techniques by software engineering practitioners because it mimics the human problem solving approach. Accuracy of such a model depends on the characteristics of the dataset, which is subject to considerable uncertainty. The inherent uncertainty in software attribute measurement has significant impact on estimation accuracy because these attributes are measured based on human judgment and are often vague and imprecise. To overcome this challenge we propose a new formal EA model based on the integration of Fuzzy set theory with Grey Relational Analysis (GRA). Fuzzy set theory is employed to reduce uncertainty in distance measure between two tuples at the k\u00a0\u2026", "num_citations": "118\n", "authors": ["418"]}
{"title": "Analogy-based software effort estimation using Fuzzy numbers\n", "abstract": " BackgroundEarly stage software effort estimation is a crucial task for project bedding and feasibility studies. Since collected data during the early stages of a software development lifecycle is always imprecise and uncertain, it is very hard to deliver accurate estimates. Analogy-based estimation, which is one of the popular estimation methods, is rarely used during the early stage of a project because of uncertainty associated with attribute measurement and data availability.AimsWe have integrated analogy-based estimation with Fuzzy numbers in order to improve the performance of software project effort estimation during the early stages of a software development lifecycle, using all available early data. Particularly, this paper proposes a new software project similarity measure and a new adaptation technique based on Fuzzy numbers.MethodEmpirical evaluations with Jack-knifing procedure have been carried out\u00a0\u2026", "num_citations": "84\n", "authors": ["418"]}
{"title": "Improving analogy software effort estimation using fuzzy feature subset selection algorithm\n", "abstract": " One of the major problems with software project management is the difficulty to predict accurately the required effort for developing software applications. Analogy Software effort estimation appears well suited to model problems of this nature. The analogy approach may be viewed as a systematic development of the expert opinion through experience learning and exposure to analogue case studies. The accuracy of such model depends on characteristics of datasets. This paper examines the impact of feature subset selection algorithms on improving the accuracy of analogy software effort estimation model. We proposed a feature subset selection algorithm based on fuzzy logic for analogy software effort estimation models. Validation using two established datasets (ISBSG, Desharnais) shows that using fuzzy features subset selection algorithm in analogy software effort estimation contribute to significant results as\u00a0\u2026", "num_citations": "73\n", "authors": ["418"]}
{"title": "A replicated assessment and comparison of adaptation techniques for analogy-based effort estimation\n", "abstract": " Variants of adaptation techniques have been proposed in previous studies to improve the performance of analogy-based effort estimation. The results of these studies are often contradictory and cannot simply be generalized because there are many uncontrollable source of variations between adaptation studies. The study presented in this paper has been carried out in order to replicate the assessment and comparison of different adaptation techniques utilised in analogy-based software effort prediction. Empirical evaluation of variants of adaptation techniques with Jack-knifing procedure have been carried out. Seven datasets come from PROMISE data repository were used for benchmarking. The results are also investigated within the presence/absence of feature subset selection algorithm. The current study permitted us to discover that linear adjustment approaches are more accurate than nonlinear\u00a0\u2026", "num_citations": "62\n", "authors": ["418"]}
{"title": "A Comparison Between Decision Trees and Decision Tree Forest Models for Software Effort Estimation\n", "abstract": " Accurate software effort estimation has been a challenge for many software practitioners and project managers. Underestimation leads to disruption in the project's estimated cost and delivery. On the other hand, overestimation causes outbidding and financial losses in business. Many software estimation models exist; however, none have been proven to be the best in all situations. In this paper, a decision tree forest (DTF) model is compared to a traditional decision tree (DT) model, as well as a multiple linear regression model (MLR). The evaluation was conducted using ISBSG and Desharnais industrial datasets. Results show that the DTF model is competitive and can be used as an alternative in software effort prediction.", "num_citations": "36\n", "authors": ["418"]}
{"title": "A Comparative Study for Predicting Heart Diseases Using Data Mining Classification Methods\n", "abstract": " Improving the precision of heart diseases detection has been investigated by many researchers in the literature. Such improvement induced by the overwhelming health care expenditures and erroneous diagnosis. As a result, various methodologies have been proposed to analyze the disease factors aiming to decrease the physicians practice variation and reduce medical costs and errors. In this paper, our main motivation is to develop an effective intelligent medical decision support system based on data mining techniques. In this context, five data mining classifying algorithms, with large datasets, have been utilized to assess and analyze the risk factors statistically related to heart diseases in order to compare the performance of the implemented classifiers (e.g., Na\\\"ive Bayes, Decision Tree, Discriminant, Random Forest, and Support Vector Machine). To underscore the practical viability of our approach, the selected classifiers have been implemented using MATLAB tool with two datasets. Results of the conducted experiments showed that all classification algorithms are predictive and can give relatively correct answer. However, the decision tree outperforms other classifiers with an accuracy rate of 99.0% followed by Random forest. That is the case because both of them have relatively same mechanism but the Random forest can build ensemble of decision tree. Although ensemble learning has been proved to produce superior results, but in our case the decision tree has outperformed its ensemble version.", "num_citations": "35\n", "authors": ["418"]}
{"title": "Software Cost Estimation based on Modified Use Case Points for Global Software Development\n", "abstract": " This paper investigates the applicability of Use Case Point estimation model to global software project development. Nowadays, there is growing trend among leading software companies to outsource their project geographically, in countries with lower labor rate. This new trend increases competitiveness in the software market, which in turn shortens the development lifecycle time. This leads to several challenges in project management, especially global project management. Effort and cost estimation are two of these significant challenges. The present paper analyzes the potential of Use Case Point estimation model for global projects and uses this as a basis to discuss three proposed factors (Global team trust, Global team composition and Culture value) that will help in managing the global software project development.", "num_citations": "35\n", "authors": ["418"]}
{"title": "Software effort estimation based on optimized model tree\n", "abstract": " Background: It is widely recognized that software effort estimation is a regression problem. Model Tree (MT) is one of the Machine Learning based regression techniques that is useful for software effort estimation, but as other machine learning algorithms, the MT has a large space of configurations and requires to carefully setting its parameters. The choice of such parameters is a dataset dependent so no general guideline can govern this process which forms the motivation of this work. Aims: This study investigates the effect of using the most recent optimization algorithm called Bees algorithm to specify the optimal choice of MT parameters that fit a specific dataset and therefore improve prediction accuracy. Method: We used MT with optimal parameters identified by the Bees algorithm to construct software effort estimation model. The model has been validated over eight datasets come from two main sources\u00a0\u2026", "num_citations": "30\n", "authors": ["418"]}
{"title": "Software effort estimation based on weighted fuzzy grey relational analysis\n", "abstract": " Delivering accurate software effort estimation has been a research challenge for a long time, where none of the existing estimation methods has proven to consistently deliver an accurate estimate. Previous studies have demonstrated that estimation by analogy (EBA) is a viable alternative to other conventional estimation methods in terms of predictive accuracy. EBA offers a way to use a formal method with data from a past project to derive a new estimate. Two important research areas in EBA are addressed in this paper: software projects similarity measurement and attribute weighting. However, the inherent uncertainty of attribute measurement makes similarity measurement between two software projects subject to considerable uncertainty. To tolerate such inherent uncertainty we propose a new similarity measurement method by combining the advantages of Fuzzy Set Theory and Grey Relational Analysis. In\u00a0\u2026", "num_citations": "28\n", "authors": ["418"]}
{"title": "Software project similarity measurement based on fuzzy C-means\n", "abstract": " A reliable and accurate similarity measurement between two software projects has always been a challenge for analogy-based software cost estimation. Since the effort for a new project is retrieved from similar historical projects, it is essentially to use the appropriate similarity measure that finds those close projects which in turn increases the estimation accuracy. In software engineering literature, there is a relatively little research addressed the issue of how to find out similarity between two software projects when they are described by numerical and categorical features. Despite simplicity of exiting similarity techniques such as: Euclidean distance, weighted Euclidean distance and maximum distance, it is hard to deal with categorical features. In this paper we present two approaches to measure similarity between two software projects based on fuzzy C-means clustering and fuzzy logic. The new\u00a0\u2026", "num_citations": "28\n", "authors": ["418"]}
{"title": "What facilitates the delivery of citizen-centric e-government services in developing countries: Model development and validation through structural equation modeling\n", "abstract": " Yet, existing research on the delivery of citizen-centric e-government services in developing countries is still lacking explanatory power for the following reasons: 1) focus either on the supply-side of these services, or on the demand-side separately, thus there is no enough research on the integration between them, and 2) focus on the results of previous research, thus ignoring the development of theories fit the new context under investigation by understanding the relationship between the implementation of ICT and social structures in the same context. This study aims to fill these gaps by employing a holistic approach to enable in-depth understanding and gain valuable insights for success factors in the delivery of citizencentric e-government services from multiple perspectives, and in the real context of one of the Arab countries, namely Jordan. This would reduce the gap between strategies and government policies on the one hand, and the perceptions of citizens on the other hand about the determinants of the delivery of citizen-centric e-government services in developing countries, allowing a better understanding of citizens' needs and priorities that must be taken into account by those governments in order to ensure the success of those services on a large scale. This study is part of a two-phase research aims to propose an integrated model of success factors in the delivery of citizen centric e-government services and then validate it in the context of developing countries in general and Jordan in particular. The first phase, which is beyond the scope of this study, employs grounded theory method to develop a research model, as well as survey\u00a0\u2026", "num_citations": "24\n", "authors": ["418"]}
{"title": "Learning best K analogies from data distribution for case-based software effort estimation\n", "abstract": " Case-Based Reasoning (CBR) has been widely used to generate good software effort estimates. The predictive performance of CBR is a dataset dependent and subject to extremely large space of configuration possibilities. Regardless of the type of adaptation technique, deciding on the optimal number of similar cases to be used before applying CBR is a key challenge. In this paper we propose a new technique based on Bisecting k-medoids clustering algorithm to better understanding the structure of a dataset and discovering the the optimal cases for each individual project by excluding irrelevant cases. Results obtained showed that understanding of the data characteristic prior prediction stage can help in automatically finding the best number of cases for each test project. Performance figures of the proposed estimation method are better than those of other regular K-based CBR methods.", "num_citations": "17\n", "authors": ["418"]}
{"title": "Model tree based adaption strategy for software effort estimation by analogy\n", "abstract": " Background: Adaptation technique is a crucial task for analogy based estimation. Current adaptation techniques often use linear size or linear similarity adjustment mechanisms which are often not suitable for datasets that have complex structure with many categorical attributes. Furthermore, the use of nonlinear adaptation technique such as neural network and genetic algorithms needs many user interactions and parameters optimization for configuring them (such as network model, number of neurons, activation functions, training functions, mutation, selection, crossover, etc.). Aims: In response to the abovementioned challenges, the present paper proposes a new adaptation strategy using Model Tree based attribute distance to adjust estimation by analogy and derive new estimates. Using Model Tree has an advantage to deal with categorical attributes, minimize user interaction and improve efficiency of model\u00a0\u2026", "num_citations": "15\n", "authors": ["418"]}
{"title": "Software stage-effort estimation based on association rule mining and fuzzy set theory\n", "abstract": " Relaying on early effort estimation to predict the required number of resources is not often sufficient, and could lead to under or over estimation. It is widely acknowledge that that software development process should be refined regularly and that software prediction made at early stage of software development is yet kind of guesses. Even good predictions are not sufficient with inherent uncertainty and risks. The stage-effort estimation allows project manager to re-allocate correct number of resources, re-schedule project and control project progress to finish on time and within budget. In this paper we propose an approach to utilize prior effort records to predict stage effort. The proposed model combines concepts of Fuzzy set theory and association rule mining. The results were good in terms of prediction accuracy and have potential to deliver good stage-effort estimation.", "num_citations": "15\n", "authors": ["418"]}
{"title": "An Optimized Analogy-Based Project Effort Estimation\n", "abstract": " Despite the predictive performance of Analogy-Based Estimation (ABE) in generating better effort estimates, there is no consensus on how to predict the best number of analogies, and which adjustment technique produces better estimates. This paper proposes a new adjusted ABE model based on optimizing and approximating complex relationships between features and reflects that approximation on the final estimate. The results show that the predictive performance of ABE has noticeably been improved, and the number of analogies was remarkably variable for each test project.", "num_citations": "13\n", "authors": ["418"]}
{"title": "Prioritize E-Government Strategies Using SWOT-Ranked Voting Analysis Technique: The Case of Jordan\n", "abstract": " Studies performed on the factors and strategies that affect the success of e-government programs around the world are many and varied. But many of those studies ignored the implementation mechanism of the factors and strategies explored in different contexts, including Jordan, to ensure the success of these programs there. Jordan is currently working hard to capture this opportunity through action plans to overcome the economic, social and managerial problems, which may affect the effectiveness of the implementation. Despite these efforts, there is still a lag in performance for lack of identification and also give importance to the strengths, weaknesses, opportunities and threats (SWOT) in the implementation of the program at the national level on the one hand, and on the other hand, the evaluation and ranking of the strategies developed for egovernment program depending on the SWOT analysis at the same level. In this paper, SWOT-Ranked voting (Borda count method) was used to achieve this task. Ranked voting theory (also called preferential voting approach) is a measure of individual interests and preferences as an aggregate towards a collective decision. Voters can rank potential candidates in order of their preferences and some aggregating rules are then used to find the winner or group of winners among the various candidates. E-government strategies have been evaluated and prioritized with the participation of 20 experts in the experiment who have experience in such large scale programs to ensure its success in the context under investigation. Kendall tau rank correlation coefficient was also used to measure the ordinal\u00a0\u2026", "num_citations": "11\n", "authors": ["418"]}
{"title": "Adjusted case-based software effort estimation using bees optimization algorithm\n", "abstract": " Case-Based Reasoning (CBR) has achieved a considerable interest from researchers for solving non-trivial or ill-defined problems such as those encountered by project managers including support for software project management in predictions and lesson learned. Software effort estimation is the key factor for successful software project management. In particular, the use of CBR for effort estimation was favored over regression and other machine learning techniques due to its performance in generating reliable estimates. However, this method was subject to variety of design options which therefore has strong impact on the prediction accuracy. Selection of CBR adjustment method and deciding on the number of analogies are such two important decisions for generating accurate and reliable estimates. This paper proposed a new method to adjust the retrieved project efforts and find optimal number of\u00a0\u2026", "num_citations": "11\n", "authors": ["418"]}
{"title": "Online reputation model using moving window\n", "abstract": " Users are increasingly dependent on decision tools to facilitate their transactions on the internet. Reputation models offer a solution to the users in supporting their purchase decisions. The reputation model takes product ratings as input and produces product quality as score. Most existing reputation models use na\u00efve average method or weighted average method to aggregate ratings. Na\u00efve average method is unstable when there exist a clear trend in the ratings sequence. Also, the weighted methods are influenced by unfair and malicious ratings. This paper introduces a new simple reputation model that aggregates ratings based on the concept of moving window. This approach enables us to study variability of ratings over time which allows us to investigate the trend of ratings and account for sudden changes in ratings trend. The window size can be defined by either number of ratings or duration. The proposed model has been validated against stat-ofart reputation models using Mean Absolute Error and Kendall tau correlation.", "num_citations": "5\n", "authors": ["418"]}
{"title": "A Comparative Usability Study on the Use of Auditory Icons to Support Virtual Lecturers in E-Learning Interfaces\n", "abstract": " Prior conducted research revealed that the auditory icons could contribute in supporting the virtual lecturers in presence of full body animation while delivering the learning content in e-learning interfaces. This paper presents further empirical investigation into the use of these supportive auditory icons by comparing three different e-learning interfaces in terms of usability aspects; effectiveness, user satisfaction and memorability. The aim is to find out which combination of the tested multimodal metaphors is the best one in terms of utilizing the auditory icons to supplement the presentation of learning material by virtual lecturer. The first experimental e-learning interface incorporates a speaking virtual lecturer with full body gestures along with supportive auditory icons. The second experimental e-learning interface includes the use of virtual lecturer speech in the absence of his body and accompanied with the same auditory icons used in the first interface. However, the third interface is similar to the second one in terms of using the virtual lecturer's speech but without any additional auditory icons. The obtained results have shown that the inclusion of auditory icons could enhance the usability and learning performance of e-learning interfaces much better if combined along with speaking virtual lectures in the absence of any body animation.", "num_citations": "4\n", "authors": ["418"]}
{"title": "Value of ranked voting methods for estimation by analogy\n", "abstract": " One long-standing issue in estimation by analogy (EBA) is finding closest analogies. Prior studies revealed that existing similarity measures are easily influenced by extreme values and irrelevant features. Instead of identifying closest projects based on the aggregated similarity degrees, the authors propose to use ranked voting methods that rank projects per feature, and then aggregate those ranks over all features using voting count rules. The project(s) with highest score will be the winners and form new estimate for the target project. This also enables us to automatically come up with the preferred number of analogies for each target project, since the winner set may contain more than a single winner. Empirical evaluation with Jack-knifing procedure has been carried out, in which nine datasets come from two repositories (PROMISE and ISBSG) were used for benchmarking. The proposed models are compared\u00a0\u2026", "num_citations": "4\n", "authors": ["418"]}
{"title": "Adjusting Analogy Software Effort Estimation Based on Fuzzy Logic.\n", "abstract": " Analogy estimation is a well known approach for software effort estimation. The underlying assumption of this approach is the more similar the software project description attributes are, the more similar the software project effort is. The most difficult activity in analogy estimation is how to derive a new estimate from retrieved solutions. Using retrieved solutions without adjustment to considered problem environment is not often sufficient. Thus, they need some adjustment to minimize variation between current case and retrieved cases. The main objective of the present paper is to investigate the applicability of fuzzy logic based software projects similarity measure to adjust analogy estimation and derive a new estimate. We proposed adaptation techniques which take into account the similarity between two software projects in terms of each feature. In earlier work, we proposed and validated a similarity measure between software projects based on fuzzy C-means. This similarity measure will be guided towards deriving new estimate.", "num_citations": "3\n", "authors": ["418"]}
{"title": "On obtaining a stable vote ranking methodology for implementing e-government strategies\n", "abstract": " Recently, a lot of studies have been conducted on what factors and strategies affect the successful implementation of e-government programs around the world. Most of these studies lack a clear plan for selecting strategies and setting their priorities for implementation and thus ensuring the success of such programs. In a previous study conducted by the same researchers, these strategies and their implementation priorities were defined based on the opinions of 20 government experts who were interviewed and asked to rank the strategies based on 23 factors resulting from the SWOT analysis that summarizes the context under study, specifically Jordan as an example of a developing country. However, there is a problem that still exists facing many of these countries, which is the divergence of opinions of government decision-makers in ranking their priorities for implementing various government strategies\u00a0\u2026", "num_citations": "2\n", "authors": ["418"]}
{"title": "Online Reputation Model Using Multiple Quality Factors\n", "abstract": " Users on internet are looking for ways to minimize their experiences on performing online transactions. Reputation systems as a decision support tool are trying to facilitate online transactions. However, many reputation systems use Na\u00efve methods to compute the reputation of an item. These methods are unstable when there is sparsity in the ratings. In addition, they do not have the ability to discover trends emerging from recent ratings. Other methods, which use weighted average or probabilistic model, usually focus on one aspect of the reviewer ratings. Even though models that combine multiple factors often accomplish that through arbitrary set of weights. This research study looks at various aspects of reviewers\u2019 ratings, and proposes a new reputation model that attempts to assess the reviewer reputation by combining four factors through a Fuzzy model. These weights are then involved in computing the item reputation. The proposed reputation model has been validated against state-of-art reputation models, and presented significant accuracy in terms of Mean Absolute Errors (MAE) and Kendall correlation. The proposed reputation model also works well with sparse and dense dataset.", "num_citations": "2\n", "authors": ["418"]}
{"title": "Comparative analysis of online rating systems\n", "abstract": " Online rating systems serve as decision support tool for choosing the right transactions on the internet. Consumers usually rely on others\u2019 experiences when do transaction on the internet, therefore their feedbacks are helpful in succeeding such transactions. One important form of such feedbacks is the product ratings. Most online rating systems have been proposed either by researchers or industry. But there is much debate about their accuracies and stability. This paper looks at the accuracy and stability of set of common online rating systems over dense and sparse datasets. To accomplish that we used three evaluation measures namely, Mean Absolute Errors (MAE), Mean Balanced Relative Error (MBRE) and Mean Inverse Balanced Relative Error (MIBRE), in addition to Borda count to assess the stability of ranking among various rating systems. The results showed that both median and Dirichlet are the most accurate models for both sparse and dense datasets, whereas the BetaDR model is the most stable model across different evaluation measures. Therefore we recommend using Dirichlet or BetaDR for the products with few number of ratings and using the median model with products of large number of ratings.", "num_citations": "2\n", "authors": ["418"]}
{"title": "Dataset Quality Assessment: An extension for analogy based effort estimation\n", "abstract": " Estimation by Analogy (EBA) is an increasingly active research method in the area of software engineering. The fundamental assumption of this method is that the similar projects in terms of attribute values will also be similar in terms of effort values. It is well recognized that the quality of software datasets has a considerable impact on the reliability and accuracy of such method. Therefore, if the software dataset does not satisfy the aforementioned assumption then it is not rather useful for EBA method. This paper presents a new method based on Kendall\u2019s row-wise rank correlation that enables data quality evaluation and providing a data pre-processing stage for EBA. The proposed method provides sound statistical basis and justification for the process of data quality evaluation. Unlike Analogy-X, our method has the ability to deal with categorical attributes individually without the need for partitioning the dataset. Experimental results showed that the proposed method could form a useful extension for EBA as it enables: dataset quality evaluation, attribute selection and identifying abnormal observations.", "num_citations": "2\n", "authors": ["418"]}
{"title": "Application of Machine Learning for Online Reputation Systems\n", "abstract": " Users on the Internet usually require venues to provide better purchasing recommendations. This can be provided by a reputation system that processes ratings to provide recommendations. The rating aggregation process is a main part of reputation systems to produce global opinions about the product quality. Naive methods that are frequently used do not consider consumer profiles in their calculations and cannot discover unfair ratings and trends emerging in new ratings. Other sophisticated rating aggregation methods that use a weighted average technique focus on one or a few aspects of consumers\u2019 profile data. This paper proposes a new reputation system using machine learning to predict reliability of consumers from their profile. In particular, we construct a new consumer profile dataset by extracting a set of factors that have a great impact on consumer reliability, which serve as an input to machine\u00a0\u2026", "num_citations": "1\n", "authors": ["418"]}
{"title": "Metaheuristic Techniques in Optimizing Traffic Control Lights: A Systematic Review\n", "abstract": " Traffic congestion is a serious problem on every roadway and streets in many cities around the world. This systematic review is devoted to analyze research papers that deal with the optimization of traffic signal timing. The main objective of such optimization is maximizing the number of the vehicles leaving the network in a given period of time. This will lead to enhancing the performance of the road system. In this work, we researched the most recent metaheuristic optimized traffic light control techniques. It was shown that integrating optimization techniques in the field of traffic lights control had a great impact on the performance of traffic monitoring. During our research, we found that the most used method was the Genetic Algorithm (GA).", "num_citations": "1\n", "authors": ["418"]}
{"title": "Employing auditory icons to support virtual lecturers in e-learning interfaces\n", "abstract": " Previous experimental results demonstrated that the auditory icons could complement the role of virtual lecturers in delivering the learning content in e-learning interfaces. This paper presents a proposal to conduct an empirical study to compare three different e-learning interfaces in which three different ways of incorporating supportive auditory icons will be used. The main aim is to find out which is the best among these experimental interfaces in terms of usability aspects. The first experimental e-learning interface incorporates a speaking virtual lecturer with full body gestures along with supportive auditory icons. The second experimental e-learning interface includes the use of virtual lecturer speech in the absence of his body and accompanied with the same auditory icons used in the first interface. However, in the third interface, the virtual lecturer speech will be used in the same way like in the second one but\u00a0\u2026", "num_citations": "1\n", "authors": ["418"]}