{"title": "On the naturalness of software\n", "abstract": " Natural languages like English are rich, complex, and powerful. The highly creative and graceful use of languages like English and Tamil, by masters like Shakespeare and Avvaiyar, can certainly delight and inspire. But in practice, given cognitive constraints and the exigencies of daily life, most human utterances are far simpler and much more repetitive and predictable. In fact, these utterances can be very usefully modeled using modern statistical methods. This fact has led to the phenomenal success of statistical approaches to speech recognition, natural language translation, question-answering, and text mining and comprehension. We begin with the conjecture that most software is also natural, in the sense that it is created by humans at work, with all the attendant constraints and limitations---and thus, like natural language, it is also likely to be repetitive and predictable. We then proceed to ask whether (a) code\u00a0\u2026", "num_citations": "816\n", "authors": ["192"]}
{"title": "Greenminer: A hardware based mining software repositories software energy consumption framework\n", "abstract": " Green Mining is a field of MSR that studies software energy consumption and relies on software performance data. Unfortunately there is a severe lack of publicly available software power use performance data. This means that green mining researchers must generate this data themselves by writing tests, building multiple revisions of a product, and then running these tests multiple times (10+) for each software revision while measuring power use. Then, they must aggregate these measurements to estimate the energy consumed by the tests for each software revision. This is time consuming and is made more difficult by the constraints of mobile devices and their OSes. In this paper we propose, implement, and demonstrate Green Miner: the first dedicated hardware mining software repositories testbed. The Green Miner physically measures the energy consumption of mobile devices (Android phones) and\u00a0\u2026", "num_citations": "154\n", "authors": ["192"]}
{"title": "On the personality traits of stackoverflow users\n", "abstract": " In the last decade, developers have been increasingly sharing their questions with each other through Question and Answer (Q&A) websites. As a result, these websites have become valuable knowledge repositories, covering a wealth of topics related to particular programming languages. This knowledge is even more useful as the developer community evaluates both questions and answers through a voting mechanism. As votes accumulate, the developer community recognizes reputed members and further trusts their answers. In this paper, we analyze the community's questions and answers to determine the developers' personality traits, using the Linguistic Inquiry and Word Count (LIWC). We explore the personality traits of Stack Overflow authors by categorizing them into different categories based on their reputation. Through textual analysis of Stack Overflow posts, we found that the top reputed authors are\u00a0\u2026", "num_citations": "147\n", "authors": ["192"]}
{"title": "Understanding android fragmentation with topic analysis of vendor-specific bugs\n", "abstract": " The fragmentation of the Android ecosystem causes portability and compatibility issues within the entire Android platform, which increases developer workload, delays application deployment, and ultimately disappoints users. This subject is discussed in the press and in scientific publications but it has yet to be systematically examined. The Android bug reports, as submitted by Android-device users, span across operating-system versions and hardware platforms and can provide interesting evidence about the problem. In this paper, we analyze the bug reports related to two popular vendors, HTC and Motorola. First, we manually label the bug reports. Next, we use Labeled-LDA (Latent Dirichlet Allocation) on the labeled data and LDA on the original data, to infer topics. Finally, by examining the relevance of the top 18 bug topics for each vendor's bug reports over time, we classify topics as common or unique\u00a0\u2026", "num_citations": "147\n", "authors": ["192"]}
{"title": "Energy profiles of java collections classes\n", "abstract": " We created detailed profiles of the energy consumed by common operations done on Java List, Map, and Set abstractions. The results show that the alternative data types for these abstractions differ significantly in terms of energy consumption depending on the operations. For example, an ArrayList consumes less energy than a LinkedList if items are inserted at the middle or at the end, but consumes more energy than a LinkedList if items are inserted at the start of the list. To explain the results, we explored the memory usage and the bytecode executed during an operation. Expensive computation tasks in the analyzed bytecode traces appeared to have an energy impact, but memory usage did not contribute. We evaluated our profiles by using them to selectively replace Collections types used in six applications and libraries. We found that choosing the wrong Collections type, as indicated by our profiles, can cost\u00a0\u2026", "num_citations": "133\n", "authors": ["192"]}
{"title": "What's hot and what's not: Windowed developer topic analysis\n", "abstract": " As development on a software project progresses, developers shift their focus between different topics and tasks many times. Managers and newcomer developers often seek ways of understanding what tasks have recently been worked on and how much effort has gone into each; for example, a manager might wonder what unexpected tasks occupied their team's attention during a period when they were supposed to have been implementing new features. Tools such as Latent Dirichlet Allocation (LDA) and Latent Semantic Indexing (LSI) can be used to extract a set of independent topics from a corpus of commit-log comments. Previous work in the area has created a single set of topics by analyzing comments from the entire lifetime of the project. In this paper, we propose windowing the topic analysis to give a more nuanced view of the system's evolution. By using a defined time-window of, for example, one month\u00a0\u2026", "num_citations": "129\n", "authors": ["192"]}
{"title": "A contextual approach towards more accurate duplicate bug report detection\n", "abstract": " Bug-tracking and issue-tracking systems tend to be populated with bugs, issues, or tickets written by a wide variety of bug reporters, with different levels of training and knowledge about the system being discussed. Many bug reporters lack the skills, vocabulary, knowledge, or time to efficiently search the issue tracker for similar issues. As a result, issue trackers are often full of duplicate issues and bugs, and bug triaging is time consuming and error prone. Many researchers have approached the bug-deduplication problem using off-the-shelf information-retrieval tools, such as BM25F used by Sun et al. In our work, we extend the state of the art by investigating how contextual information, relying on our prior knowledge of software quality, software architecture, and system-development (LDA) topics, can be exploited to improve bug-deduplication. We demonstrate the effectiveness of our contextual bug-deduplication\u00a0\u2026", "num_citations": "127\n", "authors": ["192"]}
{"title": "Green mining: A methodology of relating software change to power consumption\n", "abstract": " Power consumption is becoming more and more important with the increased popularity of smart-phones, tablets and laptops. The threat of reducing a customer's battery-life now hangs over the software developer who asks, \u201cwill this next change be the one that causes my software to drain a customer's battery?\u201d One solution is to detect power consumption regressions by measuring the power usage of tests, but this is time-consuming and often noisy. An alternative is to rely on software metrics that allow us to estimate the impact that a change might have on power consumption thus relieving the developer from expensive testing. This paper presents a general methodology for investigating the impact of software change on power consumption, we relate power consumption to software changes, and then investigate the impact of static OO software metrics on power consumption. We demonstrated that software\u00a0\u2026", "num_citations": "93\n", "authors": ["192"]}
{"title": "Syntax errors just aren't natural: Improving error reporting with language models\n", "abstract": " A frustrating aspect of software development is that compiler error messages often fail to locate the actual cause of a syntax error. An errant semicolon or brace can result in many errors reported throughout the file. We seek to find the actual source of these syntax errors by relying on the consistency of software: valid source code is usually repetitive and unsurprising. We exploit this consistency by constructing a simple N-gram language model of lexed source code tokens. We implemented an automatic Java syntax-error locator using the corpus of the project itself and evaluated its performance on mutated source code from several projects. Our tool, trained on the past versions of a project, can effectively augment the syntax error locations produced by the native compiler. Thus we provide a methodology and tool that exploits the naturalness of software source code to detect syntax errors alongside the parser.", "num_citations": "80\n", "authors": ["192"]}
{"title": "Green mining: a methodology of relating software change and configuration to power consumption\n", "abstract": " Power consumption is becoming more and more important with the increased popularity of smart-phones, tablets and laptops. The threat of reducing a customer\u2019s battery-life now hangs over the software developer, who now asks, \u201cwill this next change be the one that causes my software to drain a customer\u2019s battery?\u201d One solution is to detect power consumption regressions by measuring the power usage of tests, but this is time-consuming and often noisy. An alternative is to rely on software metrics that allow us to estimate the impact that a change might have on power consumption thus relieving the developer from expensive testing. This paper presents a general methodology for investigating the impact of software change on power consumption, we relate power consumption to software changes, and then investigate the impact of OO software metrics and churn metrics on power consumption. We\u00a0\u2026", "num_citations": "78\n", "authors": ["192"]}
{"title": "Detecting duplicate bug reports with software engineering domain knowledge\n", "abstract": " Bug deduplication, ie, recognizing bug reports that refer to the same problem, is a challenging task in the software\u2010engineering life cycle. Researchers have proposed several methods primarily relying on information\u2010retrieval techniques. Our work motivated by the intuition that domain knowledge can provide the relevant context to enhance effectiveness, attempts to improve the use of information retrieval by augmenting with software\u2010engineering knowledge. In our previous work, we proposed the software\u2010literature\u2010context method for using software\u2010engineering literature as a source of contextual information to detect duplicates. If bug reports relate to similar subjects, they have a better chance of being duplicates. Our method, being largely automated, has a potential to substantially decrease the level of manual effort involved in conventional techniques with a minor trade\u2010off in accuracy. In this study, we extend\u00a0\u2026", "num_citations": "70\n", "authors": ["192"]}
{"title": "Latent Dirichlet allocation: extracting topics from software engineering data\n", "abstract": " Topic analysis is a powerful tool that extracts \u201ctopics\u201d from document collections. Unlike manual tagging, which is effort intensive and requires expertise in the documents\u2019 subject matter, topic analysis (in its simplest form) is an automated process. Relying on the assumption that each document in a collection refers to a small number of topics, it extracts bags of words attributable to these topics. These topics can be used to support document retrieval or to relate documents to each other through their associated topics. Given the variety and amount of textual information included in software repositories, in issue reports, in commit and source-code comments, and in other forms of documentation, this method has found many applications in the software-engineering field of mining software repositories.This chapter provides an overview of the theory underlying latent Dirichlet allocation (LDA), the most popular topic\u00a0\u2026", "num_citations": "62\n", "authors": ["192"]}
{"title": "The power of system call traces: predicting the software energy consumption impact of changes.\n", "abstract": " Battery is a critical resource for smartphones. Software developers as the builders and maintainers of applications, are responsible for updating and deploying energy efficient applications to end users. Unfortunately, the impact of software change on energy consumption is still unclear. Estimation based on software metrics has proved difficult. As energy consumption profiling requires special infrastructure, developers have difficulty assessing the impact of their actions on energy consumption. System calls are the interface between applications and the OS kernel and provide insight into how software utilizes hardware and software resources. As profiling system calls requires no specialized infrastructure, unlike energy consumption, it is much easier for the developers to track changes to system calls. Thus we relate software change to energy consumption by tracing the changes in an application\u2019s pattern of system call invocations. We find that significant changes to system call profiles often induce significant changes in energy consumption.", "num_citations": "60\n", "authors": ["192"]}
{"title": "A contextual approach towards more accurate duplicate bug report detection and ranking\n", "abstract": " The issue-tracking systems used by software projects contain issues, bugs, or tickets written by a wide variety of bug reporters, with different levels of training and knowledge about the system under development. Typically, reporters lack the skills and/or time to search the issue-tracking system for similar issues already reported. As a result, many reports end up referring to the same issue, which effectively makes the bug-report triaging process time consuming and error prone. Many researchers have approached the bug-deduplication problem using off-the-shelf information-retrieval (IR) tools. In this work, we extend the state of the art by investigating how contextual information about software-quality attributes, software-architecture terms, and system-development topics can be exploited to improve bug deduplication. We demonstrate the effectiveness of our contextual bug-deduplication method at ranking\u00a0\u2026", "num_citations": "57\n", "authors": ["192"]}
{"title": "Involvement, contribution and influence in GitHub and stack overflow.\n", "abstract": " Software developers are increasingly adopting social-media platforms to contribute to software development, learn and develop a reputation for themselves. GitHub supports version-controlled code sharing and social-networking functionalities and Stack Overflow is a social forum for question answering on programming topics. Motivated by the features\u2019 overlap of the two networks, we set out to mine and analyze and correlate the members\u2019 core contributions, editorial activities and influence in the two networks. We aim to better understand the similarities and differences of the members\u2019 contributions in the two platforms and their evolution over time. In this context, while studying the activities of different user groups, we conducted a three-step investigation of GitHub activity, Stack Overflow activity and inter-network activity over a five-year period. We report our findings on interesting membership and activity patterns within each platform and some relations between the two.", "num_citations": "56\n", "authors": ["192"]}
{"title": "Green mining: energy consumption of advertisement blocking methods\n", "abstract": " Extending battery life on mobile devices has become an important topic recently due to the increasing frequency of smartphone adoption. A primary component of smart phone energy consumption is the apps that run on these devices. Many apps have embedded advertising and web browser apps will show ads that are embedded on webpages. Other researchers have found that advertising libraries and advertisements tend to increase power usage. But is the converse true? If we use advertisement blocking software will we consume less energy, or will the overhead of ad-blocking consume more energy?", "num_citations": "56\n", "authors": ["192"]}
{"title": "Greenoracle: Estimating software energy consumption with energy measurement corpora\n", "abstract": " Software energy consumption is a relatively new concern for mobile application developers. Poor energy performance can harm adoption and sales of applications. Unfortunately for the developers, the measurement of software energy con-sumption is expensive in terms of hardware and difficult in terms of expertise. Many prior models of software energy consumption assume that developers can use hardware instrumentation and thus cannot evaluate software runningwithin emulators or virtual machines. Some prior modelsrequire actual energy measurements from the previous versions of applications in order to model the energy consumption of later versions of the same application.In this paper, we take a big-data approach to software energy consumption and present a model that can estimate software energy consumption mostly within 10% error (in joules) and does not require the developer to train on energy\u00a0\u2026", "num_citations": "52\n", "authors": ["192"]}
{"title": "Syntax and sensibility: Using language models to detect and correct syntax errors\n", "abstract": " Syntax errors are made by novice and experienced programmers alike; however, novice programmers lack the years of experience that help them quickly resolve these frustrating errors. Standard LR parsers are of little help, typically resolving syntax errors and their precise location poorly. We propose a methodology that locates where syntax errors occur, and suggests possible changes to the token stream that can fix the error identified. This methodology finds syntax errors by using language models trained on correct source code to find tokens that seem out of place. Fixes are synthesized by consulting the language models to determine what tokens are more likely at the estimated error location. We compare n-gram and LSTM (long short-term memory) language models for this task, each trained on a large corpus of Java code collected from GitHub. Unlike prior work, our methodology does not rely that the\u00a0\u2026", "num_citations": "51\n", "authors": ["192"]}
{"title": "Green mining: Investigating power consumption across versions\n", "abstract": " Power consumption is increasingly becoming a concern for not only electrical engineers, but for software engineers as well, due to the increasing popularity of new power-limited contexts such as mobile-computing, smart-phones and cloud-computing. Software changes can alter software power consumption behaviour and can cause power performance regressions. By tracking software power consumption we can build models to provide suggestions to avoid power regressions. There is much research on software power consumption, but little focus on the relationship between software changes and power consumption. Most work measures the power consumption of a single software task; instead we seek to extend this work across the history (revisions) of a project. We develop a set of tests for a well established product and then run those tests across all versions of the product while recording the power usage of\u00a0\u2026", "num_citations": "42\n", "authors": ["192"]}
{"title": "Greenadvisor: A tool for analyzing the impact of software evolution on energy consumption\n", "abstract": " Change-impact analysis, namely \u201cidentifying the potential consequences of a change\u201d is an important and well studied problem in software evolution. Any change may potentially affect an application's behaviour, performance, and energy consumption profile. Our previous work demonstrated that changes to the system-call profile of an application correlated with changes to the application's energy-consumption profile. This paper evaluates and describes GreenAdvisor, a first of its kind tool that systematically records and analyzes an application's system calls to predict whether the energy-consumption profile of an application has changed. The GreenAdvisor tool was distributed to numerous software teams, whose members were surveyed about their experience using GreenAdvisor while developing Android applications to examine the energy-consumption impact of selected commits from the teams' projects\u00a0\u2026", "num_citations": "40\n", "authors": ["192"]}
{"title": "SWARMED: Captive Portals, Mobile Devices, and Audience Participation in Multi-User Music Performance.\n", "abstract": " Audience participation in computer music has long been limited by resources such as sensor technology or the material goods necessary to share such an instrument. A recent paradigm is to take advantage of the incredible popularity of the smart-phone, a pocket sized computer, and other mobile devices, to provide the audience an interface into a computer music instrument. In this paper we discuss a method of sharing a computer music instrument\u2019s interface with an audience to allow them to interact via their smartphones. We propose a method that is relatively cross-platform and device-agnostic, yet still allows for a rich user-interactive experience. By emulating a captive-portal or hotspot we reduce the adoptability issues and configuration problems facing performers and their audience. We share our experiences with this system, as well as an implementation of the system itself.", "num_citations": "39\n", "authors": ["192"]}
{"title": "Judging a commit by its cover\n", "abstract": " Developers summarize their changes to code in commit messages. When a message seems \u201cunusual\u201d, however, this puts doubt into the quality of the code contained in the commit. We trained n-gram language models and used cross-entropy as an indicator of commit message \u201cunusualness\u201d of over 120,000 commits from open source projects. Build statuses collected from Travis-CI were used as a proxy for code quality. We then compared the distributions of failed and successful commits with regards to the \u201cunusualness\u201d of their commit message. Our analysis yielded significant results when correlating cross-entropy with build status.", "num_citations": "36\n", "authors": ["192"]}
{"title": "What can Android mobile app developers do about the energy consumption of machine learning?\n", "abstract": " Machine learning is a popular method of learning functions from data to represent and to classify sensor inputs, multimedia, emails, and calendar events. Smartphone applications have been integrating more and more intelligence in the form of machine learning. Machine learning functionality now appears on most smartphones as voice recognition, spell checking, word disambiguation, face recognition, translation, spatial reasoning, and even natural language summarization. Excited app developers who want to use machine learning on mobile devices face one serious constraint that they did not face on desktop computers or cloud virtual machines: the end-user\u2019s mobile device has limited battery life, thus computationally intensive tasks can harm end users\u2019 phone availability by draining batteries of their stored energy. Currently, there are few guidelines for developers who want to employ machine\u00a0\u2026", "num_citations": "34\n", "authors": ["192"]}
{"title": "Mining stackoverflow to filter out off-topic irc discussion\n", "abstract": " Internet Relay Chat (IRC) is a commonly used tool by Open Source developers. Developers use IRC channels to discuss programming related problems, but much of the discussion is irrelevant and off-topic. Essentially if we treat IRC discussions like email messages, and apply spam filtering, we can try to filter out the spam (the off-topic discussions) from the ham (the programming discussions). Yet we need labelled data that unfortunately takes time to curate. To avoid costly cur ration in order to filter out off-topic discussions, we need positive and negative data-sources. On-line discussion forums, such as Stack Overflow, are very effective for solving programming problems. By engaging in open-data, Stack Overflow data becomes a powerful source of labelled text regarding programming. This work shows that we can train classifiers using Stack Overflow posts as positive examples of on-topic programming\u00a0\u2026", "num_citations": "34\n", "authors": ["192"]}
{"title": "Client-side energy efficiency of HTTP/2 for web and mobile app developers\n", "abstract": " Recent technological advancements have enabled mobile devices to provide mobile users with substantial capability and accessibility. Energy is evidently one of the most critical resources for such devices, in spite of the substantial gain in popularity of mobile devices, such as smart phones, their utility is severely constrained by the bounded battery capacity. Mobile users are very interested in accessing the Internet although it is one of the most expensive operations in terms of energy and cost. HTTP/2 has been proposed and accepted as the new standard for supporting the World Wide Web. HTTP/2 is expected to offer better performance, such as reduced page load time. Consequently, from the mobile users point of view, the question arises:does HTTP/2 offer improved energy consumption performance achieving longer battery life?In this paper, we compare the energy consumption of HTTP/2 with its predecessor\u00a0\u2026", "num_citations": "33\n", "authors": ["192"]}
{"title": "Preventing duplicate bug reports by continuously querying bug reports\n", "abstract": " Bug deduplication or duplicate bug report detection is a hot topic in software engineering information retrieval research, but it is often not deployed. Typically to de-duplicate bug reports developers rely upon the search capabilities of the bug report software they employ, such as Bugzilla, Jira, or Github Issues. These search capabilities range from simple SQL string search to IR-based word indexing methods employed by search engines. Yet too often these searches do very little to stop the creation of duplicate bug reports. Some bug trackers have more than 10% of their bug reports marked as duplicate. Perhaps these bug tracker search engines are not enough? In this paper we propose a method of attempting to prevent duplicate bug reports before they start: continuously querying. That is as the bug reporter types in their bug report their text is used to query the bug database to find duplicate or related bug\u00a0\u2026", "num_citations": "31\n", "authors": ["192"]}
{"title": "Deficient documentation detection a methodology to locate deficient project documentation using topic analysis\n", "abstract": " A project's documentation is the primary source of information for developers using that project. With hundreds of thousands of programming-related questions posted on programming Q&A websites, such as Stack Overflow, we question whether the developer-written documentation provides enough guidance for programmers. In this study, we wanted to know if there are any topics which are inadequately covered by the project documentation. We combined questions from Stack Overflow and documentation from the PHP and Python projects. Then, we applied topic analysis to this data using latent Dirichlet allocation (LDA), and found topics in Stack Overflow that did not overlap the project documentation. We successfully located topics that had deficient project documentation. We also found topics in need of tutorial documentation that were outside of the scope of the PHP or Python projects, such as MySQL and HTML.", "num_citations": "31\n", "authors": ["192"]}
{"title": "Green software engineering: the curse of methodology\n", "abstract": " Computer Science often seems distant from itsnatural science cousins, especially software engineering whichfeels closer to sociology and psychology than to physics. Physicalmeasurements are often rare in software engineering, except in afew niches. One such important niche is that of software energyconsumption, green mining, green IT, and sustainable computing, which all fall under the umbrella of green software engineering. With the physical measurement of energy consumption comesall of the limitations of measurement and experimentation thatexist in the natural sciences and engineering. Issues abound, fromattribution of energy use, isolation of components, to replicableexperiments. These get further complicated by cloud computingwhereby systems are virtualized and attribution of resource usageis a serious issue. Thus in this work we discuss the current state of softwareenergy consumption, and\u00a0\u2026", "num_citations": "29\n", "authors": ["192"]}
{"title": "Greenscaler: training software energy models with automatic test generation\n", "abstract": " Software energy consumption is a performance related non-functional requirement that complicates building software on mobile devices today. Energy hogging applications (apps) are a liability to both the end-user and software developer. Measuring software energy consumption is non-trivial, requiring both equipment and expertise, yet researchers have found that software energy consumption can be modelled. Prior works have hinted that with more energy measurement data we can make more accurate energy models. This data, however, was expensive to extract because it required energy measurement of running test cases (rare) or time consuming manually written tests. In this paper, we show that automatic random test generation with resource-utilization heuristics can be used successfully to build accurate software energy consumption models. Code coverage, although well-known as a heuristic for\u00a0\u2026", "num_citations": "26\n", "authors": ["192"]}
{"title": "Analyzing the effects of test driven development in GitHub\n", "abstract": " Testing is an integral part of the software development lifecycle, approached with varying degrees of rigor by different process models. Agile process models recommend Test Driven Development (TDD) as a key practice for reducing costs and improving code quality. The objective of this work is to perform a cost-benefit analysis of this practice. To that end, we have conducted a comparative analysis of GitHub repositories that adopts TDD to a lesser or greater extent, in order to determine how TDD affects software development productivity and software quality. We classified GitHub repositories archived in 2015 in terms of how rigorously they practiced TDD, thus creating a TDD spectrum. We then matched and compared various subsets of these repositories on this TDD spectrum with control sets of equal size. The control sets were samples from all GitHub repositories that matched certain characteristics, and\u00a0\u2026", "num_citations": "25\n", "authors": ["192"]}
{"title": "Characterizing energy-aware software projects: Are they different?\n", "abstract": " The improvement in battery technology for battery-driven devices is insignificant compared to their computing ability. In spite of the overwhelming advances in processing ability, adoption of sophisticated applications is hindered by the fear of shorter battery life. This is one of the several reasons software developers are becoming conscious of writing energy efficient code.", "num_citations": "25\n", "authors": ["192"]}
{"title": "Do the stars align? Multidimensional analysis of Android's layered architecture\n", "abstract": " In this paper we mine the Android bug tracker repository and study the characteristics of the architectural layers of the Android system. We have identified the locality of the Android bugs in the architectural layers of the its infrastructure, and analysed the bug lifetime patterns in each one of them. Additionally, we mined the bug tracker reporters and classified them according to its social centrality in the Android bug tracker community. We report three interesting findings, firstly while some architectural layers have a diverse interaction of people, attracting not only non-central reporters but highly important ones, other layers are mostly captivating for peripheral actors. Second, we exposed that even the bug lifetime is similar across the architectural layers, some of them have higher bug density and differential percentages of unsolved bugs. Finally, comparing the popularity distribution between layers, we have identified\u00a0\u2026", "num_citations": "25\n", "authors": ["192"]}
{"title": "What do developers know about machine learning: a study of ML discussions on StackOverflow\n", "abstract": " Machine learning, a branch of Artificial Intelligence, is now popular in software engineering community and is successfully used for problems like bug prediction, and software development effort estimation. Developers' understanding of machine learning, however, is not clear, and we require investigation to understand what educators should focus on, and how different online programming discussion communities can be more helpful. We conduct a study on Stack Overflow (SO) machine learning related posts using the SOTorrent dataset. We found that some machine learning topics are significantly more discussed than others, and others need more attention. We also found that topic generation with Latent Dirichlet Allocation (LDA) can suggest more appropriate tags that can make a machine learning post more visible and thus can help in receiving immediate feedback from sites like SO.", "num_citations": "24\n", "authors": ["192"]}
{"title": "Deep green: Modelling time-series of software energy consumption\n", "abstract": " Inefficient mobile software kills battery life. Yet, developers lack the tools necessary to detect and solve energy bugs in software. In addition, developers are usually tasked with the creation of software features and triaging existing bugs. This means that most developers do not have the time or resources to research, build, or employ energy debugging tools. We present a new method for predicting software energy consumption to help debug software energy issues. Our approach enables developers to align traces of software behavior with traces of software energy consumption. This allows developers to match run-time energy hot spots to the corresponding execution. We accomplish this by applying recent neural network models to predict time series of energy consumption given a software's behavior. We compare our time series models to prior state-of-the-art models that only predict total software energy\u00a0\u2026", "num_citations": "24\n", "authors": ["192"]}
{"title": "Is HTTP/2 more energy efficient than HTTP/1.1 for mobile users?\n", "abstract": " Recent technological advancements have enabled mobile devices to provide mobile users with substantial capability and accessibility. Energy is evidently one of the most critical resources for such devices; in spite of the substantial gain in popularity of mobile devices, such as smartphones, their utility is severely constrained by battery life. Mobile users are very interested in accessing the Internet while it is one of the most expensive operations in terms of energy and cost. HTTP/2 has been proposed and accepted as the new standard for supporting the World Wide Web. HTTP/2 is expected to offer better performance, such as reduced page load time. Consequently, from the mobile users point of view, question arises: Does HTTP/2 offer improved energy consumption performance achieving longer battery life?In this paper, we compare the energy consumption of HTTP/2 with its predecessor (i.e., HTTP/1.1) using a variety of real world and synthetic test scenarios. We also investigate how Transport Layer Security (TLS) impacts the energy consumption of the mobile devices. Our study suggests that Round Trip Time (RTT) is one of the biggest factors in deciding how advantageous is HTTP/2 compared to HTTP/1.1. We conclude that for networks with higher RTTs, HTTP/2 has better energy consumption performance than HTTP/1.1.", "num_citations": "23\n", "authors": ["192"]}
{"title": "How does Docker affect energy consumption? Evaluating workloads in and out of Docker containers\n", "abstract": " Context: Virtual machines provide isolation of services at the cost of hypervisors and more resource usage. This spurred the growth of systems like Docker that enable single hosts to isolate several applications, similar to VMs, within a low-overhead abstraction called containers.Motivation: Although containers tout low overhead performance, how much do they increase energy use?Methodology: This work statistically compares the energy consumption of three application workloads in Docker and on bare-metal Linux.Results: In all cases, there was a statistically significant (t-test and Wilcoxon p\u202f<\u202f.05) increase in energy consumption when running tests in Docker, mostly due to the performance of I/O system calls. Developers worried about I/O overhead could consider baremetal deployments over Docker container deployments.", "num_citations": "19\n", "authors": ["192"]}
{"title": "The unreasonable effectiveness of traditional information retrieval in crash report deduplication\n", "abstract": " Organizations like Mozilla, Microsoft, and Apple are flooded with thousands of automated crash reports per day. Although crash reports contain valuable information for debugging, there are often too many for developers to examine individually. Therefore, in industry, crash reports are often automatically grouped together in buckets. Ubuntu's repository contains crashes from hundreds of software systems available with Ubuntu. A variety of crash report bucketing methods are evaluated using data collected by Ubuntu's Apport automated crash reporting system. The trade-off between precision and recall of numerous scalable crash deduplication techniques is explored. A set of criteria that a crash deduplication method must meet is presented and several methods that meet these criteria are evaluated on a new dataset. The evaluations presented in this paper show that using off-the-shelf information retrieval\u00a0\u2026", "num_citations": "17\n", "authors": ["192"]}
{"title": "Crowdsourced bug triaging: Leveraging q&a platforms for bug assignment\n", "abstract": " Bug triaging, i.e.,\u00a0assigning a bug report to the \u201cbest\u201d person to address it, involves identifying a list of developers that are qualified to understand and address the bug report, and then ranking them according to their expertise. Most research in this area examines the description of the bug report and the developers\u2019 prior development and bug-fixing activities. In this paper, we propose a novel method that exploits a new source of evidence for the developers\u2019 expertise, namely their contributions in Stack Overflow, the popular software Question and Answer (Q&A) platform. The key intuition of our method is that the questions a developer asks and answers in Stack Overflow, or more generally in software Q&A platforms, can potentially be an excellent indicator of his/her expertise. Motivated by this idea, our method uses the bug-report description as a guide for selecting relevant Stack Overflow\u00a0contributions on\u00a0\u2026", "num_citations": "17\n", "authors": ["192"]}
{"title": "Software process recovery: recovering process from artifacts\n", "abstract": " Often stakeholders, such as developers, managers, or buyers, want to find out what software development processes are being followed within a software project. Their reasons include: CMM and ISO 9000 compliance, process validation, management, acquisitions, and business intelligence. Recovering the software development processes from an existing project is expensive if one must rely upon manual inspection of artifacts and interviews of developers and their managers. Researchers have suggested live observation and instrumentation of a project to allow for more measurement, but this is costly, invasive, and also requires a live running project. Instead, we propose an after the fact analysis: software process recovery. This approach analyzes version control systems, bug trackers and mailing list archives using a variety of supervised and unsupervised techniques from machine learning, topic analysis\u00a0\u2026", "num_citations": "17\n", "authors": ["192"]}
{"title": "CloudOrch: A Portable SoundCard in the Cloud.\n", "abstract": " One problem with live computer music performance is the transport of computers to a venue and the following setup of the computers used in playing and rendering music. The more computers involved, the longer the setup and teardown of a performance. Each computer adds power and cabling requirements that the venue must accommodate. Cloud computing can change all this by simplifying the setup of many (10s, 100s) of machines with the click of a button. But there\u2019sa catch, the cloud is not physically near you, you cannot run an audio cable to the cloud. The audio from a computer music instrument in the cloud needs to be streamed back to the performer and listeners. There are many solutions for streaming audio over networks and the internet, most of them suffer from high latency, heavy buffering, or proprietary/non-portable clients. This paper proposes a portable cloud-friendly method of streaming, almost a cloud soundcard, whereby performers can use mobile devices (Android, iOS, laptops) to stream audio from the cloud with far lower latency than technologies like Icecast. This technology enables near-realtime control over computer music networks enabling performers to travel light and perform live with more computers than ever before.", "num_citations": "16\n", "authors": ["192"]}
{"title": "The build dependency perspective of Android's concrete architecture\n", "abstract": " Android is an operating system designed specifically for mobile devices. It has a layered architecture. In this paper, we extract Android's concrete layered architecture by analyzing the build dependency relation between Android sub-projects and use it to validate the proposed conceptual architecture. Our experiment shows that Android's concrete architecture conforms to the conceptual architecture. Apart from that, we also show the extracted architecture can help developers and users better understand the Android system and further demonstrate its potential benefits in studying the impact of changes.", "num_citations": "16\n", "authors": ["192"]}
{"title": "Visualizing project evolution through abstract syntax tree analysis\n", "abstract": " What is a developer's contribution to a repository? By only counting commits and number of lines changed, existing tools that visualize source code repositories (such as GitHub's graphs) fall short on showing the effective contributions made by each developer. When many commits are viewed as a group, the details are lost. Commit information can be misleading since lines of code give no indication of what was actually being worked on without careful examination of the changed code. Providing a semantic view of this information could provide deeper insights into how software projects evolve since changes to design and features are not clearly visible from line changes alone. We present TypeV: a method for visualizing Java source code repositories. Instead of counting line changes in a commit we extract detailed type information over time by using the differences between abstract syntax trees (ASTs). We are\u00a0\u2026", "num_citations": "14\n", "authors": ["192"]}
{"title": "A system-call based model of software energy consumption without hardware instrumentation\n", "abstract": " The first challenge to develop an energy efficient application is to measure the application's energy consumption, which requires sophisticated hardware infrastructure and significant amounts of developers' time. Models and tools that estimate software energy consumption can save developers time, as application profiling is much easier and more widely available than hardware instrumentation for measuring software energy consumption. Our work focuses on modelling software energy consumption by using system calls and machine learning techniques. This system call based model is validated against actual energy measurements from five different Android applications. These results demonstrate that system call counts can successfully model software energy consumption if the idle energy consumption of an application is estimated or known. In the absence of any knowledge of an application's idle energy\u00a0\u2026", "num_citations": "14\n", "authors": ["192"]}
{"title": "A green miner's dataset: mining the impact of software change on energy consumption\n", "abstract": " With the advent of mobile computing, the responsibility of software developers to update and ship energy efficient applications has never been more pronounced. Green mining attempts to address this responsibility by examining the impact of software change on energy consumption. One problem with green mining is that power performance data is not readily available, unlike many other forms of MSR research. Green miners have to create tests and run them across numerous versions of a software project because power performance data was either missing or never existed for that particular project. In this paper we describe multiple open green mining datasets used in prior green mining work. The dataset includes numerous power traces and parallel system call and CPU/IO/Memory traces of multiple versions of multiple products. These datasets enable those more interested in data-mining and modeling to work\u00a0\u2026", "num_citations": "13\n", "authors": ["192"]}
{"title": "Using machine translation for converting Python 2 to Python 3 code\n", "abstract": " In this paper, we have tried to use statistical machine translation in order to convert Python 2 code to Python 3 code. We use data from two projects and achieve a high BLEU score. We also investigate the cross-project training and testing to analyze the errors so as to ascertain differences with previous case. We have described a pilot study on modeling programming languages as natural language to build translation models on the lines of natural languages. This can be further worked on to translate between versions of a programming language or cross-programming-languages code translation.", "num_citations": "11\n", "authors": ["192"]}
{"title": "Finding and correcting syntax errors using recurrent neural networks\n", "abstract": " Minor syntax errors are made by novice and experienced programmers alike; however, novice programmers lack the years of intuition that help them resolve these tiny errors. Standard LR parsers typically resolve syntax errors and their precise location poorly. We propose a methodology that helps locate where syntax errors occur, but also suggests possible changes to the token stream that can fix the error identified. This methodology finds syntax errors by checking if two language models \u201cagree\u201d on each token. If the models disagree, it indicates a possible syntax error; the methodology tries to suggest a fix by finding an alternative token sequence obtained from the models. We trained two LSTM (Long short-term memory) language models on a large corpus of JavaScript code collected from GitHub. The dual LSTM neural network model predicts the correct location of the syntax error 54.74% in its top 4 suggestions and produces an exact fix up to 35.50% of the time. The results show that this tool and methodology can locate and suggest corrections for syntax errors. Our methodology is of practical use to all programmers, but will be especially useful to novices frustrated with incomprehensible syntax errors.", "num_citations": "9\n", "authors": ["192"]}
{"title": "Continuous maintenance\n", "abstract": " There are many \"continuous\" practices in software engineering, for example continuous integration (CI), continuous delivery (CD), continuous release (CR), and DevOps. However, the maintenance aspect of continuity is rarely mentioned in publication or education. The continuous practices and applications depend on many repositories and artifacts, such as databases, servers, virtual machines, storage, data, meta-data, various logs, and reports. Continuous maintenance (CM) seeks to maintain these repositories and artifacts properly and consistently through automation, summarization, compaction, archival, and removal. For example, retaining builds and test results created by CI consumes storage. An automated CM process can remove the irrelevant artifacts and compact the relevant artifacts to reduce storage usage. Proper CM is essential for applications' long term sustainability. There are two sides of CM: pre\u00a0\u2026", "num_citations": "9\n", "authors": ["192"]}
{"title": "What\u2019s in a name? on the automated topic naming of software maintenance activities\n", "abstract": " Within the field of mining software repositories, many approaches, such as topic modeling and concept location, rely on the automated application of machine learning algorithms to corpora. Unfortunately the output of these tools is often difficult to distinguish and interpret as they are often so abstract. Thus to have a meaningful discussion about the topics of software development, we must be able to devise appropriate labels for extracted topics. However, these approaches neither use domain-specific knowledge to improve results, nor contextualize those results for developers. While too much specificity can produce non-generalizable results, too little produces broad learners that do not provide much immediately useful detail. This paper implements labelled topic extraction, in which topics are extracted from commit comments and given labels relating to a cross-project taxonomy. We focus on non-functional requirements related to software quality as a potential generalization, since there is some shared belief that these qualities apply broadly across many software systems and their development artifacts. We evaluated our approach with an experimental study on two large-scale database projects, MySQL and MaxDB. We extracted topics using Latent Dirichlet Allocation (LDA) from the commit log comments of their version control systems (CVS and BitKeeper). Our results were generalizable across the two projects, showing that non-functional requirements were commonly discussed, and we identified topic trends over time. Our labelled topic extraction technique allowed us to devise appropriate, context-sensitive labels across these two projects\u00a0\u2026", "num_citations": "9\n", "authors": ["192"]}
{"title": "On the time-based conclusion stability of cross-project defect prediction models\n", "abstract": " Researchers in empirical software engineering often make claims based on observable data such as defect reports. Unfortunately, in many cases, these claims are generalized beyond the data sets that have been evaluated. Will the researcher\u2019s conclusions hold a year from now for the same software projects? Perhaps not. Recent studies show that in the area of Software Analytics, conclusions over different data sets are usually inconsistent. In this article, we empirically investigate whether conclusions in the area of cross-project defect prediction truly exhibit stability throughout time or not. Our investigation applies a time-aware evaluation approach where models are trained only on the past, and evaluations are executed only on the future. Through this time-aware evaluation, we show that depending on which time period we evaluate defect predictors, their performance, in terms of F-Score, the area under the\u00a0\u2026", "num_citations": "8\n", "authors": ["192"]}
{"title": "Tracing forum posts to MOOC content using topic analysis\n", "abstract": " Massive Open Online Courses are educational programs that are open and accessible to a large number of people through the internet. To facilitate learning, MOOC discussion forums exist where students and instructors communicate questions, answers, and thoughts related to the course. The primary objective of this paper is to investigate tracing discussion forum posts back to course lecture videos and readings using topic analysis. We utilize both unsupervised and supervised variants of Latent Dirichlet Allocation (LDA) to extract topics from course material and classify forum posts. We validate our approach on posts bootstrapped from five Coursera courses and determine that topic models can be used to map student discussion posts back to the underlying course lecture or reading. Labeled LDA outperforms unsupervised Hierarchical Dirichlet Process LDA and base LDA for our traceability task. This research is useful as it provides an automated approach for clustering student discussions by course material, enabling instructors to quickly evaluate student misunderstanding of content and clarify materials accordingly.", "num_citations": "8\n", "authors": ["192"]}
{"title": "On improving green mining for energy-aware software analysis\n", "abstract": " Consumer demand for longer lasting battery life in mobile computers, as well as industry interest in energy efficient cloud infrastructure, creates a need for hardware and software energy efficiency improvements. One way to tackle this problem is from a software perspective. If it were known which software changes influenced energy consumption, then tools could be created to help software professionals create more energy efficient software. The process of extracting energy consumption information, Green Mining, is time demanding because researchers must run many tests, with sufficient coverage, on each revision in a software product multiple times. The time required for testing acts as a barrier to extracting energy consumption measurements from new software systems. Therefore, this work proposes, implements, and evaluates a search-based approximation method that trades some precision for a speedup in the mining process. This speed-up enables researchers to study additional software systems that were too costly to investigate before.", "num_citations": "8\n", "authors": ["192"]}
{"title": "Stopping duplicate bug reports before they start with Continuous Querying for bug reports\n", "abstract": " Bug deduplication is a hot topic in software engineering information retrieval research, but it is often not deployed. Typically to de-duplicate bug reports developers rely upon the search capabilities of the bug report software they employ, such as Bugzilla, Jira, or Github Issues. These search capabilities range from simple SQL string search to IR-based word indexing methods employed by search engines. Yet too often these searches do very little to stop the creation of duplicate bug reports. Some bug trackers have more than 10\\% of their bug reports marked as duplicate. Perhaps these bug tracker search engines are not enough? In this paper we propose a method of attempting to prevent duplicate bug reports before they start: continuous querying. That is as the bug reporter types in their bug report their text is used to query the bug database to find duplicate or related bug reports. This continuous querying allows the reporter to be alerted to duplicate bug reports as they report the bug, rather than formulating queries to find the duplicate bug report. Thus this work ushers in a new way of evaluating bug report deduplication techniques, as well as a new kind of bug deduplication task. We show that simple IR measures show some promise for addressing this problem but also that further research is needed to refine this novel process that is integrate-able into modern bug report systems.", "num_citations": "7\n", "authors": ["192"]}
{"title": "Web servers energy efficiency under HTTP/2\n", "abstract": " Server energy consumption has been a subject of research for more than a decade now. With Internet scaling rapidly all over the world, more servers are being added continuously. With global warming and financial cost associated with running servers, it has now become a more pressing concern to optimize the power consumption of these servers while still not affecting the performance. The optimization that can be carried out at the hardware level has its limits and therefore the onus comes on to the software developers as well to optimize their web interacting services and use protocols that are more efficient. Recently, Internet Engineering Task Force (IETF) formalized the specification for the successor of HTTP/1.1 protocol. Named HTTP/2, it has been projected to overcome all the limitations of HTTP/1.1 protocol for which web services developers have to optimize their applications. Understandably, HTTP/2 has been drawing a lot of interest from users, web administrators to big organizations. With HTTP/2 as the future of the Internet communication and servers acting as the backbone of the Internet, we are interested in knowing if HTTP/2 will provide energy efficiency benefits to servers or it will just improve users web experience. In this paper, we evaluate the energy efficiency of two web servers while they communicate over HTTP/1.1 and HTTP/2 protocol. We also investigate how Transport layer security (TLS) affects the power consumption of the servers. In our tests, we have introduced HTTP/2 features one by one so that readers can see for themselves what benefits the HTTP/2 over HTTP/1.1. Our study suggests that multiplexing and Round\u00a0\u2026", "num_citations": "7\n", "authors": ["192"]}
{"title": "An empirical study of end-user programmers in the computer music community\n", "abstract": " Computer musicians are a community of end-user programmers who often use visual programming languages such as Max/MSP or Pure Data to realize their musical compositions. This research study conducts a multifaceted analysis of the software development practices of computer musicians when programming in these visual music-oriented languages. A statistical analysis of project metadata harvested from software repositories hosted on GitHub reveals that in comparison to the general population of software developers, computer musicians' repositories have less commits, less frequent commits, more commits on weekends, yet similar numbers of bug reports and similar numbers of contributing authors. Analysis of source code in these repositories reveals that the vast majority of code can be reconstructed from duplicate fragments. Finally, these results are corroborated by a survey of computer musicians and\u00a0\u2026", "num_citations": "7\n", "authors": ["192"]}
{"title": "Greenscaler: Automatically training software energy model with big data\n", "abstract": " Software energy consumption is a performance related non-functional requirement that complicates building software on mobile devices today. Energy hogging applications are a liability to both the end-user and software developer. Measuring software energy consumption is non-trivial, requiring both equipment and expertise, yet many researchers have found that software energy consumption can be modelled. Prior works have hinted that with more energy measurement data one can make more accurate energy models but this data was expensive to extract because it required energy measurement of running test cases (rare) or time consuming manually written tests. We address these concerns by automatically generating test cases to drive applications undergoing energy measurement. Automatic test generation allows a model to be continuously improved in a model building process whereby applications are extracted, tests are generated, energy is measured and combined with instrumentation to train a grander big-data model of software energy consumption. This continuous process has allowed the authors to generate and extract measurements from hundreds of applications in order to build accurate energy models capable of predicting the energy consumption of applications without end-user energy measurement. We clearly show that models built from more applications reduce energy modelling error.", "num_citations": "6\n", "authors": ["192"]}
{"title": "Understanding devops education with grounded theory\n", "abstract": " DevOps stands for Development-Operations. It arises from the IT industry as a movement aligning development and operations teams. DevOps is broadly recognized as an IT standard, and there is high demand for DevOps practitioners in industry. Since ACM & IEEE suggest that undergraduate computer science curricula\" must adequately prepare [students] for the workforce\", we studied whether undergraduates acquired adequate DevOps skills to fulfill the demand for DevOps practitioners in industry. We employed Grounded Theory (GT), a social science qualitative research methodology, to study DevOps education from academic and industrial perspectives. In academia, academics were not motivated to learn or adopt DevOps, and we did not find strong evidence of academics teaching DevOps. Academics need incentives to adopt DevOps, in order to stimulate interest in teaching DevOps. In industry, DevOps\u00a0\u2026", "num_citations": "5\n", "authors": ["192"]}
{"title": "Sourcerer's Apprentice and the study of code snippet migration\n", "abstract": " On the worldwide web, not only are webpages connected but source code is too. Software development is becoming more accessible to everyone and the licensing for software remains complicated. We need to know if software licenses are being maintained properly throughout their reuse and evolution. This motivated the development of the Sourcerer's Apprentice, a webservice that helps track clone relicensing, because software typically employ software licenses to describe how their software may be used and adapted. But most developers do not have the legal expertise to sort out license conflicts. In this paper we put the Apprentice to work on empirical studies that demonstrate there is much sharing between StackOverflow code and Python modules and Python documentation that violates the licensing of the original Python modules and documentation: software snippets shared through StackOverflow are often being relicensed improperly to CC-BY-SA 3.0 without maintaining the appropriate attribution. We show that many snippets on StackOverflow are inappropriately relicensed by StackOverflow users, jeopardizing the status of the software built by companies and developers who reuse StackOverflow snippets.", "num_citations": "5\n", "authors": ["192"]}
{"title": "Orchestrating your cloud orchestra.\n", "abstract": " Cloud computing potentially ushers in a new era of computer music performance with exceptionally large computer music instruments consisting of 10s to 100s of virtual machines which we propose to call a \u2018cloud-orchestra\u2019. Cloud computing allows for the rapid provisioning of resources, but to deploy such a complicated and interconnected network of software synthesizers in the cloud requires a lot of manual work, system administration knowledge, and developer/operator skills. This is a barrier to computer musicians whose goal is to produce and perform music, and not to administer 100s of computers. This work discusses the issues facing cloud-orchestra deployment and offers an abstract solution and a concrete implementation. The abstract solution is to generate cloud-orchestra deployment plans by allowing computer musicians to model their network of synthesizers and to describe their resources. A model optimizer will compute near-optimal deployment plans to synchronize, deploy, and orchestrate the start-up of a complex network of synthesizers deployed to many computers. This model driven development approach frees computer musicians from much of the hassle of deployment and allocation. Computer musicians can focus on the configuration of musical components and leave the resource allocation up to the modelling software to optimize.", "num_citations": "5\n", "authors": ["192"]}
{"title": "Automated topic naming to support analysis of software maintenance activities\n", "abstract": " Researchers have used topic modeling and concept location to understand the latent topics of software development artifacts. These techniques use unsupervised machine-learning algorithms to recover topics. These topics are word-lists and are difficult to distinguish and interpret. Topics are not meaningful until they have been named or interpreted. Current topic labelling approaches are manual, and do not use domain-specific knowledge to improve, contextualize, or describe results for the developers. We propose a solution: labelled topic extraction. Topics are extracted using Latent Dirichlet Allocation (LDA) from commit-log comments recovered from source control systems such as CVS and Bit-Keeper. These topics are given labels relating to a generalizable cross-project taxonomy consisting of non-functional requirements. Our approach was evaluated with experiments and case studies on two large-scale RDBMS projects: MySQl and MaxDB. Labelled topic extraction produces appropriate, context-sensitive labels relevant to these projects, which provides fresh insight into their evolving software development activities.", "num_citations": "5\n", "authors": ["192"]}
{"title": "Syntax and Stack Overflow: A methodology for extracting a corpus of syntax errors and fixes\n", "abstract": " One problem when studying how to find and fix syntax errors is how to get natural and representative examples of syntax errors. Most syntax error datasets are not free, open, and public, or they are extracted from novice programmers and do not represent syntax errors that the general population of developers would make. Programmers of all skill levels post questions and answers to Stack Overflow which may contain snippets of source code along with corresponding text and tags. Many snippets do not parse, thus they are ripe for forming a corpus of syntax errors and corrections. Our primary contribution is an approach for extracting natural syntax errors and their corresponding human made fixes to help syntax error research. A Python abstract syntax tree parser is used to determine preliminary errors and corrections on code blocks extracted from the SOTorrent data set. We further analyzed our code by executing\u00a0\u2026", "num_citations": "4\n", "authors": ["192"]}
{"title": "Isolated guitar transcription using a deep belief network\n", "abstract": " Music transcription involves the transformation of an audio recording to common music notation, colloquially referred to as sheet music. Manually transcribing audio recordings is a difficult and time-consuming process, even for experienced musicians. In response, several algorithms have been proposed to automatically analyze and transcribe the notes sounding in an audio recording; however, these algorithms are often general-purpose, attempting to process any number of instruments producing any number of notes sounding simultaneously. This paper presents a polyphonic transcription algorithm that is constrained to processing the audio output of a single instrument, specifically an acoustic guitar. The transcription system consists of a novel note pitch estimation algorithm that uses a deep belief network and multi-label learning techniques to generate multiple pitch estimates for each analysis frame of the input audio signal. Using a compiled dataset of synthesized guitar recordings for evaluation, the algorithm described in this work results in an 11% increase in the f-measure of note transcriptions relative to Zhou et al.\u2019s (2009) transcription algorithm in the literature. This paper demonstrates the effectiveness of deep, multi-label learning for the task of polyphonic transcription.", "num_citations": "4\n", "authors": ["192"]}
{"title": "Judging a commit by its cover; or can a commit message predict build failure?\n", "abstract": " Developers summarize their changes to code in commit messages. When a message seems \u201cunusual,\u201d however, this puts doubt into the quality of the code contained in the commit. We trained -gram language models and used cross-entropy as an indicator of commit message \u201cunusualness\u201d of over 120 000 commits from open source projects. Build statuses collected from Travis-CI were used as a proxy for code quality. We then compared the distributions of failed and successful commits with regards to the \u201cunusualness\u201d of their commit message. Our analysis yielded significant results when correlating cross-entropy with build status.", "num_citations": "3\n", "authors": ["192"]}
{"title": "The charming code that error messages are talking about\n", "abstract": " The intent of high test coverage is to ensure that the dark nooks and crannies of code are exercised and tested. In a language like Python this is especially important as syntax errors can lurk in unevaluated blocks, only to be discovered once they are finally executed. Bugs that present themselves as error messages mentioning a line of code which is unrelated to the cause of the bug can be difficult and time-consuming to fix when a developer must first determine the actual location of the fault. A new code metric, charm, is presented. Charm can be used by developers, researchers, and automated tools to gain a deeper understanding of source code and become aware of potentially hidden faults, areas of code which are not sufficiently tested, and areas of code which may be more difficult to debug. Charm quantifies the property that error messges caused by a fault at one location don't always reference that location. In fact, error messages seem to prefer to reference some locations far more often than others. The quantity of charm can be estimated by averaging results from a random sample of similar programs to the one being measured by a procedure of random-mutation testing. Charm is estimated for release-quality Python software, requiring many thousands of similar Python programs to be executed. Charm has some correlation with a standard software metric, cyclomatic complexity. 21 code features which may have some relationship with charm and cyclomatic complexity are investigated, of which five are found to be significantly related with charm. These five features are then used to build a linear model which attempts to estimate charm\u00a0\u2026", "num_citations": "3\n", "authors": ["192"]}
{"title": "Mining the temporal evolution of the android bug reporting community via sliding windows\n", "abstract": " The open source development community consists of both paid and volunteer developers as well as new and experienced users. Previous work has applied social network analysis (SNA) to open source communities and has demonstrated value in expertise discovery and triaging. One problem with applying SNA directly to the data of the entire project lifetime is that the impact of local activities will be drowned out. In this paper we provide a method for aggregating, analyzing, and visualizing local (small time periods) interactions of bug reporting participants by using the SNA to measure the betweeness centrality of these participants. In particular we mined the Android bug repository by producing social networks from overlapping 30-day windows of bug reports, each sliding over by day. In this paper we define three patterns of participant behaviour based on their local centrality. We propose a method of analyzing the centrality of bug report participants both locally and globally, then we conduct a thorough case study of the bug reporter's activity within the Android bug repository. Furthermore, we validate the conclusions of our method by mining the Android version control system and inspecting the Android release history. We found that windowed SNA analysis elicited local behaviour that were invisible during global analysis.", "num_citations": "3\n", "authors": ["192"]}
{"title": "Multilabel 12-Lead Electrocardiogram Classification Using Beat to Sequence Autoencoders\n", "abstract": " The 12-lead electrocardiogram (ECG) measures the electrical activity of the heart for physicians to use in diagnosing cardiac disorders. This paper investigates the multi-label, multi-class classification of ECG records into one or more of 27 possible medical diagnoses. Our multi-step approach uses conventional physiological algorithms for segmentation of heartbeats from the baseline signals. We stack a heartbeat autoencoder over heartbeat windows to make embeddings, then we encode this sequence of embeddings to make an ECG embedding which we then classify on. We utilize the public dataset of 43,101 available ECG records provided by the PhysioNet/CinC 2020 challenge, performing repeated random subsampling and splitting the available records into 80% training, 10% validation, and 10% test splits, 20 times. We attain a mean test split challenge score of 0.248 with an overall macro F 1  score of 0\u00a0\u2026", "num_citations": "2\n", "authors": ["192"]}
{"title": "Multilabel 12-lead electrocardiogram classification using gradient boosting tree ensemble\n", "abstract": " The 12-lead electrocardiogram (ECG) is a commonly used tool for detecting cardiac abnormalities such as atrial fibrillation, blocks, and irregular complexes. For the Phy-sioNet/CinC 2020 Challenge, we built an algorithm using gradient boosted tree ensembles fitted on morphology and signal processing features to classify ECG diagnosis. For each lead, we derive features from heart rate variability, PQRST template shape, and the full signal wave-form. We join the features of all 12 leads to fit an ensemble of gradient boosting decision trees to predict probabilities of ECG instances belonging to each class. We train a phase one set of feature importance determining models to isolate the top 1,000 most important features to use in our phase two diagnosis prediction models. We use repeated random sub-sampling by splitting our dataset of 43,101 records into 100 independent runs of 85:15 training/validation splits for\u00a0\u2026", "num_citations": "2\n", "authors": ["192"]}
{"title": "Executability of Python Snippets in Stack Overflow\n", "abstract": " Online resources today contain an abundant amount of code snippets for documentation, collaboration, learning, and problem-solving purposes. Their executability in a \"plug and play\" manner enables us to confirm their quality and use them directly in projects. But, in practice that is often not the case due to several requirements violations or incompleteness. However, it is a difficult task to investigate the executability on a large scale due to different possible errors during the execution. We have developed a scalable framework to investigate this for SOTorrent Python snippets. We found that with minor adjustments, 27.92% of snippets are executable. The executability has not changed significantly over time. The code snippets referenced in GitHub are more likely to be directly executable. But executability does not affect the chances of the answer to be selected as the accepted answer significantly. These properties help us understand and improve the interaction of users with online resources that include code snippets.", "num_citations": "2\n", "authors": ["192"]}
{"title": "On the Time-Based Conclusion Stability of Software Defect Prediction Models.\n", "abstract": " Researchers in empirical software engineering often make claims based on observable data such as defect reports. Unfortunately, in many cases, these claims are generalized beyond the data sets that have been evaluated. Will the researcher's conclusions hold a year from now for the same software projects? Perhaps not. Recent studies show that in the area of Software Analytics, conclusions over different data sets are usually inconsistent. In this article, we empirically investigate whether conclusions in the area of defect prediction truly exhibit stability throughout time or not. Our investigation applies a time-aware evaluation approach where models are trained only on the past, and evaluations are executed only on the future. Through this time-aware evaluation, we show that depending on which time period we evaluate defect predictors, their performance, in terms of F-Score, the area under the curve (AUC), and Mathews Correlation Coefficient (MCC), varies and their results are not consistent. The next release of a product, which is significantly different from its prior release, may drastically change defect prediction performance. Therefore, without knowing about the conclusion stability, empirical software engineering researchers should limit their claims of performance within the contexts of evaluation, because broad claims about defect prediction performance might be contradicted by the next upcoming release of a product under analysis.", "num_citations": "2\n", "authors": ["192"]}
{"title": "Did I make a mistake? Finding the impact of code change on energy regression\n", "abstract": " Software energy consumption is a performance related non-functional requirement that complicates building software on mobile devices today. Energy hogging applications are a liability to both the end-user and software developer. Measuring software energy consumption is non-trivial, requiring both equipment and expertise, yet many researchers have found that software energy consumption can be modelled. Prior works have hinted that with more energy measurement data one can make more accurate energy models but this data was expensive to extract because it required energy measurement of running test cases (rare) or time consuming manually written tests. We address these concerns by automatically generating test cases to drive applications undergoing energy measurement. Automatic test generation allows a model to be continuously improved in a model building process whereby applications are extracted, tests are generated, energy is measured and combined with instrumentation to train a grander big-data model of software energy consumption. This continuous process has allowed the authors to generate and extract measurements from hundreds of applications in order to build accurate energy models capable of predicting the energy consumption of applications without end-user energy measurement. We clearly show that models built from more applications reduce energy modelling error.", "num_citations": "2\n", "authors": ["192"]}
{"title": "Patches and patchcords: An analysis of how computer music end-user programmers develop musical code\n", "abstract": " Musicians currently have a multitude of tools at their disposal for creating music. Before the digital age, music was created by physically manipulating a conventional musical instrument to produce sound. With the advent of synthesizers, samplers, and sequencers came a rapid paradigm shift in the music creation process that increasingly challenged the definition of\" instrument\", the role of musicians, and their technical proficiency. Many musicians have embraced the technical challenges arising from the changing landscape of the music creation process, forming a relatively small but tight-knit [1] community of individuals who are invested in developing their own music-making applications on computers or mobile devices.Computer musicians are end-user programmers who often have no formal training in the field of computing science; however, end-user programmers\" face software engineering challenges that are similar to their professional counterparts\"[2]. Computer musicians often use visual programming languages to realize their musical compositions. This research study conducts a multifaceted analysis of the software development practices of computer musicians when programming in visual music programming languages. An example of a visual music patch programmed in Max/MSP is displayed in Figure 1.", "num_citations": "2\n", "authors": ["192"]}
{"title": "Evidence-based software process recovery: A post-doctoral view\n", "abstract": " Software development processes are often viewed as a panacea for software quality: prescribe a process and a quality project will emerge. Unfortunately this has not been the case, as practitioners are prone to push against processes that they do not perceive as helpful, often much to the dismay of stakeholders such as their managers. Yet practitioners still tend to follow some sort of software development processes regardless of the prescribed processes. Thus if a team wants to recover the software development processes of a project or if team is trying to achieve a certification such as ISO9000 or CMM, the team will be tasked with describing their development processes. Previous research has tended to focus on modifying existing projects in order to extract process related information. In contrast, our approach of software process recovery attempts to analyze software artifacts extracted from software repositories\u00a0\u2026", "num_citations": "2\n", "authors": ["192"]}
{"title": "Multifractal aspects of software development: NIER track\n", "abstract": " Software development is difficult to model, particularly the noisy, non-stationary signals of changes per time unit, extracted from version control systems (VCSs). Currently researchers are utilizing timeseries analysis tools such as ARIMA to model these signals extracted from a project's VCS. Unfortunately current approaches are not very amenable to the underlying power-law distributions of this kind of signal. We propose modeling changes per time unit using multifractal analysis. This analysis can be used when a signal exhibits multi-scale self-similarity, as in the case of complex data drawn from power-law distributions. Specifically we utilize multifractal analysis to demonstrate that software development is multifractal, that is the signal is a fractal composed of multiple fractal dimensions along a range of Hurst exponents. Thus we show that software development has multi-scale self-similarity, that software\u00a0\u2026", "num_citations": "2\n", "authors": ["192"]}
{"title": "Complexity: Let's Not Make This Complicated\n", "abstract": " This article discusses the simplicity in agile software, the relationship between architectural patterns and complexity, the value of simplicity in software engineering research, and why we should refer to the formerly perceived complexity in software as complicated software. Complex software in software engineering typically refers to complicated code. Most measures of complexity are measures of information content in the code, whether it is McCabe's cyclomatic complexity measuring branching or Halstead's volume measuring the information within a block of code- Halstead's volume is very similar to the entropy of tokens multiplied by the number of tokens in a code block.", "num_citations": "1\n", "authors": ["192"]}
{"title": "Source code analysis and manipulation\n", "abstract": " Source code analysis and manipulation \u00d7 Close The Infona portal uses cookies, ie strings of text saved by a browser on the user's device. The portal can access those files and use them to remember the user's data, such as their chosen settings (screen view, interface language, etc.), or their login data. By using the Infona portal the user accepts automatic saving and using this information for portal operation purposes. More information on the subject can be found in the Privacy Policy and Terms of Service. By closing this window the user confirms that they have read the information on cookie usage, and they accept the privacy policy and the way cookies are used by the portal. You can change the cookie settings in your browser. I accept Polski English Login or register account remember me Password recovery INFONA - science communication portal INFONA Search advanced search Browse series books journals \u00d7 -\u2026", "num_citations": "1\n", "authors": ["192"]}
{"title": "Performance with an electronically excited didgeridoo.\n", "abstract": " The didgeridoo is a wind instrument composed of a single large tube often used as drone instrument for backing up the mids and lows of an ensemble. A didgeridoo is played by buzzing the lips and blowing air into the didgeridoo. To play a didgeridoo continously one can employ circular breathing but the volume of air required poses a real challenge to novice players. In this paper we replace the expense of circular breathing and lip buzzing with electronic excitation, thus creating an electro-acoustic didgeridoo or electronic didgeridoo. Thus we describe the didgeridoo excitation signal, how to replicate it, and the hardware necessary to make an electro-acoustic didgeridoo driven by speakers and controllable from a computer. To properly drive the didgeridoo we rely upon 4th-order ported bandpass speaker boxes to help guide our excitation signals into an attached acoustic didgeridoo. The results somewhat replicate human didgeridoo playing, enabling a new kind of mid to low electro-acoustic accompaniment without the need for circular breathing.", "num_citations": "1\n", "authors": ["192"]}
{"title": "Anatomy of a crash repository\n", "abstract": " This work investigates the properties of crash reports collected from Ubuntu Linux users. Understanding crash reports is important to better store, categorize, prioritize, parse, triage, assign bugs to, and potentially synthesize them. Understanding what is in a crash report, and how the metadata and stack traces in crash reports vary will help solve, debug, and prevent the causes of crashes. 10 different aspects of 40,592 crash reports about 1,921 pieces of software submitted by users and developers to the Ubuntu project were analyzed, plotted, and statistical distributions were fitted to some of them. We investigated the structure and properties of crash reports. Crashes have many properties that seem to have distributions similar to standard statistical distributions, but with even longer tails than expected. These aspects of crash reports have not been analyzed statistically before. We found that many applications only had a single crash, while a few applications had a large number of crashes reported. Crash bucket size (clusters of similar crashes) also followed a Zipf-like distribution. The lifespan of buckets ranged from less than an hour to over four years. Some stack traces were short, and some were so long they were truncated by the tool that produced them. Many crash reports had no recursion, some contained recursion, and some displayed evidence of unbounded recursion. Linguistics literature hinted that sentence length follows a gamma distribution; this is not the case for function name length. Additionally, only two hardware architectures, and a few signals are reported for almost all of the crashes in the Ubuntu dataset. Many crashes were\u00a0\u2026", "num_citations": "1\n", "authors": ["192"]}
{"title": "Analyzing test driven development based on GitHub evidence\n", "abstract": " Testing is an integral part of the software development lifecycle, approached with varying degrees of rigor by different process models. Agile process models advocate Test Driven Development (TDD) as one among their key practices for reducing costs and improving code quality. In this paper we comparatively analyze GitHub repositories that adopt TDD against repositories that do not, in order to determine how TDD affects a number of variables related to productivity and developer satisfaction, two aspects that should be considered in a cost-benefit analysis of the paradigm. In this study, we searched through GitHub and found that a relatively small subset of Java-based repositories can be seen to adopt TDD, and an even smaller subset can be confidently identified as rigorously adhering to TDD. For comparison purposes, we created two same-size control sets of repositories. We then compared the repositories in these two sets in terms of number of test files, average commit velocity, number of commits that reference bugs, number of issues recorded, whether they use continuous integration, and the sentiment of their developers\u2019 commits. We found some interesting and significant differences between the two sets, including higher commit velocity and increased likelihood of continuous integration for TDD repositories.", "num_citations": "1\n", "authors": ["192"]}
{"title": "Hacking NIMEs.\n", "abstract": " NIMEs typically focus on novelty but the cost of novelty is often to ignore other non-functional requirements and concerns such as usability or security. Digital security has probably not been a concern for performers due to the duration of their performances and lack of disrespectful hackers, known as crackers, in attendance carrying the appropriate equipment and software necessary to hack a performance. Yet many modern NIMEs could be hacked from smart-phones in the audience. The lack of security hardening makes NIMEs an easy target\u2014but a question arises: if hacking can interrupt or modify a performance couldn\u2019t hacking itself also be performance? Thus would music hacking, live-hacking, be similar to live-coding? In this paper we discuss how NIMEs are in danger of being hacked, and yet how hacking can be an act of performance too.", "num_citations": "1\n", "authors": ["192"]}
{"title": "Error location in python: where the mutants hide\n", "abstract": " Dynamic scripting programming languages present a unique challenge to software engineering tools that depend on static analysis. Dynamic languages do not benefit from the full lexical and syntax analysis provided by compilers and static analysis tools. Prior work exploited a statically typed language (Java) and a simple-gram language model to find syntax-error locations in programs. This work investigates whether-gram-based error location on source code written in a dynamic language is effective without static analysis or compilation. UnnaturalCode. py is a syntax-error locator developed for the Python programming language. The UnnaturalCode. py approach is effective on Python code, but faces significantly more challenges than its Java counterpart did. UnnaturalCode. py generalizes the success of previous statically-typed approaches to a dynamically-typed language.", "num_citations": "1\n", "authors": ["192"]}
{"title": "The shake stick\n", "abstract": " We present a new embedded instrument, with discussion on the challenges of developping embedded instruments, and the practice and theory of NIME evaluation and design. The Shake Stick is a Raspberry Pi-based embedded instrument using SuperCollider for granular synthesis. In our analysis and design, we explore the MINUET design framework, dimension space analysis for inter-instrument comparison, and learning curves. Furthermore, we discuss lessons learned from using the instrument in group improvisation, as well as challenges and prospects for the creation of sound palettes used in the granular synthesis.", "num_citations": "1\n", "authors": ["192"]}
{"title": "Green Mining: A Methodology of Relating Software Change and Configuration to Power Consumption\u2013WEB EDITION\n", "abstract": " Power consumption is becoming more and more important with the increased popularity of smartphones, tablets and laptops. The threat of reducing a customer\u2019s battery-life now hangs over the software developer, who now asks,\u201cwill this next change be the one that causes my software to drain a customer\u2019s battery?\u201d One solution is to detect power consumption regressions by measuring the power usage of tests, but this is time-consuming and often noisy. An alternative is to rely on software metrics that allow us to estimate the impact that a change might have on power consumption thus relieving the developer from expensive testing. This paper presents a general methodology for investigating the impact of software change on power consumption, we relate power consumption to software changes, and then investigate the impact of OO software metrics and churn metrics on power consumption. We demonstrated that software change can effect power consumption using the Firefox web-browser and the Azureus/Vuze BitTorrent client. We found evidence of a potential relationship between some software metrics and power consumption. We also investigate the effect of library versioning on the power consumption of rTorrent. In conclusion, we investigate the effect of software change on power consumption on two projects; and we provide an initial investigation on the impact of software metrics on power consumption.", "num_citations": "1\n", "authors": ["192"]}
{"title": "Evidence-based software process recovery\n", "abstract": " Developing a large software system involves many complicated, varied, and inter-dependent tasks, and these tasks are typically implemented using a combination of defined processes, semi-automated tools, and ad hoc practices.  Stakeholders in the development process --- including software developers, managers, and customers --- often want to be able to track the actual practices being employed within a project.  For example, a customer may wish to be sure that the process is ISO 9000 compliant, a manager may wish to track the amount of testing that has been done in the current iteration, and a developer may wish to determine who has recently been working on a subsystem that has had several major bugs appear in it.    However, extracting the software development processes from an existing project is expensive if one must rely upon manual inspection of artifacts and interviews of developers and their managers. Previously, researchers have suggested the live observation and instrumentation of a project to allow for more measurement, but this is costly, invasive, and also requires a live running project.    In this work, we propose an approach that we call software process recovery that is based on after-the-fact analysis of various kinds of software development artifacts.  We use a variety of supervised and unsupervised techniques from machine learning, topic analysis, natural language processing, and statistics on software repositories such as version control systems, bug trackers, and mailing list archives. We show how we can combine all of these methods to recover process signals that we map back to software development processes\u00a0\u2026", "num_citations": "1\n", "authors": ["192"]}
{"title": "Ostitch: MIR applied to musical instruments\n", "abstract": " The paper discusses the use of MIR in computer music instruments. This paper proposes and implements a performance time MIR based instrument (Ostitch) that produces \u201caudio mosaics\u201d or \u201caudio collages\u201d. Buffering, overlapping and stitching (audio concatenation) algorithms are discussed\u2013problems around these issues are evaluated in detail. Overlapping and mixing algorithms are proposed and implemented.", "num_citations": "1\n", "authors": ["192"]}
{"title": "On the Effectiveness of Simhash for Detecting Near-Miss Clones in Large Scale Software Systems\n", "abstract": " Clone detection techniques essentially cluster textually, syntactically and/or semantically similar code fragments in or across software systems. For large datasets, similarity identification is costly both in terms of time and memory, and especially so when detecting near-miss clones where lines could be modified, added and/or deleted in the copied fragments. The capability and effectiveness of a clone detection tool mostly depends on the code similarity measurement technique it uses. A variety of similarity measurement approaches have been used for clone detection, including fingerprint based approaches, which have had varying degrees of success notwithstanding some limitations. In this paper, we investigate the effectiveness of simhash, a state of the art fingerprint based data similarity measurement technique for detecting both exact and near-miss clones in large scale software systems. Our experimental data show that simhash is indeed effective in identifying various types of clones in a software system despite wide variations in experimental circumstances. The approach is also suitable as a core capability for building other tools, such as tools for: incremental clone detection, code searching, and clone management.", "num_citations": "1\n", "authors": ["192"]}