{"title": "Comparative Analysis of Two Popular Agile Process Models: Extreme Programming and Scrum\n", "abstract": " Since last two decades, agile software development methodologies have been one of the most debating topics for researchers. These are called light weight development methods because of informal, adaptive and flexible approach. These models are based on the collection of best practices which help to handle problems related to changing requirements, customer satisfaction, and product quality. A number of agile models are available to meet the needs of different projects. However Extreme Programming and Scrum are two most familiar and commonly used models. This study makes a valuable contribution by exploring these models in detail. In this paper a detailed comparison of Extreme programming and Scrum is conducted to find their similarities, differences and explores those features which complement each other.", "num_citations": "89\n", "authors": ["2239"]}
{"title": "Machine Learning Techniques for Sentiment Analysis: A Review\n", "abstract": " Social media platforms and micro blogging websites are the rich sources of user generated data. Through these resources, users from all over the world express and share their opinions about a variety of subjects. The analysis of such a huge amount of user generated data manually is impossible, therefore an effective and intelligent technique is needed which can analyze and provide the polarity of this textual data. Multiple tools and techniques are available today for automatic sentiment classification for this user generated data. Mostly, three approaches are used for this purpose Lexicon based techniques, Machine Learning based techniques and hybrid techniques (which combines lexicon based and machine learning based approach). Machine Learning approach is effective and reliable for opinion mining and sentiment classification. Many variants and extensions of machine learning techniques and tools are available today. The purpose of this study is to explore the different machine learning techniques to identify its importance as well as to raise an interest for this research area.", "num_citations": "68\n", "authors": ["2239"]}
{"title": "Agile Software Development Models TDD, FDD, DSDM, and Crystal Methods: A Survey\n", "abstract": " Software development is a critical task that requires a detailed and well-structured guideline in the form of software development process model. A good software development process model can play very important role in developing high quality software. Traditional software development models like Water fall, RUP, V-Model and Spiral Model remained dominant in software industry for a long time but to cope with growing needs and technology change in software industry, software developers tried to explore more improved software development models that lead to advent of agile development models. Agile models were warmly welcomed by software community because of their focus towards customer satisfaction, changing requirements and early software delivery. This paper provides a comprehensive review of different agile models which are used in software industry.", "num_citations": "62\n", "authors": ["2239"]}
{"title": "Sentiment Analysis of Tweets using SVM\n", "abstract": " Community's view and feedback have always proved to be the most essential and valuable resource for companies and organizations. With social media being the emerging trend among everyone, it paves way for unprecedented analysis and evaluation of various aspects for which organizations had to rely on unconventional, time consuming and error prone methods earlier. This technique of analysis directly falls under the domain of\" sentiment analysis\". Sentiment analysis encompasses the vast field of effective classification of user generated text under defined polarities. There are several tools and algorithms available to perform sentiment detection and analysis including supervised machine learning algorithms that perform classification on the target corpus, after getting trained with training data. Lexical techniques which performs classification on the basis of dictionary based annotated corpus and Hybrid tools which are combination of machine learning and lexicon based algorithms. In this paper we have used Support Vector Machine (SVM) for sentiment analysis in Weka. SVM is one of the widely used supervised machine learning algorithms for textual polarity detection. To analyze the performance of SVM, two pre classified datasets of tweets are used and for comparative analysis, three measures are used: Precision, Recall and F-Measure. Results are shown in the form of tables and graphs.", "num_citations": "61\n", "authors": ["2239"]}
{"title": "SVM Optimization for Sentiment Analysis\n", "abstract": " Exponential growth in mobile technology and mini computing devices has led to a massive increment in social media users, who are continuously posting their views and comments about certain products and services, which are in their use. These views and comments can be extremely beneficial for the companies which are interested to know about the public opinion regarding their offered products or services. This type of public opinion otherwise can be obtained via questionnaires and surveys, which is no doubt a difficult and complex task. So, the valuable information in the form of comments and posts from micro-blogging sites can be used by the companies to eliminate the flaws and to improve the products or services according to customer needs. However, extracting a general opinion out of a staggering number of users\u2019 comments manually cannot be feasible. A solution to this is to use an automatic method for sentiment mining. Support Vector Machine (SVM) is one of the widely used classification techniques for polarity detection from textual data. This study proposes a technique to tune the SVM performance by using grid search method for sentiment analysis. In this paper, three datasets are used for the experiment and performance of proposed technique is evaluated using three information retrieval metrics: precision, recall and f-measure.", "num_citations": "37\n", "authors": ["2239"]}
{"title": "Analyzing the Performance of SVM for Polarity Detection with Different Datasets\n", "abstract": " Social media and micro-blogging websites have become the popular platforms where anyone can express his/her thoughts about any particular news, event or product etc. The problem of analyzing this massive amount of user-generated data is one of the hot topics today. The term sentiment analysis includes the classification of a particular text as positive, negative or neutral, is known as polarity detection. Support Vector Machine (SVM) is one of the widely used machine learning algorithms for sentiment analysis. In this research, we have proposed a Sentiment Analysis Framework and by using this framework, analyzed the performance of SVM for textual polarity detection. We have used three datasets for experiment, two from twitter and one from IMDB reviews. For performance evaluation of SVM, we have used three different ratios of training data and test data, 70: 30, 50: 50 and 30: 70. Performance is measured in terms of precision, recall and f-measure for each dataset.", "num_citations": "37\n", "authors": ["2239"]}
{"title": "Hybrid Tools and Techniques for Sentiment Analysis: A Review\n", "abstract": " Sentiment analysis and opinion mining is closely coupled with each other. An extensive research work is being carried out in these areas by using different methodologies. Sentiments in a given text are identified by these methodologies as either positive, negative or neutral. Tweets, facebook posts, user comments about certain topics and reviews regarding product, software and movies can be the good source of information. Sentiment Analysis techniques can be used on such data by businesses executives for future planning and forecasting. As the data is obtained from multiple sources and it depends directly on the user which can be from any part of the world so the noisiness in data is a common issue such as mistake in spellings, grammatical errors and improper punctuation. Different approaches are available for sentiment analysis which can automatically sort and categorize the data. These approaches are mainly categorized as Machine Learning based, Lexicon based and Hybrid. A hybrid approach is the combination of machine learning and lexicon based approach for the optimum results, this approach generally yields better results. In this research work different hybrid techniques and tools have been discussed and analyzed from different aspects.", "num_citations": "33\n", "authors": ["2239"]}
{"title": "Rainfall Prediction in Lahore City using Data Mining Techniques\n", "abstract": " Rainfall prediction has extreme significance in countless aspects and scopes. It can be very helpful to reduce the effects of sudden and extreme rainfall by taking effective security measures in advance. Due to climate variations, an accurate rainfall prediction has become more complex than before. Data mining techniques can predict the rainfall through extracting the hidden patterns among weather attributes of past data. This research contributes by exploring the use of various data mining techniques for rainfall prediction in Lahore city. Techniques include: Support Vector Machine (SVM), Na\u00efve Bayes (NB), k Nearest Neighbor (kNN), Decision Tree (J48) and Multilayer Perceptron (MLP). The dataset is obtained from a weather forecasting website and consists of several atmospheric attributes. For effective prediction, pre-processing technique is used which consists of cleaning and normalization processes. Performance of used data mining techniques is analyzed in terms of precision, recall and f-measure with various ratios of training and test data.", "num_citations": "31\n", "authors": ["2239"]}
{"title": "Sentiment Analysis using SVM: A Systematic Literature Review\n", "abstract": " The world has revolutionized and phased into a new era, an era which upholds the true essence of technology and digitalization. As the market has evolved at a staggering scale, it is must to exploit and inherit the advantages and opportunities, it provides. With the advent of web 2.0, considering the scalability and unbounded reach that it provides, it is detrimental for an organization to not to adopt the new techniques in the competitive stakes that this emerging virtual world has set along with its advantages. The transformed and highly intelligent data mining approaches now allow organizations to collect, categorize, and analyze users' reviews and comments from micro-blogging sites regarding their services and products. This type of analysis makes those organizations capable to assess, what the consumers want, what they disapprove of, and what measures can be taken to sustain and improve the performance of\u00a0\u2026", "num_citations": "29\n", "authors": ["2239"]}
{"title": "Performance Analysis of Machine Learning Techniques on Software Defect Prediction using NASA Datasets\n", "abstract": " Defect prediction at early stages of software development life cycle is a crucial activity of quality assurance process and has been broadly studied in the last two decades. The early prediction of defective modules in developing software can help the development team to utilize the available resources efficiently and effectively to deliver high quality software product in limited time. Until now, many researchers have developed defect prediction models by using machine learning and statistical techniques. Machine learning approach is an effective way to identify the defective modules, which works by extracting the hidden patterns among software attributes. In this study, several machine learning classification techniques are used to predict the software defects in twelve widely used NASA datasets. The classification techniques include: Na\u00efve Bayes (NB), Multi-Layer Perceptron (MLP). Radial Basis Function (RBF), Support Vector Machine (SVM), K Nearest Neighbor (KNN), kStar (K*), One Rule (OneR), PART, Decision Tree (DT), and Random Forest (RF). Performance of used classification techniques is evaluated by using various measures such as: Precision, Recall, F-Measure, Accuracy, MCC, and ROC Area. The detailed results in this research can be used as a baseline for other researches so that any claim regarding the improvement in prediction through any new technique, model or framework can be compared and verified.", "num_citations": "28\n", "authors": ["2239"]}
{"title": "Rainfall prediction using data mining techniques: a systematic literature review\n", "abstract": " Rainfall prediction is one of the challenging tasks in weather forecasting. Accurate and timely rainfall prediction can be very helpful to take effective security measures in advance regarding: ongoing construction projects, transportation activities, agricultural tasks, flight operations and flood situation, etc. Data mining techniques can effectively predict the rainfall by extracting the hidden patterns among available features of past weather data. This research contributes by providing a critical analysis and review of latest data mining techniques, used for rainfall prediction. Published papers from year 2013 to 2017 from renowned online search libraries are considered for this research. This review will serve the researchers to analyze the latest work on rainfall prediction with the focus on data mining techniques and also will provide a baseline for future directions and comparisons.", "num_citations": "23\n", "authors": ["2239"]}
{"title": "SXP: Simplified Extreme Programing Process Model\n", "abstract": " Extreme programming is one of the widely used agile models in the software industry. It can handle unclear and changing requirements with the good level of customer satisfaction. However Lack of documentation, poor architectural structure and less focus on design are its major drawbacks that affects its performance. Due to these problems it cannot be used for all kinds of projects. It is considered suitable for small and low risk projects. It also has some controversial practices that cannot be applied in each and every situation like pair programming and on-site customer. To overcome these limitations a modified version of XP called \u201cSimplified Extreme Programming\u201d is proposed in this paper. This model provides solution of these problems without affecting simplicity and agility of extreme programming.", "num_citations": "23\n", "authors": ["2239"]}
{"title": "Simplified FDD Process Model\n", "abstract": " Feature driven development (FDD) is a process oriented and client centric agile software development model which develops a software according to client valued features. Like other agile models it also has adaptive and incremental nature to implement required functionality in short iterations. FDD mainly focus on designing and building aspects of software development with more emphasis on quality. However less responsiveness to changing requirements, reliance on experienced staff and less appropriateness for small scale projects are the main problems. To overcome these problems a Simplified Feature Driven Development (SFDD) model is proposed in this paper. In SFDD we have modified the phases of classical FDD for small to medium scale projects that can handle changing requirements with small teams in efficient and effective manner.", "num_citations": "21\n", "authors": ["2239"]}
{"title": "A Feed-Forward and Pattern Recognition ANN Model for Network Intrusion Detection\n", "abstract": " Network security is an essential element in the day-to-day IT operations of nearly every organization in business. Securing a computer network means considering the threats and vulnerabilities and arrange the countermeasures. Network security threats are increasing rapidly and making wireless network and internet services unreliable and insecure. Intrusion Detection System plays a protective role in shielding a network from potential intrusions. In this research paper, Feed Forward Neural Network and Pattern Recognition Neural Network are designed and tested for the detection of various attacks by using modified KDD Cup99 dataset. In our proposed models, Bayesian Regularization and Scaled Conjugate Gradient, training functions are used to train the Artificial Neural Networks. Various performance measures such as Accuracy, MCC, R-squared, MSE, DR, FAR and AROC are used to evaluate the performance of proposed Neural Network Models. The results have shown that both the models have outperformed each other in different performance measures on different attack detections.", "num_citations": "20\n", "authors": ["2239"]}
{"title": "Tools and Techniques for Lexicon Driven Sentiment Analysis: A Review\n", "abstract": " The growth of user\u2019s generated content increased in microblogging platforms like Facebook, Twitter and Blogger in form of client reviews, comments and opinion. Using this bulk of helpful data is difficult to analyze and also a time consuming task. So it is needed to have such an intelligent text mining system that automatically analyze such vast data and categorize them into positive or negative class. Due to the noisiness in data, it is difficult to design such text mining systems because they suffer from mistakes of spelling, grammatical and improper punctuation. Opinion mining is a useful tool to monitor consumer\u2019s feedback and public mood about certain product in terms of negativity or positivity. For example the management of customer relations can use these feedbacks and improve the products by keeping in view the complaints. Lexical tools are one of the famous and useful techniques for sentiment classification. Many extensions and modifications of these tools are available now days. The purpose of this research is to study the available lexical tools and techniques to raise an interest for this research area.", "num_citations": "20\n", "authors": ["2239"]}
{"title": "eXRUP: A Hybrid Software Development Model for Small to Medium Scale Projects\n", "abstract": " The conventional and agile software development process models are proposed and used nowadays in software industry to meet emergent requirements of the customers. Conventional software development models such as Waterfall, V model and RUP have been predominant in industry until mid 1990s, but these models are mainly focused on extensive planning, heavy documentation and team expertise which suit only to medium and large scale projects. The Rational Unified Process is one of the widely used conventional models. Agile process models got attention of the software industry in last decade due to limitations of conventional models such as slow adaptation to rapidly changing business requirements and they overcome problems of schedule and cost. Extreme Programming is one of the most useful agile methods that provide best engineering practices for a good quality product at small scale. XP follows the iterative and incremental approach, but its key focus is on programming, and reusability becomes arduous. In this paper, we present characteristics, strengths, and weaknesses of RUP and XP process models, and propose a new hybrid software development model eXRUP (eXtreme Programming and Rational Unified Process), which integrates the strengths of RUP and XP while suppressing their weaknesses. The proposed process model is validated through a controlled case study.", "num_citations": "20\n", "authors": ["2239"]}
{"title": "A feature selection based ensemble classification framework for software defect prediction\n", "abstract": " Software defect prediction is one of the emerging research areas of software engineering. The prediction of defects at early stage of development process can produce high quality software at lower cost. This research contributes by presenting a feature selection based ensemble classification framework which consists of four stages: 1) Dataset selection, 2) Feature Selection, 3) Classification, and 4) Results. The proposed framework is implemented from two dimensions, one with feature selection and second without feature selection. The performance is evaluated through various measures including: Precision, Recall, F-measure, Accuracy, MCC and ROC. 12 Cleaned publically available NASA datasets are used for experiments. The results of both the dimensions of proposed framework are compared with the other widely used classification techniques such as:\u201cNa\u00efve Bayes (NB), Multi-Layer Perceptron (MLP). Radial Basis Function (RBF), Support Vector Machine (SVM), K Nearest Neighbor (KNN), kStar (K*), One Rule (OneR), PART, Decision Tree (DT), and Random Forest (RF)\u201d. Results reflect that the proposed framework outperformed other classification techniques in some of the used datasets however class imbalance issue could not be fully resolved.", "num_citations": "14\n", "authors": ["2239"]}
{"title": "A Classification Framework to Detect DoS Attacks.\n", "abstract": " The exponent increase in the use of online information systems triggered the demand of secure networks so that any intrusion can be detected and aborted. Intrusion detection is considered as one of the emerging research areas now days. This paper presents a machine learning based classification framework to detect the Denial of Service (DoS) attacks. The framework consists of five stages, including: 1) selection of the relevant Dataset, 2) Data pre-processing, 3) Feature Selection, 4) Detection, and 5) reflection of Results. The feature selection stage incudes the Decision Tree (DT) classifier as subset evaluator with four well known selection techniques including: Genetic Algorithm (GA), Particle Swarm Optimization (PSO), Best First (BF), and Rank Search (RS). Moreover, for detection, Decision Tree (DT) is used with bagging technique. Proposed framework is compared with 10 widely used classification techniques including Na\u00efve Bayes (NB), Support Vector Machine (SVM), Multi-Layer Perceptron (MLP), K-Nearest Neighbor (kNN), Decision Tree (DT), Radial Basis Function (RBF), One Rule (OneR), PART, Bayesian Network (BN) and Random Tree (RT). A part of NSL-KDD dataset related to Denial of Service attack is used for experiments and performance is evaluated by using various accuracy measures including: Precision, Recall, F measure, FP rate, Accuracy, MCC, and ROC. The results reflected that the proposed framework outperformed all other classifiers", "num_citations": "14\n", "authors": ["2239"]}
{"title": "Latest Customizations of XP: A Systematic Literature Review\n", "abstract": " Software development process model plays a key role in developing high quality software. However there is no fit-for-all type of process model exist in software industry. To accommodate some specific project\u2019s needs, process models have to be tailored. Extreme Programming (XP) is a well-known agile model. Due to its simplicity, best practices and disciplined approach researchers tried to mold it for various types of projects and situations. As a result a large number of customized versions of XP are available now days. The aim of this paper is to analyze the latest customizations of XP. For this purpose a systematic literature review is conducted on studies published during 2013 to 2017. This detailed review identifies the objectives of customizations, specific areas in which customizations are done and practices & phases which are being targeted for customizations. This work will not only serve the best for scholars to find the current XP states but will also help researchers to predict the future directions of software development with XP.", "num_citations": "14\n", "authors": ["2239"]}
{"title": "A Classification Framework for Software Defect Prediction Using Multi-filter Feature Selection Technique and MLP\n", "abstract": " Production of high quality software at lower cost can be possible by detecting defect prone software modules before the testing process. With this approach, less time and resources are required to produce a high quality software as only those modules are thoroughly tested which are predicted as defective. This paper presents a classification framework which uses Multi-Filter feature selection technique and Multi-Layer Perceptron (MLP) to predict defect prone software modules. The proposed framework works in two dimensions: 1) with oversampling technique, 2) without oversampling technique. Oversampling is introduced in the framework to analyze the effect of class imbalance issue on the performance of classification techniques. The framework is implemented by using twelve cleaned NASA MDP datasets and performance is evaluated by using: F-measure, Accuracy, MCC and ROC. According to results the proposed framework with class balancing technique performed well in all of the used datasets.", "num_citations": "13\n", "authors": ["2239"]}
{"title": "Performance analysis of resampling techniques on class imbalance issue in software defect prediction\n", "abstract": " Predicting the defects at early stage of software development life cycle can improve the quality of end product at lower cost. Machine learning techniques have been proved to be an effective way for software defect prediction however an imbalance dataset of software defects is the main issue of lower and biased performance of classifiers. This issue can be resolved by applying the re-sampling methods on software defect dataset before the classification process. This research analyzes the performance of three widely used resampling techniques on class imbalance issue for software defect prediction. The resampling techniques include:\u201cRandom Under Sampling\u201d,\u201cRandom Over Sampling\u201d and \u201cSynthetic Minority Oversampling Technique (SMOTE)\u201d. For experiments, 12 publically available cleaned NASA MDP datasets are used with 10 widely used supervised machine learning classifiers. The performance is evaluated through various measures including: F-measure, Accuracy, MCC and ROC. According to results, most of the classifiers performed better with \u201cRandom Over Sampling\u201d technique in many datasets.", "num_citations": "11\n", "authors": ["2239"]}
{"title": "Comparative Analysis of FDD and SFDD\n", "abstract": " No doubt, development of high quality software depends upon the selection of software process model. Conventional software development models such as Water fall, Spiral and V-Model have been dominant in software industry till mid 1990s and then the era of agile development models started. Agile process models got the attention of software industry by proposing the solutions of problems which developers were facing with conventional models. Feature Driven Development (FDD) is one of the widely used software development models from agile family. FDD is known as client centric model as it develops the software product according to client valued features. It follows adaptive and incremental approach to implement the required functionality and focuses on designing and building aspects of software development with more emphasis on quality. However besides the benefits, FDD lacks at some areas. Having less ability to respond towards the changing requirements, reliance on experienced staff and no focus on small projects are the main problems of FDD. To overcome these issues, Simplified Feature Driven Development (SFDD) was proposed. This paper empirically compares both the models by presenting the results, which are obtained from the development of real time client oriented projects.", "num_citations": "11\n", "authors": ["2239"]}
{"title": "Software defect prediction using ensemble learning: A systematic literature review\n", "abstract": " Recent advances in the domain of software defect prediction (SDP) include the integration of multiple classification techniques to create an ensemble or hybrid approach. This technique was introduced to improve the prediction performance by overcoming the limitations of any single classification technique. This research provides a systematic literature review on the use of the ensemble learning approach for software defect prediction. The review is conducted after critically analyzing research papers published since 2012 in four well-known online libraries: ACM, IEEE, Springer Link, and Science Direct. In this study, five research questions that cover the different aspects of research progress on the use of ensemble learning for software defect prediction are addressed. To extract the answers to identified questions, 46 most relevant papers are shortlisted after a thorough systematic research process. This study will\u00a0\u2026", "num_citations": "9\n", "authors": ["2239"]}
{"title": "Proposal of Tailored Extreme Programming Model for Small Projects\n", "abstract": " Extreme Programming is a well-known agile process model that can fulfill the needs of today's software industry. It is suitable for small to medium scale projects. Its strength lies in its practices that are applied in extreme manner to get the best results. However in some scenarios these practices overburdened the software development process. In small scale projects where requirements are almost stable and no detailed design and planning activity is required, the overall structure and some of XP practices require extra effort and cause unnecessary delay in project completion. Practices like on-site customer, continuous testing and continuous integration can be a hurdle in timely completion of small project. To overcome these problems a modified form of Extreme Programming model called Tailored Extreme Programming (TXP) is presented in this research that can be applied to small scale projects to make the development process effective and efficient.", "num_citations": "8\n", "authors": ["2239"]}
{"title": "Energy Demand Forecasting Using Fused Machine Learning Approaches\n", "abstract": " The usage of IoT-based smart meter in electric power consumption shows a significant role in helping the users to manage and control their electric power consumption. It produces smooth communication to build equitable electric power distribution for users and improved management of the entire electric system for providers. Machine learning predicting algorithms have been worked to apply the electric efficiency and response of progressive energy creation, transmission, and consumption. In the proposed model, an IoT-based smart meter uses a support vector machine and deep extreme machine learning techniques for professional energy management. A deep extreme machine learning approach applied to feature-based data provided a better result. Lastly, decision-based fusion applied to both datasets to predict power consumption through smart meters and get better results than previous techniques. The established model smart meter with automatic load control increases the effectiveness of energy management. The proposed EDF-FMLA model achieved 90.70 accuracy for predicting energy consumption with a smart meter which is better than the existing approaches.", "num_citations": "7\n", "authors": ["2239"]}
{"title": "A Framework for Software Defect Prediction Using Feature Selection and Ensemble Learning Techniques.\n", "abstract": " Testing is one of the crucial activities of software development life cycle which ensures the delivery of high quality product. As software testing consumes significant amount of resources so, if, instead of all software modules, only those are thoroughly tested which are likely to be defective then a high quality software can be delivered at lower cost. Software defect prediction, which has now become an essential part of software testing, can achieve this goal. This research presents a framework for software defect prediction by using feature selection and ensemble learning techniques. The framework consists of four stages: 1) Dataset Selection, 2) Pre Processing, 3) Classification, and 4) Reflection of Results. The framework is implemented on six publically available Cleaned NASA MDP datasets and performance is reflected by using various measures including: F-measure, Accuracy, MCC and ROC. First the performance of all search methods within the framework on each dataset is compared with each other and the method with highest score in each performance measure is identified. Secondly, the results of proposed framework with all search methods are compared with the results of 10 well-known supervised classification techniques. The results reflect that the proposed framework outperformed all of other classification techniques.", "num_citations": "7\n", "authors": ["2239"]}
{"title": "Software Defect Prediction Using Variant based Ensemble Learning and Feature Selection Techniques.\n", "abstract": " Testing is considered as one of the expensive activities in software development process. Fixing the defects during testing process can increase the cost as well as the completion time of the project. Cost of testing process can be reduced by identifying the defective modules during the development (before testing) stage. This process is known as \u201cSoftware Defect Prediction\u201d, which has been widely focused by many researchers in the last two decades. This research proposes a classification framework for the prediction of defective modules using variant based ensemble learning and feature selection techniques. Variant selection activity identifies the best optimized versions of classification techniques so that their ensemble can achieve high performance whereas feature selection is performed to get rid of such features which do not participate in classification and become the cause of lower performance. The proposed framework is implemented on four cleaned NASA datasets from MDP repository and evaluated by using three performance measures, including: F-measure, Accuracy, and MCC. According to results, the proposed framework outperformed 10 widely used supervised classification techniques, including:\u201cNa\u00efve Bayes (NB), Multi-Layer Perceptron (MLP), Radial Basis Function (RBF), Support Vector Machine (SVM), K Nearest Neighbor (KNN), kStar (K*), One Rule (OneR), PART, Decision Tree (DT), and Random Forest (RF)\u201d.", "num_citations": "6\n", "authors": ["2239"]}
{"title": "Cloud-Based Diabetes Decision Support System Using Machine Learning Fusion\n", "abstract": " Diabetes mellitus, generally known as diabetes, is one of the most common diseases worldwide. It is a metabolic disease characterized by insulin deficiency, or glucose (blood sugar) levels that exceed 200 mg/dL (11.1 ml/L) for prolonged periods, and may lead to death if left uncontrolled by medication or insulin injections. Diabetes is categorized into two main types-type 1 and type 2-both of which feature glucose levels above \"normal,\" defined as 140 mg/dL. Diabetes is triggered by malfunction of the pancreas, which releases insulin, a natural hormone responsible for controlling glucose levels in blood cells. Diagnosis and comprehensive analysis of this potentially fatal disease necessitate application of techniques with minimal rates of error. The primary purpose of this research study is to assess the potential role of machine learning in predicting a person's risk of developing diabetes. Historically, research has supported the use of various machine algorithms, such as naive Bayes, decision trees, and artificial neural networks, for early diagnosis of diabetes. However, to achieve maximum accuracy and minimal error in diagnostic predictions, there remains an immense need for further research and innovation to improve the machine-learning tools and techniques available to healthcare professionals. Therefore, in this paper, we propose a novel cloud-based machine-learning fusion technique involving synthesis of three machine algorithms and use of fuzzy systems for collective generation of highly accurate final decisions regarding early diagnosis of diabetes.", "num_citations": "5\n", "authors": ["2239"]}
{"title": "Using FDD for Small Project: An Empirical Case Study\n", "abstract": " Empirical analysis evaluates the proposed system via practical experience and reveals its pros and cons. Such type of evaluation is one of the widely used validation approach in software engineering. Conventional software process models performed well till mid of 1990s, but then gradually replaced by agile methodologies. This happened due to the various features, the agile family offered, which the conventional models failed to provide. However besides the advantages, agile models lacked at some areas as well. To get the extreme benefits from any agile model, it is necessary to eliminate the weaknesses of that model by customizing its development structure. Feature Driven Development (FDD) is one of the widely used agile models in software industry particularly for large scale projects. This model has been criticized by many researchers due to its weaknesses such as explicit dependency on experienced staff, little or no guidance for requirement gathering, rigid nature to accommodate requirement changes and heavy development structure. All these weaknesses make the FDD model suitable only for large scale projects where the requirements are less likely to change. This paper deals with the empirical evaluation of FDD during the development of a small scale web project so that the areas and practices of this model can be identified with empirical proof, which made this model suitable only for large projects. For effective evaluation, the results of FDD case study are compared with a published case study of Extreme Programing (XP), which is widely used for the development of small scale projects.", "num_citations": "5\n", "authors": ["2239"]}
{"title": "Empirical Comparison of XP & SXP\n", "abstract": " Extreme Programming (XP) is a renowned agile model, commonly used for small scale projects. It uses an iterative approach for software development, assisted with agile practices used in extreme manner. Although XP provides the opportunity to handle shortcomings of traditional software development models however it is not exempt from limitations. Lack of proper design, no documentation and poor architectural structure are some of its drawbacks. Furthermore, some of its practices like on-site customer and pair programming are not beneficial in every situation and may cause an extra burden on development process. Simplified Extreme Programming (SXP) process model was proposed to cover these problems without affecting the agility of development process. This paper compares classical XP and proposed SXP with the help of empirical case studies.", "num_citations": "5\n", "authors": ["2239"]}
{"title": "Empirical Evaluation of Modified Agile Models\n", "abstract": " Empirical evaluation is one of the widely accepted validation method in the domain of software engineering which investigates the proposed technique via practical experience and reflects its benefits and limitations. Due to various advantages, agile models have been taking over the conventional software development methodologies since last two decades. However besides the benefits, various limitations have been noticed as well by the researchers and software industry in agile family. To achieve the maximum benefits it is vital to fix the limitations by customizing the development structure of agile models. This paper deals with the empirical analysis of modified agile models called Simplified Extreme Programing (SXP) and Simplified Feature Driven Development (SFDD), which are the modified forms of Extreme Programing (XP) and Feature Driven Development (FDD). SXP was presented to eliminate the issues of conventional XP such as, lack of documentation, poor architectural structure and less focus on design. SFDD was proposed to take care of reported issues in FDD such as explicit dependency on experienced staff, little or no guidance for requirement gathering, rigid nature to accommodate requirement changes and heavy development structure. This study evaluates SXP and SFDD through implementing client oriented projects and discusses the results with empirical analysis.", "num_citations": "4\n", "authors": ["2239"]}
{"title": "Exploring the Agile Family: A Survey\n", "abstract": " Selection of an appropriate software development process model is the key aspect, which leads to the development of high-quality product within scheduled time. The selection of development model depends upon various aspects, related to the project, such as: size, complexity, and scheduled time. Agile family has been satisfying the software industry since last two decades by providing various flavors of development models. Each model of the agile family consists of different practices and characteristics appropriate for specific projects. This paper provides a detail view about the work flow, structure, practices, principles, advantages and disadvantages of various famous and widely used agile models including: Test-driven Development, Extreme Programming, Scrum, Crystal Models, Feature-driven Development and Dynamic System Development Methodology.", "num_citations": "3\n", "authors": ["2239"]}
{"title": "Prediction of Defect Prone Software Modules using MLP based Ensemble Techniques\n", "abstract": " Prediction of defect prone software modules is now considered as an important activity of software quality assurance. This approach uses the software metrics to predict whether the developed module is defective or not. This research presents MLP based ensemble classification framework to predict the defect prone software modules. The framework predicts the defective modules by using three dimensions: 1) Tuned MLP, 2) Tuned MLP with Bagging 3) Tuned MLP with Boosting. In first dimension only the MLP is used for the classification after optimization. In second dimension, the optimized MLP is integrated with bagging technique. In third dimension, the optimized MLP is integrated with boosting technique. Four publically available cleaned NASA MDP datasets are used for the implementation of proposed framework and the performance is evaluated by using F-measure, Accuracy, Roc Area and MCC. The performance of the proposed framework is compared with ten widely used supervised classification techniques by using Scott-Knott ESD test and the results reflects the high performance of the proposed framework.", "num_citations": "3\n", "authors": ["2239"]}
{"title": "Data Mining Framework for Nutrition Ranking: Methodology: SPSS Modeller\n", "abstract": " The goal of this research is to use the technology of Data Mining in a dataset for a ranking of three diets on the respondents and investigate such tools advantages and limitations such as large amount of manipulation before analysis replications with the same results from the analysis bias. Here we can see the ethical consequences of such programs in details.", "num_citations": "2\n", "authors": ["2239"]}
{"title": "Data and Machine Learning Fusion Architecture for Cardiovascular Disease Prediction\n", "abstract": " Heart disease, which is also known as cardiovascular disease, includes various conditions that affect the heart and has been considered a major cause of death over the past decades. Accurate and timely detection of heart disease is the single key factor for appropriate investigation, treatment, and prescription of medication. Emerging technologies such as fog, cloud, and mobile computing provide substantial support for the diagnosis and prediction of fatal diseases such as diabetes, cancer, and cardiovascular disease. Cloud computing provides a cost-efficient infrastructure for data processing, storage, and retrieval, with much of the extant research recommending machine learning (ML) algorithms for generating models for sample data. ML is considered best suited to explore hidden patterns, which is ultimately helpful for analysis and prediction. Accordingly, this study combines cloud computing with ML, collecting datasets from different geographical areas and applying fusion techniques to maintain data accuracy and consistency for the ML algorithms. Our recommended model considered three ML techniques: Artificial Neural Network, Decision Tree, and Na\u00efve Bayes. Real-time patient data were extracted using the fuzzy-based model stored in the cloud.", "num_citations": "1\n", "authors": ["2239"]}