{"title": "Search\u2010based software test data generation: a survey\n", "abstract": " The use of metaheuristic search techniques for the automatic generation of test data has been a burgeoning interest for many researchers in recent years. Previous attempts to automate the test generation process have been limited, having been constrained by the size and complexity of software, and the basic fact that, in general, test data generation is an undecidable problem. Metaheuristic search techniques offer much promise in regard to these problems. Metaheuristic search techniques are high\u2010level frameworks, which utilize heuristics to seek solutions for combinatorial problems at a reasonable computational cost. To date, metaheuristic search techniques have been applied to automate test data generation for structural and functional testing; the testing of grey\u2010box properties, for example safety constraints; and also non\u2010functional properties, such as worst\u2010case execution time. This paper surveys some of\u00a0\u2026", "num_citations": "1716\n", "authors": ["277"]}
{"title": "A theoretical and empirical study of search-based testing: Local, global, and hybrid search\n", "abstract": " Search-based optimization techniques have been applied to structural software test data generation since 1992, with a recent upsurge in interest and activity within this area. However, despite the large number of recent studies on the applicability of different search-based optimization approaches, there has been very little theoretical analysis of the types of testing problem for which these techniques are well suited. There are also few empirical studies that present results for larger programs. This paper presents a theoretical exploration of the most widely studied approach, the global search technique embodied by Genetic Algorithms. It also presents results from a large empirical study that compares the behavior of both global and local search-based optimization on real-world programs. The results of this study reveal that cases exist of test data generation problem that suit each algorithm, thereby suggesting that a\u00a0\u2026", "num_citations": "444\n", "authors": ["277"]}
{"title": "Search-Based Software Testing: Past, Present and Future\n", "abstract": " Search-Based Software Testing is the use of a meta-heuristic optimizing search technique, such as a Genetic Algorithm, to automate or partially automate a testing task, for example the automatic generation of test data. Key to the optimization process is a problem-specific fitness function. The role of the fitness function is to guide the search to good solutions from a potentially infinite search space, within a practical time limit. Work on Search-Based Software Testing dates back to 1976, with interest in the area beginning to gather pace in the 1990s. More recently there has been an explosion of the amount of work. This paper reviews past work and the current state of the art, and discusses potential future research areas and open problems that remain in the field.", "num_citations": "322\n", "authors": ["277"]}
{"title": "A multi-objective approach to search-based test data generation\n", "abstract": " There has been a considerable body of work on search-based test data generation for branch coverage. However, hitherto, there has been no work on multi-objective branch coverage. In many scenarios a single-objective formulation is unrealistic; testers will want to find test sets that meet several objectives simultaneously in order to maximize the value obtained from the inherently expensive process of running the test cases and examining the output they produce. This paper introduces multi-objective branch coverage. The paper presents results from a case study of the twin objectives of branch coverage and dynamic memory consumption for both real and synthetic programs. Several multi-objective evolutionary algorithms are applied. The results show that multi-objective evolutionary algorithms are suitable for this problem, and illustrates the way in which a Pareto optimal search can yield insights into the trade\u00a0\u2026", "num_citations": "186\n", "authors": ["277"]}
{"title": "The state problem for evolutionary testing\n", "abstract": " This paper shows how the presence of states in test objects can hinder or render impossible the search for test data using evolutionary testing. Additional guidance is required to find sequences of inputs that put the test object into some necessary state for certain test goals to become feasible. It is shown that data dependency analysis can be used to identify program statements responsible for state transitions, and then argued that an additional search is needed to find required transition sequences. In order to be able to deal with complex examples, the use of ant colony optimization is proposed. The results of a simple initial experiment are reported", "num_citations": "137\n", "authors": ["277"]}
{"title": "Evolving readable string test inputs using a natural language model to reduce human oracle cost\n", "abstract": " The frequent non-availability of an automated oracle means that, in practice, checking software behaviour is frequently a painstakingly manual task. Despite the high cost of human oracle involvement, there has been little research investigating how to make the role easier and less time-consuming. One source of human oracle cost is the inherent unreadability of machine-generated test inputs. In particular, automatically generated string inputs tend to be arbitrary sequences of characters that are awkward to read. This makes test cases hard to comprehend and time-consuming to check. In this paper we present an approach in which a natural language model is incorporated into a search-based input data generation process with the aim of improving the human readability of generated strings. We further present a human study of test inputs generated using the technique on 17 open source Java case studies. For 10\u00a0\u2026", "num_citations": "92\n", "authors": ["277"]}
{"title": "An empirical investigation into branch coverage for C programs using CUTE and AUSTIN\n", "abstract": " Automated test data generation has remained a topic of considerable interest for several decades because it lies at the heart of attempts to automate the process of Software Testing. This paper reports the results of an empirical study using the dynamic symbolic-execution tool, CUTE, and a search based tool, AUSTIN on five non-trivial open source applications. The aim is to provide practitioners with an assessment of what can be achieved by existing techniques with little or no specialist knowledge and to provide researchers with baseline data against which to measure subsequent work. To achieve this, each tool is applied \u2018as is\u2019, with neither additional tuning nor supporting harnesses and with no adjustments applied to the subject programs under test. The mere fact that these tools can be applied \u2018out of the box\u2019 in this manner reflects the growing maturity of Automated test data generation. However, as might be\u00a0\u2026", "num_citations": "82\n", "authors": ["277"]}
{"title": "Evolutionary testing using an extended chaining approach\n", "abstract": " Fitness functions derived from certain types of white-box test goals can be inadequate for evolutionary software test data generation (Evolutionary Testing), due to a lack of search guidance to the required test data. Often this is because the fitness function does not take into account data dependencies within the program under test, and the fact that certain program statements may need to have been executed prior to the target structure in order for it to be feasible. This paper proposes a solution to this problem by hybridizing Evolutionary Testing with an extended Chaining Approach. The Chaining Approach is a method which identifies statements on which the target structure is data dependent, and incrementally develops chains of dependencies in an  event sequence . By incorporating this facility into Evolutionary Testing, and by performing a test data search for each generated event sequence, the search can be\u00a0\u2026", "num_citations": "81\n", "authors": ["277"]}
{"title": "Evolutionary testing of state-based programs\n", "abstract": " The application of Evolutionary Algorithms to structural test data generation, known as Evolutionary Testing, has to date largely focused on programs with input-output behavior. However, the existence of state behavior in test objects presents additional challenges for Evolutionary Testing, not least because certain test goals may require a search for a sequence of inputs to the test object. Furthermore, state-based test objects often make use of internal variables such as boolean flags, enumerations and counters for managing or querying their internal state. These types of variables can lead to a loss of information in computing fitness values, producing coarse or flat fitness landscapes. This results in the search receiving less guidance, and the chances of finding required test data are decreased. This paper proposes an extended approach based on previous works. Input sequences are generated, and internal\u00a0\u2026", "num_citations": "81\n", "authors": ["277"]}
{"title": "Input Domain Reduction through Irrelevant Variable Removal and its Effect on Local, Global and Hybrid Search-Based Structural Test Data Generation\n", "abstract": " Search-Based Test Data Generation reformulates testing goals as fitness functions so that test input generation can be automated by some chosen search-based optimization algorithm. The optimization algorithm searches the space of potential inputs, seeking those that are \u201cfit for purpose,\u201d guided by the fitness function. The search space of potential inputs can be very large, even for very small systems under test. Its size is, of course, a key determining factor affecting the performance of any search-based approach. However, despite the large volume of work on Search-Based Software Testing, the literature contains little that concerns the performance impact of search space reduction. This paper proposes a static dependence analysis derived from program slicing that can be used to support search space reduction. The paper presents both a theoretical and empirical analysis of the application of this approach to\u00a0\u2026", "num_citations": "79\n", "authors": ["277"]}
{"title": "Automated test data generation for coverage: Haven't we solved this problem yet?\n", "abstract": " Whilst there is much evidence that both concolic and search based testing can outperform random testing, there has been little work demonstrating the effectiveness of either technique with complete real world software applications. As a consequence, many researchers have doubts not only about the scalability of both approaches but also their applicability to production code. This paper performs an empirical study applying a concolic tool, CUTE, and a search based tool, AUSTIN, to the source code of four large open source applications. Each tool is applied `out of the box'; that is without writing additional code for special handling of any of the individual subjects, or by tuning the tools' parameters. Perhaps surprisingly, the results show that both tools can only obtain at best a modest level of code coverage. Several challenges remain for improving automated test data generators in order to achieve higher levels of\u00a0\u2026", "num_citations": "77\n", "authors": ["277"]}
{"title": "Modelling complex biological systems using an agent-based approach\n", "abstract": " Many of the complex systems found in biology are comprised of numerous components, where interactions between individual agents result in the emergence of structures and function, typically in a highly dynamic manner. Often these entities have limited lifetimes but their interactions both with each other and their environment can have profound biological consequences. We will demonstrate how modelling these entities, and their interactions, can lead to a new approach to experimental biology bringing new insights and a deeper understanding of biological systems.", "num_citations": "76\n", "authors": ["277"]}
{"title": "Reducing qualitative human oracle costs associated with automatically generated test data\n", "abstract": " Due to the frequent non-existence of an automated oracle, test cases are often evaluated manually in practice. However, this fact is rarely taken into account by automatic test data generators, which seek to maximise a program's structural coverage only. The test data produced tends to be of a poor fit with the program's operational profile. As a result, each test case takes longer for a human to check, because the scenarios that arbitrary-looking data represent require time and effort to understand. This short paper proposes methods to extracting knowledge from programmers, source code and documentation and its incorporation into the automatic test data generation process so as to inject the realism required to produce test cases that are quick and easy for a human to comprehend and check. The aim is to reduce the so-called qualitative human oracle costs associated with automatic test data generation. The\u00a0\u2026", "num_citations": "69\n", "authors": ["277"]}
{"title": "An integrated systems biology approach to understanding the rules of keratinocyte colony formation\n", "abstract": " Closely coupled in vitro and in virtuo models have been used to explore the self-organization of normal human keratinocytes (NHK). Although it can be observed experimentally, we lack the tools to explore many biological rules that govern NHK self-organization. An agent-based computational model was developed, based on rules derived from literature, which predicts the dynamic multicellular morphogenesis of NHK and of a keratinocyte cell line (HaCat cells) under varying extracellular Ca++ concentrations. The model enables in virtuo exploration of the relative importance of biological rules and was used to test hypotheses in virtuo which were subsequently examined in vitro. Results indicated that cell\u2013cell and cell\u2013substrate adhesions were critically important to NHK self-organization. In contrast, cell cycle length and the number of divisions that transit-amplifying cells could undergo proved non-critical to the\u00a0\u2026", "num_citations": "68\n", "authors": ["277"]}
{"title": "Search-Based Test Input Generation for String Data Types Using the Results of Web Queries\n", "abstract": " Generating realistic, branch-covering string inputs is a challenging problem, due to the diverse and complex types of real-world data that are naturally encodable as strings, for example resource locators, dates of different localised formats, international banking codes, and national identity numbers. This paper presents an approach in which examples of inputs are sought from the Internet by reformulating program identifiers into web queries. The resultant URLs are downloaded, split into tokens, and used to augment and seed a search-based test data generation technique. The use of the Internet as part of test input generation has two key advantages. Firstly, web pages are a rich source of valid inputs for various types of string data that may be used to improve test coverage. Secondly, the web pages tend to contain realistic, human-readable values, which are invaluable when test cases need manual confirmation\u00a0\u2026", "num_citations": "66\n", "authors": ["277"]}
{"title": "Supervised software modularisation\n", "abstract": " This paper is concerned with the challenge of reorganising a software system into modules that both obey sound design principles and are sensible to domain experts. The problem has given rise to several unsupervised automated approaches that use techniques such as clustering and Formal Concept Analysis. Although results are often partially correct, they usually require refinement to enable the developer to integrate domain knowledge. This paper presents the SUMO algorithm, an approach that is complementary to existing techniques and enables the maintainer to refine their results. The algorithm is guaranteed to eventually yield a result that is satisfactory to the maintainer, and the evaluation on a diverse range of systems shows that this occurs with a reasonably low amount of effort.", "num_citations": "46\n", "authors": ["277"]}
{"title": "Search-based failure discovery using testability transformations to generate pseudo-oracles\n", "abstract": " Testability transformations are source-to-source program transformations that are designed to improve the testability of a program. This paper introduces a novel approach in which transformations are used to improve testability of a program by generating a pseudo-oracle. A pseudo-oracle is an alternative version of a program under test whose output can be compared with the original. Differences in output between the two programs may indicate a fault in the original program. Two transformations are presented. The first can highlight numerical inaccuracies in programs and cumulative roundoff errors, whilst the second may detect the presence of race conditions in multi-threaded code.", "num_citations": "43\n", "authors": ["277"]}
{"title": "Hybridizing evolutionary testing with the chaining approach\n", "abstract": " Fitness functions derived for certain white-box test goals can cause problems for Evolutionary Testing (ET), due to a lack of sufficient guidance to the required test data. Often this is because the search does not take into account data dependencies within the program, and the fact that some special intermediate statement (or statements) needs to have been executed in order for the target structure to be feasible. This paper proposes a solution which combines ET with the Chaining Approach. The Chaining Approach is a simple method which probes the data dependencies inherent to the test goal. By incorporating this facility into ET, the search can be directed into potentially promising, unexplored areas of the test object\u2019s input domain. Encouraging results were obtained with the hybrid approach for seven programs known to originally cause problems for ET.", "num_citations": "43\n", "authors": ["277"]}
{"title": "Agent based modelling helps in understanding the rules by which fibroblasts support keratinocyte colony formation\n", "abstract": " BackgroundAutologous keratincoytes are routinely expanded using irradiated mouse fibroblasts and bovine serum for clinical use. With growing concerns about the safety of these xenobiotic materials, it is desirable to culture keratinocytes in media without animal derived products. An improved understanding of epithelial/mesenchymal interactions could assist in this.Methodology/Principal FindingsA keratincyte/fibroblast o-culture model was developed by extending an agent-based keratinocyte colony formation model to include the response of keratinocytes to both fibroblasts and serum. The model was validated by comparison of the in virtuo and in vitro multicellular behaviour of keratinocytes and fibroblasts in single and co-culture in Greens medium. To test the robustness of the model, several properties of the fibroblasts were changed to investigate their influence on the multicellular morphogenesis of keratinocyes and fibroblasts. The model was then used to generate hypotheses to explore the interactions of both proliferative and growth arrested fibroblasts with keratinocytes. The key predictions arising from the model which were confirmed by in vitro experiments were that 1) the ratio of fibroblasts to keratinocytes would critically influence keratinocyte colony expansion, 2) this ratio needed to be optimum at the beginning of the co-culture, 3) proliferative fibroblasts would be more effective than irradiated cells in expanding keratinocytes and 4) in the presence of an adequate number of fibroblasts, keratinocyte expansion would be independent of serum.ConclusionsA closely associated computational and biological approach is a powerful tool\u00a0\u2026", "num_citations": "42\n", "authors": ["277"]}
{"title": "IGUANA: Input generation using automated novel algorithms. A plug and play research tool\n", "abstract": " IGUANA is a tool for automatically generating software test data using search-based approaches. Search-based approaches explore the input domain of a program for test data and are guided by a fitness function. The fitness function evaluates input data and measures how suitable it is for a given purpose, for example the execution of a particular statement in a program, or the falsification of an assertion statement. The IGUANA tool is designed so that researchers can easily compare and contrast different search methods (eg random search, hill climbing and genetic algorithms), fitness functions (eg for obtaining branch coverage of a program) and program analysis techniques for test data generation.", "num_citations": "41\n", "authors": ["277"]}
{"title": "Evolutionary search for test data in the presence of state behaviour\n", "abstract": " The application of metaheuristic search techniques, such as evolutionary algorithms, to the problem of automatically generating software test data has been a burgeoning interest for many researchers in recent years. To date, work in applying search techniques to structural test data generation has largely focused on generating inputs for test objects with input-output behaviour. This thesis aims to extend the approach for test objects with state behaviour. This presents several challenges, not least because test goals with state-based test objects may require input sequences to be generated. Another problem includes generating test data in the presence of internal variables such as flags, enumerations and counters. Such variables are often responsible for managing the \u201cstate\u201d of the test object. However, their use can lead to information loss with regards to the original input conditions that lead to the fulfilment of certain test goals. Consequently the search receives less guidance, and may fail to find test data. This thesis proposes an extended evolutionary structural test data generation approach that allows input sequences to be generated, and tackles internal variable problems through hybridization of the method with an extended chaining approach. The basic idea of the chaining approach is to find a sequence of statements, involving internal variables, which need to be executed prior to the test goal. By requiring that these statements are executed, information previously unavailable to the search can be made use of, possibly guiding it into potentially promising and unexplored areas of the test object\u2019s input domain. A number of experiments\u00a0\u2026", "num_citations": "36\n", "authors": ["277"]}
{"title": "Design and analysis of different alternating variable searches for search-based software testing\n", "abstract": " Manual software testing is a notoriously expensive part of the software development process, and its automation is of high concern. One aspect of the testing process is the automatic generation of test inputs. This paper studies the Alternating Variable Method (AVM) approach to search-based test input generation. The AVM has been shown to be an effective and efficient means of generating branch-covering inputs for procedural programs. However, there has been little work that has sought to analyse the technique and further improve its performance. This paper proposes two different local searches that may be used in conjunction with the AVM, Geometric and Lattice Search. A theoretical runtime analysis proves that under certain conditions, the use of these searches results in better performance compared to the original AVM. These theoretical results are confirmed by an empirical study with five programs, which\u00a0\u2026", "num_citations": "33\n", "authors": ["277"]}
{"title": "Automated discovery of valid test strings from the web using dynamic regular expressions collation and natural language processing\n", "abstract": " Classic approaches to test input generation -- such as dynamic symbolic execution and search-based testing -- are commonly driven by a test adequacy criterion such as branch coverage. However, there is no guarantee that these techniques will generate meaningful and realistic inputs, particularly in the case of string test data. Also, these techniques have trouble handling path conditions involving string operations that are inherently complex in nature. This paper presents a novel approach of finding valid values by collating suitable regular expressions dynamically that validate the format of the string values, such as an email address. The regular expressions are found using web searches that are driven by the identifiers appearing in the program, for example a string parameter called email Address. The identifier names are processed through natural language processing techniques to tailor the web queries\u00a0\u2026", "num_citations": "32\n", "authors": ["277"]}
{"title": "Using compression algorithms to support the comprehension of program traces\n", "abstract": " Several software maintenance tasks such as debugging, phase-identification, or simply the high-level exploration of system functionality, rely on the extensive analysis of program traces. These usually require the developer to manually discern any repeated patterns that may be of interest from some visual representation of the trace. This can be both time-consuming and inaccurate; there is always the danger that visually similar trace-patterns actually represent distinct program behaviours. This paper presents an automated phase-identification technique. It is founded on the observation that the challenge of identifying repeated patterns in a trace is analogous to the challenge faced by data-compression algorithms. This applies an established data compression algorithm to identify repeated phases in traces. The SEQUITUR compression algorithm not only compresses data, but organises the repeated patterns into a\u00a0\u2026", "num_citations": "29\n", "authors": ["277"]}
{"title": "Automatic generation of valid and invalid test data for string validation routines using web searches and regular expressions\n", "abstract": " Classic approaches to automatic input data generation are usually driven by the goal of obtaining program coverage and the need to solve or find solutions to path constraints to achieve this. As inputs are generated with respect to the structure of the code, they can be ineffective, difficult for humans to read, and unsuitable for testing missing implementation. Furthermore, these approaches have known limitations when handling constraints that involve operations with string data types.This paper presents a novel approach for generating string test data for string validation routines, by harnessing the Internet. The technique uses program identifiers to construct web search queries for regular expressions that validate the format of a string type (such as an email address). It then performs further web searches for strings that match the regular expressions, producing examples of test cases that are both valid and realistic\u00a0\u2026", "num_citations": "26\n", "authors": ["277"]}
{"title": "Validation and discovery from computational biology models\n", "abstract": " Simulation software is often a fundamental component in systems biology projects and provides a key aspect of the integration of experimental and analytical techniques in the search for greater understanding and prediction of biology at the systems level. It is important that the modelling and analysis software is reliable and that techniques exist for automating the analysis of the vast amounts of data which such simulation environments generate. A rigorous approach to the development of complex modelling software is needed. Such a framework is presented here together with techniques for the automated analysis of such models and a process for the automatic discovery of biological phenomena from large simulation data sets. Illustrations are taken from a major systems biology research project involving the in vitro investigation, modelling and simulation of epithelial tissue.", "num_citations": "26\n", "authors": ["277"]}
{"title": "An identification of program factors that impact crossover performance in evolutionary test input generation for the branch coverage of C programs\n", "abstract": " Context: Genetic Algorithms are a popular search-based optimisation technique for automatically generating test inputs for structural coverage of a program, but there has been little work investigating the class of programs for which they will perform well.Objective: This paper presents and evaluates a series of program factors that are hypothesised to affect the performance of crossover, a key search operator in Genetic Algorithms, when searching for inputs that cover the branching structure of a C function.Method: Each program factor is evaluated with example programs using Genetic Algorithms with and without crossover. Experiments are also performed to test whether crossover is acting as macro-mutation operator rather than usefully recombining the component parts of input vectors when searching for test data.Results: The results show that crossover has an impact for each of the program factors studied\u00a0\u2026", "num_citations": "16\n", "authors": ["277"]}
{"title": "Mutation Operators for Agent-Based Models\n", "abstract": " This short paper argues that agent-based models are an independent class of software application with their own unique properties, with the consequential need for the definition of suitable, tailored mutation operators. Testing agent-based models can be very challenging, and no established testing technique has yet been introduced for such systems. This paper discusses the application of mutation testing techniques, and mutation operators are proposed that can imitate potential programmer errors and result in faulty simulation runs of a model.", "num_citations": "16\n", "authors": ["277"]}
{"title": "Establishing the source code disruption caused by automated remodularisation tools\n", "abstract": " Current software remodularisation tools only operate on abstractions of a software system. In this paper, we investigate the actual impact of automated remodularisation on source code using a tool that automatically applies remodularisations as refactorings. This shows us that a typical remodularisation (as computed by the Bunch tool) will require changes to thousands of lines of code, spread throughout the system (typically no code files remain untouched). In a typical multi-developer project this presents a serious integration challenge, and could contribute to the low uptake of such tools in an industrial context. We relate these findings with our ongoing research into techniques that produce iterative commit friendly\" code changes to address this problem.", "num_citations": "14\n", "authors": ["277"]}
{"title": "A theoretical runtime and empirical analysis of different alternating variable searches for search-based testing\n", "abstract": " The Alternating Variable Method (AVM) has been shown to be a surprisingly effective and efficient means of generating branch-covering inputs for procedural programs. However, there has been little work that has sought to analyse the technique and further improve its performance. This paper proposes two new local searches that may be used in conjunction with the AVM, Geometric and Lattice Search. A theoretical runtime analysis shows that under certain conditions, the use of these searches is proven to outperform the original AVM. These theoretical results are confirmed by an empirical study with four programs, which shows that increases of speed of over 50% are possible in practice.", "num_citations": "14\n", "authors": ["277"]}
{"title": "An Agent-Based software platform for modelling systems biology\n", "abstract": " 3 Design Decisions 10 3.1 Feature Identification....................... 10 3.2 System Description........................ 10 3.3 Biology Example: Keratinocyte colony formation....... 11 3.4 Unified Modelling Framework.................. 24 3.5 Handling Of Time........................ 24 3.5. 1 Communication...................... 26 3.5. 2 Updating Agents..................... 26 3.6 Communication Networks.................... 27 3.6. 1 Agent-Environment Interaction............. 27 3.6. 2 Agent-Agent Interaction................. 27 3.7 Simulation Output and Data Storage.............. 29", "num_citations": "9\n", "authors": ["277"]}
{"title": "How does program structure impact the effectiveness of the crossover operator in evolutionary testing?\n", "abstract": " Recent results in Search-Based Testing show that the relatively simple Alternating Variable hill climbing method outperforms Evolutionary Testing (ET) for many programs. For ET to perform well in covering an individual branch, a program must have a certain structure that gives rise to a fitness landscape that the crossover operator can exploit. This paper presents theoretical and empirical investigations into the types of program structure that result in such landscapes. The studies show that crossover lends itself to programs that process large data structures or have an internal state that is reached over a series of repeated function or method calls. The empirical study also investigates the type of crossover which works most efficiently for different program structures. It further compares the results obtained by ET with those obtained for different variants of hill climbing algorithm, which are found to be effective for many\u00a0\u2026", "num_citations": "7\n", "authors": ["277"]}
{"title": "Co-testability transformation\n", "abstract": " This paper introduces the notion of \u2018co-testability transformation\u2019. As opposed to traditional testability transformations, which replace the original program in testing, co-testability transformations are designed to be used in conjunction with the original program (and any additional co-transformations as well). Until now, testability transformations have only been used to improve test data generation. However, co-testability transformations can function as partial oracles. This paper demonstrates practical usage of a co-testability transformation for automatically detecting floating-point errors in program code.", "num_citations": "7\n", "authors": ["277"]}
{"title": "Improving Evolutionary Testing in the Presence of State Behavior\n", "abstract": " Evolutionary testing employs optimisation techniques, notably evolutionary algorithms, to search for test data for some given test criteria. However state behaviour in programs can hinder or render impossible the search for test data for certain test goals. In such instances the test object often needs to be in some given state for the test goal to be feasible.This research centres on the identification of program state types that complicate evolutionary testing, as well as analyses for the identification of states and locations of transitions in programs. Finally strategies are outlined for making test goals achievable for evolutionary testing in the presence of program states. Where the search for a transition sequence to get into some state appears to be intractable, the use of ant colony optimisation algorithms is proposed.", "num_citations": "7\n", "authors": ["277"]}
{"title": "Superstate identification for state machines using search-based clustering\n", "abstract": " State machines are a popular method of representing a system at a high level of abstraction that enables developers to gain an overview of the system they represent and quickly understand it.", "num_citations": "6\n", "authors": ["277"]}
{"title": "Effectively incorporating expert knowledge in automated software remodularisation\n", "abstract": " Remodularising the components of a software system is challenging: sound design principles (e.g., coupling and cohesion) need to be balanced against developer intuition of which entities conceptually belong together. Despite this, automated approaches to remodularisation tend to ignore domain knowledge, leading to results that can be nonsensical to developers. Nevertheless, suppling such knowledge is a potentially burdensome task to perform manually. A lot information may need to be specified, particularly for large systems. Addressing these concerns, we propose the SUpervised reMOdularisation (SUMO) approach. SUMO is a technique that aims to leverage a small subset of domain knowledge about a system to produce a remodularisation that will be acceptable to a developer. With SUMO, developers refine a modularisation by iteratively supplying corrections. These corrections constrain the type of\u00a0\u2026", "num_citations": "4\n", "authors": ["277"]}
{"title": "An analysis of the performance of the bunch modularisation algorithm\u2019s hierarchy generation approach\n", "abstract": " The Bunch modularisation tool can automatically modularise software systems. The level-by-level approach it uses restricts solutions available at deeper levels due to choices made at earlier levels in the optimization process, however. This paper identifies two sources of sub-optimal results with Bunch. Fitness of higher levels in the hierarchy are impacted by lower levels as a result of overfitting. Mean module fitness values are shown to be improved in approximately 30% of cases by generating non-layered hierarchies using a random mutation approach.", "num_citations": "3\n", "authors": ["277"]}
{"title": "A multiobjective optimisation approach for the dynamic inference and refinement of agent-based model specifications\n", "abstract": " Despite their increasing popularity, agent-based models are hard to test, and so far no established testing technique has been devised for this kind of software applications. Reverse engineering an agent-based model specification from model simulations can help establish a confidence level about the implemented model and in some cases reveal discrepancies between observed and normal or expected behaviour. In this study, a multiobjective optimisation technique based on a simple random search algorithm is deployed to dynamically infer and refine the specification of three agent-based models from their simulations. The multiobjective optimisation technique also incorporates a dynamic invariant detection technique which serves to guide the search towards uncovering new model behaviour that better captures the model specification. The Non-dominated Sorting Genetic Algorithm (NSGA-II) was also\u00a0\u2026", "num_citations": "3\n", "authors": ["277"]}
{"title": "Using dictionary compression algorithms to identify phases in program traces\n", "abstract": " Program execution traces record the sequences of events or functions that are encountered during a program execution. They can provide valuable insights into the run-time behaviour of software systems and form the basis for dynamic analysis techniques. Execution traces of large software systems can be huge, incorporating hundreds of thousands of elements, rendering them difficult to interpret and understand. One recognised problem is the phase-detection problem where the challenge is to identify repeating phases within a trace that may correspond to the execution of particular features within the software system. This paper proposes an abstraction technique that uses the well-known LZW dictionary compression algorithm to systematically identify such phases. The feasibility of this approach is demonstrated with respect to a small case study on a Java program.", "num_citations": "2\n", "authors": ["277"]}
{"title": "Testing multi-agent based simulations using MASTER\n", "abstract": " In order to gain confidence that a model is implemented correctly, several simulation runs need to be performed and inspected. In practice, however, the inspection process tends to be performed manually\u2014a tedious and time-consuming process. This paper presents MASTER, a tool that aims to semi-automatically identify \u2018suspicious\u2019 simulation runs, which may be due to implementation faults in an underlying multiagent based model.The paper presents a case study using MASTER with the flockers model supplied with MASON, with results detailing the identification of \u2018suspicious\u2019 simulation executions when the model\u2019s code is mutated to introduce small faults. Results are also presented with a further model for simulating skin tissue written using FLAME. Noise is injected into some of the model\u2019s key statistics, the presence of which is correctly identified by MASTER.", "num_citations": "1\n", "authors": ["277"]}
{"title": "An investigation into qualitative human oracle costs\n", "abstract": " The test data produced by automatic test data generators are often \u2018unnatural\u2019particularly for the programs that make use of human-recognisable variables such as \u2018country\u2019,\u2018name\u2019,\u2018date\u2019,\u2018time\u2019,\u2018age\u2019and so on. The test data generated for these variables are usually arbitrarylooking values that are complex for human testers to comprehend and evaluate. This is due to the fact that automatic test data generators have no domain knowledge about the program under test and thus the test data they produce are hardly recognised by human testers. As a result, the tester is likely to spend additional time in order to understand such data. This paper demonstrates how the incorporation of some domain knowledge into an automatic test data generator can significantly improve the quality of the generated test data. Empirical studies are proposed to investigate how this incorporation of knowledge can reduce the overall testing costs.", "num_citations": "1\n", "authors": ["277"]}