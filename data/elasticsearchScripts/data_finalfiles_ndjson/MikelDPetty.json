{"title": "A composability lexicon\n", "abstract": " Composability is the capability to select and assemble simulation components in various combinations into simulation systems to satisfy specific user requirements. The defining characteristic of composability is the ability to combine and recombine components into different simulation systems for different purposes. Composability would have many benefits for the practice of simulation. However, the precise meaning of the term \u201ccomposability\u201d and related terminology in the research literature is surprisingly varied. Composability has multiple related meanings or levels that differ primarily by the question of what is being composed. Nine distinct levels of composability are identified, defined, and compared. Two views on composability, concerned with the syntactic and semantic aspects of composability respectively, are contrasted. Interoperability and configurability are related to composability, but they are different\u00a0\u2026", "num_citations": "212\n", "authors": ["1402"]}
{"title": "Validity of models and classes of models in semantic composability\n", "abstract": " Composability is the capability to select and assemble simulation components in various combinations into simulation systems. The defining characteristic of composability is the ability to combine and recombine components. Composability exists in two forms, syntactic and semantic (also known as engineering and modeling). Syntactic composability is the implementation of components so that they can be combined. Semantic composability is the question of whether the models embodied by the composed components can be meaningfully composed. A theory of semantic composability has been developed that examines the semantic composability of models using formal definitions and reasoning.In this paper results of semantic composability theory concerned with validity are presented. After briefly restating formal definitions of model and simulation, labeled transition systems are defined and introduced as models of the computation of models and compositions. Bisimulation, which is a general relation between the states of labeled transition simulations, is specialized with the addition of a validity metric, and shown to serve as a formal definition of validity. The power of different validity metrics to represent application-specific validity is explained. Classes of models are defined and compared with the models used in simulation. Certain classes of models and validity metrics for which validity is (or is not) preserved under composition are defined and their validity (or lack thereof) under composition is proven.", "num_citations": "71\n", "authors": ["1402"]}
{"title": "A formal basis for a theory of semantic composability\n", "abstract": " Composability is the capability to select and assemble simulation components in various combinations into simulation systems. The defining characteristic of composability is the ability to combine and recombine simulation components into different simulation systems. Two types of composability are considered: syntactic and semantic. Syntactic composability is the actual implementation of composability, and requires that the composable components be constructed so that their implementation details are compatible for the different configurations that might be composed. In contrast, modeling (semantic) composability is a question of whether the models that make up the composed simulation system can be meaningfully composed, ie, if their combined computation is semantically valid. Although there has been some work on syntactic composability there has been almost none on semantic composability, though\u00a0\u2026", "num_citations": "69\n", "authors": ["1402"]}
{"title": "Verification, validation, and accreditation\n", "abstract": " Verification and validation (V&V) are essential prerequisites to the credible and reliable use of a model and its results. As such, they are important aspects of any simulation project and most developers and users of simulations have at least a passing familiarity with the terms. But what are they exactly, and what methods and processes are available to perform them? Similarly, what is accreditation, and how does it relate to V&V? Those questions are addressed in this chapter.* Along the way, three central concepts of verification, validation, and accreditation (VV&A) will be identified used to unify the material.This chapter is composed of five sections. This first section motivates the need for VV&A and provides definitions necessary to their understanding. The second section places VV&A in the context of simulation projects and discusses overarching issues in the practice of VV&A. The third section", "num_citations": "65\n", "authors": ["1402"]}
{"title": "Computer-generated forces in distributed interactive simulation\n", "abstract": " Distributed Interactive Simulation (DIS) is an architecture for building large-scale simulation models from a set of independent simulator nodes communicating via a common network protocol. DIS is most often used to create a simulated battlefield for military training. Computer Generated Forces (CGF) systems control large numbers of autonomous battlefield entities in a DIS simulation using computer equipment and software rather than humans in simulators. CGF entities serve as both enemy forces and supplemental friendly forces in a DIS exercise. Research into various aspects of CGF systems is ongoing. Several CGF systems have been implemented.", "num_citations": "50\n", "authors": ["1402"]}
{"title": "Data distribution management migration from DoD 1.3 to IEEE 1516\n", "abstract": " In September 2000, the IEEE approved the three High Level Architecture (HLA) documents as standards, 1516, 1516.1, and 1516.2. The form, functionality, and content of these documents are significantly the same as their DoD sources, the HLA 1.3 standards. Unlike the other service areas, Data Distribution Management (DDM) experienced noticeable changes in response to input from the community and a focused re-engineering effort. These changes included the removal or routing spaces and the introduction of default ranges. While these changes did not affect the fundamental functionality of DDM, they simplified implementation of some approaches for users. With little effort, users can design DDM approaches which will migrate smoothly from DoD 1.3 to IEEE 1516.", "num_citations": "49\n", "authors": ["1402"]}
{"title": "Integrating crowd-behavior modeling into military simulation using game technology\n", "abstract": " Crowds of noncombatants play a large and increasingly recognized role in modern military operations and often create substantial difficulties for the combatant forces involved. However, realistic models of crowds are essentially absent from current military simulations. To address this problem, the authors are developing a crowd simulation capable of generating crowds of noncombatant civilians that exhibit a variety of realistic individual and group behaviors at differing levels of fidelity. The crowd simulation is interoperable with existing military simulations using a standard, distributed simulation architecture. Commercial game technology is used in the crowd simulation to model both urban terrain and the physical behaviors of the human characters that make up the crowd. The objective of this article is to present the design and development process of a simulation that integrates commercially available game\u00a0\u2026", "num_citations": "46\n", "authors": ["1402"]}
{"title": "Verification and validation\n", "abstract": " This chapter contains sections titled:   Introduction   Performing Verification and Validation   Verification and Validation Examples   Conclusion   Key Terms   References", "num_citations": "43\n", "authors": ["1402"]}
{"title": "Crowd behavior cognitive model architecture design\n", "abstract": " \u2013Design Decisions\u2013The Cognitive Model Architecture\u2013Crowd/Group/Individual Parameters\u2013Computational Model Algorithm\u2013Implementation\u2014The Tables", "num_citations": "40\n", "authors": ["1402"]}
{"title": "A formal approach to composability\n", "abstract": " Composability is the capability to select and assemble simulation components in various combinations into simulation systems to satisfy specific user requirements. The defining characteristic of composability is the ability to combine and recombine simulation components into different simulation systems. Two types of composability are considered: syntactic and semantic. Syntactic composability is the actual implementation of composability and requires that the composable components be constructed so that their implementation details are compatible for the different configurations that might be composed. In contrast, semantic composability is a question of whether the models that make up the composed simulation system can be meaningfully composed, that is, if their combined computation is valid. There has been significant work on syntactic composability but almost none on semantic composability. We have\u00a0\u2026", "num_citations": "34\n", "authors": ["1402"]}
{"title": "A call to arms: standards for agent-based modeling and simulation\n", "abstract": " Standards are as old as civilization itself and they are vital to human development. Standards touch almost every part of our lives, from the water we drink to the language used to write this article. A sign of a good standard is one that we do not notice. Good standards exist and so do processes and organizations to create and maintain them. As agent-based modeling and simulation matures as a methodology, a discussion of standards applicable to it becomes increasingly important. Descriptive standards for agent-based models, such as the Overview, Design concepts, and Details protocol and agent-based extensions to the Unified Modeling Language, have already begun to emerge. Software tools for implementing such models, such as Netlogo and Repast Simphony, are increasingly well-known and have the potential to become de facto standards among the wider scientific community for agent-based simulation. Based on the findings of a series of workshops that brought together experts throughout the modeling and simulation community, we argue that agent-based modeling and simulation is no different from the other emerging technical subjects in the sense that standards, both existing and new, may be applicable to it, and that the community should both adopt existing standards that are relevant and exploit the already existing standards processes and organizations to develop new ones.", "num_citations": "33\n", "authors": ["1402"]}
{"title": "Computational complexity of selecting components for composition\n", "abstract": " Composability is the capability to select and assemble simulation components in various combinations into simulation systems to satisfy specific user requirements. The defining characteristic of composability is the ability to combine and recombine components. Composability exists in two forms, syntactic and semantic (also known as engineering and modeling). Syntactic composability is the implementation of components so that they can be connected. Semantic composability is the question of whether the models implemented in the composed components can be meaningfully composed, ie, is their combined computation valid? A theory of semantic composability has been developed that examines the semantic composability of models using formal definitions and reasoning.The computational complexity of the problem of selecting a set of components that meet a set of objectives is examined. In earlier work, Page and Opper defined four variants of this component selection problem based on two forms of objectives decidability (bounded and unbounded) and two forms of composition (emergent and non-emergent). They gave a proof that the bounded non-emergent variant of the component selection problem is NP-complete. In this paper an additional form of composition (anti-emergent) is defined, leading to two additional variants of the problem. Then a general form of the component selection problem that subsumes all six variants is defined. The general component selection problem is proven to be NP-complete even if the objectives met by a component or composition are known. Several related but different problems, including\u00a0\u2026", "num_citations": "32\n", "authors": ["1402"]}
{"title": "Experimental comparison of d-rectangle intersection algorithms applied to HLA data distribution\n", "abstract": " The High Level Architecture (HLA) is a standard for constructing distributed simulations. The Data Distribution Management services of HLA reduce the amount of data delivered to an HLA federate by allowing communications connections to be based on federates\u2019 expressed data production and requirements. At the core of determining which connections to make is a geometric problem: finding the dynamic intersection of d-dimensional rectilinear hyperrectangles in d-space. Four different algorithms for solving that problem are described, including a new one developed through application of a data structure from computational geometry. Those algorithms are then compared in an experiment designed to reveal how well they perform in the specific context of the data distribution application. Both intersection performance and connectivity efficiency results are reported.", "num_citations": "32\n", "authors": ["1402"]}
{"title": "The computational complexity of the high level architecture data distribution management matching and connecting processes\n", "abstract": " Abstract The High Level Architecture (HLA) is an architecture standard for constructing federations of distributed simulations that exchange data at run-time. HLA includes interest management capabilities (known as \u201cData Distribution Management\u201d) that reduce the data sent during a federation execution using the simulations' run-time declarations describing the data they plan to send and wish to receive. The total computation associated with Data Distribution Management during the execution of a federation can be separated into four processes: declaring, matching, connecting, and routing. These processes are defined and the computational complexities of the matching and connecting processes during a federation execution are determined. The matching process requires total time with a lower bound in \u03a9 (n log n) and an upper bound in O (n 2), where n is the number of run-time data distribution actions\u00a0\u2026", "num_citations": "31\n", "authors": ["1402"]}
{"title": "Developing a crowd federate for military simulation\n", "abstract": " Crowds of non-combatants play a large and increasingly recognized role in modern military operations, and often create substantial difficulties for the combatant forces involved. US military actions in Mogadishu, Bosnia, and Iraq exemplify the significant effects crowds may have on military operations. However, in spite of their potential significance, realistic models of crowds are essentially absent from current military simulations. For the scenarios considered likely in future conflicts the absence of crowds and of non-combatants in general would be a serious departure from realism.We are engaged in a two-phase research project aimed at developing a crowd modeling capability for military simulation. The first phase, recently completed, consisted of three parts: a requirements analysis to identify military simulation crowd modeling requirements, a literature survey to examine psychological research relevant to crowd modeling, and a design study to explore design issues in the implementation of a crowd simulation. In the second phase, now well underway, we are developing a crowd simulation, implemented as a distributed simulation federate, that will be interoperable with existing military simulations and will have a credible psychological basis for the crowd behavior it generates.", "num_citations": "31\n", "authors": ["1402"]}
{"title": "Calculating and using confidence intervals for model validation\n", "abstract": " A confidence interval is an interval (ie, a range of values) estimate of a parameter of a population (eg, a mean) calculated from a sample drawn from the population. A confidence interval has an associated confidence level, which is frequency with which a calculated confidence interval is expected to contain the population parameter. Confidence intervals are of interest in modeling and simulation because they are often used in model validation. Typically, a set of executions of the model to be validated, which is a sample from the population of all possible executions of the model, are run and from their results a confidence interval is calculated as an estimate of the population parameter (eg, mean model output value) that would result if all possible model executions had been run. Then, if the corresponding known or observed value for the simuland is within the confidence interval calculated from the model executions, or within some acceptable tolerance of the confidence interval\u2019s endpoints, the model is considered to be valid for the parameter in question. This paper is an introductory tutorial and survey on confidence intervals in model validation. Confidence intervals are introduced in a statistical context, their interpretation and use in model validation is explained, and examples of the application of confidence intervals in validation are presented.", "num_citations": "30\n", "authors": ["1402"]}
{"title": "Machine learning cyberattack and defense strategies\n", "abstract": " Cybersecurity is an increasingly important challenge for computer systems. In this work, cyberattacks were modeled using an extension of the well-known Petri net formalism. That formalism, designated Petri nets with players, strategies, and costs, models the states of the cyberattack and events during the attack as markings and transition firings in the net respectively. The formalism models the attacker and defender as competing players who may observe the marking of a subset of the net and based on the observed marking act by changing the stochastic firing rates of a subset of the transitions in order to achieve their competing goals. Rate changes by the players incur a cost. Using the formalism, nets were constructed to model specific cyberattack patterns (cross-site scripting and spear phishing) documented in the Common Attack Pattern Enumeration and Classification database. The models were validated by a\u00a0\u2026", "num_citations": "26\n", "authors": ["1402"]}
{"title": "A high level architecture-based medical simulation system\n", "abstract": " The Combat Trauma Patient Simulation (CTPS) is a dual purpose training and analysis simulation system that provides an \"end-to- end\" simulation of the military medical treat ment process for combat trauma injuries from the time of occurrence through initial treatment at the field hospital. CTPS was built by inte grating a set of existing commercial and mili tary simulations, each specialized for a differ ent part of the process. None of the integrated simulators was initially interoperable, or even designed to be interoperable, with the others. The integration was achieved using the High Level Architecture (HLA), together with inter face modules for each of the simulators. The HLA ownership transfer services were central to the system's design.", "num_citations": "24\n", "authors": ["1402"]}
{"title": "Software frameworks for model composition\n", "abstract": " A software framework is an architecture or infrastructure intended to enable the integration and interoperation of software components. Specialized types of software frameworks are those specifically intended to support the composition of models or other components within a simulation system. Such frameworks are intended to simplify the process of assembling a complex model or simulation system from simpler component models as well as to promote the reuse of the component models. Several different types of software frameworks for model composition have been designed and implemented; those types include common library, product line architecture, interoperability protocol, object model, formal, and integrative environment. The various framework types have different components, processes for composing models, and intended applications. In this survey the fundamental terms and concepts of software\u00a0\u2026", "num_citations": "20\n", "authors": ["1402"]}
{"title": "High level architecture data distribution management migration from DoD 1.3 to IEEE 1516\n", "abstract": " The High Level Architecture (HLA) is an infrastructure for assembling federations of distributed simulations, or federates, that interoperate by exchanging simulation data at run time. HLA's Data Distribution Management (DDM) services control the amount of simulation data delivered to an HLA federate by establishing dynamic data communications connections based on the federates' run\u2010time declarations of data production and requirements. In September 2000 the Institute for Electrical and Electronic Engineers (IEEE) approved a new standard for HLA, the IEEE 1516 standard. Most parts of the IEEE 1516 standard are generally similar to the U.S. Department of Defense HLA standard upon which it was based, the DoD 1.3 standard. However, the DDM services were significantly changed in the IEEE 1516 standard. The changes to DDM included the removal of routing spaces and the introduction of default\u00a0\u2026", "num_citations": "20\n", "authors": ["1402"]}
{"title": "Comparing high level architecture data distribution management specifications 1.3 and 1516\n", "abstract": " The high level architecture (HLA) is a standard for federations of distributed simulations that exchange run-time data. HLA's data distribution management (DDM) services reduce data delivered to simulations based on their declarations of data produced and required. The HLA specifications, including DDM, were changed substantially from the Department of Defense 1.3 standard to the IEEE 1516 standard. The two DDM specifications' (DDM 1.3 and DDM 1516) power to define intersimulation data flows are compared. A transformation from DDM 1.3 to DDM 1516 configurations and a mapping from DDM 1516 to DDM 1.3 configurations prove that the DDM specifications are equivalently powerful.", "num_citations": "19\n", "authors": ["1402"]}
{"title": "Modeling and validation challenges for Complex Systems\n", "abstract": " This chapter, which is meant as an introductory tutorial and brief literature survey, has four main sections. The first describes complex systems and lists their defining characteristics, and motivates the interest in validating models of complex systems by discussing validation risk. Then, each of the following sections discusses one of three selected defining characteristics of complex systems (sensitivity to initial conditions, emergent behavior, and composition of components), explaining why the characteristic in question makes modeling and validation more difficult and offering some approaches to dealing with and mitigating the difficulties.ABSTRACT", "num_citations": "17\n", "authors": ["1402"]}
{"title": "Multi\u2010Resolution Combat Modeling\n", "abstract": " This chapter is a tutorial on the basic concepts and operations of multi\u2010resolution combat modeling and a review of existing multi\u2010resolution combat models as they have been developed, implemented, and used. Reasons for developing such systems are given, key terms and concepts of multi\u2010resolution combat models are defined, typical software architectures for implementing them are described, the history and literature of multi\u2010resolution combat model implementations are surveyed, and challenges and open research issues in their implementation and use are identified. The chapter begins with brief introductions to entity level and unit level combat models as preparatory background. It provides the conceptual framework of multi\u2010resolution combat modeling, within which specific details and capabilities can be placed in context. A generic implementation architecture is described, identifying the primary\u00a0\u2026", "num_citations": "17\n", "authors": ["1402"]}
{"title": "Usefulness of software architecture description languages for modeling and analysis of federates and federation architectures\n", "abstract": " Software architecture is high-level software design dealing with the structure and                 organization of large software systems. Architecture description languages (ADLs)                 are languages designed to represent software designs at the architecture level. ADLs                 are not widely used in the development of simulation systems. This research                 investigates the utility and effectiveness of ADLs for architecture-level design and                 analysis of simulation systems. Experimental applications of two ADLs to the                 specification and analysis of simulation architectures were conducted. Rapide was                 used to model the EnviroFed federation architecture and analyze data volume with and                 without interest management. Acme was used to model the ModSAF federate architecture                 and to analyze execution time at the component and federate levels in ModSAF. The\u00a0\u2026", "num_citations": "16\n", "authors": ["1402"]}
{"title": "Crowd federate architecture and api design\n", "abstract": " Our United States military is increasingly engaged in urban combat or peace-keeping missions. As a result, soldiers are also increasingly engaged with the civilian non-combatant inhabitants of various nations. Yet, current military simulation models have little or no representation of these effects which can lead to suboptimal training or experimentation results. More realistic and sophisticated crowd models are desired to address this growing need.It is unlikely that one crowd model will meet all our military\u2019s crowd requirements since models are needed with a variety of behaviors depending upon the type of mission, the size of encounters, and the user application. A federate that may be used to provide such variety of civilian behaviors in a crowd context would need to be flexible, configurable, and extensible. In this paper, we report about one such instantiation and the framework which supports it. The framework is a layered architecture that is composed of a physical layer in which movements and other actions of the crowd are manifested; and also a cognitive layer in which the motivations of these activities are generated and propagated. Connecting these two layers is an API layer that provides mapping and communication services for the stimuli, activities, and accompanying parameters that are crowd behavior centric.", "num_citations": "16\n", "authors": ["1402"]}
{"title": "Combat modeling with the high level architecture and base object models\n", "abstract": " Modeling and simulation (M&S) has arguably been used more extensively by the US Department of Defense (DoD) than any other organization. Taken in total, the application and use of M&S has been hugely successful. M&S in general is often used in situations where exercising or experimenting with the real-world subject of the simulation would be too difficult, too expensive, or too dangerous, and military applications in particular include some of the most extreme examples of difficult, expensive, and dangerous situations. Consequently, M&S is used widely and frequently in the DoD to support training, acquisition, analysis, experimentation, engineering, and test and evaluation (Castro et al., 2002). The nearly ubiquitous adoption of M&S technologies throughout the DoD provides incontrovertible evidence of its efficacy and cost-effectiveness.", "num_citations": "12\n", "authors": ["1402"]}
{"title": "Modeling crowd behavior for military simulation applications\n", "abstract": " The prelims comprise:   Abstract   Introduction   Crowd Simulation Requirements Analysis   Psychological and Computational Models of Crowd Behavior   Crowd Simulation Design Study   Conclusions   Acknowledgements   References", "num_citations": "11\n", "authors": ["1402"]}
{"title": "Automated search-based robustness testing for autonomous vehicle software\n", "abstract": " Autonomous systems must successfully operate in complex time-varying spatial environments even when dealing with system faults that may occur during a mission. Consequently, evaluating the robustness, or ability to operate correctly under unexpected conditions, of autonomous vehicle control software is an increasingly important issue in software testing. New methods to automatically generate test cases for robustness testing of autonomous vehicle control software in closed-loop simulation are needed. Search-based testing techniques were used to automatically generate test cases, consisting of initial conditions and fault sequences, intended to challenge the control software more than test cases generated using current methods. Two different search-based testing methods, genetic algorithms and surrogate-based optimization, were used to generate test cases for a simulated unmanned aerial vehicle attempting to fly through an entryway. The effectiveness of the search-based methods in generating challenging test cases was compared to both a truth reference (full combinatorial testing) and the method most commonly used today (Monte Carlo testing). The search-based testing techniques demonstrated better performance than Monte Carlo testing for both of the test case generation performance metrics: (1) finding the single most challenging test case and (2) finding the set of fifty test cases with the highest mean degree of challenge.", "num_citations": "10\n", "authors": ["1402"]}
{"title": "The value of modeling and simulation standards\n", "abstract": " In the current economic climate, there is a requirement to justify all government spending by demonstrating the added-value that the expenditure gives. Modeling and Simulation (M&S) community is not exempted from this rule. This paper focuses on attempts to value standards and demonstrates that even though a myriad of different standards exist in the United States today, no one has\" cracked the nut\" on determining their value. This does not mean that standards are without value. The paper highlights their importance to our society and human development as a whole. Thus if we cannot give a value to M&S standards, we can at least minimize their cost. The paper concludes with some discussions on cost-savings in the development of standards though a study of organizational misbehavior.", "num_citations": "10\n", "authors": ["1402"]}
{"title": "Electronic warfare and distributed interactive simulation\n", "abstract": " The experimentation and demonstration of electronic warfare (EW) capabilities in distributed interactive simulation (DIS) was performed through the development of these capabilities in the Institute for Simulation and Training\u2019s (1ST) Computer Generated Forces (CGF) Testbed. The 1ST CGF Testbed with EW capabilities can create land, sea, and air entities that can generate and receive electromagnetic emissions across a DIS network.    Previous efforts in behavior definition for CGF systems, including the 1ST CGF Testbed, have emphasized land forces performing visual contact engagements. Adding EW capabilities to the CGF Testbed has expanded the sensing horizon of entities beyond visual range providing long range contacts for air and sea engagements. This work provided useful testing of the DIS Standards in the area of electronic warfare.", "num_citations": "10\n", "authors": ["1402"]}
{"title": "Composition of cyberattack models\n", "abstract": " Cyberattacks are threats that every system developer and administrators need to be familiar with. The use of Petri Nets to model cyberattacks has been shown to provide additional knowledge on the planning stages of defense systems. There have been several proposed extensions to a Petri Net model to accommodate the simulation of these attacks. Petri Nets with Players, Strategies, and Cost was recently proposed to model individual cyberattacks on systems. This study introduces the formalism required to compose individual Petri Nets with Players, Strategies, and Cost models from a single system attack to a full system, which may include different methods of attacks being attempted, modeling a more realistic situation. The model composition described includes the sequential and parallel possibilities of multiple attacks. An example of a possible attack scenario is described in order to demonstrate the practical application of the current results.", "num_citations": "9\n", "authors": ["1402"]}
{"title": "Advanced topics in calculating and using confidence intervals for model validation\n", "abstract": " A confidence interval is an interval estimate of a parameter of a population, such as a mean, calculated from a sample drawn from the population. In addition to its endpoints, a confidence interval has an associated confidence level, which is a statistically justified degree of confidence that the interval actually contains the population parameter. Confidence intervals are often used in model validation. The model to be validated is executed multiple times; those executions compose a sample from the population of all possible executions of the model. A confidence interval is calculated from the results of the model executions as an estimate of the model\u2019s response variable that would be found if all possible model executions had been run. If the known or observed value for the simuland corresponding to the response variable is within the confidence interval, or within some acceptable tolerance of its endpoints, the model is considered to be valid for the variable in question.This paper is a continuation of a Fall 2012 Simulation Interoperability Workshop paper; that earlier paper was an introductory tutorial and survey on the calculation and use of confidence intervals for model validation. This paper covers three advanced topics in the same area. The first is a useful quantification of the notion of \u201cclose enough\u201d with respect to confidence interval inclusion. The second is a confidence interval adjustment applicable when multiple potentially non-independent model response variables are being validated. The third is the calculation of confidence intervals for the difference of two means. For all three of these topics, the explanations are motivated and\u00a0\u2026", "num_citations": "9\n", "authors": ["1402"]}
{"title": "Improving air combat maneuvering skills through self-study and simulation-based practice\n", "abstract": " Background. Although instructor participation is generally thought to improve the effectiveness of simulation-based training, trainees may have time and opportunity to practice skills in simulation when an instructor is not available.Aim. The question of whether complex psychomotor skills, such as air combat maneuvering, can be acquired and improved entirely without instructor assistance through self-study and simulation-based practice is investigated.Method. The results of an extended sequence of simulated air combat missions flown by a single experimental subject are reported and analyzed. Over a period of eight calendar years the subject flew 2,950 simulated air combat missions organized into 138 campaigns using seven different aircraft types. Throughout the period the subject studied air combat maneuvering in a self-guided, self-study mode and consciously used maneuvers and tactics learned during that\u00a0\u2026", "num_citations": "8\n", "authors": ["1402"]}
{"title": "Organizational simulation for model based systems engineering\n", "abstract": " Organizations creating complex systems often have hierarchical team networks; this structure affects system performance because some teams have greater influence over dimensions of parts produced by other teams. Dimensional interdependencies among components produce ripple-effects; examples include load paths and thermal flows. Simulating such phenomena requires finite element and computational fluid dynamics models. Characteristics, like weight, cost, and reliability can be calculated for parts and summed to accumulate or roll-up the values at subsystem and system levels. This paper describes multi-agent models and simulations of an organization developing a complex system. One model determined that NetLogo can generate a hierarchical model of thousands of highly interconnected teams. A second model accumulates weights and reliabilities of components with some accretion at each level\u00a0\u2026", "num_citations": "8\n", "authors": ["1402"]}
{"title": "Computational geometry techniques for terrain reasoning and data distribution problems in distributed battlefield simulation\n", "abstract": " In a distributed battlefield simulation, computer generated forces (CGF) systems generate battlefield entities that are primarily controlled by software rather than humans. To produce tactically realistic behavior CGF systems must algorithmically reason about the battlefield terrain. Data distribution is the process of maintaining network connectivity to deliver simulation data among the CGF and other systems in the distributed simulation. We show that practical problems of distributed battlefield simulation can be effectively addressed using computational geometry and geometric terrain reasoning.", "num_citations": "8\n", "authors": ["1402"]}
{"title": "Tactical simulation in an object-oriented animated graphics environment\n", "abstract": " Action Graphics (AG) combines the ideas of animated graphics, free-form spreadsheets and object-oriented programming to produce a problem solving and simulation environment.", "num_citations": "8\n", "authors": ["1402"]}
{"title": "Applying reinforcement learning to plan manufacturing material handling Part 1: Background and formal problem specification\n", "abstract": " Applying machine learning to improve the efficiency of complex manufacturing processes, particularly logistics and material handling, can be a challenging problem. The interconnectedness of the multiple components that compose such processes and the typically large number of variables required to specify procedures and plans within those processes combine to make it very difficult to map the details of real-world manufacturing processes to an abstract mathematical representation suitable for machine learning methods. In this paper, we report on the application of machine learning methods, in particular reinforcement learning, to generate increasingly efficient plans for material handling to satisfy temporally varying product demands in a representative manufacturing facility. The essential steps in the research included defining a formal representation of a realistically complex material handling plan, defining a\u00a0\u2026", "num_citations": "7\n", "authors": ["1402"]}
{"title": "Component-based implementation of cyberattack simulation models\n", "abstract": " One of the fundamental concepts of software engineering today is reusability. The capability to reuse selected components and assemble them in such way to satisfy specific user requirements is a challenge that is also faced by the simulation and modeling community. Although the primary interest is in the ability to combine and recombine components, there is also a problem on locating and selecting the components that will best fit the requirements. The component selection process has been proven to be NP-complete. This research focuses on the identification of components specific to Petri Nets with Players, Strategies, and Cost (PNPSC) which are used to model cyberattack patterns. PNPSC models are briefly described and a process of determining their basic components is presented. Such initial decomposition requires the previous knowledge of the intended granularity, level of functionality of the\u00a0\u2026", "num_citations": "7\n", "authors": ["1402"]}
{"title": "A principles-based model of ethical considerations in military decision making\n", "abstract": " When comparing alternative courses of action, modern military decision makers often must consider both the military effectiveness and the ethical consequences of the available alternatives. The basis, design, calibration, and performance of a principles-based computational model of ethical considerations in military decision making are reported in this article. The relative ethical violation (REV) model comparatively evaluates alternative military actions based upon the degree to which they violate contextually relevant ethical principles. It is based on a set of specific ethical principles deemed by philosophers and ethicists to be relevant to military courses of action. A survey of expert and non-expert human decision makers regarding the relative ethical violation of alternative actions for a set of specially designed calibration scenarios was conducted to collect data that was used to calibrate the REV model. Perhaps\u00a0\u2026", "num_citations": "7\n", "authors": ["1402"]}
{"title": "Expanded Analysis of the Correlation of Characterizing Attributes and Success in Military M&S Standards\n", "abstract": " A crucial enabling factor in the success of defense-related M&S has been the development and use of standards. Interoperability standards (eg, DIS, TENA, and HLA), natural environment standards (eg, SEDRIS), simulation development process standards (eg, FEDEP), and many others have all made contributions to enhancing the interoperability, reusability, and capability of defense-related M&S systems. The technical capabilities of these standards are important predictors of their success. However, the governance structures and processes and other nontechnical characteristics of the various standards have also affected their acceptance and utilization and ultimately their success and impact. An initial study was conducted to identify possible correlations between the elements of a set of characterizing attributes of military M&S standards and the success of those standards. A total of 22 standards in 9 categories were studied and 10 characterizing attributes of those standards were identified and evaluated. The standards' success was assessed by a group of standards experts using a Likert-type scale. Technical specificity and governance formality were found to correlate with standards success. Range of opinion among the experts was not found to correlate with standards success.", "num_citations": "7\n", "authors": ["1402"]}
{"title": "Semantic composability and XMSF\n", "abstract": " Composability is the capability to select and assemble simulation components in various combinations into simulation systems. The defining characteristic of composability is the ability to combine and recombine components. There are both syntactic and semantic forms of composability; they deal respectively with technical aspects of enabling components to work together and with whether their combined computation is meaningful. Composability is a central requirement for XMSF. Interoperability is necessary but not sufficient for composability. The envisioned XMSF infrastructure is oriented towards interoperability and syntactic composability and does not directly address semantic composability. Guaranteeing or enforcing semantic composability may not be within the scope of XMSF. However, certain features of XMSF, together with a formal theory of composability, could support semantic composability.", "num_citations": "7\n", "authors": ["1402"]}
{"title": "Petri nets with players, strategies, and cost: A formalism for modeling cyberattacks\n", "abstract": " The constant threat of computer attacks to organizations and individuals has created the need for new areas of research in the study of security mechanisms. One of these areas is the simulation and cost analysis of an attack, which can help an organization to prepare and react to any offensive action. This study focus on modeling cyber attacks with Petri nets and adding to these models the interaction between attackers and defenders, based on success or failures of steps taken during the attack. A cost analysis of such actions is also considered in order to support a decision process. This study presents the new model, known as Petri Nets with Players, Strategies, and Cost by introducing its formal definitions, a walk-through example of a simulation and techniques used to interpret the action of the two sides involved in the attack.", "num_citations": "6\n", "authors": ["1402"]}
{"title": "A multi-resolution HEALPix data structure for spherically mapped point data\n", "abstract": " Data describing entities with locations that are points on a sphere are described as spherically mapped. Several data structures designed for spherically mapped data have been developed. One of them, known as Hierarchical Equal Area iso-Latitude Pixelization (HEALPix), partitions the sphere into twelve diamond-shaped equal-area base cells and then recursively subdivides each cell into four diamond-shaped subcells, continuing to the desired level of resolution. Twelve quadtrees, one associated with each base cell, store the data records associated with that cell and its subcells.HEALPix has been used successfully for numerous applications, notably including cosmic microwave background data analysis. However, for applications involving sparse point data HEALPix has possible drawbacks, including inefficient memory utilization, overwriting of proximate points, and return of spurious points for certain\u00a0\u2026", "num_citations": "6\n", "authors": ["1402"]}
{"title": "A reuse lexicon: terms, units, and modes in M&S asset reuse\n", "abstract": " Using a previously developed asset again, either for the purpose for which it was originally developed or for a new purpose or in a new context. Reuse may save time, effort, or cost for development or testing. Reuse may add credibility to the new application if the asset underwent verification, validation, and accreditation for its previous use.", "num_citations": "6\n", "authors": ["1402"]}
{"title": "Benefits and Consequences of Automated Learning in Computer Generated Forces Systems\n", "abstract": " Should computer generated forces (CGF) systems include automated learning capabilities? The CGF research literature contains many statements by CGF experts that the ability to learn will be generally valuable, even necessary, in future CGF systems. A variety of significant benefits for CGF systems and military simulation in general are claimed to follow from automated learning. However, upon closer examination, it seems to be not so obvious that learning by CGF systems would necessarily be beneficial for many uses of CGF systems. This paper takes a respectfully skeptical position regarding CGF learning and provides arguments that CGF learning could compromise and confound the utility of CGF systems for the most common CGF applications.This paper begins by defining CGF systems and grouping CGF simulation applications into three broad types. Calls in the CGF research literature for automated\u00a0\u2026", "num_citations": "6\n", "authors": ["1402"]}
{"title": "Data distribution management specifications 1.3 and 1516 are equivalently powerful\n", "abstract": " The High Level Architecture (HLA) is an architecture for constructing federations of distributed simulations, or federates, that exchange simulation data at run-time. HLA\u2019s Data Distribution Management (DDM) services reduce the amount of simulation data transported in a federation and delivered to an HLA federate by allowing data communications connections to be based on federates\u2019 run-time expressions of data production and requirements.The definition of HLA in general, and DDM in particular, is different from the previous Department of Defense 1.3 version to the new Institute for Electrical and Electronic Engineers 1516 version of the HLA specifications. The power of DDM to represent federates\u2019 data production and requirements under the old specification (DDM 1.3) and new specification (DDM 1516) have been compared using formal methods. It was previously reported that DDM 1516 is at least as\u00a0\u2026", "num_citations": "6\n", "authors": ["1402"]}
{"title": "Reinforcement Learning in an Environment Synthetically Augmented with Digital Pheromones.\n", "abstract": " Reinforcement learning requires information about states, actions, and outcomes as the basis for learning. For many applications, it can be difficult to construct a representative model of the environment, either due to lack of required information or because of that the model\u2019s state space may become too large to allow a solution in a reasonable amount of time, using the experience of prior actions. An environment consisting solely of the occurrence or nonoccurrence of specific events attributable to a human actor may appear to lack the necessary structure for the positioning of responding agents in time and space using reinforcement learning. Digital pheromones can be used to synthetically augment such an environment with event sequence information to create a more persistent and measurable imprint on the environment that supports reinforcement learning. We implemented this method and combined it with the ability of agents to learn from actions not taken, a concept known as fictive learning. This approach was tested against the historical sequence of Somali maritime pirate attacks from 2005 to mid-2012, enabling a set of autonomous agents representing naval vessels to successfully respond to an average of 333 of the 899 pirate attacks, outperforming the historical record of 139 successes.", "num_citations": "5\n", "authors": ["1402"]}
{"title": "Towards a methodological approach to identify future M&S standard needs\n", "abstract": " Although Modeling and Simulation is successfully applied for several decades, the community only established a handful of M&S specific standards. Although the standards were applied enabling worldwide distributed simulation applications, in particular in the training application domain of military simulation systems, the general success of M&S standard efforts and their potential for general applicability has been debated repeatedly during several conferences and workshop. This collection of position statements discusses related questions, such as, \u201cWhat makes M&S special that we need M&S standards,\u201d \u201cAre M&S standards truly different from Software Engineering Standards,\u201d and \u201cWhat metrics can be used to measure M&S standard success,\u201d and tries to contribute to establishing a methodological approach to identify future M&S standard needs. These position statements have been contributed in\u00a0\u2026", "num_citations": "5\n", "authors": ["1402"]}
{"title": "An experimental application of a trait-based personality model to the simulation of military decision-making\n", "abstract": " Jean CATANZARO Department of Psychology Old Dominion University Norfolk VA 23508 Email jcatanza@ odu. edu", "num_citations": "5\n", "authors": ["1402"]}
{"title": "Preliminary investigations into efficient line of sight determination in polygonal terrain\n", "abstract": " Following this subsection, the remainder of section 1 introduces the Line of Sight problem in the context of real-time battlefield simulation, describes polygonal terrain, and lays out the objectives of this research. section 2 explains in detail the Line of Sight algorithms developed during this project, and the existing algorithm used as a standard for comparison. Section 3 presents the comparison experiment performed on those algorithms, and the experimental results obtained. section 4 summarizes the results and identifies potential areas of future related work. Sections 5 and 6 contain references and appendices related to the project.This document assumes that the reader is familiar with computer algorithm design in general, but not with the specific algorithms or data structures used for Line of Sight determination. It further assumes that the reader has some familiarity with real-time battlefield simulation, as\u00a0\u2026", "num_citations": "5\n", "authors": ["1402"]}
{"title": "Applying Reinforcement Learning to Plan Manufacturing Material Handling Part 2: Experimentation and Results\n", "abstract": " Applying machine learning to improve the efficiency of complex manufacturing processes, particularly logistics and material handling, can be a challenging problem. The interconnectedness of the multiple components that compose such processes and the typically large number of variables required to specify procedures and plans within those processes combine to make it very difficult to map the details of real-world manufacturing processes to an abstract mathematical representation suitable for machine learning methods. In this paper, we report on the application of machine learning methods, in particular reinforcement learning, to generate increasingly efficient plans for material handling to satisfy temporally varying product demands in a representative manufacturing facility. The essential steps in the research included defining a formal representation of a realistically complex material handling plan, defining a\u00a0\u2026", "num_citations": "4\n", "authors": ["1402"]}
{"title": "An Analysis of Machine Learning Online Training Approaches for Simulation Optimization\n", "abstract": " Simulation optimization refers to an optimization problem with a stochastic and potentially computationally expensive objective function. Machine learning (surrogate modeling) techniques have significant potential for enabling efficient simulation optimization, but the resulting surrogate model must have sufficient global accuracy to locate the region of the optimum, and sufficient local accuracy to pinpoint it. Historical surrogate modeling procedures have used a priori experimental designs, while more model literature has explored the use of training utility functions that select the next experimental point based on the current surrogate model state. Analysis of the performance of two training utility functions in comparison to random experimental selection is performed in the context of an online trained surrogate model. The results indicate that for a statically sized surrogate model, both utility functions provide equivalent\u00a0\u2026", "num_citations": "4\n", "authors": ["1402"]}
{"title": "Component selection process in assembling cyberattack simulation models\n", "abstract": " Studies in cybersecurity are being conducted ranging from algorithms to the modeling and simulations of cyberattacks. With all of research and initiatives towards cyber security, the first step is being able to understand the vulnerabilities that make an infrastructure non-secure and the process that is followed by attackers to exploit those vulnerabilities. Using models to understand the steps that an attacker is required to take allows for the graphical representation of the steps taken by the attacker and leads to the ability to simulate these attacks where defense techniques may be studied. Cyberattacks may involve multiple attack techniques and be a combination of attack previously modeled. This study covers a selection process of basic components in which single models or other basic elements may be composed in a broader cyberattack model.", "num_citations": "4\n", "authors": ["1402"]}
{"title": "Physics-based modeling of crowd evacuation in the Unity game engine\n", "abstract": " Crowds of people are often found in enclosed or constricted spaces. Evacuation in such situations is usually conducted calmly, but real or perceived danger may trigger panic. In panicked crowds, the interpersonal distance crowd members normally observe is often overwhelmed by the physical pressure of crowd members pushing against each other. That pressure can both slow evacuation and lead to injury or death. Models have been developed to study crowd evacuation in a range of situations. This paper describes the implementation, testing, and validation of a crowd evacuation model using Unity, a commercial computer game engine. A realistic physics-based model of crowd movement that calculates and considers the physical pressure crowd members exert on each other was implemented in Unity. The implemented model was tested under both nonpanicked and panicked scenarios; those tests exhibited\u00a0\u2026", "num_citations": "4\n", "authors": ["1402"]}
{"title": "Model Reuse, Composition, and Adaptation\n", "abstract": " It is often the case that models and simulations of subsystems such as the components making up a vehicle are created in isolation and must later be integrated with other models to create a model of the overall system. However, the reuse of existing models and simulations can be costly and time-consuming, and can yield uncertain results. Advances are needed to enable cost-effective reuse of models and simulations and to ensure that integrated models produce reliable results. Key findings discussed in this chapter include the need for advances in the theory of reuse to provide a firm theoretical foundation for producing robust and reliable reuse practices, the need for guides documenting best practices on reuse, and the need for research advances on the social, behavioral, and cultural aspects of reuse in addition to technical issues.", "num_citations": "4\n", "authors": ["1402"]}
{"title": "Exploiting spatio-temporal patterns using partial-state reinforcement learning in a synthetically augmented environment\n", "abstract": " Responding to or anticipating a sequence of events caused by adversarial human actors, such as crimes, can be a difficult task. Reinforcement learning has not been highly utilized as a method for positioning agents to respond to such events. In our earlier work, which was applied to positioning naval vessel agents to respond to Somali maritime piracy attacks, we developed a method to synthetically augment the information in the events\u2019 environment with digital pheromones and other information augmenters, used the resulting augmenter signatures as states that agents could react to, and applied reinforcement learning to exploit regularities in the timing and location of events to position agents in spatio-temporal proximity of anticipated events. This work extends that methodology with a new learning boosting method wherein learning is improved as partial augmenter signatures are reinforced, which is not\u00a0\u2026", "num_citations": "4\n", "authors": ["1402"]}
{"title": "Educating the workforce: M&S professional education\n", "abstract": " As Modeling & Simulation (M&S) becomes increasingly important, there is a significant and growing need to educate and train M&S practitioners and researchers. The Department of Defense (DoD) has a growing need for an educated M&S workforce. This need includes users, developers, managers and executive-level personnel, which can effectively apply M&S to DoD requirements. While several universities offer academic M&S degree programs, the time and expense of earning these degrees often limit the number of people that go through these programs. Professional education is an alternative for gaining M&S skills and knowledge, and courses are offered by a range of university and commercial groups. The observations in this paper begin to outline both the need and available options for M&S professional education. This collection of position papers begins a conversation on the DoD's need for\u00a0\u2026", "num_citations": "4\n", "authors": ["1402"]}
{"title": "Model verification and validation methods\n", "abstract": " Tutorial 47, Model Verification and Validation Methods Page 1 Mikel D. Petty, Ph.D. University of Alabama in Huntsville Model Verification and Validation Methods Page 2 Model Verification and Validation Methods 2 \u00a9 2014 University of Alabama in Huntsville; \u00a9 2014 Mikel D. Petty, Ph.D. Outline \u2022 Motivation and introduction \u2022 Definitions and concepts \u2022 A survey of verification and validation methods \u25aa Informal methods \u25aa Static methods \u25aa Dynamic methods \u25aa Formal methods \u2022 Case studies \u25aa Validation using confidence intervals \u25aa Validation using a statistical hypothesis test \u25aa Comparing real and simulated missile impact data \u2022 Summary Page 3 Model Verification and Validation Methods 3 \u00a9 2014 University of Alabama in Huntsville; \u00a9 2014 Mikel D. Petty, Ph.D. Motivation and introduction Page 4 Model Verification and Validation Methods 4 \u00a9 2014 University of Alabama in Huntsville; \u00a9 2014 Mikel D. Petty, Ph.D. \u2026", "num_citations": "4\n", "authors": ["1402"]}
{"title": "Correlation of characterizing attributes and success in military M&S standards\n", "abstract": " Acrucial ENABLING FACTOR IN THE SUCCESS OF DEFENSE-RELATED M&S HAS BEEN THE DEVEL-OPMENT AND USE OF STANDARDS. INTEROPERABILITY STANDARDS (EG, DIS, TENA, AND HLA),", "num_citations": "4\n", "authors": ["1402"]}
{"title": "A Systems Engineering Perspective on the Development and Execution of Multi-Architecture LVC Environments\n", "abstract": " The Live Virtual Constructive Architecture Roadmap (LVCAR) defines a time-sequenced set of actions and activities which collectively improves LVC interoperability and lowers the technical and cost risks associated with the development of multi-architecture simulation environments. One of the key activities identified in the Roadmap is the establishment of a common systems engineering process for multi-architecture LVC applications. The stated goal of this process is to improve communication and collaboration among the disparate architecture communities that must interact and work together toward common goals during a multi-architecture development. During LVCAR Phase II (Roadmap Implementation), this product was developed as an overlay on top of the IEEE P1730 Distributed Simulation Engineering and Execution Process (DSEEP). This paper reports on the mapping of multi-architecture pertinent issues to the various activities defined in the DSEEP, and provides examples of the guidance that is provided for how users of this process may address the various issues. Finally, this paper discusses current plans for standardization of this product under SISO and the IEEE.", "num_citations": "4\n", "authors": ["1402"]}
{"title": "Crowd Modelling for Military Simulations Using Game Technology\n", "abstract": " Crowds of non-combatants play a large and increasingly recognized role in modern military operations, and often create substantial difficulties for the combatant forces involved. However, realistic models of crowds are essentially absent from current military simulations. To address this problem we are developing a crowd simulation capable of generating crowds of non-combatant civilians that exhibit a variety of realistic individual and group behaviours at differing levels of fidelity. The crowd simulation is interoperable with existing military simulations using a standard distributed simulation architecture. Commercial game technology is utilized in the crowd simulation to model both urban terrain and the physical behaviours of the human characters that make up the crowd. The objective of this paper is to present the process involved with the design and development of a simulation that integrates commercially\u00a0\u2026", "num_citations": "4\n", "authors": ["1402"]}
{"title": "Crowd Modeling in Military Simulations: Requirements Analysis, Survey, and Design Study\n", "abstract": " This report provides a comprehensive requirements analysis, literature survey, and engineering design study of crowd modeling in military simulations. This report has five main sections. An introductory section is followed by a brief overview of the research effort, with descriptions of the project motivation, history, and methodology. An analysis of requirements for crowd modeling in military simulations is detailed. Following that, a survey of the state of the art in psychological and computational models of crowd behavior is presented. Then a design study of a crowd simulation federate, which is based in part on a set of implementation experiments, is reported. Finally, a statement of findings and recommended research concludes the primary content of the report. The appendices of this report include a list of references, a list of acronyms and abbreviations, a partial list of sources for the requirements analysis process, and brief biographies of the authors.Descriptors:", "num_citations": "4\n", "authors": ["1402"]}
{"title": "Using a software architecture description language to model the architecture and run-time performance of a federate\n", "abstract": " Software architecture is high-level software design, dealing with the structure and organization of software systems. A software architecture is defined in terms of computational components and interactions among those components. Architecture description languages (ADLs) represent architecture-level software designs. Different ADLs often have different intents; e.g., Rapide supports architecture simulation and Acme is intended to be both an ADL and an ADL interchange format. Experimental applications of two ADLs were conducted to determine the effectiveness of ADLs for architecture-level analysis of simulation systems; one of them is reported. Acme was used to model the architecture of ModSAF and to analyze its run-time performance. The model was used to analyze execution time at the component and federate levels and to estimate the maximum number of internal and external simulation entities that\u00a0\u2026", "num_citations": "4\n", "authors": ["1402"]}
{"title": "Plowshares: an emergency management training simulation\n", "abstract": " The Plowshares project is applying military constructive simulation technology to training for emergency management. The U. S. Army's Janus combat simulation model was enhanced to support emergency management scenarios that include hurricanes, fires, and chemical spills. The enhanced Janus software, known as TERRA, has been used in a county Emergency Operations Center to provide the stimulus for training events structured as command post exercises. The first phase of the project culminated in a demon stration exercise using the TERRA system at the Orange County Florida Emergency Operations Center. In that demonstration Emergency Operations Center and response agency personnel responded to a hurricane and its subsequent tornadoes, fires, and rubble. This paper describes the overall Plowshares project, the hurricane, tornado, and fire models used in TERRA, and the methods for using\u00a0\u2026", "num_citations": "4\n", "authors": ["1402"]}
{"title": "Intervisibility heuristics for computer generated forces\n", "abstract": " Intervisibility between entities in a Distributed Interactive Simulation (DIS) environment is a mandatory, computationally expensive process. A Computer Generated Forces (CGF) system must determine the intervisibility status between each of its controlled entities and each of the other entities in the simulation and it must make these determinations at frequent intervals. Previous work has focused on developing algorithms to perform intervisibility determinations as quickly as possible. In this work, the problem was approached differently. Instead of speeding each intervisibility determination, heuristics were developed for reducing the number of determinations needed, thereby reducing the computational expense of intervisibility. These results are independent of terrain representation and thereby applicable to any CGF system.", "num_citations": "4\n", "authors": ["1402"]}
{"title": "Linking constructive and virtual simulation in DIS\n", "abstract": " This paper is a tutorial on the problem of integrating constructive and virtual simulations. It describes constructive and virtual simulations, discusses motivations for integrating such simulations, and addresses general problems that must be solved by any constructive and virtual integration. (Author)", "num_citations": "4\n", "authors": ["1402"]}
{"title": "Machine Learning Cyberattack Strategies with Petri Nets with Players, Strategies, and Costs\n", "abstract": " A model of a structured query language injection attack was designed using the Petri nets with players, strategies, and costs formalism. The formalism models the attacker and defender as competing players that can observe a specific subset of the net and act by changing the transition firing rates that the respective player can control. This model of the attack was based on the Common Attack Pattern and Enumeration Classification database and was validated by a panel of subject matter experts to be representative of a structured query language injection attack. The model was simulated with a reinforcement learning algorithm using an $$\\upvarepsilon $$-greedy selection method. The algorithm learned within each iteration an optimal solution by varying player-controlled transitions rates. This paper describes the validation of the model, the design of the algorithm, and the results from 4 different\u00a0\u2026", "num_citations": "3\n", "authors": ["1402"]}
{"title": "Modeling Cyberattacks with Extended Petri Nets: Research Program Overview and Status Report\n", "abstract": " A research program in cyberattack modeling consisting of four interconnected research projects is described. First, a public database of known cyberattack patterns is automatically processed to generate cyberattack models, one for each attack pattern, that can be executed to simulate cyberattacks. The models are expressed using a form of Petri nets extended with additional features specific to modeling cyberattacks. Second, cyberattack models that have been previously generated and stored in a repository are automatically selected and composed so as to form a complete model of a particular target computer system. The composition process depends on metadata associated with each model. Third, the assembled composite cyberattack model is then validated as an accurate model of the target system by comparing the model and target system using application-relevant verification and validation\u00a0\u2026", "num_citations": "3\n", "authors": ["1402"]}
{"title": "Recreating the Battle of 73 Easting in a constructive combat model\n", "abstract": " Center for Modeling, Simulation, and Analysis University of Alabama in Huntsville 301 Sparkman Drive, Shelby Center 144, Huntsville AL 35899 USA danielw@ uah. edu, pettym@ uah. edu", "num_citations": "3\n", "authors": ["1402"]}
{"title": "Topics, Structure, and Delivery of the New Certified Modeling and Simulation Professional Examination\n", "abstract": " The Certified Modeling and Simulation Professional (CMSP) examination is a professional certification examination promoted and administered by the National Training and Simulation Association (NTSA). Award of the CMSP designation is intended to recognize individuals who have attained a significant degree of knowledge and experience in modeling and simulation. The examination was first developed and offered to the community in 2001. Recognizing the need for a renewal of the examination\u2019s content, NTSA commissioned a community effort beginning in 2009 to develop a new version of the examination. That renewal has been completed. The new CMSP examination is based on a consensus-based topic index intended to cover the essential parts of the modeling and simulation body of knowledge. A set of approximately 2000 new questions was developed; the set includes questions for every topic and subtopic in the index and supports two different types of CMSP certification. Every question is explicitly traceable to a published source in the modeling and simulation literature. In parallel with the question development, a new on-line examination system was designed and implemented to allow CMSP candidates to attempt the examination conveniently and intuitively. This paper will detail the new exam's types, topics, and structure, and will introduce the new on-line delivery system for the examination.", "num_citations": "3\n", "authors": ["1402"]}
{"title": "Cognitive and behavioral psychological research for crowd modeling\n", "abstract": " Psychologically based crowd modeling is essentially absent from current computer simulations and training. A two-fold method is proposed for incorporating a cognitive psychological layer into models of crowd behavior. Naturalistic observation techniques are employed to measure human behaviors during the 1999 World Trade Organization protest, a 2004 antiwar protest, and military MOUT training exercises involving crowds. Survey research is employed to identify and describe crowd and control force interaction variables and the strength of the relation between variables and a crowd turning violent. The results of these two studies will contribute to an on-going effort to provide a psychological basis for a more realistic model of crowd-control force interactions.", "num_citations": "3\n", "authors": ["1402"]}
{"title": "Psychological Research for Crowd Modeling\n", "abstract": " Bert Useem University of New Mexico Institute for Social Research 2808 Central Ave, SE Albuquerque, NM 87106 505-277-4257 rgaskins@ odu. edu,, cboon005@ odu. edu, mpetty@ vmasc. odu. edu, useem@ unm. edu", "num_citations": "3\n", "authors": ["1402"]}
{"title": "From battlefield to emergency management\n", "abstract": " The test demonstration served to reveal Janus's flexibility and also its limitations when it was applied without code modification to emergency management simulation. In order to modify Janus for use by emergency managers, we first identified users' needs. Some of the changes made to Janus to produce TERRA included modeling a set of disaster types (hurricanes, tornados, fires), urban terrain and the effects on it from the disasters, and emergency response actions by field units. We also demilitarized the user interface in order to make it more relevant to emergency management personnel. An example of this interface can be seen in Figure 2. The resulting TERRA program is a realtime simulation of emergency events and response. Like the version of Janus used as the developmental starting point, the TERRA simulation is written in FORTRAN. It runs on Hewlett-Packard Apollo workstations under the UNIX\u00a0\u2026", "num_citations": "3\n", "authors": ["1402"]}
{"title": "Design and implementation of lunar communications satellite and server federates for the 2012 SISO Smackdown Federation\n", "abstract": " The Simulation Interoperability Standards Organization (SISO) Smackdown is a two-year old annual event held at the 2012 Spring Simulation Interoperability Workshop (SIW). A primary objective of the Smackdown event is to provide college students with hands-on experience in developing distributed simulations using High Level Architecture (HLA). The University of Alabama in Huntsville (UAHuntsville) fielded teams in 2011 and 2012. Both the 2011 and 2012 smackdown scenarios were a lunar resupply mission. The 2012 UAHuntsville fielded four federates: a communications network Federate called Lunar Communications and Navigation Satellite Service (LCANServ) for sending and receiving messages, a Lunar Satellite Constellation (LCANSat) to put in place radios needed by the communications network for Line-Of-Sight communication calculations, and 3D graphical displays of the orbiting satellites and a 3D visualization of the lunar surface activities. This paper concentrates on the first two federates by describing the functions, algorithms, the modular FOM, experiences, lessons learned and recommendations for future Smackdown events.", "num_citations": "3\n", "authors": ["1402"]}
{"title": "Model Composition and Reuse\n", "abstract": " Model composition is combining separate models into an integrated composite model. Composability is the capability to select and compose models in various combinations into simulation systems to satisfy specific user requirements. Composability has been an important research objective for some time, especially in the defense-related modeling and simulation community, but general composability remains a challenging goal. Model reuse is reusing an existing model for a new application. Reuse can reduce the time, effort, and expense of model development and testing. Because the models to be reused are usually implemented as software components, general software reuse techniques are often applicable. In addition, frameworks and standards specific to modeling and simulation have developed to support model composition and reuse. Model composition and reuse are closely related. Composability is\u00a0\u2026", "num_citations": "2\n", "authors": ["1402"]}
{"title": "Recognizing the contributions of reviewers in publishing and peer review\n", "abstract": " For authors, publishing in high-quality peer-reviewed journals is critical for gaining recognition from their peers. Frequently it is also mandatory as part of the \u2018\u2018publish or perish\u2019\u2019environment of academia and other research organizations. In addition to gaining recognition via publishing, access to peer-reviewed journals is one way researchers and practitioners can remain abreast of the latest work in their field, get inspired by exposure to new ideas, and gain detailed knowledge about the cutting-edge work of others, all of which are requirements for professional and intellectual growth. For a research discipline, having high-quality, peer-reviewed, recognizable and reputable journals is critical for fostering a sense of community and providing a central focus that allows researchers to identify as part of a group.SIMULATION is the monthly journal of the Society for Modeling and Simulation International (SCS). It focuses\u00a0\u2026", "num_citations": "2\n", "authors": ["1402"]}
{"title": "A Bayesian approach to assessing expected utility in the simulation decision\n", "abstract": " The study of the complex relationship between simulation and reality is a defining characteristic that differentiates simulation science from science that uses simulation. Significant progress has been made toward a robust understanding of accuracy in this relationship. Although broadly accepted by the community as an important consideration, how a simulation is, or is to be, used has received less rigorous attention. Decision theory provides a mathematical structure to define use that is suitable to inform decision-making using models and simulations. Posed in this way, use is essentially formulated in the style of a decision problem within the context of theoretical computer science. While decision theory is useful to frame the decision problem, quantifying error remains as a key challenge that must be resolved before a meaningful evaluation of expected value or expected utility can be calculated. In this paper we\u00a0\u2026", "num_citations": "2\n", "authors": ["1402"]}
{"title": "A Model of\" Evil\" for Course of Action Analysis\n", "abstract": " Military planners must consider the undesirable secondary effects of military operations, such as civilian casualties, physical infrastructure damage, and societal disruption. A quantitative model that can be used to evaluate and compare the intentional harm, or \"evil,\" caused by alternative courses of action (COAs) would be useful to military planners. Two versions of a \"Metric of 'Evil,'\" a model of the harm associated with military COAs intended to allow the comparison of COAs on an ethical basis, have been developed. The models consider both the results of a COA and the intentions of those executing it. The models were experimentally validated by comparing their assessments with those of human experts with backgrounds in ethics, religion, political science, and military history. Pairwise comparisons of the relative evil of pairs of COAs from a set of selected historical events were made by four sets of raters: human\u00a0\u2026", "num_citations": "2\n", "authors": ["1402"]}
{"title": "Curriculum Structure and Graduation Requirements in the UAHuntsville M&S Degree Programs\n", "abstract": " Modeling and Simulation (M&S) is increasingly important in the modern technological world. Nearly all aspects of engineering and the physical sciences already depend on M&S, and M&S is steadily finding more use in economics, politics, and social science. Concurrently, M&S is emerging as a distinct, crosscutting, interdisciplinary academic discipline, and successful M&S degree programs are already well established at the University of Central Florida (Orlando FL) and Old Dominion University (Norfolk VA).The University of Alabama System Board of Trustees gave final approval earlier this year to UAHuntsville\u2019s new graduate degree programs in M&S. Motivated by the extensive use of M&S in the North Alabama technical community, the university will offer MS and Ph. D. degrees in M&S, making it one of only three universities in the country offering M&S Ph. D. degrees. The UAHuntsville programs will provide students with broad exposure to many important modeling paradigms, simulation applications, professional practices, and supporting technologies. The new degree programs will be in full operation beginning with the Fall 2010 semester. This paper will provide details regarding the degree programs. In particular, it will explain the curriculum structure and content for both degrees and detail entrance and graduation requirements.", "num_citations": "2\n", "authors": ["1402"]}
{"title": "Observations of Crowd Behavior\n", "abstract": " Chaos and violence exploded in Seattle Washington from the 29th of November through the 3rd of December 1999 when several environmentalist, trade unionist, student and other protest groups staged demonstrations at the meeting of the World Trade Organization (WTO). The demonstrators sought to disrupt the Organization\u2019s meeting, which was attended by 134 trade ministers worldwide. Problems occurred when the planned and city\u2013supported AFL-CIO march was penetrated by members of other protest groups. The crowds became violent as these \u201csplinter groups\u201d vandalized law enforcement vehicles, ignited fires, turned over dumpsters, looted, and attacked one another1. This observational study was used to examine what types of behaviors crowds exhibited during the course of the protest and when violent behaviors were most likely to occur.", "num_citations": "2\n", "authors": ["1402"]}
{"title": "Applying reinforcement learning to plan manufacturing material handling\n", "abstract": " Applying machine learning methods to improve the efficiency of complex manufacturing processes, such as material handling, can be challenging. The interconnectedness of the multiple components that make up real-world manufacturing processes and the typically very large number of variables required to specify procedures and plans within them combine to make it very difficult to map the details of such processes to a formal mathematical representation suitable for conventional optimization methods. Instead, in this work reinforcement learning was applied to produce increasingly efficient plans for material handling in representative manufacturing facilities. Doing so included defining a formal representation of a realistically complex material handling plan, specifying a set of suitable plan change operators as reinforcement learning actions, implementing a simulation-based multi-objective reward\u00a0\u2026", "num_citations": "1\n", "authors": ["1402"]}
{"title": "Matrix Reduction Verification of Extended Petri Nets\n", "abstract": " In modeling and simulation, verification is a crucial activity to ensure that models meet specifications. The Petri net is a well-studied modeling technique that has been shown effective in modeling concurrency, distributed processes, and temporal system state. Petri nets that can be decomposed into smaller Petri nets with intermediate goals have been studied. In this paper, Petri net extensions that are automatically generated to model processes consisting of multiple, distinct phases are considered. The automatically generated nets are modeled using an extended Petri net formalism known as Petri Nets with Players, Strategies, and Costs (PNPSC). The output of each segment verification includes all paths that can be executed by a sequence of enabled transitions, as well as the markings that would result from the execution of those paths. The automatic verification is based on the matrix reduction method.", "num_citations": "1\n", "authors": ["1402"]}
{"title": "Heuristic methods for synthesizing realistic social networks based on personality compatibility\n", "abstract": " Social structures and interpersonal relationships may be represented as social networks consisting of nodes corresponding to people and links between pairs of nodes corresponding to relationships between those people. Social networks can be constructed by examining actual groups of people and identifying the relationships of interest between them. However, there are circumstances where such empirical social networks are unavailable or their use would be undesirable. Consequently, methods to generate synthetic social networks that are not identical to real-world networks but have desired structural similarities to them have been developed. A process for generating synthetic social networks based on assigning human personality types to the nodes and then adding links between nodes based on the compatibility of the nodes\u2019 personalities was developed. Two new algorithms, Probability Search and Compatibility-Degree Matching, for finding an effective assignment of personality types to the nodes were developed, implemented, and tested. The two algorithms were evaluated in terms of realism, i.e., the similarity of the generated synthetic social to exemplar real-world social networks, for 14 different real-world social networks using 20 standard quantitative network metrics. Both search algorithms produced networks that were, on average, more realistic than a standard network generation algorithm that does not use personality, the Configuration Model. The algorithms were also evaluated in terms of computational complexity.", "num_citations": "1\n", "authors": ["1402"]}
{"title": "A case study of the use of design of experiments methods to calibrate a semi-automated forces system\n", "abstract": " Semi-automated forces systems are computer software programs used to generate and model simulated entities, such as tanks and aircraft, in combat simulations. Calibration is an iterative process of executing a simulation model, comparing its results to data describing the modeled system, and adjusting the model to increase its accuracy. Semi-automated forces systems are often calibrated using retrodiction, a method which involves simulating a historical battle and comparing the simulation results to the historical battle's outcome. The Battle of 73 Easting took place during the Gulf War between US and Iraqi forces; its outcome was unexpectedly one-sided. That outcome is not well replicated by most semi-automated forces systems, which typically produce unrealistically high US losses, making calibration of a semi-automated forces systems using retrodiction of that battle problematic. To overcome that difficulty\u00a0\u2026", "num_citations": "1\n", "authors": ["1402"]}
{"title": "A Comparison of Simulation Optimization Algorithm Performance\n", "abstract": " Simulation optimization refers to an optimization problem with a stochastic and potentially computationally expensive objective function. Machine learning (surrogate modeling) techniques have significant potential for enabling efficient simulation optimization, but typically require the user to retain all input/output pairs evaluated by the objective function and can suffer from numerical stability issues if samples are tightly clustered. A modification to the traditional RBF surrogate modeling process is presented that enables robust online learning through use of a Kalman filter. The capability of this modified surrogate modeling approach is presented in comparison to a number of other simulation optimization algorithms. Results indicate that the filtered surrogate model algorithm is more efficient than other methods for optimization problems that do not have complex topology.", "num_citations": "1\n", "authors": ["1402"]}
{"title": "A Technique to Enable Online Machine Learning Applications for Simulation Optimization\n", "abstract": " Simulation optimization refers to an optimization problem with a stochastic and potentially computationally expensive objective function. Machine learning (surrogate modeling) techniques have significant potential for enabling efficient simulation optimization, but typically require the user to retain all input/output pairs evaluated by the objective function and can suffer from numerical stability issues if samples are tightly clustered. A modification to the traditional surrogate modeling process is presented that enables robust online learning through use of a Kalman filter. The results of training kriging and radial basis function models are presented for both the conventional training process and the modified training process. The modification results in minimal loss in the ability of the surrogate model to represent moderately complex objective functions.", "num_citations": "1\n", "authors": ["1402"]}
{"title": "Synthesizing Social Networks with Iterated Prisoners' Dilemma\n", "abstract": " The success and survival of future long duration space exploration missions will depend on their crews working together effectively in dangerous environments. Social network analysis is the study of social structures and interpersonal relationships. It can be applied to team selection by simulating how personality compatibility affects the formation of links in social networks. Social scientists apply the Iterated Prisoners' Dilemma (IPD) to study cooperation. Several IPD strategies have been defined; they differ in how likely they are to cooperate and how tolerant they are of non-cooperation. Fifteen IPD strategies were treated as personality types. A round-robin tournament in which all pairs of strategies interacted in IPD sequences was conducted. The tournament payoffs were used to calculate personality compatibilities. The compatibilities were used to generate synthetic social networks similar in structure to realworld\u00a0\u2026", "num_citations": "1\n", "authors": ["1402"]}
{"title": "The Certified Modeling and Simulation Professional Certification and Examination\n", "abstract": " The Certified Modeling and Simulation Professional (CMSP) certification is a professional certification organized and administered by the National Training and Simulation Association (NTSA). The CMSP certification identifies individuals who have attained a significant degree of knowledge and experience in modeling and simulation (M&S). To earn the CMSP designation, a candidate must show evidence of education and experience in M&S, provide references supporting his or her certification, sign a statement of ethics, and demonstrate substantial expertise in M&S by passing a comprehensive certification examination. The current CMSP examination\u2019s scope is defined by a consensus-based M&S \u201cbody of knowledge\u201d topic index intended to cover the essential knowledge of the M&S professional discipline. The examination is based on a \u201cquestion bank\u201d of more than 2000 questions drawn from every topic and subtopic in the body of knowledge. Every question is explicitly traceable to a published, publicly available, and peer-reviewed source in the M&S literature. Each candidate is challenged with an automatically generated examination instance consisting of questions selected from the question bank that is customized to the type of CMSP certification (user/manager or developer/technical) that the candidate is seeking. An online examination system allows candidates to attempt the examination conveniently and intuitively.", "num_citations": "1\n", "authors": ["1402"]}
{"title": "Using a mock simulation event and Monte Carlo simulation to compare alternative network architectures for distributed training simulation\n", "abstract": " A systems engineering effort produced three alternative network architectures for a system to support simulation-based training events. A mock simulation event was used to compare the effort required for each alternative to prepare for, conduct, and analyze an event. The mock event was a manual execution of a process for conducting simulation events consisting of a sequence of defined activities. As a mock event, no simulations were executed and the activities\u2019 work was not performed. Instead, the labor required for each activity for each alternative was estimated by simulation subject matter experts using the Delphi method. The experts\u2019 estimates drove a set of Monte Carlo trials. Probability distributions were parameterized with the estimates, from which values for the labor required for each activity for each alternative were stochastically generated. Those values were summed to estimate the mean total labor\u00a0\u2026", "num_citations": "1\n", "authors": ["1402"]}
{"title": "implementing a physics-based model of crowd movement using the Unreal development Kit\n", "abstract": " Crowds of people form in both civilian and military contexts, often in emergency or dangerous situations. Consequently, modelling crowd behaviour and movement has consistently been a subject of research, and such models have been used to study, for example, evacuation scenarios and the effect of crowds on military operations. Recreating realistic crowd behaviour and crowd movement within game engines is of growing interest for both gaming and non-gaming applications. A physically realistic mathematical model of crowd movement that considers the physical pressures the crowd members exert on each other was implemented using computer games technology, specifically the Unreal Engine and the Unreal Development Kit. The model and its implementation were used to simulate two typical evacuation scenarios, orderly and panicked, and the simulations exhibited specific known characteristics of such\u00a0\u2026", "num_citations": "1\n", "authors": ["1402"]}
{"title": "Enhancing a commercial game engine to support research on route realism for synthetic human characters\n", "abstract": " Generating routes for entities in virtual environments, such as simulated vehicles or synthetic human characters, is a long-standing problem, and route  planning algorithms have been developed and studied for some time. Existing route planning algorithms, including the widely used A* algorithm, are generally intended to achieve optimality in some metric, such as minimum length or minimum time. Comparatively little  attention has been given to route realism, defined as the similarity of the algorithm-generated route to the route followed by real humans in the same terrain  with the same constraints and goals. Commercial game engines have seen increasing use as a context for research. To study route realism in a game engine,  two developments were needed: a quantitative metric for measuring route realism and a game engine able to capture route data needed to compute the realism metric. Enhancements for\u00a0\u2026", "num_citations": "1\n", "authors": ["1402"]}
{"title": "A Terrain Reasoning Algorithm for Defending a Fire Zone\n", "abstract": " Distributed simulation is an approach to building large-scale simulation models from a set of independent simulator nodes communicating via a network. The US Army uses distributed simulation systems for both training and analysis. Those systems include both crewed simulators and computer generated forces (CGF) systems; the latter use software, rather than human crews, to generate the behavior of entities in the simulated battlefield.CGF systems must include algorithms for all of the tactical behaviors that are needed for the simulation. One such tactical behavior is \u201cFire Zone Defense\u201d. An algorithm for this behavior must select defensive deployment locations on the terrain for the individual entities (eg, tanks) of a unit (eg, a company) to effectively defend an assigned engagement area. The entities of the unit then move to those locations.", "num_citations": "1\n", "authors": ["1402"]}
{"title": "Linking constructive and virtual simulation in distributed interactive simulation (DIS)\n", "abstract": " Existing Distributed Interactive Simulation (DIS) systems (which are a class of virtual simulations) are limited by computational power and network bandwidth in the number of vehicle platforms that can take part in a single battlefield simulation. One way to improve this limit is to integrate an aggregate constructive wargame into the DIS simulation; the constructive wargame supplies the context for a large-scale battle without adding significant computational load to the network. A critical idea in such an integration is that the events in the constructive simulation influence the events in the virtual battlefield and vice versa; this maintains the unity of the entire battlefield simulation.   This paper is a tutorial on the problem of integrating constructive and virtual simulations. It describes constructive and virtual simulations, discusses motivations for integrating such simulations, and addresses general problems that must be\u00a0\u2026", "num_citations": "1\n", "authors": ["1402"]}
{"title": "Graph Isomorphism Algorithms: Investigation Of The Graph Isomorphism Problem\n", "abstract": " As a precursor to possible applications of graph isomorphism to simulation, IST examined the current status of the computational complexity of graph isomorphism.", "num_citations": "1\n", "authors": ["1402"]}
{"title": "Three Dimensional Computer Graphics Federates for the 2012 SISO Smackdown Federation\n", "abstract": " The Simulation Interoperability Standards Organization (SISO) Smackdown is a two-year old annual event held at the 2012 Spring Simulation Interoperability Workshop (SIW). A primary objective of the Smackdown event is to provide college students with hands-on experience in developing distributed simulations using High Level Architecture (HLA). Participating for the second time, the University of Alabama in Huntsville (UAHuntsville) deployed four federates, two federates simulated a communications server and a lunar communications satellite with a radio. The other two federates generated 3D computer graphics displays for the communication satellite constellation and for the surface based lunar resupply mission. Using the Light-Weight Java Graphics Library, the satellite display federate presented a lunar-texture mapped sphere of the moon and four Telemetry Data Relay Satellites (TDRS), which received object attributes from the lunar communications satellite federate to drive their motion. The surface mission display federate was an enhanced version of the federate developed by ForwardSim, Inc. for the 2011 Smackdown simulation. Enhancements included a dead-reckoning algorithm and a visual indication of which communication satellite was in line of sight of Hadley Rille. This paper concentrates on these two federates by describing the functions, algorithms, HLA object attributes received from other federates, development experiences and recommendations for future, participating Smackdown teams.", "num_citations": "1\n", "authors": ["1402"]}