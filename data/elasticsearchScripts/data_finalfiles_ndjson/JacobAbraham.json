{"title": "Algorithm-based fault tolerance for matrix operations\n", "abstract": " The rapid progress in VLSI technology has reduced the cost of hardware, allowing multiple copies of low-cost processors to provide a large amount of computational capability for a small cost. In addition to achieving high performance, high reliability is also important to ensure that the results of long computations are valid. This paper proposes a novel system-level method of achieving high reliability, called algorithm-based fault tolerance. The technique encodes data at a high level, and algorithms are designed to operate on encoded data and produce encoded output data. The computation tasks within an algorithm are appropriately distributed among multiple computation units for fault tolerance. The technique is applied to matrix compomations which form the heart of many computation-intensive tasks. Algorithm-based fault tolerance schemes are proposed to detect and correct errors when matrix operations such\u00a0\u2026", "num_citations": "1526\n", "authors": ["493"]}
{"title": "FERRARI: A flexible software-based fault and error injection system\n", "abstract": " A major step toward the development of fault-tolerant computer systems is the validation of the dependability properties of these systems. Fault/error injection has been recognized as a powerful approach to validate the fault tolerance mechanisms of a system and to obtain statistics on parameters such as coverages and latencies. This paper describes the methodology and guidelines for the design of flexible software based fault and error injection and presents a tool, FERRARI, that incorporates the techniques. The techniques used to emulate transient errors and permanent faults in software are described in detail. Experimental results are presented for several error detection techniques, and they demonstrate the effectiveness of the software-based error injection tool in evaluating the dependability properties of complex systems.< >", "num_citations": "542\n", "authors": ["493"]}
{"title": "An improved algorithm for network reliability\n", "abstract": " Boolean algebra has been used to find the probability of communication between a pair of nodes in a network by starting with a Boolean product corresponding to simple paths between the pair of nodes and making them disjoint (mutually exclusive). A theorem is given, the use of which enables the disjoint products to be found much faster than by existing methods. An algorithm and results of its implementation on a computer are given. Comparisons with existing methods show the usefulness of the algorithm for large networks.", "num_citations": "530\n", "authors": ["493"]}
{"title": "Test generation for microprocessors\n", "abstract": " The goal of this paper is to develop test generation procedures for testing microprocessors in a user environment. Classical fault detection methods based on the gate and flip-flop level or on the state diagram level description of microprocessors are not suitable for test generation. The problem is further compounded by the availability of a large variety of microprocessors which differ widely in their organization, instruction repertoire, addressing modes, data storage, and manipulation facilities, etc. In this paper, a general graph-theoretic model is developed at the register transfer level. Any microprocessor can be easily modeled using information only about its instruction set and the functions performed. This information is readily available in the user's manual. A fault model is developed on a functional level quite independent of the implementation details. The effects of faults in the fault model are investigated at the\u00a0\u2026", "num_citations": "522\n", "authors": ["493"]}
{"title": "FERRARI: A Tool for The Validation of System Dependability Properties.\n", "abstract": " A major step toward the development of faulttolerant computer systems is the validation of the dependability properties of these systems. Fault/error injection has been recognized as a powerful approach to validate the fault tolerance mechanisms of a system and to obtain statistics on parameters such as coverages and latencies. This paper presents FER-", "num_citations": "344\n", "authors": ["493"]}
{"title": "Design and evaluation of system-level checks for on-line control flow error detection\n", "abstract": " This paper evaluates the concurrent error detection capabilities of system-level checks, using fault and error injection. The checks comprise application and system level mechanisms to detect control flow errors. We propose Enhanced Control-Flow Checking Using Assertions (ECCA). In ECCA, branch-free intervals (BFI) in a given high or intermediate level program are identified and the entry and exit points of the intervals are determined. BFls are then grouped into blocks, the size of which is determined through a performance/overhead analysis. The blocks are then fortified with preinserted assertions. For the high level ECCA, we describe an implementation of ECCA through a preprocessor that will automatically insert the necessary assertions into the program. Then, we describe the intermediate implementation possible through modifications made on gee to make it ECCA capable. The fault detection capabilities\u00a0\u2026", "num_citations": "343\n", "authors": ["493"]}
{"title": "Load balancing in distributed systems\n", "abstract": " In a distributed computing system made up of different types of processors each processor in the system may have different performance and reliability characteristics. In order to take advantage of this diversity of processing power, a modular distributed program should have its modules assigned in such a way that the applicable system performance index, such as execution time or cost, is optimized. This paper describes an algorithm for making an optimal module to processor assignment for a given performance criteria. We first propose a computational model to characterize distributed programs, consisting of tasks and an operational precedence relationship. This model alows us to describe probabilistic branching as well as concurrent execution in a distributed program. The computational model along with a set of seven program descriptors completely specifies a model for dynamic execution of a program on a\u00a0\u2026", "num_citations": "335\n", "authors": ["493"]}
{"title": "Fault-tolerant matrix arithmetic and signal processing on highly concurrent computing structures\n", "abstract": " Hardware for executing matrix arithmetic and signal processing algorithms at high speeds is in great demand in many real-time and scientific applications. With the advent of VLSI technology, large numbers of processing elements which cooperate with each other at high speed have become economically feasible. Since any functional error in a high-performance system may seriously jeopardize the operation of the system and its data integrity, some level of fault tolerance must be incorporated in order to ensure that the results of long computations are valid. Since the major computational requirements for many important real-time signal processing tasks can be reduced to a common set of basic matrix operations, the development of a unified fault-tolerant scheme for matrix operations can solve the problems of both reliable signal processing and reliable matrix operations. Earlier work proposed a low-cost\u00a0\u2026", "num_citations": "328\n", "authors": ["493"]}
{"title": "Fault-tolerant FFT networks\n", "abstract": " Two concurrent error detection (CED) schemes are proposed for N-point fast Fourier transform (FFT) networks that consists of log/sub 2/N stages with N/2 two-point butterfly modules for each stage. The method assumes that failures are confined to a single complex multiplier or adder or to one input or output set of lines. Such a fault model covers a broad class of faults. It is shown that only a small overhead ratio, O(2/log/sub 2/N) of hardware, is required for the networks to obtain fault-secure results in the first scheme. A novel data retry technique is used to locate the faulty modules. Large roundoff errors can be detected and treated in the same manner as functional errors. The retry technique can also distinguish between the roundoff errors and functional errors that are caused by some physical failures. In the second scheme, a time-redundancy method is used to achieve both error detection and location. It is sown\u00a0\u2026", "num_citations": "318\n", "authors": ["493"]}
{"title": "Functional testing of microprocessors\n", "abstract": " This paper presents a new and systematic method to generate tests for microprocessors. A functional level model for the microprocessor is used and it is represented by a reduced graph. A new and comprehensive model of the instruction execution process is developed. Various types of faults are analyzed and it is shown that with the use of appropriate codewords all faults can be classified into three types. This gives rise to a systematic procedure to generate tests which is independent of the microprocessor implementation details. Tests are given to detect faults in any microprocessor, first for the READ register instructions, and then for the remaining instructions. These tests can be executed by the microprocessor in a self-test mode, thus dispensing with the need for an external tester.", "num_citations": "262\n", "authors": ["493"]}
{"title": "Efficient algorithms for testing semiconductor random-access memories\n", "abstract": " A fault model which views faults in semiconductor random-access memories at a functional level instead of at a basic gate level is presented. An efficient 0 (n) algorithm to detect all faults in the fault model is described. The fault model is then extended to incorporate more complex faults. An 0 (n? log2 n) algorithm is presented for one such extended fault model.", "num_citations": "261\n", "authors": ["493"]}
{"title": "Quantitative evaluation of soft error injection techniques for robust system design\n", "abstract": " Choosing the correct error injection technique is of primary importance in simulation-based design and evaluation of robust systems that are resilient to soft errors. Many low-level (eg, flip-flop-level) error injection techniques are generally used for small systems due to long execution times and significant memory requirements. High-level error injections at the architecture or memory levels are generally fast but can be inaccurate. Unfortunately, there exists very little research literature on quantitative analysis of the inaccuracies associated with high-level error injection techniques. In this paper, we use simulation and emulation results to understand the accuracy trade-offs associated with a variety of high-level error injection techniques. A detailed analysis of error propagation explains the causes of high degrees of inaccuracies associated with error injection techniques at higher levels of abstraction.", "num_citations": "241\n", "authors": ["493"]}
{"title": "Native mode functional test generation for processors with applications to self test and design validation\n", "abstract": " New methodologies based on functional testing and built-in self-test can narrow the gap between necessary solutions and existing techniques for processor validation and testing. We present a versatile automatic functional test generation methodology for microprocessors. The generated assembly instruction sequences can be applied to both design validation and manufacturing test, especially in high speed \"native\" mode. All the functional capabilities of complex processors can be exercised, leading to high quality validation sequences and manufacturing tests with high fault coverage. The tests can also be applied in a built-in self-test fashion. Experimental results on two microprocessors show that this method is very effective in generating high quality manufacturing tests as well as in functional design validation.", "num_citations": "233\n", "authors": ["493"]}
{"title": "Fault and error models for VLSI\n", "abstract": " This paper describes a variety of fault and error models which are used as the basis for designing fault-tolerant Very Large Scale Integrated (VLSI) systems. The fault models describe physical defects and failures and the input patterns which will expose them, and are suitable for testing, while error models describe the effects on the functional outputs of defects and are useful for on-line error detection. The models are described at various levels of abstraction. The differences between fault and error models for identical functional modules are also illustrated.", "num_citations": "208\n", "authors": ["493"]}
{"title": "CRIS: A test cultivation program for sequential VLSI circuits\n", "abstract": " This paper discusses a novel approach to cultivating a test for combinational and sequential VLSI circuits described hierarchically at the transistor, gate, and higher levels. The approach is based on continuous mutation of a given input sequence and on analyzing the mutated vectors for selecting the test set. The approach uses hierarchical simulation technique in the analysis to drastically reduce the memory requirement, thus allowing the test generation for large VLSI circuits. The algorithms are at the switch level so that general MOS digital designs can be handled, and both stuck-at and transistor faults are handle accurately. The approach has been implemented in a hierarchical test generation system, CRIS, that runs under UNIX on SPARC workstations. CRIS has been used successfully to generate tests with high fault coverage for large combinational and sequential circuits.", "num_citations": "207\n", "authors": ["493"]}
{"title": "Real-number codes for fault-tolerant matrix operations on processor arrays\n", "abstract": " A generalization of existing real numer codes is proposed. It is proven that linearity is a necessary and sufficient condition for codes used for fault-tolerant matrix operations such as matrix addition, multiplication, transposition, and LU decomposition. It is also proven that for every linear code defined over a finite field, there exists a corresponding linear real-number code with similar error detecting capabilities. Encoding schemes are given for some of the example codes which fall under the general set of real-number codes. With the help of experiments, a rule is derived for the selection of a particular code for a given application. The performance overhead of fault tolerance schemes using the generalized encoding schemes is shown to be very low, and this is substantiated through simulation experiments.< >", "num_citations": "199\n", "authors": ["493"]}
{"title": "Algorithm-based fault tolerance on a hypercube multiprocessor\n", "abstract": " The design of fault-tolerant hypercube multiprocessor architecture is discussed. The authors propose the detection and location of faulty processors concurrently with the actual execution of parallel applications on the hypercube using a novel scheme of algorithm-based error detection. System-level error detection mechanisms have been implemented for three parallel applications on a 16-processor Intel iPSC hypercube multiprocessor: matrix multiplication, Gaussian elimination, and fast Fourier transform. Schemes for other applications are under development. Extensive studies have been done of error coverage of the system-level error detection schemes in the presence of finite-precision arithmetic, which affects the system-level encodings. Two reconfiguration schemes are proposed that allow the authors to isolate and replace faulty processors with spare processors.< >", "num_citations": "183\n", "authors": ["493"]}
{"title": "Electrically controlled water permeation through graphene oxide membranes\n", "abstract": " Controlled transport of water molecules through membranes and capillaries is important in areas as diverse as water purification and healthcare technologies 1, 2, 3, 4, 5, 6, 7. Previous attempts to control water permeation through membranes (mainly polymeric ones) have concentrated on modulating the structure of the membrane and the physicochemical properties of its surface by varying the pH, temperature or ionic strength 3, 8. Electrical control over water transport is an attractive alternative; however, theory and simulations 9, 10, 11, 12, 13, 14 have often yielded conflicting results, from freezing of water molecules to melting of ice 14, 15, 16 under an applied electric field. Here we report electrically controlled water permeation through micrometre-thick graphene oxide membranes 17, 18, 19, 20, 21. Such membranes have previously been shown to exhibit ultrafast permeation of water 17, 22 and molecular\u00a0\u2026", "num_citations": "175\n", "authors": ["493"]}
{"title": "Abstraction techniques for validation coverage analysis and test generation\n", "abstract": " The enormous state spaces which must be searched when verifying the correctness of, or generating tests for, complex circuits precludes the use of traditional approaches. Hard-to-find abstractions are often required to simplify the circuits and make the problems tractable. This paper presents a simple and automatic method to extract the control flow of a circuit so that the resulting state space can be explored for validation coverage analysis and automatic test generation. This control flow, capturing the essential \"behavior\" of the circuit, is represented as a finite state machine called the ECFM (Extracted Control Flow Machine). Simulation is currently the primary means of verifying large circuits, but the definition of a coverage measure for simulation vectors is an open problem. We define functional coverage as the amount of control behavior covered by the test suite. We then combine formal verification techniques\u00a0\u2026", "num_citations": "166\n", "authors": ["493"]}
{"title": "Fault tolerance techniques for systolic arrays\n", "abstract": " D igital systems that are operated in applications where there is a D high cost of failure require high reliability and continuous operation. Since it is impossible to guarantee that portions of a system will never fail, such systems need to be designed to tolerate failures of the system components. The discipline of fault-tolerant computing is, therefore, one which has attracted a great deal of research interest. Researchers have attempted to derive highly effective and, at the same time, efficient techniques to tolerate failures in complex digital systems. The high computation needs of many applications can now be met through the use of highly parallel special-purpose systems that can be produced very cost effectively through the use of very large scale integration (VLSI) technology. Systolic arrays, such as the ESL systolic array'and the Carnegie Mellon Warp proces-sor, 2 are examples of such systems. This article deals\u00a0\u2026", "num_citations": "159\n", "authors": ["493"]}
{"title": "A comparison of Dadda and Wallace multiplier delays\n", "abstract": " The two well-known fast multipliers are those presented by Wallace and Dadda. Both consist of three stages. In the first stage, the partial product matrix is formed. In the second stage, this partial product matrix is reduced to a height of two. In the final stage, these two rows are combined using a carry propagating adder. In the Wallace method, the partial products are reduced as soon as possible. In contrast, Dadda's method does the minimum reduction necessary at each level to perform the reduction in the same number of levels as required by a Wallace multiplier. It is generally assumed that, for a given size, the Wallace multiplier and the Dadda multiplier exhibit similar delay. This is because each uses the same number of pseudo adder levels to perform the partial product reduction. Although the Wallace multiplier uses a slightly smaller carry propagating adder, usually this provides no significant speed advantage\u00a0\u2026", "num_citations": "157\n", "authors": ["493"]}
{"title": "Fault-based automatic test generator for linear analog circuits\n", "abstract": " Recognizing that specification testing of analog circuits involves a high cost and lacks any quantitative measure of the testing process, we adopt a fault-based technique. With the help of hierarchical fault models for parametric and catastrophic faults, and a very efficient fault simulator, our simulation-assisted technique automatically determines the test frequencies to detect AC faults in linear analog circuits. By a suitable choice of parameters in the test generator, we can either determine the best test (maximize the error between the good and the faulty responses) for every fault (resulting in a large test set), or generate the smallest test set for all the faults. Finally, fault coverage values provide a quantitative evaluation of the final test set.", "num_citations": "141\n", "authors": ["493"]}
{"title": "Automatic test pattern generation for crosstalk glitches in digital circuits\n", "abstract": " As clock speeds of current deep submicron design technologies increase over 1 GHz and metal line spacings narrow, unexpected crosstalk effects start to degrade the circuit performance significantly. It is important for the designer to test the effects before taping out the designs. Unfortunately, conventional tests for stuck-at or delay faults are not guaranteed to expose potential crosstalk effects. This paper presents an efficient methodology for generating test vectors to detect crosstalk glitch effects in digital circuits. The ATEG (Automatic Test Extractor for Glitch) algorithm uses the multiple backrace technique, and uses a \"forward-evaluation\" technique in its backtacking phase which searches for the \"right\" entry to select by propagating \"suggested values\" to minimize the number of backtracks. In the glitch propagation phase, we employ a criterion function which gives a metric for determining the propagation of a\u00a0\u2026", "num_citations": "117\n", "authors": ["493"]}
{"title": "An algorithm for the accurate reliability evaluation of triple modular redundancy networks\n", "abstract": " There are several instances where the classical method of triple-modular redundancy (TMR) reliability modeling may provide predictions which are inadequate. It is shown that for even simple networks such as those exhibiting fan-in and fan-out, classical methods may predict a reliability that is higher than or lower than the actual reliability. Furthermore, the classical method gives no hint as to whether the predicted number is high or low. As a solution to this problem, a method of partitioning an arbitrary network into cells such that faults in a cell are independent of faults in other cells is proposed. An algorithm is then given to calculate the reliability of any such cell, by considering only the structure of the interconnections within the cells. The value of the reliability found is exact if TMR is assumed to be a coherent system. An approximation to the algorithm is also described; this can be used to find a lower bound to the\u00a0\u2026", "num_citations": "116\n", "authors": ["493"]}
{"title": "A novel functional test generation method for processors using commercial ATPG\n", "abstract": " As the sizes of general and special purpose processors increase rapidly, generating high quality manufacturing tests for them is becoming a serious problem in industry. This paper describes a novel method for hierarchical functional test generation for processors which targets one embedded module at a time and uses commercial ATPG tools to derive tests for faults within the module. Applying the technique to benchmark processor designs, we were able to obtain test efficiencies for the embedded modules of the processors which were extremely close to what the commercial ATPG could do with complete access to the module. The hierarchical approach used produced this result, using the same commercial tool, but required a CPU time several orders of magnitude less than when using a conventional, flat view of the circuit.", "num_citations": "110\n", "authors": ["493"]}
{"title": "Analog testing with time response parameters\n", "abstract": " This paper describes a simple test generation technique which derives sinusoidal test waveforms that detect several fault classes. In addition, the authors show that certain stimuli will provoke variations in delay, rise time, and overshoot that indicate faulty behavior. Simple algorithms compute the different parameters.", "num_citations": "104\n", "authors": ["493"]}
{"title": "Bounds on algorithm-based fault tolerance in multiple processor systems\n", "abstract": " An important consideration in the design of high-performance multiple processor systems should be in ensuring the correctness of results computed by such complex systems which are extremely prone to transient and intermittent failures. The detection and location of faults and errors concurrently with normal system operation can be achieved through the application of appropriate on-line checks on the results of the computations. This is the domain of algorithm-based fault tolerance, which deals with low-cost system-level fault-tolerance techniques to produce reliable computations in multiple processor systems, by tailoring the fault-tolerance techniques toward specific algorithms. This paper presents a graph-theoretic model for determining upper and lower bounds on the number of checks needed for achieving concurrent fault detection and location. The objective is to estimate ate the overhead in time and the\u00a0\u2026", "num_citations": "104\n", "authors": ["493"]}
{"title": "Van der Waals pressure and its effect on trapped interlayer molecules\n", "abstract": " Van der Waals assembly of two-dimensional crystals continue attract intense interest due to the prospect of designing novel materials with on-demand properties. One of the unique features of this technology is the possibility of trapping molecules between two-dimensional crystals. The trapped molecules are predicted to experience pressures as high as 1 GPa. Here we report measurements of this interfacial pressure by capturing pressure-sensitive molecules and studying their structural and conformational changes. Pressures of 1.2\u00b10.3 GPa are found using Raman spectrometry for molecular layers of 1-nm in thickness. We further show that this pressure can induce chemical reactions, and several trapped salts are found to react with water at room temperature, leading to two-dimensional crystals of the corresponding oxides. This pressure and its effect should be taken into account in studies of van der Waals\u00a0\u2026", "num_citations": "102\n", "authors": ["493"]}
{"title": "TESTING OF SEMICONDUCTOR RANDOM ACCESS MEMORIES.\n", "abstract": " Sauf mention contraire ci-dessus, le contenu de cette notice bibliographique peut \u00eatre utilis\u00e9 dans le cadre d\u2019une licence CC BY 4.0 Inist-CNRS/Unless otherwise stated above, the content of this bibliographic record may be used under a CC BY 4.0 licence by Inist-CNRS/A menos que se haya se\u00f1alado antes, el contenido de este registro bibliogr\u00e1fico puede ser utilizado al amparo de una licencia CC BY 4.0 Inist-CNRS", "num_citations": "102\n", "authors": ["493"]}
{"title": "Property checking via structural analysis\n", "abstract": " This paper describes a structurally-guided framework for the decomposition of a verification task into subtasks, each solved by a specialized algorithm for overall efficiency. Our contributions include the following: (1) a structural algorithm for computing a bound of a state-transition diagram\u2019s diameter which, for several classes of netlists, is sufficiently small to guarantee completeness of a bounded property check; (2) a robust backward unfolding technique for structural target enlargement: from the target states, we perform a series of compose- based pre-image computations, truncating the search if resource limitations are exceeded; (3) similar to frontier simplification in symbolic reachability analysis, we use induction via don\u2019t cares for enhancing the presented target enlargement. In many practical cases, the verification problem can be discharged by the enlargement process; otherwise, it is passed in\u00a0\u2026", "num_citations": "100\n", "authors": ["493"]}
{"title": "On correlating structural tests with functional tests for speed binning of high performance design\n", "abstract": " The use of functional vectors has been an industry standard for speed binning of high-performance ICs. This practice can be prohibitively expensive as ICs become faster and more complex. In comparison, structural patterns target performance related faults in a more systematic manner. To make structural testing an effective alternative for speed binning, this paper investigates the correlation between functional test frequency and the test frequencies of various types of structural patterns on MPC7455, a Motorola microprocessor compatible to PowerPC/spl trade/ instruction set architecture.", "num_citations": "96\n", "authors": ["493"]}
{"title": "Automatic Test Knowledge Extraction from VHDL (ATKET).\n", "abstract": " Behavioral information about modules in a design is exploited by test engineers to handle the complex problem of test generation for large designs. However, automation of the process of exploiting behavioral information to facilitate test generation is in its infancy. This paper introduces ATKET, an Automatic Test linowledge Extraction Tool which automatically generates pieces of test knowledge by using structural and behavioral information in the VHDL description of a design. Results obtained from ATIiET for a circuit which is dificult to test are presented.", "num_citations": "96\n", "authors": ["493"]}
{"title": "Hierarchical fault modeling for analog and mixed-signal circuits\n", "abstract": " Presents a comprehensive approach, based on functional error characterization, for modeling faults in analog and mixed-signal circuits. A case study based on a CMOS and an nMOS operational amplifier is discussed, and a full listing of derived behavioral fault models is presented. These fault models are then mapped to the faulty behavior at the macro-circuit level.< >", "num_citations": "94\n", "authors": ["493"]}
{"title": "Test compaction for sequential circuits\n", "abstract": " The authors describe a number of heuristic algorithms to compact a set of test sequences generated by a sequential circuit automatic test pattern generator (ATPG). A model has been developed and analyzed which shows that finding the optimal solution has an exponential worst-case complexity. To achieve an acceptable run time, some heuristics have been developed that yield good suboptimal solutions in a very short time. Three heuristic algorithms were developed. These algorithms were implemented in C and lex and applied to several of the ISCAS-89 benchmark sequential circuits. They reduce the test length by 17%-63% with a very small time overhead, while having little effect on the original fault overage.< >", "num_citations": "93\n", "authors": ["493"]}
{"title": "An easily computed functional level testability measure\n", "abstract": " The authors consider the problem of estimating the testability of a digital circuit at the functional level. Using an information-theoretic approach, they have developed a functional testability measure for both controllability and observability. They introduce two techniques that can efficiently and accurately estimate the measure. In addition, some applications of the testability measure for automated design for testability, such as automatic circuit partitioning and test point insertion, are described.< >", "num_citations": "93\n", "authors": ["493"]}
{"title": "The design of PLAs with concurrent error detection\n", "abstract": " CiNii \u8ad6\u6587 - The design of PLAs with concurrent error detection CiNii \u56fd\u7acb\u60c5\u5831\u5b66\u7814\u7a76\u6240 \u5b66\u8853 \u60c5\u5831\u30ca\u30d3\u30b2\u30fc\u30bf[\u30b5\u30a4\u30cb\u30a3] \u65e5\u672c\u306e\u8ad6\u6587\u3092\u3055\u304c\u3059 \u5927\u5b66\u56f3\u66f8\u9928\u306e\u672c\u3092\u3055\u304c\u3059 \u65e5\u672c\u306e\u535a\u58eb\u8ad6\u6587\u3092\u3055\u304c\u3059 \u65b0\u898f \u767b\u9332 \u30ed\u30b0\u30a4\u30f3 English \u691c\u7d22 \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u9589\u3058\u308b \u30bf\u30a4\u30c8\u30eb \u8457\u8005\u540d \u8457\u8005ID \u8457\u8005\u6240\u5c5e \u520a\u884c\u7269\u540d ISSN \u5dfb\u53f7\u30da\u30fc\u30b8 \u51fa\u7248\u8005 \u53c2\u8003\u6587\u732e \u51fa\u7248\u5e74 \u5e74\u304b\u3089 \u5e74\u307e\u3067 \u691c\u7d22 \u691c\u7d22 \u691c\u7d22 CiNii\u7a93\u53e3\u696d\u52d9 \u306e\u518d\u958b\u306b\u3064\u3044\u3066 The design of PLAs with concurrent error detection MAK GP \u88ab\u5f15\u7528\u6587\u732e: 1\u4ef6 \u8457\u8005 MAK GP \u53ce\u9332\u520a\u884c\u7269 Proc. 12th Int. Symposium on Fault-Tolerant Computing Proc. 12th Int. Symposium on Fault-Tolerant Computing, 303-310, 1982 \u88ab\u5f15\u7528\u6587\u732e: 1\u4ef6\u4e2d 1-1\u4ef6\u3092 \u8868\u793a 1 On Multiple-Valued Separable Unordered Codes NAGATA Yasunori , MUKAIDONO Masao IEICE transactions on information and systems 79(2), 99-106, 1996-02-25 \u53c2\u8003\u6587\u732e14\u4ef6 Tweet \u5404\u7a2e\u30b3\u30fc\u30c9 NII\u8ad6\u6587ID(NAID) 10006936877 \u8cc7\u6599\u7a2e\u5225 \u4f1a\u8b70\u8cc7\u6599 \u30c7\u30fc\u30bf\u63d0\u4f9b\u5143 CJP\u5f15\u7528 /(\u2026", "num_citations": "93\n", "authors": ["493"]}
{"title": "CEDA: Control-flow error detection through assertions\n", "abstract": " This paper presents an efficient software technique, control flow error detection through assertions (CEDA), for online detection of control flow errors. Extra instructions are automatically embedded into the program at compile time to continuously update run-time signatures and to compare them against pre-assigned values. The novel method of computing run-time signatures results in a huge reduction in the performance overhead, as well as the ability to deal with complex programs and the capability to detect subtle control flow errors. The widely used C compiler, GCC, has been modified to implement CEDA, and the SPEC benchmark programs were used as the target to compare with earlier techniques. Fault injection experiments were used to evaluate the fault detection capabilities. Based on a new comparison metric, method efficiency, which takes into account both error coverage and performance overhead\u00a0\u2026", "num_citations": "91\n", "authors": ["493"]}
{"title": "On combining formal and informal verification\n", "abstract": " We propose algorithms which combine simulation with symbolic methods for the verification of invariants. The motivation is two-fold. First, there are designs which are too complex to be formally verified using symbolic methods; however the use of symbolic techniques in conjunction with traditional simulation results in better \u201ccoverage\u201d relative to the computational resources used. Additionally, even on designs which can be symbolically verified, the use of a hybrid methodology often detects the presence of bugs faster than either formal verification or simulation.", "num_citations": "90\n", "authors": ["493"]}
{"title": "Probabilistic verification of Boolean functions\n", "abstract": " We present a novel method for verifying the equivalence of two Boolean functions. Each function is hashed to an integer code by assigning random integer values to the input variables and evaluating an integer-valued transformation of the original function. The hash codes for two equivalent functions are always equal. Thus the equivalence of two functions can be verified with a very low probability of error, which arises from unlikely \u201ccollisions\u201d between inequivalent functions. An upper bound, \u2208, on the probability of error is known a priori. The bound can be decreased exponentially by making multiple runs. Results indicate significant time and space advantages for this method over techniques that represent each function as a single OBDD. Some functions known to require space (and time) exponential in the number of input variables for these techniques require only polynomial resources using our\u00a0\u2026", "num_citations": "90\n", "authors": ["493"]}
{"title": "Design of test pattern generators for built-in test\n", "abstract": " This paper presents methods for designing cost-effective on-chip built-in test generators. Given an unordered test set, these methods produce an area and time efficient generator circuit using a small ROM and some additional logic. Some simulation results are presented.", "num_citations": "90\n", "authors": ["493"]}
{"title": "Automatic extraction of the control flow machine and application to evaluating coverage of verification vectors\n", "abstract": " Simulation is still the primary, although inadequate, resource for verifying the conformity of a design to its functional specification. Fortunately, most errors in the early stages of design involve only the control flow in the circuit. We define the functional coverage of a given sequence of verification vectors as the amount of control behavior exercised by them. We present a novel technique for automatically extracting the control flow of a design on the basis of the underlying mathematical model. Significantly, this extraction is independent of the circuit description style. The Extracted Control Flow Machine (ECFM) is then used for estimation of functional coverage and to provide information that will help the designer improve the quality of his or her tests.", "num_citations": "89\n", "authors": ["493"]}
{"title": "ACCE: Automatic correction of control-flow errors\n", "abstract": " Detection of control-flow errors at the software level has been studied extensively in the literature. However, there has not been any published work that attempts to correct these errors. Low-cost correction of CFEs is important for real-time systems where checkpointing is too expensive or impossible. This paper presents automatic correction of control-flow errors (ACCE), an efficient error correction algorithm involving addition of redundant code to the program. ACCE has been implemented by modifying GCC, a widely used C compiler, and performance measurements show that the overhead is very low. Fault injection experiments on SPEC and MiBench benchmark programs compiled with ACCE show that the correct output is produced with high probability and that CFEs are corrected with a latency of a few hundred instructions.", "num_citations": "88\n", "authors": ["493"]}
{"title": "Functional partitioning for verification and related problems\n", "abstract": " We present functional partitioning methods to reduce the computational resources used in analyzing Boolean functions. One general partitioning strategy, orthogonal partitioning, divides a function's truth table into disjoint partitions; the Bo9lean and of any two subfunctions is always zero. Functional partitioning techniques can significantly augment current OBDD-based verification methods. The design and specifications are partitioned into smaller functions that can be independently verified in a pairwise manner; these functions can even be ordered separately. Thfs allows verification in polynomial space and time of several functions whose OBDD representations have teen proven to be exponential in the number of input variables such as Bryant's hidden weighted bit function, as well as much more efficient verification of other difficult functions such as multipliers. Functional partitioning improves the efficiency of probabilistic verification as well, where even further improvements can be realized for the above functions. In the presence 9f design errors, these partitioning techniques can provide even greater efficiency gains.", "num_citations": "85\n", "authors": ["493"]}
{"title": "Iterative simulation-based genetics+ deterministic techniques= complete ATPG\n", "abstract": " Simulation-based test vector generators require much less computer time than deterministic ATPG but they generate longer test sequences and sometimes achieve lower fault coverage. This is due to the divergence in the search process. In this paper, we propose a correction technique for simulation-based ATPG. This technique is based on identifying the diverging state and on computing a fault cluster (faults close to each other). A set of candidate faults from the cluster is targeted with a deterministic ATPG and the resulting test sequence is used to restart the search process of the simulation-based technique. This above process is repeated until all faults are detected or proven to be redundanthntestable. The program implementing this approach has been used to generate tests with very high fault coverage, and runs about 10 times faster than traditional deterministic techniques with very good test quality in terms\u00a0\u2026", "num_citations": "83\n", "authors": ["493"]}
{"title": "Characterization and testing of physical failures in MOS logic circuits\n", "abstract": " Studies indicate that the conventional stuck-at fault model is inadequate for modeling the effects of physical failures on MOS circuits. The authors illustrate various types of non-stuck-at behavior, such as indeterminate logic levels, timing errors, and alteration of logic functions. They discuss the generation of tests for detecting the failures in simple and complex MOS circuits. An advantage of testing for failures at the circuit level is that in some cases it may be possible to utilize the structural properties of the circuit to design a much simpler test set compared to one that is based on a gatelevel description of the circuit. The authors outline a methodology whereby functional fault models are derived by studying the effects of physical failures at the circuit level for functional modules.", "num_citations": "80\n", "authors": ["493"]}
{"title": "Synthesis of delay fault testable combinational logic\n", "abstract": " The synthesis of combinational logic which is robust delay fault testable is developed. In a circuit, any reconvergent fanout may result in the presence of blocked paths and/or paths which can be sensitized only if some other path is also sensitized. Implicit don't care terms are used to detect these problems and a local transformation at the reconvergence point is used to upgrade the delay fault testability of the circuit. The sharing of terms in a multilevel circuit is preserved to the greatest extent possible. Good results have been obtained based on an implementation of the algorithm in the LISP programming language on a TI Explorer machine.<>", "num_citations": "79\n", "authors": ["493"]}
{"title": "Viper: An efficient vigorously sensitizable path extractor\n", "abstract": " University of Texas at Austin Austin, TX 78758 is a critical issue in VLSI design. Several tim-ing verification algorithms have been proposed in the last few years. However, due to the huge computation time needed to eliminate false paths, existing algorithms have difficulty in performing timing verification for large circuits. This paper presents an efficient timing verification algorithm, with a new sensitization criterion, which directly identifies the critical path without eliminating false pat hs from a path list. The inputs which sensitize the critical path are determined as well.", "num_citations": "78\n", "authors": ["493"]}
{"title": "VLSI logic and fault simulation on general-purpose parallel computers\n", "abstract": " The authors define a general framework for the parallel simulation of digital systems and develop and evaluate tools for logic and fault simulation that have a good cost-performance ratio. They first review previous work and identify central issues. Then a high-level process model of parallel simulation is presented to clarify essential design choices. Algorithms for parallel logic and fault simulation of synchronous gate-level designs are introduced. The algorithms are based on a partitioning approach that reduces the number of necessary synchronizations between processors. A simple performance model characterizes the dependence on some crucial parameters. Experimental results for some large benchmarks are given, using prototype implementations for both message-passing and shared-memory machines.< >", "num_citations": "77\n", "authors": ["493"]}
{"title": "Probabilistic design verification\n", "abstract": " The authors present a novel method for verifying the equivalence of two Boolean functions. Each function is hashed to an integer code by assigning random integer values to the input variables and evaluating its integer-valued representation. The equivalence of two functions can be verified with a very low probability of error. The probability of error can be exponentially decreased by making multiple runs. Results indicate significant time and space advantages for this method over deterministic techniques. Some functions known to require space (and time) exponential in the number of input variables for deterministic verification require only polynomial resources using the proposed technique.< >", "num_citations": "77\n", "authors": ["493"]}
{"title": "A multivalued algebra for modeling physical failures in MOS VLSI circuits\n", "abstract": " This paper proposes a new logical model for nMOS and CMOS circuits. Existing gate-level and switch-level models are limited in their ability to simulate MOS circuit behavior accurately when modeling physical failures. The model proposed in this paper is in the form of a multivalued algebra defined on a set of node states. The state of a node is represented as a pair <a,b> where \"a\" specifies the condition of a node and \"b\" specifies the logic level: There are five conditions and five logic levels. The assignment of node states is done dynamically during the process of logic simulation. The rules of the algebra are used to derive state tables that model the behavior of transistors. Our general model of a transistor allows for strong interactions between all three terminals of a transistor. This enables us to model the effects of physical failures such as a short between the gate and drain of a transistor. A simulation algorithm\u00a0\u2026", "num_citations": "76\n", "authors": ["493"]}
{"title": "A signature analyzer for analog and mixed-signal circuits\n", "abstract": " While the design of signature analyzers for digital circuits has been well researched in the past, signature analyzers for analog signals are relatively unknown. In this paper, a novel signature analysis scheme for analog and mixed-signal circuits is proposed. The signatures possess the interesting properly that if the input analog signal is imprecise within certain bounds (an inherent property of analog signals), then the generated signature is also imprecise within certain bounds. A failure is indicated by the generated signature being different from the expected signature by a margin greater than a predetermined threshold.< >", "num_citations": "73\n", "authors": ["493"]}
{"title": "Clear: Cross-layer exploration for architecting resilience: Combining hardware and software techniques to tolerate soft errors in processor cores\n", "abstract": " We present a first of its kind framework which overcomes a major challenge in the design of digital systems that are resilient to reliability failures: achieve desired resilience targets at minimal costs (energy, power, execution time, area) by combining resilience techniques across various layers of the system stack (circuit, logic, architecture, software, algorithm). This is also referred to as cross-layer resilience. In this paper, we focus on radiation-induced soft errors in processor cores. We address both single-event upsets (SEUs) and single-event multiple upsets (SEMUs) in terrestrial environments. Our framework automatically and systematically explores the large space of comprehensive resilience techniques and their combinations across various layers of the system stack (798 cross-layer combinations in this paper), derives cost-effective solutions that achieve resilience targets at minimal costs, and provides\u00a0\u2026", "num_citations": "71\n", "authors": ["493"]}
{"title": "CEDA: Control-flow error detection using assertions\n", "abstract": " This paper presents an efficient software technique, control-flow error detection through assertions (CEDA), for online detection of control-flow errors. Extra instructions are automatically embedded into the program at compile time to continuously update runtime signatures and to compare them against preassigned values. The novel method of computing runtime signatures results in a huge reduction in the performance overhead, as well as the ability to deal with complex programs and the capability to detect subtle control-flow errors. The widely used C compiler, GCC, has been modified to implement CEDA, and the SPEC benchmark programs were used as the target to compare with earlier techniques. Fault injection experiments were used to demonstrate the effect of control-flow errors on software and to evaluate the fault detection capabilities of CEDA. Based on a new comparison metric, method efficiency\u00a0\u2026", "num_citations": "71\n", "authors": ["493"]}
{"title": "Fault simulation of linear analog circuits\n", "abstract": " Research in the areas of analog circuit fault simulation and test generation has not achieved the same degree of success as its digital counterpart owing to the difficulty in modeling the more complex analog behavior. This article presents a novel approach to this problem by mapping the good and faulty circuits to thediscrete Z-domain. An efficient fault simulation is then performed on this discretized circuit for the given input test wave form. This simulator provides an order of magnitude speedup over traditional circuit simulators. An efficient fault simulator and the formulation of analog fault models opens up the ground for analog automatic test generation.", "num_citations": "69\n", "authors": ["493"]}
{"title": "Compaction of ATPG-generated test sequences for sequential circuits\n", "abstract": " Currently available Automatic Test Pattern Generators (ATPGs) generate test sets that are non-optimal in length. Since the cost of testing depends on the lcngth of the test set, previous researchers reduced the tcst length by applying compaction techniques. These techniques were directed towards combinational circuits and are not directly applicable to sequential CircuitS bcCauSe the test sequences must maintain a strict ordering of the palternS. This paper describes a number of new heuristic techniques to reduce the length of the test set for a sequential circuit by compaction of the sequences generated by an ATPG. Based on these techniques, a program has been written in C that achievcd 56%-73% reduction in test length of a highly sequential circuit obtained from industry.", "num_citations": "68\n", "authors": ["493"]}
{"title": "An RTL abstraction technique for processor microarchitecture validation and test generation\n", "abstract": " Design validation is becoming more and more a bottleneck in the microprocessor design process. The difficulty of validation stems from the complexity of the design, which requires searching an enormous space to check correctness. This is exacerbated by features for enhancing performance, such as pipelines, which are becoming common in most microprocessors. This paper describes a new abstraction technique to handle this problem. Our solution is a novel method to identify the control states automatically from the processor HDL description and to extract an abstract finite state machine model which preserves the behaviors of the design accurate to the clock cycle, so that the state space to be analyzed is drastically reduced.               This model is used to evaluate microarchitecture-level coverage of validation tests. We also present validation test generation algorithm for traversing state transition paths\u00a0\u2026", "num_citations": "67\n", "authors": ["493"]}
{"title": "Complex gate implementations for quantum dot cellular automata\n", "abstract": " This paper presents a quantum dot cellular automata complex gate composed from simple 3-input majority gates. This 7-input gate can be configured into many useful gate structures such as a 4-input AND gate, a 4-input OR gate, a product of sums representation, a sum of products representation, and other variations.", "num_citations": "66\n", "authors": ["493"]}
{"title": "Test generation for gigahertz processors using an automatic functional constraint extractor\n", "abstract": " As the sizes of general and special purpose processors increase rapidly, generating high quality manufacturing tests which can be run at native speeds is becoming a serious problem. One solution is a novel method for functional test generation in which a transformed module is built manually, and which embodies functional constraints described using virtual logic. Test generation is then performed on the transformed module using commercial tools and the transformed module patterns are translated back to the processor level. However, the technique is useful only if the virtual logic can be generated automatically. This paper describes an automatic functional constraint extraction algorithm and a procedure to build the transformed module. We describe the tool, FALCON, used to extract the functional constraints of a given embedded module from a Verilog RTL model. The constraint extraction for embedded\u00a0\u2026", "num_citations": "66\n", "authors": ["493"]}
{"title": "Automatic test vector cultivation for sequential VLSI circuits using genetic algorithms\n", "abstract": " This paper discusses a new approach for generating test vectors, using test cultivation, for both combinational and sequential VLSI circuits described hierarchically at the transistor, gate, and higher levels. The approach is based on continuous mutation of a given input sequence and on analyzing the mutated vectors for selecting the test set. The hierarchical technique used in the analysis drastically reduces the memory requirements, allowing test generation for large circuits. The test cultivation algorithms are simulation-based and a test set can be cultivated for any circuit that can be simulated logically. In particular, general MOS digital designs can be handled, and both stuck-at and transistor faults can be accurately modeled. Using the approach, tests were generated with very high fault coverage for gate-level circuits as well as for transistor level circuits.", "num_citations": "66\n", "authors": ["493"]}
{"title": "A methodology for functional level testing of microprocessors\n", "abstract": " The task of fault detection in microprocessors is very difficult because of their complexity. In this paper we present a general model for microprocessors in terms of their control and data processing sections, which includes features of a large variety of available microprocessors. We then develop a fault model on a functional level which takes into account a large number offailures, includingfaultsin instruction decoding, control commands, data transfer mechanisms, datastoragefacilities, and the arithmetic and logic unit. The basic requirement of a test procedure for a microprocessor is that it be able to check for proper execution of every instruction using other potentially faulty instructions. A set of test procedures is presented; this set is proved to be capable of detecting all faults in the fault model.", "num_citations": "66\n", "authors": ["493"]}
{"title": "A 5-GS/s 10-b 76-mW time-interleaved SAR ADC in 28 nm CMOS\n", "abstract": " This paper presents a 5-GS/s 12-way 10-b time-interleaved successive approximation register (SAR) ADC for direct sampling receivers. Proper signal and clock distribution along the multiple channels are utilized to mitigate interchannel bandwidth and timing mismatches. A digitally assisted calibration is introduced to remove the interchannel offset, gain, and timing mismatch. The T-type bootstrapped sampling switches minimize the interchannel crosstalk among top-plate sampling SAR channels and the signal-dependent leakage current during SAR conversion cycles. The power efficiency of this ADC is significantly improved by many design techniques. The merged capacitor switching algorithm leads to high switching efficiency and a smaller area. The modified reference voltage scheme optimizes input common-mode voltage of the comparators. The optimal subradix-2 capacitive DAC results in low-power\u00a0\u2026", "num_citations": "64\n", "authors": ["493"]}
{"title": "On-chip delay measurement for silicon debug\n", "abstract": " Efficient test and debug techniques are indispensable for performance characterization of large complex integrated circuits in deep-submicron and nanometer technologies. Performance characterization of such chips requires on-chip hardware and efficient debug schemes in order to reduce time to market and ensure shipping of chips with lower defect levels. In this paper we present an on-chip scheme for delay fault detection and performance characterization. The proposed technique allows for accurate measurement of delays of speed paths for speed binning and facilitates a systematic and efficient test and debug scheme for delay faults. The area overhead associated with the proposed technique is very low.", "num_citations": "64\n", "authors": ["493"]}
{"title": "A fast, accurate and simple critical path monitor for improving energy-delay product in dvs systems\n", "abstract": " This paper introduces a design scheme that improves Energy-Delay Product (EDP) in conventional Dynamic Voltage Scaling (DVS) systems by exploiting timing margins. To achieve this scheme, we designed a high-speed Critical Path Monitor composed of several Critical Path Replicas, a Timing Checker, and a Toggle Flip-Flop. The replicas are implemented based on our proposed algorithm, which considers the following two facts: (a) the voltage scaling behavior of logic and interconnect are fundamentally different; (b) various logic gates show different sensitivity in regard to process, temperature, as well as voltage changes. Because the replicas are connected in parallel by C-elements, the longest delay selection among all of the replica delays is performed automatically, improving the system response time. If the utilizable margin is detected by the Timing Checker, the frequency controller increases system clock\u00a0\u2026", "num_citations": "62\n", "authors": ["493"]}
{"title": "Automatic generation of instruction sequences targeting hard-to-detect structural faults in a processor\n", "abstract": " Testing a processor in native mode by executing instructions from cache has been shown to be very effective in discovering defective chips. In previous work, we showed an efficient technique for generating instruction sequences targeting specific faults. We generated tests using traditional techniques at the module level and then mapped them to instruction sequences using novel methods. However, in that technique, the propagation of module test responses to primary outputs was not automated. In this paper, we present the algorithm and experimental results for a technique which automates the functional propagation of module level test responses. This technique models the propagation requirement as a Boolean difference problem and uses a bounded model checking engine to perform the instruction mapping. We use a register transfer level (RT-Level) abstraction which makes it possible to express Boolean\u00a0\u2026", "num_citations": "62\n", "authors": ["493"]}
{"title": "Compiler-assisted static checkpoint insertion\n", "abstract": " This paper describes a compiler-assisted approach for static checkpoint insertion. Instead of fixing the checkpoint location before program execution, a compiler enhanced polling mechanism is utilized to maintain both the desired checkpoint intervals and reproducible checkpoint locations. The technique has been implemented in a GNU CC compiler for Sun 3 and Sun 4 Sparc processors. Experiments demonstrate that the approach provides for stable checkpoint intervals and reproducible checkpoint placements with performance overhead comparable to a previously presented compiler-assisted dynamic scheme CATCH utilizing the system clock 17. Static checkpoints, checkpoint placement, checkpoint interval and compilers.Descriptors:", "num_citations": "62\n", "authors": ["493"]}
{"title": "IBDDs: An efficient functional representation for digital circuits\n", "abstract": " A central issue in the solution of many computer aided design problems is finding a concise representation for circuit designs and their functional specifications. Ordered binary decision diagrams (OBDDs) have recently emerged as a popular representation for various CAD applications such as design verification, synthesis, testing, modeling and simulation. Unfortunately, there is no efficient OBDD representation for many circuits, even in some cases for circuits which perform such apparently simple functions as multiplication. The authors present a new BDD representation scheme, called indexed BDDs (IBDDs), and show that it allows polynomial representations of functions which provably require exponential space using OBDDs. The key idea in IBDDs is to allow multiple occurrences of the input variables, subject to ordering constraints. The authors give an algorithm for verifying the equivalence of two IBDDs and\u00a0\u2026", "num_citations": "62\n", "authors": ["493"]}
{"title": "Speed up of test generation using high-level primitives\n", "abstract": " A general methodology to speed up the test generation process for combinational circuits with high-level primitives is proposed. The technique is able to handle circuits in a hierarchical fashion, treats the signal at a bit-vector level rather than the bit level and takes advantage of the complex operations that are available in the computer system. The technique has been implemented and the results are presented for five circuits. It is shown that by using the high-level primitives a significant speed-up and significant reduction in storage requirement are achieved. More importantly, the reduction in storage size permits test generation for very large circuits. It is clear that use of high-level primitives is more efficient than use of low-level primitives in test generation. A dependency-directed backtracking mechanism is also present which reduces the number of backtracks. The technique presented is complete, permits test\u00a0\u2026", "num_citations": "61\n", "authors": ["493"]}
{"title": "Fault simulation in a distributed environment\n", "abstract": " Fault simulation of VLSI circuits takes considerable computing resources and there have been significant efforts to speed up the fault simulation process. A distributed fault simulator implemented on a loosely-coupled network of general-purpose computers is described. The techniques used result in a close to linear speedup and can be used effectively in most industrial VLSI CAD (computer-aided design) environments.< >", "num_citations": "61\n", "authors": ["493"]}
{"title": "Design of testable CMOS logic circuits under arbitrary delays\n", "abstract": " The sequential behavior of CMOS logic circuits in the presence of stuck-open faults requires that an initialization input followed by a test input be applied to detect such a fault. However, a test set based on the assumption that delays through all gates and interconnections are zero, can be invalidated in the presence of arbitrary delays in the circuit. In this paper, we will present a necessary and sufficient condition for the existence of a test set, which cannot be invalidated under arbitrary delays, for an AND-OR or OR-AND CMOS realization for any given function. We will also introduce a Hybrid CMOS realization which, for any given function, is guaranteed to have a valid test set under arbitrary delays.", "num_citations": "61\n", "authors": ["493"]}
{"title": "Fault-tolerant matrix operations on multiple processor systems using weighted checksums\n", "abstract": " Hardware for performing matrix operations at high speeds is in great demand in signal and image processing and in many real-time and scientific applications. VLSI technology has made it possible to perform fast large-scale vector and matrix computations by using multiple copies of low-cost processors. Since any functional error in a high performance system may seriously jeopardize the operation of the system and its data integrity, some level of fault-tolerance must be obtained to ensure that the results of long computations are valid. A low-cost checksum scheme had been proposed to obtain fault-tolerant matrix operations on multiple processor systems. However, this scheme can only correct errors in matrix multiplication; it can detect, but not correct errors in matrix-vector multiplication, LU-decomposition, and matrix inversion. In order to solve these problems with the checksum scheme, a very general matrix\u00a0\u2026", "num_citations": "61\n", "authors": ["493"]}
{"title": "DRAFTS: Discretized analog circuit fault simulator\n", "abstract": " The areas of analog circuit fault simulation and test generation have not achieved the same degree of success as their digital counterparts owing to the difficulty in modeling the more complex analog behavior. We present a novel approach to this problem by mapping the circuit and circuit-level faults to the discrete domain. An efficient fault simulation is then performed on this discretized circuit for the given input test waveform.", "num_citations": "60\n", "authors": ["493"]}
{"title": "Forward Recovery Using Checkpointing in Parallel Systems.\n", "abstract": " The inherent redundancy in parallel systems provides the potential for low cost fault tolerance. This paper describes a forward recovery strategy that exploits this opportunity. The approach is based on lookahead execution with rollback validation. It uses replicated tasks executing on different processors for forward recovery and checkpoint comparison for error detection. Two alternatives based on this strategy are analyzed and compared with TMR and two common backward recovery methods. A simulation study of the impact of fault distribution, checkpointing time and checkpoint comparison time is presented. Compared with classic checkpointing techniques, the scheme of this paper provides for rapid recovery and requires, on average, fewer processors than standard NMR methods. storage by a general interconnection network. A reliable file system provides the secondary storage. A task is an independent computation. It can also be a group of related sub-tasks. This paper is concerned with faults in individual processors. The scheme is able to detect and recover from faults in individual processors which result in a single erroneous (corrupted) checkpoint. Faults in the software, processor interconnection network, or secondary storage may not be detectable nor recoverable.Each checkpoint made by a processor can be used to restart a task on any other processor. A checkpoint is assumed to provide a complete recovery line for a task [4). The comparison of checkpoints serves as the mechanism for error detection and checkpoint validation in our approach. At the end of each checkpoint interval, a, the new checkpoints are compared. A match in\u00a0\u2026", "num_citations": "60\n", "authors": ["493"]}
{"title": "Load redistribution under failure in distributed systems\n", "abstract": " In order to implement a distributed system with fail-soft capabilities it is necessary to specify algorithms which redistribute the work load of a failed processor to the remaining good processors. This paper develops a general model to analyze the behavior of these algorithms in a distributed system. Such algorithms should be used with caution as they have the capability of making the entire system Unstable. By unstable we mean that if a processor fails, and its workload is redistributed, then the increased workload directed towards the rest of the system could drive one or more of the processors into overload resulting in a serious degradation of system performance. Using the general model we have studied a class of load redistribution algorithms which use various techniques to redistribute workload. These techniques include: buffering jobs arriving to the failed processor, transmitting only the jobs in the queue of the\u00a0\u2026", "num_citations": "60\n", "authors": ["493"]}
{"title": "Automated mapping of pre-computed module-level test sequences to processor instructions\n", "abstract": " Executing instructions from the cache has been shown to improve the defect coverage of real chips. However, although the faults detected by such tests can be determined, there has been no technique to target test generation for an undetected fault. This paper presents a novel technique to map pre-computed test sequences at the module level of a processor, to sequences of instructions. The module level pre-computed test sequence is translated into a temporal logic property and the negation of the property is passed to a bounded model checker. The model checker produces a counter-example for the temporal logic property. This counter-example trace contains the instruction sequence that can be applied at the primary inputs to produce the pre-computed test sequence at the module inputs. This technique has no restrictions on the type of test sequences, so it can be used to map test sequences for any kind of\u00a0\u2026", "num_citations": "59\n", "authors": ["493"]}
{"title": "CHEETA: Composition of hierarchical sequential tests using ATKET\n", "abstract": " An approach to modular and hierarchical sequential circuit test generation, which exploits a top-down design methodology, uses high level test knowledge and constraint driven module test generation to target faults at the structural level, is introduced in this paper. Results obtained for several designs are provided to demonstrate the effectiveness of our approach and the need for high level knowledge along with global constraints while deriving sequential circuit tests.< >", "num_citations": "59\n", "authors": ["493"]}
{"title": "DCIATP-an iterative analog circuit test generation program for generating DC single pattern tests\n", "abstract": " An algorithm is proposed for automatic test input generation for nonlinear analog circuits and digital circuits with analog behavior under fault. The algorithm uses high-level reasoning with simple iteration to find inputs which will detect resistive shorts and opens that cause DC errors. A simple version of the algorithm, for the time-domain case, has been implemented. Current work includes incorporating the heuristics into the path-generating algorithm, creating tests for larger circuits, extending the fault model to a parameter change in branches other than resistors (e.g. a beta change in a transistor), creating sensitivity metrics for the frequency domain and developing a transient error solution.< >", "num_citations": "59\n", "authors": ["493"]}
{"title": "Fault-tolerant systems for the computation of eigenvalues and singular values\n", "abstract": " The computations of eigenvalues and singular values are key to applications including signal and image processing. Since large amounts of computation are needed for these algorithms, and since many digital signal processing applications have real-time requirements, many different special-purpose processor array structures have been proposed to solve these two algorithms. This paper develops a new methodology to incorporate fault tolerance capability into processor arrays which have been proposed for these problems. In the first part of this paper, earlier techniques of algorithm-based fault tolerance are applied to QR factorization and QR iteration. This technique encodes input data at a high level by using the specific property of each algorithm and checks the output data before they leave the systems. In the second part of the paper, special properities of eigenvalues and singular values are used to\u00a0\u2026", "num_citations": "59\n", "authors": ["493"]}
{"title": "Small-delay defect detection in the presence of process variations\n", "abstract": " Interconnect-based defects such as partial opens are becoming more prevalent in nanoscale designs. These are latent defects that affect circuit reliability and can be modeled as small-delay defects. Detecting such defects therefore requires faster than at-speed test clocks. In the paper we analyze the uncertainty introduced by process variations in detecting these defects. We propose new path selection algorithms that increase the probability of defect detection by taking into account the variability in path delays. Our results show that the new technique detects much smaller defects than the traditional approach of selecting the longest paths for test.", "num_citations": "58\n", "authors": ["493"]}
{"title": "A unified framework for design validation and manufacturing test\n", "abstract": " New approaches to address the difficult problems in test are necessary if its current status as a major bottleneck in the production of quality integrated circuits is to be changed. The authors propose a new direction for solving the test problem using powerful methods already employed for the formal verification of large circuits. More specifically, they discuss how abstraction techniques can assist conventional ATPG tools when attacking hard to detect faults. The same abstractions can also be used in design verification to increase the level of confidence in a design following simulation, by providing a meaningful measure of the coverage achieved by the verification vectors. In this sense, the authors' approach is geared toward providing a unified fled framework for design validation and manufacturing test.", "num_citations": "58\n", "authors": ["493"]}
{"title": "Quadruple time redundancy adders [error correcting adder]\n", "abstract": " This paper presents a concurrent error correcting adder design employing fault masking through a combination of time and hardware redundancy. This new method, quadruple time redundancy, is compared with a non-redundant adder, a triple modular redundancy adder, and a time shared triple modular redundancy adder with respect to the hardware complexity and the delay for adders of various sizes. In comparison with time shared triple modular redundancy to which it is most closely related, quadruple time redundancy results in a 40%-55% reduction in hardware complexity while incurring a reasonable delay increase.", "num_citations": "56\n", "authors": ["493"]}
{"title": "Evaluation of integrated system-level checks for on-line error detection\n", "abstract": " This paper evaluates the capabilities of an integrated system level error detection technique using fault and error injection. This technique is comprised of two software level mechanisms for concurrent error detection, control flow checking using assertions (CCA) and data error checking using application specific data checks. Over 300,000 faults and errors were injected and the analysis of the results reveals that the CCA detects 95% of all the errors while the data checks are able to detect subtle errors that go undetected by the CCA technique. Latency measurements also shelved that the CCA technique is faster than the data checks in detecting the error. When both techniques were incorporated, the system was able to detect over 98% of all injected errors.", "num_citations": "56\n", "authors": ["493"]}
{"title": "On-chip programmable capture for accurate path delay test and characterization\n", "abstract": " The increasing gap between modern chip frequencies and test clock frequencies provided by external test equipment, makes at-speed delay testing a challenge. We present a novel technique to generate a capture signal on-chip, with programmable delay, which enables faster than at-speed test. The test clock frequency can be programmed as a part of the test vector itself. Since test clock frequency can be controlled, it is no longer required to depend only on the long paths for detecting small delay defects, which provides flexibility in selecting test paths. The technique has minimal overhead in terms of area and design effort and can be easily incorporated into the current scan based delay test methods.", "num_citations": "55\n", "authors": ["493"]}
{"title": "Test generation for arithmetic units by graph labelling\n", "abstract": " CiNii \u8ad6\u6587 - Test generation for arithmetic units by graph labelling CiNii \u56fd\u7acb\u60c5\u5831\u5b66\u7814\u7a76\u6240 \u5b66\u8853 \u60c5\u5831\u30ca\u30d3\u30b2\u30fc\u30bf[\u30b5\u30a4\u30cb\u30a3] \u65e5\u672c\u306e\u8ad6\u6587\u3092\u3055\u304c\u3059 \u5927\u5b66\u56f3\u66f8\u9928\u306e\u672c\u3092\u3055\u304c\u3059 \u65e5\u672c\u306e\u535a\u58eb\u8ad6\u6587\u3092\u3055\u304c\u3059 \u65b0\u898f \u767b\u9332 \u30ed\u30b0\u30a4\u30f3 English \u691c\u7d22 \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u9589\u3058\u308b \u30bf\u30a4\u30c8\u30eb \u8457\u8005\u540d \u8457\u8005ID \u8457\u8005\u6240\u5c5e \u520a\u884c\u7269\u540d ISSN \u5dfb\u53f7\u30da\u30fc\u30b8 \u51fa\u7248\u8005 \u53c2\u8003\u6587\u732e \u51fa\u7248\u5e74 \u5e74\u304b\u3089 \u5e74\u307e\u3067 \u691c\u7d22 \u691c\u7d22 \u691c\u7d22 CiNii\u306e\u30b5\u30fc\u30d3\u30b9 \u306b\u95a2\u3059\u308b\u30a2\u30f3\u30b1\u30fc\u30c8\u3092\u5b9f\u65bd\u4e2d\u3067\u3059\uff0811/11(\u6c34)-12/23(\u6c34)\uff09 Test generation for arithmetic units by graph labelling CHATTERJEE A. \u88ab\u5f15\u7528\u6587\u732e: 1\u4ef6 \u8457\u8005 CHATTERJEE A. \u53ce\u9332\u520a\u884c\u7269 Proc. Int. Symp. Fault-tolerant Comput., Pittsburgh, PA, July 1987 Proc. Int. Symp. Fault-tolerant Comput., Pittsburgh, PA, July 1987, 1987 \u88ab\u5f15\u7528\u6587\u732e: 1\u4ef6\u4e2d 1-1\u4ef6\u3092 \u8868\u793a 1 A Method of Test Generation for Iterative Logic Arrays (\u7279\u96c6:VLSI\u30d7\u30ed\u30bb\u30c3\u30b5\u53ca\u3073\u65b0\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3LSI\u6280\u8853,\u4e00\u822c) Boateng Kwame Osei , \u9ad8\u6a4b \u5bdb , \u9ad8\u677e \u96c4\u4e09 \u96fb\u5b50\u60c5\u5831\u901a\u4fe1\u5b66\u4f1a\u6280\u8853\u7814\u7a76\u5831\u544a. ICD, \u96c6\u7a4d\u56de\u8def 99(\u2026", "num_citations": "54\n", "authors": ["493"]}
{"title": "BiCMOS fault models: Is stuck-at adequate?\n", "abstract": " The adequacy of the stuck-at fault model for BiCMOS logic is investigated. Realistic failures in basic logic blocks are examined, and their coverage by the stuck-at model is explored. It is shown that the static stuck-at model cannot cover the complete range of possible failures, and more importantly, tests for stuck-at faults will not detect realistic features in BiCMOS technology. This is because most open faults manifest themselves as delay failures. Through the use of transient analysis it is shown that the only way to insure proper functioning of BiCMOS circuits is to test for delay faults.< >", "num_citations": "53\n", "authors": ["493"]}
{"title": "Beneficial neurocognitive effects of transcranial laser in older adults\n", "abstract": " Transcranial infrared laser stimulation (TILS) at 1064\u00a0nm, 250\u00a0mW/cm2 has been proven safe and effective for increasing neurocognitive functions in young adults in controlled studies using photobiomodulation of the right prefrontal cortex. The objective of this pilot study was to determine whether there is any effect from TILS on neurocognitive function in older adults with subjective memory complaint at risk for cognitive decline (e.g., increased carotid artery intima-media thickness or mild traumatic brain injury). We investigated the cognitive effects of TILS in older adults (ages 49\u201390, n\u00a0=\u00a012) using prefrontal cortex measures of attention (psychomotor vigilance task (PVT)) and memory (delayed match to sample (DMS)), carotid artery intima-media thickness (measured by ultrasound), and evaluated the potential neural mechanisms mediating the cognitive effects of TILS using exploratory brain studies of\u00a0\u2026", "num_citations": "52\n", "authors": ["493"]}
{"title": "Indexed BDDs: Algorithmic advances in techniques to represent and verify Boolean functions\n", "abstract": " A new Boolean function representation scheme, the Indexed Binary Decision Diagram (IBDD), is proposed to provide a compact representation for functions whose Ordered Binary Decision Diagram (OBDD) representation is intractably large. We explain properties of IBDDs and present algorithms for constructing IBDDs from a given circuit. Practical and effective algorithms for satisfiability testing and equivalence checking of IBDDs, as well as their implementation results, are also presented. The results show that many functions, such as multipliers and the hidden-weighted-bit function, whose analysis is intractable using OBDDs, can be efficiently accomplished using IBDDs. We report efficient verification of Booth multipliers, as well as a practical strategy for polynomial time verification of some classes of unsigned array multipliers.", "num_citations": "51\n", "authors": ["493"]}
{"title": "An evaluation of system-level fault tolerance on the Intel hypercube multiprocessor\n", "abstract": " A discussion is presented of a fault-tolerant hypercube multiprocessor architecture which uses a novel algorithm-based fault-detection approach for identifying faulty processors. The scheme involves the detection and location of faulty processors concurrently with the actual execution of parallel applications on the hypercube. The authors have implemented system-level fault-detection mechanisms for various parallel applications on a 16-processor Intel iPSC hypercube multiprocessor. They report on the results of two applications: matrix multiplication and fast Fourier transform. They have performed extensive studies of fault coverage of their system-level fault-detection schemes in the presence of finite-precision arithmetic, which affects the system-level encodings. They propose a reconfiguration strategy for reconfiguring the system around faulty processors by introducing spare links and nodes.< >", "num_citations": "51\n", "authors": ["493"]}
{"title": "Built-in tests for VLSI finite-state machines\n", "abstract": " IDEALS @ Illinois: Built-In Tests for VLSI Finite-State Machines IDEALS Login Search IDEALS This Collection query Advanced Search IDEALS Logo IDEALS Logo The Alma Mater Round Barn Grainger Engineering Library Built-In Tests for VLSI Finite-State Machines Welcome to the IDEALS Repository Browse IDEALS TitlesAuthorsContributorsSubjectsDateCommunities This Collection TitlesAuthorsContributorsSubjectsDateSeries/Report My Account LoginRegister Information Getting StartedAboutContact Us Access Key Closed Access Private / Closed Access Campus Access Limited Access: U. of I. Users Only IDEALS Home \u2192 College of Engineering \u2192 Coordinated Science Laboratory \u2192 Report - Coordinated Science Laboratory \u2192 View Item Built-In Tests for VLSI Finite-State Machines Hua, Kien Anh Use this link to cite this item: http://hdl.handle.net/2142/75302 Files in this item Files Description Format /pdf /pdf -\u2026", "num_citations": "50\n", "authors": ["493"]}
{"title": "Fault-secure algorithms for multiple-processor systems\n", "abstract": " In this paper we describe techniques for achieving fault secureness with low cost in multiple processor7 systems. In order to do this we consider the relationshipsN between algorithms, parallel architectures, and fault tolerance. The concept of fault-secure algorithms, described in this paper, involves the application of the ideas of fault tolerance at the system level to high-performance multiple-processor algorithms to make the results of the computation reliable. Algorithms are classified into broad classes called paradigms which are determined exclusively by the communication patterns of the processors. Fault-secure techniques are presented for three powerful paradigms: the multiplex, the recursive combination, and the multiplex-demultiplex paradigms. The basic idea used in the design of fault-tolerant algorithms is that the algorithms operate on encoded input data and produce encoded output data such that the\u00a0\u2026", "num_citations": "50\n", "authors": ["493"]}
{"title": "Test generation for programmable logic arrays\n", "abstract": " The problem of fault detection and test generation for programmable logic arrays (PLAs) is investigated. The effect of actual physical failures is viewed in terms of the logical changes of the product terms (growth, shrinkage, appearance and disappearance) constituting the PLA. Methods to generate a minimal single fault detection test set (T /sub S/) from the product term specification of the PLA, are presented. It is shown that such a test set can be derived using a set of simple, easily implementable algorithms. Methods to augment Ts in order to obtain a multiple fault detection test set (T /sub M/) are also presented.", "num_citations": "48\n", "authors": ["493"]}
{"title": "Subband filtering for time and frequency analysis of mixed-signal circuit testing\n", "abstract": " A new technique is proposed to analyze and compress the output responses from analog circuits. We first describe the subband filtering scheme to decompose responses from the analog circuit under test (CUT). A subband or wavelet filter takes the response, then generates the decomposed signals for each frequency band. The decomposed signal for each frequency band is rectified and then fed into its respective integrator. Two kinds of wavelet filters are used to decompose the test response and effectively detect the faults in the circuit. Implementation issues including hardware overhead are also discussed.", "num_citations": "47\n", "authors": ["493"]}
{"title": "Automatic verification of arithmetic circuits in RTL using stepwise refinement of term rewriting systems\n", "abstract": " This paper presents a novel technique for proving the correctness of arithmetic circuit designs described at the register transfer level (RTL). The technique begins with the automatic translation of circuits from a Verilog RTL description into a term rewriting system (TRS). We prove the correctness of the designs via an equivalence proof between TRSs for the implementation circuit design and a much simpler specification circuit design. We present this notion of equivalence between the TRSs and a stepwise refinement method for its decomposition, which we leverage in our tool Verifire. We demonstrate the effectiveness of our technique by using the tool for the verification of several multiplier designs that have hitherto been impossible to verify with existing approaches and tools.", "num_citations": "46\n", "authors": ["493"]}
{"title": "Signature analysis for analog and mixed-signal circuit test response compaction\n", "abstract": " While the design of signature analyzers for digital circuits has been well researched in the past, signature analyzers for analog signals are relatively unknown. The primary difficulty in analyzing signatures for analog signals is that the latter are imprecise in nature. Therefore, deterministic signature analysis schemes, such as those based on finite-field arithmetic using linear feedback shift registers, are unsuitable for analog circuits. In this paper, a novel signature analysis scheme for analog and mixed signal circuits is proposed. The signatures possess the interesting property that if the input analog signal is imprecise within certain bounds (an inherent property of analog signals), then the generated signature is also imprecise within certain bounds. A failure is indicated by the generated signature being different from the expected signature by a margin greater than a predetermined threshold; the larger the effects of\u00a0\u2026", "num_citations": "46\n", "authors": ["493"]}
{"title": "General linear codes for fault-tolerant matrix operations on processor arrays\n", "abstract": " Various checksum codes have been suggested for fault-tolerant matrix computations on processor arrays. Use of these codes is limited due to potential roundoff and overflow errors. Numerical errors may also be misconstrued as errors due to physical faults in the system. The authors identify a set of linear codes which can be used for fault-tolerant matrix operations such as matrix addition, multiplication, transposition, and LU-decomposition, with minium numerical error. Encoding schemes are given for some of the example codes which fall under the general set of codes. With the help of experiments, the authors derive a rule of thumb for the selection of a particular code for a given application. Since the overall error in the code will also depend on the method of implementation of the coding scheme, they suggest the use of specific algorithms and special hardware realizations for the check element computation.<>", "num_citations": "46\n", "authors": ["493"]}
{"title": "CHIEFS: A Concurrent, Hierachical, and Extensible Fault Simulator.\n", "abstract": " CHIEFS is a concurrent fault simulator which simulates directly from a hierarchical circuit description without requiring the circuit to be flattened to the level of primitives. The simulator is also decoupled from the fault model and supports user-extensible fault libraries and interface procedures. The combination of hierarchical evaluation and extensible fault libraries allows easy user-modification of the fault model and mixing of functional and technology-dependent fault modes.", "num_citations": "46\n", "authors": ["493"]}
{"title": "Concurrent hierarchical fault simulation: A performance model and two optimizations\n", "abstract": " This paper presents the technique of concurrent hierarchical fault simulation, a performance model, and two hierarchical optimization techniques to enhance fault simulator performance. The mechanisms for these enhancements are demonstrated with a performance model and are validated experimentally via CHIEFS, the Concurrent Hierarchical and Extensible Fault Simulator, and WRAP, an offline hierarchy compressor. Hieararchy-based fault partitioning and circuit reconfiguration are shown to improve simulator performance to O(n log n) under appropriate conditions. A decoupled fault modeling technique permits further performance improvements via a bottom-up hierarchy compression technique where macros of primitives are converted to single primitives. When combined, these techniques have produced a factor of 180 speedup on a mantissa multiplier. The performance model indicates that the speedup\u00a0\u2026", "num_citations": "45\n", "authors": ["493"]}
{"title": "A scheme for on-chip timing characterization\n", "abstract": " We present a novel technique for performing post-silicon timing characterization, i.e., delay fault test and debug, using on-chip delay measurement of critical paths in Integrated Circuits. In Deep Submicron technologies, timing related failures have become a major source of defective silicon, making it imperative to carry out efficient delay fault testing on such chips. In addition to test, there is also a need for an efficient and systematic silicon debug methodology for timing related failures. Existing timing characterization strategies are not effective in Deep Submicron technologies due to limitations on controllability and observability. The proposed technique uses a novel scheme to perform on-chip delay measurement and thus facilitate quick and efficient testing and debugging of delay faults in chips. The scheme has minimal hardware overhead and is robust in face of process variations.", "num_citations": "44\n", "authors": ["493"]}
{"title": "A novel test generation approach for parametric faults in linear analog circuits\n", "abstract": " While analog test generation tools are still in their infancy, the corresponding tools in the digital domain have reached a fair degree of maturity and acceptance. Recognizing this fact, we propose a novel test generation method for linear analog circuits that employs well established digital test software to generate time-domain tests for analog parametric faults. We transform the analog circuit to an equivalent digital circuit, and target only those stuck-at faults in the digital circuit that could possibly capture parametric failures in the original analog circuit. Hence, the sequence of digital test vectors obtained from any test generator represents a test waveform for the analog parametric faults. The technique is illustrated using examples that show this to be a simple, yet attractive alternative to costlier simulation-based analog test generation approaches.", "num_citations": "44\n", "authors": ["493"]}
{"title": "Delay fault testing and silicon debug using scan chains\n", "abstract": " This paper describes a novel technique to reuse the existing scanpaths in a chip for delay fault testing and silicon debug. Efficient test and debug techniques for VLSI chips are indispensable in Deep Submicron technologies. A systematic debug scheme is also necessary in order to reduce time-to-market. Due to stringent timing requirements of modern chips, test and debug schemes have to be tailored for detection and debug of functional defects as well as delay faults quickly and efficiently. The proposed technique facilitates an efficient scheme for detecting and debugging delay faults and has minimal area and power overhead.", "num_citations": "43\n", "authors": ["493"]}
{"title": "FACTOR: A hierarchical methodology for functional test generation and testability analysis\n", "abstract": " This paper develops an improved approach for hierarchical functional test generation for complex chips. In order to deal with the increasing, complexity of functional test generation, hierarchical approaches have been suggested wherein functional constraints are extracted for each module under test (MUT) within a design. These constraints describe a simplified ATPG view for the MUT and thereby speed up the test generation process. This paper develops an improved approach which applies this technique at deeper levels of hierarchy, so that effective tests can he developed for large designs with complex submodules. A tool called FACTOR (FunctionAl ConsTraint extractOR), which implements this methodology is described in this work. Results on the ARM design prove the effectiveness of FACTOR-ising large designs for test generation and testability analysis.", "num_citations": "43\n", "authors": ["493"]}
{"title": "Prediction of analog performance parameters using oscillation based test\n", "abstract": " Oscillation based test (OBT) is a low-cost and vectorless test technique for analog and mixed-signal integrated circuits. Previous research with OBT has focused primarily on structural issues with an emphasis on fault detection rather than determining the conformance of the circuit under test (CUT) with its specifications, or evaluation of CUT performance. This paper presents a novel methodology for efficient interpretation of OBT results. The proposed predictive oscillation based test (POBT) methodology uses adaptive regression models to predict the performance parameters of the CUT from the oscillation measurements. Simulation results indicate that, under parametric variations, this methodology can determine CUT performance parameters, resulting in enhanced test effectiveness.", "num_citations": "42\n", "authors": ["493"]}
{"title": "High level test generation using data flow descriptions\n", "abstract": " To significantly expedite the test generation process for sequential VLSI circuits, the hierarchy in the circuit descriptions should be exploited. Conventional test generators can provide tests for relatively small modules, which are typically embedded in large circuits. This paper considers test generation for complex VLSI circuits composed of many interconnected modules. In contrast to the previous approaches, the authors use high-level primitives and data flow descriptions to perform hierarchical test generation. Data flow descriptions provide the set of valid control signals to be activated for a particular data path to be active. Sequential propagation and justification of signals is carried out recursively. Results are presented based on an implementation of the algorithm in LISP on a Texas Instruments Explorer.<>", "num_citations": "42\n", "authors": ["493"]}
{"title": "Cache design for low power and high yield\n", "abstract": " A novel circuit approach to increase SRAM static noise margin (SNM) and enable lower operating voltage is described. Increasing process variability [1] [2] for new technologies coupled with increased reliability effects like negative bias temperature instability (NBTI) [3] all contribute to raising the minimum voltage required for stable SRAM. Our strategy is to improve the noise margin of the 6T SRAM cell by reducing the effect of parametric variation of the cell [4], especially in the low voltage operation mode. This is done using a novel circuit that selectively reduces the voltage swing on the world line and reduces the memory supply voltage during write operation. The proposed design increases the SRAM static noise margin (SNM) and write margin using a single voltage supply and with minimum impact to chip area, complexity, and timing. The technique supports both on-chip corner identification to adapt the SRAM\u00a0\u2026", "num_citations": "41\n", "authors": ["493"]}
{"title": "Automatic decomposition for sequential equivalence checking of system level and RTL descriptions\n", "abstract": " Sequential equivalence checking between system level descriptions of designs and their register transfer level (RTL) implementations is a very challenging and important problem in the context of systems on a chip (SoCs). We propose a technique to alleviate the complexity of the equivalence checking problem, by efficiently decomposing it using compare points. Traditionally, equivalence checking techniques use nominal or functional mapping of latches as compare points. Since we operate at a level where design descriptions are in system level languages or hardware description languages, we leverage the information available to us at this level in deducing sequential compare points. Sequential compare points encapsulate the sequential behavior of designs and are obtained by statically analyzing the design descriptions. We decompose the design using sequential compare points and represent the design\u00a0\u2026", "num_citations": "41\n", "authors": ["493"]}
{"title": "A low-cost concurrent error detection technique for processor control logic\n", "abstract": " This paper presents a concurrent error detection technique targeted towards control logic in a processor with emphasis on low area overhead. Rather than detect all modeled transient faults, the technique selects faults which have a high probability of causing damage to the architectural state of the processor and protects the circuit against these faults. Fault detection is achieved through a series of assertions. Each assertion is an implication from inputs to the outputs of a combinational circuit. Fault simulation experiments performed on control logic modules of an industrial processor suggest that high reduction in damage causing faults can be achieved with a low overhead.", "num_citations": "40\n", "authors": ["493"]}
{"title": "Automatic generation of instructions to robustly test delay defects in processors\n", "abstract": " We present a technique for generating instruction sequences to test a processor functionally. We target delay defects with this technique using an ATPG engine to generate delay tests locally, a verification engine to map the tests globally, and a feedback mechanism that makes the entire procedure faster. We demonstrate nearly 96% coverage of delay faults with the instruction sequences generated. These instruction sequences can be loaded into the cache to test the processor functionally.", "num_citations": "40\n", "authors": ["493"]}
{"title": "Verifying properties using sequential ATPG [IC design]\n", "abstract": " This paper develops a novel approach for formally verifying both safety and liveness properties of designs using sequential ATPG tools. The properties are automatically mapped into a monitor circuit with a target fault so that finding a test for the fault corresponds to formally establishing the property. The mapping of the properties to the monitor circuit is described in detail and the process is shown to be sound and complete. Experimental results show that the ATPG-based approach performs better than existing verification techniques, especially for large designs.", "num_citations": "40\n", "authors": ["493"]}
{"title": "Native mode functional self-test generation for systems-on-chip\n", "abstract": " With the rapid increase in the functionality of a single chip, the generation of high quality manufacturing tests which can be applied at-speed has become a serious issue. The problem is further compounded with an increasing level of integration in the case of Systems-On-Chip (SOCs), for which existing test generation tools are inadequate. Many of the peripherals in a SOC design may not include testability features, which renders conventional design for testability (DFT) approaches ineffective. Functional tests applied at-speed in the native mode of a microprocessor have been shown to be effective in detecting realistic defects. A novel approach to adopt this strategy to generate test patterns for SOCs is presented in this paper. This approach utilizes the core processor's instruction set to test its own functionality and that of the peripheral components. A SOC based on a model of the Intel 8085 processor is used to\u00a0\u2026", "num_citations": "40\n", "authors": ["493"]}
{"title": "A single-die 124 dB stereo audio delta-sigma ADC with 111 dB THD\n", "abstract": " This paper presents a highly power-efficient stereo delta-sigma ADC designed for high-precision applications, with measured inter-channel isolation over 130 dB. This design adopts a single-loop, fifth-order, 33 level analog modulator with positive and negative feedforward paths. An interpolated multilevel quantizer with unevenly weighted quantization levels replaces a conventional 5-bit flash type quantizer. These new techniques suppress signal dependent energy inside the delta-sigma loop, reduce internal channel coupling and power consumption. Manufactured in 0.35 mum double poly, three metal CMOS process, the single-die chip includes two analog modulators, on-chip bandgap reference circuit, decimation filter and serial interface circuits. The core die area is around 14.8 mm 2 . The ADC achieves 124 dB dynamic range (A-weighted), -111 dB THD over 20 kHz bandwidth. Total power consumption is\u00a0\u2026", "num_citations": "39\n", "authors": ["493"]}
{"title": "Reuse of addressable system bus for SOC testing\n", "abstract": " Describes a novel test methodology for core-based SOCs. The methodology is based on the use of the existing system bus and/or peripheral bus to access the ports on embedded cores. The microprocessor situated in an SOC can access the addressable terminals of embedded cores to feed test stimuli and to read captured test responses. This novel approach does not need additional bus structures from chip I/Os to cores for the test access mechanism; hence it significantly reduces area overhead and enables the use of the microprocessor's computing power to control the test process of the deeply embedded cores on an SOC.", "num_citations": "39\n", "authors": ["493"]}
{"title": "TOTALLY SELF-CHECKING MOS CIRCUITS UNDER REALISTIC PHYSICAL FAILURES.\n", "abstract": " A new approach is presented to implement totally self-checking (TSC) circuits in nMOS and Domino-CMOS technolgies. Existing realizations of the TSC circuits are at the logic gatelevel based on stuck-at faults. The implementations are made TSC with repsect to realistic physical failures. It is shown how self-checking gate-level designs can be implemented in these technologies so that the same set of input codewords is sufficient to detect stuck-gate faults of transistors in the MOS implementation. Many of the other device and interconnect failures are shown to be mapped to stuck-at faults or stuck-open faults. Conditions and layout rules are given for the detection of those failures which cannot be mapped, notably some diffusion and metal shorts. The fault-secure and code-disjoint properties are also investigated with respect to these physical failures.", "num_citations": "39\n", "authors": ["493"]}
{"title": "Testability-driven statistical path selection\n", "abstract": " In the face of large-scale process variations, statistical timing methodology has advanced significantly over the last few years, and statistical path selection takes advantage of it in at-speed testing. In deterministic path selection, the separation of path selection and test generation is known to require time consuming iteration between the two processes. This paper shows that in statistical path selection, this is not only the case, but also the quality of results can be severely degraded even after the iteration. To deal with this issue, we consider testability in the first place by integrating a satisfiability (SAT) solver, and this necessitates a new statistical path selection method. We integrate the SAT solver in a novel way that leverages the conflict analysis of modern SAT solvers, which provides more than 4X speedup without special optimizations of the SAT solver for this particular application. Our proposed method is based\u00a0\u2026", "num_citations": "38\n", "authors": ["493"]}
{"title": "Physical design of testable VLSI: Techniques and experiments\n", "abstract": " It is shown that the layout of VLSI circuits can affect testability and in some cases reduce the number of faults likely in a design, easing test generation. A method for analyzing circuits at the symbolic layout level and enhancing testability using local transformations is presented. To demonstrate the application of the technique a set of CMOS standard cells was redesigned. The standard cells are used in the MIS synthesis system, allowing the designer to modify interactively designs to perform tradeoff analysis on testable designs. To show the usefulness of the technique, an experiment was performed: example circuits were synthesized, and test vectors were generated and then used in a transistor-level fault simulator. It was found that the modified designs have significantly higher fault coverage than unmodified designs. A strategy for the synthesis of easily testable combinational random logic circuits is presented.", "num_citations": "38\n", "authors": ["493"]}
{"title": "Test generation for digital systems\n", "abstract": " This is a nicely written piece surveying the literature in test generation techniques. The D-algorithm, a classical approach, is covered along with the new random testing and signature analysis techniques. A background in digital systems is necessary for the reader to appreciate the complexity of test generation. Readers familiar with digital system design can read this paper with ease. There are a few typographical errors in the paper:(1) P. 52, Fig. 1.5. 1 (b): No D value is being propagated.(2) P. 69: The authors use M and italicized m for tree structures. What are these__ __ How are they related to the tree structure__ __ An example would be helpful.(3) P. 72: The formula for L needs fixing. The authors propose that the answer for L is 3\u00d7 2 n, while their figures imply it is? 3\u00d7 2 n.(4) P. 85: Ref. REDD83: The word Test is printed as Est. The references are quite good. An excellent source that is not mentioned is a book by\u00a0\u2026", "num_citations": "38\n", "authors": ["493"]}
{"title": "Characterization of standard cells for intra-cell mismatch variations\n", "abstract": " With the adoption of statistical timing across industry, there is a need to characterize all gates/cells in a digital library for delay variation (referred to as statistical characterization). Statistical characterization needs to be performed efficiently with acceptable accuracy as a function of several process and environmental parameter variations. In this paper, we propose an approach to consider intra-cell process mismatch variations to characterize a cell's delay and output transition time (output slew) variations. A straightforward approach to address this problem is to model these mismatch variations by characterizing for each device fluctuation separately. However, the runtime complexity for such characterization becomes of the order of number of devices in the cell and the number of simulations required can easily become infeasible. We analyze the fluctuations in switching and nonswitching devices and their impact on\u00a0\u2026", "num_citations": "37\n", "authors": ["493"]}
{"title": "A comprehensive signature analysis scheme for oscillation-test\n", "abstract": " A low-cost and comprehensive built-in self-test (BIST) methodology for analog and mixed-signal circuits is described. We implement a time-division multiplexing (TDM) comparator to analyze the response of a circuit under test with minimum hardware overhead. The TDM comparator scheme is an effective signature analyzer for on-chip analog response compaction and pass/fail decision. We apply this scheme to an oscillation-test environment and implement a low-cost and comprehensive vectorless BIST methodology for high fault and yield coverage. Our scheme allows a tolerance in the output response, a feature necessary for analog circuits. Both oscillation frequency and oscillation amplitude are measured indirectly to increase the fault coverage. We provide a theoretical analysis of the oscillation that explains why the amplitude measurement is essential. Simulation results demonstrate that the proposed\u00a0\u2026", "num_citations": "36\n", "authors": ["493"]}
{"title": "Verification of transient response of linear analog circuits\n", "abstract": " With the introduction of complex analog designs the need to verify the circuit behavior completely and efficiently cannot be overemphasized. Recognizing the limitation of circuit simulation to achieve this goal, we present a novel approach based on formal techniques developed for digital circuits. Given a transfer function (specification) and its implementation using operational amplifier macro circuits, we verify the correctness of the transient behavior of the implementation over all possible input waveforms. Transforming the specification and the extracted state equations of the implementation from the s-domain to the Z-domain facilitates a digital representation in terms of adders, multipliers and delay elements. These two digitized circuits are then compared using techniques for checking compatibility of states in finite state machines. An example that illustrates the technique is presented.", "num_citations": "36\n", "authors": ["493"]}
{"title": "Concurrent error detection in highly structured logic arrays\n", "abstract": " Two strategies for encoding the inputs and outputs of highly structured logic arrays (HSLAs) are introduced. The two schemes are particularly relevant for concurrent error detection of both permanent and nonpermanent errors in programmable logic arrays (PLAs) and read-only memories (ROMs). The first method of concurrent error detection (CED) is based on a comprehensive fault model and relies on detection of unidirection errors. The second approach relies on a detailed examined of decoder layouts resulting in fault avoidance through layout rules, which avoid failures causing unidirectional errors. Efficient parity techniques are shown to provide a low-overhead solution to concurrent error detection when coupled with appropriate fault-avoidance techniques.", "num_citations": "36\n", "authors": ["493"]}
{"title": "Performance/availability model of shared resource multiprocessors\n", "abstract": " Shared-resource systems are particularly vulnerable to faults which contaminate the shared-resource, and result eventually in system failure. In shared-memory computer systems, resource guardians can provide some protection. A model is developed which incorporates both a performance and availability measure of a general gracefully-degradable shared-resource multiprocessor. Various system parameters are studied and their effects are compared on the basis of a performance/cost ratio. As the number of processors increases or as the reliability of a processor or memory module decreases, some resource protection becomes neccessary. There is a class of systems for which adding resource protection provides no measurable benefit.", "num_citations": "36\n", "authors": ["493"]}
{"title": "Spectral prediction for specification-based loopback test of embedded mixed-signal circuits\n", "abstract": " A traditional specification-based core-level test method is no longer attractive in testing deeply embedded analog and mixed-signal circuits due to limited accessibility and resource issues. In order to overcome such difficulties, loopback testing has been considered as a promising solution when circuits include data conversion units; however its widespread adoption has been hindered due to fault masking, which may cause serious yield loss and test escape. The combination of seriously degraded components in a signal path and overqualified components in another signal path, may result in the overall performance of the loopback path being completely fault-free. This paper presents an efficient loopback test methodology which provides test accuracy equivalent to a traditional specification-based test. In our approach, a traditional loopback scheme is re-configured with an analog filter and an adder\u00a0\u2026", "num_citations": "35\n", "authors": ["493"]}
{"title": "Designing for concurrent error detection in VLSI: Application to a microprogram control unit\n", "abstract": " An integrated approach to the design of a microprogram control unit (MCU) that possesses the distinction of having comprehensive concurrent-error-detection (CED) capability for errors generated by VLSI physical failures is presented. The implementation of the functionally complex single-chip MCU is discussed and the fault model used is explained. Circuit design techniques that have recently been developed for self-checking VLSI systems are introduced. The first critical appraisal based on actual mask-level layouts of custom CED design versus error detection through duplication and comparison, are also presented.", "num_citations": "35\n", "authors": ["493"]}
{"title": "A high throughput FFT processor with no multipliers\n", "abstract": " A novel technique for implementing very high speed FFTs based on unrolled CORDIC structures is proposed in this paper. There has been a lot of research in the area of FFT algorithm implementation; most of the research is focused on reduction of the computational complexity by selection and efficient decomposition of the FFT algorithm. However there has not been much research on using the CORDIC structures for FFT implementations, especially for large, high speed and high throughput FFT transforms, due to the recursive nature of the CORDIC algorithms. The key ideas in this paper are replacing the sine and cosine twiddle factors in the conventional FFT architecture by non-iterative CORDIC micro-rotations which allow substantial (~50%) reduction in read-only memory (ROM) table size, and total removal of complex multipliers. A new method to derive the optimal unrolling/unfolding factor for a desired FFT\u00a0\u2026", "num_citations": "34\n", "authors": ["493"]}
{"title": "Quasi-oscillation based test for improved prediction of analog performance parameters\n", "abstract": " Oscillation based test (OBT) techniques in the past have focussed on detecting the existence of catastrophic and parametric faults. Recent work on predictive oscillation based test (POBT) has used OBT techniques to predict the performance parameters of the circuit under test (CUT). However, this technique cannot be used to predict the performance parameters of the CUT for process parameter variations that cause a loss of oscillation in test mode. This work presents a novel predictive quasi-oscillation based technique (PQOBT) to extend the usability of POBT over a wide range of process parameter variations with minimal test generation overhead.", "num_citations": "34\n", "authors": ["493"]}
{"title": "A hierarchical test generation approach using program slicing techniques on hardware description languages\n", "abstract": " Sequential Automatic Test Pattern Generation is extremely computation intensive and produces acceptable results only on relatively small designs. Hierarchical approaches that target one module at a time and use ad-hoc abstractions for the rest of the design, have shown promising results in reducing the test generation complexity. This paper develops an elegant theoretical basis, based on program slicing, for hierarchical test generation. The technique to systematically obtain a \u201cconstraint slice\u201d for each embedded module under test within a design, is described in detail. The technique has been incorporated in an automated tool for Verilog designs, and results on large benchmark circuits show the significant benefits of the approach.", "num_citations": "34\n", "authors": ["493"]}
{"title": "Validating PowerPC microprocessor custom memories\n", "abstract": " Due to the high cost of correcting errors in a final product, there is a growing impetus in industry towards methodologies that can yield correct designs in the first manufacturing run. Design validation methodologies that combine simulation techniques with formal reasoning can be effective in ensuring correct operation of software and hardware systems. We show why simulation is necessary to complement formal mathematical reasoning in verifying certain classes of custom designed circuits. We present a validation methodology for PowerPC custom memories based on symbolic simulation.", "num_citations": "34\n", "authors": ["493"]}
{"title": "Functional verification of the Equator MAP1000 microprocessor\n", "abstract": " The Advanced VLIW architecture of the Equator MAP1000 processor has many features that present significant verification challenges. We describe a functional verification methodology to address this complexity. In particular, we present an efficient method to generate directed assembly tests and a novel technique using the processor itself to control self-tests and check the results at speed using native instructions only. We also describe the use of emulation in both pre-silicon and post-silicon verification stages.", "num_citations": "34\n", "authors": ["493"]}
{"title": "RNA editing: the creation of nucleotide sequences in mRNA\u2014a minireview\n", "abstract": " RNA editing changes the nucleotide sequence of mRNAs that are encoded in genes which contain the sequences in an abbreviated form. Editing adds uridines that are not encoded in the gene to the transcripts and less frequently removes encoded uridines. The process appears to be posttranscriptional and to proceed in the 3\u2032 -to-5\u2032 direction. Some sites may undergo multiple editings until the final sequence is produced ; in some cases uridines may be added and subsequently removed. A general hypothesis is proposed that predicts a series of reactions that may occur in association with a macromolecular complex, the editosome, which interacts with a multinucleotide region.", "num_citations": "34\n", "authors": ["493"]}
{"title": "CHAMP: Concurrent hierarchical and multilevel program for simulation of VLSI circuits\n", "abstract": " This paper reports on the development, implementation and applica-This paper discusses the design and implementation of a hierarchical switch-level simulator for complex digital circuits. The hierarchy is exploited to reduce the memory requirements of the simulation, thus allowing the simulation of circuits that are too large to simulate at the fiat level. The algorithm used in the simulator opcrates directly on the hierarchical circuit description. Speedup is obtained through the use of high level models. The simulator is implemented on a SUN workstation and has been used to simulate a switch level description of the Motorola 68000 microprocessor.", "num_citations": "34\n", "authors": ["493"]}
{"title": "Memory system design for tolerating single event upsets\n", "abstract": " This paper presents a new memory system design which employs fault-tolerant design techniques for tolerating errors due to single event radiation upsets. The classical Triple Modular Redundancy (TMR) technique has extremely high insertion cost (a factor of 3 to 4); duplication techniques can detect, but not correct, errors and have a factor of 2 to 3 insertion cost. Data encoding techniques have low cost but can correct only stored data, not control, errors. What is needed therefore is an effective mixture of known and new techniques to achieve adequately high reliability at minimum cost. The proposed memory system design uses coding, control duplication, and scrubbing for tolerating soft errors from single event upsets. This memory permits the use of less costly conventional unhardened memory technology and has a very low insertion cost of approximately 25% for achieving fault tolerance. Furthermore, events\u00a0\u2026", "num_citations": "34\n", "authors": ["493"]}
{"title": "TEST GENERATION FOR GENERAL MICROPORCESSOR ARCHITECTURES\n", "abstract": " Sauf mention contraire ci-dessus, le contenu de cette notice bibliographique peut \u00eatre utilis\u00e9 dans le cadre d\u2019une licence CC BY 4.0 Inist-CNRS/Unless otherwise stated above, the content of this bibliographic record may be used under a CC BY 4.0 licence by Inist-CNRS/A menos que se haya se\u00f1alado antes, el contenido de este registro bibliogr\u00e1fico puede ser utilizado al amparo de una licencia CC BY 4.0 Inist-CNRS", "num_citations": "34\n", "authors": ["493"]}
{"title": "Test generation for crosstalk effects in VLSI circuits\n", "abstract": " Crosstalk in VLSI circuits results from parasitic capacitances between interconnect lines. The more signal pathways close to each other on the chip, the greater the coupling effect. Most current timing analyzers, however, have not addressed the effect. We investigated two crosstalk effects: the Crosstalk Glitch (CTG) and the Crosstalk Delay (CTD). The CTG effect can be provoked when line drivers are unbalanced and the coupling capacitance is dominant over the ground capacitances. The signal duration of the CTG increases as the clock transition time decreases. The CTD effect can be provoked regardless of the balancing condition of the drivers. We developed an algorithm, ATEG (Automatic Test Extractor for Glitch), which can generate test vectors to activate and propagate CTG signals by employing several new techniques such as simultaneous gate input assignment, optimized backtracking, and dynamic signal\u00a0\u2026", "num_citations": "33\n", "authors": ["493"]}
{"title": "AMBIANT: Automatic generation of behavioral modifications for testability\n", "abstract": " The paper discusses techniques that help a designer to consider testability features early in the design cycle. The behavioral specification of a design is used to perform high level testability analysis, based on which behavioral modifications for testability are suggested to the designer. Results show that the overhead for incorporating these modifications is minimal.< >", "num_citations": "33\n", "authors": ["493"]}
{"title": "Sequential redundancy identification using verification techniques\n", "abstract": " Access Control Requirements (ACRs) are software requirements about limiting privileges of users from accessing sensitive software resources. ACRs often interact with functional requirements (FRs), conform to an access control model, are constrained by authorization constraints of organizations, and change frequently. Current requirements modeling technologies, such as use case modeling approaches, object-oriented analysis, and feature-oriented approaches, are initially designed to model FRs, and cannot explicitly and efficiently model the ACRs. In real world, it is still very difficult for the developers to model and evolve the complex and changeful ACRs of large-scale software applications while ensuring the consistency between ACRs and FRs. This paper analyzes the challenges of modeling ACRs, presents a feature-based approach to modeling the ACRs to overcome them, and illustrates the approach in\u00a0\u2026", "num_citations": "33\n", "authors": ["493"]}
{"title": "Concurrent fault diagnosis in multiple processor systems\n", "abstract": " CiNii \u8ad6\u6587 - Concurrent fault diagnosis in multiple processor systems CiNii \u56fd\u7acb\u60c5\u5831\u5b66 \u7814\u7a76\u6240 \u5b66\u8853\u60c5\u5831\u30ca\u30d3\u30b2\u30fc\u30bf[\u30b5\u30a4\u30cb\u30a3] \u65e5\u672c\u306e\u8ad6\u6587\u3092\u3055\u304c\u3059 \u5927\u5b66\u56f3\u66f8\u9928\u306e\u672c\u3092\u3055\u304c\u3059 \u65e5\u672c\u306e\u535a\u58eb \u8ad6\u6587\u3092\u3055\u304c\u3059 \u65b0\u898f\u767b\u9332 \u30ed\u30b0\u30a4\u30f3 English \u691c\u7d22 \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u9589\u3058\u308b \u30bf\u30a4\u30c8\u30eb \u8457\u8005\u540d \u8457\u8005ID \u8457\u8005\u6240\u5c5e \u520a\u884c\u7269\u540d ISSN \u5dfb\u53f7\u30da\u30fc\u30b8 \u51fa\u7248\u8005 \u53c2\u8003\u6587\u732e \u51fa\u7248\u5e74 \u5e74\u304b\u3089 \u5e74\u307e\u3067 \u691c\u7d22 \u691c\u7d22 \u691c\u7d22 Concurrent fault diagnosis in multiple processor systems BANERJEE P. \u88ab \u5f15\u7528\u6587\u732e: 1\u4ef6 \u8457\u8005 BANERJEE P. \u53ce\u9332\u520a\u884c\u7269 Proc. 16th Symp. Fault Tolerant Comput Proc. 16th Symp. Fault Tolerant Comput, 298-303, 1986 \u88ab\u5f15\u7528\u6587\u732e: 1\u4ef6\u4e2d 1-1\u4ef6\u3092 \u8868\u793a 1 Checking Scheme for ABFT Systems Based on Modified PD Graph under an Error Generation / Propagation Model PARK Choon-sik , KANEKO Mineo IEICE transactions on fundamentals of electronics, communications and computer sciences 82(6), 1002-1008, 1999-06-25 \u53c2\u8003\u6587\u732e9\u4ef6 Tweet \u5404\u7a2e\u30b3\u30fc\u30c9 NII(/\u2026", "num_citations": "33\n", "authors": ["493"]}
{"title": "Design of testable structures defined by simple loops\n", "abstract": " A methodology is given for generating combinational structures from high-level descriptions (using assignment statements, \"if' statements, and single-nested loops) of register-transfer (RT) level operators. The generated structures are cellular, and are interconnected in a tree structure. A general algorithm is given to test cellular tree structures with a test length which grows only linearly with the size of the tree. It is proved that this test length is optimal to within a constant factor. Ways of making the structures self-checking are also indicated.", "num_citations": "33\n", "authors": ["493"]}
{"title": "Low cost schemes for fault tolerance in matrix opebations with processor abbays\n", "abstract": " Matrix encoding schemes are proposed to detect and correct errors when matrix operations are performed using processor arrays. The method proposed assumes that failures are confined to a single processor. Such a fault model covers a broad class of faults. This method is not only applicable to errors caused by permanent faults but also to transient errors. Two processor array architectures for matrix multiplication are investigated and compared from a fault-tolerance viewpoint; it is shown that only small redundancy ratios - O(1/n) of hardware and O(log 2 (n)/n) of time, are required for processor array systems to achieve reliable matrix operations.", "num_citations": "32\n", "authors": ["493"]}
{"title": "Portable parallel logic and fault simulation\n", "abstract": " Consideration is given to the use of general-purpose multiprocessors for various simulation tasks. The aims of the work are to define a general framework for the parallel simulation of digital systems and to develop and evaluate tools for logic and fault simulation that have a good cost-performance ratio. Specifically, a novel partitioning approach is introduced and used as the basis for the parallel logic and fault simulation of synchronous gate-level designs. Performance experiments with prototype implementations on a message passing and a shared memory machine give promising results, in particular for fault simulation.<>", "num_citations": "32\n", "authors": ["493"]}
{"title": "A unified approach to concurrent error detection in highly structured logic arrays\n", "abstract": " CiNii \u8ad6\u6587 - A unified approach to concurrent error detection in highly structured logic arrays CiNii \u56fd\u7acb\u60c5\u5831\u5b66\u7814\u7a76\u6240 \u5b66\u8853\u60c5\u5831\u30ca\u30d3\u30b2\u30fc\u30bf[\u30b5\u30a4\u30cb\u30a3] \u65e5\u672c\u306e\u8ad6\u6587\u3092\u3055\u304c\u3059 \u5927\u5b66\u56f3\u66f8\u9928\u306e\u672c\u3092\u3055\u304c\u3059 \u65e5\u672c\u306e\u535a\u58eb\u8ad6\u6587\u3092\u3055\u304c\u3059 \u65b0\u898f\u767b\u9332 \u30ed\u30b0\u30a4\u30f3 English \u691c\u7d22 \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u9589\u3058\u308b \u30bf\u30a4\u30c8\u30eb \u8457\u8005\u540d \u8457\u8005ID \u8457\u8005\u6240\u5c5e \u520a\u884c\u7269\u540d ISSN \u5dfb\u53f7\u30da\u30fc\u30b8 \u51fa\u7248\u8005 \u53c2\u8003\u6587\u732e \u51fa\u7248\u5e74 \u5e74\u304b\u3089 \u5e74\u307e\u3067 \u691c\u7d22 \u691c\u7d22 \u691c\u7d22 A unified approach to concurrent error detection in highly structured logic arrays FUCHS WK \u88ab\u5f15\u7528\u6587\u732e: 1\u4ef6 \u8457\u8005 FUCHS WK \u53ce\u9332\u520a\u884c\u7269 Proc. FTCS-14 Proc. FTCS-14, 4-9, 1984 \u88ab\u5f15\u7528\u6587\u732e: 1\u4ef6\u4e2d 1-1\u4ef6\u3092 \u8868\u793a 1 Testing for the Programming Circuit of SRAM-Based FPGAs MICHINISHI Hiroyuki , YOKOHIRA Tokumi , OKAMOTO Takuji , INOUE Tomoo , FUJIWARA Hideo IEICE transactions on information and systems 82(6), 1051-1057, 1999-06-25 \u53c2\u8003\u6587\u732e20\u4ef6 Tweet \u5404\u7a2e\u30b3\u30fc\u30c9 NII\u8ad6\u6587ID(NAID) 10006474240 \u8cc7\u6599\u7a2e\u5225 \u4f1a\u8b70\u8cc7\u6599 \u306b/| \u2026", "num_citations": "32\n", "authors": ["493"]}
{"title": "Hierarchical fault modeling for linear analog circuits\n", "abstract": " This paper presents a hierarchical fault modeling approach for catastrophic as well as out-of-specification parametric faults in analog circuits. These include both, ac and dc faults in passive as well as active components. The fault models are based on functional error characterization. Case studies based on CMOS and nMOS operational amplifiers are discussed, and a full listing of derived behavioral fault models is presented. These fault models are then mapped to the faulty behavior at the macro-circuit level. Application of these fault models in an efficient fault simulator for analog circuits is also described.", "num_citations": "31\n", "authors": ["493"]}
{"title": "A study of faulty signatures using a matrix formulation\n", "abstract": " The authors describe a novel matric method of analyzing faulty signatures from both single-input and multiple-input signature registers (MISRs). In particular, the information contained in the faulty signature of an MISR is investigated. The results indicate that the proposed method can be used to locate the faulty cycle or channel(s) of the failing net. The formulation has the advantage of simplicity over the polynomial representation and can be used to determine the relation between an error and its syndrome. The results provide new insights into the application of MISRs to built-in self-test.< >", "num_citations": "31\n", "authors": ["493"]}
{"title": "Program slicing for hierarchical test generation\n", "abstract": " Sequential Automatic Test Pattern Generation (ATPG) is extremely computation intensive and produces good results only on relatively small designs. This paper develops an elegant theoretical basis, based on program slicing, for hierarchical ATPG which targets one module at a time and abstracts the rest of the design. The technique for obtaining a \"constraint slice\" for each embedded Module Under Test (MUT) within a design is described in detail. The technique has been incorporated in an automated tool for designs described in Verilog, and results on large benchmark circuits show the significant benefits of the approach.", "num_citations": "30\n", "authors": ["493"]}
{"title": "Fault tolerance techniques for highly parallel signal processing architectures\n", "abstract": " This overview paper describes techniques for fault tolerance which can be applied to highly parallel signal processing architec-tures. Classical techniques are outlined and shown applicable to memories and data communications. The recent approach of algorithm-based fault tolerance, which tailors the fault tolerance to the systolic algorithm and processor architecture, is shown to be a natural one for such systems. Various data encoding techniques and resulting fault-tolerant systems are described and critiqued.", "num_citations": "30\n", "authors": ["493"]}
{"title": "Fault collapsing techniques for MOS VLSI circuits\n", "abstract": " The time needed to generate complete tests for complex circuits can be reduced significantly through the use of fault collapsing techniques where equivalent faults as well as dominan\u0131 faults (faults which will be detected by any test for some other fault) are removed from consideration. This paper develops two fault collapsing techniques (inter-gate fault collapsing and intra-gale fault collapsing) for both nMOS and CMOS circuits including line stuck-at faults, transistor stuck short faults, and transistor stuck open faults. In inter-gale fault collapsing, check point procedures are developed for single and multiple faults in nMOS circuits, and for single faults in CMOS circuits by measuring both the voltage and the current. In intra-gale fault collapsing, some single (multiple) stuck open or sluck short faults are collapsed by observing the connection structure of transistors in a network. Procedures are also developed for identifying representative check points. fail (each line may assume one of the three possible states: normal, stuck-at-1, or stuck-at-0, and each transistor may assume one of the three possible states: normal, stuck short, or stuck open), the computation time for test generation and fault simulation will be greatly increased. The natural questions are. what are the check points in an nMOS or CMOS combinational circuit? Is there any technique to futher reduce the number of faulls? Stancil (8) developed a checkpoint theorem for CMOS combinational networks. However, the multiple faull and stuck short fault models are not included in his work. In this paper, we will develop two fault collapsing techniques:(1) inter-gate fault collapsing, and (2) intra-gate\u00a0\u2026", "num_citations": "30\n", "authors": ["493"]}
{"title": "Efficient soft error vulnerability estimation of complex designs\n", "abstract": " Analyzing design vulnerability for soft errors has become a challenging process in large systems with a large number of memory elements. Error injection in a complex system with a sufficiently large sample of error candidates for reasonable accuracy takes a large amount of time. In this paper we describe RAVEN, a statistical method to estimate the outcomes of a system in the presence of soft errors injected into flip-flops, as well as the vulnerability for each memory element. This method takes advantage of fast local simulations for each error injection, and calculates the probabilities for the system outcomes for every possible soft error in a period of time. Experimental results, on an out-of-order processor with SPECINT2000 workloads, show that RAVEN is an order of magnitude faster compared with traditional error injection while maintaining accuracy.", "num_citations": "29\n", "authors": ["493"]}
{"title": "Low cost RF receiver parameter measurement with on-chip amplitude detectors\n", "abstract": " This paper describes the theory and chip measurements of a built-in test technique for RF receivers which uses simple RF amplitude detectors. The method has been used to measure the performance parameters of a 940 MHz RF receiver front-end with a mixer and LNA. The detector has small area overhead with low frequency output. The sampled output waveform is analyzed using an FFT, and the low frequency measurements are used to deduce the conversion gain and Third Order Intercept point (TOI, IIP3) of the receiver. A test chip was fabricated in a commercial 0.18 mum CMOS process. By using two detectors, both the system performance and specifications of discrete components have been accurately measured. Measurement results show accurate prediction of system and component specifications.", "num_citations": "29\n", "authors": ["493"]}
{"title": "Built-In test of RF mixers using RF amplitude detectors\n", "abstract": " This paper describes a low cost, high resolution, built-in test technique for RF mixers which uses a simple RF amplitude detector. The method has been used to predict the performance parameters of a 940 MHz RF mixer. The detector has small area overhead with very low frequency output, which can be sampled at 10MHz. The sampled output waveform is analyzed using an FFT, and the low frequency measurements are used to predict the conversion gain and third order intercept point (TOI) of the mixer. The authors have completed the design and layout of the 940 MHz test mixer with on chip RF detector in the United Micro Electronics Corpora-tion (UMC) 0.18mum CMOS process. Post layout simulation results show accurate prediction of the mixer specifications", "num_citations": "29\n", "authors": ["493"]}
{"title": "A model for the analysis of fault-tolerant signal processing architectures\n", "abstract": " This paper develops a new model, using matrices, for the analysis of fault-tolerant multiprocessor systems. The relationship between processors computing useful data, the output data, and the check processors is defined in terms of matrix entries. Unlike the matrix based models proposed previously for the analysis of digital systems, this model uses only numerical computations rather than logical operations for the analysis of a system. We present algorithms to evaluate the fault detection and location capability of the system. These algorithms are much less complex than the existing ones. We also use the new model to analyze some fault-tolerant architectures proposed for signal processing applications.", "num_citations": "29\n", "authors": ["493"]}
{"title": "Formal verification using bounded model checking: SAT versus sequential ATPG engines\n", "abstract": " Industry is beginning to use Satisfiability (SAT) solvers extensively for formally verifying the correctness of digital designs. In this paper we compare the performance of SAT solvers with sequential Automatic Test Pattern Generation (ATPG) techniques for property verification. Our experimental results on the ISCAS benchmarks as well as a model of the 8085 microprocessor show that, contrary to popular belief, ATPG techniques perform much better than SAT based verification techniques, especially for large designs.", "num_citations": "28\n", "authors": ["493"]}
{"title": "Critical path identification and delay tests of dynamic circuits\n", "abstract": " Dynamic circuit families are commonly used to achieve high operating speeds in recent microprocessor designs. Because of their noise sensitivity, it is necessary to design dynamic circuits accurately to achieve performance goals and avoid problems with noise. Although individual cells can be analyzed effectively, timing verification of the entire design is not easy because of the increased complexity. In this paper, we develop a new approach to find critical paths and generate test vectors for delay test of large dynamic circuits, given information on the path delays of the unit cells. We introduce the concept of \"path gates\" to represent the discharge paths in a dynamic circuit, and have developed an extraction tool (PEAR) to construct the path gates. The critical path analyzer (CRITIC) is used to identify the critical paths and generate delay tests for the integrated units. The technique has been successfully applied to\u00a0\u2026", "num_citations": "28\n", "authors": ["493"]}
{"title": "Dependability evaluation using hybrid fault/error injection\n", "abstract": " This paper presents a new hybrid fault/error injection technique which overcomes the limitations of both software-based and hardware-based approaches. The logic for the hardware fault unit injection circuitry is implemented using field programmable gate arrays, and the software is an extension of FERRARI, the software-based fault injection system. The combination of these techniques allows the incorporation of new capabilities by the use of mechanisms to trigger and synchronize the injection of a fault or error with events in the system. Results of physical fault/error injection experiments on a SPARC1 system are presented. The injection was synchronized to the executing modes and load conditions of the system. These results show that the system behavior is very sensitive to the internal state and load. Therefore, in order to validate the dependability properties of a system, it is imperative to inject faults/errors\u00a0\u2026", "num_citations": "28\n", "authors": ["493"]}
{"title": "A design methodology for software fault injection in embedded systems\n", "abstract": " Existing fault injection tools are tightly coupled to their implementation platforms, are not easy to port, and have dealt primarily with time-sharing systems. This paper addresses the problem of designing software fault injection systems for dependability evaluation of embedded systems and, more speci cally, the implementation of such a fault injector FIESTA (Fault Injection for Embedded System Target Applications). A generic design methodology, which enables quick prototyping of softwareimplemented fault injectors, is presented. Unlike most other design methodologies for fault injection tools, our approach achieves a very fast and e cient implementation of such tools for commercial embedded systems. Our technique is compatible with most existing embedded systems and distributed architectures in the commercial market. The proposed methodology has been implemented through the realization of a fault injection tool (FIESTA) for a commercial real-time MC68040 VME-based system running the VxWorks 5.3 (TORNADO) operating system, Results of fault injection experiments using FIESTA are also presented.We have attempted to use fault injection for two purposes. The rst objective is to evaluate the dependability properties of an embedded system by emulating hardware faults in software. The other function is for evaluating and testing the robustness of the software applications running on such systems. Traditionally, most fault injection tools have attempted to cover the rst objective; recently, there has been a growing interest in using fault injection for\\software robustness\" evaluation and software fault tolerance. This has always been true for\u00a0\u2026", "num_citations": "27\n", "authors": ["493"]}
{"title": "Automatic verification of implementations of large circuits against HDL specifications\n", "abstract": " This paper addresses the problem of verifying the correctness of gate-level implementations of large synchronous sequential circuits with respect to their higher level specifications in a hardware description language (HDL). The verification strategy is to verify containment of the finite state machine (FSM) represented by the HDL description in the gate-level FSM by computing pairs of compatible states. This formulation of the verification problem dissociates the verification process from the specification of initial states, whose encoding may be unknown or obscured during optimization and also enables verification of reset circuitry. To make verification of large circuits with merged data path and control tractable, the concept of strong containment is introduced. This is a conservative approach which exploits correspondence between data path-registers in the two descriptions without requiring any correspondence\u00a0\u2026", "num_citations": "27\n", "authors": ["493"]}
{"title": "Efficient multisine testing of analog circuits\n", "abstract": " An efficient method has been developed for generating test waveforms for linear analog circuits which minimize the test effort and maximize the test confidence. The method makes use of a fault-based automatic test pattern generator (ATPG) to generate a set of test frequencies. A successive gradient method is used to combine these individual sinusoidal signals in a way that maximizes the fault coverage. The compressed waveform can be stored on-chip and used for built-in test of analog circuits.", "num_citations": "27\n", "authors": ["493"]}
{"title": "Efficient parallel algorithms for processor arrays\n", "abstract": " With the advent of VLSI technology, it is possible to provide extremely high but inexpensive computational capability with a system consisting of a large number of identical processors organised in a simple, regular structure. In order to exploit the high computational capability of the arrays, however, it is important to employ an efficient parallel algorithm. In this paper a measure is proposed which can calculate the efficiency of an algorithm performed in a processor array. This measure is used to compare several proposed array architectures for a variety of algorithms. Finally, efficient parallel algorithms for recursive filtering problems, matrix-vector multiplication, and matrix multiplication are also proposed. 7 references.", "num_citations": "27\n", "authors": ["493"]}
{"title": "Verification of processor microarchitectures\n", "abstract": " This paper develops a new abstraction technique for processor microarchitecture validation. An abstract finite-state machine model is derived directly from the processor HDL description. This model, along with information about the instruction set, is used for validation coverage analysis. We also present automatic test generation algorithms for generating sequences for traversing state transition paths and covering snapshot and temporal events.", "num_citations": "26\n", "authors": ["493"]}
{"title": "A unified approach for fault simulation of linear mixed-signal circuits\n", "abstract": " The rapidly evolving role of analog signal processing has spawned off a variety of mixed-signal circuit applications. The integration of the analog and digital circuits has created a lot of concerns in testing these devices. This paper presents an efficient unified fault simulation platform for mixed-signal circuits while accounting for the imprecision in analog signals. While the classical stuck-at fault model is used for the digital part, faults in the analog circuit cover catastrophic as well as parametric defects in the passive and active components. A unified framework is achieved by combining a discretized representation of the analog circuit with the Z-domain representation of the digital part. Due to the imprecise nature of analog signals, an arithmetic distance based fault detection criterion and a statistical measure of digital fault coverage are proposed.", "num_citations": "26\n", "authors": ["493"]}
{"title": "Test generation for iterative logic arrays based on an N-cube of cell states model\n", "abstract": " The authors present a novel approach to the test generation problem for a more general class of two-dimensional iterative logic arrays (ILAs) than considered by previous researchers. For certain ILAs it is possible to find a test set whose size remains fixed irrespective of the size of the ILA, while for others it varies with array size. Given an arbitrary ILA cell truth table and a cell interconnection structure, the goal is to determine if a fixed-size test can be found. If not, then a test set whose size grows as slowly as possible with the size of the array should be found. The authors propose a new model, called the n-cube of cell states model, for representing the cell truth table and interconnection structure. The test generation problem is shown to be related to certain properties of cycles in a set of graphs obtained from this model. By careful analysis of these cycles, efficient testing schedules can be obtained. The proposed\u00a0\u2026", "num_citations": "26\n", "authors": ["493"]}
{"title": "Concurrent error detection in VLSI interconnection networks\n", "abstract": " Comprehensive VLSI fault models are proposed for three broad classes of interconnection networks between multiple processors and multiple memory modules. System-level algorithms are given for concurrent detection of errors produced by these faults during the normal use of the networks. The proposed algorithms are shown to be applicable to the three classes of interconnection networks with minimal changes in their classical design. The algorithms are appropriate for the broad classes of permanent and transient faults predominant in dense VLSI and wafer-scale integration with a minimal amount of network redundancy required for implementation.", "num_citations": "26\n", "authors": ["493"]}
{"title": "Critical path selection for delay test considering coupling noise\n", "abstract": " Identifying the set of real critical paths of a circuit is an important step in delay testing. Since path delays are vector dependent, the set of critical paths selected depends on the vectors assumed when estimating the path delays. To find the real critical paths, it is important to consider the effect of dynamic (vector dependent) delay effects such as coupling noise, supply noise etc. during path selection. In this work a methodology to incorporate the effect of coupling noise during path selection is described. For any given path, both logic and timing constraints are extracted and a constrained optimization problem is formulated to estimate the maximum path delay in the presence of coupling noise.", "num_citations": "25\n", "authors": ["493"]}
{"title": "Parallel loopback test of mixed-signal circuits\n", "abstract": " Parallel testing of mixed-signal circuits has been considered a difficult task due to the limited resources in generating and analyzing multiple analog signals. A number of methods have been proposed to perform parallel testing of mixed-signal circuits using built-in test circuitry; however, these techniques are vulnerable to fault masking issues which may degrade the test accuracy. This paper presents an efficient parallel test algorithm for mixed-signal circuits based on a loopback test method. Multiple DUTs (devices under test) are loopbacked externally on a loadboard which is loaded with a simple analog adder and an RMS detector. The performance parameters of each DUT are calculated separately from the composite responses, while removing the effect of fault masking. Parallelism is increased by sharing common test equipment and a DUT loadboard among the multiple DUTs. The mathematical theory and\u00a0\u2026", "num_citations": "25\n", "authors": ["493"]}
{"title": "An efficient filter-based approach for combinational verification\n", "abstract": " Combinational verification is a co-NP complete problem. However, in reality, several techniques exist which perform reasonably well on many practical circuits. Also, it is often found that while one technique efficiently verifies a given circuit it fails badly on another circuit, whereas a certain other technique is efficient on the latter circuit but cannot handle the former circuit. Therefore, clearly, a robust verification methodology cannot depend on any single technique. Our goal in this research is to build a verification methodology whose performance is more immune to circuit variations. We have developed a methodology where several fundamentally different techniques can be combined to provide efficient heuristic solutions to combinational verification, and possibly other intractable problems as well. Such an integrated methodology is far more robust and efficient on a majority of combinational verification problems than\u00a0\u2026", "num_citations": "25\n", "authors": ["493"]}
{"title": "Functional abstraction of logic gates for switch-level simulation\n", "abstract": " Switch-level simulation has become a common means accurate modeling of MOS circuit behavior. In this paper, the authors propose a new method for detecting logic gate implementation and accurately modeling their switch-level behavior. The functional abstraction replaces logic gate implementation in the switch-level description with an accurate high-level model which incorporates all relevant switch-level phenomena. The switch-level accuracy of the simulation is, therefore, preserved. However, since the gate implementations are modeled at a higher, more abstract level, the simulation speed is greatly increased. The functional abstraction is automatic and completely transparent to the user. Detection of a gate is determined by expressing the logic function of a transistor network in the sum-of-product notation and is not limited to a specific design style. The proposed algorithms have been implemented and\u00a0\u2026", "num_citations": "25\n", "authors": ["493"]}
{"title": "Hierarchical design and analysis of fault-tolerant multiprocessor systems using concurrent error detection\n", "abstract": " A composition technique for building large fault-tolerant systems hierarchically using the concept of checks at different levels in the hierarchy is described. A small system of known fault detectability and locatability is replicated several times, and new checks are added at the next higher level. Such checks at different levels can be introduced into most of the existing multiprocessor systems. An analysis technique based on a matrix model is developed. Relationships between the fault detectability and locatability of a basic system are derived, and the corresponding values of the complete system are computed hierarchically. Finally, the techniques are extended to complex systems in which individual processors produce multiple sets of data elements.< >", "num_citations": "25\n", "authors": ["493"]}
{"title": "Budget-dependent control-flow error detection\n", "abstract": " The problem of detection of control flow errors in software has been studied extensively in literature and many detection techniques have been proposed. These techniques typically have high memory and performance overheads and hence are unusable for real-time embedded systems which have tight memory and performance budgets. This paper presents two algorithms by which the overheads associated with any detection technique can be lowered by trading off fault coverage. These algorithms are generic and can be applied to any detection technique. They can be applied either individually or cumulatively. The algorithms are validated on a previously proposed detection technique using SPEC benchmark programs. Fault injection experiments suggest that massive savings in overheads can be achieved using the algorithms with just a minor drop off in fault coverage.", "num_citations": "24\n", "authors": ["493"]}
{"title": "Improved verification of hardware designs through antecedent conditioned slicing\n", "abstract": " Static slicing has shown itself to be a valuable tool, facilitating the verification of hardware designs. In this paper, we present a sharpened notion, antecedent conditioned slicing that provides a more effective abstraction for reducing the size of the state space. In antecedent conditioned slicing, extra information from the antecedent is used to permit greater pruning of the state space. In a previous version of this paper, we applied antecedent conditioned slicing to safety properties of the form G(antecedent  \u21d2  consequent) where antecedent and consequent were written in propositional logic. In this paper, we use antecedent conditioned slicing to handle safety and bounded liveness property specifications written in linear time temporal logic. We present a theoretical justification of our technique. We provide experimental results on a Verilog RTL implementation of the USB 2.0 functional core, which is a large\u00a0\u2026", "num_citations": "24\n", "authors": ["493"]}
{"title": "Jitter decomposition by time lag correlation\n", "abstract": " Jitter decomposition is important for accurately deriving the bit-error-rate in a system and for aiding in identifying the root causes of jitter. Limits of conventional solutions to this problem are discussed and a new approach to overcome the limitations is proposed. Our method uses time lag correlation functions to decompose different jitter components. The approach is validated by hardware measurements by applying the techniques to a phase locked loop into which jitter is injected", "num_citations": "24\n", "authors": ["493"]}
{"title": "EMAX-An automatic extractor of high-level error models\n", "abstract": " Thzs paper presents EMAY, a Hzgh-Level Error Model Automatzc Extractor EMAX szmulates all user-selected, low-level faults (at the gate and/or the swztch level) that may occur tmzde a procebsor chzp, and generates the error output patterns produced by the faulty czrcuzts. These generated patterns are used to extract hzgh-level error models. LVhen these error models are further analyzed, a sequence of znstructzons can be derzved whzch, when executed, produce the same error patterns as those obtatned when szmulatzng the hardware wzth low level faults When thzs sequence of znstructzons zs fed to a software fault and error znjectzon tool, zt wtll allow the use of accurate and co~ teffectzve hzgher level fault/error znlectzon patterns for ualtdatzng the dependabzlzty propertzes of a system. Error models extracted for an example processor are preserlted and are analyzed", "num_citations": "24\n", "authors": ["493"]}
{"title": "A numerical technique for the hierarchical evaluation of large, closed fault-tolerant systems\n", "abstract": " This paper describes a novel approach for evaluating the reliability of large fault-tolerant systems. The design hierarchy of the system is preserved during the evaluation, allowing large systems to be analyzed. Semi-Markov models are used at each level in the hierarchy, and a numerical technique is used to combine models from a given level for use at the next level. Different values of parameters, such as coverage, can then be used appropriately at any level, resulting in a much more accurate prediction of reliability. The proposed technique has been validated through comparison with analytical calculations, results from existing tools and Monte-Carlo simulation.", "num_citations": "24\n", "authors": ["493"]}
{"title": "Test considerations for BiCMOS logic families\n", "abstract": " The testability of various BiCMOS logic families is examined. For each design style the adequacy of stuck-at and quiescent current testing is explored. It is shown that while stuck-at and I/sub ddq/ testing can cover many faults there are still a large number of faults that are not detectable. Using the results presented the various logic families can be ranked by testability when being evaluated for an application.< >", "num_citations": "24\n", "authors": ["493"]}
{"title": "Transistor-level test generation for physical failures in CMOS circuits\n", "abstract": " A new methodology is proposed for generating tests at the transistor level for realistic failures including bridging faults, and transistor gate-to-source short and gate-to-drain short faults in CMOS combinational circuits. A new tree model for a fault-free CMOS complex gate is used to propagate errors due to faults with much less computation time. The technique adapts the tree structure representation for MOS gates to the D-Algorithm.", "num_citations": "24\n", "authors": ["493"]}
{"title": "Efficient model checking of hardware using conditioned slicing\n", "abstract": " In this work, we present an abstraction based property verification technique for hardware using conditioned slicing. We handle safety property specifications of the form G (a n t e c e d e n t\u21d2 c o n s e q u e n t). We use the antecedent of the properties to create our abstractions, Antecedent Conditioned Slices. We extend conditioned slicing to Hardware Description Languages (HDLs). We provide a theoretical foundation for our conditioned slicing based verification technique. We also present experimental results on the Verilog RTL implementation of the USB 2.0. We demonstrate very high performance gains achieved by our technique when compared to static program slicing, using state-of-the-art model checkers.", "num_citations": "23\n", "authors": ["493"]}
{"title": "Hierarchical multi-level fault simulation of large systems\n", "abstract": " This article discusses an approach for hierarchical multilevel fault simulation for large systems described at the transistor, gate, and higher levels. The approach reduces the memory requirement of the simulation drastically, thus allowing the simulation of circuits that are too large to simulate at one flat level on typical engineering workstations. This is achieved by exploiting the regularity and modularity found in a hierarchical circuit description that contains many repeated substructures. The hierarchical setup also allows flexible multilevel simulation: behavioral models can replace subcircuits at any level of the hierarchy for accelerated simulation. The simulation algorithms are at the switch level so that general MOS digital designs with bidirectional signal flow can be handled, and both stuck-at and transistor faults are treated accurately. The approach has been implemented in the hierarchical logic and fault\u00a0\u2026", "num_citations": "23\n", "authors": ["493"]}
{"title": "Average interconnection length and interconnection distribution based on Rent's rule\n", "abstract": " In this paper we show that it is necessary to utilize different partitioning coefficients in interconnection length analyses which are based on Rent's rule, depending on whether one- or two-dimensional placement strategies are used. \u03b2, the partitioning coefficient in the power-law relationship \u03b1\u03b2/sup \u03b2/, provides a measure of the number of interconnections which cross a boundary enclosing \u03b2 blocks. The partitioning coefficients are \u03b2=\u03c1/2 and \u03b2=\u03c1 for two- and one-dimensional arrays, respectively, where \u03c1 is the experimental coefficient of the Rent relationship \u03a4=\u03b1\u03b2/sup \u03c1/. Based on these separate partitioning coefficients, an average interconnection length prediction is presented for rectangular arrays that outperforms existing predictions. Examples are given to support this theory.", "num_citations": "23\n", "authors": ["493"]}
{"title": "Jitter decomposition in high-speed communication systems\n", "abstract": " Jitter impairs the bit-error rate in high-speed communication systems. Jitter decomposition is important for accurately deriving the total jitter in a system and for aiding in identifying the root causes of jitter. We extend a previous approach for jitter decomposition in clock signals is to enable separation of correlated and uncorrelated jitter in both data and clock signals. We use time lag correlation functions with special test patterns to estimate the characteristic parameters of different jitter components such as peak-to-peak value of DDJ and RMS value of RJ. Our approach can be implemented using only one-shot capture instead of multiple captures to average out the uncorrelated jitter from the correlated jitter. Hardware measurements are presented to validate the proposed technique.", "num_citations": "22\n", "authors": ["493"]}
{"title": "A comprehensive TDM comparator scheme for effective analysis of oscillation-based test\n", "abstract": " We propose a comprehensive built-in self-test (BIST) methodology for analog and mixed-signal circuits. A time-division multiplexing (TDM) comparator scheme was proposed as an effective signature analyzer for on-chip analog response compaction and pass/fail decision with minimum hardware overhead. By applying this scheme to the oscillation-based test, the oscillation frequency can be measured indirectly as well as the oscillation amplitude to increase the fault coverage. The experimental results demonstrate that the proposed scheme can significantly reduce test time of the oscillation-based test with higher fault coverage.", "num_citations": "22\n", "authors": ["493"]}
{"title": "Automatic generation of behavioral models from switch-level descriptions\n", "abstract": " This paper discusses the automatic generation of high-level software models from switch-level circuit descriptions. The proposed algorithms operate directly on the hierarchical description, and incorporate information about the design such as the structure, regularity, functionality, and control signals in the generation process. New algorithms are proposed and have been implemented for combinational modules and bus structures. A significant speedup has been obtained for these modules of a commercially available chip.", "num_citations": "22\n", "authors": ["493"]}
{"title": "A combinatorial solution to the reliability of interwoven redundant logic networks\n", "abstract": " A combinatorial procedure is given to calculate the reliability of an interwoven redundant logic network to any desired degree of accuracy. The procedure consists of enumerating combinations of gate failure which are tolerated by the redundant network, and is explained with reference to a quadded logic network. Since the exact reliability calculation might be too time consuming for large networks, a formula is given for a lower bound which can be used in conjunction with the exact method to give a very accurate reliability figure with a comparatively small computation time.", "num_citations": "22\n", "authors": ["493"]}
{"title": "Pseudorandom test for nonlinear circuits based on a simplified Volterra series model\n", "abstract": " Pseudorandom test of analog and mixed-signal circuits provides a low-cost test solution; however, its application has been restricted to linear circuit testing. This paper presents an efficient pseudorandom test method for nonlinear circuits. The method uses a simplified Volterra series model to characterize nonlinear behaviors of devices under test (DUTs) accurately with a low complexity algorithm. A multilevel pseudorandom sequence is used to excite DUTs over a wide range of frequencies and generate a spread-spectrum output response. The cross spectral density of the input test pattern and output response is computed to estimate the parameters of Volterra series and predict the performance of DUTs. In addition, the method can be used to compensate nonlinear errors of DUTs and improve performance. The mathematical background and simulation results are presented to validate the proposed method", "num_citations": "21\n", "authors": ["493"]}
{"title": "False timing path identification using ATPG techniques and delay-based information\n", "abstract": " A well-known problem in timing verification of VLSI circuits using static timing analysis tools is the generation of false timing paths. This leads to a pessimistic estimation of the processor speed and wasted engineering effort spent optimizing unsensitizable paths. Earlier results have shown how ATPG techniques can be used to identify false paths efficiently [6],[9], as well as how to bridge the gap between the physical design on which the static timing analysis is based and the test view on which ATPG technique is applied to identify false paths [9]. In this paper, we will demonstrate efficient techniques to identify more false timing paths by utilizing information from an ordered list of timing paths according to the delay information. More than 10% of additional false timing paths out of the total timing paths analyzed are identified compared to earlier results on the MPC7455, a Motorola processor executing to the PowerPC\u2122.", "num_citations": "21\n", "authors": ["493"]}
{"title": "Design and development paradigm for industrial formal verification CAD tools\n", "abstract": " CAD tool designers have given priority to providing features that will let circuit and logic designers use this custom-memory formal verification and analysis tool without a steep learning curve. This article discusses a few fundamental design decisions behind the successful deployment of a second-generation formal custom-memory equivalence-checking tool, Versys2, in the PowerPC design flows. The Versys2 symbolic simulator was developed at Motorola for verifying equivalence between register-transfer-level (RTL) designs and custom transistor circuit schematics.", "num_citations": "21\n", "authors": ["493"]}
{"title": "Implementing forward recovery using checkpoints in distributed systems\n", "abstract": " This paper describes the implementation of a forward recovery strategy in a Sun NFS environment. The implementation is based on the concept of lookahead execution with rollback validation. It uses replicated tasks executing on different processors for forward recovery and checkpoint comparison for error detection. In the experiment described, the recovery strategy has nearly error-free execution time and an average redundancy lower than TMR.", "num_citations": "21\n", "authors": ["493"]}
{"title": "SNEL: A switch-level simulator using multiple levels of functional abstraction\n", "abstract": " A novel switch-level simulator, called SNEL, is presented. The SNEL simulator preprocesses the circuit description to abstract its functionality prior to simulation. Functional abstraction is concisely defined in terms of the functional domain and the functional application of circuit constructs. SNEL uses four algorithms that operate on levels ranging from single circuit elements to multiple DC-connected components. Since the functional abstraction preserves the complete functionality of the circuit, the accuracy of the simulation is maintained. However, SNEL models the circuit at a higher and more abstract level, which increases its simulation speed. The presented algorithms were implemented and tested on commercial designs. Without functional abstraction, the simulation speed of SNEL is competitive with current simulators. When functional abstraction was used, the simulation speed increased by more than an order\u00a0\u2026", "num_citations": "21\n", "authors": ["493"]}
{"title": "A model for the analysis design and comparison of fault-tolerant WSI architectures\n", "abstract": " A new model has been proposed for fault-tolerant multiprocessor systems, especially those using algorithm-based fault tolerance techniques (ABFT). Design of complicated ABFT systems was considered to be intractable by previous researchers. In this paper we propose a systematic procedure to design such systems. Complexity of the proposed techniques are less than the previously derived bounds.", "num_citations": "21\n", "authors": ["493"]}
{"title": "Knowledge based test generation for VLSI circuits\n", "abstract": " Test generation for a VLSI circuit is a complex prob-lem. Previous attempts to use hierarchical representations have not been able solve the complexity problem in search, and they have been unable to use appropriate knowledge embedded in the hierarchical design of the circuit. This paper introduces an algo-rithm for test generation which differs from previous approaches in two ways:(1) The hierarchical representation of the circuit stores the knowledge of how to backtrace and propagate signals through a component of the circuit. This permits us to fully exploit the hierarchical structure of the device to reduce the search space, and (2) The components and their interconnections are separated into data and control types, and the algorithm uses different heuristics to propagate and backtrace faults in the con-trol and data components and thus reduces the search space. the stack pointer and condition registers, cannot be observed or controlled directly. While functional testing (11-15) does use information about data and control paths, it lacks the detailed structural information of the circuit and therefore results in very long tests and poor fault coverage for the control path.", "num_citations": "21\n", "authors": ["493"]}
{"title": "Tolerating soft errors in processor cores using clear (cross-layer exploration for architecting resilience)\n", "abstract": " We present cross-layer exploration for architecting resilience, a first of its kind framework which overcomes a major challenge in the design of digital systems that are resilient to reliability failures: achieve desired resilience targets at minimal costs (energy, power, execution time, and area) by combining resilience techniques across various layers of the system stack (circuit, logic, architecture, software, and algorithm). This is also referred to as cross-layer resilience. In this paper, we focus on radiation-induced soft errors in processor cores. We address both single-event upsets and single-event multiple upsets in terrestrial environments. Our framework automatically and systematically explores the large space of comprehensive resilience techniques and their combinations across various layers of the system stack (586 cross-layer combinations in this paper), derives cost-effective solutions that achieve resilience targets at\u00a0\u2026", "num_citations": "20\n", "authors": ["493"]}
{"title": "Program slicing for ATPG-based property checking\n", "abstract": " This paper presents a novel technique for abstracting designs in order to increase the efficiency of formal property checking. Bounded Model Checking (BMC), using Satisfiability (SAT) techniques, are beginning to be widely used for checking properties of designs. Recent approaches using sequential ATPG techniques, which harness the structural information of the design, have been shown to perform better than SAT-based BMC. However, these techniques require an effective methodology to deal with the size of commercial designs. A program slicing methodology that has been shown to accelerate sequential ATPG is adapted and integrated into an ATPG-based BMC framework. Furthermore, a generalization of the ATPG-based approach, which checks for unbounded liveness, is also presented.", "num_citations": "20\n", "authors": ["493"]}
{"title": "Test data compression and test time reduction using an embedded microprocessor\n", "abstract": " Systems-on-a-chip (SOCs) with many complex intellectual property cores require a large volume of data for manufacturing test. The computing power of the embedded processor in a SOC can be used to test the cores within the chip boundary, reducing the test time and memory requirements. This paper discusses techniques that use the computing power of the embedded processor in a more sophisticated way. The processor can generate and reuse random numbers to construct test patterns and selectively apply only those patterns that contribute to the fault coverage, significantly reducing the pattern generation time, the total number of test applications and, hence, the test time. It can also apply deterministic test patterns that have been compressed using the characteristics of the random patterns as well as those of the deterministic patterns themselves, which leads to high compression of test data. We compare\u00a0\u2026", "num_citations": "20\n", "authors": ["493"]}
{"title": "On the C-testability of generalized counters\n", "abstract": " This paper investigates the testability of a class of circuits, called counters, that perform the addition of sets of input bits of equal arithmetic weight. These circuits consist of full and half adders interconnected in an iterative manner defined by the counting process. The general class of counter circuits contain reconvergent fanouts and are not as structurally regular as one- or two-dimensional iterative logic arrays. A model for analyzing the structure of counter circuits is proposed. Several schemes for generating test sets that exploit the iterative structure of counter circuits are presented. The testability of such circuits is enhanced by imposing certain design constraints on them. Some methods for generating easily testable counter circuits are proposed. It is shown that counter circuits can always be designed to be testable with either eight or ten tests, irrespective of the input size.", "num_citations": "20\n", "authors": ["493"]}
{"title": "FAUST: An MOS fault simulator with timing information\n", "abstract": " This paper describes FAUST, an MOS fault simulator with timing information. FAUST simulates the effects of realistic physical failures on MOS circuits and uses a static concurrent fault-simulation technique to evaluate the fault-free circuit and all the faulty circuits in one pass. FAUST produces voltage waveforms as well as logic tables with delay information for the fault-free circuit and for each of the faulty circuits.", "num_citations": "20\n", "authors": ["493"]}
{"title": "Distributed control of computer systems\n", "abstract": " In order to be able to take full advantage of a distributed computing facility it is important not only to distribute the hardware but also to distribute the control of these resources. However, distributed control is very different from centralized control since at any time, several processes or several controllers may observe different and inconsistent views of the global system state. The task of scheduling jobs in a distributed system must also be done Without full knowledge of the system state. In this correspondence we define a totally new distributed scheduling algorithm LP (linear predictive). scheduling, which not only implements distributed control of task scheduling but is also able to adapt itself to workload fluctuations. Using a general-purpose distributed system simulator we have shown the performance rnitince advantages of this new algorithm.", "num_citations": "20\n", "authors": ["493"]}
{"title": "A built-in repair analyzer with optimal repair rate for word-oriented memories\n", "abstract": " This paper presents a built-in self repair analyzer with the optimal repair rate for memory arrays with redundancy. The proposed method requires only a single test, even in the worst case. By performing the must-repair analysis on the fly during the test, it selectively stores fault addresses, and the final analysis to find a solution is performed on the stored fault addresses. To enumerate all possible solutions, existing techniques use depth first search using a stack and a finite-state machine. Instead, we propose a new algorithm and its combinational circuit implementation. Since our formulation for the circuit allows us to use the parallel prefix algorithm, it can be configured in various ways to meet area and test time requirements. The total area of our infrastructure is dominated by the number of content addressable memory entries to store the fault addresses, and it only grows quadratically with respect to the number of\u00a0\u2026", "num_citations": "19\n", "authors": ["493"]}
{"title": "Tri-scan: a novel DFT technique for CMOS path delay fault testing\n", "abstract": " We propose a novel design for testability technique to apply two pattern tests for path delay fault testing. Due to stringent timing requirements of deep-submicron VLSI chips, design-for-test schemes have to be tailored for detecting stuck-at as well as delay faults quickly and efficiently. Existing techniques such as enhanced scan add substantial hardware overhead, whereas techniques such as scan-shifting or functional justification make the test generation process complex and produce lower coverage for scan based designs as compared to non-scan designs. We exploit the characteristics of CMOS circuitry to enable the application of two-pattern tests. The proposed technique reduces the problem of path delay fault testing for scan based designs to that of path delay fault testing with complete accessibility to the combinational logic, and has minimal area overhead. The scheme also provides significant reduction in\u00a0\u2026", "num_citations": "19\n", "authors": ["493"]}
{"title": "An efficient linearity test for on-chip high speed ADC and DAC using loop-back\n", "abstract": " Our method extracts the linearity of on-chip high speed data converters with minimum area overhead. With a loop-back setup in the presence of noise, differential nonlinearities (DNLs) and integral nonlinearities (INLs) of analog-to-digital converters (ADCs) and digital-to-analog converters (DACs) can be extracted by the proposed method. Our approach exploits the fact that the loop-back output distribution due to noise is distorted by nonlinearities of the ADC, but not by those of the DAC. We first fully characterize the ADC in the loop-back system, exclusive of the DAC. Then, the DAC is characterized using the extracted nonlinearities of the ADC. Numerical simulation shows a maximum error of less than\u00b10.1 LSB for the ADC and the DAC.", "num_citations": "19\n", "authors": ["493"]}
{"title": "On-line error detecting constant delay adder\n", "abstract": " Fault tolerance requires the inclusion of redundant information. In this paper an on-line error detecting adder is presented in which the redundant information serves a dual purpose. It provides fault tolerance during the arithmetic operations while also providing a method by which addition is constrained to become a constant delay operation regardless of the word size of the operands.", "num_citations": "19\n", "authors": ["493"]}
{"title": "Efficient loop-back testing of on-chip ADCs and DACs\n", "abstract": " This paper presents an efficient approach to testing on-chip Analog to Digital Converters (ADCs) and Digital to Analog Converters (DACs) in loop-back mode. On-chip digital signal processing units can be used to generate stimuli. With this methodology, go/no-go tests as well as characterization of the individual ADCs and DACs are possible. The proposed approach is simple and overcomes the low parametric fault coverage of conventional loop-back tests. Simulations on a Matlab model of loop-backed converters are presented to validate the feasibility of the method.", "num_citations": "19\n", "authors": ["493"]}
{"title": "Delay test techniques for boundary scan based architectures\n", "abstract": " This paper proposes techniques for appl ing delay tests in a Boundary Scan environment. &e'limitation of current public instructions of IEEE 11 9.1 when used for delay test is described and an eicient de-lay test tech. nigue is presented. Delay tests for chips, their interconnection and the glue logic are proposed. An approach for a pi ang delay tests to Level Sensitive Scan Design basel clips with Boundary Scan is also presented.", "num_citations": "19\n", "authors": ["493"]}
{"title": "High quality tests for switch-level circuits using current and logic test generation algorithms\n", "abstract": " This paper presents an approach to developing high quality tests for switch-level circuits using both current. and logic test generation algorithms. Clear definitions for analyzing the effectiveness of the joint!, est generation approach are derived. Results on a set of switch-level circuits show very high coverage of stuickat, st, uck-on, and stuck-open faults when both current and logic tests are used.", "num_citations": "19\n", "authors": ["493"]}
{"title": "The complexity of accurate logic simulation\n", "abstract": " The complexity of logic simulation is popularly supposed to be close to a linear function of the number of gates in a circuit. However,\" unknown\" values are necessary in a logic simulation to define the uninitialized states for sequential devices and also for the detection of races and hazards. This paper shows that it is an NP-complete problem to accurately simulate a circuit with the existence of unknown values. Also, it is demonstrated that current gate-level simulators cannot handle unknown propagation properly, causing pessimistic results. A set of algorithms, based on high-level descriptions of functions, has been developed for accurate logic simulation. Experiments on large benchmark functions show the advantage of using high-level descriptions, and provide empirical rules for when approximate simulation would provide satisfactory results. are erroneously assigned to some lines which should have logic value 0 or 1. It is shown in this paper that accurate logic simu-lation with the consideration of unknown values is NP-complete, making this problem as complex as other well-known problems including the traveling salesman problem.This paper also demonstrates the inaccuracy associated with the conventional pessimistic approach. Therefore, the use of high-level descriptions is proposed to handle this situation correctly with lower computation requirements. Some real PLAs are used as examples for studying the effects of unknown value propagation under approximate and exact situations. From the experimental results, a more efficient and accurate simula-tion system based on high-level descriptions is proposed for alleviating the problem\u00a0\u2026", "num_citations": "19\n", "authors": ["493"]}
{"title": "Design of a microprogram control unit with concurrent error detection\n", "abstract": " The authors present an integrated approach to the design and implementation of a microprogram control unit (MCU) with comprehensive concurrent error detection (CED) capability. The AM2910 microprogram sequencer is used as a functional model. It has been shown that the classical stuck-at fault model alone is not sufficient to describe failures in VLSI circuits. Various potential failures in the MCU are discussed and a functional level fault model is used, namely, arbitrary failures in single copies of functional units and unidirectional errors in stored words and on control or data transfer buses. Based on this fault model, Berger Coding (BERG61) and duplication techniques are used to provide the error-detecting capability of the MCU. 16 references.", "num_citations": "19\n", "authors": ["493"]}
{"title": "Sequential equivalence checking between system level and rtl descriptions\n", "abstract": " Sequential equivalence checking between system level descriptions of designs and their Register Transfer Level (RTL) implementations is a very challenging and important problem in the context of Systems on a Chip (SoCs). We propose a technique to alleviate the complexity of the equivalence checking problem, by efficiently decomposing it using compare points. Traditionally, equivalence checking techniques use nominal or functional mapping of latches as compare points. Since we operate at a level where design descriptions are in System Level Languages or Hardware Description Languages, we leverage the information available to us at this level in deducing sequential compare points. Sequential compare points encapsulate the sequential behavior of designs and are obtained by statically analyzing the design descriptions. We decompose the design using sequential compare points and\u00a0\u2026", "num_citations": "18\n", "authors": ["493"]}
{"title": "Transistor level synthesis for static CMOS combinational circuits\n", "abstract": " This paper introduces a novel framework to synthesize static CMOS circuits at the transistor level. A new class of binary decision diagrams (BDDs) which represent inverting Boolean functions, called transistor mapped BDDs (TM-BDDs), is used in the synthesis process. There is a one-to-one correspondence between a transistor netlist and its TM-BDD. Nodes in a TM-BDD represent gate inputs and the edges represent the transistors in the netlist. TM-BDDs can be optimized using BDD operations, and the data structure can retain device aspect ratios and geometries for performance optimization. The synthesis process involves a transformation from logic functions to transistor netlists using TM-BDDs. We show how a transistor netlist can be automatically generated during a depth-first traversal on a TM-BDD. The synthesis process is not only independent of any library, but also capable of generating a cell library for a\u00a0\u2026", "num_citations": "18\n", "authors": ["493"]}
{"title": "Impact of behavioral modifications for testability\n", "abstract": " Behavioral specification of a VLSI design can be used to suggest behavioral modifications that improve testability of the design. Past work has been targeted at identifying the techniques that will enable such modifications. However, the impact of such behavioral modifications on the testability of a design has not been analyzed with regards to fault coverage and area overhead which is the focus of this paper. Results obtained show that the area overhead is low and the fault coverage is higher when the behavior is modified for testability. These results are compared with the results obtained when partial scan is used to improve the testability of a design.< >", "num_citations": "18\n", "authors": ["493"]}
{"title": "TESTABLE CMOS LOGIC CIRCUITS UNDER DYNAMIC BEHAVIOR.\n", "abstract": " Owing to the sequential behavior of CMOS logic circuits in the presence of a stuck-open fault, it is required that an initialization input followed by a test input be applied to detect such a fault. However, a test set generated under a static behavior assumption can be invalidated in the presence of time-skews in the variable changes and/or unequal delays in the different interconnections of the circuits. The authors present a necessary and sufficient condition for the existence of a test set that cannot be invalidated under dynamic behavior (variable delays assumed), for an AND-OR or OR-AND CMOS realization for any given function. They also introduce a hybrid CMOS realization which, for any given function, is guaranteed to have a valid test set under the assumption of dynamic behavior.", "num_citations": "18\n", "authors": ["493"]}
{"title": "An aging-aware flip-flop design based on accurate, run-time failure prediction\n", "abstract": " As process technology continues to shrink, Negative Bias Temperature Instability (NBTI) has become a major reliability issue in CMOS circuits. NBTI degrades the threshold voltage of the PMOS transistor and, over time, causes the operating speed of the circuit to become slower (also known as the aging effect). In this paper, we introduce a new aging-aware Flip-Flop (FF) that is based on accurate, run-time Failure Prediction. In order to maintain prediction accuracy despite aging, we use two schemes: (a) the master latch in the main FF is duplicated and used as an aging monitor so that it can have the same aging effect as that of the main FF; (b) the delay element that is used for the guardband is inserted into the clock network to utilize the recovery effect of NBTI. These schemes keep the guardband virtually constant, which reduces the likelihood of both overestimating the aging effect and failing to detect it. The\u00a0\u2026", "num_citations": "17\n", "authors": ["493"]}
{"title": "Efficient loopback test for aperture jitter in embedded mixed-signal circuits\n", "abstract": " Accurate measurement of aperture jitter for high-speed data converters is a difficult problem, since aperture jitter should be precisely separated from other jitter components as well as additive noise. This problem results in low test accuracy and high-yield loss. This paper presents a novel methodology for accurately predicting aperture jitter using a cost-effective loopback methodology. By using an efficient spectral loopback scheme, aperture jitter is precisely separated from input jitter and clock jitter as well as additive noise present in the DUT. Hardware measurement results show that this approach can be effectively used to predict the aperture jitter of a DUT, with an 89% reduction in the prediction error compared with previous approaches.", "num_citations": "17\n", "authors": ["493"]}
{"title": "A novel methodology for hierarchical test generation using functional constraint composition\n", "abstract": " The increasing functionality of processor designs has posed a severe challenge for generating high quality manufacturing tests, which can be applied at native speeds. A previous approach was to target one module at a time and extract functional constraints on the module under test (MUT) in order to reduce the complexity for test generation. However, when this technique is applied to large designs, the embedded modules themselves become too complex for an ATPG tool to handle. If sub-modules within these complex modules are considered, the extraction of constraints may prove to be too tedious. In this paper, a novel methodology to extract constraints hierarchically is presented. We use synthesis tools to eliminate redundant logic during the constraint extraction process. The proposed methodology also facilitates the reuse of constraints extracted for different sub-modules at a given level of hierarchy. This\u00a0\u2026", "num_citations": "17\n", "authors": ["493"]}
{"title": "Performance and functional verification of microprocessors\n", "abstract": " We address the problem of verifying the correctness of pre-silicon models of a microprocessor. We touch on the latest advances in this area by considering two different aspects of the validation problem: (a) verifying the functional integrity of the model and (b) testing the model for timing accuracy at the architectural level. The latter area, that of performance verification, is of increasing importance in the design of server-class processor chips, with one or more high performance cores on a single die. We show how simulation-based test cases can be generated under a unified defect and coverage model to detect both performance and functional bugs. We present and discuss examples of such integrated validation methodologies used in real processor development projects.", "num_citations": "17\n", "authors": ["493"]}
{"title": "Synthesis of native mode self-test programs\n", "abstract": " Recent studies show that at-speed functional tests are better for finding realistic defects than tests executed at lower speeds. This advantage has led to growing interest in design for at-speed tests. In addition, time-to-market requirements dictate development of tests early in the design process. In this paper, we present a new methodology for synthesis of at-speed self-test programs for microprocessors. Based on information about the instruction set, this high-level test generation methodology can generate instruction sequences that exercise all the functional capabilities of complex processors. Modern processors have large memory modules, register files and powerful ALUs with comprehensive operations, which can be used to generate and control built-in tests and to evaluate the response of the tests. Our method exploits the functional units to compress and check the test response at chip internal speeds\u00a0\u2026", "num_citations": "17\n", "authors": ["493"]}
{"title": "Test generation, design-for-testability and built-in self-test for arithmetic units based on graph labeling\n", "abstract": " In this article we discuss a test generation, design-for-testability and built-in self-test methodology for two-dimensional iterative logic arrays (ILAs) that perform arithmetic functions. Our approach is unique because a single graph labeling procedure is used to generate test vectors, implement design-for-testability as well as design the circuitry for built-in self-test. The graph labeling is based on mathematical properties of full-addition such as symmetry and self-duality. Circuit modifications are introduced by a systematic procedure based on the graph labeling, that enable them to be tested with a fixed number of tests irrespective of their size. The approach is novel as it also greatly simplifies the processes of on-chip test vector generation and response comparison that are necessary for built-in self-test. Each circuit module is tested in a pseudo-exhaustive manner with deterministic as opposed to random test\u00a0\u2026", "num_citations": "17\n", "authors": ["493"]}
{"title": "Implementing forward recovery using checkpointing in distributed systems\n", "abstract": " The paper describes the implementation of a forward recovery scheme using checkpoints and replicated tasks. The implementation is based on the concept of lookahead execution and rollback validation. In the experiment, two tasks are selected for the normal execution and one for rollback validation. It is shown that the recovery strategy has nearly error-free execution time and an average redundancy lower than TMR.", "num_citations": "17\n", "authors": ["493"]}
{"title": "Mixed-level sequential test generation using a nine-valued relaxation algorithm\n", "abstract": " A powerful automatic test generation system which uses a novel nine-valued relaxation algorithm is presented. This algorithm, which brings together the relaxation technique from circuit simulation and a nine-valued algebra from sequential circuit test generation, is applied to the strongly connected DC coupling components so that for every possible choice of node values a stable state could be achieved through relaxation. Circuits which use bidirectional transistors and depend on the transistor strengths for correct operation are properly handled by this algorithm. A method called DC path sensitization is used to detect the stuck-at, stuck-on, stuck-open and bridging faults for CMOS transistors. The algorithm has been implemented in C++ and preliminary results are very promising. The algorithm can easily be extended to different circuit models and other technologies, or be incorporated into a higher level test\u00a0\u2026", "num_citations": "17\n", "authors": ["493"]}
{"title": "Incidence of hydatidosis in animals slaughtered in Kerala\n", "abstract": " Incidence of hydatidosis in animals slaughtered in Kerala FAO_logo home-icon English Espa\u00f1ol Fran\u00e7ais \u0627\u0644\u0639\u0631\u0628\u064a\u0629 \u4e2d\u6587 \u0420\u0443\u0441\u0441\u043a\u0438\u0439 home-icon Translate with Google Access the full text NOT AVAILABLE Lookup at Google Scholar google-logo Bibliographic information Language : English Type : Journal Article In AGRIS since : 2013 Volume : 11 Issue : 2 Start Page : 247 End Page : 251 All titles : \" Incidence of hydatidosis in animals slaughtered in Kerala \" Save as: AGRIS_AP RIS EndNote(XML) Incidence of hydatidosis in animals slaughtered in Kerala Loading... Paper Written Paper Incidence of hydatidosis in animals slaughtered in Kerala [1980] Abraham, J. Pillai, KM Iyer, RP Access the full text NOT AVAILABLE Lookup at Google Scholar google-logo Access the full text NOT AVAILABLE Lookup at Google Scholar google-logo Loading... Bibliographic information Language : English Type : Journal Article In AGRIS : : 11 \u2026", "num_citations": "17\n", "authors": ["493"]}
{"title": "Efficient cross-layer concurrent error detection in nonlinear control systems using mapped predictive check states\n", "abstract": " The rapid proliferation of sensor networks and robots in a wide range of societal applications has focused renewed attention on error-free operation of their underlying signal processing and control functions for reasons of safety and reliability. While real-time error detection in linear systems has been investigated in the past, error detection in nonlinear control functions has largely relied on implementing redundancy in components, units, or subsystems resulting in excessive area/performance overheads. In this paper, we introduce a realtime error detection methodology for nonlinear control state space systems that uses mapped predictive check states for detecting sensor and actuator malfunctions and transient errors in the execution of the control algorithm on the underlying processor. In our approach, the check state at time t bears a known relationship with the corresponding states of the nonlinear system. This\u00a0\u2026", "num_citations": "16\n", "authors": ["493"]}
{"title": "Eagle: A regression model for fault coverage estimation using a simulation based metric\n", "abstract": " Evaluating the fault coverage of manufacturing tests has become a time-consuming process due to today's large and complex digital designs. The computation cost is even more pronounced in software-based self test, on-line test, and logic BIST schemes, which require fault simulation of sequential circuits. In this paper, we build a regression model for estimating stuck-at fault coverage. This model is built based on partial fault simulation along with a statistical metric, which is calculated by a single pass of fault-free simulation. Our results on ISCAS'85, ISCAS'89, and the OR1200 processor show that by only fault simulating 7.6% of the test vectors, on average, over 94% of the fault coverage bounds are estimated correctly.", "num_citations": "16\n", "authors": ["493"]}
{"title": "Dynamic trace signal selection for post-silicon validation\n", "abstract": " In order to gain market share in today's competitive high-tech industry, fast time-to-market (TTM) is one of the key factors for the success of a product. Since pre-silicon verification cannot be applied exhaustively as the size and complexity of the integrated circuit design increases, post-silicon validation becomes crucial to capture bugs and design errors that escape from the pre-silicon verification phase. However, because of the limited observability of internal states due to the limited storage capacity available for post-silicon validation, selecting optimal sets of trace signals has always been a challenging task for debugging engineers. This paper proposes a new dynamic trace signal selection algorithm to maximize the restoration ratio for internal circuit states. Experimental results on benchmark circuits and an industry design show that the proposed technique provides a high degree of state restoration regardless of\u00a0\u2026", "num_citations": "16\n", "authors": ["493"]}
{"title": "A reduced voltage swing circuit using a single supply to enable lower voltage operation for SRAM-based memory\n", "abstract": " This paper presents a new read and write assist technique to enable lower voltage operation for Static Random Access Memory (SRAM). The ability to scale the operating voltage with frequency of the chip has big impact on power consumption (P\u03b1v2). The lower end of the operating voltage (Vddmin) for most chips is determined by the stability of the SRAM cell. The new technique uses a contention-free circuit to generate a Reduced Voltage Swing (RVS) on the wordline (VWL) and selectively reduce the supply to the bitcell (Vddmem) during write. The required VWL and bitcell voltages are programmable and controllable to adapt to performance and yield requirements. An 8\u00a0KB memory test-chip was designed to demonstrate this technique in a low-leakage 45\u00a0nm process technology. Results show a 7 to 19% improvement in Vddmin depending on the process corner, which translates into 14\u201340% reduction on\u00a0\u2026", "num_citations": "16\n", "authors": ["493"]}
{"title": "Frequency response verification of analog circuits using global optimization techniques\n", "abstract": " This paper develops a new formal technique to verify the frequency response of analog circuits using global optimization techniques. Since simulation-based approaches are unable to cover the design space, there is a need for formal approaches to verify large circuits. Drawing parallels from the digital domain, the verification problem in the analog domain is modeled as a non-linear optimization problem and solved using global optimization techniques by ensuring that the implementation response is bounded within an envelope around the specification. We also address the problem of verifying frequency response under the influence of parameter variations. Direct as well as indirect techniques are illustrated using accurate frequency response models. Experimental results are presented to show the effectiveness of the proposed methodology.", "num_citations": "16\n", "authors": ["493"]}
{"title": "Hierarchical test generation for systems on a chip\n", "abstract": " The rapid increase in functionality on a single chip in the last few years has increased the gap between the complexity of the design and the capability of commercial test tools. In particular the test needs for systems on a chip (SOC) are not addressed by existing tools. Because some of the cores integrated on a single SOC may not have embedded testability features, it is not always possible to use conventional design for testability (DFT) methodologies directly. This paper presents a novel approach for generating tests for complex SOCs which targets one module (or core) at a time, by extracting its environment elegantly in the form of constraints and storing it as virtual logic. Information about the core processor and internal bus is used to reduce the size of the virtual logic so that a commercial ATPG tool can be used to generate tests. These tests are then automatically translated to system-level tests. The approach is\u00a0\u2026", "num_citations": "16\n", "authors": ["493"]}
{"title": "Architectural performance verification: PowerPC processors\n", "abstract": " We consider the problem of validating a functional (architectural) timing model coded to predict instructions-per-cycle (IPC) performance for an advanced superscalar processor family. We present a methodology based on loop test cases for validating such models. For the purpose of this paper, we focus on two key strategies within our overall validation methodology: transient mode testing; and steady-state parametric testing. We state a few key lemmas characterizing the underlying theory and present a set of experimental results to illustrate the use of these validation strategies.< >", "num_citations": "16\n", "authors": ["493"]}
{"title": "High level testability analysis using VHDL descriptions\n", "abstract": " Tests generated for modules in a design may not be applicable from the design boundaries due to global constraints. The authors discuss high level techniques that can be used to analyze a sequential circuit and precompute useful information that reflects the controllability of inputs and observability of outputs of modules in the design. The information can then be used in conjunction with the tests generated for modules to obtain high quality tests for the design quickly. Results obtained for example circuits are presented to demonstrate the techniques.< >", "num_citations": "16\n", "authors": ["493"]}
{"title": "A novel approach to accurate timing verification using RTL descriptions\n", "abstract": " Timing verification is a critical part of VLSI circuit design. A new approach to timing verification using Register Transfer Level (RTL) descriptions is presented, which eliminates false paths that occur due to (i) redundancy, (ii) reconvergent fanout or (iii) control signal constraints, and generates a test for the critical paths. High level instructions of the circuit are used to test for any timing violations. An algorithm to identify a minimal set of instructions that tests the circuit for all timing errors in valid paths is proposed. Results are presented based on an implementation of the algorithm in LISP programming language on a TI Explorer machine.", "num_citations": "16\n", "authors": ["493"]}
{"title": "Design of sytems with concurrent error detection using software redundancy\n", "abstract": " Thii paper describes a methodology for the design of systems with concurrent error detection through the use of software redundancy. We present a technique for the development of self-checking programs in which assertions are generated systematically from the design of the program-Software that is developed using thii technique is guaranteed to be self-checking with respect to all software faults and can be shown in practice to detect most errors due to hardware faults. A less rigorous technique using data encoding is also described. The self-checking capabilities are evaluated using a fault simulation method that allows us to study the coverage of errors due to hardware and software faults. We also propose a dual processor technique that can improve the perforn=\u2018me. reliability and availability of the self-checking system.", "num_citations": "16\n", "authors": ["493"]}
{"title": "TECHNIQUES FOR EFFICIENT MOS IMPLEMENTATION OF TOTALLY SELF-CHECKING CHECKERS.\n", "abstract": " Some new techniques for reducing the transistor count of MOS implementations of totally self-checking (TSC) checker designs are presented. These techniques include transfer of fanouts, removal of inverters, and use of multilevel realizations of functions. They also increase the speed of the circuit and may reduce the number of required tests. Impressive reductions of up to 90% in the transistor count in some cases have been obtained for these MOS implementations. This directly translates into saving of chip area.", "num_citations": "16\n", "authors": ["493"]}
{"title": "An area efficient on-chip static IR drop detector/evaluator\n", "abstract": " As the supply voltage shrinks with technology scaling, the slightest drop in the voltage level has a significant impact on chip functionality. It is, therefore, important to accurately measure supply voltage noise to evaluate the actual IR drop on-chip and to feed the results to a power management unit, which can scale the voltage and perform on-chip compensation based on the IR drop. In this paper, we propose a detection scheme based on a ring oscillator, which can detect and evaluate static IR drop on-chip with minimal additional area and design complexity.", "num_citations": "15\n", "authors": ["493"]}
{"title": "A statistical digital equalizer for loopback-based linearity test of data converters\n", "abstract": " This paper presents a new built-in self test (BIST) method based on efficient digital equalization and spectral prediction techniques. The method enables accurate built-in characterization of the static performance parameters of data converters, and thus test and calibration costs can be significantly alleviated. Based on recent work on dynamic performance parameter characterization using a loopback test, the transfer function of a DAC in loopback mode is estimated with a spectral prediction technique and Chebyshev polynomials. A digital equalizer is designed to compensate for the non-linearity of the DAC in the pre-conversion stage, hence the ADC can be tested with the digitally calibrated analog signals. The digital equalizer overcomes accuracy limitations encountered in a traditional compensation technique, and thus a standard histogram test which may suffer from INL masking problems can be successfully\u00a0\u2026", "num_citations": "15\n", "authors": ["493"]}
{"title": "Performance characterization of mixed-signal circuits using a ternary signal representation\n", "abstract": " Signatures used in low-cost schemes for testing analog and mixed-signal circuits do not directly represent or characterize the behavior of the device-under-test (DUT), since the lossy compression or complicated mathematical relations used can result in the loss of physical performance information. We develop a novel scheme where the signature is generated by built-in circuits based on a ternary signal representation (TSR), which represents the behavior of a signal with three levels, positive, zero, and negative. The signatures can be used directly to characterize DUTs or can be manipulated to obtain widely accepted dynamic performance parameters, such as SNR, THD, etc. Simulation results on a /spl Delta//spl Sigma/ DAC and a /spl Delta//spl Sigma/ ADC using TSR signatures through built-in circuits are presented to show the feasibility of the proposed method.", "num_citations": "15\n", "authors": ["493"]}
{"title": "The case for microarchitectural awareness of lifetime reliability\n", "abstract": " Ensuring long processor lifetimes by limiting failures due to hard errors is a critical requirement for all microprocessor manufacturers. Current methodologies for qualifying long-term lifetime reliability are overly conservative since they seek to maintain reliability for peak usage of the processor. This paper makes the case that the continued use of such methodologies will significantly and unnecessarily constrain performance. Instead, lifetime reliability awareness at the microarchitectural design stage can mitigate this problem, by designing processors that dynamically adapt in response to the observed usage to meet a reliability target. We make two specific contributions. First, we describe an architecture-level model and its implementation, called RAMP, that can dynamically track lifetime reliability, responding to changes in application behavior. We use stateof-the-art models for different wear-out mechanisms and apply them to calculate failure rates of individual architectural structures. These failure rates are a function of temperature, switching activity, and voltage. RAMP is coupled with a conventional performance and power simulator to track these parameters over an application run. Second, we propose dynamic reliability management (DRM)\u2013a technique where the processor can respond to changing application behavior to maintain its lifetime reliability target. In contrast to current worst-case behavior based reliability qualification methodologies, DRM allows processors to be qualified for reliability at lower (but more likely) operating points than the worst case. Using RAMP, we show that this can save cost and/or improve performance, dynamic\u00a0\u2026", "num_citations": "15\n", "authors": ["493"]}
{"title": "A non-binary capacitor array calibration circuit with 22-bit accuracy in successive approximation analog-to-digital converters\n", "abstract": " A novel capacitor array calibration circuit is presented in this paper. A non-binary capacitor array with 20 capacitors is used. The capacitor calibration algorithm is based on a perceptron learning rule, developed for artificial intelligence applications. The capacitor weights are adaptively calibrated to match the physical capacitors with up to 22 bit accuracy. Capacitor matching is not a limiting factor to the resolution. A mixed-signal microcontroller architecture is used to efficiently implement the novel capacitor array calibration algorithm. This calibration circuit is being used to design a 1.5 Mega samples per second (MSPS), 16 bit, 50 mW successive approximation analog-to-digital converter (ADC).", "num_citations": "15\n", "authors": ["493"]}
{"title": "Subband filtering scheme for analog and mixed-signal circuit testing\n", "abstract": " A new technique is proposed to analyze and compress the output responses from analog circuits. We first describe the subband filtering scheme to decompose responses from the analog circuit under test (CUT). A subband filter or wavelet takes the response, then generates the decomposed signals for each frequency band. The decomposed signal for each frequency band is fed into its respective integrator. Two kinds of wavelets are used to decompose the test response and effectively detect the faults in the circuit. Implementation issues including hardware overhead are also discussed.", "num_citations": "15\n", "authors": ["493"]}
{"title": "NCUBE: An automatic test generation program for iterative logic arrays\n", "abstract": " NCUBE applies all possible input patterns to each array cell while ensuring that the effects of incorrect transitions are observable at the array outputs. If the array is testable with a constant number of test vectors irrespective of its size (C-testable), then NCUBE generates the constant-size test set for the array. If the array cannot be tested with a constant number of test vectors, then the test size is proportional either to the number of rows or columns of the array or to the number of cells. In that case, NCUBE generates a minimal or near-minimal test set that depends on the size of the array.<>", "num_citations": "15\n", "authors": ["493"]}
{"title": "The morphology and biology of the cashew flower Anacardium occidentale L..\n", "abstract": " Contenido: Studies were carried out on anthesis, dehiscence of anthers, receptivity of sitgma, pollination, fruit-set and fruit development in Cashew. Staminate flowers were found to open very early in the morning and it continued till about. 2 PM Over 80 percent of the perfect flowers opened between 10 AM and 12 Noon. The peak period of degiscence of anthers was from 9.30 AM to 11.30 AM and the rate of dehiscence was slightly higher on the subnny side of the tree as compared to that on the shady side. The stigma was found to be receptive 24 hours efore anthesis and continued to be so for about 48 hours after anthesis, the optimum period being round about 12 Noon on the day of anthesis. Morphological features of pollen grains were similar in all trees and 94 per cent of th grains were viable. In artificial medium, the highest germination obtained was only 36.2 per cent in 30 per cent sucrose solution at rooim temperature (28oC). Staminodes did not play any significant role in pollination under natural conditions, even though they contained some viable pollen grains. Only 4-6 per cent of the perfect flowers were ccarried to maturity. There was reduction in the size of the nut to the extent of about 25 per cent after the shell began to harden. It took54 days on an average from fruit-set to full ripening of the fruit.", "num_citations": "15\n", "authors": ["493"]}
{"title": "A 0.1\u20133.5-GHz duty-cycle measurement and correction technique in 130-nm CMOS\n", "abstract": " A duty-cycle correction technique using a novel pulsewidth modification cell is demonstrated across a frequency range of 100 MHz-3.5 GHz. The technique works at frequencies where most digital techniques implemented in the same technology node fail. An alternative method of making time domain measurements such as duty cycle and rise/fall times from the frequency domain data is introduced. The data are obtained from the equipment that has significantly lower bandwidth than required for measurements in the time domain. An algorithm for the same has been developed and experimentally verified. The correction circuit is implemented in a 0.13-\u03bcm CMOS technology and occupies an area of 0.011 mm 2 . It corrects to a residual error of less than 1%. The extent of correction is limited by the technology at higher frequencies.", "num_citations": "14\n", "authors": ["493"]}
{"title": "Efficient and product-representative timing model validation\n", "abstract": " Timing analysis is a key sign-off step in the design of today's chips, but as technology advances, it becomes ever more challenging to create timing models that accurately reflect real timing-related behavior. Complex dependencies on second order phenomena, such as pattern density and stress/strain make it very difficult to develop device models and simulation tools that accurately predict the timing behavior that will be seen in actual product silicon. As a result, it is necessary to validate timing models in silicon. Traditional ways to validate timing models use ring oscillators or perform delay testing but both approaches have significant drawbacks. Ring oscillators lack diversity in circuit structure and present layout configurations that are not typical of real products. Delay test can be expensive to apply and provides directly only path delay information not individual gate delays. To address these limitations, we explore\u00a0\u2026", "num_citations": "14\n", "authors": ["493"]}
{"title": "Cache organization for embeded processors: cam-vs-sram\n", "abstract": " Caches are becoming an increasingly important part of embedded processor design because of the impact they have on performance as well as implementation, specifically, area, power and timing. Different cache organizations make tradeoffs between these metrics. One of the main architectural choices is whether to use standard SRAM-based tag design or to go with a CAM- based organization. This choice has far reaching consequences on all other aspects of the cache design. We will compare these two cache styles using results from a recently completed DSP core design. Our conclusion is that, contrary to popular belief, an SRAM-tag based design provided a more optimal overall design point and is superior in energy respect. Some of driving factors such as the increasing dominance of wire and leakage power will be extrapolated forward to next generation processes.", "num_citations": "14\n", "authors": ["493"]}
{"title": "Timing verification and delay test generation for hierarchical designs\n", "abstract": " This paper develops an effective solution for timing verification and delay test generation at the full chip level by exploiting the hierarchy in large designs. Currently, timing verification can only be done at the module level. We consider the timing verification problem when a module is instantiated in a larger design, where the module-level critical paths might no longer hold. In order to check whether a module-level critical path is true at the chip level, we use a fault injection circuit where detecting a stuck-at fault in this circuit will result in a pair of vectors which sensitize the critical path in the module. Unfortunately, existing sequential automatic test pattern generators (ATPG) cannot deal with complete chip designs in generating tests using the above approach. Therefore, we use a hierarchical test generation approach which abstracts the rest of the large chip into just the logic behavior relevant to the embedded module\u00a0\u2026", "num_citations": "14\n", "authors": ["493"]}
{"title": "Parallel and scalable architecture for solving SATisfiability on reconfigurable FPGA\n", "abstract": " In this paper, we present different architectures and implementation for solving the general SATisfiability (SAT) problem on reconfigurable devices. In particular, we address the solution of this basic and important problem using multiple small FPGAs. Our approach utilizes partitioning and decomposition to map any large SAT problem on more than one small FPGA. First, a SAT problem is decomposed into several independent sub-problems. This way, all sub-problems may be solved on different FPGAs simultaneously. If any of the sub-problems can not fit on a single FPGA, then a second technique is used to divide the sub-problem into dependent parts. We compute the solution time and hardware resources for both approaches and also compare our results with the previously published results.", "num_citations": "14\n", "authors": ["493"]}
{"title": "Efficient combinational verification using BDDs and a hash table\n", "abstract": " We propose a novel methodology that combines local BDDs with a hash table for very efficient verification of combinational circuits. The main purpose of this technique is to remove the considerable overhead associated with the case-by-case verification of internal node pairs in typical internal correspondence based verification methods. Two heuristics based on the number of structural levels of circuitry looked at and the total number of nodes in the BDD manager are used to control the BDD sizes and introduce new cutsets based on already found equivalent nodes. We verify the ISCAS85 benchmark circuits and demonstrate significant speedup over existing methods. We also verify several hard industrial circuits and show our superiority in extracting internal equivalences.", "num_citations": "14\n", "authors": ["493"]}
{"title": "MIXER: Mixed-signal fault simulator\n", "abstract": " This paper presents an efficient unified approach to mixed-signal fault simulation. A common fault simulation platform is developed for continuous valued analog circuits, discrete time switched-capacitor circuits and binary valued digital circuits, by discretizing the analog circuit, using discrete models of switched-capacitor circuits and complementing the stuck-at digital fault models with comprehensive behavioral analog fault models.< >", "num_citations": "14\n", "authors": ["493"]}
{"title": "WRAP: an algorithm for hierarchical compression of fault simulation primitives\n", "abstract": " IDEALS @ Illinois: Wrap: An Algorithm for Hierarchical Compression of Fault Simulation Primitives IDEALS Login Search IDEALS This Collection query Advanced Search IDEALS Logo IDEALS Logo The Alma Mater Round Barn Grainger Engineering Library Wrap: An Algorithm for Hierarchical Compression of Fault Simulation Primitives Welcome to the IDEALS Repository Browse IDEALS TitlesAuthorsContributorsSubjectsDateCommunities This Collection TitlesAuthorsContributorsSubjectsDateSeries/Report My Account LoginRegister Information Getting StartedAboutContact Us Access Key Closed Access Private / Closed Access Campus Access Limited Access: U. of I. Users Only IDEALS Home \u2192 College of Engineering \u2192 Coordinated Science Laboratory \u2192 Report - Coordinated Science Laboratory \u2192 View Item Wrap: An Algorithm for Hierarchical Compression of Fault Simulation Primitives Guzolek, John Use to :\u2026", "num_citations": "14\n", "authors": ["493"]}
{"title": "High level hierarchical fault simulation techniques\n", "abstract": " This paper presents techniques for simulating directly from a hierarchical circuit description without flattening to the level of primitives. An overview of traditional fault simulation techniques is followed by details of the hierarchical techniques. The fault model is shown to be decoupled from the simulator programs through the use of a fault library. The fault library allows the user to mix both functional and technology-dependent fault models, which allows fault simulation and consequently test coverage estimation early in the design, with refinements in the fault model and test coverage as the design progresses. Thus testing problems can be detected early in the design process while they are much easier to correct. The circuit description language, SCALD, and the fault library language are described and illustrated with examples. The simulator initialization and execution phases are discussed in detail with emphasis on\u00a0\u2026", "num_citations": "14\n", "authors": ["493"]}
{"title": "DESIGN OF TOTALLY SELF-CHECKING EMBEDDED CHECKERS.\n", "abstract": " DESIGN OF TOTALLY SELF-CHECKING EMBEDDED CHECKERS. \u2014 Princeton University Skip to main navigation Skip to search Skip to main content Princeton University Logo Help & FAQ Home Profiles Research Units Facilities Projects Research Output DESIGN OF TOTALLY SELF-CHECKING EMBEDDED CHECKERS. Niraj K. Jha, Jacob A. Abraham Electrical Engineering Research output: Chapter in Book/Report/Conference proceeding \u203a Conference contribution 9 Scopus citations Overview Fingerprint Original language English (US) Title of host publication Digest of Papers - FTCS (Fault-Tolerant Computing Symposium) Publisher IEEE Pages 265-270 Number of pages 6 ISBN (Print) 0818605405 State Published - Jan 1 1984 Publication series Name Digest of Papers - FTCS (Fault-Tolerant Computing Symposium) ISSN (Print) 0731-3071 All Science Journal Classification (ASJC) codes Hardware and to to , \u2026", "num_citations": "14\n", "authors": ["493"]}
{"title": "Microwave oven with antenna array\n", "abstract": " Techniques are disclosed relating to microwave ovens. In one embodiment, an apparatus is disclosed that includes a microwave heating unit. The microwave heating unit is configured to radiate microwaves into a cavity and includes an antenna array coupled to one or more amplifiers. The antenna array is configured to generate the radiated microwaves. In some embodiments, the microwave oven is configured to measure temperatures of a item within the cavity, and to steer a microwave beam produced by the antenna array based on the measured temperatures.", "num_citations": "13\n", "authors": ["493"]}
{"title": "Real-time checking of linear control systems using analog checksums\n", "abstract": " In the recent past, there has been a proliferation of complex control problems in sensor network design, multi-agent systems such as autonomous vehicles and robotics, to name a few. While prior research has focused on the design of optimal controllers for real-time systems, in the future it will become increasingly difficult to perform periodic maintenance of such systems due to their mobile and autonomous nature. Moreover, in safety-critical real-time applications it will become increasingly necessary to perform real-time monitoring of the plant as well as its controller functions for reasons of reliability and safety. In this paper, we develop, for the first time, a theory for implementing low-overhead and high coverage detection of transient errors and permanent faults in linear control systems consisting of the plant and its controller using analog checksums. The approach is demonstrated on a servo-motor control problem\u00a0\u2026", "num_citations": "13\n", "authors": ["493"]}
{"title": "Functional test generation for hard to detect stuck-at faults using rtl model checking\n", "abstract": " At-speed functional testing has proven to be very effective at uncovering defective chips. However for processor testing, generating instruction level tests for covering all faults is a challenge given the issue of scalability. Data-path faults are relatively easier to control and observe compared to control-path faults. In this paper we present a novel method to generate instruction level tests for hard to detect control-path faults in a processor. We initially map the gate level stuck-at fault to the Register Transfer Level (RTL) and build an equivalent faulty RTL model. The fault activation and propagation constraints are captured using Control and Data Flow Graph of RTL as an Liner Temporal Logic (LTL) property. This LTL property is then negated and given to a Bounded Model Checker based on a Bit-Vector Satisfiability Module Theories (SMT) solver. From the counter-example to the property we can extract a sequence of\u00a0\u2026", "num_citations": "13\n", "authors": ["493"]}
{"title": "A Built-In Self-Test scheme for DDR memory output timing test and measurement\n", "abstract": " This paper presents a Built-In Self-Test (BIST) scheme to measure high speed double data rate (DDR) memory output timing using low cost testers. This technique uses an on-chip pattern generator to generate a time delay between data and data-strobe or clock. The time delay is controlled precisely using a phase interpolator based cycle-by-cycle control method. A novel method for testing the resolution of phase interpolator, which does not need any extra hardware, is also presented. Using the test resolution, a timing pass/fail flag is set and the timing margin is quantified as a multiple of the test clock cycle. Since these test results have high observability, output per-pin timing performance can be diagnosed easily, which is especially good for testing parallel memory interfaces. Moreover, these features make our scheme compatible with low-cost testers and decreases the time-to-market for the chip. The BIST circuit\u00a0\u2026", "num_citations": "13\n", "authors": ["493"]}
{"title": "Path criticality computation in parameterized statistical timing analysis using a novel operator\n", "abstract": " This paper presents a method to compute criticality probabilities of paths in parameterized statistical static timing analysis. We partition the set of all the paths into several groups and formulate the path criticality into a joint probability of inequalities. Before evaluating the joint probability directly, we simplify the inequalities through algebraic elimination, handling topological correlation. Our proposed method uses conditional probabilities to obtain the joint probability, and statistics of random variables representing process parameters are changed to take into account the conditions. To calculate the conditional statistics of the random variables, we derive analytic formulas by extending Clark's work. This allows us to obtain the conditional probability density function of a path delay, given the path is critical, as well as to compute criticality probabilities of paths. Our experimental results show that the proposed method\u00a0\u2026", "num_citations": "13\n", "authors": ["493"]}
{"title": "Extraction based verification method for off the shelf integrated circuits\n", "abstract": " Off-the-shelf integrated circuits (ICs) are used in the design of many products. The IC is supposed to implement a set of available specifications describing the function of the IC. Users of off-the-shelf ICs need a simple and effective method to validate the specifications to insure that the IC implements exclusively the set of available specifications. In this paper, we propose an approach to validate these specifications by a set of IC re-engineering experiments. The proposed approach is based on the construction of a high-level description of the packaged IC and on using the extracted description to validate the specifications. The approach uses the scan operations (available for manufacturing test of the IC) and the IC specification to disassemble the states/flip-flops and output functions of the packaged IC. Using the disassembled functions, a register transfer level (RTL) model suitable for computer-aided design\u00a0\u2026", "num_citations": "13\n", "authors": ["493"]}
{"title": "Characterization and testing of microelectromechnical accelerometers\n", "abstract": " Analog and Mixed Signal circuits pose a greater challenge in semiconductor testing than digital circuits due to the complexity of test requirements and the extremely large test vector sample space [?]. MicroElectroMechanical Systems (MEMS) are an emerging field combining mechanical components with semiconductor circuitry on-chip. Conventional testing and characterization methods for MEMS are very resource intensive. Alternate Testing, or Indirect Testing is a methodology used to identify and implement economical, fast and less complex tests to replace conventional complex, specification based tests [?]. This paper describes a methodology to characterize and test the mechanical subsystem of an accelerometer using its electrical sub-system. We use a novel pseudo-mechanical impetus to obtain an analog output which is then used for parameter mapping to characterize both electrical and mechanical\u00a0\u2026", "num_citations": "13\n", "authors": ["493"]}
{"title": "Detecting false timing paths: experiments on PowerPC microprocessors\n", "abstract": " We present a new algorithm for detecting both combinationally and sequentially false timing paths, one in which the constraints on a timing path are captured by justifying symbolic functions across latch boundaries.We have implemented the algorithm and we present, here, the results of using it to detect false timing paths on a recent PowerPC microprocessor design. We believe these are the first published results showing the extent of the false path problem in industry. Our results suggest that the reporting of false paths may be compromising the effectiveness of static timing analysis.", "num_citations": "13\n", "authors": ["493"]}
{"title": "Efficient algorithmic circuit verification using indexed BDDs\n", "abstract": " The Indexed Binary Decision Diagram (IBDD), a Boolean function representation scheme, provides a compact representation for functions whose OBDD representation is intractably large. In this paper, we present more efficient algorithms for satisfiability testing and equivalence checking of IBDDs. Efficient verification of Booth multipliers, as well as practical strategies for polynomial time verification of some classes of unsigned array multipliers, are demonstrated experimentally. Our results show that the verification of many instances of functions whose analysis is intractable using OBDDs, such as multipliers and the hidden-weighted-bit function, can be done efficiently using IBDDs.< >", "num_citations": "13\n", "authors": ["493"]}
{"title": "Efficient verification of multiplier and other difficult functions using ibdds\n", "abstract": " It is well known that any OBDD representing a multiplier (or some other difficult Boolean functions) requires exponential space. Alternate representation schemes have been suggested for multipliers. However, such representations have difficulties in capturing even some simple modifications in a multiplier circuit. We show how Indexed BDDs (IBDDs), a generally applicable representation scheme, allow efficient descriptions of a multiplier from a circuit represeiitat, ion or an abstract specification. An algorithm to detect inequivalence between two arbitrary IBDD multiplier graphs is also given. We also discuss how to verify other difficult functions by augmenting the IBDD representation scheme with functional partitioning.", "num_citations": "13\n", "authors": ["493"]}
{"title": "Probabilistic evaluation of online checks in fault-tolerant multiprocessor systems\n", "abstract": " The analysis of fault-tolerant multiprocessor systems that use concurrent error detection (CED) schemes is much more difficult than the analysis of conventional fault-tolerant architectures. Various analytical techniques have been proposed to evaluate CED schemes deterministically. However, these approaches are based on worst-case assumptions related to the failure of system components. Often, the evaluation results do not reflect the actual fault tolerance capabilities of the system. A probabilistic approach to evaluate the fault detecting and locating capabilities of online checks. in a system is developed. The various probabilities associated with the checking schemes are identified and used in the framework of the matrix-based model. Based on these probabilistic matrices, estimates for the fault tolerance capabilities of various systems are derived analytically.", "num_citations": "13\n", "authors": ["493"]}
{"title": "Just-in-time methods for semiconductor manufacturing\n", "abstract": " A method is presented that allows job shops, such as wafer fabrication lines (fabs), to be analyzed for the application of just-in-time (JIT) production methods. From the resources available and the process recipe, a Kanban policy is derived that allows a pull-type production system to be implemented. Using this method example fabs were analyzed and the JIT/Kanban policies derived were simulated. The simulations show that the proposed method performs very well compared to other fab scheduling policies.< >", "num_citations": "13\n", "authors": ["493"]}
{"title": "Automatic assertion generation for simulation, formal verification and emulation\n", "abstract": " Verification is a critical step in the Integrated Circuit (IC) design process. In order to verify a design, a set of assertions based on the design, is generated. The design is checked, either using simulation or formal tools, to make sure that the design does not violate any of the generated assertions. If any of the assertions is violated, then a design bug is detected. The quality of the verification is directly connected to the set of assertions and how much of the design functionality they cover. In this paper, we propose a method to automatically generate a set of high-quality design assertions. The method is based on the design description and on Automatic Test Pattern Generation (ATPG). The proposed method generates design assertions that cover 100% of the design input space and could span multiple clock cycles. In our experiment, we generated properties/assertions for a USB2.0 model from OpenCores. These\u00a0\u2026", "num_citations": "12\n", "authors": ["493"]}
{"title": "Fast evaluation of test vector sets using a simulation-based statistical metric\n", "abstract": " Evaluating the coverage of tests for large circuits is computationally very intensive, particularly for logic BIST, software-based self test and on-line test schemes. This has led to research into techniques for rapidly evaluating the coverage of proposed test. We introduce a new metric which is highly correlated with fault coverage measured by gate-level simulators. Based on this metric, we estimate the time when the fault coverage saturates. This is done with only one pass of simulation and it provides a measure of the effectiveness of the test sequence when applied to the circuit-under-test; additionally, the fault coverage can be estimated with a relatively small number of test vectors. Experimental results on the ISCAS'85 and ISCAS'89 benchmarks, and a RISC processor (OR1200), show an average error of 2.85% in the estimated fault coverage compared with the fault coverage from full fault simulation, with an average\u00a0\u2026", "num_citations": "12\n", "authors": ["493"]}
{"title": "Connecting different worlds\u2014Technology abstraction for reliability-aware design and Test\n", "abstract": " The rapid shrinking of device geometries in the nanometer regime requires new technology-aware design methodologies. These must be able to evaluate the resilience of the circuit throughout all System on Chip (SoC) abstraction levels. To successfully guide design decisions at the system level, reliability models, which abstract technology information, are required to identify those parts of the system where additional protection in the form of hardware or software coun-termeasures is most effective. Interfaces such as the presented Resilience Articulation Point (RAP) or the Reliability Interchange Information Format (RIIF) are required to enable EDA-assisted analysis and propagation of reliability information. The models are discussed from different perspectives, such as design and test.", "num_citations": "12\n", "authors": ["493"]}
{"title": "Rethinking error injection for effective resilience\n", "abstract": " Soft errors, caused by radiation, have become a major challenge in today's computer systems and networking equipment, making it imperative that systems be designed to be resilient to errors. Error injection is a powerful approach to evaluate system resilience, and current practice is to inject errors in architectural registers of processors, program variables of applications, or storage elements in the hardware model. This paper, using answers to frequently asked questions, discusses the need for rethinking conventional approaches to error injection, showing data from recent research and our simulation results. Approaches to improving current error injections are also suggested.", "num_citations": "12\n", "authors": ["493"]}
{"title": "Post-silicon timing validation method using path delay measurements\n", "abstract": " In the nanometer era, the mismatch between the pre-silicon model and the post-silicon timing behavior is becoming severer. Therefore, it is necessary to validate timing with post-silicon data. We propose a method that estimates all the segment delays in the observed paths of a design from post-silicon path delay measurements. Our method is based on equality-constrained least squares methods, which enable us to find a unique and optimized solution of segment delays from underdetermined systems. Experimental results show that segment delays obtained using our method achieved correlation ranged from 0.848 to 0.992 to the sampled segment delays for different ISCAS-85 benchmark circuits.", "num_citations": "12\n", "authors": ["493"]}
{"title": "Lfsr-based performance characterization of nonlinear analog and mixed-signal circuits\n", "abstract": " This paper presents an efficient pseudorandom (PR) test method to characterize the performance of nonlinear analog and mixed-signal (AMS) circuits including those embedded in SoC devices. Previous applications of the PR test method to BIST have been limited to digital and linear analog circuits. In this paper, we extend the application of PR test to nonlinear AMS circuits. In doing so, we reduce the cost of testing nonlinear circuits, and increase the test coverage of embedded AMS circuits without incurring a large area overhead to accommodate a test stimulus generator. Our method maintains good test accuracy by using a Volterra series model to describe the behavior of the device under test (DUT). A PR sequence generated from a simple LFSR is used to excite the DUTs over a wide range of frequencies and estimate the parameters of the Volterra series, which are then used to predict the performance of\u00a0\u2026", "num_citations": "12\n", "authors": ["493"]}
{"title": "Transformer-coupled loopback test for differential mixed-signal specifications\n", "abstract": " Loopback tests for a differential mixed-signal device under test (DUT) have rarely been attempted, since any imbalance introduced by a design for test (DfT) circuitry on differential signaling delivers an imperfect sinusoidal wave to the DUT input, thereby degrading the DUT performance. In addition, this methodology inherently suffers from fault masking. These problems result in low test accuracy and serious yield loss. This paper presents a novel methodology for efficient prediction of individual DUT dynamic performance parameters with a radio-frequency (RF) transformer in loopback mode to overcome the imbalance problem of DfT circuitry. Cascaded RF transformer in loopback mode produces differently weighted loopback responses, which are used to characterize the DUT dynamic performance. Hardware measurement results show that this approach can be effectively used to predict the specifications of a DUT.", "num_citations": "12\n", "authors": ["493"]}
{"title": "Adaptive design for performance-optimized robustness\n", "abstract": " We present adaptive design techniques that compensate for manufacturing induced process variations in deep sub-micron (DSM) integrated circuits. Process variations have a significant impact on parametric behavior of modern chips, and adaptive design techniques that make a chip self-configuring to work optimally across process corners are fast evolving as a potential solution to this problem. Such schemes have two main components, a mechanism for sensing process perturbations, and one or more process compensation schemes that are driven by this mechanism. The adaptive design schemes presented in this paper are simple, low overhead techniques for noise tolerance in DSM CMOS circuits, to enhance their manufacturing yield. The process perturbation sensing scheme is based on on-chip delay measurement with a performance based bound on adaptation, which enables performance optimized\u00a0\u2026", "num_citations": "12\n", "authors": ["493"]}
{"title": "DSP-based statistical self test of on-chip converters\n", "abstract": " We propose a DSP-based statistical self test approach for testing on-chip data converters. Analog to digital converters (ADCs) and digital to analog converters (DACs) can be tested in a loop-back mode, providing a go/no-go result; however such tests focus on catastrophic fault coverage. We develop a technique for testing converters in loop-back mode which is simple, but has good parametric as well as catastrophic fault coverage. We use the on-chip digital signal processing unit to generate test stimuli. The analysis of the results is done through the use of software on the DSP unit which is capable of monitoring primary inputs, outputs and/or internal nodes. Characterization of actual /spl Delta//spl Sigma/ converters was performed to show the feasibility of the proposed method.", "num_citations": "12\n", "authors": ["493"]}
{"title": "CHAN: An efficient critical path analysis algorithm\n", "abstract": " An efficient critical path analysis algorithm (CHAN) based on the automatic test pattern generation (ATPG) method PODEM is presented. The approach does not require generation of the path list and elimination of false paths, and the critical path of the circuit is found accurately. In CHAN, the limitations of conventional approaches are overcome, and the critical path is detected in vastly improved times in most cases considered.< >", "num_citations": "12\n", "authors": ["493"]}
{"title": "Issues in logic synthesis for delay and bridging faults\n", "abstract": " The synthesis and testing issues for certain nonclassical faults (delay and bridging faults) are considered. Cost versus performance tradeoffs in synthesizing circuits which are robust delay fault testable are first examined. A new type of scan latch design is presented for robust delay fault testability of sequential circuits. The testability of bridging faults in combinational logic is also addressed. Tests are generated using implicit don't cares obtained for the circuit during the synthesis process. A PODEM-based algorithm has been developed for fast detection of a class of bridging faults.< >", "num_citations": "12\n", "authors": ["493"]}
{"title": "Derivation of signal flow for switch-Level simulation\n", "abstract": " This paper presents a new algorithm for deriving the direction of signal flow in MOS circuits. The algorithm detects so-called unidirectional transistors. In a unidirectional transistor, signal flow is restricted to one direction during switch-level simulation, without compromising the simulation results. The algorithm uses a static analysis of the switch-level characteristics of the circuit, such as the transistor strengths and node capacitor sizes. It was implemented and used in the simulation of a large, commercial microprocessor. For this processor, 98.5% of the transistors were determined to be unidirectional by the algorithm. The simulation time for this processor decreased significantly when unidirectional transistors were detected.<>", "num_citations": "12\n", "authors": ["493"]}
{"title": "A framework for low overhead hardware based runtime control flow error detection and recovery\n", "abstract": " Transient errors during execution of a process running on a processor can lead to serious system failures or security lapses. It is necessary to detect, and if possible, correct these errors before any damage is caused to the system. Of the many approaches, monitoring the control flow of an application during runtime is one of the techniques used for transient error detection during an application execution. Although promising, the cost of implementing the control flow checks in software has been prohibitively high and hence is not widely used in practice. In this paper we describe a hardware based control flow monitoring technique which has the capability to detect errors in control flow and the instruction stream being executed on a processor. Our technique achieves a high coverage of control flow error detection (99.98 %) and has the capability to quickly recover from the error, making it resilient to transient control\u00a0\u2026", "num_citations": "11\n", "authors": ["493"]}
{"title": "Control flow deviation detection for software security\n", "abstract": " Provided are methods and systems for control flow deviation detection. Provided are methods for software security, comprising executing a software program, generating a run-time signature variable, updating the run-time signature variable as the software program executes, comparing the run-time signature variable with a pre-computed signature, and detecting a deviation in control flow of the software program based on the comparison between the run-time signature variable and the pre-computed signature.", "num_citations": "11\n", "authors": ["493"]}
{"title": "Predicting mixed-signal dynamic performance using optimised signature-based alternate test\n", "abstract": " Accurate generation of circuit specifications from test signatures is a difficult problem, since analytical expressions cannot precisely describe the nonlinear relationships between signatures and specification. In addition, it is difficult to precisely control physical factors in built-in self-test circuitry, which can cause errors in the signatures. A methodology for efficient prediction of circuit specifications with optimised signatures has been proposed. The proposed optimised signature-based alternate test methodology accurately predicts the specifications of a Device Under Test (DUT) using a strong correlation mapping function. Hardware measurement results show that this approach can be effectively used to predict the specifications of a DUT, with a significant reduction in the prediction error compared with previous approaches.", "num_citations": "11\n", "authors": ["493"]}
{"title": "Automatic insertion of low power annotations in RTL for pipelined microprocessors\n", "abstract": " We propose instruction-driven slicing, a technique for annotating microprocessor descriptions at the register transfer level (RTL) in order to achieve lower power dissipation. Our technique automatically annotates existing RTL code to optimize the circuit for lowering power dissipated by switching activity. Our technique can be applied at the architectural level as well, achieving similar power gains. We demonstrate our technique on architectural and RTL models of a 32-bit OpenRISC processor (OR1200), showing power gains for the SPEC2000 benchmarks", "num_citations": "11\n", "authors": ["493"]}
{"title": "A low latency and low power dynamic carry save adder\n", "abstract": " This paper presents a 4-to-2 Carry Save Adder (CSA) using dynamic logic and the Limited Switch Dynamic Logic (LSDL) circuit family. Adders are a crucial portion of all floating-point units, since they form the base element of all arithmetic functions. The 4-to-2 circuits reported previously do not meet the requirements of the next generation of processors. The adder presented here is built using a dynamic circuit style that improves performance significantly. Further a latching element after each dynamic evaluation node controls the power of the dynamic circuits. In this paper we project some of the salient features of the LSDL circuit family by comparing this 4-2 circuit with the most similar static implementation. Use of the LSDL circuit family displays significant improvement not only in terms of performance but also with respect to power dissipation, leakage and area.", "num_citations": "11\n", "authors": ["493"]}
{"title": "Design and modeling of a 16-bit 1.5 MSPS successive approximation ADC with non-binary capacitor array\n", "abstract": " The design and modeling of a high performance successive approximation analog-to-digital converter (ADC) using non-binary capacitor array are presented in this paper. A non-binary capacitor array with 20 capacitors is used to design a 16-bit, 1.5 mega samples per second (MSPS) successive approximation ADC. A perceptron learning rule, originally developed for Artificial Intelligence applications, is used as the capacitor calibration algorithm. The system architecture and the circuit design for the capacitor array, the sampling network and the high performance comparator are discussed. The capacitor weights are adaptively calibrated to match the physical capacitors with better than 22-bit accuracy. Capacitor matching is not a limiting factor to the accuracy. Various sources of noise, interference and distortion are modeled to evaluate their effects and to ensure the robustness of the calibration algorithm. This\u00a0\u2026", "num_citations": "11\n", "authors": ["493"]}
{"title": "Full chip false timing path identification: applications to the PowerPC/sup TM/microprocessors\n", "abstract": " Static timing anaylsis sets the industry standard in the design methodology of high speed/performance microprocessors to determine whether timing requirements have been met. Unfortunately, not all the paths identified using such analysis can be sensitized. This leads to a pessimistic estimation of the processor speed. Also, no amount of engineering effort spent on optimizing such paths can improve the timing performance of the chip. In the past we demonstrated initial results of how ATPG techniques can be used to identify false paths efficiently. Due to the gap between the physical design on which the static timing analysis of the chip is bused and the test view on which the ATPG techniques are applied to identify false paths, in many cases only sections of some of the paths in the full-chip were analyzed in our initial results. In this paper, we will fully analyze all the timing paths using the ATPG techniques, thus\u00a0\u2026", "num_citations": "11\n", "authors": ["493"]}
{"title": "A quick and inexpensive method to identify false critical paths using ATPG techniques: an experiment with a PowerPC/sup TM/microprocessor\n", "abstract": " Static timing analysis tools are used by designers of high speed/high performance circuits to determine whether timing requirements are met. Timing analysis tools can report critical paths which are characterized by a transition on each node along the path, however, they cannot generate a \"witness\" vector which would sensitize that path. This gives rise to the possibility of having paths which are reported by the static timing analysis tool as potential critical paths, whereas there exists no vector sequence which can sensitize them. Our goal is to identify these \"false critical timing paths\" safely and without much overhead, so that the efforts needed to redesign and/or optimize critical paths can be reduced. We have devised a simple technique using a tool that we have written and a commercial ATPG tool to meet this goal. We applied the technique on the state of the art fourth generation MPC7400 PowerPC/sup TM\u00a0\u2026", "num_citations": "11\n", "authors": ["493"]}
{"title": "Efficient techniques for the analysis of algorithm-based fault tolerance (ABFT) schemes\n", "abstract": " This paper presents a model which can be used to characterize the diagnosability of Algorithm-Based Fault Tolerant (ABFT) systems. In the model, the relationship between processors computing useful data, the output data, and the check processors is defined in terms of matrix entries. Necessary and sufficient conditions for detecting and locating faults in the processors are derived, and based on them, efficient algorithms to evaluate the fault detection and location capabilities of the system are developed.", "num_citations": "11\n", "authors": ["493"]}
{"title": "Challenges in fault detection\n", "abstract": " A basic capability required of fault-tolerant systems is the detection of a variety of faults. There has been a much research over the past years in this area, and many papers have been published in the various Symposia on Fault-Tolerant Computing (FTCS) which started in 1971. This paper discusses the basic problems and approaches in dealing with hardware faults and points out some of the techniques which deal with them, with particular reference to those published in the symposia. Technology trends are examined to provide an idea of problems emerging in the future, and possible directions for solution are identified.", "num_citations": "11\n", "authors": ["493"]}
{"title": "A new scheme to compute variable orders for binary decision diagrams\n", "abstract": " Introduces some new methods for estimating the \"importance\" of a variable in a Boolean function, and uses them to compute variable orders for OBDD construction. These measures are based on information theoretic criteria, and require the computation of the entropy of a variable in a given function. These entropy measures prove quite effective in distinguishing the importance of variables. Experimental results show this to be a very encouraging approach to help in the solution of this well known problem.< >", "num_citations": "11\n", "authors": ["493"]}
{"title": "EFFICIENT CONCURRENT ERROR DETECTION IN PLAS AND ROMS.\n", "abstract": " Efficient schemes are presented for concurrent detection of errors produced by physical failures during the normal operation of typical VLSI implementations of programmable logic arrays (PLAs) and read-only memories (ROMs). Comprehensive error detection is provided for both permanent and nonpermanent (transient/intermittent) faults. The schemes are based on general fault models and are applicable to various modified implementations of PLAs and ROMs. Fault avoidance for certain classes of failures is ensured by suggested mask-level design rules. A low-overhead solution is provided to the problem of concurrent error detection in a variety of VLSI logic arrays.", "num_citations": "11\n", "authors": ["493"]}
{"title": "In-depth soft error vulnerability analysis using synthetic benchmarks\n", "abstract": " Statistical fault injection is widely used for analyzing hardware in the presence of soft errors. Although this method can give accurate results for averaged erroneous outcomes with a fairly small sample size, it will not be accurate for vulnerability analysis of each sequential element in the design with small sample sizes. This paper describes a novel and highly efficient technique which is suitable for detailed vulnerability analysis of a processor. The technique involves specific sets of assembly language routines, and is shown to be much more efficient and comprehensive compared with traditional statistical error injection on a predetermined set of benchmarks. We have shown the effectiveness of the method using error injection in an ARM Amber25 processor model. Our analysis is based on more than 330,000 simulation runs with single bit-flips on the sequential elements of this processor running our synthetic\u00a0\u2026", "num_citations": "10\n", "authors": ["493"]}
{"title": "FALCON: Rapid statistical fault coverage estimation for complex designs\n", "abstract": " FALCON (FAst fauLt COverage estimatioN) is a scalable method for fault grading which uses local fault simulations to estimate the fault coverage of a large system. The generality of this method makes it applicable for any modular design. Our analysis shows that the run time of our algorithm is related to the number of gates and the number of IOs in a module, while fault simulation run time is related to the total number of gates in the system. We have measured fault coverage for OR1200 and IVM processors and compared the results with fault simulation performed by a commercial tool. We have also compared our results with fault sampling. Our results show that for large designs FALCON is an order of magnitude faster compared with fault simulation. It also has a smaller error rate compared with fault sampling when the size of design under test grows.", "num_citations": "10\n", "authors": ["493"]}
{"title": "Recursive path selection for delay fault testing\n", "abstract": " This paper presents a new path selection algorithm for delay fault testing in a statistical timing framework. Existing algorithms which consider correlation between paths use an iterative process for each path or defect and require a Monte Carlo simulation for each iteration to calculate the conditional fault probability. The proposed algorithm does not require the iteration process and selects a requested number of paths simultaneously once it performs a statistical timing analysis at the beginning. If selection of k paths is required in a set of paths, it partitions the set into two path sets and determines how many paths should be selected in each path set out of the k paths. It recursively continues this process and ends up with k paths. The partitioning is easily performed during the recursive traversal of a circuit, which produces an imaginary path tree, where paths are already grouped based on their prefix. Experimental\u00a0\u2026", "num_citations": "10\n", "authors": ["493"]}
{"title": "A timing methodology considering within-die clock skew variations\n", "abstract": " Timing margining is a key component of timing sign-off. Insufficient margin can lead to silicon failure and excessive pessimistic margin will entail unnecessary design optimization effort. Timing margin is intended to cover the uncertainty in clock arrival times and clock skews arising from within-die process variations. In highly scaled technologies, the increased process variations tend to enforce an overestimation of timing margins making it difficult for the designs to achieve the target performance. In this paper, we present a more efficient margining methodology to account for clock-skew variations arising due to within-die variations. The proposed methodology fits well within current corner based timing sign-off framework and allows for significant reduction in margin pessimism. We present the results and observations on a low power processor for hold-time margin correction. Evaluation of the proposed methodology\u00a0\u2026", "num_citations": "10\n", "authors": ["493"]}
{"title": "Analytical model for the impact of multiple input switching noise on timing\n", "abstract": " The timing models used in current Static Timing Analysis tools use gate delays only for single input switching events. It is well known that the temporal proximity of signals arriving at different inputs causes significant variation in the gate delay. This variation in delay affects the accuracy of our timing estimates. In this paper, we derive simple analytical models for incorporating the effect of simultaneous multiple input switching events on gate delay. The model presented requires minimum additional characterization effort, and can be employed in a statistical timing engine. The dynamic delay variability of a path caused by MIS noise can be accurately estimated using the proposed model.", "num_citations": "10\n", "authors": ["493"]}
{"title": "A robust top-down dynamic power estimation methodology for delay constrained register transfer level sequential circuits\n", "abstract": " We present a top-down dynamic power estimation methodology for delay constrained sequential circuits. The methodology works at the register transfer level (RT-Level), and applies to both structural and behavioral descriptions of circuits. The average power consumption of a circuit varies with the worst case cycle-time or frequency of operation. As the cycle-time is reduced, the increase in the capacitance of the circuit due to technology mapping and optimization is captured by our technique at the RT-Level using the principles of logical effort. Switching activity is obtained at the RT-Level visible nodes through RT-Level functional simulation. This information is utilized to approximate the activities at the remaining nodes of the circuit and combined with capacitance to estimate dynamic power. Power estimation results for RT-Level sequential circuits indicate good accuracy (average error<10%) with respect to the\u00a0\u2026", "num_citations": "10\n", "authors": ["493"]}
{"title": "Formal verification of a system-on-chip using computation slicing\n", "abstract": " Formal verification of systems-on-chips (SoCs) is an immense challenge to current industrial practice. Most existent formal verification techniques are extremely computation intensive and produce good results only when used on individual sub-components of SoCs. Without major modifications they are of little effectiveness in the SoC world. We attack the problem of SoC verification using an elegant abstraction mechanism, called computation slicing, and show that it enables effective temporal property verification on large designs. The technique targets a set of execution sequences, that is exhaustive with respect to an intended subset of system level properties, and automatically finds counter-example execution sequences in case of errors in the design. We have obtained exponential gains in reducing the global state space using a polynomial-time algorithm, and also applied a polynomial-time algorithm for\u00a0\u2026", "num_citations": "10\n", "authors": ["493"]}
{"title": "Optimal BIST using an embedded microprocessor\n", "abstract": " Systems-on-a-chip (SOCs) with many complex intellectual property (IP) cores require a large number of test patterns and a large volume of data. The computing power of the embedded processor in an SOC can be used to test the cores within the chip boundary, reducing the test time and memory requirements. This paper discusses techniques that use the computing power of the embedded processor in a more sophisticated way to significantly reduce memory requirements and the number of test applications, and hence the testing time. The processor can generate random patterns and selectively apply those patterns that contribute to the fault coverage. It can also apply deterministic test patterns that have been compressed using the characteristics of the random patterns as well as the deterministic patterns themselves. Fast run-length coding schemes which are easily implemented and effective for test data\u00a0\u2026", "num_citations": "10\n", "authors": ["493"]}
{"title": "Causality based generation of directed test cases\n", "abstract": " Simulation is considered to be the irreplaceable part of design verification. How ev er, the efficiency of this method depends greatly on the test cases used. Random test cases can be generated quickly but ha vepoor coverage. Directed test cases on the other hand require time and manual effort. This paper presents a method for generating directed test cases automatically by making use of signal relationships in the specification. An algorithm is presented that was applied to the GL85 microprocessor, a clone of Intel's 8085. The results are compared with other methods to see the gain with the proposed method.", "num_citations": "10\n", "authors": ["493"]}
{"title": "Automatic validation test generation using extracted control models\n", "abstract": " We present a procedure for the automatic generation of tests covering control states of a sequential circuit. The procedure consists of extracting a control model of the circuit under test and then using this model to guide the search for concrete executions or witnesses. We present results of experiments using the procedure on a communication chip from industry as well as an implementation of the ARM 2 processor.", "num_citations": "10\n", "authors": ["493"]}
{"title": "Efficient variable ordering and partial representation algorithm\n", "abstract": " In this paper we introduce some new methods for constructing Ordered Partial Decision Diagrams (OPDDs), The algorithms are effective in capturing a significant fraction of a given function's truth table using only a very small space. Using such data structures the importance of a variable in a Boolean function can be computed. Such methods can easily be used for computing effective variable orders to construct BDDs. The measures of a variable's importance are based on information-theoretic criteria, and require computation of the entropy of a variable for a given function. We have found that entropy measures can be quite efficient in distinguishing the importance of variables, and at times provide very effective variable order. The results show an encouraging approach towards the understanding and the solution of this well known problem.", "num_citations": "10\n", "authors": ["493"]}
{"title": "Safety design of a convolutional neural network accelerator with error localization and correction\n", "abstract": " Recently neural network accelerators have grown into prominence with significant power and performance efficiency improvements over CPU and GPU. In this paper, we proposed two safety design techniques include Algorithm Based Atomic Error Checking-1 (ABAEC-1) and ABAEC-2 for a Weight Stationary (WS) Convolutional Neural Network (CNN) accelerator focusing on low latency and low overhead error detection and correction with no performance degradation. The proposed design techniques not only detect the errors on-the-fly but also perform error diagnosis to localize the errors to a Processing Element (PE) for on-line fault management and recovery. We applied the design techniques on an industry quality CNN accelerator and demonstrated that we could achieve the required Diagnostic Coverage (DC) goal with minimal area and power overhead for selected configurations. Furthermore, we\u00a0\u2026", "num_citations": "9\n", "authors": ["493"]}
{"title": "Enhanced algorithm of combining trace and scan signals in post-silicon validation\n", "abstract": " As the complexity of integrated circuit design increases and production schedules become shorter, the dependency on post-silicon validation for capturing design errors that escape from pre-silicon verification also increases. A major challenge in post-silicon validation is the limited observability of internal states caused by the limited storage capacity available for post-silicon validation. Recent research has shown that observability can be enhanced if trace and scan signals are combined together, compared with the debugging scenario where only trace signals are monitored. This paper proposes an enhanced and systematic algorithm for the efficient combination of trace and scan signals to maximize the observability of internal circuit states. Experimental results on benchmark circuits show that the proposed technique provides a higher number of restored states compared to the existing techniques.", "num_citations": "9\n", "authors": ["493"]}
{"title": "Imbalance-based self-test for high-speed mixed-signal embedded systems\n", "abstract": " Precisely measuring specifications of differential analog and mixed-signal circuits is a difficult problem for self-test development because the imbalance introduced by the design-for-test circuitry on the differential signaling causes nonlinearity on the test stimulus, resulting in degrading device-under-test (DUT) performance. This problem triggers low test accuracy and serious yield loss. This brief proposes a novel test methodology to accurately predict individual DUT specifications by overcoming the imbalance problem with the imbalance generator, a radio-frequency transformer, and a programmable capacitor array based on a loopback test configuration. The imbalance generator produces spectral loopback responses of different weight. Nonlinear equations are then derived to characterize DUT specifications. Hardware measurement results show that this approach can be used to predict the specifications of a DUT\u00a0\u2026", "num_citations": "9\n", "authors": ["493"]}
{"title": "Delay defect diagnosis methodology using path delay measurements\n", "abstract": " With aggressive device scaling, timing failures have become more prevalent due to manufacturing defects and process variations. When timing failure occurs, it is important to take corrective actions immediately. Therefore, an efficient and fast diagnosis method is essential. In this paper, we propose a new diagnostic method using timing information. Our method approximately estimates all the segment delays of measured paths in a design using inequality-constrained least squares methods. Then, the proposed method ranks the possible locations of delay defects based on the difference between estimated segment delays and the expected values of segment delays. The method works well for multiple delay defects as well as single delay defects. Experiment results show that our method yields good diagnostic resolution. With the proposed method, the average first hit rank (FHR), was within 7 for single delay defect\u00a0\u2026", "num_citations": "9\n", "authors": ["493"]}
{"title": "Toward reliable SRAM-based device identification\n", "abstract": " Due to process variation, power-up values of embedded SRAM memory are unique for individual devices. They are used as SRAM fingerprints to identify integrated-circuits which is fundamental for security applications. The fingerprints, however, are sensitive to environmental changes. Consequently, during the identification process, errors may occur. To overcome this inherent nondeterminism, we provide a systematic approach to designing reliable SRAM-based identification system. We also discuss how to evaluate its system performance. We present a generic score-fusion-based matching recipe to identify devices with high confidence across a wide range of environmental conditions.", "num_citations": "9\n", "authors": ["493"]}
{"title": "Critical path selection for delay testing considering coupling noise\n", "abstract": " The objective of delay testing is to detect any defects or variations that manifest into timing failures. In path based delay testing this is done by testing a subset of paths in the circuit that are more likely to fail and hence are critical. Since path delays are vector dependent, the set of critical paths selected depends on the vectors assumed when estimating the path delays. This implies that to find the real critical paths, it is important to consider the effect of dynamic (vector dependent) delay effects such as coupling noise and supply noise during path selection. In this work a methodology to incorporate the effect of coupling noise during path selection is described. For any given path, both logic and timing constraints are extracted and a constrained optimization problem is formulated to estimate the maximum path delay in the presence of coupling noise.", "num_citations": "9\n", "authors": ["493"]}
{"title": "On-line calibration and power optimization of RF systems using a built-in detector\n", "abstract": " This paper develops a technique, using a built-in detector, for measuring the specifications of RF subsystems and fine-tuning them with a feedback control algorithm. At the same time, optimum power consumption points can be chosen from different biasing schemes. The detector has small area overhead with low frequency output. The sampled output waveform is analyzed using an FFT. The low frequency measurements are directly used to calculate the circuit specifications, without requiring learning steps. The technique was used to measure the performance parameters of a 940 MHz to 40 MHz down conversion mixer in a RF receiver front-end test chip, fabricated in a commercial 0.18 mum CMOS process. The tuning algorithm was implemented in the Labview environment, and tuning knobs on board were used for biasing and control optimization. Results show that the approach can provide accurate calibration\u00a0\u2026", "num_citations": "9\n", "authors": ["493"]}
{"title": "Optimized signature-based statistical alternate test for mixed-signal performance parameters\n", "abstract": " Accurate generation of circuit specifications from test signatures is a difficult problem, since analytical expressions cannot precisely describe the nonlinear relationships between signatures and specification. In addition, it is difficult to precisely control physical factors in built-in self-test (BIST) circuitry, which can cause errors in the signatures. This paper presents a novel methodology for efficient prediction of circuit specifications with optimized signatures. The proposed optimized signature based alternate test (OSBAT) methodology accurately predicts the specifications of a DUT using a strong correlation mapping function. Hardware measurement results show that this approach can be effectively used to predict the specifications of a DUT, with a significant reduction in the prediction error compared with previous approaches", "num_citations": "9\n", "authors": ["493"]}
{"title": "Circuit modeling apparatus, systems, and methods\n", "abstract": " Apparatus and systems, as well as methods and articles, may perform operations including selecting a monitor associated with a property of a circuit module, augmenting the circuit module with the monitor to provide an augmented circuit, searching for a test for an output of the augmented circuit to find a sequence of states having a length up to n, establishing a witness to the property if the test is found, and if no test is found to exist within the sequence of states, determining the property to be invalid or false for a bound of n.", "num_citations": "9\n", "authors": ["493"]}
{"title": "A comprehensive fault model for deep submicron digital circuits\n", "abstract": " Identifies the broad categories of defects which need to be considered in DSM technologies. We show that many of these defects cannot be detected using existing fault models and test approaches, and propose a new fault model for DSM circuits which incorporates logic levels as well as path delay information to deal with both functionality and performance. We show that tests derived using this model can be used to effectively screen chips for defects which affect the functionality and performance of the chips, and that the approach reduces the test costs and defect levels when compared with conventional approaches. Experimental results on large benchmark circuits are used to demonstrate the usefulness of the approach.", "num_citations": "9\n", "authors": ["493"]}
{"title": "Validation of PowerPC/sup TM/custom memories using symbolic simulation\n", "abstract": " This paper describes the use of Symbolic Trajectory Evaluation (STE), a modified form of symbolic simulation, to verify the equivalence between RTL and transistor-level representations of on-chip custom memories for the latest PowerPC microprocessor. The validation of embedded memories and their associated control logic poses a special problem for traditional formal equivalence checking tools due to the inherently sequential and self-timed nature of the internal control logic and the large number of state-holding elements. The use of the VERSYS STE engine to validate these custom memories is illustrated. We present our array verification methodology, discuss some of the results of our approach, and outline plans for future development.", "num_citations": "9\n", "authors": ["493"]}
{"title": "A novel hierarchical test generation method for processors\n", "abstract": " This paper describes a novel method for hierarchical functional test generation for processors. This method targets one embedded module at a time and uses commercial ATPG tools to derive tests for faults within the module. Since the commercial tools are unable to deal with the entire design, functional constraints are first extracted for the module. The extracted constraints are described in Verilog/VHDL and synthesized to the gate level. Then a commercial sequential ATPG is used to generate module level test vectors for faults within the module. Finally, these module level vectors are translated to processor level functional vectors and fault simulated to verify that the same coverage is obtained. Applying the technique to a benchmark processor design, we were able to obtain a test efficiency for the embedded ALU of the processor which was extremely close to what the commercial ATPG could do with complete\u00a0\u2026", "num_citations": "9\n", "authors": ["493"]}
{"title": "A modular robust binary tree\n", "abstract": " This paper presents a new robust binary tree designed using the theory of robust data structures. A basic module composed of three elements is replicated as necessary to form the robust tree, allowing a tree to be built up in a modular fashion while preserving its robust characteristics. The proposed structure is shown to be able to detect 2 errors (2-detectable) or correct 1 error (1-correctable) in any module of the tree under a generic fault model. The advantages of the tree when compared with other proposed trees presented in the literature are presented. Results of fault/error injection experiments on an implementation of the proposed structure in C++, including error coverage and performance overhead, are also provided.", "num_citations": "9\n", "authors": ["493"]}
{"title": "Interlock schemes for micropipelines: Application to a self-timed rebound sorter\n", "abstract": " Event controlled pipelines (micropipelines) have several advantages over clocked pipelines as they offer modularity and speed independence. The concept of micropipelines can be easily applied to any linear pipeline. In practice, most pipelines include feedback loops and, therefore, are not as easy to control as linear pipelines. A novel interlocking scheme to solve the control flow problem is proposed. As an application, the design of a self-timed rebound sorter is considered where interlocking schemes are required to ensure proper operation of the pipeline. Similar interlocking schemes can be used for other pipelines with feedback.<>", "num_citations": "9\n", "authors": ["493"]}
{"title": "Design and evaluation tools for fault-tolerant systems\n", "abstract": " This paper discusses the automated tools, both hardware and software, that are necessary for the design and evaluation of large complex high performance faulttolerant systems. The design and evaluation tasks to be performed are summarized. The effectiveness of automated tools depends upon the characteristics and appropriateness of the models of computing systems used in the tools. The paper begins with a brief discussion of appropriate models with desirable properties. Requirements for a set of tools are discussed. Taken together, these tools will aid design and evaluation at all levels of computer system design, beginning with defining system goals and ending with monitoring system operation. Tools currently used for such work are described briefly and references to complete descriptions given. A plan for developing an integrated set of design and evaluation tools is outlined, and the risks in embarking\u00a0\u2026", "num_citations": "9\n", "authors": ["493"]}
{"title": "Real-time error detection in nonlinear control systems using machine learning assisted state-space encoding\n", "abstract": " Successful deployment of autonomous systems in a wide range of societal applications depends on error-free operation of the underlying signal processing and control functions. Real-time error detection in nonlinear systems has mostly relied on redundancy at the component or algorithmic level causing expensive area and power overheads. This paper describes a real-time error detection methodology for nonlinear control systems for detecting sensor and actuator degradations as well as malfunctions due to soft errors in the execution of the control algorithm on a digital processor. Our approach is based on creation of a redundant check state in such a way that its value can be computed from the current states of the system as well as from a history of prior observable state values and inputs (via machine learning algorithms). By checking for consistency between the two, errors are detected with low latency. The\u00a0\u2026", "num_citations": "8\n", "authors": ["493"]}
{"title": "Congenital anomalies: the spectrum of distribution and associated maternal risk factors in a tertiary teaching hospital\n", "abstract": " Background: To study the system-wise occurrence of congenital anomalies in newborns admitted in a tertiary hospital and to study the associated maternal factors.Methods: This is a retrospective study of all the mothers and their newborn babies with congenital anomalies who were delivered or referred to the Obstetrical Department/Neonatology unit during a two-year study period. The maternal risk factors and associated Obstetric complications were studied.Results: Among the babies born with congenital anomalies, the systems most involved were Genito-urinary System (28.5%) and Cardiovascular System (20.5%). Among the maternal risk factors, Diabetes (14.01%), previous abortions (12.7%) and hypothyroidism (8.7%) were the most significant associated factors. Intrauterine growth restriction (17.4%) was noted to be more common in these babies.Conclusions: The incidence of anomalies was most involving\u00a0\u2026", "num_citations": "8\n", "authors": ["493"]}
{"title": "Phase-aware multitone digital signal based test for RF receivers\n", "abstract": " This paper presents a method for testing RF receivers that utilizes a multitone digital signal generation scheme and relies on the analysis of the receiver baseband output to compute the RF performance parameters. The proposed method eliminates the cost of expensive RF instrumentation on the input side of receiver testing and only requires low-cost on-chip baseband digitization at the output of the receiver. The complexity of the RF signal generation inherent to standard methods is traded off with extensive signal processing in the baseband. The analysis necessary for tackling the problem of extracting RF metrics is addressed and presented herein. Whereas the proposed test scheme was implemented and experimentally verified on a load board for testing UHF receivers, generalized use in BIST applications in need of multi-GHz RF stimuli is also discussed in the paper. RF performance parameters like Gain\u00a0\u2026", "num_citations": "8\n", "authors": ["493"]}
{"title": "Run-time prediction of the optimal performance point in DVS-based dynamic thermal management\n", "abstract": " Due to the increasing trend toward greater processor power density and computationally intensive applications, Dynamic Thermal Management (DTM) has become an essential technique in modern processors. Among many DTM techniques, Dynamic Voltage Scaling (DVS) is widely used because of its chief virtue - a cubic reduction in power at the relatively minor cost of a linear performance penalty. Because this reduction comes at a cost in execution speed, a key point of DVS-based DTM research is how accurately the processor predicts the optimal performance point where it can meet the thermal constraints while also minimizing the performance penalty. In this paper, we propose a new DVS-based DTM technique that makes the prediction of the optimal performance point more accurate. To achieve this, run-time prediction techniques are used and different power compositions due to process variations are\u00a0\u2026", "num_citations": "8\n", "authors": ["493"]}
{"title": "On-chip delay measurement based response analysis for timing characterization\n", "abstract": " We present techniques for response analysis for timing characterization, i.e., delay test and debug of Integrated Circuits (ICs), using on-chip delay measurement of critical paths of the IC. Delay fault are a major source of failure in modern ICs designed in Deep Sub-micron technologies, making it imperative to perform delay fault testing on such ICs. Delay fault testing schemes should enable detection of gross as well as small delay faults in such ICs to be efficient. Additionally there is a need for performing efficient and systematic silicon debug for timing related failures. The timing characterization techniques presented in this paper overcome the observability limitations of existing timing characterization schemes in achieving the aforementioned goals, thus enabling quick and efficient timing characterization of DSM ICs. Additionally the schemes have low hardware overhead and are robust in face of process\u00a0\u2026", "num_citations": "8\n", "authors": ["493"]}
{"title": "Reducing test time and area overhead of an embedded memory array built-in repair analyzer with optimal repair rate\n", "abstract": " This paper presents a built-in self repair analyzer with the optimal repair rate for embedded memory arrays. The proposed method requires only a single test, even in the worst case. By performing the must-repair analysis on the fly during the test, it selectively stores fault addresses, and the final analysis to find a solution is performed on the stored fault addresses. To enumerate all possible solutions, existing techniques use depth first search using a stack and a FSM. Instead, we propose a new algorithm and its combinational circuit implementation. Since our formulation for the circuit allows us to use the parallel prefix algorithm, it can be configured in various ways to meet area and test time requirements. The total area of our infrastructure is dominated by the number of CAM entries to store the fault addresses, and it only grows quadratically with respect to the number of repair elements.", "num_citations": "8\n", "authors": ["493"]}
{"title": "Adaptive SRAM memory for low power and high yield\n", "abstract": " SRAMs typically represent half of the area and more than half of the transistors on a chip today. Variability increases as feature size decreases, and the impact of variability is especially pronounced on SRAMs since they make extensive use of minimum sized devices. Variability leads to a large amount of guard banding in the design phase in order to meet frequency and yield targets. We develop an SRAM architecture that eliminates guard banding. Specifically, our SRAM uses multiple supply voltages that are assigned post-manufacturing. We compensate for variation by powering up manufactured devices that are slower than designed. Specifically, we assign supply voltages to 6T cells on a per-column basis; this gives us sufficiently fine-grained control over devices without excessive area overhead. We show that post-manufacturing voltage assignment results in a 28% reduction in bitline energy compared to a\u00a0\u2026", "num_citations": "8\n", "authors": ["493"]}
{"title": "On efficient generation of instruction sequences to test for delay defects in a processor\n", "abstract": " We present a technique that deals with the problem of efficiently generating instruction sequences to test for delay defects in a processor. These instruction sequences are loaded into the cache of a processor and the processor is run in its normal functional (native) mode to test itself. The methodology that we present avoids the significant increase in search space of a previous method while generating tests. We also present a technique which increases the probability of detecting multiple delay faults with a single instruction sequence. This technique can help immensely in reducing the cost of test. We demonstrate the effectiveness of our technique on an off-the shelf processor.", "num_citations": "8\n", "authors": ["493"]}
{"title": "Effects of noise and nonlinearity on the calibration of a non-binary capacitor array in a successive approximation analog-to-digital converter\n", "abstract": " A successive approximation analog-to-digital converter using a nonbinary capacitor array is presented. A perceptron learning rule is used as the capacitor calibration algorithm. The nonlinearity is analyzed using the Volterra series. The effects of noise and nonlinearity are modeled to verify the calibration robustness. With the presence of noise and nonlinearity, the capacitor weights are adaptively calibrated to match the physical capacitors with better than 22-bit accuracy. The accuracy is no longer limited by capacitor matching.", "num_citations": "8\n", "authors": ["493"]}
{"title": "Static program transformations for efficient software model checking\n", "abstract": " Ensuring correctness of software by formal methods is a very relevant and widely studied problem. Automatic verification of software using model checking suffers from the state space explosion problem. Abstraction is emerging as the key candidate for making the model checking problem tractable, and a large body of research exists on abstraction based verification. Many useful abstractions are performed at the syntactic and semantic levels of programs and their representations.               In this paper, we explore abstraction based verification techniques that have been used at the program source code level. We provide a brief survey of these program transformation techniques. We also examine, in some detail, Program Slicing, an abstraction technique that holds great promise when dealing with complex software. We introduce the idea of using more specialized forms of slicing, Conditioned Slicing and\u00a0\u2026", "num_citations": "8\n", "authors": ["493"]}
{"title": "Mixed-signal micro-controller for non-binary capacitor array calibration in data converter\n", "abstract": " A mixed-signal micro-controller architecture is used to efficiently implement a novel non-binary capacitor array calibration algorithm for a successive approximation charge-redistribution data converter. The non-binary capacitor array calibration algorithm is based on a simple perceptron learning rule, originally developed for Artificial Intelligence applications. The digital capacitor weights are calibrated with up to 22-bit accuracy that is better than the noise level in the system. The resolution of the data converter is no longer limited by the capacitor array matching. The mixed-signal micro-controller is portable, scalable, and seamlessly handles the converter's analog and digital datapaths.", "num_citations": "8\n", "authors": ["493"]}
{"title": "Test generation for resistive opens in CMOS\n", "abstract": " This paper develops new techniques for detecting both stuck-open faults and resistive open faults, which result in increased delays along some paths. The improved detection of CMOS open defects is made possible by a new delay fault model which combines the advantages of the gate delay fault model and the path delay fault model. We develop a test generation methodology for this fault model which enables generation of test vectors that test a percentage of the longest sensitizable paths in the design and also test each net for spot defects through their longest sensitizable paths. Real delay values are used to determine the true critical paths in the circuit. The high degree of effectiveness of this fault model under realistic assumptions for process characteristics is first enumerated, and experimental results demonstrate the improved coverage possible with the proposed approach.", "num_citations": "8\n", "authors": ["493"]}
{"title": "Using Abstract Specifications to Verify PowerPC\u2122 Custom Memories by Symbolic Trajectory Evaluation\n", "abstract": " We present a methodology in which the behavior of a switch level device is specified using abstract parameterized regular expressions. These specifications are used to generate a finite automaton representing an abstraction of the behavior of a block of memory comprised of a set of such switch level devices. The automaton, in conjunction with an Efficient Memory Model [1], [2] for the devices, forms a symbolic simulation model representing an abstraction of the array core embedded in a larger design under analysis. Using Symbolic Trajectory Evaluation, we check the equivalence between a register transfer level description and a schematic description augmented with abstract specifications for one of the custom memories embedded in the MPC7450 PowerPC processor.", "num_citations": "8\n", "authors": ["493"]}
{"title": "Verification of delta-sigma converters using adaptive regression modeling\n", "abstract": " A new verification technique for /spl Delta//spl Sigma/ analog-to-digital converters (ADC) is proposed. The ADC is partitioned into functional blocks, and adaptive regression models for each partition are constructed using transistor-level simulation data. Non-idealities in circuit behavior are captured by the adaptive regression technique from the collected data. The algorithms have been implemented in a simulation program ARSIM (Adaptive Regression Simulator), which performs data sampling, model building, and simulation. Experimental results using ARSIM are shown on a second-order /spl Delta//spl Sigma/ modulator, and they demonstrate the effectiveness of our technique as a fast and accurate approach for verifying /spl Delta//spl Sigma/ converters.", "num_citations": "8\n", "authors": ["493"]}
{"title": "An efficient critical path tracing algorithm for designing high performance VLSI systems\n", "abstract": " Fast and correct timing verification is a critical issue in VLSIdesign. Several timing verification algorithms have been proposed in thelast few years. However, due to the huge computation time needed toeliminate false paths, existing algorithms have difficulty in performingtiming verification for large circuits. This paper presents efficientcritical path analysis algorithm based on test pattern generation with a newsensitization criterion. The algorithm does not require generation of a pathlist and elimination of false paths to find out the correct critical path ofthe circuit. The inputs which sensitize the critical path are determined aswell. The efficiency and speed of our algorithm are demonstrated using theISCAS benchmark circuits, and the critical paths are found in vastlyimproved times.", "num_citations": "8\n", "authors": ["493"]}
{"title": "Distributed mixed level logic and fault simulation on the Pentium/sup (R/) Pro microprocessor\n", "abstract": " Logic and fault simulation are crucial steps in the design process for verifying the correctness of a circuit and generating high quality manufacturing tests. Traditionally, Intel has been relying on dedicated hardware accelerators to meet its fault grading needs. The unprecedented size and complexity of the Pentium/sup (R/)Pro microprocessor were foreseen to severely stretch the existing compute resources at Intel. Exploiting the design hierarchy and using the processing power of distributed computers were identified to be key areas which could alleviate the simulation problem. This paper describes a distributed mixed level logic and fault simulator that has been developed using an RTL simulation engine at the core, in conjunction with a gate level logic/fault simulator. The techniques and algorithm developed have been successfully applied on the Pentium/sup (R/)Pro microprocessor.", "num_citations": "8\n", "authors": ["493"]}
{"title": "Non-robust tests for stuck-fault detection using signal waveform analysis: feasibility and advantages\n", "abstract": " In this paper we propose to use an output signal waveform analysis method called signal waveform integration for detection of stuck-at failures in combinational circuits. Non-robust tests are applied at-speed or faster to achieve high fault coverage, low test application time and detectability of redundant faults using directed random test generation techniques.", "num_citations": "8\n", "authors": ["493"]}
{"title": "Verification of circuits described in VHDL through extraction of design intent\n", "abstract": " Verification of an implementation against its specification in the design hierarchy is of paramount importance and becomes increasingly difficult with the size and complexity of the circuit. We present a comprehensive verification framework (VEHICLE) which integrates a BDD package with theorem-proving techniques, and requires minimal user interaction. VEHICLE can verify VHDL designs from the scheduled behavioral level down to the gate level by capturing the design intent, on the basis of a formal semantics, in a form appropriate for input to the verifier. Results are given for the verification of several example circuits.< >", "num_citations": "8\n", "authors": ["493"]}
{"title": "A high-level approach to test generation\n", "abstract": " A high-level test generation algorithm based upon the branch and bound search procedure is presented. The algorithm is described in detail, highlighting the various tradeoffs that are involved. A complete dependency-directed backtracking scheme that has significant advantage over chronological backtracking is introduced. Different uses for the algorithm are presented to show its versatility. Results showing significant performance improvement over gate level test generation are also presented. The ability to generate tests for incomplete designs is a major strength of this scheme.< >", "num_citations": "8\n", "authors": ["493"]}
{"title": "Generation of testable designs from behavioral descriptions using high level synthesis tools\n", "abstract": " Develops a synthesis-for-testability procedure wherein behavioral modeling techniques are used to generate testable designs. Knowledge about the accessibility of embedded modules is extracted from the behavioral design, analyzed, and any modification required subsequently incorporated in the behavioral design. Results show that when the resulting testable circuit is synthesized from this modified design using a high level synthesis tool, the overhead for testability is quite small, especially for large circuits.< >", "num_citations": "8\n", "authors": ["493"]}
{"title": "Generation and evaluation of current and logic tests for switch-level sequential circuits\n", "abstract": " This article presents an approach to developing high quality tests for switch-level circuits using both current and logic test generation algorithms. Faults that are aborted or undetectable by logic tests may be detected by current tests, or vice versa. An efficient switch level test generation algorithm for generating current and logic tests is introduced. Clear definitions for analyzing the effectiveness of the joint test generation approach are derived. Experimental results are presented for demonstrating high coverage of stuck-at, stuck-on, and stuck-open faults for switch level circuits when both current and logic tests are used.", "num_citations": "8\n", "authors": ["493"]}
{"title": "Hardware acceleration alone will not make fault grading ulsi a reality\n", "abstract": " A low cost deterministic fault simulation technique for practical ultra large scale integrated WSI) circuits, in a hardware accelerator environment is presented. A dynamic test directed partitioning scheme is implemented to select the faults which should be deterministically simulated for a given pattern. Experimental data show a speedup greater than an order of magnitude over conventionad fault simulation.", "num_citations": "8\n", "authors": ["493"]}
{"title": "Improved methods of simulating RLC coupled and uncoupled transmission lines based on the method of characteristics\n", "abstract": " Techniques are described for simulating lossy (RLC) transmission lines based on the method of characteristics. For uncoupled lossy transmission lines, a method is presented which speeds up the simulation time by a factor of two compared with existing techniques. A method is also presented for the transient analysis of coupled lossy lines in an inhomogeneous medium. Previously, simulation techniques were limited to coupled lossy lines in a homogeneous medium.< >", "num_citations": "8\n", "authors": ["493"]}
{"title": "Adaptive interpretation as a means of exploiting complex instruction sets\n", "abstract": " In this paper we concentrate on the effect of instruction set architecture on the performance potential of a computer system. These issues are key in considerations of what instruction set is most appropriate for the support of high level languages on general purpose machines. Two possible approaches are so called complex instruction sets, such as those of the VAX and IAPX 432, and the \u201creduced\u201d instruction set of the RISC I [2] microcomputer, which is expected to have performance similar to that of a VAX 11-780.", "num_citations": "8\n", "authors": ["493"]}
{"title": "Fertility rate in hydatid cysts in domestic animals.\n", "abstract": " We collect your name, email address, institutional affiliation and login credentials. We use this information to provide you with access to the My CABI service, to provide you with technical or product support, and to keep the service working securely.We may also use digital footprint connection information such as your IP address and other technical identifiers, to collect usage data, click stream data, and information about the pages you visited and searched, to analyse usage for the purpose of enhancing and improving our service.", "num_citations": "8\n", "authors": ["493"]}
{"title": "Roving diagnosis for high performance digital systems\n", "abstract": " Sauf mention contraire ci-dessus, le contenu de cette notice bibliographique peut \u00eatre utilis\u00e9 dans le cadre d\u2019une licence CC BY 4.0 Inist-CNRS/Unless otherwise stated above, the content of this bibliographic record may be used under a CC BY 4.0 licence by Inist-CNRS/A menos que se haya se\u00f1alado antes, el contenido de este registro bibliogr\u00e1fico puede ser utilizado al amparo de una licencia CC BY 4.0 Inist-CNRS", "num_citations": "8\n", "authors": ["493"]}
{"title": "A novel fractional-N PLL based on a simple reference multiplier\n", "abstract": " A wide loop bandwidth in fractional-N PLL is desirable for good jitter performance. However, a wider bandwidth reduces the effective oversampling ratio between update rate and loop bandwidth, making quantization error a much bigger noise contributor. A successful implementation of a wideband frequency synthesizer is in managing jitter and spurious performance. In this paper we present a new PLL architecture for bandwidth extension. By using clock squaring buffers with built-in offsets, multiple clock edges are extracted from a single reference cycle and utilized for phase update, thereby effectively forming a reference multiplier. This enables a higher oversampling ratio for better quantization noise shaping and makes a wideband fractional-N PLL possible.", "num_citations": "7\n", "authors": ["493"]}
{"title": "Concurrent path selection algorithm in statistical timing analysis\n", "abstract": " Circuit timing is becoming more and more uncertain under greater process variation as technology scales. Given the fault probability of each timing path and their statistical correlation from a statistical timing framework, the path selection problem for delay faults has a nature similar to the problem of designing a portfolio of stocks or assets or determining the size of bets in gambling to minimize risk. This observation allows us to develop a very different path selection approach from the conventional ones. If selection of k paths is required in a set of paths, we partition the set into two path sets and determine how many paths should be selected in each path set out of the k paths based on the probabilities of each path set containing faulty paths. We recursively continue this process, which results in the paths to be targeted during tests. The partitioning is easily performed because the paths are already grouped into the\u00a0\u2026", "num_citations": "7\n", "authors": ["493"]}
{"title": "An oscillation-based test structure for timing information extraction\n", "abstract": " Technology scaling introduces many sources of variability and uncertainty that are difficult to model and predict [3]. The result of these uncertainties is a degradation in our ability to predict the performance of fabricated chips, i.e., a lack of model-to-hardware matching. The prediction of circuit performance is the result of a complex hierarchy of models starting at the basic MOSFET device model and rising to full-chip models of important performance metrics like power, frequency of operation, etc. The assessment of the quality of such models is an important activity, but it is becoming harder and more complex with rising levels of variability, as well as with the increase in the number of systematic effects observed in modern CMOS processes. The purpose of this paper is to introduce a special-purpose test structure that specifically focuses on ensuring the accuracy of gate timing models. The certification of digital design\u00a0\u2026", "num_citations": "7\n", "authors": ["493"]}
{"title": "At-speed test of high-speed DUT using built-off test interface\n", "abstract": " This paper presents an efficient test framework to extend a use of low-cost ATE (Automatic Test Equipment) to at-speed test of high-speed DUT (Device Under Test). To bridge the speed gap between the ATE and the DUT, an off-chip test interface circuit, called Built-off Test Interface (BOTI), has been developed. Unlike the previous methods which use on-chip or off-chip self-test circuits, in our method, the ATE plays main role in testing high-speed DUTs by actively controlling the BOTI operation, and monitoring the overall test procedure. This makes the presented method flexible to be applied to various test applications without compromising the test coverage. Also, since the BOTI is implemented off-chip, it does not require hardware modifications of the ATE or the DUT except the DUT load board to accommodate the BOTI module. To maintain reliable off-chip signal communication between the BOTI and the DUT, the\u00a0\u2026", "num_citations": "7\n", "authors": ["493"]}
{"title": "Dedicated Rewriting: Automatic Verification of Low Power Transformations in Register Transfer Level\n", "abstract": " We present dedicated rewriting, a novel technique to automatically prove the correctness of low power transformations in hardware systems described at the Register Transfer Level (RTL). We guarantee the correctness of any low power transformation by providing a functional equivalence proof of the hardware design before and after the transformation. We characterize low power transformations as rules, within our system. Dedicated rewriting is a highly automated deductive verification technique specially honed for proving correctness of low power transformations. We provide a notion of equivalence and establish the equivalence proof within our dedicated rewriting system. We demonstrate our technique on a non-trivial case study. We show equivalence of a Verilog RTL implementation of a Viterbi decoder, a component of the DRM SoC, before and after the application of multiple low power transformations. We\u00a0\u2026", "num_citations": "7\n", "authors": ["493"]}
{"title": "Characterization of sequential cells for constraint sensitivities\n", "abstract": " For timing analysis, each flip-flop and latch in a standard library is characterized for two constraints: setup time and hold time constraints. These constraints need to be characterized for their sensitivities to the variation parameters in order to perform statistical timing analysis. Several approaches have been proposed to perform statistical characterization of delays. However, the predominant computation time requirements during statistical characterization are for constraint sensitivity characterization. In this paper we propose a new delay-based approach for statistical characterization of constraint sensitivities. We show that the sensitivities obtained using such an approach on an average result in 150times runtime improvement comparing with traditional approaches, without significant loss of accuracy.", "num_citations": "7\n", "authors": ["493"]}
{"title": "Improving bandwidth while managing phase noise and spurs in fractional-N PLL\n", "abstract": " The loop bandwidth of fractional-N PLL is a desirable parameter for many wireless communication applications. To improve bandwidth design tradeoffs must be made among different circuit blocks. The key to successful implementation of a wideband fractional-N synthesizer is in managing jitter and spurious performance. In this paper we compare several techniques for bandwidth enhancement including an improved version of one recently proposed by the authors. Circuits that suppress fractional spurs along the signal path are discussed. Simulations results from Matlab/Simulink are also presented.", "num_citations": "7\n", "authors": ["493"]}
{"title": "Case study of atpg-based bounded model checking: Verifying usb2. 0 ip core\n", "abstract": " This paper presents the ATPG performances of verifying USB2.0 IP core. Using the USB protocol and typical properties, the ATPG-based bounded model checking mechanism is revealed. Heuristics to accelerate the ATPG search are presented and their impacts are analyzed. We feel that results from this case study are applicable to serial communication circuits of the same family and can be scaled to industrial-sized circuits.", "num_citations": "7\n", "authors": ["493"]}
{"title": "A formal framework for verification of embedded custom memories of the Motorola MPC7450 microprocessor\n", "abstract": " In this presentation, we will deal with verification of custom designed embedded memories. Using our paradigm, one can abstract the behavior of a memory block by a couple of artifacts\u2014one representing its contents, and another representing its interface. We make use of the well known behavioral model known as the Efficient Memory Model (EMM) [29, 30] to represent contents of memories. We provide a methodology using which the behavior of a switch (or equivalently, transistor) level device can be specified using parameterized regular expressions. These entities can be used to abstractly describe the behavior of a bunch of switches that represent the interface of a memory. An automaton that we construct out of an abstract memory interface definition represents an abstraction of the memory interface itself. We show that such an automaton also forms a transducer that is a simulation model in a symbolic\u00a0\u2026", "num_citations": "7\n", "authors": ["493"]}
{"title": "Effects of multi-cycle sensitization on delay tests\n", "abstract": " Existing delay test generation techniques focus on test generation for combinational blocks, and assume the inputs and outputs of the block to be unconstrained. Test application for delay tests is done by means of enhanced scan, scan shifting or functional justification; all these techniques impose minimal constraints on the inputs and the outputs of the combinational block targeted. This leads to over-testing the components for delay defects. This paper analyzes the gains associated with determining the multi-cycle (sequential) sensitization of delay tests. The advantages of determining multi-cycle sensitization is then illustrated on benchmark designs with and without a delay-specific fault model.", "num_citations": "7\n", "authors": ["493"]}
{"title": "To model check or not to model check\n", "abstract": " In the past, hardware design validation has relied primarily on simulation. New techniques such as model checking have been introduced but no objective study investigating the advantages such techniques provide over simulation has been made. Simulation is model checking over a trace elicited by executing a test vector; model checking can be viewed as exhaustive simulation. Each has its own set of advantages and limitations. A platform, \"Sherlock\", was available wherein one could use properties or specifications expressed as CTL-like formulae interchangeably for checking simulation runs or for model checking. In this paper we describe and present results from an experimental study undertaken on a real implementation to better understand the efficacies of the two methods. We also present improved methods for accommodating liveness, fairness (of arbitration) and existence conditions in simulation and\u00a0\u2026", "num_citations": "7\n", "authors": ["493"]}
{"title": "VQ-based image coding and vector filter bank\n", "abstract": " It is well known that vector quantisation (VQ) is a good technique for image coding. For VQ-based image coding, vector-based signal processing techniques should be used to better match with VQ. The paper reports some preliminary results of extending the concept of the filter bank from a scalar-based operation to a vector-based operation and the performance of a vector filter bank for image coding.< >", "num_citations": "7\n", "authors": ["493"]}
{"title": "RAFT: A novel program for rapid-fire test and diagnosis of digital logic for marginal delays and delay faults\n", "abstract": " The problem of testing manufactured digital parts has i become very difficult due to high levels of integration and the ncrease in complexity of circuit designs. In many instances, p entirely novel and unorthodox methods have been used to simlify the testing problem and to expose failure modes that are a not easily detectable by conventional testing techniques. Such n example is current testing. With evolving technology, the p", "num_citations": "7\n", "authors": ["493"]}
{"title": "An object-oriented approach for implementing algorithm-based fault tolerance\n", "abstract": " The authors demonstrate the practical use of an object-oriented system to incorporate fault tolerance and reliability into data objects. The object-based fault tolerance scheme uses abstraction to conceal algorithm-based fault tolerance layers. The scheme allows a layer of fault tolerance to be added to data objects without affecting or altering the use of the data objects. It is shown that the C++ class mechanisms of overloading and derivation permit the added fault tolerance to be transparent to the original data objects. To demonstrate the feasibility of this approach, using C++, a libray of matrix functions is presented and a layer of fault tolerance around matrix data objects is added. The weighted checksum code technique was implemented to create fault-tolerant matrix data objects. This allows programmers to add algorithm-based fault tolerance onto existing matrix applications without requiring modification to the\u00a0\u2026", "num_citations": "7\n", "authors": ["493"]}
{"title": "Distributed VLSI simulation on a network of workstations\n", "abstract": " Switch level simulation has been mapped to a distributed platform using a network of workstations. Model parallelism is used with preprocessing to partition the circuit to be simulated among the processors. A high-level pipelining scheme with multiple buffers is proposed to overcome the effects of a low-bandwidth network. Speedups of up to 4.1 with five processors have been obtained for medium sized ISCAS benchmark circuits. The speedups achieved using distributed simulation are very close to those obtained with the same switch-level simulator implemented on a shared-memory parallel machine.<>", "num_citations": "7\n", "authors": ["493"]}
{"title": "Design and evaluation of executable assertions for concurrent error detection\n", "abstract": " System level concurrent detection of errors due to hardware faults can be accomplished by introducing software redundancy in the form of executable assertions into a program to monitor the correct operation of the system during its execution. This paper attempts to formalize the use of executable assertions for the purpose of concurrent error detection, and discusses a transformation approach to the design of assertion statements. The second part deals with the effectiveness of the error detection technique. A fault simulation method for the evaluation of the reliability of the assertion statements is described. This scheme was used to study the effectiveness of a nontrivial program in providing protection against faults in the underlying system. The experimental results shows that the use of executable assertions provides an effective approach to achieve concurrent error detection.", "num_citations": "7\n", "authors": ["493"]}
{"title": "TIDBITS: Speedup via time-delay bit-slicing in ALU design for VLSI technology\n", "abstract": " A novel word-wide ALU organtTation based on bitlevel pipelln~ ng is proposed. This ALU pipeline is shown to be area e~ cient and easy to control. The performance of this ALU organization is analyzed in the context of two applications: integer summation/multiplication and multidimensional array address stream generation. Near optimal speedup with respect to the added area is shown to be achieved for these and other applications where integer additions predominate. mutually dependent. Moreover, the simplicity and regularity of the pipeline control logic allows very compact MOS implementation. Thus the implied constant in the O (n) additional area is quite small.This paper is organized as follows: Section two motivates the problem with a critique of conventional plpelined ALU design, followed by a description of the proposed TIDBITS ALU organization. In section three the performance of the TIDBITS ALU is\u00a0\u2026", "num_citations": "7\n", "authors": ["493"]}
{"title": "A Proactive Voltage-Droop-Mitigation System in a 7nm Hexagon\u2122 Processor\n", "abstract": " A proactive clock-gating system (PCGS) in a 7nm Qualcomm \u00af  Hexagon\u2122 digital signal processor (DSP) predicts supply voltage (V DD ) droops based on microarchitectural events and a power-delivery-network (PDN) model and adapts clock frequency (F CLK ) to reduce the V DD  droop. Silicon measurements demonstrate 10% higher F CLK  or 5% lower V DD .", "num_citations": "6\n", "authors": ["493"]}
{"title": "Methods for making zoned apertured webs\n", "abstract": " A method for making a zoned apertured nonwoven web comprises providing an unapertured nonwoven web having a first plurality of weakened locations in a first pattern in a first zone and a second plurality of weakened locations in a second pattern in a second zone. The method comprises applying a substantially cross machine directional tensioning force to the nonwoven web to cause the nonwoven web to rupture at some of the first and second pluralities of weakened locations. The applying step creates a plurality of first apertures in the nonwoven web coincident with the some of the first plurality of weakened locations and a plurality of second apertures in the nonwoven web coincident with the some of the second plurality of weakened locations. The cross machine directional width of the nonwoven web after the applying step is substantially the same in the first zone and the second zone.", "num_citations": "6\n", "authors": ["493"]}
{"title": "Esift: Efficient system for error injection\n", "abstract": " Computer use in high dependability applications is rapidly increasing. These applications require the computer to be able to detect, locate, isolate and recover from software, hardware or security attacks errors. To evaluate the dependability of a computer system design, it is critical to be able to assess its ability to detect, locate, recover from errors, and to estimate coverage and latencies. Fault-injection tools play critical role in the evaluation and validation of dependable systems. They generate statistics on error coverage and latencies. This helps to identify good fault tolerance technique that detects and prevent system failures. In this paper, we present an Efficient Fault Injection System for Transient Fault (ESIFT). ESIFT is based on Python extended GDB which makes ESIFT portable across a wide variety of systems. ESFIT operates at near native speed enabling the dependability evaluation of large system. Unlike\u00a0\u2026", "num_citations": "6\n", "authors": ["493"]}
{"title": "Power prediction of embedded scalar and vector processor: Challenges and solutions\n", "abstract": " We present our methodology in applying a well established statistical dynamic power prediction technique in a production environment to an embedded commercial `scalar and vector processor'. The pitfalls faced and solutions to guide the statistical solver to build a low error power prediction model are discussed. In our proposed method, we extracted stall probe-points in a processor, used selective microarchitectural events, created instruction groups and selected sub-set of performance events to refine the power-model. Our approach to determine the processor's dynamic power floor and power-sampling window size in an architectural trace and the selection of tests for training are explained. Our flow results in power weights for a set of architecturally visible events as well as few optional microarchitectural events of the processor. Using these weights, a canonical power prediction equation (that is configurable\u00a0\u2026", "num_citations": "6\n", "authors": ["493"]}
{"title": "Single trojan injection model generation and detection\n", "abstract": " Driven by cheaper fabrication facilities around the world, IC design houses are increasingly outsourcing the fabrication of their ICs. This poses the risk of intellectual property loss and to the possibility of design modifications and insertion of Trojans for sinister purposes. We present a Trojan modeling and test generation techniques for small and large Trojans. The quality of the generated test set is evaluated in term of its ability to detect modeled and un-modeled Trojans. We show experimentally that the derived tests detect all injected detectable Trojans.", "num_citations": "6\n", "authors": ["493"]}
{"title": "A novel low power 11-bit hybrid ADC using flash and delay line architectures\n", "abstract": " This paper presents a novel low power 11-bit hybrid ADC using flash and delay line architectures, where a 4-bit flash ADC is followed by a 7-bit delay-line ADC. This hybrid ADC inherits accuracy and power efficiency from flash ADCs and delay-line ADCs, respectively. Also, in order to reduce the power of the first stage flash ADC, a power-saving technique is adopted by biasing the DC tail current of the preamplifiers at 5\u03bcA instead of the operational current, 47\u03bcA in stand-by mode. The hybrid ADC was designed and simulated in a commercial 65nm process. With 1.1 V supply and 100 MS/s, the ADC achieves an SNDR of 60 dB and consumes 1.6 mW, which results in a figure of merit (FOM) of 19.4 fJ/conversion-step without any calibration technique. Also, Monte Carlo simulations are performed with a 3\u03c3 device mismatch for the SNDR estimation, and the SNDR is observed to be better than 58.5 dB.", "num_citations": "6\n", "authors": ["493"]}
{"title": "Built-in Self Test of RF Subsystems with Integrated Detectors\n", "abstract": " This paper describes a built-in self test technique for RF subsystems, using low-overhead on-chip detectors to calculate circuit specifications. A novel on-chip amplitude detector has been designed and optimized for RF circuit specification test. The detector has small area overhead with a low-frequency output. A test chip was fabricated in a commercial 0.18 \u03bcm CMOS process. By using on-chip detectors in a loopback setup, both the system performance and specifications of the individual components can be accurately measured. Measurements show accurate prediction of system and component specifications.", "num_citations": "6\n", "authors": ["493"]}
{"title": "Refactoring of timing graphs and its use in capturing topological correlation in SSTA\n", "abstract": " Reconvergent paths in circuits have been a nuisance in various computer-aided design (CAD) algorithms, but no elegant solution to deal with them has been found yet. In statistical static timing analysis (SSTA), they cause difficulty in capturing topological correlation. This paper presents a technique that in arbitrary block-based SSTA reduces the error caused by ignoring topological correlation. We interpret a timing graph as an algebraic expression made up of addition and maximum operators. We define the division operation on the expression and propose algorithms that modify factors in the expression without expansion. As a result, the algorithms produce an expression to derive the latest arrival time with better accuracy in SSTA. Existing techniques handling reconvergent fanouts usually use dependency lists, requiring quadratic space complexity. Instead, the proposed technique has linear space complexity by\u00a0\u2026", "num_citations": "6\n", "authors": ["493"]}
{"title": "Pseudorandom test of nonlinear analog and mixed-signal circuits based on a volterra series model\n", "abstract": " This paper presents new test methods for nonlinear Analog and Mixed-Signal (AMS) circuits which use a pseudorandom signal to test multiple Devices Under Test (DUTs) accurately. The goal of the studies presented in this paper is to understand the behaviors of nonlinear AMS circuits in a low-cost test environment and to develop the algorithm to extract the performance information of the DUTs using simple test measurements. The extracted information is then used to estimate the various specifications of DUTs. In order to achieve this goal, we analyze the behaviors of AMS circuits using a Volterra series model, and investigate the stochastic properties of the pseudorandom signals to develop the efficient performance characterization algorithms. The mathematical theory and experimental results are presented to validate the presented test methods.", "num_citations": "6\n", "authors": ["493"]}
{"title": "Path criticality computation in parameterized statistical timing analysis\n", "abstract": " This paper presents a method to compute criticality probabilities of paths in parameterized statistical static timing analysis (SSTA). We partition the set of all the paths into several groups and formulate the path criticality into a joint probability of inequalities. Before evaluating the joint probability directly, we simplify the inequalities through algebraic elimination, handling topological correlation. Our proposed method uses conditional probabilities to obtain the joint probability, and statistics of random variables representing process parameters are changed due to given conditions. To calculate the conditional statistics of the random variables, we derive analytic formulas by extending Clark's work. This allows us to obtain the conditional probability density function of a path delay, given the path is critical, as well as to compute criticality probabilities of paths. Our experimental results show that the proposed method\u00a0\u2026", "num_citations": "6\n", "authors": ["493"]}
{"title": "An improved SOM-based visualization technique for DNA microarray data analysis\n", "abstract": " Effective and meaningful visualization techniques are quite important for multidimensional DNA microarray gene expression data analysis. Elucidating the cluster properties of these multidimensional data are often complex. Patterns, hypotheses on the relationships, and ultimately of the function of the gene can be analyzed and visualized by non-linear reduction of the multidimensional data to a lower dimension. In this paper, an improved SOM visualization technique named Improved Side Intensity Modulated (ISIM) Self-Organizing Map (SOM) has been proposed and compared with other SOM based visualization techniques. On different datasets, ISIM-SOM is found to offer better cluster boundary, simplicity and clarity.", "num_citations": "6\n", "authors": ["493"]}
{"title": "A random jitter RMS estimation technique for BIST applications\n", "abstract": " This paper describes a RMS value measurement technique for random jitter. A jittery clock signal is combined with a reference clock signal using an OR operation and an AND operation in sequence, and the pulse width outputs modulated by the amount of the random jitter are used to charge or discharge a capacitor. The voltage at the capacitor, in turn, modulates the frequency of VCO having a current-starved inverter, and whose frequency difference from the OR operation and the AND operation is used in calculating the RMS value of the random jitter. Circuit-level simulations show the validity of the proposed technique for up to 20% peak-to-peak jitter in the clock even with process variations. The proposed technique can be applied to BIST solutions for random jitter measurement on a transmitted clock signal.", "num_citations": "6\n", "authors": ["493"]}
{"title": "Hybrid BiST solution for analog to digital converters with low-cost automatic test equipment compatibility\n", "abstract": " The cost of testing mixed signal circuitry with conventional analog-stimulus is significantly higher than digital circuitry due to higher cost automatic test equipment (ATE) required for generation of analog stimulus. Multiple variants of low cost testers have been developed for digital testing which rely on relaxed timing, power or tester channel requirements to lower hardware cost. Systems containing mixed-signal/RF components can thus not be tested on such ATE due to the cost and limitations of analog/RF stimulus and measurement modules. This paper proposes a hybrid BIST scheme for analog to digital converters (ADCs) to enable full production-quality testing with low cost ATE. The two major challenges addressed are generating the input stimulus, and a fully functional at-speed test to maintain the test quality of a pure analog ATE solution.", "num_citations": "6\n", "authors": ["493"]}
{"title": "Functionally valid gate-level peak power estimation for processors\n", "abstract": " Traditionally, peak power consumption has been estimated at the module-level and there has been no attempt to check the functional validity of the gate-level estimate through instruction execution. This leads to the over design of the processor components that deliver current to the modules. In this work, we present a methodology to estimate the peak dynamic power at the module-level which is functionally valid at the processor-level and thus, avoid the over design. We tackle the problem of module-level peak power estimation by building our algorithm using the reactive tabu search technique. We use a bounded model checker for verifying the instruction validity of the module-level peak power estimates at the processor level. Our algorithm intelligently combines the module-level power estimates with these instruction validity checks and efficiently derives functionally valid peak power estimates. In addition, we\u00a0\u2026", "num_citations": "6\n", "authors": ["493"]}
{"title": "Efficient microprocessor verification using antecedent conditioned slicing\n", "abstract": " The authors present a technique for automatic verification of pipelined microprocessors using model checking. Antecedent conditioned slicing is an efficient abstraction technique for hardware designs at the register transfer level (RTL). Antecedent conditioned slicing prunes the verification state space, using information from the antecedent of a given LTL property. In this work, the authors model instructions of a pipelined processor as LTL properties, such that the instruction opcode forms the antecedent. The antecedent conditioned slicing to decompose the problem space of pipelined processor verification on an instruction-wise basis was used. We pass the resulting smaller, tractable problems through a lower level verification engine. We thereby verify that every instruction behaves according to the specification and ensure that non-target registers are not modified by the instruction. The SMV model checker to\u00a0\u2026", "num_citations": "6\n", "authors": ["493"]}
{"title": "HDL Program Slicing to Reduce Bounded Model Checking Search Overhead\n", "abstract": " The size of the hardware description model for a complex modern digital system is increasing rapidly. CAD tools used to analyze these models are challenged by this increase in model complexity. This paper presents a technique that extracts for a given set of variables, a smaller HDL executable design slice that includes all the behavioral elements that affect those variables directly or indirectly. The design slice when compiled produces a behavior for the set of variables equivalent to the one computed by the original unsliced design. ATPG and verification tools analyzing this design could use the sliced model to reduce computation overhead. This technique was implemented in a computer program and evaluated its impact on the bounded model checker, SMV. Results show a reduction for both CPU time and memory needed by SMV to verify a publicly available model of the USB 2.0 IP core", "num_citations": "6\n", "authors": ["493"]}
{"title": "Testing and debugging delay faults in dynamic circuits\n", "abstract": " We propose novel design for test and debug techniques to apply two patterns for delay fault test and debug in dynamic circuits. Dynamic circuits, which have traditionally been difficult to test, pose new challenges for AC tests due to the presence of a reset phase between applications of any two patterns, which impedes delay fault testing of such circuits. We present two sets of design for test and debug techniques. The first set facilitates application of two patterns to dynamic circuits in general, overcoming the issue of reset phase, and reduces the problem of test generation for dynamic circuits to test generation for pull down paths of static CMOS circuits. The second set enables application of two patterns to scan based dynamic circuits. The proposed techniques reduce the problem of delay test generation for scan based dynamic circuits to that of delay test generation for static CMOS circuits with complete accessibility\u00a0\u2026", "num_citations": "6\n", "authors": ["493"]}
{"title": "Test and debug in deep-submicron technologies\n", "abstract": " With the scaling of feature sizes into Deep-Submicron (DSM) values, the level of integration and performance achievable in VLSI chips increases. A lot of work has been directed to tackle design related issues arising out of scaling, like leakage mitigation etc. However efforts to enhance testability of such designs have not been sufficient. It is not viable to overlook testability issues arising out of these designs because the defect sizes do not scale proportional to the feature sizes. Previously effective fault models like stuck-at appear archaic and are unable to model faults accurately. This necessitates the need for more detailed models which can more explicitly model the behavior of faulty DSM chips. Also there is a significant increase in delay faults in logical paths of integrated circuits. Delay faults cause the delay of paths in a chip to be larger than expected resulting in the output of a chip to be deviant from the expected behavior, in spite of the chip being functionally correct. Efficient techniques are needed for detecting such defects in first silicon and eliminating them before the final versions of the chips are shipped. This requires efficient debug techniques for performance characterization of large complex integrated circuits in deep-submicron and nanometer technologies. In this paper we present an insight into test challenges arising out of deep submicron technologies and effective approaches to tackle the same.", "num_citations": "6\n", "authors": ["493"]}
{"title": "Model checking of security protocols with pre-configuration\n", "abstract": " Security protocols are very vulnerable to design errors. Thus many techniques have been proposed for validating the correctness of security protocols. Among these, general model checking is one of the preferred methods. Using tools such as Mur\u03d5, model checking can be performed automatically. Thus protocol designers can use it even if they are not proficient in formal techniques. Although this is an attractive approach, state space explosion prohibits model checkers from validating secure protocols with a significant number of communicating participants. In this paper, we propose \u201cmodel checking with pre-configuration\u201d which is a \u201cdivide-and-conquer\u201d approach that reduces the amount of memory needed for verification. The verification time is also reduced since the method permits the use of symmetry more effectively in model checking. The performance of the method is shown by checking the\u00a0\u2026", "num_citations": "6\n", "authors": ["493"]}
{"title": "A novel solution for chip-level functional timing verification\n", "abstract": " Existing timing verification tools can provide methodologies for identifying and optimizing critical true paths in a embedded combinational module; however the problem of justifying these paths to the chip level is a very difficult one. This paper addresses the problem of timing verification at the entire chip level. We use a critical path tool, CRITIC, to obtain critical paths in an embedded combinational module. In order to reduce the complexity of checking whether the module-level critical path is indeed critical at the chip level, we use techniques from formal verification to extract the control behavior of the circuit, and check whether there is any control sequence which will justify the path to the chip level. The results of the experiments on several processor designs show that our approach is very effective in large sequential circuits such as microprocessors, where conventional ATPG techniques require inordinate amounts\u00a0\u2026", "num_citations": "6\n", "authors": ["493"]}
{"title": "VERTEX: VERification of Transistor-level circuits based on model EXtraction\n", "abstract": " VERTEX, a program that performs formal verification of synchronous sequential circuits that are characterized at the transistor-level is described. Additionally, VERTEX can compare gate-level designs or Boolean specifications against their switch-level implementations. VERTEX verifies a hardware design by employing novel techniques to extract the relevant state variables of a switch-level circuit and to compare the finite state machine descriptions of hardware designs based on formal methods for the verification of sequential circuits.< >", "num_citations": "6\n", "authors": ["493"]}
{"title": "A framework for distributed VLSI simulation on a network of workstations\n", "abstract": " A distributed framework for logic simulation is presented. Switch-level simulation has been mapped to a distributed platform using a network of workstations on an Ethernet bus. Model parallelism is used with preprocessing to partition the circuit to be simulated among the processors. The simulation algorithm is decoupled from the communication layers to ensure easy portability. We have proposed a high level pipelining scheme with multiple buffers to overcome the effects of a low bandwidth network. Speedups of up to 4.1 with 5 processors have been obtained for medium sized ISCAS benchmark circuits. The speedups achieved using distributed simulation are very close to that obtained by the same switch-level simulator imple mented on a shared memory parallel machine. Novel techniques to improve the performance of distributed simulation have also been implemented on a shared memory parallel machine.", "num_citations": "6\n", "authors": ["493"]}
{"title": "Design of a scalable parallel switch-level simulator for VLSI.\n", "abstract": " The problem of mapping a computation-intensive task of irregular structure onto a parallel framework is examined. The application considered is the switch-level logic simulation of digital circuits, a technique that is in wide use for the verification of VLSI designs. The authors focus on medium-grain multiprocessors and only consider model parallel computation, where the model of the design to be simulated is partitioned among processors. They address the issues of portability and scalability and look at specific features of the application that can be exploited. Different ways of mapping the simulation problem onto a parallel framework are presented. A prototype implementation of the algorithms is described.", "num_citations": "6\n", "authors": ["493"]}
{"title": "Fault-Tolerant Algorithms and Architectures for Real Time Signal Processing.\n", "abstract": " An encoding technique, the weighted checksum code (WCC), is proposed to achieve concurrent error detection in matrix arithmetic and signal processing on highly concurrent VLSI structures. In order not to increase the roundoff errors when we incorporate the WCC into the computation, a simple roundoff error analysis is used to guide the construction of the WCC. A new data retry technique is then proposed to locate the faulty processors and identify the correct outputs. Such an approach provides rapid error detection with low hardware overhead while system performance is not significantly degraded for the sake of fault tolerance.For simplicity of treatment, this discussion will be based on linear array architectures which are believed to hold the most promise in VLSI computing structures for their flexibility, low cost, and applicability to most of the interesting algorithms. A similar discussion clearly holds for two-dimensional array architectures as well.", "num_citations": "6\n", "authors": ["493"]}
{"title": "Companson and Diagnosis of Large Replicated Files\n", "abstract": " This paper examines the problem of comparing large replicated files in a context in which communication dominates the cost of comparison. A low-cost checking matrix is proposed for comparison of these replicated files. The checking matrix is composed of check symbols generated by a divide-and-conquer encoding algorithm. The matrix allows for detection and diagnosis of disagreeing pages with very little communication overhead. In contrast to a previous O(N) proposal, the storage requirement for the checking matrix is O(log N), where N is the number of pages in the file. The matrix can be stored in main memory without the need for extra accesses to disk during normal updates of pages.", "num_citations": "6\n", "authors": ["493"]}
{"title": "A Probabilistic Model of Algorithm-Based Fault Tolerance in Array Processors for Real-Time Systems.\n", "abstract": " This paper presents a probabilistic analysis of the recently proposed concept of algorithm-based fault-tolerance which deals with system-level methods of obtaining reliable results from computations performed on array processors for real-time systems. Expressions for the reliability of the results of a computation, and the time for completion of the computation, using a particular algorithm are derived in terms of various parameters: the number of processors involved, the time for execution, and the fault-detecting.-locating, and-tolerating capabilities of the algorithm. We model the system with concurrent fault detection followed by recovery and reconfiguration by using a semi-Markov Process model. The model can handle any failure distribution of the processors. and any reconfiguration and retry time distribution. The only restriction is that it is assumed that the time for executing a single instance of a problem is fixed. This measure can aid in comparing various alternate algorithms to solve a given problem if the selection is made on the basis of the reliability of the produced results. A second measure of effectiveness of a fault tolerance scheme is related to the number of problems that can be solved on a system with a bound on the number of processors that can be used and a bound on the reliability of the result.The scheme of algorithm-based fault detection/tolerance has recently been proposed to concurrently detect and/or locate faults in the system while the system is executing an algorithm for a problem (1, 2, 3, 4, 5). A probabilistic analysis of the scheme of algorithm-based concurrent fault detection and tolerance for array processor systems is\u00a0\u2026", "num_citations": "6\n", "authors": ["493"]}
{"title": "TOTALLY SELF-CHECKING CMOS CIRCUITS USING A HYBRID REALIZATION.\n", "abstract": " Totally self-checking (TSC) circuits are a class of circuits which are used to detect errors concurrently with normal operation. They map encoded inputs to encoded outputs, which are monitored by checkers. In the presence of a fault in a TSC circuit at least one input codeword produces a noncode at the output (self-testing property), which is detected by the checker. The self-testing property is difficult to satisfy for TSC CMOS circuits, especially in the presence of timing skews in the input variables changes and unequal delays in the circuit. A new method for implementing TSC circuits in CMOS technology is presented. The method is guaranteed to be self-testing even under a dynamic behavior assumption (ie when arbitrary delays and timing skews are assumed).", "num_citations": "6\n", "authors": ["493"]}
{"title": "An MOS fault simulator with timing information\n", "abstract": " This paper describes an MOS fault simulator which produces output waveforms for circuits under realistic physical failures. The basic primitives of the fault model for this simulator are node-short faults and line-open faults. This simulator uses a table lookup of the transistor IV characteristics as well as a newly proposed static concurrent fault simu-lation technique to simulate both the fault-free circuit and many faulty circuits in one pass. Both single and multiple faults can be inserted into each faulty circuit. The simulator can either plot the voltage waveforms, or extract both the logic and delay values for both the fault-free circuit and the faulty circuits from the waveforms. This is intended as a powerful tool for characterizing the behavior of circuits under realistic physical failures.", "num_citations": "6\n", "authors": ["493"]}
{"title": "Cross-layer resilience in low-voltage digital systems: key insights\n", "abstract": " CLEAR (Cross-Layer Exploration for Architecting Resilience) is a first of its kind framework which overcomes a major challenge in the design of digital systems that are resilient to hardware errors: achieve desired resilience targets at low cost (energy, power, execution time, area) by combining resilience techniques across various layers of the system stack (circuit, logic, architecture, software, algorithm). CLEAR automatically and systematically explores the large space of resilience techniques and their combinations, derives cost-effective solutions, provides guidelines for designing new techniques, and offers insights into how to design cost-effective digital systems resilient to hardware errors: 1. circuit-level techniques are crucial; 2. application-level guidance is essential; 3. existing architecture and software techniques are generally expensive or provide too little resilience; 4. some previously published techniques\u00a0\u2026", "num_citations": "5\n", "authors": ["493"]}
{"title": "Maternal determinants and fetal outcome of twin pregnancy: a five-year survey\n", "abstract": " Background: Study prevalence of twin pregnancy, maternal risk factors and fetal outcome in twin pregnancy. Methods: A retrospective study of mothers with twin pregnancies who delivered during the period of 5 years. There were 109 mothers who gave birth to 218 babies. Maternal details, antenatal complications and fetal outcomes were analysed.Results: There were 5432 deliveries which included 109 twin births. Prevalence of twinning was 20/1000 deliveries. The mean age was 28.11 ([+ or-] SD 4.89) with 69.7% in the younger age groups. No association with parity, BMI and ovulation induction was found. Most common complication was preterm delivery (64.2%) with mean gestational age being 35.07 ([+ or-] SD 2.32). Others were diabetes (25.7%), hypertension (22.9%), hypothyroidism (14.6%) and postpartum hemorrhage (13.7%). Cesarean section was the commonest mode of delivery (78.0%) with fetal\u00a0\u2026", "num_citations": "5\n", "authors": ["493"]}
{"title": "Application of under-approximation techniques to functional test generation targeting hard to detect stuck-at faults\n", "abstract": " Running at-speed functional tests has shown to be a very effective method to detect faulty chips. In our previous paper we presented a methodology for generating functional tests aimed at hard to detect gate level faults in the control logic of a processor. In that methodology gate level tests were mapped to the register transfer level (RTL) and a faulty RTL model was built. The propagation constraints of the fault through the design were captured as linear temporal logic (LTL) properties. These constraints reduced the search space. Further, the constraints also allowed us to do structural reductions like cone of influence reduction and removal of irrelevant duplicated signals. Overall the constraints provided improved scaling. Not all the design behaviours are required to generate a test for a fault. In this paper we use this insight to scale our previous methodology further. Under-approximations are design abstractions that\u00a0\u2026", "num_citations": "5\n", "authors": ["493"]}
{"title": "Transformer-coupled loopback test for differential mixed-signal dynamic specifications\n", "abstract": " Loopback tests for a differential mixed-signal device under test (DUT) have rarely been attempted since any imbalance introduced by design-for-test (DfT) circuitry on differential signaling delivers an imperfect sinusoidal wave to the DUT input or output, thereby degrading the DUT performance. In addition, this methodology inherently suffers from fault masking. These problems trigger low test accuracy and serious yield loss. This paper presents a novel methodology for the efficient prediction of individual DUT dynamic performance parameters with a radio-frequency (RF) transformer in loopback mode to overcome the imbalance of DfT circuitry and the fault masking. Cascaded RF transformers with different multiplicative weights are selected in three combinations by a multiplexer to create three separate loopback responses. These responses are used to characterize the DUT dynamic performance. Hardware\u00a0\u2026", "num_citations": "5\n", "authors": ["493"]}
{"title": "Real-time dynamic hybrid BiST solution for Very-Low-Cost ATE production testing of A/D converters with controlled DPPM\n", "abstract": " The ideal goal of semiconductor quality assurance is to provide zero defective parts to the customer. In practice this goal is limited by test quality and test cost due to expensive ATE resources. It is typically not feasible to provide zero Defective Parts per Million (DPPM) for majority of applications due to the high costs of testing involved. Comprehensive functional tests to find all detectable faults typically have large test times resulting in a prohibitive cost. Work has been done in the field of digital testing with patterns where statistical tools are used in order to optimize the test cost and DPPM by real-time analysis. Our goal is to propose an Analog to Digital Converter (ADC) Built in Self Test (BiST) scheme which has compatibility with similar dynamic optimization measures. Multiple variants of Very Low Cost (VLC)- ATE have been developed for digital testing which rely on relaxed timing, power or tester channel\u00a0\u2026", "num_citations": "5\n", "authors": ["493"]}
{"title": "A novel characterization technique for high speed I/O mixed signal circuit components using random jitter injection\n", "abstract": " Timing problems in high-speed serial communications are mitigated with phase-interpolator (PI) circuitry. Linearity testing of PI has been challenging, even though PI is widely used in modern high speed I/O architectures. Previous research has focused on implementing additional built-in circuits to measure PI linearity. In this paper, we present a cost effective PI linearity measurement technique which requires no significant modification of existing I/O circuits. Our method uses jitter distributions obtained from random jitter injected into the data channel. Two distributions are separately obtained using undersampling and sampling using PI. The proposed algorithm calculates the differential nonlinearity (DNL) from the difference of these distributions. Simulation results show that the average prediction RMS error for the DNL calculation is 0.31 LSB.", "num_citations": "5\n", "authors": ["493"]}
{"title": "Vector based Analog to Digital Converter sequential testing methodology to minimize ATE memory and analysis requirements\n", "abstract": " Mixed signal circuits typically require more complex specification based testing as compared to digital circuits, which can be completely tested with structural or simple functional tests. Due to the analog nature of some of the internal nodes and external signals in mixed signal circuits, qualitative functional tests may be required to assure circuit performance at all operating points. Mixed signal blocks such as Analog to Digital Converters (ADC) and Digital to Analog Converters (DAC) act as interfaces between the digital processing modules of the System on a Chip (SoC) and interfacing analog domains. These converters are increasingly common on SoCs due to ever-increasing presence of real world analog signals that use the processing capabilities of the digital blocks. High volume production testing of these mixed-signal components is inefficient due to test complications, resulting in the use of high-performance\u00a0\u2026", "num_citations": "5\n", "authors": ["493"]}
{"title": "Estimating path delay distribution considering coupling noise\n", "abstract": " Accurately estimating critical path delays is extremely important for yield optimization and for path selection in delay testing. It is well known that dynamic effects such ascoupling noise can significantly affect critical path delays. In traditional static timing analysis, the coupling effect isincorporated by estimating the switching window overlaps between aggressor and victim and then assuming a constant (worst case) coupling factor if any overlap is present. However in path based statistical timing analysis, using a constant coupling factor can overestimate the mean delay while under estimating the delay variance. In this paper, we propose a technique to estimate the dynamic variation in pathdelay caused by coupling noise. We treat the effective coupling capacitance as a random variable that varies as a function of the relative signal arrival times between victim andaggressor nodes. A modeling technique to estimate the\u00a0\u2026", "num_citations": "5\n", "authors": ["493"]}
{"title": "Tutorial T4A: formal verification techniques and tools for complex designs\n", "abstract": " Integrated circuit technology has evolved from micro-controllers and discrete components to fully integrating a large system on a single chip (SoC). Today, verification is the most expensive component in the design cycle in term of cost and time. This cost is estimated to consume about 70% to 80% of the total design effort. The verification cost is expected to increase for SoC designs. This is mainly due to the increase in complexity and to the shrinking of the product design cycle. For example, the color TV took over 10 years to sell 1 million units, while the DVD player took just over a year. This shrinking of the design cycle is going to put more pressure on increasing designer productivity which is affected directly by the cost of verification. For these reasons, verification of complex designs is becoming a bottleneck in the process of producing integrated SoC systems. This tutorial provides an overview of emerging\u00a0\u2026", "num_citations": "5\n", "authors": ["493"]}
{"title": "Jitter decomposition in ring oscillators\n", "abstract": " It is important to separate random jitter from deterministic jitter to quantify their contributions to the total jitter. This paper identifies the limitations of the existing methodologies for jitter decomposition, and develops a new and efficient approach using time lag correlation functions to decompose different jitter components. The theory of the approach is developed and it is applied to a ring oscillator simulated in a 0.6-mum AMI CMOS process. Results show good agreement between the theory and Hspice simulation", "num_citations": "5\n", "authors": ["493"]}
{"title": "LFSR-based BIST for analog circuits using slope detection\n", "abstract": " This paper presents a new analog BIST scheme using a slope detection technique. In test mode, a circuit under test (CUT) is stimulated with a periodic rectangular pulse generated from a Linear Feed-Back Shift Register (LFSR) and a periodic invariant response is generated. The width of the pulse is a BIST parameter to allow a trade-off between test time and fault coverage. In order to maximize fault coverage and minimize the hardware overhead, we propose a slope detection technique which analyzes the response of CUT using a counter and a simple digital gate. Simulation results are presented to show the feasibility of this scheme.", "num_citations": "5\n", "authors": ["493"]}
{"title": "A language formalism for verification of PowerPC/sup TM/custom memories using compositions of abstract specifications\n", "abstract": " We present a methodology in which the behavior of custom memories can be abstracted by a couple of artifacts-one for the interface and another for the contents. Memories consisting of several ports result into several user-provided abstract specifications, which in turn can be converted to simulation models. We show that (i) a simulation model is an approximation of the corresponding abstract specification and (ii) the abstracted memory core can be composed with the un-abstracted surrounding logic using a simple theory of composition. We make use of this methodology to verify equivalence between register transfer level and transistor level descriptions of custom memories.", "num_citations": "5\n", "authors": ["493"]}
{"title": "Suitability of kraft pulp from oil palm trunk for cellulose fibre reinforced cement boards.\n", "abstract": " see more details method to obtain pulps with different kappa numbers. The pulps were used to make cellulose fibre reinforced cement (CFRC) boards. The boards were air-cured and subjected to modulus of rupture, modulus of elasticity, toughness, fracture toughness, water absorption and thickness swelling tests. The CFRC boards had good board properties. Oil palm trunk pulps with kappa numbers between 15 and 28 had optimum properties for making air-cured CFRC boards, giving modulus of rupture as high as 29 MPa. This study shows that the kraft pulp from oil palm trunk can be used as reinforcing material for CFRC boards, thus indicating another use of the oil palm waste.", "num_citations": "5\n", "authors": ["493"]}
{"title": "Automated verification of temporal properties specified as state machines in vhdl\n", "abstract": " This paper presents a new verification methodology to prove that a high level HDL description of a synchronous sequential circuit satisfies certain desired behavior or that it is free of certain malicious behavior. The correctness specifications are modeled as state machines with some transitions having unspecified inputs. We show that this suffices for specification of a large class of properties, including both safety and liveness properties. The properties are described as VHDL programs to enable the designer to simulate them for sample inputs and gain some measure of confidence in their correctness. Experimental results are presented for the Viper microprocessor.", "num_citations": "5\n", "authors": ["493"]}
{"title": "A new asynchronous multiplier using enable/disable CMOS differential logic\n", "abstract": " This paper presents a technique for asynchronous logic design using ECDL (Enable/Disable CMOS Differential Logic). A pipelined serial-parallel multiplier clocked at 55.6 MHz has been designed to show the implementation of this technique. The serial-parallel multiplier architecture has been designed in ECDL using MAGIC, and circuit simulations have been done in HSPICE using a 2 /spl mu/m model from MOSIS. An evaluation of the area using ECDL is presented and compared against techniques used in the past to show that a significant reduction in area overhead is possible.< >", "num_citations": "5\n", "authors": ["493"]}
{"title": "A reconfigurable parallel signature analyzer for concurrent error correction in DRAM\n", "abstract": " An efficient strategy for utilizing a parallel signature analyzer (PSA) for concurrent soft-error correction in DRAMs (dynamic random-access memories) is described. For a two-level w-bit, n-word memory system, the proposed technique needs only one additional chip as opposed to log/sub 2/w+2 in the conventional Hamming code. Such an error-correction circuit significantly improves the reliability of the memory system.< >", "num_citations": "5\n", "authors": ["493"]}
{"title": "Concurrent Error Detection In Vlsi Processor Arrays\n", "abstract": " This paper describes a novel technique using residue codes to detect errors (caused by either permanent or transient faults) in numerical systolic arrays concurrently with the normal operation of the system. A careful analysis of errors is used to drastically reduce the number of residue generators and checkers necessary. Undetectable errors are avoided by suitably choosing the modulo size of the residue code and by slightly modifying the implementation of the multipliers in the truncating circuits or applying few residue code checkers to the array. Error propagation in the array is analyzed in detail to ensure that an erroneous result gen-erated by any adder or multiplier will always be detected at the outputs of the arrays. VLSI implementations of dif-ferent kinds of adders and multipliers are analyzed to show that errors due to faults inside a single bit slice will always produce a detectable error at the output of the arrays\u00a0\u2026", "num_citations": "5\n", "authors": ["493"]}
{"title": "Techniques for efficiently implementing totally self-checking checkers in MOS technology\n", "abstract": " This paper presents some new techniques for reducing the transistor count oof MOS implementations of totally self-checking (TSC) checkers. The techniques are (1) transfer of fanouts, (2) removal of inverters and (3) use of multi-level realizations of functions. These techniques also increase the speed of the circuit and may reduce the number of required tests. Their effectiveness has been demonstrated by applying them to m-out-of-n and Berger code checkers. Impressive reductions of up to 90% in the transistor count in some cases have been obtained for the MOS implementation of these checkers. This directly translates into saving of chip area.", "num_citations": "5\n", "authors": ["493"]}
{"title": "Design for testability\n", "abstract": " ABSTRACT PHYSICAL FAILURES AND FAULT MODELS This tutorial paper discusses the issues related to the design of complex circuits so that they can be tested easily. These issues include the models used to describe the actual physical failures, measures of testability, ad hoc well structured techniques for testable designs, and the detection of transient errors concurrently with computation. as or Occur as or", "num_citations": "5\n", "authors": ["493"]}
{"title": "Reliability analysis of digital systems protected by massive redundancy.\n", "abstract": " Whenever a digital system is used in a critical application such as air-traffic control., spacecraft guidance or hospital patient monitoring, it is essential that it be very reliable. Even if human life is not at stake, a reliable digital system is desirable to reduce inconveniences to users. The basic factor determining the degree of reliability of a system is economics, since a very reliable system generally costs more than a less reliable one. Therefore a system should ideally have a degree of reliability such that i tS cost is about the same as the apparent cost to the users in case the system fails.", "num_citations": "5\n", "authors": ["493"]}
{"title": "Randomized Pulse-Modulating Instruction-Issue Control Circuit for a Current and Temperature Limiting System in a 7nm Hexagon\u2122 Compute DSP\n", "abstract": " A randomized pulse-modulation (RPM) circuit controls the instruction-issue rate in a Qualcomm \u00ae  Hexagon\u2122 compute DSP (CDSP) for adapting performance to limit current and temperature below target thresholds. The current and temperature limiting system contains on-die current and temperature sensors, a limits evaluation (LE) circuit, and the RPM instruction-issue control circuit. When current or temperature exceeds a target threshold, the RPM instruction-issue control circuit adjusts performance in ~5 CDSP clock cycles after accounting for the clock-domain-crossing synchronization overhead to satisfy the 1\u03bcs latency requirement for the entire limiting system. Silicon measurements from a 7nm Hexagon\u2122 CDSP demonstrate that the RPM instruction-issue control circuit enables a 0.4 %  performance resolution across a wide range of operation from 100 %  to 0.4 %  while avoiding thread starvation during multi\u00a0\u2026", "num_citations": "4\n", "authors": ["493"]}
{"title": "Methods and systems for selecting compatible resources in networked storage environments\n", "abstract": " Methods and systems for a networked storage environment are provided. One method includes storing at a storage device by a processor a data structure with information regarding a plurality of compatible software and hardware components configured to operate in a plurality of configurations as part of a storage solution for using resources at a networked storage environment for storing data; obtaining by a processor executable configuration advisor information regarding a plurality of deployed components of a deployed configuration from among the plurality of configurations; comparing the deployed configuration with a latest configuration having the plurality of deployed components from among the plurality of configurations stored at the data structure; and recommending a change in at least one of the plurality of deployed components to upgrade the deployed configuration, when the deployed configuration is\u00a0\u2026", "num_citations": "4\n", "authors": ["493"]}
{"title": "Cross-layer control adaptation for autonomous system resilience\n", "abstract": " The last decade has seen tremendous advances in the transformation of ubiquitous control, computing and communication platforms that are anytime, anywhere. These platforms allow humans to interact with machines through sensing, control and actuation functions in ways not imaginable a few decades ago. While robust control techniques aim to maintain autonomous system performance in the presence of bounded modeling errors, they are not designed to manage large multi- parameter variations and internal component failures that are inevitable during lengthy periods of field deployment. To address the trustworthiness of autonomous systems in the field, we propose a cross-layer error resilience approach in which errors are detected and corrected at appropriate levels of the design (hardware-through software) with the objective of minimizing the latency of error recovery while maintaining high failure\u00a0\u2026", "num_citations": "4\n", "authors": ["493"]}
{"title": "Diabetes related distress in adults with type 2 diabetes mellitus: a community-based study\n", "abstract": " Background: Living with diabetes can be difficult, since it can affect the patient physically as well as psychologically. Patients with diabetes face psychological issues which may be part of the spectrum of disease experience, distinct from depression, which hinder glycaemic control. The objective of the study was to determine the prevalence of diabetes related distress, and its association with socio-demographic characteristics, in adults with type 2 diabetes.Methods: A community based cross sectional study was conducted among 250 individuals of 30-60 years, with type 2 diabetes.Results: The prevalence of diabetes related distress in the study population was 13.3%; among the sub scales highest reported was regimen related distress 21.6%, followed by physician related 17.2%, emotional burden 16.4%, and inter personal distress 14.8%. Diabetes related distress was found to have significant statistical association with occupational class. In occupational class, distress was higher among unemployed while least in unskilled workers. It was higher among older (above 50 years) participants, males, members of joint family, unmarried and those with more years of education though there was no significant difference. Conclusions: The prevalence of diabetes related distress (13.2%) especially regimen and physician related, underscores need for better clinician involvement paying appropriate attention to systematic diabetes self-care and management education, and timely diagnosis of distress for positive clinical outcome.", "num_citations": "4\n", "authors": ["493"]}
{"title": "Checksum based error detection in linearized representations of non linear control systems\n", "abstract": " While the past decades have seen major revolutions in the computing and communication industries, the next decade will experience a proliferation of intelligent autonomous systems capable of performing complex tasks. These systems will consist of a plethora of sensors communicating with each other to achieve specified system level objectives. The success and deployment of these systems in commercial arena depends on the reliability and security of the system design. In this work, we discuss error detection methodologies that can detect sensor failures and actuator malfunctions in a linearized representation of a non-linear control system. We demonstrate the proposed approaches on the linearized representation of the classical non-linear problem of an inverted pendulum which has a linear Kalman filter as its sensor and a linear servo motor as its actuator. Simulation results show the effectiveness of the\u00a0\u2026", "num_citations": "4\n", "authors": ["493"]}
{"title": "Error resilient real-time state variable systems for signal processing and control\n", "abstract": " The advent of sensor networks, robots, autonomous vehicles and the smart grid have made the dependability of circuits and systems that control them critical to society and national defense. While significant advances in the design of linear and nonlinear control systems have been made to allow modes of operation not possible in the past, the problem of resilience to errors induced by hostile operating environments remains largely unexplored even though the probability of such errors occurring during real-time operation has increased. In this talk we propose mechanisms for detecting transient errors in control systems and circuitry as well as diagnosing and correcting for their effects on overall system operation. It is shown how real-number checksum encodings of circuit function can be used to detect and correct errors in the plant and feedback subsystems of linear control systems. Applications to signal\u00a0\u2026", "num_citations": "4\n", "authors": ["493"]}
{"title": "On computing criticality in refactored timing graphs\n", "abstract": " The maximum operator in statistical static timing analysis (SSTA) is a decent approximation for timing sign-off, but often causes significant error in SSTA applications. This paper presents a timing criticality computation method based on non-maximum analytic operators in a parameterized SSTA. After an SSTA run, the proposed method computes the criticality for all edges and nodes in a single graph traversal. Although we do not employ the max operator in the computation process, the error in the maximum operator still degrades the accuracy of the computed criticality because the criticality is a joint probability of expressions, including arrival times, which are computed by the maximum operator during SSTA. To address this issue, we employ the refactoring technique, which was recently proposed to reduce common path pessimism in combinational circuits. This paper shows that refactoring is also very useful in\u00a0\u2026", "num_citations": "4\n", "authors": ["493"]}
{"title": "Frequency-independent parametric built in test solution for PLLs with low speed test resources\n", "abstract": " Phase Locked Loops (PLLs) are required to meet analog specifications such as lock time, phase error, jitter in addition to the frequency lock test in production. Parametric testing of PLLs is resource intensive and requires high precision hardware on the Automatic Test Equipment (ATE). This paper proposes a Built in Self Test (BIST) scheme to perform functional and parametric testing using low frequency ATE resources. The BIST module is independent of the PLL frequency and can be used effectively in multi-PLL System-on-a-Chip (SoC) modules. The scheme uses the Phase Detector (PD) output signature to test for lock and transient response using a digital BIST module. The BIST scheme is designed without impacting the mission mode performance by not perturbing analog nodes and the phase sensitive feedback loop. The test methodology enables parametric testing for process variation and specification\u00a0\u2026", "num_citations": "4\n", "authors": ["493"]}
{"title": "System accuracy estimation of SRAM-based device authentication\n", "abstract": " It is known that power-up values of embedded SRAM memory are unique for each individual chip. The uniqueness enables the power-up values to be considered as SRAM fingerprints used to verify device identities, which is a fundamental task in security applications. However, as the SRAM fingerprints are sensitive to environmental changes, there always exists a chance of error during the authentication process. Hence, the accuracy of a device authentication system with the SRAM fingerprints should be carefully estimated and verified in order to be implemented in practice. Consequently, a proper system evaluation method for the SRAM-based device authentication system should be provided. In this paper, we introduce tractable and computationally efficient system evaluation methods, which include novel parametric models for the distributions of matching distances among genuine and imposter devices. In\u00a0\u2026", "num_citations": "4\n", "authors": ["493"]}
{"title": "A Low Cost Built-In Self-Test Circuit for High-Speed Source Synchronous Memory Interfaces\n", "abstract": " A built-in self-test (BIST) for testing high speed source-synchronous memory interfaces has been designed using 0.18-\u03bcm TSMC process. To overcome limitations of the resolution and the accuracy in low-cost automated test equipment (ATE), a cycle-by-cycle controllable embedded pattern generator in the proposed BIST scheme is presented to specify performance-related I/O parameters. Using this method, the I/O parameters affected by the internal and the external mismatches are investigated by measuring the relative timing differences between the data lines and the strobe signal. The measurement results are monitored with low frequency output by using dividers and the embedded pattern generator. The advantage of this low cost approach is that it does not require ATE to access high frequency signals for testing. Monte Carlo simulations are performed to verify the circuit operations, and the experimental\u00a0\u2026", "num_citations": "4\n", "authors": ["493"]}
{"title": "A Built-In Self-Test scheme for high speed I/O using cycle-by-cycle edge control\n", "abstract": " This paper presents a Built-In Self-Test (BIST) circuit for high speed I/O, based on an embedded pattern generator to remove external factors which could affect the I/O parameters. The rising and falling edge positions of the generated patterns can be controlled independently during every cycle. In the basic operation mode, ATE provides the codes for controlling the edge positions, while in extended mode, an embedded counter generates the control codes. The control of both rising and falling edges makes this scheme especially good for systems with Double-Data Rate (DDR) interfaces. Moreover, the cycle-by-cycle control allows us to analyze efficiently the influence of mismatch trees and per-pin skew on I/O performance. The proposed BIST circuit has been simulated using a 0.18-\u03bcm process.", "num_citations": "4\n", "authors": ["493"]}
{"title": "Multitone digital signal based test for RF receivers\n", "abstract": " This paper presents a new method for testing radio frequency (RF) receivers that utilizes a multitone digital signal generation scheme and relies on the analysis of the receiver baseband output to compute the RF performance parameters. The proposed method takes out the cost of expensive RF instrumentation on the input side of receiver testing and only requires the less expensive baseband digitization at the receiver output. The complexity of the RF signal generation inherent to standard methods is traded off with extensive signal processing on the baseband side, with the tedious analysis necessary for tackling the problem being addressed and presented herein. While the proposed test scheme was implemented and experimentally verified on a load board for testing UHF receivers, generalized use in BIST applications in need of multi-GHz RF stimuli is also discussed in the paper. RF performance parameters\u00a0\u2026", "num_citations": "4\n", "authors": ["493"]}
{"title": "A hierarchy of subgraphs underlying a timing graph and its use in capturing topological correlation in SSTA\n", "abstract": " This paper shows that a timing graph has a hierarchy of specially defined subgraphs, based on which we present a technique that captures topological correlation in arbitrary block-based statistical static timing analysis (SSTA). We interpret a timing graph as an algebraic expression made up of addition and maximum operators. We define the division operation on the expression and propose algorithms that modify factors in the expression without expansion. As a result, they produce an expression to derive the latest arrival time with better accuracy in SSTA. Existing techniques handling reconvergent fanouts usually use dependency lists, requiring quadratic space complexity. Instead, the proposed technique has linear space complexity by using a new directed acyclic graph search algorithm. Our results show that it outperforms an existing technique in speed and memory usage with comparable accuracy.", "num_citations": "4\n", "authors": ["493"]}
{"title": "Closed-loop Built in Self Test for PLL production testing with minimal tester resources\n", "abstract": " Phase locked loops (PLLs) are extensively used in modern system on a chip (SoC) modules for generating timing, clock signal recovery and to provide a timing reference for communication interfaces. Due to their use in crucial and omnipresent applications, PLLs are the only mixed signal components on many otherwise digital blocks. The mixed signal nature makes testing of PLLs complicated as the output test requirements include non-digital parameters such as phase error, lock time, jitter along with the typical frequency locking test. Automatic test equipment (ATE) resources needed for these added parameters may require the use of a higher-end ATE for an otherwise digital block, driving up the production test costs. Built in self test (BiST) approaches for PLLs need to be carefully designed due to the load-sensitive nature of the internal analog nodes as well as sensitivity of the feedback path to any added delays\u00a0\u2026", "num_citations": "4\n", "authors": ["493"]}
{"title": "A 6-bit 300-MS/s 2.7 mW ADC based on linear voltage controlled delay line\n", "abstract": " This paper describes a novel ADC architecture based on time domain processing. By using a voltage controlled delay line, a linear transfer function of the input dependent delay is formed. The time delay difference is then compared to a reference to generate a digital code for the input. We have designed a 300-MS/s 6 bit ADC using this architecture in a 0.13 m standard digital CMOS. The simulation results show 36.6 dB SNR, 34.1 dB SNDR for 99 MHz input, DNL<0.2LSB, and INL<0.5LSB. Overall chip power is 2.7 mW with a 1.2 V power supply.", "num_citations": "4\n", "authors": ["493"]}
{"title": "Low-cost test of timing mismatch among time-interleaved A/D converters in high-speed communication systems\n", "abstract": " Time interleaved A/D converters (TIADCs) provide an attractive solution to the realization of analog front ends in high speed communication systems. However, gain mismatch, offset mismatch, and sampling time mismatch between time-interleaved channels limit the performance of TIADCs. This paper presents a low-cost test scheme to measure timing mismatch using an undersampling clock. Our method is applicable to an arbitrary number of channels, achieving picosecond resolution with low power consumption. Both simulation and hardware measurements are presented to validate the proposed technique.", "num_citations": "4\n", "authors": ["493"]}
{"title": "Built-in Fault Diagnosis for Tunable Analog Systems Using an Ensemble Method\n", "abstract": " This paper presents a new low-cost fault diagnosis technique based on built-in self test (BIST). The method enables rapid and accurate identification of weak spots in a design and potential problems in the manufacturing process, thereby leading to a significant reduction in time-to-market. Fault diagnosis is accelerated with available on-chip BIST which can generate low-cost signatures (performance parameters). Imperfect signatures due to limited on-chip resources and accuracy are compensated in two ways. Supplemental signatures are obtained from a re-configured device under test (DUT) by parameter tuning, leading to improvements in diagnosability. Secondly, diagnosis accuracy is significantly improved by using an ensemble method which has been widely used in data mining. The technique can be used to identify single as well as multiple faults, and can also be used to facilitate a self-repair mechanism\u00a0\u2026", "num_citations": "4\n", "authors": ["493"]}
{"title": "Checking nested properties using bounded model checking and sequential atpg\n", "abstract": " This paper develops a novel approach to formally verify nested VLSI circuit properties, using bounded model checking and gate-level sequential ATPG tools. This approach improves the verification quality by devising an algorithm that checks nested realistic properties. This makes ATPG verification based tools applicable to realistic properties. We also show that the performance of our approach is superior when compared to SAT-based techniques in both efficiency and capacity, especially for large bounds and for complex properties.", "num_citations": "4\n", "authors": ["493"]}
{"title": "Efficient combinational verification using overlapping local BDDs and a hash table\n", "abstract": " We propose a novel methodology that combines local BDDs with a hash table for very efficient verification of combinational circuits. The main purpose of this technique is to remove the considerable overhead associated with case-by-case verification of internal node pairs in typical internal correspondence based verification methods. Two heuristics based on the number of structural levels of circuitry looked at and the total number of nodes in the BDD manager are used to control the BDD sizes and introduce new cutsets based on already found equivalent nodes. We verify the ISCAS85 benchmark circuits and demonstrate significant speedup over existing methods. We also verify several hard industrial circuits and show our superiority in extracting internal equivalences.", "num_citations": "4\n", "authors": ["493"]}
{"title": "Is state mapping essential for equivalence checking custom memories in scan-based designs?\n", "abstract": " Equivalence checking between Register Transfer Level (RTL) descriptions and transistor level descriptions of custom memories is an important step in the design flow of high performance microprocessors. Equivalence checking can be done with or without the knowledge of state mapping between the two descriptions. We present evidence that because of state mapping, our verification technique exercises system behavior that exposes hard-to-detect bugs that might otherwise go undetected. This paper defines Crossover Bugs (CB's) that can be present in scan-based custom designs and that are inherently hard-to-detect without state mapping. We demonstrate that such bugs can be missed by equivalence checking techniques that do not have state mappings between the two descriptions. By identifying the state correspondences between the RTL and the transistor implementation of custom memories, a more\u00a0\u2026", "num_citations": "4\n", "authors": ["493"]}
{"title": "Selective-run built-in self-test using an embedded processor\n", "abstract": " Many systems-on-a-chip (SOCs) include processors as central units to implement diverse algorithms and control peripheral units such as embedded cores. The computing power of the embedded processor can be used to self-test its own functions as well as to test the other cores within the chip boundary. In BIST methodology, pseudo-random pattern testing can reduce the memory requirements. In addition to general pseudo-random pattern testing, this paper proposes and evaluates a novel selective-random pattern test technique. This technique increases the fault coverage while significantly reducing test application time. This also greatly decreases the memory requirements compared to traditional BIST schemes. The cost for extra hardware is low and the technique is easily integrated with parallel scan and boundary scan designs.", "num_citations": "4\n", "authors": ["493"]}
{"title": "An efficient 3-bit-scan multiplier without overlapping bits, and its 64/spl times/64 bit implementation\n", "abstract": " In this paper, we present an efficient 3-bit-scan multiplier without overlapping bits which has good power-delay area trade-offs. Generation of partial product terms in this multiplier is performed in parallel with the multiplication operation. Parallel partial product generation results in a multiplier which is faster than conventional sequential multipliers. The architecture of the 3-bit-scan multiplier without overlapping bits is therefore suitable for synchronous sequential multipliers which are required to operate at low power and at relatively high speed for their area.", "num_citations": "4\n", "authors": ["493"]}
{"title": "Full chip false timing-path identification\n", "abstract": " Static timing analysis sets the industry standard in the design methodology to gage the speed of high performance microprocessors. Unfortunately, not all the paths identified using such analysis can be sensitized. This leads to a pessimistic estimation of processor speed, and the engineering efforts spent optimizing such paths can not improve the performance of the chip. In the past, we demonstrated initial results of how ATPG technique can be used to eliminate false paths efficiently. Due to the gap between the physical design on which the static timing analysis of the chip is based and the test view on which the ATPG technique is applied to eliminate false paths, in many cases only sections of some of the paths in the full-chip were analyzed in our initial results. In this paper, we fully analyze all the timing paths using the ATPG technique overcoming the gap between the testing and timing analysis techniques. We\u00a0\u2026", "num_citations": "4\n", "authors": ["493"]}
{"title": "A mixed-signal BIST scheme with time-division multiplexing (TDM) comparator and counters\n", "abstract": " A low-cost and efficient built-in self-test (BIST) scheme is proposed for analog and mixed-signal circuits. We implement the time-division multiplexing (TDM) comparator to analyze the response of a circuit under test (CUT) with minimum hardware overhead. The comparator converts the response to a sequence of ones and zeros by comparing the response to the reference voltages at each time slot using time-division multiplexing. Simple counters are connected to the comparator to accumulate the ones generated at each time slot and used as a signature analyzer. This TDM comparator can be used to monitor internal nodes in addition to the classical primary output nodes to improve circuit testability. Simulation results are presented to illustrate the effectiveness of this scheme.", "num_citations": "4\n", "authors": ["493"]}
{"title": "Using verification technology for validation coverage analysis and test generation\n", "abstract": " Despite great advances in Formal Verification (FV) simulation is still the primary means for design validation. The definition of pragmatic measures for the coverage achieved and the problem of automatic test generation (ATG) are of great importance. In this paper we introduce a new set of metrics, the Event Sequence Coverage Metrics (ESCMs). Our approach is based on an automatic method to extract the control flow of a circuit which can be explored for coverage analysis and ATG. We combine FV and traditional ATPG techniques to automatically generate sequences which traverse uncovered parts of the control graph or exercise uninstantiated control event sequences.", "num_citations": "4\n", "authors": ["493"]}
{"title": "Benchmarking parallel processing platforms: an applications perspective\n", "abstract": " Given the increased availability of general purpose parallel computers two issues arise: One needs to compare the performance of the different available platforms using realistic examples, and it is necessary to write application software that can be ported easily in order to take advantage of different platforms. The authors address these issues from an applications point of view. They are interested in the use of general purpose parallel computers for simulation tasks needed during the design of very large scale integrated (VLSI) circuits. They characterize the simulation task as a useful benchmark and introduce a high level process view of parallel simulation that is helpful for deriving portable parallel programs. Details of the partitioning strategy and the simulation algorithm used in the application are given. They discuss their implementation on different parallel machines and give statistics of various experiments.< >", "num_citations": "4\n", "authors": ["493"]}
{"title": "Parallel switch-level simulation for VLSI\n", "abstract": " Switch-level simulation is widely used in the design verification process of Very Large Scale Integrated (VLSI) MOS circuits. In this paper, the authors present methods for accelerating switch-level simulation by mapping it onto general purpose parallel computers. Their target machines are medium-grain multiprocessors (shared memory or message passing machines) and they only consider model parallel computation, where the model of the design to be simulated is partitioned among processors. Efficient strategies are introduced for circuit partitioning as well as the corresponding simulation algorithms. In the authors' approach, they try to minimize the total number of synchronizations between processors, as well as ensure portability and scalability. A preprocessor and simulator were implemented and good performance was obtained for a set of benchmarks. The problem of tight coupling between processors that\u00a0\u2026", "num_citations": "4\n", "authors": ["493"]}
{"title": "The testability of generalized counters under multiple faulty cells\n", "abstract": " The testability of a class of circuits called generalized counters is investigated under a more powerful fault model than examined in earlier work. It is assumed that any number of full adders in a generalized counter can assume an incorrect function under fault, as long as the function remains combinational. The testability of the overall class of generalized counters is examined and it is shown that under a restricted fault model it is possible to detect all multiple faults with a test set that grows linearly with the number of counter inputs. It is then shown that for a subset of the class of generalized counters it is possible to detect multiple faults with a larger number of tests, linear to the number of counter inputs, when the restrictions on the fault model are relaxed.< >", "num_citations": "4\n", "authors": ["493"]}
{"title": "Automatic classification of node types in switch-level descriptions\n", "abstract": " In switch-level simulation, nodes carry a charge on their parasitic capacitance from one evaluation to the next, which gives them a memory quality. A node is classified as temporary if its memory aspect is lost and cannot affect the circuit operation, whereas a node is classified as a memory node if the memory of the node is maintained and can affect the circuit operation. Accurate classification of nodes into temporary and memory nodes increases the performance of compiled simulators and high-level model generators. An approach for reliable automatic classification of nodes in a switch-level description is introduced. Both an exhaustive, exponential-time algorithm and a polynomial-time heuristic are presented. The heuristic was implemented and tested for several large circuits, including a commercial microprocessor. For this processor, the proposed heuristics identified an average of 92% of all nodes as temporary\u00a0\u2026", "num_citations": "4\n", "authors": ["493"]}
{"title": "Average checksum codes for fault-tolerant matrix operations on processor arrays\n", "abstract": " Algorithm based fault tolerance is a new technique to achieve fault tolerance with low overhead in processor arrays. In an application of this technique, checksum and weighted checksum codes have been suggested for fault tolerant matrix computations in low cost processor arrays. The disadvantage with these encoding schemes is that the encoding itself can introduce overflow errors even when there are no such errors in the computation of the unencoded data elements. In this paper we suggest two modified techniques, the Average Checksum Code (ACC) and the Weighted Average Checksum Code (WACC), for matrix computations. These encodings will not cause any overflow errors unless there is an overflow problem in a computation involving the original, unencoded information matrix. The performance overhead for the schemes is evaluated using a simulator environment. tasks can be formulated using a common set of matrix computations, most proposed VLSI arithmetic devices have been vector and matrix type computations.The application areas of these high speed computers demand a high degree of reliability of the computed results. However, the probability of errors in the result increases with amount of computation (this is related to the amount of computation, the hardware used and time taken for the computation). Thus, in order to accommodate two contradicting features, high complexity and high reliability, the system has to be designed to be fault tolerant.", "num_citations": "4\n", "authors": ["493"]}
{"title": "On the Design of Fault-Tolerant Systolic Arrays with Linear Cells.\n", "abstract": " In many numerical systolic arrays, each processing element in the regular part of the array is itself a linear sytem (called a linear cell). A systematic approach to the design of fault-tolerant systems for such systolic arrays is developed. Most of the proposed systolic arrays for matrix operations, polynomial operations, and digital signal processing can be made fault-tolerant using the authors' procedure. The design procedure preserves the structure of the original (non-fault-tolerant) systolic array, making it easy to incorporate fault tolerance; the faulty units can be identified, which permits reconfiguration if necessary. The design methodology encodes the inputs data at a high level and ensures that the algorithm generates encoded output data; the encoding is tailored to the structure of the systolic array. The encoded input data are passed through the systolic array in ways which will avoid problems with error masking due to failures, resulting in an extremely low overhead for fault tolerance.", "num_citations": "4\n", "authors": ["493"]}
{"title": "Self-test for microprocessors.\n", "abstract": " This paper presents a methodology for self-test of complex microprocessor chips which include integrated peripheral control modules. Novel test techniques are presented for test data compression and pattern generation using microprocessor instructions and CPU data registers. An enhanced instruction execution fault model and new fault models for peripheral devices are proposed. A specific application of this methodology to test the Intel 80186 microprocessor is described.", "num_citations": "4\n", "authors": ["493"]}
{"title": "Common disease conditions among cattle slaughtered in Trichur municipal slaughter house-a preliminary study.\n", "abstract": " In a survey of 1100 cattle slaughtered between December 1976 and September 1978, there were 468 cases of echinococcosis echinococcosis Subject Category: Diseases, Disorders, and Symptoms", "num_citations": "4\n", "authors": ["493"]}
{"title": "Design of a safe convolutional neural network accelerator\n", "abstract": " Recently Machine Learning (ML) accelerators have grown into prominence with significant power and performance efficiency improvements over CPU and GPU. In this paper, we developed an Algorithm Based Error Checker (ABEC) for Concurrent Error Detection (CED) based on an industry quality Convolution Neural Network (CNN) accelerator with priority to meet high safety Diagnostic Coverage (DC) requirement and enhanced area and power efficiency. Furthermore, we developed an Algorithm Based Cluster Checker (ABCC) with coarse-grained error localization to improve run-time availability. Experimental results showed that we could achieve above 99% DC with only 30% area and power overhead for a selected configuration.", "num_citations": "3\n", "authors": ["493"]}
{"title": "Cross-layer resilience: Challenges, insights, and the road ahead\n", "abstract": " Resilience to errors in the underlying hardware is a key design objective for a large class of computing systems, from embedded systems all the way to the cloud. Sources of hardware errors include radiation, circuit aging, variability induced by manufacturing and operating conditions, manufacturing test escapes, and early-life failures. Many publications have suggested that cross-layer resilience, where multiple error resilience techniques from different layers of the system stack cooperate to achieve cost-effective resilience, is essential for designing cost-effective resilient digital systems. This paper presents a comprehensive overview of cross-layer resilience by addressing fundamental cross-layer resilience questions, by summarizing insights derived from recent advances in cross-layer resilience research, and by discussing future cross-layer resilience challenges.", "num_citations": "3\n", "authors": ["493"]}
{"title": "Effective control flow integrity checks for intrusion detection\n", "abstract": " Ensuring run time Control Flow Integrity (CFI) has proven to be a good way to detect and prevent intrusions which result from exploitation of unknown vulnerabilities in the software. Attackers need to change the control flow and/or the code text of the victim application to achieve their malicious intent. However, existing techniques for monitoring run time CFI have been impractical due to their large software and hardware costs. In this paper, we describe a practical hardware based approach at a fine granularity to ensure integrity of code and the control flow of an executing application. We utilize the low power benefits and randomness of a stream cipher based hash, combined with the efficient hardware based monitoring, to provide a practical and functional defense against intrusion attacks.", "num_citations": "3\n", "authors": ["493"]}
{"title": "The future of fault tolerant computing\n", "abstract": " Fault tolerant (or dependable) computing has always been an exciting research area in the intersection of computer science and engineering and electrical and electronics engineering. During the last two decades the applicability of the methods and tools that the fault tolerance research community produces has expanded to virtually all application domains. The type of fault tolerance methods employed in a computing system depend on: (a) the faults expected to affect the system, (b) the importance of errors in the system operation, (c) the design, cost and power budgets that can allocated to fault tolerance and reliable operation. New solutions and tools in fault tolerant computing are emerging to deal with the very broad spectrum of values that all (a), (b) and (c) can take in today's computing landscape.", "num_citations": "3\n", "authors": ["493"]}
{"title": "ILL Effects of internet addiction-knowledge and attitude among nursing students\n", "abstract": " Computer and internet have become an important part of our life that no individual can even think of living without it. And it is the INTERNET facility in it due to which everything is possible. Internet is a device that connects you to the whole of the world.. The adolescent population is highly prone to internet addiction and this can affect their personal, family, academic, interpersonal and social life. To assess the knowledge and attitude regarding the ill effects of internet addiction among the nursing students, find the correlation between the knowledge and attitude, and find association between knowledge, attitude and the selected demographic variables. A descriptive survey approach was used to assess the knowledge and attitude regarding ill effects of internet addiction among nursing students in selected nursing institutions in Mangaluru. Non probability purposive sampling technique was used to select 100 nursing students. Data was gathered using the structured knowledge questionnaire and attitude scale on ill effects of internet addiction. Data was analyzed using descriptive and inferential statistics. The study result showed that the majority (67%) of the sample had moderately adequate knowledge regarding ill effects of internet addiction. Majority (77%) of the sample showed favorable attitude towards ill effects of internet addiction. In the study there was a significant association between knowledge and selected demographic variables and the attitude and the selected demographic variables. There was also a significant correlation between the knowledge and attitude of the nursing students regarding ill effects of internet addiction. The study\u00a0\u2026", "num_citations": "3\n", "authors": ["493"]}
{"title": "Designing nonlinearity characterization for mixed-signal circuits in system-on-chip\n", "abstract": " Long test times and the use of conventional automatic test equipment (ATE) makes conventional mixed-signal linearity performance testing costly. Diminishing test time of linearity test significantly reduces system-on-a-chip production test costs and, therefore, lessens total product manufacturing costs. Several low-cost linearity test methods have addressed this issue for a single-ended mixed-signal circuit testing. On the other hand, a low-cost test approach has rarely been proposed for differential mixed-signal circuits, due to a new class of test obstacles from differential circuits that are widely employed for high-speed I/O products. This paper presents a cost-effective self-test methodology to characterize the linearity performance of differential mixed-signal circuits in loopback mode. The proposed method precisely predicts the device-under-test (DUT) linearity specifications by building accurate DUT nonlinear\u00a0\u2026", "num_citations": "3\n", "authors": ["493"]}
{"title": "Real-time correction of dc servo motor and controller failures using analog checksums\n", "abstract": " Recently optimal controller design for real-time systems has been a prominent research focus for their application in intelligent autonomous systems. However, in the future it will be extremely important for real-time monitoring of safety-critical applications for their reliability. In this paper, we extend the theory for real-time compensation of transient errors and permanent faults in linear control systems derived from a low-overhead error detection scheme developed for plant-controller systems. The approach is demonstrated for transient error correction and parametric fault compensation on a servo-motor control application.", "num_citations": "3\n", "authors": ["493"]}
{"title": "Capacitor-coupled built-off self-test in analog and mixed-signal embedded systems\n", "abstract": " Design-for-test (DfT) circuitry that employs differential terminals inherently suffers from an imbalance in the output of its differential pair. By providing the imbalanced differential test stimulus from the DfT circuitry, nonlinearity is eventually introduced in a differential mixed-signal circuit under test, resulting in low test accuracy and significant yield loss during production testing. Consequently, in only rare cases are attempts made to measure dynamic performance of differential mixed-signal circuits using a self-test approach. This brief suggests an efficient testing technique based on built-off self-test for differential analog and mixed-signal circuits. This technique precisely predicts individual device-under-test (DUT) specifications by resolving the imbalance problem using simple variable capacitors in loopback mode. The variable capacitor generates predefined imbalances to give different weights on the spectral\u00a0\u2026", "num_citations": "3\n", "authors": ["493"]}
{"title": "Stream cipher hash based execution monitoring (SCHEM) framework for intrusion detection on embedded processors\n", "abstract": " Hardware based execution monitoring of applications holds the promise for an effective and tamper-proof solution for intrusion detection on processor. This paper presents a practical hardware based intrusion detection framework which uses stream cipher based hashing techniques for runtime control flow and instruction integrity monitoring. This framework enables accurate monitoring of the control flow of a process with an instruction level granularity. Additional hardware required for implementation of our framework has very low power and area overheads which makes it possible to practically implement execution monitoring even on embedded processors. Our technique achieves an order of magnitude lower power overhead compared to other similar techniques. Furthermore, our implementation of the developed framework has a low intrusion detection latency, which enables us to verify the control flow integrity\u00a0\u2026", "num_citations": "3\n", "authors": ["493"]}
{"title": "On-Chip Programmable Dual-Capture for Double Data Rate Interface Timing Test\n", "abstract": " Memory interface speed has been rapidly increasing to overcome the performance gaps between microprocessor and memory. Testing the I/O timing parameters at-speed has become a challenge because of the limitations on the test clock frequencies provided by low-cost testers. This paper presents a technique to generate a dual-capture signal with a programmable delay for both rising and falling transitions, which effectively tests double-data rate memory interface timing. The relative delay difference between data and clock paths is measured for the I/O timing test instead of using complicated test vectors. The test clock frequency is programmed in a wide operating range with 20 ps resolution. The proposed on-chip programmable double-capture generator can be also easily integrated with the current scan-based delay test methods. The scheme has low area overhead, low design effort, and is also compatible\u00a0\u2026", "num_citations": "3\n", "authors": ["493"]}
{"title": "PLL lock time prediction and parametric testing by lock waveform characterization\n", "abstract": " Phase Locked Loop (PLL) testing is complicated due to its mixed signal nature. Internal nodes, which are digital as well as analog, are sensitive to phase, delays and parasitic loads. Parametric characteristics such as lock time, jitter, phase etc. may be critical for a PLL depending on the application and comprehensive production testing of any of these parameters is impractical due to test cost implications. This paper proposes a method to sample and analyze the PLL lock waveform in order to predict the PLL lock time and perform parametric testing of the internal analog blocks. Data is presented across process corners to support the proposal. This method uses either an Automatic Test Equipment (ATE) or on-chip Digital Signal Processor (DSP) to compute FFT values for the lock waveform which is then used for lock time prediction. The test scheme includes parametric fault coverage in addition to catastrophic fault\u00a0\u2026", "num_citations": "3\n", "authors": ["493"]}
{"title": "Test Education in the Global Economy\n", "abstract": " There is an increasing demand for test and diagnosis expertise in the global semiconductor industry, in sectors ranging from foundries to test houses, to IDM companies, and from fabless design houses to EDA companies. Test education, however remains a niche, highly specialized subject area in the graduate curriculum and is seldom covered in undergraduate classes. In this panel, we evaluate the current health of academic test education and debate the role and goals of future test education, as well as those changes that need to be made to meet the global market demands.", "num_citations": "3\n", "authors": ["493"]}
{"title": "Delay constrained Register Transfer Level dynamic power estimation\n", "abstract": " We present a top-down technique to estimate the average dynamic power consumption of combinational circuits at the register transfer level. The technique also captures the power-delay characteristics of a given combinational circuit. It uses the principles of logical effort to estimate the variation in capacitance, and a combination of existing techniques to estimate the variation in activity, over the delay curve of operation of the circuit. The technique does not involve post-estimation characterization and is applicable across technology nodes. The estimated power obtained from our method shows good accuracy with respect to the power obtained from a commercial gate-level power estimation tool.", "num_citations": "3\n", "authors": ["493"]}
{"title": "Conditioned HDL Slicing A way to Speed-up Formal Verification\n", "abstract": " Modern complex digital systems are described in Hardware Description Language (HDL). The increase in design complexity are causing verification tools to require large amount of resources. In this paper, we propose a technique to obtain a HDL executable conditioned design slice that is behaviorally equivalent to the original design. The conditioned slice is less complex than the original and static sliced design and require less resources to analyze by a verification tool. The slicer is implemented in a computer program which is used as a pre-processor to SMV verification. The results show that the conditioned slice significantly reduces the CPU and memory overhead needed to verify the USB2.0 IP core by SMV.", "num_citations": "3\n", "authors": ["493"]}
{"title": "Improving witness search using orders on states\n", "abstract": " We present a method for constructing concrete executions or witnesses to abstract behaviour specifications. The key concept is the use of an ordering on states which preserves containment of behaviours seen from the states. We present a modified depth-first search algorithm which uses the ordering to prune the requisite search paths and the memory needed for the history of the search. We apply the search to a model of a superscalar pipeline.", "num_citations": "3\n", "authors": ["493"]}
{"title": "AD&T Roundtable: Online Test\n", "abstract": " Online testing has been a known field for sometime now. However, today\u2019s situation with online testing may be a little bit different than it was in the past. With the nanometer technologies, we may see more soft errors that cause problems. The goal of this roundtable is to analyze where online test was and where future technologies are going. D&T thanks participants Jacob A. Abraham (Univ. of Texas at Austin), Miron Abramovici (Bell Labs), Silvano Motto (CAEN Microeletronica), Isaac Levendel (Motorola), and Michael Nicolaidis (TIMA). Our moderator was Yervant Zorian (LogicVision). Also attending were Lorena Anghel (TIMA) and Fabian Vargas (Puers, Brazil). D&T Roundtable Editor Kaushik Roy (Purdue Univ.) organized the event.", "num_citations": "3\n", "authors": ["493"]}
{"title": "AJA Formal checking of reliable user interfaces\n", "abstract": " This paper develops techniques for the formal analysis of user interfaces based on a nitestate machine representation. Parameters related to the dependability of an interface can be mapped to desirable properties of the interface, which are themselves represented as nite-state machines. Formalverication tools can then be used to determine whether an interface satises desirable properties and to ensure that it does not satisfy undesirable ones. The techniques are illustrated with an example of a simplied Automatic Teller Machine.", "num_citations": "3\n", "authors": ["493"]}
{"title": "BiCMOS logic testing\n", "abstract": " With the anticipated growth of BiCMOS technology for high-performance ASIC design, the issue of testing takes on great significance. This paper addresses the testing of BiCMOS logic circuits. Since many different implementations of BiCMOS gates have been proposed, four representative ones are studied. The adequacy of stuck-at, quiescent current, and delay testing are examined based on circuit level faults. It is demonstrated that a large portion of the defects cannot be detected by common stuck-at or quiescent current tests since they manifest themselves as delay faults. By using the results presented, the test methodologies and the logic families can be ranked based on fault coverage. This ranking can then be used to help decide which BiCMOS solution is proper for a given application.< >", "num_citations": "3\n", "authors": ["493"]}
{"title": "Abstraction of data path registers for multilevel verification of large circuits\n", "abstract": " Automatic verification of implementations against their specifications in the design hierarchy is largely based on state machine comparison. This paper presents a simple technique that exploits information about correspondence between registers in the data path to enable abstraction of data path registers and make automatic verification of circuits with large date paths tractable. Correspondence between registers which encode the control states is not required. This generality enables efficient verification of large circuits with data paths structured differently, as well as verification against specifications devoid of structural information. Results are presented for the verification of realistic circuits at different levels in the design hierarchy.< >", "num_citations": "3\n", "authors": ["493"]}
{"title": "Test trade-offs for different dynamic testing techniques for analog and mixed-signal circuits\n", "abstract": " Several methods for testing the dynamic characteristics and the frequency response of analeg and mixed-signal circuits include input excitations consisting of single or multiple sine waves, pulses, pseudo/white noise or normal operating signals. These techniques differ widely in the test measurement time and the data processing time required for the frequency response characterization, as well as in their effectiveness for detecting errors. This paper will provide a comparative study of the different dynamic testing techniques in terms of the measurement and analysis times as well as test effectiveness.", "num_citations": "3\n", "authors": ["493"]}
{"title": "The effect of multiple charge-discharge paths on testing of BiCMOS logic circuits\n", "abstract": " Due to the presence of multiple paths to charge or discharge the output node of a BiCMOS logic gate, many of the realistic open and short faults appear as rise or fall time delay faults without changing the functionality of the circuit. Based on circuit level faults, it has been observed that delay fault tests can produce a fault coverage as high as 92% compared to 29% produced by stuck-at tests, for the same set of faults for a BiCMOS inverter. The implications of this dominant failure mode are discussed and a gate level design-for-testability (DFT) scheme is presented.<>", "num_citations": "3\n", "authors": ["493"]}
{"title": "Training Multi-bit Quantized and Binarized Networks with A Learnable Symmetric Quantizer\n", "abstract": " Quantizing weights and activations of deep neural networks is essential for deploying them in resource-constrained devices, or cloud platforms for at-scale services. While binarization is a special case of quantization, this extreme case often leads to several training difficulties, and necessitates specialized models and training methods. As a result, recent quantization methods do not provide binarization, thus losing the most resource-efficient option, and quantized and binarized networks have been distinct research areas. We examine binarization difficulties in a quantization framework and find that all we need to enable the binary training are a symmetric quantizer, good initialization, and careful hyperparameter selection. These techniques also lead to substantial improvements in multi-bit quantization. We demonstrate our unified quantization framework, denoted as UniQ, on the ImageNet dataset with various\u00a0\u2026", "num_citations": "2\n", "authors": ["493"]}
{"title": "Simulation Study on the Optimization of Photon Energy Delivered to the Prefrontal Cortex in Low-Level-Light Therapy Using Red to Near-Infrared Light\n", "abstract": " Brain functions have been proved to be affected by external stimuli. Low-Level-Light Therapy (LLLT) using near-infrared photons is one of the effective ways to modulate the hemodynamic activities in the brain. However, the biphasic hormetic dose-response where bioenergetics are stimulated at a low dose and inhibited at a high dose is well observed in all photon stimulations. The amount of photon energy delivered to the brain are affected by the wavelength as well as the multilayered head structure with variations of optical parameters (OPs). A real 3D volume head model is built for each participant in this study, and the boundary conditions of each OP in each layer is considered. The Monte Carlo simulation with wavelengths ranging from 650 nm to 1064 nm is implemented to investigate the energy delivered to the brain under different radiation profiles. Results show that 1064-nm photons penetrate deeper than\u00a0\u2026", "num_citations": "2\n", "authors": ["493"]}
{"title": "A multi-band low noise amplifier with strong immunity to interferers\n", "abstract": " A multi-band low noise amplifier (LNA) is designed to operate over a wide range of frequencies (with center frequencies at 1.2, 1.7 and 2.2\u00a0GHz respectively) using an area efficient switchable  network. The LNA can be tuned to different gain and linearity combinations for different band settings. Depending upon the location of the interferers, a specific band can be selected to provide optimum gain and the best signal-to-intermodulation ratio. This is accomplished by the use of an on-chip built-in-self-test circuit. The maximum power gain of the amplifier is 19\u00a0dB with a return loss better than 10\u00a0dB for 7\u00a0mW of power consumption. The noise figure is 3.2\u00a0dB at 1\u00a0GHz and its third-order intercept point () ranges from \u221215 to 0\u00a0dBm. Implemented in a 0.13\u00a0m CMOS technology, the LNA occupies an active area of about 0.29\u00a0mm. This design can be used for cognitive radio and other wideband applications\u00a0\u2026", "num_citations": "2\n", "authors": ["493"]}
{"title": "Design of efficient error resilience in signal processing and control systems: From algorithms to circuits\n", "abstract": " The proliferation of cyber physical systems in society, from the smart grid to sensor networks and robots has raised the importance of error resilience in signal processing and control systems to unprecedented levels. Resilience to errors in sensing and control algorithm execution in processors all the way down to circuits for sensing and actuation is of critical importance in safety-critical applications where undetected errors can have disastrous consequences. In this presentation, we describe how ideas in the domain of algorithm-based fault tolerance developed in the mid-80s for signal processing and matrix computations can be applied to a vast domain of circuits and systems in electrical engineering; from digital and analog filters to complex nonlinear autonomous control systems. The key insight is that electrical systems can be fundamentally represented by linear and nonlinear differential equations with\u00a0\u2026", "num_citations": "2\n", "authors": ["493"]}
{"title": "CLEAR: Cross-Layer Exploration for Architecting Resilience\n", "abstract": " CLEAR is a first of its kind framework which overcomes a major challenge in the design of digital systems that are resilient to reliability failures achieve desired resilience targets at minimal costsenergy, power, execution time, area by combining resilience techniques across various layers of the system stack circuit, logic, architecture, software, algorithm. CLEAR automatically and systematically explores the large space of techniques and their combinations 586 cross-layer combinations in this paper, derives cost-effective solutions, and provides guidelines for designing new techniques. Carefully optimized combinations of circuit-level hardening, logic-level parity checking, and micro-architectural recovery provide highly cost-effective soft error resilience for general-purpose processor cores. 50x silent data corruption rate improvement is achieved at 2.1 energy cost for out-of-order 6.1 for in-order cores, with no speed impact. Selective circuit-level hardening alone, guided by thorough application benchmark analysis, also provides cost-effective solutions 1 additional energy cost for the same 50x improvement.Descriptors:", "num_citations": "2\n", "authors": ["493"]}
{"title": "Cross-layer resilience: are high-level techniques always better?\n", "abstract": " Computers are pervasive in society because advances in integrated circuit (IC) technology have enabled increased performance and reduced costs. In many critical applications, the ICs need to continue to operate correctly in spite of manufacturing defects, as well as failures during operation due to wearout or external disturbances. Although thorough testing of the ICs is part of the manufacturing cycle, some defects may escape the screening; during operation, interconnects may wear out due to electromigration and transistors could degrade (for example, due to negative bias temperature instability (NBTI)). This could result in incorrect results produced by the circuits. Errors can also be produced during operation due to crosstalk, voltage droops (which lead to increased delays in critical paths), single event upsets due to external radiation, etc. Therefore, systems comprising the ICs need to be designed to be resilient\u00a0\u2026", "num_citations": "2\n", "authors": ["493"]}
{"title": "Power-aware multi-voltage custom memory models for enhancing RTL and low power verification\n", "abstract": " We describe a methodology to model the low power and voltage behavior of multi-voltage custom memories in processors. These models facilitate early power-aware verification by abstracting the transistor-level representation of the memory to its power-aware behavioral RTL model. To the best of our knowledge, this is the first attempt at addressing the power-aware RTL model generation problem for custom memories. In our method, we identify voltage crossing points in transistors across channel connected components and use these crossing points to transform the RTL for power-awareness closely matching its circuit implementation. Without the proposed abstraction technique to generate power-aware RTL, low-power verification of such memories will need to be done using transistor-level simulations that are prohibitively time-intensive and hence impractical. We check for correctness of these generated\u00a0\u2026", "num_citations": "2\n", "authors": ["493"]}
{"title": "Digital calibration for 8-bit delay line ADC using harmonic distortion correction\n", "abstract": " Delay line ADCs are becoming increasingly attractive with technology scaling to smaller dimensions with lower voltages. However, linearity, which has always been an issue, becomes a problem with longer delay lines. Resolutions of reported delay-line ADCs are hardly more than 4 bits with sampling rates of hundreds of MHz. In this paper, we first present a technique which extends harmonic distortion correction techniques to digital calibration of a delay-line ADC. In our simulation results, digital calibration improves SNDR and SFDR to 42.5 dB and 45.4 dB, respectively, compared with the original SNDR of 25.6 dB and the original SFDR of 25.7 dB. In order to reduce the convergence time of the calibration, we inject a periodic 3-bit gray code sequence instead of three pseudorandom numbers for harmonic distortion correction to digitally calibrate an 8-bit delay line ADC. In our simulation results, the SNDR\u00a0\u2026", "num_citations": "2\n", "authors": ["493"]}
{"title": "Bitstream-driven built-in characterization for analog and mixed-signal embedded circuits\n", "abstract": " The conventional analog and mixed-signal production testing of system-on-a-chip systems provides limited controllability and observability because automatic test equipment (ATE) access is limited to device under test (DUT) ballouts. More importantly, production mixed-signal testing, due to long test times and expensive ATE, represents a substantial portion of the total manufacturing costs. This brief presents a cost-effective and advanced-signature-based dynamic test method that uses a built-in self-test (BIST) platform that enhances the controllability and observability of mixed-signal embedded systems. The BIST platform includes a simple on-chip signature generator that comprises a clipper and a single-bit comparator. This brief precisely predicts the dynamic nonlinearity of individual mixed-signal circuits connected in a loopback configuration by clipping the loopback-path signal to efficiently give the loopback\u00a0\u2026", "num_citations": "2\n", "authors": ["493"]}
{"title": "Non-speculative double-sampling technique to increase energy-efficiency in a high-performance processor\n", "abstract": " In the past few years, many techniques have been introduced which try to utilize excessive timing margins of a processor. However, these techniques have limitations due to one of the following reasons: first, they are not suitable for high-performance processor designs due to the power and design overhead they impose; second, they are not accurate enough to effectively exploit the timing margins, requiring substantial safety margin to guarantee correct operation of the processor. In this paper, we introduce an alternative, more effective technique that is suitable for high-performance processor designs, in which a processor predicts timing errors in the critical paths and undertakes preventive steps in order to avoid the errors in the event that the timing margins fall below a critical level. This technique allows a processor to exploit timing margins, while only requiring the minimum safety margin. Our simulation results\u00a0\u2026", "num_citations": "2\n", "authors": ["493"]}
{"title": "On-chip source synchronous interface timing test scheme with calibration\n", "abstract": " This paper presents an on-chip test circuit with a high resolution for testing source synchronous interface timing. Instead of a traditional strobe-scanning method, an on-chip delay measurement technique which detects the timing mismatches between data and clock paths is developed. Using a programmable pulse generator, the timing mismatches are detected and converted to pulse widths. To obtain digital test results compatible with low-cost ATE, an Analog-to-Digital Converter (ADC) is used. We propose a novel calibration method for the input range for the ADC using a binary search algorithm. This enables test results to be measured with high resolution using only a 4-bit flash ADC (which keeps the area overhead low). The method achieves a resolution of 21.88 ps in 0.18\u03bc technology. We also present simulation results of the interface timing characterization, including timing margins and timing pass/fail\u00a0\u2026", "num_citations": "2\n", "authors": ["493"]}
{"title": "Off-chip skew measurement and compensation module (SMCM) design for built-off test chip\n", "abstract": " Skew calibration and compensation are critical ATE features for reliable functional test, particularly for applications such as memory chips since most mainstream memories use a source-synchronous interface. This paper presents a new Skew Measurement and Compensation Module (SMCM) design for off-chip skew calibration from Time Domain Reflectometry (TDR) measurements. It consists of coarse and fine parts which enable the circuit to detect a large skew range with high resolution. Circuit complexity is reduced through use of the proposed automatic edge detection method which controls coarse/fine operations. We also present skew compensation circuits which can de-skew off-chip signals based on the skew calibration. The SMCM occupies a small area, making it suitable for implementation in a Built-Off Test (BOT) chip. The circuits were implemented using a 130\u00a0nm technology in a Built-Off Test\u00a0\u2026", "num_citations": "2\n", "authors": ["493"]}
{"title": "High speed recursion-free CORDIC architecture\n", "abstract": " This paper proposes a novel unrolled CORDIC (Co-Ordinate Rotation DIgital Computer) architecture based on parallel operations of a series of micro-rotation stages in the conventional CORDIC. To improve the speed and lower the energy consumption, a Wallace tree reduction is used for the summation of the computed parallel terms. For a large number of micro-rotation stages, a first order approximation is used to reduce the complexity while maintaining the output data accuracy. The circuit has been implemented using a 65 nm process. The results show a speed improvement of 20% and an energy-delay reduction of 27% with a minimal expense of 5% increase in the circuit area relative to a conventional CORDIC architecture.", "num_citations": "2\n", "authors": ["493"]}
{"title": "Calibration-enabled scalable built-in current sensor compatible with very low cost ATE\n", "abstract": " Semiconductor testing, aimed at detecting manufacturing defects and marginalities, should be able to screen out fabrication artifacts that affect immediate as well as future mission-mode device performance. While a large amount of resources are dedicated towards catastrophic fault detection, parametric fault detection is an increasingly important research area. Parametric faults marginally affect device performance and may affect functionality in prolonged field use. In this work, Circuit-under-test (CUT) static bias current is monitored in order to identify catastrophic as well as parametric defects. Any active circuit requires a deterministic amount of DC bias current which may vary outside the specifications when faults exist within the circuit. We propose a process-voltage-temperature (PVT) compensated current measurement built-in-self-test (BIST) scheme, which can be used for sub-system level/circuit-level bias\u00a0\u2026", "num_citations": "2\n", "authors": ["493"]}
{"title": "A delay measurement method using a shrinking clock signal\n", "abstract": " This paper describes a delay measurement method using a shrinking clock signal. The shrinking clock is generated from an AND operation on two clock signals having slightly different periods, which are provided by an external tester. Instead of measuring the number of clocks before it vanishes, another AND operation is utilized to reduce the size of the counter. A differential approach is used to minimize the effect from any non-ideal behavior of circuits used for the measurement as well as to substitute for calibration. In the proposed method, the dynamic range, the measurement resolution and accuracy do not depend on the measurement circuit itself, but on the external clocks from the tester. Circuit-level simulations show good linearity and measurement accuracy regardless of process, voltage, and temperature (PVT) variations when the edge placement accuracy of the external tester amounts to 100ps.", "num_citations": "2\n", "authors": ["493"]}
{"title": "Error detection in 2-D Discrete Wavelet lifting transforms\n", "abstract": " Discrete Wavelet transform is a powerful mathematics technique which is being adopted in different applications including physics, image processing, biomedical signal processing, and communication. Due to its pipelined structure and multirate processing requirements, a single numerical error in one stage can easily affect multiple outputs in final result. In this paper, we propose a weighted checksum code based fault tolerance technique for 2-D discrete wavelet transform. The technique encodes the input array at the 2-D discrete wavelet transform algorithm level, and algorithms are designed to operate on encoded data and produce encoded output data. The proposed encoding technique can perfectly fit into the lifting structure and existing general purpose 2-D discrete wavelet lifting VLSI architectures, without significant modification and overhead. We present the mathematics proof of this coding technique and\u00a0\u2026", "num_citations": "2\n", "authors": ["493"]}
{"title": "Balancing Virtual And Physical Prototyping Across A Multicourse Vlsi/Embedded Systems/Soc Design Curriculum\n", "abstract": " NOTE: The first page of text has been automatically extracted and included below in lieu of an abstract", "num_citations": "2\n", "authors": ["493"]}
{"title": "Low-Complexity Off-Chip Skew Measurement and Compensation Module (SMCM) Design for Built-Off Test Chip\n", "abstract": " Skew calibration and compensation are critical ATE features for reliable functional test, particularly for applications such as memory chips. This paper presents a new time-to-digital converter (TDC) design for off-chip skew calibration from time domain reflectometry (TDR) measurements. It consists of coarse and fine parts which enable the circuit to detect a large skew range with high resolution. Circuit complexity is reduced through use of the proposed automatic edge detection methods which control coarse/fine operations. We also present skew compensation circuits which can de-skew off-chip signals based on the skew calibration. The TDC occupies a small area, making it suitable for implementation in a built-off test (BOT) chip.The circuits were implemented using a 130 nm technology in a built-off test interface (BOTI) developed for 800 Mbps DDR2 memory functional test.", "num_citations": "2\n", "authors": ["493"]}
{"title": "Distributed computing grids\u2014Safety and security\n", "abstract": " 316Harnessing idle CPU cycles from PCs across a network (Internet or intranet) has proven to be an economically attractive solution for solving many problems. Research has shown that the average idle time for most PCs is over 90%, representing a virtually limitless source of untapped computing power. As such, distributed computing grids 1 have become an increasingly popular form of grid computing in research communities as well as in industry.", "num_citations": "2\n", "authors": ["493"]}
{"title": "Design of shifting and permutation units using LSDL circuit family\n", "abstract": " Migration of designs into a smaller technology node, that traditionally resulted in an increase in performance, is yielding reduced returns as we scale into the sub-90 nm domain. This has made it imperative to explore alternative methods like improvements in circuit design to sustain growth in performance of Integrated Circuits. The Limited switching dynamic logic (LSDL) circuit family has been suggested as an efficient and high performance circuit design technique to overcome the problem of stagnating performance. In this paper, we present case studies in design of arithmetic units using LSDL selector circuits. The first unit we present is a shifter, with the added novelty that it can provide the shifted data in complemented or non-complemented form without requiring an additional module for performing the negate operation. The second unit is a permute unit, used to line up data in media units using a single\u00a0\u2026", "num_citations": "2\n", "authors": ["493"]}
{"title": "XML query algebra operators, and strategies for their implementation\n", "abstract": " We propose an improvement to the logical view of the selection operator in Niagara Algebra [L. Galanis et al. (2002), S. D. Viglas et al. (2002)]. We demonstrate the improvement due to our implementation by experimental results. We observe that the degree of improvement depends on the nature of the XML data. The new operator is particularly better at working with data that has a large number of elements that need to be unnested in order to run a select In addition, it shows a significant improvement in situations where a large number of elements must be evaluated to check if the selection criteria are met.", "num_citations": "2\n", "authors": ["493"]}
{"title": "Towards the complete elimination of gate/switch level simulations\n", "abstract": " This paper presents the reasoning behind eliminating full-chip gate/switch-level simulations for microprocessors/digital system designs and utilizing RTL models for the purpose, provided formal boolean equivalence between RTL and gate/switch-level models have been established using symbolic simulation for all blocks that comprise the chip. No logic bug should go undetected if only RTL models are used for full chip simulations provided existing design methodologies are enhanced to incorporate a constraints checking flow coupled with a rigorous circuit metastability/contention prevention flow.", "num_citations": "2\n", "authors": ["493"]}
{"title": "Functional Verification Environment for Object-oriented Hardware Designs.\n", "abstract": " The rapid increase in the complexity of modern ASICs raises the need for an increase in the abstraction level used to design these chips. The Odette project has introduced the SystemC Plus design methodology, based on the use of object-oriented techniques for hardware modeling and design. This paper describes the Odette verification environment. The verification environment is designed to take advantage of the opportunities made available by object-oriented design methods, and to address the needs these methods create. It is built of a set of base classes that provide local verification services to design classes, and a verification manager that connects between these classes and external verification tools. To illustrate the use of the verification environment, we provide two examples of tools that use the verification environment for checking and test generation.", "num_citations": "2\n", "authors": ["493"]}
{"title": "Automatic Verification of Arithmetic Circuits in RTL using term rewriting systems\n", "abstract": " This paper presents a novel technique for proving the correctness of arithmetic circuit designs described at the Register Transfer Level (RTL). The technique begins with the automatic transla-tion of circuits from a Verilog RTL description into a Term Rewriting System (TRS). We prove the correctness of the designs via an equivalence proof between TRSs for the implementation cir-cuit design and a much simpler specification circuit design. We present this notion of equivalence between the TRSs and a stepwise refinement method for its decomposition which we leverage in our tool Verifire. We demonstrate the effectiveness of our technique by using the tool for the ver-ification of several multiplier designs which have hitherto been impossible to verify with existing approaches and tools.", "num_citations": "2\n", "authors": ["493"]}
{"title": "Massively parallel/reconfigurable emulation model for the d-algorithm\n", "abstract": " In this paper, we propose an approach to test generation based on reconfigurable devices, emulators, and Field Programmable Gate Arrays (FPGA). This approach is based on automatically designing a circuit which implements the D-algorithm specialized for the circuit under test. This approach exploits fine-grain parallelism in the forward/ backward implications, and conflict checking. In this paper, we show an implementation with a lower hardware overhead than previous approaches making this technique more attractive.", "num_citations": "2\n", "authors": ["493"]}
{"title": "The myth of fault tolerance in complex systems\n", "abstract": " The Myth of Fault Tolerance in Complex Systems IEEE.org Help Cart Jobs Board Create Account Toggle navigation IEEE Computer Society Digital Library Jobs Tech News Resource Center Press Room Browse By Date Advertising About Us IEEE IEEE Computer Society IEEE Computer Society Digital Library My Subscriptions Magazines Journals Conference Proceedings Institutional Subscriptions IEEE IEEE Computer Society More Jobs Tech News Resource Center Press Room Browse By Date Advertising About Us Cart All Advanced Search Conference Cover Image 1.Home 2.Proceedings 3.prdc 1999 The Myth of Fault Tolerance in Complex Systems 1999, pp. , DOI Bookmark: UNKNOWN Keywords Authors J. Abraham About Us Advertising Resource Center Jobs Board Corporate Programs Help Contact Press Room Browse By Date Privacy Accessibility Statement IEEE Nondiscrimination Policy \u00a9 2020 IEEE \u2013 . \u2026", "num_citations": "2\n", "authors": ["493"]}
{"title": "FzCRITIC-A Functional timing verifier using a novel fuzzy delay model\n", "abstract": " Chip performance and density are increasing tremendously and the CAD tools are always lagging behind. In this paper, we introduce a functional timing verifier using a novel fuzzy delay model which bridges the gap between the front-end timing verification and the back-end delay fault testing. The proposed fuzzy delay model can handle uncertainties with respect to timing characteristics, and manufacturing anomalies. Experimental results are presented for the ISCAS-85 benchmark circuits.", "num_citations": "2\n", "authors": ["493"]}
{"title": "A D&T roundtable: on-line test\n", "abstract": " Online testing has been a known field for sometime now. However, to- day\u2019s situation with online testing may be a little bit different than it was in the past. With the nanometer technologies, we may see more soft errors that cause problems. The goal of this roundtable is to analyze where on- line test was and where future technologies are going. D&T thanks participants Jacob A. Abraham (Univ. of Texas at Austin), Miron Abramovici (Bell Labs), Silvano Motto (CAEN Microeletronica), Isaac Levendel (Motorola), and Michael Nicolaidis (TIMA). Our moderator was Yervant Zorian (LogicVision). Also attending were Lorena Anghel (TIMA) and Fabian Vargas (Puers, Brazil).  D&T Roundtable Editor Kaushik Roy (Purdue Univ.) organized the event. The roundtable was held at the Fourth IEEE International On-Line Testing Workshop in July 1998.  D&T thanks the workshop for hosting it.", "num_citations": "2\n", "authors": ["493"]}
{"title": "On more efficient combinational ATPG using functional learning\n", "abstract": " Learning techniques like SOCRATES and recursive learning have greatly enhanced the technology of FAN-based ATPG. In this paper we present a test generation methodology for combinational circuits using functional learning, discuss application of novel functional information to enhance ATPG and present ATPG results on ISCAS 85 benchmark circuits. The test generation methodology combines the use of structural (topology) based analysis methods with the function representation techniques (such as BDDs).", "num_citations": "2\n", "authors": ["493"]}
{"title": "Selective pseudo scan: combinational ATPG with reduced scan in a full custom RISC microprocessor\n", "abstract": " This paper presents a novel test generation technique, called Selective Pseudo Scan (SPS), which incurs very nil SGNALS low overhead. SPS uses a commercial combinational ATPG tool to generate tests with high fault coverage by reconfiguring sequential circuits to appear combinational without inserting ml scan. Results of applying SPS to several complex control blocks of a fult custom RISC Microprocessor, demonstrate its superiority compared to traditional futt scan or partial scan in a futt custom design environment. Pnz SGNALS", "num_citations": "2\n", "authors": ["493"]}
{"title": "Control Flow Checking In Object-Based Distributed Systems\n", "abstract": " Object-based distributed systems are becoming increasingly popular since objects provide a secure and easy means of using the abstraction of shared memory. In this paper we develop a new object-based control flow checking technique called (COTM) to detect errors due to hardware faults in such systems. The proposed technique monitors objects and thread flow across objects in two stages. The first stage applies control flow checking for every object invocation. In the second stage, the legality of a terminating thread is examined. Results of fault injection experiments on several applications written in C++ and modified to incorporate the object-based checks show that the proposed technique achieves high fault coverage with low performance overhead.", "num_citations": "2\n", "authors": ["493"]}
{"title": "A Reduced Cost Fault Simulation Strategy for the Am29050 Microprocessor\n", "abstract": " The Am29050 microprocessor is ihe latest addition to the Am29000\u2122 microprocessor family of high performance RISC microprocessors for embedded applications. At 428K transistors it has over 1.3 Million stuck at faults which have to be tested [l] and fault simulated [2] to achieve a very high fault coverage. Due to the sheer magnitude of the task it was important to minimize test generation cost and detect as many of these faults as possible during Phase I Test Generation. An extensive functional test suite which covers all areas of the chip was available but these tests had to be fault simulated and compacted before they could be used. Fault simulating every one of these test patterns against all 1.3 Million faults is an impossible task and would take several CPU years even on the very fast hardware accelerators we use, so a more efficient fault simulation technique was required. Initial studies indicated that the bulk of the fault simulation time was being spent repeatedly simulating undetected faults. For every test pattern simulating only the\" right\" set of faults would eliminate much of the wasted effort without loss of fault coverage. There are a few papers on practical fault simulation experiences which allude to fault selection [3]. But they do not give enough information or supporting data to show the relative tradeoffs and performance improvements. In this paper we explain the fault selection technique and present the Am29050 microprocessor fault simulation strategy which is built around this technique. In the\" Fault Selection\" technique, for every test pattern we select only those faults which have a fairly good chance of being detected. A cost function\u00a0\u2026", "num_citations": "2\n", "authors": ["493"]}
{"title": "Performance analysis of numerical problems on a loosely coupled system\n", "abstract": " Distributed computer systems consisting of a collection of workstations and mainframe machines connected through a local area network are becoming a standard way of providing com puter service. Such systems provide resource sharing, good performance, highly interactive user interfaces, and possibly improved reliability over centralized systems. Stankovic [1] gives an excel lent survey of distributed computer systems.Good performance in distributed computer systems results from resource dedication and/or load balancing. Resource dedication calls for reserving a resource or set of resources for the exclusive use of one task Cor job) in order to minimize its turnaround time. Load balancing, on the other hand, consists of moving tasks or subtasks from a loaded resource to unused or slightly loaded resource in order to maximize the throughput of the whole system. Note that in the case of a single job and an idle system, resource dedication and load balancing are identical. In this report we consider the performance of a single job distributed on a cluster of worksta tions dedicated to that job. Our work is motivated by the abundance of computation-intensive jobs and the availability of workstations. Dedicating otherwise idle (at night time for example) works tations to a computation intensive job may offer a low cost solution to expensive problems. Performance of distributed systems may be obtained through one of three methods: modeling, simulation, and direct measurement [2]. These methods provide a range of flexibility and accuracy. In general, modeling is the least accurate since it requires many simplifying assumptions in order to make the\u00a0\u2026", "num_citations": "2\n", "authors": ["493"]}
{"title": "Multiple Processor Systems\n", "abstract": " An important consideration in the design of high- performance multiple processor systems should be in ensuring the correctness of results computed by such complex systems which are extremely prone to transient and intermittent failures. The detection and location of faults and errors concurrently with normal system operation can be achieved through the application of appropriate on-line checks on the results of the computations. This is the domain of algorithm-based fault tolerance, which deals with low-cost system-level fault-tolerance techniques to produce reliable computations in multiple processor systems, by tailoring the fault-tolerance techniques toward specific algorithms. This paper presents a graph-theoretic model for determining upper and lower bounds on the number of checks needed for achieving concurrent fault detection and location. The objective is to estimate ate the overhead in time and the\u00a0\u2026", "num_citations": "2\n", "authors": ["493"]}
{"title": "Functional test sequences for inducing voltage droops in a multi-threaded processor\n", "abstract": " Precisely controlled power delivery is critical for high performance systems-on-chip. This work describes functional test sequences to induce large dynamic and static supply voltage droops impacting the minimum operating voltage (V MIN ) of the processor. An algorithm is provided to generate high and low power sequences that are functions targeting a wide range of power delivery network (PDN) frequencies. The voltagedroop tests induce large dynamic voltage droops by aligning hardware threads using a method that generates relatively prime sequence lengths across threads in a multi-threaded processor system. Functional tests also create symmetric and asymmetric high and low power sequences, introducing delay in processor pipeline stages. Additionally, tests consisting of a loop of sustained high-power sequences are also generated causing static droops. Simulation and silicon results of voltage-droop tests\u00a0\u2026", "num_citations": "1\n", "authors": ["493"]}
{"title": "A Broadband CMOS RF Front End for Direct Sampling Satellite Receivers\n", "abstract": " This paper presents a comparative analysis between two new architectures for RF programmable-gain amplifiers (RFPGAs): voltage-mode RFPGA-V and current-mode RFPGA-I. RFPGA-V utilizes multiple-switch-multiple-amplifier configuration and gain interpolation method to achieve a fine gain step of 0.25-dB over 42-dB gain range for the band of 250 MHz to 2.3 GHz. Meanwhile, RFPGA-I uses a current steering approach to achieve a fine gain step of 0.25-dB over 42-dB gain range for an even wider band of 250 MHz to 3.4 GHz. Since the active feedback topology is used, no off-chip inductor is needed in either RFPGA, especially for the low-frequency band. In addition, both RFPGA-V and RFPGA-I are able to handle maximum 4.4 V peak-to-peak input signal without compromising their high operating bandwidth. Two broadband RF front ends for direct sampling receivers, which include either RFPGA-V or\u00a0\u2026", "num_citations": "1\n", "authors": ["493"]}
{"title": "FSNoC: Safe Network-on-Chip Design with Packet Level Lock Stepping\n", "abstract": " Functional safety is the top priority for the design of automotive and other mission-critical systems. We proposed Functional Safe NoC (FSNoC) with a new Packet Level Lock Stepping (PLLS) concept for Concurrent Error Detection (CED) of Network-on-Chip (NoC) with high Diagnostic Coverage (DC) and reduced area overhead. Furthermore, we proposed to divide the NoC network of a System-On-Chip(SOC) design into partitions with different performance requirements and apply separate but inter-operable safety mechanisms based on Performance Power Area (PPA) trade-off given the design meet safety requirement. The proposed techniques were used on an industry NoC design to achieve over 99% DC coverage with 11-33% of area, 12-29% power overhead and 5-22% of wiring overhead depending on partition choices.", "num_citations": "1\n", "authors": ["493"]}
{"title": "Spectral leakage-driven loopback scheme for prediction of mixed-signal circuit specifications\n", "abstract": " The rising cost of production testing for a system-on-a-chip (SoC) is one of the crucial matters to chip makers, due to long test time and costly automated-test-equipment. This paper proposes a spectral leakage-driven built-in self-test (BIST) scheme to precisely predict the nonlinearity of mixed-signal circuits in the loopback mode, thereby accomplishing cost-effectiveness (compared to previous BIST-based works). A digitally synthesized single-tone sinusoidal stimulus used for conventional harmonic testing is incoherently sampled by a device under test (DUT). The DUT output signal exhibits the correlation between the DUT harmonics and the spectral leakage introduced by the incoherent sampling. The DUT output signal is then fed to another DUT through a loopback path, so that the harmonics of a pair of DUTs are correlated with the spectral leakage on the loopback response; the magnitude of the spectral leakage\u00a0\u2026", "num_citations": "1\n", "authors": ["493"]}
{"title": "Tutorial T2A: Safe Autonomous Systems: Real-Time Error Detection and Correction in Safety-Critical Signal Processing and Control Algorithms\n", "abstract": " While the last two decades have seen revolutions in computing and communications systems, the next few decades will see a revolution in the use of every-day robotics and artificial intelligence in broad societal applications. Examples of such systems include sensor networks, the smart power grid, self-driven cars and autonomous drones. Such systems are driven by signal processing, control and learning algorithms that process sensor data, actuate control functions and learn about the environment in which these systems operate. The trustworthiness and safety of such systems is of paramount importance and has significant impact on the commercial viability of the underlying technology. As a consequence, anomalies in system operation due to computation errors in on-board processors, degradation and failure of embedded sensors, actuators and electro-mechanical subsystems and unforeseen changes in their\u00a0\u2026", "num_citations": "1\n", "authors": ["493"]}
{"title": "Quality aware error detection in 2-D separable linear transformation\n", "abstract": " In this paper, we propose a generic weighted checksum code based quality aware error detection scheme for 2-D separable linear transformation. These key components are widely used in multimedia compression systems, e.g., video and image codecs. The technique encodes the input array at the 2-D linear transformation level, and algorithms are designed to operate on encoded data and produce encoded output data. The proposed error detection technique is a system-level method therefore can be used in existing hardware or software 2-D linear transformation architectures with low overhead. More importantly, the proposed weighted checksum code based error detection can enable quality metric aware error detection which only flags an error when the quality of the result is lower than expectation. The error tolerance detection technique can fit perfectly in multimedia compression systems.", "num_citations": "1\n", "authors": ["493"]}
{"title": "Heart transplantation: the path ahead\n", "abstract": " Forty-nine years ago the first human-to-human heart transplant was performed in South Africa by Christiaan Barnard in December 1967. 1 This was the result of the pioneering work done by Norman Shumway, Alexis Carrel and others for many years. 2 Profulla Kumar Sen at the KEM hospital, Mumbai in 1968, performed the first human to human heart transplant in India. The brain stem death law was not established at that time so they had to wait till the donor heart stopped beating. The donor heart was excised and implanted to the recipient with a biatrial method. But the heart failed to eject enough to sustain the recipient who was suffering from end stage congestive cardiac failure. Nevertheless history was made. Due to nonavailability of immunosuppressive agents, the transplant program plummeted all over the world during 1970s. But with the availability of novel agents especially cyclosporine in 1980s a\u00a0\u2026", "num_citations": "1\n", "authors": ["493"]}
{"title": "Dynamic performance characterization of embedded single-ended mixed-signal circuits\n", "abstract": " The inherent fault-masking characteristic of the traditional loopback test produces overly pessimistic estimates of device-under-test (DUT) performance, which negatively impacts product yield, although the loopback test provides a promising low-cost test solution. The proposed method overcomes the fault-masking shortcomings of the loopback test for single-ended mixed-signal circuits by accurately characterizing the performance parameters of the individual DUTs, based on the imbalance generation technique of our previous work. Basically, the proposed method is the case study of the previous work to validate the expansibility to a single-ended circuit testing. A sinusoidal stimulus is applied to the DUT, and our loopback configuration converts the external single-ended loopback path to differential mode, in order to generate the phase imbalance between the differential pair. The imbalanced differential signals\u00a0\u2026", "num_citations": "1\n", "authors": ["493"]}
{"title": "Harmonic distortion correction for 8-bit delay line ADC using gray code\n", "abstract": " Harmonic distortion correction (HDC) is an effective digital calibration technique to estimate and correct errors and distortions in an analog circuit. However, the convergence time is still a concern. In this paper, we propose the injection of a periodic 3-bit gray code sequence for HDC to digitally calibrate an 8-bit delay line ADC. In our simulation results, digital calibration with the gray code injection improves SNDR and SFDR to 42.5 dB and 45.4 dB, respectively, compared with the original SNDR of 25.6 dB and the original SFDR of 25.7 dB, with a 13.5 milliseconds calibration time, which is 64X faster than with injection of pseudorandom numbers (860 milliseconds). Also the SNDR converges to 41.6 dB after averaging 2 24  samples, while the SNDR with injection of pseudorandom numbers converges to 41.5 dB after 2 37  samples.", "num_citations": "1\n", "authors": ["493"]}
{"title": "Asynchronous Measurement of Transient Phase-Shift Resulting From RF Receiver State-Change\n", "abstract": " In a wireless receiver, a down-converted RF signal undergoes a transient phase shift, when the gain state is changed to adjust for varying conditions in transmission and propagation. A method is developed, in which such phase shifts are detected asynchronously, and their undesirable effects on the bit error rate are corrected. The method was developed for and used in, the system-level characterization and calibration of a 65-nm CMOS UHF receiver. The phase-shifts associated with specific gain-state transitions were measured within a test framework, and used in the baseband signal processing blocks to compensate for errors, whenever the receiver anticipated a gain-state transition.", "num_citations": "1\n", "authors": ["493"]}
{"title": "On a rewriting strategy for dynamically managing power constraints and power dissipation in socs\n", "abstract": " We present a novel and highly automated technique for dynamic system level power management of System-on-a-Chip (SoC) designs. We present a formal system to represent power constraints and power intent as rules. We also present a Term Rewriting Systems based rule rewriting engine as our dynamic power manager. We provide a notion of formal correctness of our rule engine execution and provide a robust algorithm to dynamically and automatically manage power consumption in large SoC designs. There are two fundamental building blocks at the core of our technique. First, we present a powerful formal system to capture power constraints and power intent as rules. This is a self-checking system and will automatically flag conflicting constraints or rules. Next, we present a rewriting strategy for managing power constraint rules using a formal deductive logic technique specially honed for dynamic power\u00a0\u2026", "num_citations": "1\n", "authors": ["493"]}
{"title": "Calibration Enabled Scalable Current Sensor Module for Quiescent Current Testing\n", "abstract": " Semiconductor testing is aimed at screening fabrication defects that impact expected functionality. While catastrophic defects result in non working devices, parametric faults result in marginalities and are of increasing concern with deep sub-micron process technologies. This work presents a scheme to monitor Circuit-Under-Test (CUT) static bias current to identify catastrophic as well as parametric faults. All circuits require a deterministic amount of DC bias current which may vary outside the specifications when faults exist within the circuit. We propose a compensated current measurement Built-in-Current-Sensor (BICS) scheme, which can be used for sub-system level/circuit-level bias current measurements. The BICS provides accessibility to internal blocks and enables isolated parametric testing. Calibration routine enables process independence and provides robustness. The BICS is compatible with\u00a0\u2026", "num_citations": "1\n", "authors": ["493"]}
{"title": "Automatic and correct register transfer level annotations for low power microprocessor design\n", "abstract": " We propose instruction-driven slicing, a new technique for annotating microprocessor descriptions at the Register Transfer Level (RTL) in order to achieve lower power dissipation. Our technique automatically annotates existing RTL code to optimize the circuit for lowering power dissipated by switching activity. Our technique can be applied at the architectural level as well, achieving similar power gains. We first demonstrate our technique on architectural and RTL models of a 32-bit OpenRISC pipelined processor (OR1200), showing power gains for the SPEC2000 benchmarks. These annotations achieve reduction in power dissipation by changing the logic of the design. We give a proof that the annotations on the OR1200 processor preserve the original functionality of the machine using the ACL2 theorem prover. We then further extend our technique to an outof-order superscalar core and demonstrate power\u00a0\u2026", "num_citations": "1\n", "authors": ["493"]}
{"title": "Indirect method for random jitter measurement on SoCs using critical path characterization\n", "abstract": " This paper presents a new method for random jitter measurement on systems-on-a-chip (SoCs) by exploiting shmoo plotting in automatic test equipment (ATE). After finding the maximum operating frequency of a microprocessor using functional test patterns that can sensitize its critical paths, the proposed method constructs a cumulative distribution function (CDF) whose standard deviation represents the root mean square (RMS) value of the random jitter of the clock signals used in the microprocessor. By leveraging tester period resolution with a frequency multiplying phase-locked loop (PLL) in the SoC, the shmoo plot with a fine period step size can detect the jitter component in the clock signal, which reflects the actual jitter that most critical paths undergo. The proposed idea was verified with circuit-level simulations, and was validated by silicon measurements using one of the latest SoC products.", "num_citations": "1\n", "authors": ["493"]}
{"title": "Testing and Fault Diagnosis of Time-Interleaved S? Modulators Using Checksums\n", "abstract": " Mismatch of components in a time-interleaved ADC (TIADC) is a major problem which can significantly degrade the performance, even with a 0.5% mismatch. This paper describes a new technique which uses checksums for diagnosing the mismatch of components among sub-ADCs in a TIADC. In our checksum formulation, a transition matrix is used to represent the transition relationship between the current state and the next state of modulators, while another matrix is used to represent the states of a TIADCs, where the state of a sub-ADC is presented in a column vector. We have applied the checksum technique to a TIADC which contains two 1-bit fifth-order \u03a3\u0394 modulators with matched resistances and capacitances, and demonstrated that the checksums successfully detect and locate mismatches.", "num_citations": "1\n", "authors": ["493"]}
{"title": "Manufacturing test of systems-on-a-chip (SoCs)\n", "abstract": " Summary form only given. Testing chips after manufacture, unlike producing transistors on a chip, does not enjoy the scaling offered by Moore's law. This tutorial will outline the increasing difficulties with manufacturing test and discuss approaches to manage the complexity of testing SoCs, including generation and design-for-test techniques for classic \u201cstuck-at\u201d faults as well as small delay defects which are becoming more common in scaled technologies. Issues with testing embedded analog, mixed-signal and RF modules will be addressed. Test approaches which use the computational resources within a (SoC) to test itself will also be discussed. The embedded processor in the SoC can test itself by running instruction sequences from memory. The processor can be used to test other cores in the SoC, including mixed-signal cores for analog and RF specifications, with the help of design-for-test structures such as\u00a0\u2026", "num_citations": "1\n", "authors": ["493"]}
{"title": "Robust power gating reactivation by dynamic wakeup sequence throttling\n", "abstract": " The wakeup sequence for power gating techniques has become an important issue as the rush current typically causes a high voltage drop. This paper proposes a new wakeup scheme utilizing an on-chip detector which continuously monitors the power supply noise in real time. Therefore, this scheme is able to dynamically throttle the wakeup sequence according to ambient voltage level. As a result, even the adjacent active circuit blocks induce an unexpectedly high voltage drop, the possibility of the occurrence of excessive voltage drop is reduced significantly.", "num_citations": "1\n", "authors": ["493"]}
{"title": "Estimation of maximum application-level power supply noise\n", "abstract": " This paper presents a systematic technique that relates the instructions at the application-level to the cycle-average supply noise. Cost metrics affecting supply noise are maximized and the corresponding activity event is mapped to instructions. We performed experiments on an open-source processor and were able to obtain a higher voltage drop (>;20%) when compared to that of random simulation in a significantly less amount of time (96% reduction).", "num_citations": "1\n", "authors": ["493"]}
{"title": "Performing analog-to-digital conversion by computing delay time between traveling waves in transmission lines\n", "abstract": " A method and device for converting an analog input electrical signal to a digital signal. A plurality of integrated active and/or passive transmission lines may be implemented with signal-dependant propagation velocities. The delay differences of pulses traveling through these transmission lines are compared, and the collective results are used to evaluate and subsequently quantize the input signal.", "num_citations": "1\n", "authors": ["493"]}
{"title": "SNR-Aware Error Detection for Low-Power Discrete Wavelet Lifting Transform in JPEG 2000\n", "abstract": " This paper presents a SNR-aware error detection technique for a low-power wavelet lifting transform architecture in JPEG 2000. Power reduction is done by over-scaling the supply voltage (voltage-over-scaling (VOS)). A low-cost SNR-aware detection logic is integrated into the discrete wavelet lifting transform architecture, to check if the image quality degradation caused by the resulting timing errors is acceptable, in order to determine the optimal voltage setting in operating condition at run time. The technique behind the SNR-aware detection logic is the weighted checksum code. It is shown that image quality measured in SNR can be correlated with the image checksum difference. If the image checksum difference is above a certain threshold, the SNR of the image will be below the minimal requirement and image quality will be unacceptable. This novel quality-based error detection is significantly different from\u00a0\u2026", "num_citations": "1\n", "authors": ["493"]}
{"title": "An examination of urban public transportation equity in San Antonio, Texas\n", "abstract": " Copyright by Tricia Ann Barrow 2009 Page 1 Copyright by Tricia Ann Barrow 2009 Page 2 An Examination of Urban Public Transportation Equity in San Antonio, Texas by Tricia Ann Barrow, BA Report Presented to the Faculty of the Graduate School of The University of Texas at Austin in Partial Fulfillment of the Requirements for the Degree of Master of Science in Community and Regional Planning The University of Texas at Austin August 2009 Page 3 An Examination of Urban Public Transportation Equity in San Antonio, Texas Approved by Supervising Committee: Talia McCray Michael Oden Page 4 Dedication To my mother, Margaret Shepherd, who taught me the joys and pains of riding public transit. Page 5 v Acknowledgements I extend my gratitude to my advisor, Talia McCray, for her guidance, support, and patience. I would also like to thank Michael Oden for his valuable comments and suggestions. It has an I'\u2026", "num_citations": "1\n", "authors": ["493"]}
{"title": "Controllability of static CMOS circuits for timing characterization\n", "abstract": " Timing violations, also known as delay faults, are a major source of defective silicon in modern Integrated Circuits (ICs), designed in Deep Sub-micron (DSM) technologies, making it imperative to perform delay fault testing in these ICs. However, DSM ICs, also suffer from limited controllability and observability, which impedes easy and efficient testing for such ICs. In this paper, we present a novel Design for Testability (DFT) scheme to enhance controllability for delay fault testing. Existing DFT techniques for delay fault testing either have very high overhead, or increase the complexity of test generation significantly. The DFT technique presented in this paper, exploits the characteristics of CMOS circuit family and reduces the problem of delay fault testing of scan based sequential static CMOS circuits to delay fault testing of combinational circuits with complete access to all inputs. The scheme has low overhead\u00a0\u2026", "num_citations": "1\n", "authors": ["493"]}
{"title": "A 128dB dynamic range 1kHz bandwidth stereo ADC with 114dB THD\n", "abstract": " A high performance, single die stereo delta sigma ADC is designed for high precision measurement applications. A single loop, fifth-order, thirty-three level delta-sigma analog modulator with positive and negative feedforward path is implemented. An interpolated multilevel quantizer with unevenly weighted quantization levels replaces a conventional 5-bit flash type quantizer in this design. These new techniques suppress signal dependent energy inside the delta sigma loop, reduce internal channel coupling and power consumption. Integrated with an on-chip bandgap reference circuit and decimation filter, the ADC achieves 128dB dynamic range, \u2212114dB THD over 1kHz bandwidth. Power consumption is less than 140mW per channel.", "num_citations": "1\n", "authors": ["493"]}
{"title": "Morgagni\u2019s hernia, repair during coronary bypass surgery\n", "abstract": " Morgagni\u2019s hernia is a rare type of diaphragmatic hernia. We have operated on an unstable coronary disease patient with associated central diaphragmatic hernia which was only a presumptive diagnosis before surgery. Emergency Offpump Coronary bypass was done along with mesh repair of the hernia. Patient made an uneventful recovery.", "num_citations": "1\n", "authors": ["493"]}
{"title": "Reducing verification overhead with RTL slicing\n", "abstract": " Design complexity is increasing with every technology generation, causing verification tools to require large amounts of resources. In this paper, we develop a technique to reduce the complexity of verifying digital designs described in a Hardware Description Language (HDL). For a given property to be verified, we derive an HDL executable design slice that is behaviorally equivalent to the original design. The slice is less complex than the original design and requires fewer resources for analysis by a verification tool. The slicer is implemented as a pre-processor to SMV, a SAT-based verification tool, and Formal, an ATPG-based verification tool. Experimental results on the USB2. 0 IP core show that RTL slicing reduces both CPU and memory overhead for both SMV and Formal. This reduction allows the verification tools to effectively deal with complex designs.", "num_citations": "1\n", "authors": ["493"]}
{"title": "Built-in test of RF receivers using RF amplitude detectors\n", "abstract": " This paper describes a low cost, high resolution, built-in test technique for RF receivers which uses simple RF amplitude detectors. The method has been used to predict the performance parameters of a 940 MHz RF receiver frontend with a mixer and LNA. The detector has small area overhead with very low frequency output, which can be sampled at 10MHz. The sampled output waveform is analyzed using an FFT, and the low frequency measurements are used to predict the conversion gain and Third Order Intercept point (TOI, IIP3) of the receiver. By using two detectors, both of the system performance and discrete components\u2019 specification can be accurately measured. We have completed the design and layout of the 940 MHz test chip in the United Micro Electronics Corporation (UMC) 0.18 \u00b5m CMOS process. Post layout simulation results show accurate prediction of the system specifications.", "num_citations": "1\n", "authors": ["493"]}
{"title": "Taming the complexity of ste-based design verification using program slicing\n", "abstract": " This paper presents the development of a hierarchical methodology for speeding-up a symbolic trajectory evaluation (STE) based verification flow, using \"program slicing\" techniques. An overview of the proposed methodology is described, along with the details of a prototype tool that has been developed to automate the approach. The tool, called FACTOR, has been successfully applied to reduce the size of the RTL implementation of a floating-point unit in an Intelreg Pentiumreg 4 processor model needed for formally verifying an embedded module. The existing proof specification for the module was validated using Forte, Intel's STE tool, on both the original and the reduced model. A comparison of the results demonstrates the effectiveness of the methodology, and its immense potential to scale up the verification flow to larger design sizes", "num_citations": "1\n", "authors": ["493"]}
{"title": "An emulation model for sequential ATPG-based bounded model checking\n", "abstract": " Bounded model checking based on sequential ATPG (automatic test pattern generation) is virtually the sequential ATPG state-justification phase. The state-justification phase is a very complicated and expensive process in term of CPU time. Previous work to speed the search concentrated on developing heuristics to achieve speed-up. In this paper we develop a novel architecture to emulate the state-justification on reconfigurable hardware. The feature of fine-grain massive parallelism of reconfigurable hardware is exploited to achieve speed-up.", "num_citations": "1\n", "authors": ["493"]}
{"title": "RTL Annotations for Low Power Microprocessor Design\n", "abstract": " We propose instruction-driven slicing, a new technique for annotating microprocessor descriptions at the Register Transfer Level (RTL) in order to achieve lower power dissipation. Our technique automatically annotates existing RTL code to optimize the circuit for lowering power dissipated by switching activity. Our technique (in fact, the very same annotations) can be applied at the architectural level as well, achieving similar power gains. We demonstrate our technique on architectural and RTL models of PUMA, a fixed point PowerPC microprocessor, showing power gains for the SPEC2000 benchmarks.", "num_citations": "1\n", "authors": ["493"]}
{"title": "Pipeline wall inspection apparatus\n", "abstract": " Apparatus to inspect sections of a pipeline wall at the site where the pipeline is located, comprising a vehicle in which to install and transport such apparatus to the pipeline site, a radiographic camera operably installed in said vehicle, a computer with monitor installed in said vehicle, a digital scanner installed in said vehicle, flexible phosphor sheets carried in said vehicle on which to impinge radiographic images respective sections of said pipeline wall that are to be inspected, a flexible carrier to receive a respective one of said phosphor sheets and to mount adjacent a said section of said pipeline wall that is to be inspected and on which radiographic rays from said radiographic camera are impinged after passing through said pipeline wall to record on said phosphor sheet the characteristics of said section of pipeline wall, such flexible phosphor sheet with said radiographic images thereon to be run through said\u00a0\u2026", "num_citations": "1\n", "authors": ["493"]}
{"title": "Test generation for realistic defects\n", "abstract": " The rapidly evolving process technologies and device complexity that have fueled the exponential growth in the performance of microprocessors have made the manufacturing test of these devices a hard problem. In addition to making the detection of defects modeled by the classical fault models like the stuck-at and the transition fault model more complex, these process technologies have resulted in additional types of defects (like the resistive opens, defects due to the process parameter variations and crosstalk defects) becoming more prominent.", "num_citations": "1\n", "authors": ["493"]}
{"title": "Formal verification: Current use and future perspectives\n", "abstract": " Nowadays, designs can contain up to several hundred million transistors. Moreover; up to 80% of the overall design costs are due to verification. Formal verification techniques provide an attractive alternative by proving the circuits',correct behavior. These techniques-for example, equivalence checking and property checking-are extensively used in many industrial flows. Difficulties with formal verification techniques include the definition and use of appropriate languages, formulation of properties, and lack of motivation of designers to use the new approaches. IEEE Design & Test thanks roundtable participants Jacob A. Abraham (University of Texas at Austin), Andrew Betts (Qualis Europe), Hans Eveking (Darmstadt University of Technology), Harry D. Foster (Verplex Systems), Thomas Kropf (Bosch), Matthew J. Morley (Verisity Design), Thomas R. Shiple (Synopsys), and Michael Siegel (Infineon Technologies). D\u00a0\u2026", "num_citations": "1\n", "authors": ["493"]}
{"title": "On design validation using verification technology\n", "abstract": " Despite great advances in the area of Formal Verification during the last ten years, simulation is currently the primary means for performing design verification. The definition of an accurate and pragmatic measure for the coverage achieved by a suite of simulation vectors and the related problem of coverage directed automatic test generation are of great importance. In this paper we introduce a new set of metrics, called the Event Sequence Coverage Metrics (ESCMs). Our approach is based on a simple and automatic method to extract the control flow of a circuit so that the resulting state space can be explored for validation coverage analysis and automatic test generation. During simulation we monitor, in addition to state and transition coverage, whether certain control event sequences take place or not. We then combine formal verification techniques, using BDDs as the underlying representation, with\u00a0\u2026", "num_citations": "1\n", "authors": ["493"]}
{"title": "Lightweight guided random simulation\n", "abstract": " We present methods for improving the effectiveness of random simulation using guides measured during the execution of a program. The key idea is to select inputs based on the measurement of the current state and judge input sequence effectiveness by analyzing the measured direction it induces. The process allows a set of test programs to be derived from a single guide by varying a random seed. We present some experimental results for an initial implementation.", "num_citations": "1\n", "authors": ["493"]}
{"title": "From dependable computing systems to computing for integrated dependable systems?\n", "abstract": " Today, in the direction of research we can observe a trend from the investigation and development of \"pure\" systems to activities usually tailored to specific applications. Concurrently, computers have penetrated daily life in a degree which had not been forecast even in optimistic estimation scenarios 20 or 30 years ago. This trend has not yet ended; actually we can expect the emergence of many new \"intelligent products\"; i.e. digital computing systems are more and more integrated or merged into many other systems of different physical domains and characteristics. In this paper, the impact of these general trends on the current state and future directions of fault tolerance activities is discussed.", "num_citations": "1\n", "authors": ["493"]}
{"title": "Hierarchical specification of system behavior\n", "abstract": " Complex systems are commonly broken up into a hierarchy of composed components. This methodology reduces the complexity of the system os a whole and allows teams of designers to attack the problem of definition. We present and propose a hierarchical methodology for the specification of the behaviors of complex designs. We introduce a single BY operator, such that A BY B means \"A is true because of the arbitrarily repeated assertion of the truth of B\". We show its application in the temporal logic of UNITY and present example uses of the BY operator in specifications of complex systems. Usage of the BY operator induces a hierarchy in the specification of the behavior of a system which often corresponds well to the decomposition of the design definition.", "num_citations": "1\n", "authors": ["493"]}
{"title": "A memory distribution mechanism for object oriented applications\n", "abstract": " Many applications, particularly in Computer-Aided Design (CAD), require large amounts of memory, limiting the size of problems which can be handled. This paper presents a new mechanism which exploits the large amount of memory available in a cluster of work-stations for programs that are designed in an object-oriented manner. The memory required for each object may be allocated in other machines on the network, and any references to the object return the desired item from the appropriate machine in a transparent fashion. Experimental results are provided to demonstrate the effectiveness of this approach for a particular memory-intensive CAD application.", "num_citations": "1\n", "authors": ["493"]}
{"title": "POP: an efficient Performance OPtimization algorithm based on integrated approach\n", "abstract": " This paper presents an efficient performance optimization algorithm, which overcomes the drawbacks of previous approaches and can be used to produce high performance circuits without modifying the topology of the circuit. In contrast with previous approaches, the algorithm selects the minimum number of components to be optimized with vastly improved times while considering the functionality of components exactly at the same time.< >", "num_citations": "1\n", "authors": ["493"]}
{"title": "Adding capability checks enhances error detection and isolation in object-based systems\n", "abstract": " Error detection and error isolation are becoming stringent requirements for many computational problems requiring high reliability in addition to high performance. This paper presents CAPACETI, a technique for utilizing capabilities at the application level in order to achieve dependable operations. The proposed technique is further augmented with executable assertions and other software error detection techniques. The effectiveness of the techniques to detect errors, their contribution to the overall coverage, and their performance overhead were experimentally obtained using fault/error injection. Results obtained from these experiments show that high coverage with a low performance overhead can be achieved by selectively combining different error detection techniques.", "num_citations": "1\n", "authors": ["493"]}
{"title": "Impact of behavioral learning on the compilation of sequential circuit tests\n", "abstract": " Learning techniques based on behavioral descriptions of designs are introduced. The information generated using these techniques is reusable not only during the generation of test knowledge for the design, but also during the generation of tests for the design, and hence it is a one-time effort. Results are provided to demonstrate the impact of behavioral learning techniques on the quality of sequential circuit tests generated for modular and hierarchical designs. The design and test knowledge representation schemes, most of which have been introduced in the past, are discussed briefly. The behavioral learning techniques are then introduced. Results obtained for a simple CPU design are discussed.", "num_citations": "1\n", "authors": ["493"]}
{"title": "SPECIES IDENTIFICATION AND DETECTION OF ADULTERATION LEVEL IN COOKED MEAT USING AGAROSE ISOELECTRIC-FOCUSING\n", "abstract": " A study was undertaken to assess the feasibility of using agarose isoelectric focusing technique for species identification in binary mixtures of urea extracts from cooked meat and to assess the adulteration level in such mixtures. Distinguishable differences were observed on AGIEF pherograms at pH 5.0 - 8.0. Even after subjecting the meat to a temperature of 120-degrees-C, species origin could be identified in binary mixtures based on the focal points of their protein bands compared to the focal points of authentic single species band profile. It was also possible to detect 10% level of adulteration by AGIEF in binary mixtures of cooked meat urea extracts.", "num_citations": "1\n", "authors": ["493"]}
{"title": "Optimization for Behavioral/RTL Simulation\n", "abstract": " Mobile Cloud Computing (MCC) offers a new paradigm to relieve the pressure of soaring data demands and augment the capabilities of resource-poor mobile devices. In this paper, we study the multi-user computation offloading problem in MCC to reduce the total cost including energy consumption, time consumption and monetary cost on mobile devices. We formulate the computation offloading decision making problem among multiple mobile device users as a multi-user computation offloading game problem. We present a distributed computation offloading algorithm and show the existence of Nash equilibrium of the game. We show that the proposed algorithm can achieve efficient computation offloading performance.", "num_citations": "1\n", "authors": ["493"]}
{"title": "Automatic test generation for linear digital systems with bi-level search using matrix transform methods\n", "abstract": " Linear state variable digital systems, commonly implemented in bit-serial architecture using silicon compilers, are difficult to test for manufacturing defects due to deep sequentiality, low controllability and observability, and high latency. A novel hierarchical testing approach, based on matrix manipulation and constrained low-level test generation, is reported here. FEAST (Functional Extractor and Sequential Test generator) operates at the high level, where the circuit is described as an interconnection of arithmetic modules. CREST (Constrained Sequential Test generator) operates at the low level description of the individual modules, and generates test sets satisfying constraints imposed by the high-level modules and their interconnection structure. The new approach was found to perform better when compared to automatic test generation at the gate level using existing algorithms for several large circuits.", "num_citations": "1\n", "authors": ["493"]}
{"title": "Efficient testing techniques for bit and digit-serial arrays\n", "abstract": " Bit and digit-serial architectures are used extensively in digital signal processing applications. Testing these structures is a very difficult problem due to low controllability/observability and complex interconnections between the circuit components. Efficient test generation techniques have been developed and applied to three classes of bit and digit-serial circuits. The testing techniques are novel and address issues such as embedded finite state machine testing, pipelining of test vectors, time to space transformation of iterative systems, and testing of cascaded cells. Test complexity issues are also discussed.<>", "num_citations": "1\n", "authors": ["493"]}
{"title": "Probabilistic model for the evaluation of fault-tolerant multiprocessor systems using concurrent error detection\n", "abstract": " The analysis of fault-tolerant multiprocessor systems that use concurrent error detection (CED) schemes is much more difficult than the analysis of conventional fault-tolerant architectures. Various analytical techniques have been proposed to evaluate CED schemes deterministically. However these approaches are based on the worst-case assumptions related to the failure of system components. Often the evaluation results are not good measures for comparing the fault tolerance capabilities of a system. In this paper we develop a probabilistic approach to evaluate and compare fault-tolerant multiprocessor systems. Various probabilities associated with CED schemes are identified and used in the framework of the matrix-based model which we had proposed previously. Based on these probabilistic matrices we analytically derive estimates for the fault tolerance capabilities of various systems.", "num_citations": "1\n", "authors": ["493"]}
{"title": "Test generation for hybrid iterative logic arrays\n", "abstract": " The test generation problem for hybrid iterative logic arrays (ILAs) which are constructed from ILAs of different types of cells by interconnecting them along their boundaries to realize a complex combinational function is discussed. The test generation problem for hybrid ILAs is complicated by the presence of different types of cell functions that affect test controllability and observability in different ways. For an array to be testable with a fixed number of test vectors irrespective of its size (C-testable), each of the individual constituent ILAs must satisfy certain rigid conditions that make test generation a difficult task. A graphical model is presented for test computation, and it is shown how this model can be used very effectively to generate efficient tests sets for hybrid ILAs.< >", "num_citations": "1\n", "authors": ["493"]}
{"title": "Applications of superheated superconducting detectors\n", "abstract": " When an incident particle collides with 8- mu m superheated superconducting granules, if the deposition of energy within the granule is sufficient to overcome an energy threshold, the granule will change state. The Meissner effect characteristic of the superconducting state suddenly vanishes and the magnetic field penetrates into the granule, causing a change of flux in a pickup coil containing this granule. This effect provides a practical detector if large numbers of granules are used. X-ray imaging devices, transition-radiation detectors and indium solar neutrino detectors are briefly presented, together with a discussion of the main features of this type of detector.< >", "num_citations": "1\n", "authors": ["493"]}
{"title": "Isoelectric Focusing As A Method For Species Differentiation Of Raw And Cooked Meats\n", "abstract": " ABSTRACT Name of Author Title of the thesis : J. ABRAHAM : ISOELECTRIC FOCUSING AS A METHOD FOR SPECIES DIFFERENTIATION OF RAW AND COOKED MEATS. Degree to which it is submitted : DOCTOR OF PHILOSOPHY Faculty Guide University : Department of Meat Science and Technology, College of Veterinary Sciences, Tirupati-5 17 502. : Dr. P. Varada Rajalu, M,V.Sc. PhD. Professor and University Head, Department of Meat Science and Technology, College of Veterinary Sciences, Tirupati. : Andhra Pradesh Agricultural University, Rajendra Nagar, Hyderabad-500 030. Year of Submission : 1987 A study to assess the suitability of Agarose Isoelectric focusing technique (AGIEF) for the species identification of raw and cooked meats was undertaken For the extraction of proteins from raw meats, both distilled water (DWER) and 8 M urea solution (UER) were found to be suitable while only the latter was efficient for extraction of denatured proteins of cooked meats (UEC). The pherograms resolved from DWER and UER of beef, buff (buffalo meat), chevon and mutton on AGIEF at pH ranges of 3.5 to 9.5 and 5.0 - 8.0 showed distinguishable differences in their band patterns enabling species identification. It was also found that the  species identification of raw meats could be done rapidly based on the difference in the focal points of the two coloured myoglobin bands found in the unstained isoelectric focused gels at both pH ranges. An extraction time of 36 to 48 hours was found to be necessary for the solubilisation and extraction of denatured proteins from cooked meats. Both, Coomassie brilliant blue R-250 and Coomassie brilliant\u00a0\u2026", "num_citations": "1\n", "authors": ["493"]}
{"title": "The Evolution of Fault Tolerant Computing at the University of Illinois\n", "abstract": " The University of Illinois has been active in research in the fault-tolerant computing field for over 25 years. Fundamental ideas have been proposed and major contributions made by researchers at the University of Illinois in the areas of testing and diagnosis, concurrent error detection, and fault tolerance. This paper traces the origins of these ideas and their development within the University of Illinois, as well as their influence upon research at other institutions, and outlines current directions of research.", "num_citations": "1\n", "authors": ["493"]}
{"title": "Effect of storage of meat in frozen condition on the sarcoplasmic protein band patterns in agarose isoelectric focusing.\n", "abstract": " Effect of storage of meat in frozen condition on the sarcoplasmic protein band patterns in agarose isoelectric focusing. FAO_logo home-icon English Espa\u00f1ol Fran\u00e7ais \u0627\u0644\u0639\u0631\u0628\u064a\u0629 \u4e2d\u6587 \u0420\u0443\u0441\u0441\u043a\u0438\u0439 home-icon Translate with Google Access the full text NOT AVAILABLE Lookup at Google Scholar google-logo Bibliographic information Language : English Type : Journal Article In AGRIS since : 2012 Volume : 14 Extent : v.76-80(1) All titles : \" Effect of storage of meat in frozen condition on the sarcoplasmic protein band patterns in agarose isoelectric focusing. \" Other : \" 8 ref. Summary (En). \" Save as: AGRIS_AP RIS EndNote(XML) Effect of storage of meat in frozen condition on the sarcoplasmic protein band patterns in agarose isoelectric focusing. Loading... Paper Written Paper Effect of storage of meat in frozen condition on the sarcoplasmic protein band patterns in agarose isoelectric focusing. [1983] Abraham J. Nayar L. ., . [] of [\u2026", "num_citations": "1\n", "authors": ["493"]}
{"title": "Hardware Verification: Theory and Practice\n", "abstract": " This tutorial discusses the basic techniques which can be used for verifying complex designs, and the extent to which they are being used today. Although simulation is the primary means of validating large designs at this time, there is a movement towards using formal techniques. The basic theory behind various formal techniques is described, and the extent to which they are applied is pointed out. Existing state-of-the-art tools will be described and demonstrated. Methods of dealing with complexity which will allow the designer to deal with the extremely large state spaces will also be described.", "num_citations": "1\n", "authors": ["493"]}