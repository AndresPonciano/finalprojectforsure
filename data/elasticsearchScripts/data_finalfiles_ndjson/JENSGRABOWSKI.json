{"title": "Model-driven testing: Using the UML testing profile\n", "abstract": " Model-driven development has become the most important new paradigm in software development and has already demonstrated considerable impact in reducing time to market and improving product quality. However, the development of high-quality systems not only requires systematic development processes but also systematic test processes. This book is about systematic, model-driven test processes in the context of UML. As UML provides only limited means for the design and development of test artifacts, a consortium was formed by the Object Management Group (OMG) to develop a UML profile for model-driven testing\u2013the UML Testing Profile (UTP), an official OMG standard since 2005. Written by the original members of this standardization group, this book shows you how to use UML to test complex software systems. The authors introduce UTP step-by-step, using a case study that illustrates how UTP can be used for test modeling and test specification. You\u2019ll learn how UTP concepts can be used for functional and non-functional testing, with example applications and best practices for user-interfaces and service oriented architectures. In addition, the authors demonstrate how to apply UTP using frameworks like TTCN-3 and the JUnit test framework for Java. This book is the definitive reference for the only UML-based test specification language, written by the creators of that language. It is supported by an Internet site that provides information on the latest tools and uses of the profile.", "num_citations": "301\n", "authors": ["120"]}
{"title": "Tutorial on message sequence charts\n", "abstract": " An introduction to the ITU standard language Message Sequence Chart (MSC) is provided. It is pointed out that MSC in many respects is complementary to the ITU specification and design language SDL. MSC in combination with SDL or other languages, now plays a role in nearly all stages of the system development process. Since MSC has been standardized in the same study group as SDL, the language form is quite analogous, eg it has a graphical (MSC GR) and a textual (MSC PR) syntax form. The MSC language in the present recommendation Z. 120 (MSC'92), comprises basic language elements\u2014instance, message, environment, action, timer, process creation and termination, condition\u2014and structural language elements\u2014\u201ccoregion\u201d and \u201csubmsc\u201d. It is demonstrated how global and non-global conditions may be used for the composition of MSCs. Whereas in MSC'92 the main emphasis is put on the\u00a0\u2026", "num_citations": "232\n", "authors": ["120"]}
{"title": "An introduction to the testing and test control notation (TTCN-3)\n", "abstract": " The testing and test control notation (TTCN-3) is a new test specification and test implementation language that supports all kinds of black-box testing of distributed systems. TTCN-3 was developed in the years 1999 to 2002 at the European Telecommunications Standards Institute (ETSI), as a redesign of the tree and tabular combined notation (TTCN) standard (ITU-T Rec. X.292). TTCN-3 is built from a textual core language that provides interfaces to different data description languages and the possibility of different presentation formats. This makes TTCN-3 quite universal and application independent. TTCN-3 is being published as the ITU-T Rec. Z.140 series. This paper provides an introduction to TTCN-3. This includes an overall view of the TTCN-3 core language, a description of the existing presentation formats, an explanation of the implementation of TTCN-3-based test systems and a discussion about the\u00a0\u2026", "num_citations": "201\n", "authors": ["120"]}
{"title": "Grid and cloud computing: opportunities for integration with the next generation network\n", "abstract": " Carrier-grade networks of the future are currently being standardized and designed under the umbrella name of Next Generation Network (NGN). The goal of NGN is to provide a more flexible network infrastructure that supports not just data and voice traffic routing, but also higher level services and interfaces for third-party enhancements. Within this paper, opportunities to integrate grid and cloud computing strategies and standards into NGN are considered. The importance of standardized interfaces and interoperability testing demanded by carrier-grade networks are discussed. Finally, a proposal how the testing methods developed at the European Telecommunications Standards Institute (ETSI) can be applied to improve the quality of standards and implementations is presented.", "num_citations": "182\n", "authors": ["120"]}
{"title": "A comparative study to benchmark cross-project defect prediction approaches\n", "abstract": " Cross-Project Defect Prediction (CPDP) as a means to focus quality assurance of software projects was under heavy investigation in recent years. However, within the current state-of-the-art it is unclear which of the many proposals performs best due to a lack of replication of results and diverse experiment setups that utilize different performance metrics and are based on different underlying data. Within this article, we provide a benchmark for CPDP. We replicate 24 approaches proposed by researchers between 2008 and 2015 and evaluate their performance on software products from five different data sets. Based on our benchmark, we determined that an approach proposed by Camargo Cruz and Ochimizu (2009) based on data standardization performs best and is always ranked among the statistically significant best results for all metrics and data sets. Approaches proposed by Turhan et al. (2009), Menzies et\u00a0\u2026", "num_citations": "129\n", "authors": ["120"]}
{"title": "Test case generation with test purpose specification by MSCs\n", "abstract": " This paper presents a new test case generation method based on formal system speci cations in SDL. The test purpose of a speci c test case is speci ed formally by one or many Message Sequence Charts (MSCs). Based on the test purpose and the system speci cation a complete test case can be generated automatically in the TTCN format, including preamble, postamble and test body with all test verdicts.", "num_citations": "118\n", "authors": ["120"]}
{"title": "Towards a Petri net based semantics definition for Message Sequence Charts\n", "abstract": " CiNii \u8ad6\u6587 - Towards a Petri net based semantics definition for message sequence charts CiNii \u56fd\u7acb\u60c5\u5831\u5b66\u7814\u7a76\u6240 \u5b66\u8853\u60c5\u5831\u30ca\u30d3\u30b2\u30fc\u30bf[\u30b5\u30a4\u30cb\u30a3] \u65e5\u672c\u306e\u8ad6\u6587\u3092\u3055\u304c\u3059 \u5927\u5b66\u56f3\u66f8\u9928\u306e\u672c\u3092\u3055\u304c\u3059 \u65e5\u672c\u306e\u535a\u58eb\u8ad6\u6587\u3092\u3055\u304c\u3059 \u65b0\u898f\u767b\u9332 \u30ed\u30b0\u30a4\u30f3 English \u691c\u7d22 \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u9589\u3058\u308b \u30bf\u30a4\u30c8\u30eb \u8457\u8005\u540d \u8457\u8005ID \u8457\u8005\u6240\u5c5e \u520a\u884c\u7269\u540d ISSN \u5dfb\u53f7\u30da\u30fc\u30b8 \u51fa\u7248\u8005 \u53c2\u8003\u6587\u732e \u51fa\u7248\u5e74 \u5e74\u304b\u3089 \u5e74 \u307e\u3067 \u691c\u7d22 \u691c\u7d22 \u691c\u7d22 CiNii\u7a93\u53e3\u696d\u52d9\u306e\u518d\u958b\u306b\u3064\u3044\u3066 Towards a Petri net based semantics definition for message sequence charts GRAUBMANN P. \u88ab\u5f15\u7528\u6587\u732e: 1\u4ef6 \u8457\u8005 GRAUBMANN P. \u53ce\u9332\u520a\u884c\u7269 Proc. 6th SDL Forum Proc. 6th SDL Forum, 1993 \u88ab\u5f15\u7528\u6587\u732e: 1\u4ef6\u4e2d 1-1\u4ef6\u3092 \u8868\u793a 1 Construction of Global State Transition Graph for Verifying Specifications Written in Message Sequence Charts for Telecommunications Software KIM Byeong Man , KIM Hyeon Soo , KIM Wooyoung IEICE transactions on information and systems 84(2), 249-261, 2001-02-01 \u53c2\u800342\u2026", "num_citations": "110\n", "authors": ["120"]}
{"title": "Applying the ISO 9126 quality model to test specifications\u2013exemplified for TTCN-3 test specifications\n", "abstract": " Quality models are needed to evaluate and set goals for the quality of a software product. The international ISO/IEC standard 9126 defines a general quality model for software products. Software is developed in different domains and the usage of the ISO/IEC quality model requires an instantiation for each concrete domain. One special domain is the development and maintenance of test specifications. Test specifications for testing, e.g. the Internet Protocol version 6 (IPv6) or the Session Initiation Protocol (SIP), reach sizes of more than 40.000 lines of test code. Such large test specifications require strict quality assurance. In this paper, we present an adaptation of the ISO/IEC 9126 quality model to test specifications and show its instantiation for test specifications written in the Testing and Test Control Notation (TTCN-3). Example measurements of the standardised SIP test suite demonstrate the applicability of our approach.", "num_citations": "99\n", "authors": ["120"]}
{"title": "Tutorial on message sequence charts (MSC\u201996)\n", "abstract": " MSC is a trace language which in its graphical form admits a particularly intuitive representation of system runs in distributed systems while focusing on the message interchange between communicating entities and their environment. For the rst time the MSC recommendation Z. 120 (MSC'92) was approved at the ITU meeting Geneva 1992. A new revised MSC recommendation Z. 120 (MSC'96) was approved at the closing session of the last study period in April 1996.Whereas in MSC'92 main emphasis was put on the elaboration of basic concepts and a corresponding formal semantics, in the new MSC version-MSC'96-structural language constructs, essentially composition and object oriented concepts, play a dominant role. With these new concepts, the power of MSC is enhanced considerably in order to overcome the traditional restriction to the speci cation of only a few selected system runs. Within the tutorial, the use of MSC is demonstrated by means of the ISDN supplementary service'Completion of Calls to Busy Subscriber (CCBS)'.", "num_citations": "94\n", "authors": ["120"]}
{"title": "Calculation and optimization of thresholds for sets of software metrics\n", "abstract": " In this article, we present a novel algorithmic method for the calculation of thresholds for a metric set. To this aim, machine learning and data mining techniques are utilized. We define a data-driven methodology that can be used for efficiency optimization of existing metric sets, for the simplification of complex classification models, and for the calculation of thresholds for a metric set in an environment where no metric set yet exists. The methodology is independent of the metric set and therefore also independent of any language, paradigm or abstraction level. In four case studies performed on large-scale open-source software metric sets for C functions, C+\u2009+, C# methods and Java classes are optimized and the methodology is\u00a0validated.", "num_citations": "93\n", "authors": ["120"]}
{"title": "The UML 2.0 testing profile and its relation to TTCN-3\n", "abstract": " UML models focus primarily on the definition of system structure and behaviour, but provide only limited means for describing test objectives and test procedures. However, with the approach towards system engineering with automated code generation, the need for solid conformance testing has increased. In June 2001, an OMG Request For Proposal (RFP) on an UML2.0 Testing Profile (UTP) has been initiated. This RFP solicits proposals for a UML2.0 profile, which enables the specification of tests for structural and behavioural aspects of computational UML models, and which is capable to interoperate with existing test technologies for black box testing. This paper discusses different approaches for testing with UML and discusses the ongoing work of the Testing Profile. Special emphasize is laid on the mapping of UML2.0 testing concepts to the standardized Testing and Test Control Notation (TTCN-3).", "num_citations": "87\n", "authors": ["120"]}
{"title": "Sources of information and behavioral patterns in online health forums: observational study\n", "abstract": " Background: Increasing numbers of patients are raising their voice in online forums. This shift is welcome as an act of patient autonomy, reflected in the term \u201cexpert patient\u201d. At the same time, there is considerable concern that patients can be easily misguided by pseudoscientific research and debate. Little is known about the sources of information used in health-related online forums, how users apply this information, and how they behave in such forums.Objective: The intent of the study was to identify (1) the sources of information used in online health-related forums, and (2) the roles and behavior of active forum visitors in introducing and disseminating this information.Methods: This observational study used the largest German multiple sclerosis (MS) online forum as a database, analyzing the user debate about the recently proposed and controversial Chronic Cerebrospinal Venous Insufficiency (CCSVI) hypothesis. After extracting all posts and then filtering relevant CCSVI posts between 01 January 2008 and 17 August 2012, we first identified hyperlinks to scientific publications and other information sources used or referenced in the posts. Employing k-means clustering, we then analyzed the users\u2019 preference for sources of information and their general posting habits.Results: Of 139,912 posts from 11,997 threads, 8628 posts discussed or at least mentioned CCSVI. We detected hyperlinks pointing to CCSVI-related scientific publications in 31 posts. In contrast, 2829 different URLs were posted to the forum, most frequently referring to social media, such as YouTube or Facebook. We identified a total of 6 different roles of hyperlink posters\u00a0\u2026", "num_citations": "79\n", "authors": ["120"]}
{"title": "On the design of the new testing language TTCN-3\n", "abstract": " This paper gives an overview of the main concepts and features of the new testing language TTCN version 3 (TTCN-3). TTCN-3 is a complete new testing language built from a textual core notation on which a number of different presentation formats are possible. This makes TTCN-3 quite universal and application independent. One of the standardised presentation formats is based on the tree and tabular format from previous TTCN versions and another standardised presentation format is based on MSCs. TTCN-3 is a modular language and has a similar look and feel to a typical programming language. However, in addition to the typical programming constructs it contains all the important features necessary to specify test suites.", "num_citations": "69\n", "authors": ["120"]}
{"title": "Test architectures for distributed systems\u2014state of the art and beyond\n", "abstract": " A generic test architecture for conformance, interoperability and performance testing of distributed systems is presented. The generic test architecture extends current test architectures with respect to the types of systems that can be tested. Whereas in the conformance testing methodology and framework the focus is on testing protocol implementations, the generic architecture focuses on testing real distributed systems whose communication functions are implemented on different real systems and whose correctness can only be assessed when tested as a whole. In support of the latter requirement, a test system itself is regarded as being a distributed system whose behaviour is determined by the behaviour of components and their interaction using a flexible and dynamic communication structure.", "num_citations": "65\n", "authors": ["120"]}
{"title": "TimedTTCN-3 \u2014 A Real-Time Extension for TTCN-3\n", "abstract": " The Testing and Test Control Notation (TTCN-3) was originally developed as successor of the second edition of the Tree and Tabular Combined Notation TTCN-3 is a standardized test specification and implementation language to test functional behaviour of distributed systems. TimedTTCN-3 is a real-time extension for TTCN-3 that supports the test and measurement of real-time requirements. TimedTTCN-3 introduces absolute time, allows the definition of synchronization requirements for test components and provides possibilities to specify online and offline evaluation procedures for real-time requirements.", "num_citations": "62\n", "authors": ["120"]}
{"title": "Autolink-a tool for automatic test generation from SDL specifications\n", "abstract": " Due to an increasing interest in SDL (Specification and Description Language), MSC (Message Sequence Charts) and TTCN (Tree and Tabular Combined Notation) based tools for validation and test generation, Telelogic AB and the Institute for Telematics at the University of Lu/spl uml/beck are cooperating in a research and development project which adds new test generation facilities to Telelogic's Tau tool set. The result of that cooperation is Autolink, a software tool which supports the automatic generation of TTCN test suites based on SDL and MSC specifications. The project follows a pragmatic approach and is driven by practical experience. Autolink has been used by the European Telecommunications Standards Institute (ETSI) to develop a test suite for Core INAP CS-2 (Intelligent Network Application Protocol, Capability Set 2).", "num_citations": "58\n", "authors": ["120"]}
{"title": "Autolink\u2014putting SDL-based test generation into practice\n", "abstract": " Autolink is a tool for automatic test generation. It allows to generate TTCN test suites based on a given SDL specification and MSC requirements. The first big challenge for Autolink was the creation of a test suite for the Intelligent Network Application Protocol at ETSI. In this paper we discuss our experience in applying Autolink to a real-life protocol and the improvements of Autolink which were developed during this project. We also present future enhancements which will further ease the work of test suite developers.", "num_citations": "55\n", "authors": ["120"]}
{"title": "Test generation with autolink and testcomposer\n", "abstract": " Testing of telecommunication systems is a major concern in industry and standardization. Therefore, the two major SDL tool vendors have integrated automatic test generation tools into their software development suites. While Telelogic has added Autolink to the Tau tools, former Verilog has extended ObjectGeode with TestComposer.Even though both tools are based on the same concepts, many features are realized differently and their focus is put onto different aspects of the test generation process. This paper introduces the major principles of SDL-based test generation and provides an overview of how both tools support test development.", "num_citations": "47\n", "authors": ["120"]}
{"title": "Global vs. local models for cross-project defect prediction\n", "abstract": " Although researchers invested significant effort, the performance of defect prediction in a cross-project setting, i.e., with data that does not come from the same project, is still unsatisfactory. A recent proposal for the improvement of defect prediction is using local models. With local models, the available data is first clustered into homogeneous regions and afterwards separate classifiers are trained for each homogeneous region. Since the main problem of cross-project defect prediction is data heterogeneity, the idea of local models is promising. Therefore, we perform a conceptual replication of the previous studies on local models with a focus on cross-project defect prediction. In a large case study, we evaluate the performance of local models and investigate their advantages and drawbacks for cross-project predictions. To this aim, we also compare the performance with a global model and a transfer learning\u00a0\u2026", "num_citations": "46\n", "authors": ["120"]}
{"title": "Test Case Generation and Test Case Specification with Message Sequence Charts\n", "abstract": " Vorwort Das Testen von Kommunikationssystemen hat in den letzten Jahren zunehmend an Bedeutung gewonnen. Dieses hat vor allem zwei Gr\u00bf unde. Zum einen werden die Systeme immer komplexer, wodurch es immer schwieriger wird, die korrekte Zusammenarbeit der unterschiedlichen Komponenten eines solchen Systems zu garantieren. Zum anderen ist das Testen von Kommunikationssystemen eine Arbeit, die sehr h\u00bf aufig mit einem grossen Zeit-und Kostenaufwand verbunden ist. Durch die Verwendung von formalen Beschreibungssprachen zur Systemspezifikation und zur Testbeschreibung erhofft man sich, die Qualit\u00bf at von Tests zu erh\u00bf ohen und den Aufwand f\u00bf ur das Testen zu reduzieren. Auch die hier vorliegende Arbeit setzt sich mit der skizzierten Problematik auseinander. Insbesondere besch\u00bf aftigt sie sich mit den drei Sprachen MSC, SDL und TTCN sowie deren Anwendung bei der\u00a0\u2026", "num_citations": "45\n", "authors": ["120"]}
{"title": "The standardization of message sequence charts\n", "abstract": " The most relevant issues of the standardization of the Message Sequence Chart (MSC) language within the CCITT Study Group X are discussed. The history of the new MSC recommendation Z.120 is sketched. Different types of diagrams which are closely related to MSCs are compared, since they build the basis for the MSC language. The authors distinguished these diagrams from the standardized MSC language by using the term sequence charts (SCs). Subsequently, the MSC language is introduced and several approaches towards a forthcoming formal MSC semantics are presented.< >", "num_citations": "45\n", "authors": ["120"]}
{"title": "TTCN-3-A new test specification language for black-box testing of distributed systems\n", "abstract": " The Tree and Tabular Combined Notation (TTCN) is a well-established notation for the specification of test cases for OSI protocol conformance testing. The third edition of TTCN (TTCN-3) is currently under development by the European Telecommunications Standards Institute. TTCN-3 will be a complete redesign of the entire test specification language. The close relation between tabular and textual representation will be removed, OSI specific language constructs will be cleared away and new concepts will be introduced. The intention of this redesign is to modernize TTCN and to widen its application area beyond pure OSI conformance testing. This paper will motivate the need for TTCN-3 and introduce the principles of TTCN-3.", "num_citations": "41\n", "authors": ["120"]}
{"title": "SDL and MSC based test generation for distributed test architectures\n", "abstract": " Most of the SDL and MSC based test generation methods and tools produce non-concurrent TTCN test cases only. If the test equipment itself is a distributed system, the implementation of such test cases is a difficult task and requires a substantial amount of additional work. In this paper, we explain how concurrent TTCN test cases can be generated directly from SDL system specifications and MSC test purposes. To do this, explicit synchronization points have to be indicated in the MSC test purposes, and information about the existing test components and their connections has to be provided.", "num_citations": "36\n", "authors": ["120"]}
{"title": "Architecting dynamic reconfiguration in dependable systems\n", "abstract": " The need for dynamic reconfiguration is a complicating factor in the design of dependable systems, as it demands from software architects both rigour and planning. Although recent research has shown that systematic and integrated \u201cspecification-to-deployment\u201d environments are promising approaches to architecting dependable systems, few proposals have yet considered dynamic reconfiguration, and then only in specific situations. In this paper, we propose a generic approach to supporting dynamic reconfiguration in dependable systems. The proposed approach is built on our view that dynamic reconfiguration in such systems needs to be causally connected at runtime to a corresponding high-level software architecture specification. In more detail, we propose two causally-connected models: an architecture-level model and a runtime-level model. Dynamic reconfiguration can be applied either through\u00a0\u2026", "num_citations": "35\n", "authors": ["120"]}
{"title": "Towards the industrial use of validation techniques and automatic test generation methods for SDL specifications\n", "abstract": " Publisher SummaryThis chapter shows the possibilities for tool-assisted test case generation using the SDT Validator and ITEX. Practical use of these methods indicates that an improvement of the test case generation facilities is needed and is possible. Therefore, the AUTOLINK project has been set up. The first, basic version of the AUTOLINK tool is described in the chapter. The ITU-T Specification and Description Language (SDL) is worldwide the most successful standardized formal description technique. SDL has been used successfully in industrial projects and for standardization purposes. The SDT/ITEX tools from Telelogic AB are one of the most successful commercial SDL tool sets that provide a complete environment for the development of SDL specifications, SDL-based implementations, and The Tree and Tabular Combined Notation (TTCN)-based test suites. For this, they include graphical editors\u00a0\u2026", "num_citations": "35\n", "authors": ["120"]}
{"title": "Addressing problems with replicability and validity of repository mining studies through a smart data platform\n", "abstract": " The usage of empirical methods has grown common in software engineering. This trend spawned hundreds of publications, whose results are helping to understand and improve the software development process. Due to the data-driven nature of this venue of investigation, we identified several problems within the current state-of-the-art that pose a threat to the replicability and validity of approaches. The heavy re-use of data sets in many studies may invalidate the results in case problems with the data itself are identified. Moreover, for many studies data and/or the implementations are not available, which hinders a replication of the results and, thereby, decreases the comparability between studies. Furthermore, many studies use small data sets, which comprise of less than 10 projects. This poses a threat especially to the external validity of these studies. Even if all information about the studies is available\u00a0\u2026", "num_citations": "34\n", "authors": ["120"]}
{"title": "Using pharmacy data on partial adherence to inform clinical care of patients with serious mental illness\n", "abstract": " MethodsThis retrospective study was conducted with administrative and clinical chart data from four VA hospitals and their associated community-based outpatient clinics. The local institutional review board at each site approved a clinical trial of which this report is a component. The review boards approved a waiver of consent for access to personal health information for screening purposes for the study. Screening data for patients who did not enroll in the clinical trial were masked from the records.", "num_citations": "34\n", "authors": ["120"]}
{"title": "A framework for the specification of test cases for real-time distributed systems\n", "abstract": " The OSI conformance testing methodology and framework (CTMF) is a well established standard which defines and regulates the conformance testing procedure for protocol implementations. Conformance testing is meant to be functional black-box testing. Besides concepts and terminology, the CTMF standardizes testing architectures and the Tree and Tabular Combined Notation (TTCN) test specification language. As more and more distributed systems such as multimedia, safety-critical and real-time systems rely on the timely availability of information, testing of real-time requirements becomes a serious issue, too. Unfortunately, testing real-time and other non-functional requirements (performance and reliability) are outside the scope of CTMF. In this paper we present an extension of CTMF which allows us to specify test cases for testing real-time requirements. The extension includes a generic testing\u00a0\u2026", "num_citations": "31\n", "authors": ["120"]}
{"title": "Improved bug reporting and reproduction through non-intrusive gui usage monitoring and automated replaying\n", "abstract": " Most software systems are operated using a Graphical User Interface (GUI). Therefore, bugs are often triggered by user interaction with the software's GUI. Hence, accurate and reliable GUI usage information is an important tool for bug fixing, as the reproduction of a bug is the first important step towards fixing it. To support bug reproduction, a generic, easy to integrate, non-intrusive GUI usage monitoring mechanism is introduced in this paper. As supplement for the monitoring, a method for automatically replaying the monitored usage logs is provided. The feasibility of both is demonstrated through proof-of-concept implementations. A case-study shows that the monitoring mechanism can be integrated into large-scale software products without significant effort and that the logs are replayable. Additionally, a usage-based end-to-end GUI testing approach is outlined, in which the monitoring and replaying play major\u00a0\u2026", "num_citations": "30\n", "authors": ["120"]}
{"title": "Towards a harmonization of UML-sequence diagrams and MSC\n", "abstract": " Sequence Diagrams as part of UML play an important role within use case driven object oriented (OO) software engineering. They can be seen as OO variants of the ITU-T standard language Message Sequence Chart (MSC) which is very popular mainly in the telecommunication area. Both notations would benefit from a harmonization. A more formal and powerful notation for Sequence Diagrams may arise, on the one hand. On the other hand, the application area of MSC might be considerably enlarged. In this context, it has to be noted that the acceptance of a language in the OO community essentially depends on a clear visualization of constructs typical for OO modelling. It is argued that Sequence Diagrams can be transformed into MSC diagrams if some enhancements of MSC are introduced. Such a transformation demonstrates the big advantage of MSC concerning composition mechanisms, particularly, in\u00a0\u2026", "num_citations": "30\n", "authors": ["120"]}
{"title": "Refactoring and metrics for TTCN-3 test suites\n", "abstract": " Experience with the development and maintenance of test suites has shown that the Testing and Test Control Notation (TTCN-3) provides very good concepts for adequate test specification. However, experience has also demonstrated that during either the migration of legacy test suites to TTCN-3, or the development of large TTCN-3 test specifications, users have found it is difficult to construct TTCN-3 tests that are concise with respect to readability, usability, and maintainability. To address these issues, this paper investigates refactoring and metrics for TTCN-3. Refactoring restructures a test suite systematically without changing its behaviour. Complementary metrics are used to assess the quality of TTCN-3 test suites. For automation, a tool called TRex has been developed that supports refactoring and metrics for TTCN-3.", "num_citations": "29\n", "authors": ["120"]}
{"title": "Applying SaMsTaG to the B-ISDN protocol SSCOP\n", "abstract": " The test generation method SaMsTaG (SDL and MSC based test case generation) has been applied successfully to the B-ISDN ATM Adaption Layer protocol SSCOP (Service Specific Connection Oriented Protocol). For approximately 70% of the identified test purposes complete TTCN test cases have been generated automatically. In this paper we describe the experiment, discuss the results and explain how further improvements of the test generation process can be achieved.", "num_citations": "29\n", "authors": ["120"]}
{"title": "SDL and MSC based test case generation--an overall view of the SAMSTAG method\n", "abstract": " This technical report summarizes the results of the research and development project'Conformance Testing--A Tool for the Generation of Test Cases'. 1 Within this project we developed a method for the automatic generation of test cases based on formal specifications and formally defined test purposes. The method is called SaMsTaG. It is implemented in the SaMsTaG tool. Most of the work has already been published in conference proceedings [13, 30], technical reports [12, 14, 15] and project reports [11, 5, 6, 7, 8, 9, 10]. For detailed information these publications should be consulted. The report starts with a short introduction (Section 1). Then the standardized conformance testing procedure [22], in the following abbreviated by CTMF/FMCT, is compared with other test case generation methods (Section 2). Afterwards, the SaMsTaG method (Sections 4, 5) and the SaMsTaG tool are introduced (Section 6). In the last section the formal aspects of CTMF/FMCT, other test case generation met...", "num_citations": "26\n", "authors": ["120"]}
{"title": "A method for the generation of test cases based on SDL and MSCs\n", "abstract": " Within this paper a method for the generation of test cases for conformance tests is presented. The method is based on a formal speci cation written in CCITT SDL CCI92b] and on Message Sequence Charts (MSCs) CCI92a]. It assumes that the purpose of a test case is given by at least one MSC. Although SDL was chosen as formal description technique and MSCs were chosen to express test purposes, in principle, the presented method should work for any formal speci cation which can be represented as labeled transition system and for any test purpose which can be described by a nite automaton.", "num_citations": "26\n", "authors": ["120"]}
{"title": "An approach to quality engineering of TTCN-3 test specifications\n", "abstract": " Experience with the development and maintenance of large test suites specified using the Testing and Test Control Notation (TTCN-3) has shown that it is difficult to construct tests that are concise with respect to quality aspects such as maintainability or usability. The ISO/IEC standard 9126 defines a general software quality model that substantiates the term \u201cquality\u201d with characteristics and subcharacteristics. The domain of test specifications, however, requires an adaption of this general model. To apply it to specific languages such as TTCN-3, it needs to be instantiated. In this paper, we present an instantiation of this model as well as an approach to assess and improve test specifications. The assessment is based on metrics and the identification of code smells. The quality improvement is based on refactoring. Example measurements using our TTCN-3 tool TRex demonstrate how this procedure is applied\u00a0\u2026", "num_citations": "25\n", "authors": ["120"]}
{"title": "TRex-the refactoring and metrics tool for TTCN-3 test specifications\n", "abstract": " Comprehensive testing of modern communication systems often requires large and complex test suites which then have to be maintained throughout the system life-cycle. Industrial experience, with those written in the standardised testing and test control notation (TTCN-3), has shown that this maintenance is a non-trivial task and its burden could be reduced if appropriate tool support existed. To this aim, Motorola has collaborated with the University of Gottingen to develop TRex, a TTCN-3 development environment published under the Eclipse Public License, which notably provides suitable metrics and refactorings to enable the assessment and automatic restructuring of test suites. In this paper we present the TRex tool, which will make it far easier to construct and maintain TTCN-3 tests that are concise and optimally balanced with respect to readability, usability, and maintainability", "num_citations": "25\n", "authors": ["120"]}
{"title": "Adressing problems with external validity of repository mining studies through a smart data platform\n", "abstract": " Research in software repository mining has grown considerably the last decade. Due to the data-driven nature of this venue of investigation, we identified several problems within the current state-of-the-art that pose a threat to the external validity of results. The heavy re-use of data sets in many studies may invalidate the results in case problems with the data itself are identified. Moreover, for many studies data and/or the implementations are not available, which hinders a replication of the results and, thereby, decreases the comparability between studies. Even if all information about the studies is available, the diversity of the used tooling can make their replication even then very hard. Within this paper, we discuss a potential solution to these problems through a cloud-based platform that integrates data collection and analytics. We created the prototype SmartSHARK that implements our approach. Using\u00a0\u2026", "num_citations": "24\n", "authors": ["120"]}
{"title": "The UML 2.0 testing profile\n", "abstract": " Testing often accounts for more than 50% of the required effort during system development. However, testing is often not well integrated with other development phases. One reason for this is that designers, developers and testers all use different languages and tools, making it difficult to communicate with each other and to exchange documents. The UML 2.0 Testing Profile bridges the gap between designers and testers by providing a means to use UML for test specification and modeling. This allows the reuse of UML design documents for testing and enables test development in an early system development phase. The testing profile provides support for UML based model-driven testing. This paper presents the concepts defined in the UML 2.0 Testing Profile and explains their usage by applying those to an example of a simplified Automated Teller Machine (ATM).", "num_citations": "24\n", "authors": ["120"]}
{"title": "Real-time TTCN for testing real-time and multimedia systems\n", "abstract": " In this paper we define real-time TTCN and apply it to several applications. In real-time TTCN, statements are annotated with time labels that specify their earliest and latest execution times. The syntactical extensions of TTCN are the definition of a table for the specification of time names and time units, and two new columns in the dynamic behaviour description tables for the annotation of statements with time labels. We define an operational semantics for real-time TTCN by mapping real-time TTCN to timed transition systems. Alternatively, we introduce a refined TTCN snapshot semantics that takes time annotations into account.", "num_citations": "23\n", "authors": ["120"]}
{"title": "Usage-based automatic detection of usability smells\n", "abstract": " With an increasing number of supported devices, usability evaluation of websites becomes a laborious task. Therefore, usability evaluation should be automated as far as possible. In this paper, we present a summative method for automated usability evaluation of websites. The approach records user actions and transforms them into task trees. The task trees are then checked for usability smells to identify potential usability issues. The approach was applied in two case studies and shows promising results in the identification of four types of usability smells.", "num_citations": "22\n", "authors": ["120"]}
{"title": "On the standardization of a testing framework for application deployment on grid and cloud infrastructures\n", "abstract": " An important requirement in the successful deployment of grid and cloud computing technology in industry or governmental institutions is the ability to compose their infrastructures using equipment from different vendors. These equipments have to be engineered for and assessed to assure a problem-free interoperation. Focusing on software interoperability, we present a testing framework for the assessment of interoperability of grid and cloud computing infrastructures. This testing framework is part of an initiative for standardizing the use of grid and cloud technology in the context of telecommunication at the European Telecommunications Standards Institute. Following the test development process developed and used at the European Telecommunications Standards Institute, its application is exemplified by the assessment of for resource reservation and application deployment onto grid and cloud infrastructures\u00a0\u2026", "num_citations": "21\n", "authors": ["120"]}
{"title": "From Design to Test with UML-Applied to a Roaming Algorithm for Bluetooth Devices\n", "abstract": " The UML Testing Profile provides support for UML based model-driven testing. This paper introduces a methodology of how to use the testing profile in order to modify and extend an existing UML design model for test issues. As a case study, a new roaming algorithm for bluetooth devices has been develped at the University of L\u00fcbeck, is modelled using UML. The usability of the UML Testing Profile will be explained by applying it to this model. 1", "num_citations": "21\n", "authors": ["120"]}
{"title": "Software process simulation based on mining software repositories\n", "abstract": " Software development processes and their quality depend on many evolutionary factors, such as project size and collaboration of the development team, and development strategies. For project managers it would be valuable to test the interplay of different possible future scenarios in advance, before making decisions about the further process. Our aim is to build such a tool based on agent-based simulation. In this paper, we present our simulation model and our approach in mining software repositories as input for it. The simulation model focuses on system growth, bugs lifetime and developer activity. The detection of patterns related to these topics allows us to estimate adequate simulation parameters close to reality. We evaluate this approach by comparing empirical and simulated data.", "num_citations": "20\n", "authors": ["120"]}
{"title": "Mining software dependency networks for agent-based simulation of software evolution\n", "abstract": " During the software development process, the time and resources for quality assurance are limited. Therefore, project managers benefit from knowing in advance if a decision leads to decreasing quality. For this, we build an agent-based simulation tool for software processes for testing the effect of changing parameters, e.g., development team size. Since often changed software entities tend to be more defect-prone, we analyze the evolution of common file changes and evaluate its applicability for our agent-based simulation. For the estimation of simulation parameters we performed a case study focusing on change coupling dependency graphs of open source software projects. The analysis of this also provided valuable insights in the structure of these dependencies. By comparing empirical observations with simulation results we support the assumption that file dependencies can be simulated. Moreover, we are\u00a0\u2026", "num_citations": "19\n", "authors": ["120"]}
{"title": "Testing Grid application workflows using TTCN-3\n", "abstract": " The collective and coordinated usage of distributed resources for problem solution within dynamic virtual organizations can be realized with the grid computing technology. For distributing and solving a task, a grid application involves a complex workflow of dividing a task into smaller sub-tasks, scheduling and submitting jobs for solving those sub-tasks, and eventually collecting and combining the results of the sub-tasks into a final result. The quality assurance of grid applications is a challenge due to the highly distributed nature of the grid environment in which the grid application is deployed. This paper investigates the applicability of the testing and test control notation (TTCN-3) for testing the workflows of distributed grid applications. To this aim, a case study has been created that consists of a distributed grid application which includes a typical grid application workflow; as the main contribution, this case study\u00a0\u2026", "num_citations": "19\n", "authors": ["120"]}
{"title": "The Graphical Format of TTCN-3 in the context of MSC and UML\n", "abstract": " Graphical system design techniques like Message Sequence Chart (MSC) and Unified Modelling Language (UML) are gaining more and more acceptance because they ease the development, understanding, and maintenance of software systems. In the testing area no accepted graphical test specification and implementation techniques exist. To overcome this shortcoming, a graphical presentation format for the Testing and Test Control Notation (GFT) has been defined. GFT supports the graphical design, implementation, visualization, documentation and tracing of test behaviour. GFT is based on MSC and extends it with test specific concepts like verdicts and defaults. GFT is also the basis for the definition of a UML testing profile to enable the integrated system and test development with UML models. This paper discusses GFT and its relation to MSC and to the UML testing profile.", "num_citations": "19\n", "authors": ["120"]}
{"title": "Dealing with the complexity of state space exploration algorithms for SDL systems\n", "abstract": " The treatment of complexity is one of the main problems in the area of validation and ver cation of formally speci ed protocols. The problem can be tackled by using heuristics, partial order simulation methods, and optimization strategies. We present these mechanisms and describe the way they work. Their usefulness will be discussed by presenting the results of some experiments.", "num_citations": "19\n", "authors": ["120"]}
{"title": "Grid/cloud computing interoperability, standardization and the Next Generation Network (NGN)\n", "abstract": " For telecom operators, the future lies in converging fixed, mobile and data services onto the next generation network (NGN). This paper discusses the relationship between grid and cloud computing, identifies gaps and overlaps in existing standards and identifies how grid and cloud technology could be exploited to improve the efficiency of NGN resources and to offer new \u00bfdata\u00bf services to consumers. This will enable telecom operators to manage their resources in a dynamic and optimal way by a single platform. This paper describes the approach taken by the European Telecommunications Standards Institute (ETSI) Technical Committee for Grid Computing (TC GRID) to identify gaps and overlaps in grid/cloud computing standards and to support the integration of grid/cloud computing with the NGN architecture.", "num_citations": "18\n", "authors": ["120"]}
{"title": "Quality assurance for TTCN\u20103 test specifications\n", "abstract": " Comprehensive testing of modern communication systems often requires large and complex test suites, which have to be maintained throughout the system life cycle. Industrial experience, with those written using the standardized Testing and Test Control Notation (TTCN-3), has shown that this maintenance is a non-trivial task and its burden can be reduced by means of appropriate concepts and tool support. To this aim, Motorola has collaborated with the University of G\u00f6ttingen to develop TRex, an open-source TTCN-3 development environment, which notably provides suitable metrics and refactorings to enable the assessment and automatic restructuring of test suites. This article presents concepts like metrics and refactoring for the quality assurance of TTCN-3 test suites and their implementation provided by the TRex tool. These means make it far easier to construct and maintain TTCN-3 tests that are concise\u00a0\u2026", "num_citations": "18\n", "authors": ["120"]}
{"title": "Conformance testing with TTCN\n", "abstract": " TTCN [3] is the means of the Conformance Testing Methodology and Framework (CTMF) for the description of test suites for conformance testing. See terminology and explanations in Box 1. TTCN has two syntactical forms (Figure1), called TTCN/gr (TTCN GRaphical form) and TTCN/mp (TTCN Machine Processable form). TTCN/gr is intended to be used by humans and TTCN/mp is developed for the exchange of documents between different computers and for further processing of TTCN test suites. A TTCN/gr description can be translated into an equivalent TTCN/mp representation and vice versa. In this paper only TTCN/gr examples are presented.", "num_citations": "18\n", "authors": ["120"]}
{"title": "Test case specification based on MSCs and ASN. 1\n", "abstract": " Informal test speci cations are formalized by means of MSCs. Message de nitions and constraints are included. For this purpose a new concept for the reference and modi cation of constraints is introduced. The formalized test speci cations can be implemented automatically. Our approach is explained by means of a test case for a layer 3 ISDN protocol (ITU-T Rec. Q. 931). The method is implemented in a set of prototype tools.", "num_citations": "18\n", "authors": ["120"]}
{"title": "\" Testing Quality-of-Service Aspects in Multimedia Applications\n", "abstract": " Assuring and proo ng the quality of multimedia applications and services by means of testing will be a great challenge for manufacturers and service providers. Standardized methods and tools for conformance testing are applicable to traditional protocols (ie, a single data stream, no timing requirements) only. In this paper we discuss a testing methodology and framework for testing multimedia applications. We started to develop and implement a new TeleCommunication Test Speci cation and implementation Language (TelCom TSL). TelCom TSL is meant to be a tool for specifying and implementing test cases for (distributed) multimedia applications. TelCom TSL de nes a novel testing architecture. Its formal syntax and semantics de nition with real-time extensions makes TelCom TSL applicable for testing multimedia applications. The contributions of this paper are an analysis of di erent QoS semantics in the context of multimedia applications, a de nition of QoS testing and the TelCom TSL testing architecture.", "num_citations": "17\n", "authors": ["120"]}
{"title": "A model for usage-based testing of event-driven software\n", "abstract": " Event-driven software is very diverse, e.g., in form of Graphical User Interfaces (GUIs), Web applications, or embedded software. Regardless of the application, the challenges for testing event-driven software are similar. Most event-driven systems allow a huge number of possible event sequences, which makes exhaustive testing infeasible. As a possible solution, usage-based testing has been proposed for several types of event-driven software. However, previous work has always focused on one type of event-driven software. In this paper, we propose a usage-based testing model for event-driven software in general. The model is divided into three layers to provide a maximum of platform independence while allowing interoperability with existing platform dependent solutions.", "num_citations": "16\n", "authors": ["120"]}
{"title": "TimedTTCN-3 based graphical real-time test specification\n", "abstract": " The textual Testing and Test Control Notation (TTCN-3) is frequently used in combination with Message Sequence Chart (MSC) and the MSC-based Graphical Presentation Format for TTCN-3 (GFT). Both, MSC and GFT allow an automatic generation of TTCN-3 test case descriptions.               TimedTTCN-3 is an extension of TTCN-3 for testing real-time properties and has been submitted for standardization. For a complete integration of TimedTTCN-3 into the TTCN-3-based testing process, the usage of TimedTTCN-3 in combination with MSC and GFT needs to be established.               This paper presents our approach for graphical real-time test specification based on MSC and TimedGFT, which is our real-time extension of GFT. We explain how MSC can be used for the description of real-time test purposes and define TimedGFT. Our approach includes the automatic generation of TimedTTCN-3 test cases\u00a0\u2026", "num_citations": "16\n", "authors": ["120"]}
{"title": "Autolink-a tool for the automatic and semi-automatic test generation\n", "abstract": " Due to increasing interest in validation and test generation tools, Telelogic AB, Malmo, and the Institute for Telematics of the University of Lubeck have started a research and development project which aims at bringing new test generation facilities to the Tau tool set. For that purpose, a software component is developed which supports the automatic and semi-automatic generation of TTCN test suites based on SDL specifications and MSC test cases. The project follows a pragmatic approach and is driven by practical experience. Autolink is currently used by the Project Team 100 at the European Telecommunications Standardisation Institute (ETSI), where it has to prove its usefulness in everyday work. 1 Introduction Autolink is part of Tau, a commercial product by Telelogic AB, Malmo. Tau combines the two well-known tool sets SDT and ITEX. It provides tools for the formal design, implementation and testing of communicating systems software. In particular, it supports the use of the speci...", "num_citations": "16\n", "authors": ["120"]}
{"title": "Test case generation for temporal properties\n", "abstract": " The goal of testing is to make statements about the relation between the traces of an implementation and a temporal property. This is not possible for all temporal properties. Within this paper safety and guarantee properties are identi ed to be testable temporal properties and for these testable properties a test case de nition is given. This is done by representing a safety property as a labeled transition system and by representing the guarantee property as a nite automaton. The test case de nition is applied to practical testing by using SDL descriptions to specify safety properties and by using Message Sequence Charts (MSCs) to specify guarantee properties.", "num_citations": "16\n", "authors": ["120"]}
{"title": "HyperMSC-a graphical representation of TTCN\n", "abstract": " The development of an MSC based graphical representation of TTCN is part of the ETSI project STF 156 on'Specification of a Message Sequence Chart/UML format, including validation for TTCN-3'. The most important language constructs of TTCN can be translated into corresponding MSC constructs in a straightforward manner. However, it turns out that without certain extensions and modifications, the ITU standard language MSC is not capable to produce a sufficiently transparent and readable graphical representation of TTCN. In order to arrive at a really convincing solution, HMSCs are re-interpreted in a way, which has an analogy in hypertext-like specifications. MSC references may be shown also in an expanded manner and non-expanded MSC references may contain hypertext-like descriptions instead of pure reference names. As a result, there does not exist a strict borderline between HMSCs and BMSCs any longer. Such a generalisation of MSC is really efficient only together with a corresponding tool support, which allows the smooth transition between different levels of detailed description similarly to hypertext. Therefore, the name'HyperMSC'is proposed for such extended HMSCs. The paper demonstrates how HyperMSCs arise from standard MSC descriptions in an evolutionary process of stepwise improvement and simplification.", "num_citations": "15\n", "authors": ["120"]}
{"title": "Are there any unit tests? an empirical study on unit testing in open source python projects\n", "abstract": " Unit testing is an essential practice in Extreme Programming (XP) and Test-driven Development (TDD) and used in many software lifecycle models. Additionally, a lot of literature deals with this topic. Therefore, it can be expected that it is widely used among developers. Despite its importance, there is no empirical study which investigates, whether unit tests are used by developers in real life projects at all. This paper presents such a study, where we collected and analyzed data from over 70K revisions of 10 different Python projects. Based on two different definitions of unit testing, we calculated the actual number of unit tests and compared it with the expected number (as inferred from the intentions of the developers), had a look at the mocking behavior of developers, and at the evolution of the number of unit tests. Our main findings show, (i) that developers believe that they are developing more unit tests than they\u00a0\u2026", "num_citations": "14\n", "authors": ["120"]}
{"title": "Towards the third edition of ttcn\n", "abstract": " The third edition of TTCN (Tree and Tabular Combined Notation) will be a complete redesign of the entire test specification language. The close relation between graphical and textual representation will be removed, OSI specific language constructs will be cleared away and new concepts will be introduced. The intention of this redesign is to modernize TTCN and to widen its application area beyond pure OSI conformance testing. This paper motivates the need for a new TTCN, explains the design principles and describes the status of the work on the third edition of TTCN.", "num_citations": "14\n", "authors": ["120"]}
{"title": "Partial order simulation of SDL specifications\n", "abstract": " The need of e cient simulation methods for validation and veri cation of protocol specications leads to the development of partial order simulation methods. Two new algorithms for the partial order simulation of SDL speci cations are presented. Both algorithms have shown to be useful for the automatic generation of test cases. They are implemented in the test case generation tool SaMsTaG. The results of some experiments are discussed.", "num_citations": "14\n", "authors": ["120"]}
{"title": "Scientific versus experiential evidence: discourse analysis of the chronic cerebrospinal venous insufficiency debate in a multiple sclerosis forum\n", "abstract": " Background: The vascular hypothesis of multiple sclerosis (MS), called chronic cerebrospinal venous insufficiency (CCSVI), and its treatment (known as liberation therapy) was immediately rejected by experts but enthusiastically gripped by patients who shared their experiences with other patients worldwide by use of social media, such as patient online forums. Contradictions between scientific information and lay experiences may be a source of distress for MS patients, but we do not know how patients perceive and deal with these contradictions.Objective: We aimed to understand whether scientific and experiential knowledge were experienced as contradictory in MS patient online forums and, if so, how these contradictions were resolved and how patients tried to reconcile the CCSVI debate with their own illness history and experience.Methods: By using critical discourse analysis, we studied CCSVI-related posts in the patient online forum of the German MS Society in a chronological order from the first post mentioning CCSVI to the time point when saturation was reached. For that time period, a total of 117 CCSVI-related threads containing 1907 posts were identified. We analyzed the interaction and communication practices of and between individuals, looked for the relation between concrete subtopics to identify more abstract discourse strands, and tried to reveal discourse positions explaining how users took part in the CCSVI discussion.Results: There was an emotionally charged debate about CCSVI which could be generalized to 2 discourse strands:(1) the \u201cdownfall of the professional knowledge providers\u201d and (2) the \u201crise of the\u00a0\u2026", "num_citations": "13\n", "authors": ["120"]}
{"title": "Visualisation of TTCN test cases by MSCs\n", "abstract": " Practice has shown that concurrent TTCN is suitable to describe complex distributed conformance tests. Unfortunately, the entire test behaviour may be distributed over several TTCN tables and often it is difficult to keep the overall view of the test case when reading TTCN. A combined use of MSC and TTCN may improve the readability of test cases and make them more understandable. In this paper we discuss the use of MSC for the specification of test purposes and the generation of MSCs from TTCN test cases.", "num_citations": "13\n", "authors": ["120"]}
{"title": "Message Sequence Chart: composition techniques versus OO-techniques-\u2018tema con variazioni\u2019\n", "abstract": " Structural concepts for Message Sequence Charts (MSCs), ie, composition, types, inher~ itance, and virtuality, are applied to a telecom example provided by the public switching systems division of the Siemens AG. The example contains several variations of the peripheral parts of an initial MSC which may be combined independently. The independent combinations of the peripheral variations are described by means of several new composition operators and by using object-oriented techniques (OO~ techniques), ie types, inheritance, and virtuality. A comparison of both techniques shows that composition operators may provide a compact, easy, but abstract description, whereas some OO-techniques allow a graphical, intuitive, but not compact speci\ufb01cation. Typical OO-techniques like in\u2014heritance and virtuality seem to be less fruitful for the description of at least the provided example. A combination of composition operators and OO-techniques, eg, a variant type concept employing the alternative composition operator, may combine the advantages of both techniques.", "num_citations": "13\n", "authors": ["120"]}
{"title": "Are unit and integration test definitions still valid for modern Java projects? An empirical study on open-source projects\n", "abstract": " ContextUnit and integration testing are popular testing techniques. However, while the software development context evolved over time, the definitions remained unchanged. There is no empirical evidence, if these commonly used definitions still fit to modern software development.ObjectiveWe analyze, if the existing standard definitions of unit and integration tests are still valid in modern software development contexts. Hence, we analyze if unit and integration tests detect different types of defects, as expected from the standard literature.MethodWe classify 38,782 test cases into unit and integration tests according to the definition of the IEEE and use mutation testing to assess their defect detection capabilities. All integrated mutations are classified into five different defect types. Afterwards, we evaluate if there are any statistically significant differences in the results between unit and integration tests.ResultsWe could\u00a0\u2026", "num_citations": "12\n", "authors": ["120"]}
{"title": "Using mapreduce for high energy physics data analysis\n", "abstract": " At the Large Hadron Collider (LHC) High Energy Physics (HEP) experiment at CERN, 15 PB of raw data is recorded per year. As it was considered inconvenient to store, access and process this data using the traditional hardware and software tools, this data gets reduced to 10-200 TB per year. This paper investigates the applicability of the MapReduce paradigm for analyzing HEP data. In a case study, a sample HEP analysis that makes use of the HEP analysis framework ROOT has been re-implemented using the MapReduce implementation Apache Hadoop. In addition, a Hadoop input format has been developed that takes storage locality of the ROOT file format into account. This approach was evaluated in a cloud computing environment and compared to data analysis with the Parallel ROOT Facility (PROOF).", "num_citations": "12\n", "authors": ["120"]}
{"title": "A flexible framework for quality assurance of software artefacts with applications to java, uml, and ttcn-3 test specifications\n", "abstract": " Manual reviews and inspections of software artefacts are time consuming and thus, automated analysis tools have been developed to support the quality assurance of software artefacts. Usually, software analysis tools are implemented for analysing only one specific language as target and for performing only one class of analyses. Furthermore, most software analysis tools support only common programming languages, but not those domain-specific languages that are used in a test process. As a solution, a framework for software analysis is presented that is based on a flexible, yet high-level facade layer that mediates between analysis rules and the underlying target software artefact; the analysis rules are specified using high-level XQuery expressions. Hence, further rules can be quickly added and new types of software artefacts can be analysed without needing to adapt the existing analysis rules. The\u00a0\u2026", "num_citations": "12\n", "authors": ["120"]}
{"title": "TTCN-3 quality engineering: using learning techniques to evaluate metric sets\n", "abstract": " Software metrics are an essential means to assess software quality. For the assessment of software quality, typically sets of complementing metrics are used since individual metrics cover only isolated quality aspects rather than a quality characteristic as a whole. The choice of the metrics within such metric sets, however, is non-trivial. Metrics may intuitively appear to be complementing, but they often are in fact non-orthogonal, i.e. the information they provide may overlap to some extent. In the past, such redundant metrics have been identified, for example, by statistical correlation methods. This paper presents, based on machine learning, a novel approach to minimise sets of metrics by identifying and removing metrics which have little effect on the overall quality assessment. To demonstrate the application of this approach, results from an experiment are provided. In this experiment, a set of metrics that is\u00a0\u2026", "num_citations": "12\n", "authors": ["120"]}
{"title": "From design to test with UML\n", "abstract": " The UML Testing Profile provides support for UML based model-driven testing. This paper introduces a methodology of how to use the testing profile in order to modify and extend an existing UML design model for test issues. As a case study, a new roaming algorithm for bluetooth devices has been developed at the University of L\u00fcbeck, is modelled using UML. The usability of the UML Testing Profile will be explained by applying it to this model.", "num_citations": "12\n", "authors": ["120"]}
{"title": "Model Driven Cloud Orchestration by Combining TOSCA and OCCI.\n", "abstract": " To tackle the problem of a cloud-provider lock-in, several standards have emerged in the recent years which aim to provide a unified interface to cloud resources. The Open Cloud Computing Interface (OCCI) thereby focuses on the standardization of a common API for Infrastructure-as-a-Service (IaaS) providers and the Topology and Orchestration Specification for Cloud Applications (TOSCA) focuses on the standardization of a template language to enable the proper definition of the topology of cloud applications and their orchestrations on top of an IaaS cloud. TOSCA thereby does not define how the application topologies are created on the cloud. Therefore, it is worthwhile to analyse the conceptual similarities between the two approaches and the possibilities to integrate both. In this paper, we provide an overview of the similarities between the two standardization approaches. Furthermore, we define a concept of a fully model driven cloud orchestrator based on the two standards.", "num_citations": "11\n", "authors": ["120"]}
{"title": "Hidden markov models for the prediction of developer involvement dynamics and workload\n", "abstract": " The evolution of software projects is driven by developers who are in control of the developed artifacts. When analyzing the behavior of developers, the observable behaviors are, eg, commits, messages, or bug assignments. For defining dynamic activities and workload of developers, we consider underlying characteristics, which means the level of involvement according to their role in the project. In this paper, we propose to employ Hidden Markov Models (HMMs) to model this underlying behavior given the observable behavior as input. For this, we observe monthly commits, bugfixes, mailing list activity, and bug comments for each developer over the project duration. As output we get a model for each developer describing how likely it is to be in a low, medium, or high contribution state of every point in time. As a result, we discovered that same developer types exhibit similar models in terms of state patterns and\u00a0\u2026", "num_citations": "11\n", "authors": ["120"]}
{"title": "Intuition vs. truth: Evaluation of common myths about stackoverflow posts\n", "abstract": " Posting and answering questions on Stack Overflow (SO) is everyday business for many developers. We asked a group of developers what they expect to be true about questions and answers on SO. Most of their expectations were related to the likelihood of getting an answer or to voting behavior. From their comments, we formulated nine myths that they think are true about the platform. Then, we proceeded to use rather simple methods from statistics to check if these myths are supported by the data in the SO dump provided. Through our analysis, we determined that there is an effect for eight of the nine myths the developers believed in. However, for only four of the myths the effect size is large enough to actually make a difference. Hence, we could bust five myths the developers believed in.", "num_citations": "11\n", "authors": ["120"]}
{"title": "The MIDAS cloud platform for testing SOA applications\n", "abstract": " While Service Oriented Architectures (SOAs) are for many parts deployed online, and today often in a cloud, the testing of the systems still happens mostly locally. In this paper, we want to present the MIDAS Testing as a Service (TaaS), a cloud platform for the testing of SOAs. We focus on the testing of whole SOA orchestrations, a complex task due to the number of potential service interactions and the increasing complexity with each service that joins an orchestration. Since traditional testing does not scale well with such a complex setup, we employ a Model-based Testing (MBT) approach based on the Unified Modeling Language (UML) and the UML Testing Profile (UTP) within MIDAS. Through this, we provide methods for functional testing, security testing, and usage-based testing of service orchestrations. Through harnessing the computational power of the cloud, MIDAS is able to generate and execute\u00a0\u2026", "num_citations": "11\n", "authors": ["120"]}
{"title": "Pragmatic integration of cloud and grid computing infrastructures\n", "abstract": " The integration of cloud and grid infrastructures is still of current interest, because it provides a way for the scientific area to ensure sustainability of well engineered grid applications. The integration of well established grid infrastructures with cloud systems also fosters their complementary usage, simplified migration of applications, as well as efficient resource utilization. In this paper, we compare the layered conceptual grid model to the service model of clouds. Based on this comparison, we describe pragmatic possibilities to integrate cloud and grid systems. We analyze the connectivity options on the infrastructure level to gain access to both infrastructures using a unified client. In two case studies, we show the successful integration of the Amazon Web Services cloud with UNICORE~6 and the open source cloud Eucalyptus with Globus Toolkit~4. Based on these implementations, we discuss lessons learned.", "num_citations": "11\n", "authors": ["120"]}
{"title": "A testing framework for assessing grid and cloud infrastructure interoperability\n", "abstract": " The composition of grid and cloud computing infrastructures using equipment from different vendors to allow service enrichment and increase productivity is an important need in industry and for governmental institutions. Interoperability between equipment can be achieved using the gateway approach or the standardized interface approach. These approaches, as well as equipment need to be engineered and developed with the goal to allow problem-free interoperations between involved equipment. A step towards such interoperation is the assessment of interoperability. Focusing on technical interoperability, we present a testing framework for the assessment of interoperability of grid and cloud computing infrastructures. This also includes the assessment of application deployment onto several infrastructures provided by different vendors, which is a key driver for market success. This testing framework is part of an initiative for standardizing the use of grid and cloud technology in the context of telecommunication at the European Telecommunications Standards Institute (ETSI). Following the test development process developed and used at ETSI, we developed a test architecture, test configurations, compliance levels, test purposes, interoperability test descriptions, test applications, and a test selection method that together build the testing framework. Its application is exemplified by the assessment of resource reservation and application deployment onto grid and cloud infrastructures based on standardized Grid Component Model descriptors. The presented testing framework has been applied successfully in an interoperability event. In this\u00a0\u2026", "num_citations": "11\n", "authors": ["120"]}
{"title": "Towards an integrated quality assessment and improvement approach for UML models\n", "abstract": " Models defined using the Unified Modeling Language (UML) are nowadays common parts of software documentations, specifications and sometimes even implementations. However, there is a broad variety of how UML is used. Reasons can be found, for example, in the lack of generally accepted modeling norms and guidelines, the semi-formal semantics of UML, or the complexity of the language. In practice, these factors inevitably lead to quality problems in UML models that need to be addressed.We investigate and discuss existing work in the field of quality assessment and improvement of UML models and present how we envision an integrated approach to quality assessment and improvement of UML models. We assess a model with a Factor-Criteria-Metrics (FCM) based quality model, detect issues by finding smells and violated metric thresholds in UML models, and improve UML models by\u00a0\u2026", "num_citations": "11\n", "authors": ["120"]}
{"title": "A TTCN-3-based web service test framework\n", "abstract": " The increased usage of Web services for critical applications introduces a growing need for efficient testing approaches to assure their quality. The Testing and Test Control Notation (TTCN-3) is a standardised testing language that is well suited for black-box testing of distributed systems such as Web services. Also due to its abstract test specification methodology, it allows easy adaptation to different Web service frameworks or platforms. This paper presents a mapping from the Web Service Description Language (WSDL) to TTCN-3 and a corresponding automated translator.", "num_citations": "11\n", "authors": ["120"]}
{"title": "Communication patterns for expressing real-time requirements using MSC and their application to testing\n", "abstract": " This paper introduces real-time communication patterns (RTC-patterns) for capturing real-time requirements of communication systems. RTC-patterns for some of the most common real-time requirements are presented. They are formalized by using Message Sequence Charts (MSCs). The application of RTC-patterns to testing is explained by an example. The example shows how real-time requirements which are expressed using RTC-patterns can be related to TimedTTCN-3 evaluation functions.", "num_citations": "11\n", "authors": ["120"]}
{"title": "Comparison and Runtime Adaptation of Cloud Application Topologies based on OCCI.\n", "abstract": " To tackle the cloud provider lock-in, multiple standards have emerged to enable the uniform management of cloud resources across different providers. One of them is the Open Cloud Computing Interface (OCCI) which defines, in addition to a REST API, a metamodel that enables the modelling of cloud resources on different service layers. Even though the standard defines how to manage single cloud resources, no process exists that allows for the automated provisioning of full application topologies and their adaptation at runtime. Therefore, we propose a model-based approach to adapt running cloud application infrastructures, allowing a management on a high abstraction level. Hereby, we check the differences between the runtime and target state of the topology using a model comparison, matching their resources. Based on this match, we mark each resource indicating required management calls that are systematically executed by an adaptation engine. To show the feasibility of our approach, we evaluate the comparison, as well as the adaptation process on a set of example infrastructures.", "num_citations": "10\n", "authors": ["120"]}
{"title": "Combining usage-based and model-based testing for service-oriented architectures in the industrial practice\n", "abstract": " Usage-based testing focuses quality assurance on highly used parts of the software. The basis for this are usage profiles based on which test cases are generated. There are two fundamental approaches in usage-based testing for deriving usage profiles: either the system under test (SUT) is observed during its operation and from the obtained usage data a usage profile is automatically inferred, or a usage profile is modeled by hand within a model-based testing (MBT) approach. In this article, we propose a third and combined approach, where we automatically infer a usage profile and create a test data repository from usage data. Then, we create representations of the generated tests and test data in the test model from an MBT approach. The test model enables us to generate executable Testing and Test Control Notation version 3 (TTCN-3) and thereby allows us to automate the test execution. Together\u00a0\u2026", "num_citations": "10\n", "authors": ["120"]}
{"title": "Monitoring software quality by means of simulation methods\n", "abstract": " The evolution of software projects is driven by developers who are in control of the developed artifacts and the quality of software projects depends on the work of participating developers. Thus, a simulation tool requires a suitable model of the commit behavior of different developer types. In this paper, we present an agent-based model for software processes containing the commit behavior for different developer types. The description of these types results from mining software repositories. Since relationships between software entities, eg, files, classes, modules, axe represented as dependency graphs, simulation results can be assessed automatically by Conditional Random Fields (CRFs). By adjusting simulation parameters for one project we are able to give a quality trend of other projects similar in size and duration only by changing the effort and the size of other projects to simulate.", "num_citations": "10\n", "authors": ["120"]}
{"title": "A reliability assessment framework for cloud applications\n", "abstract": " Cloud computing enables users to use computing resources, platforms and applications with reduced deployment and maintenance cost. The reliability of cloud applications becomes one of the key concerns of cloud service providers and users. Meanwhile, the deep dependency stack of layered cloud objects makes it challenging to evaluate the reliability of cloud applications. To tackle this problem, we propose a layered dependency graph-based reliability assessment framework. To verify our framework, we conduct an initial case study which shows its feasibility.", "num_citations": "10\n", "authors": ["120"]}
{"title": "HyperMSCs with Connectors for Advanced Visual System Modelling and Testing\n", "abstract": " Experiences with the use of the MSC language for complex system specifications have shown that certain extensions are necessary in order to arrive at sufficiently transparent and manageable descriptions. Extended HMSCs, where MSC reference symbols may either be presented by hypertext-like descriptions or, in an expanded form, as detailed MSCs, appear to be especially suitable for a compact and transparent MSC representation. For an effective usage of such advanced MSC constructs, a corresponding tool support seems to be mandatory where interactively the event structures of special paths can explicitly be expanded while others remain hidden as MSC references that contain solely textual descriptions. The name \u2018HyperMSCs\u2019 is proposed for such extended HMSCs. Beyond that, the communication between MSC references, operator expressions or HMSCs demands a generalisation of the gate\u00a0\u2026", "num_citations": "10\n", "authors": ["120"]}
{"title": "Development of a MSC/UML Test Format.\n", "abstract": " The development of an Message Sequence Chart (MSC) based graphical representation of the Tree and Tabular Combined Notation (TTCN) is part of the ETSI project STF 156 on'Specification of a Message Sequence Chart/UML format, including validation for TTCN-3'. Experiments with different kinds of MSC representations show that certain extensions of the MSC language are requested in order to obtain a sufficiently transparent and readable MSC test format. Extended HMSCs where reference symbols may either contain hypertext\u2013like descriptions or, in the expanded form, the detailed event structure of basic MSCs appear to be especially suitable for a compact and transparent test case representation. For an effective usage of such advanced MSC constructs, a corresponding tool support appears to be mandatory where the event structure of special paths in the test case is described explicitly while others are hidden in MSC references containing pure textual descriptions. We propose the name'HyperMSCs' for such extended HMSCs. A significant difference between the standard TTCN format and the MSC format refers to the behavior description of several test components. In the concurrent case, a TTCN test case describes the communication of the main test component (MTC) and its ports. The behavior description of parallel test components (PTCs) is provided by separate functions. A simple translation of this partitioning into MSCs leads to separate MSC specifications for the different test components which have to be merged somehow by an appropriate join operation. However, such a \u2018local\u2019view does not make use of the main\u00a0\u2026", "num_citations": "10\n", "authors": ["120"]}
{"title": "A longitudinal study of static analysis warning evolution and the effects of PMD on software quality in Apache open source projects\n", "abstract": " Automated static analysis tools (ASATs) have become a major part of the software development workflow. Acting on the generated warnings, i.e., changing the code indicated in the warning, should be part of, at latest, the code review phase. Despite this being a best practice in software development, there is still a lack of empirical research regarding the usage of ASATs in the wild. In this work, we want to study ASAT warning trends in software via the example of PMD as an ASAT and its usage in open source projects. We analyzed the commit history of 54 projects (with 112,266 commits in total), taking into account 193 PMD rules and 61 PMD releases. We investigate trends of ASAT warnings over up to 17 years for the selected study subjects regarding changes of warning types, short and long term impact of ASAT use, and changes in warning severities. We found that large global changes in ASAT warnings are\u00a0\u2026", "num_citations": "9\n", "authors": ["120"]}
{"title": "The smartshark ecosystem for software repository mining\n", "abstract": " Software repository mining is the foundation for many empirical software engineering studies. The collection and analysis of detailed data can be challenging, especially if data shall be shared to enable replicable research and open science practices. SmartSHARK is an ecosystem that supports replicable and reproducible research based on software repository mining.", "num_citations": "9\n", "authors": ["120"]}
{"title": "Trace-based task tree generation\n", "abstract": " Task trees are a well-known way for the manual modeling of user interactions. They provide an ideal basis for software analysis including usability evaluations if they are generated based on usage traces. In this paper, we present a method for the automated generation of task trees based on traces of user interactions. For this, we utilize usage monitors to record all events caused by users. These events are written into log files from which we generate task trees. We validate our method in three case studies.", "num_citations": "9\n", "authors": ["120"]}
{"title": "A Proposal for a Real-Time Extension of TTCN\n", "abstract": " In this paper we propose an extension of TTCN (Tree and Tabular Combined Notation) to real-time TTCN. The extension is defined on a syntactical, and semantical level. Syntactically, we provide facilities to annotate TTCN statements with two time values, namely an earliest execution time (EET) and a latest execution time (LET). The informal interpretation of these time values is that a TTCN statement may be executed if it has been continuously enabled for at least EET units and it must be executed if it has been continuously enabled for LETunits. The operational semantics of real-time TTCN is defined by means of timed transition systems. In timed transition systems an execution of a system is modelled by a timed state sequence which counts for time (progress of time) and state (execution of TTCN statements) activities. We define a mapping of real-time TTCN to timed transition systems and give examples\u00a0\u2026", "num_citations": "9\n", "authors": ["120"]}
{"title": "Test descriptions with ETSI TDL\n", "abstract": " To address the need for abstract, high-level test descriptions that can be shared among different stakeholders, the European Telecommunications Standards Institute (ETSI) commissioned the design of the Test Description Language (TDL). TDL is designed as a domain-specific language for testing, consisting of a standardised abstract syntax (meta-model) and concrete syntaxes for textual specification, graphical design, and model exchange between tools. Its main purpose is to support a test methodology that is followed in the standardisation work for software-intense systems at ETSI and is applicable in industrial projects as well. TDL enables the formal specification of both test objectives derived from system requirements and test descriptions refining the test objectives. The latter serve as blueprint for the implementation of executable tests. A standardised mapping of TDL specifications to test scripts in\u00a0\u2026", "num_citations": "8\n", "authors": ["120"]}
{"title": "Correction of \u201cA comparative study to benchmark cross-project defect prediction approaches\u201d\n", "abstract": " Unfortunately, the article \u201cA Comparative Study to Benchmark Cross-project Defect Prediction Approaches\u201d has a problem in the statistical analysis which was pointed out almost immediately after the pre-print of the article appeared online. While the problem does not negate the contribution of the the article and all key findings remain the same, it does alter some rankings of approaches used in the study. Within this correction, we will explain the problem, how we resolved it, and present the updated results.", "num_citations": "8\n", "authors": ["120"]}
{"title": "Towards a model-based software mining infrastructure\n", "abstract": " Software mining is concerned with two primary goals: the extraction of basic facts from software repositories and the derivation of knowledge resulting from the assessment of the basic facts. Facts extraction approaches rely on custom and task-specific infrastructures and tools. The resulting facts assets are usually represented in heterogeneous formats at a low level of abstraction. Due to this, facts extracted from different sources are also not well integrated, even if they are related. To manage this, existing infrastructures often aim at supporting an all-in-one information meta-structures which try to integrate all facts in one connected whole. We propose a generic infrastructure that translates extracted facts to homogeneous high-level representations conforming to domain-specific metamodels, and then transforms these high-level model instances to instances of domain-specific models related to a particular\u00a0\u2026", "num_citations": "8\n", "authors": ["120"]}
{"title": "UML-based modeling of roaming with bluetooth devices\n", "abstract": " Bluetooth is a standard for wireless personal area networks. The current standard does not support roaming between different bluetooth networks. In this paper, we will introduce potential roaming techniques for data transfer scenarios using Bluetooth hardware devices and present a corresponding UML model.", "num_citations": "8\n", "authors": ["120"]}
{"title": "Experiences in introducing blended learning in an introductory programming course\n", "abstract": " As students shall be able to apply their gained theoretical knowledge practically, usually exercises are offered. But assessment of the exercises is a very time-consuming task and due to the increasing number of students often even infeasible. Though programming exercises can be efficiently tested against expected output, the assessment systems often only deliver feedback regarding the correctness and sometimes additionally which test cases fail. But so far, these systems are not able to identify the reason for the error. Therefore, there should also be human tutors available with which students can discuss their solution and which can help them in identifying their misconceptions. In this paper, we describe the experiences we made when we introduced blended learning to our introductory programming course, which problems occurred, and how we plan to deal with the problems.", "num_citations": "7\n", "authors": ["120"]}
{"title": "Agent-based simulation for software development processes\n", "abstract": " Software development is a costly process and requires serious quality control on the management level: Managing a project with more than 10 programmers over several years is a highly nontrivial task. We are building tools for helping the manager to predict the future development of the project based on certain adjustable parameters.                 The main idea is to view the software process as agent-based simulation in a multiagent system (MAS). This approach requires combining three different areas: (1) mining patterns from past projects, (2) modeling the software development process in a multiagent environment, and (3) running the simulation on a scalable multiagent platform.", "num_citations": "7\n", "authors": ["120"]}
{"title": "Usability of generic software in e-research infrastructures\n", "abstract": " In e-Research Infrastructures (eRIs), software is used in diverse application contexts. To support this software is often implemented generically. The usability of software is strongly context dependent. Therefore, the use of generic software in different application contexts results in varying degress of usability depending on the concrete usage scenario. This paper focuses on the challenges of implementing usability-oriented generic software. First, we provide an introduction to generic software in the context of eRIs. Next, we offer an overview of usability and appropriate considerations in the software development process. Based on this, we demonstrate discrepancies between good usability and the application of generic software in distinct contexts. Finally, we provide a first architectural concept to address the identified challenges.", "num_citations": "7\n", "authors": ["120"]}
{"title": "Model reconstruction: mining test cases\n", "abstract": " System monitors need oracles to determine whether observed traces are acceptable. One method is to compare the observed traces to a formal model of the system. Unfortunately, such models are not always available\u2014software may be developed without generating a formal model, or the implementation deviates from the original specification. In previous work, we have proposed a learning algorithm to construct a formal model of the software from its test cases, thereby providing a means to transform test cases for offline testing into an oracle for monitoring. In this paper, we refine our learning algorithm with a set of state-merging rules that help to exploit the test cases for additional information. Using the additional information mined from the test cases, models can be learned from smaller test suites.", "num_citations": "6\n", "authors": ["120"]}
{"title": "Automated Refactoring Suggestions Using the Results of Code Analysis Tools\n", "abstract": " Static analysis tools are used for the detection of errors and other problems on . The detected problems related to the internal structure of a software can be removed by source code transformations called refactorings. To automate such source code transformations, refactoring tools are available. In modern integrated development environments, there is a gap between the static analysis tools and the refactoring tools. This paper presents an automated approach for the improvement of the internal quality of software by using the results of code analysis tools to call a refactoring tool to remove detected problems. The approach is generic, thus allowing the combination of arbitrary tools. As a proof of concept, this approach is implemented as a plug-in for the integrated development environment Eclipse.", "num_citations": "6\n", "authors": ["120"]}
{"title": "Data-driven testing\n", "abstract": " Also, during testing, we are not always concerned with checking all the data observed from the system under test. Instead, we are often only concerned with a subset of the data that is pertinent to the test. Hence, a means is needed of ignoring data that is not relevant during testing. To address these concerns, we discuss how wildcards can be used to ignore data that is not of concern during test specification and then present a view of how UML instance specification can aid the decoupling of value specification from behavior specification.", "num_citations": "6\n", "authors": ["120"]}
{"title": "TRex\u2013An Open-Source Tool for Quality Assurance of TTCN-3 Test Suites\n", "abstract": " The comprehensive test of modern communication systems leads to large and complex test suites which have to be maintained throughout the system life-cycle. Experience with those written in the standardised Testing and Test Control Notation (TTCN-3) has shown that the maintenance of test suites is a non-trivial task and its burden can be reduced with appropriate tool support. To this aim, we have developed the TRex tool, published as open-source under the Eclipse Public License, which supports the assessment and automatic restructuring of TTCN-3 test suites by providing suitable metrics and refactorings. Besides presenting TRex and its functionality, the main contribution of this paper is the discussion of complexity metrics for TTCN-3 test suites.", "num_citations": "6\n", "authors": ["120"]}
{"title": "SDL-and MSC-based specification and automated test case generation for INAP\n", "abstract": " The development of the Core INAP CS-2 standard and the corresponding conformance test suites by expert teams at the European Telecommunications Standards Institute (ETSI) are historical breakthroughs for the use of SDL and MSC within the international telecommunications standardization process. For the first time, the textual description of a standard has no priority over the corresponding SDL specification. The power of a standard SDL specification has been shown by the successful application of computer aided test generation methods for the production of the necessary standard conformance test suites. This paper introduces the Core INAP CS-2 protocol specification and describes the test generation procedure.", "num_citations": "6\n", "authors": ["120"]}
{"title": "Die Spezifikationssprachen MSC und SDL. Teil 1: Message Sequence Chart (MSC)(The Specification Languages MSC and SDL. Part 1: Message Sequence Chart (MSC))\n", "abstract": " Im Telekommunikationsbereich haben sich Message Sequence Chart (MSC) und Specification and Description Language (SDL) als wichtigste Modellierungssprachen etabliert. Dieser zweiteilige Artikel beschreibt die Konzepte und den Einsatz der beiden Sprachen. Im vorliegenden ersten Teil wird ein \u00dcberblick \u00fcber SDL und MSC gegeben und die Anforderungsspezifikation mit Hilfe von MSC beschrieben.", "num_citations": "6\n", "authors": ["120"]}
{"title": "Towards the new Test Specification and Implementation Language'TelCom TSL'\n", "abstract": " The development of multi-media and real-time applications requires the assessment of new functional and non-functional properties by conformance tests. The Tree and Tabular Combined Notation (TTCN) is not adequate for this purpose since test cases cannot be specified for most of the new requirements. For this reason, in 1994 we started to develop TelCom TSL, a new test specification and implementation language for advanced telecommunications applications. TelCom TSL provides facilities for the specification of tests for non-functional properties like timing constraints, quality of service (QoS) aspects, or synchronization of multi-media data streams. In this paper we describe the requirements for QoS testing and present some ideas for the design of TelCom TSL.", "num_citations": "6\n", "authors": ["120"]}
{"title": "The generation of TTCN test cases from MSCs\n", "abstract": " In 1992 and 1993 the University of Berne cooperates with the Siemens-Albis AG Z urich in order to develop a method which allows to generate complete TTCN test cases from MSC descriptions. The goal is reached by extending the MSC language with a few new language constructs, relating MSCs and data descriptions, and developing the algorithms for the TTCN generation. The method is implemented by a set of prototype tools. The paper starts with a short introduction (Section 1). Then the current procedure of conformance testing is examined (Section 2). The extensions of the standardized MSC language are described and the speci cation of test cases with MSCs is shown (Section 3). The algorithm for the generation of TTCN behavior descriptions is sketched (Section 4) and MSCs are related to data descriptions (Section 5). The whole method is summarized and a set of prototype tools which implements the method is presented (Section 6). Finally, a short outlook is given (Section 7).", "num_citations": "6\n", "authors": ["120"]}
{"title": "Message Sequence Chart (MSC)-A Survey of the new CCITT Language for the Description of Traces within Communication Systems\n", "abstract": " Message Sequence Charts (MSCs) are a widespread means for description and graphical visualisation of selected system runs within distributed systems, especially telecommunication systems. Various kinds of MSCs with similar expressive power are used frequently within industry and standardisation bodies. Therefore, the CCITT (Comit\u00e9 Consultatif International T\u00e9l\u00e9graphique et T\u00e9l\u00e9phonique) attempts to harmonise their use by means of the new standard language Message Sequence Chart (MSC) in 1992 [Z120]. This paper presents a motivation for the MSC standardisation. The history of the standardisation process is briefly sketched. The MSC language is introduced. Some language constructs which may need further elaboration are pointed out and possible enhancements are proposed. Finally, an approach towards the definition of a clear MSC semantics is described.", "num_citations": "6\n", "authors": ["120"]}
{"title": "Deployable capture/replay supported by internal messages\n", "abstract": " End-user software systems are usually operated using a Graphical User Interface (GUI). Therefore, the quality of the GUI greatly impacts the quality of use of a software, which makes GUI testing an important part of software quality assurance. Furthermore, bugs in the software are triggered by the users through interaction with the software's GUI. Thus, reliable usage information is required to replicate bugs, which is the first important step toward bug fixing. To support both GUI testing and bug reporting, we propose a GUI execution capturing technique that can be integrated into and deployed with software products. Our capturing technique not only captures user actions but also the internal communication between GUI objects. The captured internal communication allows further analysis of the software for debugging. Additionally, we propose a replaying mechanism based on the captures. The replay utilizes the\u00a0\u2026", "num_citations": "5\n", "authors": ["120"]}
{"title": "Model-Based X-in-the-Loop Testing.\n", "abstract": " Software-driven electronic control units (ECUs) are increasingly adopted in the creation of more secure, comfortable, and flexible systems. Unlike conventional software applications, ECUs are real-time systems that may be affected directly by the physical environment they operate in. Whereas for software applications testing with specified inputs and checking whether the outputs match the expectations are in many cases sufficient, such an approach is no longer adequate for the testing of ECUs. Because of the real-time requirements and the close interrelation with the physical environment, proper testing of ECUs must directly consider the feedback from the environment, as well as the feedback from the system under test (SUT) to generate adequate test input data and calculate the test verdict. Such simulation and testing approaches dedicated to verify feedback control systems are normally realized using so-called\u00a0\u2026", "num_citations": "5\n", "authors": ["120"]}
{"title": "An interdisciplinary practical course on the application of grid computing\n", "abstract": " This article gives an overview of the organization and realization of an interdisciplinary practical course on grid computing. Both the lecturers and the attendees came from diverse backgrounds and disciplines including computer science, physics, medicine, and the humanities. We describe the management of the course, the assignments, the infrastructure developed and experienced in this interdisciplinary practical course at the University of G\u00f6ttingen. The challenges and issues, the benefits, the expectations and their fulfillment are discussed. The experiences of the course show that diverse disciplines can be brought together to convey the benefits of grid technology by experiencing varied grid applications in production-like grid environments.", "num_citations": "5\n", "authors": ["120"]}
{"title": "The test technology TTCN-3\n", "abstract": " The Testing and Test Control Notation (TTCN-3) is a widely established test technology traditionally used in the telecommunication domain. In its new version, TTCN-3 has a wider scope and applicability. It can be applied not only for testing the conformance and interoperability of communication protocols but also for testing the functionality, interoperation and performance of software-based systems in general. Therefore, TTCN-3 is nowadays used in other domains such as automotive, railways, avionics, or security systems. This chapter introduces the concepts of the TTCN-3 language and provides examples of its practical use.", "num_citations": "5\n", "authors": ["120"]}
{"title": "Component interface description using HyperMSCs and connectors\n", "abstract": " Modelling of complex systems with message sequence charts requires several extensions in order to arrive at sufficiently transparent and manageable descriptions. Two extensions of major importance are introduced in this paper: 1) extended high level MSCs denoted as HyperMSCs are allowed to contain MSC references with hypertext-like inscriptions or in expanded form as detailed MSCs; and 2) MSC connectors are introduced in form of MSCs representing high level communication patterns between MSC components. The introduction of MSC connectors may be viewed as a generalisation of the gate concept and as a completion of the MSC language for communicating operator expressions. Moreover, MSC connectors can be employed quite generally as a communication description on a higher level of abstraction - a structural language construct which is still missing in the MSC standard. These new\u00a0\u2026", "num_citations": "5\n", "authors": ["120"]}
{"title": "Improving the Quality of test suites for conformance tests by using message sequence charts\n", "abstract": " The test of a communication system is a complex procedure which comprises several phases with various tasks. The quality of a test depends on the work in the phases and an easy and smooth transition between the phases. In the current test practice the di erent phases are mainly based on informal documents and they do not always t together properly. Therefore we intend to improve the phases and the transition between the phases. This paper shows how the use of Message Sequence Charts (MSCs) may improve the test speci cation phase and facilitate the transition to the test implementation phase. We describe the di erent phases of testing, sketch the problems of practical testing, and explain the use of MSCs for test speci cation by the example of an ISDN switching system. We show how MSC based test speci cations can be transformed into executable test cases, and present a tool set which supports the test speci cation and automates the test implementation. aUniversit at Bern, Institut f ur Informatik, L anggassstrasse 51, CH-3012 Bern, ph.+ 41 31 631 86 81, fax.+ 41 31 631 39 65 bSiemens-Albis AG, O entliche Vermittlungssysteme, Steinenschanze 2, CH-4051 Basel, ph.+ 41 61 276 71 11, fax.+ 41 61 276 76 71", "num_citations": "5\n", "authors": ["120"]}
{"title": "Relating Test Purposes to Formal Specifications: Towards a Theoretical Foundation of Practical Testing\n", "abstract": " 1 The problems of current theoretical foundations of testing are its constraint to Finite State Machines (FSMs) and its inability to be related to real black box testing. In this paper we give a theoretical foundation of practical testing. This foundation also implies a test methodology. A test generation tool which is based on this methodology will be presented at the end. CR Categories and Subject Descriptors: C. 2.0 [Computer-Communication Networks]: General; C. 2.2 [Computer-Communication Networks]: Network Protocols; D. 2.5 [Software Engineering:] Testing and Debugging General Terms: Verification, Theory, Standardization Additional Key Words: Test Generation 1 This work is performed within the F & E project, no. 233,'Conformance Testing---A Tool for the Generation of Test Cases', funded by Swiss PTT. 1 Introduction In the project'Conformance Testing---A Tool for the Generation of Test Cases' we are looking for methods to generate test cases for conformance tests. This p...", "num_citations": "5\n", "authors": ["120"]}
{"title": "A systematic mapping study of developer social network research\n", "abstract": " Developer social networks (DSNs) are a tool for the analysis of community structures and collaborations between developers in software projects and software ecosystems. Within this paper, we present the results of a systematic mapping study on the use of DSNs in software engineering research. We identified 255 primary studies on DSNs. We mapped the primary studies to research directions, collected information about the data sources and the size of the studies, and conducted a bibliometric assessment. We found that nearly half of the research investigates the structure of developer communities. Other frequent topics are prediction systems build using DSNs, collaboration behavior between developers, and the roles of developers. Moreover, we determined that many publications use a small sample size regarding the number of projects, which could be problematic for the external validity of the research. Our\u00a0\u2026", "num_citations": "4\n", "authors": ["120"]}
{"title": "Towards multi-level-simulation using dynamic cloud environments\n", "abstract": " The engineering of cyber physical systems requires holistic simulation perspectives. To cope with the complexity of these systems, we aim to provide a simulation methodology that is efficient regarding model complexity. The required holistic perspective is reached on a coarse level, which is co-simulated with multiple detailed models of some areas of the system that are of particular interest to the investigated phenomena. Which areas are thus \u201czoomed in\u201d is dynamic during a simulation run. To reflect this, the resulting Multi-Level-Simulation is deployed in a dynamic cloud environment, using the provided hardware resources in a cost-efficient manner.", "num_citations": "4\n", "authors": ["120"]}
{"title": "Extended trace-based task tree generation\n", "abstract": " Task trees are a well-known way for the manual modeling of user interactions. They provide an ideal basis for software analysis including usability evaluations if they are generated based on usage traces. In this paper, we present an approach for the automated generation of task trees based on traces of user interactions. For this, we utilize usage monitors to record all events caused by users. These events are written into log files from which we generate task trees. The generation mechanism covers the detection of iterations, of common usage sequences, and the merging of similar variants of semantically equal sequence. We validate our method in two case studies and show that it is able to generate task trees representing actual user behavior.", "num_citations": "4\n", "authors": ["120"]}
{"title": "History, status, and recent trends of the testing and test control notation version 3 (TTCN-3)\n", "abstract": " This overview article presents the Testing and Test Control Notation (TTCN-3) success story and serves as an introduction to this Special Section that contains five articles selected from the TTCN-3 user conference in 2011. The article sketches the development of TTCN-3 from its very beginning. It summarizes the current status of the language by reviewing its standardization process, available test suites, tools, and services as well as its training program. In addition, the article puts the articles selected for this Special Section into perspective, with regard to the evolution of TTCN-3 and the testing methodology in general. Last but not least, it discusses indicators for possible future developments of TTCN-3.", "num_citations": "4\n", "authors": ["120"]}
{"title": "UML-based specification and generation of executable web services\n", "abstract": " This paper presents an approach for the development of executable Web services based on model transformation techniques. The approach is based on a new Web service profile for the Unified Modeling Language (UML), which allows an efficient definition of Web service models. Such Web service models allow the generation of the source code and the corresponding platform-specific configuration files necessary in order to run the modelled Web services. The code generation is realised by means of transformation and code generation rules defined in the Xpand transformation language. The feasibility of the proposed approach for the development of executable Web services is validated by implementing a library system Web service as a case study.", "num_citations": "4\n", "authors": ["120"]}
{"title": "Testing embedded systems in the automotive industry with ttcn-3\n", "abstract": " C timers,\u25aa a set of messages,\u25aa a set of predicates that are used to characterize the properties of incoming messages, and\u25aa a set OP={snap, check, enqueue, dequeue, first, encode, decode, match} of time-consuming operations that are necessary to organize the handling of messages at ports.", "num_citations": "4\n", "authors": ["120"]}
{"title": "Model-based testing\n", "abstract": " Wikipedia [41], the free encyclopedia on the World Wide Web (www), refers to model-based testing as \u201csoftware testing where test cases are derived in whole or in part from a model that describes some (if not all) aspects of the system under test (SUT)\u201d[39]. The SUT may be something as simple as a method or class, or as complex as a complete system or a solution consisting of multiple systems. For testing, a model provides a behavioral description of the SUT. 1 This description can be processed to yield a set of test cases that can be used to determine whether the SUT conforms to a desirable property that is represented in the model. In this chapter, we identify the phases in the software development process where models are designed and describe the principles of test development based on models.", "num_citations": "4\n", "authors": ["120"]}
{"title": "Using Learning Techniques to Generate System Models for Online Testing\n", "abstract": " Today\u2019s software systems are mostly modular and have to be changeable. However, the testing of such systems becomes difficult, especially when changes are applied after deployment. One way to passively test such a system is to check whether the observed traces are accepted by a system model. In this paper, we present a method to generate a model of the System Under Test from its test cases. We adapt Angluin\u2019s algorithm for learning finite automata to the special case of learning from traces ob- tained from test cases and provide the promising results of our experiment.", "num_citations": "4\n", "authors": ["120"]}
{"title": "Model-based testing with UML applied to a roaming algorithm for Bluetooth devices\n", "abstract": " In late 2001, the Object Management Group issued a Request for Proposal to develop a testing profile for UML 2.0. In June 2003, the work on the UML 2.0 Testing Profile was finally adopted by the OMG. Since March 2004, it has become an official standard of the OMG. The UML 2.0 Testing Profile provides support for UML based model-driven testing. This paper introduces a methodology on how to use the testing profile in order to modify and extend an existing UML design model for test issues. The application of the methodology will be explained by applying it to an existing UML Model for a Bluetooth device.", "num_citations": "4\n", "authors": ["120"]}
{"title": "Autolink\u2013Putting formal test methods into practice\n", "abstract": " Autolink is a tool for automatic test generation. It allows to generate TTCN test suites based on a given SDL speci cation and MSC requirements. The rst big challenge for Autolink has been the creation of a test suite for the Intelligent Network Application Protocol at ETSI. In this paper we discuss our experience in applying Autolink to a real-life protocol and the improvements of Autolinkwhich were developed during this project. We also present future enhancements which will further ease the work of test suite developers.", "num_citations": "4\n", "authors": ["120"]}
{"title": "Static source code metrics and static analysis warnings for fine-grained just-in-time defect prediction\n", "abstract": " Software quality evolution and predictive models to support decisions about resource distribution in software quality assurance tasks are an important part of software engineering research. Recently, a fine-grained just-in-time defect prediction approach was proposed which has the ability to find bug-inducing files within changes instead of only complete changes. In this work, we utilize this approach and improve it in multiple places: data collection, labeling and features. We include manually validated issue types, an improved SZZ algorithm which discards comments, whitespaces and refactorings. Additionally, we include static source code metrics as well as static analysis warnings and warning density derived metrics as features. To assess whether we can save cost we incorporate a specialized defect prediction cost model. To evaluate our proposed improvements of the fine-grained just-in-time defect prediction\u00a0\u2026", "num_citations": "3\n", "authors": ["120"]}
{"title": "OCCI-compliant, fully causal-connected architecture runtime models supporting sensor management\n", "abstract": " The Open Cloud Computing Interface (OCCI) specification describes a service provider independent application programming interface for the management of heterogeneous cloud resources. Several implementations and tools for this interface have already been provided that allow defining, triggering and executing changes to dynamic cloud systems. With the OCCI monitoring extension that is presented in this paper it is possible to additionally manage the deployment and configuration of monitoring sensors in the cloud. It enables the representation of the sensors as well as their monitoring results in an OCCI-compliant runtime model. Therefore, the provided OCCI metamodel extension is the first OCCI-compliant architecture runtime model approach realizing a full causal connection covering not only execution but also monitoring aspects. Through the extension the OCCI runtime model becomes a knowledge\u00a0\u2026", "num_citations": "3\n", "authors": ["120"]}
{"title": "Scheduling architectures for scientific workflows in the cloud\n", "abstract": " Scientific workflows describe a sequence of tasks that together form a scientific experiment. When workflows are computation or data intensive, distributed systems are used. Especially, cloud computing has gained a lot of attention due to its flexible and scalable nature. However, most approaches set up a preconfigured computation clusters or schedule tasks to existing resources. In this paper, we propose the utilization of cloud runtime models and couple them with scientific workflows to create the required architecture of a workflow task at runtime. Hereby, we schedule the architecture state required by a workflow task in order to reduce the overall amount of data transfer and resources needed. Thus, we present an approach that does not schedule tasks to be executed on resources, but schedule architectures to be deployed at runtime for the execution of workflows.", "num_citations": "3\n", "authors": ["120"]}
{"title": "On the relatively small impact of deep dependencies on cloud application reliability\n", "abstract": " Reliability is one of the key concerns of both cloud providers and consumers, who require accurate reliability evaluation methods to develop, deploy, and maintain cloud applications. However, few works assess the reliability of cloud applications considering deep dependencies in the deployment stack. To explore the impact of deep dependencies to the reliability assessment, in this paper, we propose a layered dependency graph-based reliability assessment method for cloud applications. By introducing the inner reliability of cloud components, and combining the deep dependencies between services and physical servers, the method can assess the reliability of all components as well as the application. We implement a framework for the method and compare the assessment results of our approach against existing methods. The results show that deep dependencies have few impacts on the accuracy while can\u00a0\u2026", "num_citations": "3\n", "authors": ["120"]}
{"title": "Towards a framework for mining students' programming assignments\n", "abstract": " Due to an increasing number of students, more and more learning institutions tend to use computer-supported learning tools like online learning platforms or intelligent tutoring systems. This has opened up the opportunity to collect a huge amount of students' data. Educational Data Mining (EDM) uses mining techniques to derive information from these data about students' knowledge, behavior and experience to improve education. In this paper, we present a framework for mining programming errors of computer science students by analyzing the students' solutions to a programming assignment. The framework serves as both, a computer aided assessment tool as well as an immediate feedback tool about the learning progress of the students for the educator.", "num_citations": "3\n", "authors": ["120"]}
{"title": "TRex\n", "abstract": " Paul Baker1, Dominic Evans1, Jens Grabowski2, Helmut Neukirchen2, Benjamin Zeiss2 An Eclipse-Based Tool for TTCN-3 Editing, Refa Page 1 1 TRex Paul Baker1, Dominic Evans1, Jens Grabowski2, Helmut Neukirchen2, Benjamin Zeiss2 An Eclipse-Based Tool for TTCN-3 Editing, Refactoring and Metrics 1Motorola Labs, Viables Industrial Estate, Basingstoke, UK 2Software Engineering for Distributed Systems Group, University of G\u00f6ttingen Page 2 :9 30 W Motivation W 097 .8 W #01,.947 3 W %#0 %44 W $:22,7 :9 44 Page 3 3 Motivation W Huge legacy test suites at Motorola: \u2013 Migration to TTCN-3 \u2013 Automatic conversion of a UMTS test suite: W 60,000 lines of TTCN-3 code W Hard to read, use, re-use, and maintain. W Current TTCN-3 tools: \u2013 Editors, Compiler, Test-Execution \u2013 But: No support for improving test suites! Page 4 4 W Assess test suites, W Detect issues, W Restructure test suites. Metrics, further 5 :9 \u2026", "num_citations": "3\n", "authors": ["120"]}
{"title": "Consistency of task trees generated from website usage traces\n", "abstract": " Task trees are an established method for modeling the usage of a website as required to accomplish user tasks. They define the necessary actions and the order in which users need to perform them to reach a certain goal. Modeling task trees manually can be a laborious task, especially if a website is rather complex. In previous work, we presented a methodology for automatically generating task trees based on recorded user actions on a website. We did not verify, if the approach generates similar results for different recordings of the same website. Only if this is given, the task trees can be the basis for a subsequent analysis of the usage of a website, e.g., a usability analysis. In this paper, we evaluate our approach in this respect. For this, we generated task trees for different sets of recorded user actions of the same website and compared the resulting task trees. Our results show, that the generated task\u00a0\u2026", "num_citations": "3\n", "authors": ["120"]}
{"title": "Developer Oriented and Quality Assurance Based Simulation of Software Processes.\n", "abstract": " Software process planning involves the consideration of process based factors, eg, development strategies, but also social factors, eg, collaboration of developers. To facilitate project managers in decision making during the project, we develop an agent-based simulation tool which allows them to test different alternative future scenarios. For this, it is indispensable to understand software evolution and its influences. We cover different aspects of software evolution with models tailored towards specific questions. For the investigation of system growth, developer networks and file dependency graphs, we performed two case studies of open source projects. This way, we infer parameters close to reality and are able to compare empirical with simulated results.", "num_citations": "3\n", "authors": ["120"]}
{"title": "Towards the Usage of MBT at ETSI\n", "abstract": " In 2012 the Specialists Task Force (STF) 442 appointed by the European Telcommunication Standards Institute (ETSI) explored the possibilities of using Model Based Testing (MBT) for test development in standardization. STF 442 performed two case studies and developed an MBT-methodology for ETSI. The case studies were based on the ETSI-standards GeoNetworking protocol (ETSI TS 102 636) and the Diameter-based Rx protocol (ETSI TS 129 214). Models have been developed for parts of both standards and four different MBT-tools have been employed for generating test cases from the models. The case studies were successful in the sense that all the tools were able to produce the test suites having the same test adequacy as the corresponding manually developed conformance test suites. The MBT-methodology developed by STF 442 is based on the experiences with the case studies. It focusses on integrating MBT into the sophisticated standardization process at ETSI. This paper summarizes the results of the STF 442 work.", "num_citations": "3\n", "authors": ["120"]}
{"title": "Validating the behavioral equivalence of TTCN-3 test cases\n", "abstract": " Refactoring has been proven as useful means to improve the quality of source code. However, when improperly applied, it may introduce undesired changes to the observable behavior of the software. In this paper, an equivalence checking approach is presented to validate the behavior preservation after the application of refactoring in the domain of test cases specified using the testing and test control notation version 3 (TTCN-3). The approach is based on bisimulation and incrementally checks the observable behavior of two test cases at runtime for equivalence. The approach is implemented prototypically and sample experiments are conducted to evaluate the effectiveness of the approach.", "num_citations": "3\n", "authors": ["120"]}
{"title": "Introduction to the special section on advances in test automation: the evolution of TTCN-3\n", "abstract": " Introduction to the special section on advances in test automation: the evolution of TTCN-3", "num_citations": "3\n", "authors": ["120"]}
{"title": "Constructing Test Behavior Models using simulated system answers for the analysis of test behavior anomalies\n", "abstract": " In the standardization of test specifications, it is common that no actual systems exist against which the tests can be executed. Test specifications are devel- oped abstractly in high level languages such as the Testing and Test Control Nota- tion (TTCN-3), but they can only be executed when a separate adaptation layer is implemented. Static syntactical and semantical analyses as provided by the compiler and proper manual code reviews are the only means to find mistakes in such test spec- ifications at early stages of design. In this paper, we demonstrate that it is possible to execute abstract test specifications when the system does not exist yet. We use the information provided within the test cases to simulate answers of the system by gener- ating inverse messages to expected messages in the abstract test case. By following a specific coverage-criterion strategy, we are able to execute a sufficient amount of test paths to reverse-engineer behavioral models of test cases which can then again be used for the analyses of potential problems.", "num_citations": "3\n", "authors": ["120"]}
{"title": "Formal Approaches to Software Testing\n", "abstract": " Testing often accounts for more than 50% of the required effort during system development. The challenge for research is to reduce these costs by providing new methods for the specification and generation of high-quality tests. Experience has shown that the use of formal methods in testing represents a very important means for improving the testing process. Formal methods allow for the analysis and interpretation of models in a rigorous and precise mathematical manner. The use of formal methods is not restricted to system models only. Test models may also be examined. Analyzing system models provides the possibility of generating complete test suites in a systematic and possibly automated manner whereas examining test models allows for the detection of design errors in test suites and their optimization with respect to readability or compilation and execution time. Due to the numerous possibilities for their\u00a0\u2026", "num_citations": "3\n", "authors": ["120"]}
{"title": "TTCN-3-Eine Sprache f\u00fcr die Spezifikation und Implementierung von Testf\u00e4llen (TTCN-3-A Language for the Specification and Implementation of Test Cases)\n", "abstract": " Die Testing and Test Control Notation (TTCN-3) ist eine universelle\u00a0Sprache f\u00fcr die Beschreibung von Tests f\u00fcr verteilte Systeme.  Der vorliegende\u00a0Artikel beschreibt den Hintergrund f\u00fcr die TTCN-3 Entwicklung und erl\u00e4utert\u00a0ausgew\u00e4hlte Sprachkonstrukte anhand von einfachen Beispielen.", "num_citations": "3\n", "authors": ["120"]}
{"title": "Real-time test specification with TTCN-3\n", "abstract": " In this paper we propose an extension to the third edition of the Tree and Tabular Combined Notation (TTCN-3) to support realtime testing. Our approach is based on the separation of functional and real-time requirements in the test specification. Functional requirements are defined in form of stimuli and expected responses, ie, in form of functional TTCN-3 test cases, and real-time requirements are expressed by the relationship of points in time. During test execution the points in time are collected in form of time-stamps that can be evaluated during or after the test run. We explain all extensions to TTCN-3 necessary to support the collection and evaluation of time-stamps and show the applicability of our approach by means of an example.", "num_citations": "3\n", "authors": ["120"]}
{"title": "Comparison of an Automatically Generated and a Manually Specified Abstract Test Suite for the B-ISDN Protocol SSCOP\n", "abstract": " The test generation method SaMsTaG (SDL and MSC based test case generation) has successfully been applied to the B-ISDN ATM Adaption Layer protocol SSCOP (Service Speci c Connection Oriented Protocol). A test suite has been generated covering approximately 70% of the test purposes identi ed. In parallel to our work the ATM Forum developed another test suite for SSCOP. Unlike the test suite generated automatically by the SaMsTaG tool, this one was speci ed manually. In this paper we present a comparison looking at various aspects of the two test suites.", "num_citations": "3\n", "authors": ["120"]}
{"title": "Test Case Specification with Real-time TTCN.\n", "abstract": " The ever increasing dissemination of distributed real-time and multimedia applications makes testing of real-time constraints an important issue. The current conformance testing methodology does not de ne the means to cope with this new requirement. Especially, the test notation TTCN (Tree and Tabular Combined Notation) cannot be used to express and thus test realtime behaviour. In order to adapt the well established conformance testing methodology to this new need, we extend TTCN with real-time features. This is achieved by annotating TTCN statements with time intervals specifying the earliest and latest execution times. In this paper we introduce real-time TTCN and discuss its features and characteristics. To motivate our work, in the main part of the paper we evaluate real-time TTCN against TTCN and assess its suitability for testing real-time constraints by de ning test cases for real applications. It turns out that without additional assumptions concerning the execution speed of test systems, TTCN cannot be used for testing real-time constraints.", "num_citations": "3\n", "authors": ["120"]}
{"title": "Sometimes It's Just Sloppiness-Studying Students' Programming Errors and Misconceptions\n", "abstract": " Knowledge about students' programming errors is a valuable source to get insights into students deficiencies and misconceptions. In this paper, we use data from an introductory C programming course to identify which errors are often made by students. Previous studies often focused only on syntactic and semantic errors as they can be easily identified by compilers. Studies focusing on logic errors were often restricted to a limited set of concepts or performed for a small set of data. We manually inspect 12371 submission by 280 students and have no restrictions regarding the error types we are looking for. We classify our found errors into six categories: syntactic, conceptual, strategic, sloppiness, misinterpretation, and domain knowledge. Our results show that a big portion of errors made by students is simply caused by sloppiness. But putting sloppiness aside, students seem to have most problems with strategic\u00a0\u2026", "num_citations": "2\n", "authors": ["120"]}
{"title": "[Journal First] A Comparative Study to Benchmark Cross-Project Defect Prediction Approaches\n", "abstract": " Cross-Project Defect Prediction (CPDP) as a means to focus quality assurance of software projects was under heavy investigation in recent years. However, within the current state-of-the-art it is unclear which of the many proposals performs best due to a lack of replication of results and diverse experiment setups that utilize different performance metrics and are based on different underlying data. Within this article, we provide a benchmark for CPDP. We replicate 24 approaches proposed by researchers between 2008 and 2015 and evaluate their performance on software products from five different data sets. Based on our benchmark, we determined that an approach proposed by Camargo Cruz and Ochimizu (2009) based on data standardization performs best and is always ranked among the statistically significant best results for all metrics and data sets. Approaches proposed by Turhan et al. (2009), Menzies et\u00a0\u2026", "num_citations": "2\n", "authors": ["120"]}
{"title": "System Analysis and Modeling. Technology-Specific Aspects of Models: 9th International Conference, SAM 2016, Saint-Melo, France, October 3-4, 2016. Proceedings\n", "abstract": " This book constitutes revised papers of the proceedings of the 9th International Workshop on System Analysis and Modeling, SAM 2016, held in Saint-Melo, France, in October 2016. The 15 full papers presented were carefully reviewed and selected from 31 submissions. The contributions are organized in topical theme named: Technology-Specific Aspects of Models. The volume reflects the five sessions of the conference. The first two sessions are closely aligned with the conference theme with a session on the Internet of Things and a session on Technology-specific Aspects. The other three sessions cover aspects regarding modeling languages and model-driven development in general and were organized in the sessions Languages, Configurations and Features, and Patterns and Compilation.", "num_citations": "2\n", "authors": ["120"]}
{"title": "Improving security testing with usage-based fuzz testing\n", "abstract": " Along with the increasing importance of software systems for our daily life, attacks on these systems may have a critical impact. Since the number of attacks and their effects increases the more systems are connected, the secure operation of IT systems becomes a fundamental property. In the future, this importance will increase, due to the rise of systems that are directly connected to our environment, e.g., cyber-physical systems and the Internet of Things. Therefore, it is inevitable to find and fix security-relevant weaknesses as fast as possible. However, established automated security testing techniques such as fuzzing require significant computational effort. In this paper, we propose an approach to combine security testing with usage-based testing in order to increase the efficiency of security testing. The main idea behind our approach is to utilize that little tested parts of a system have a higher probability\u00a0\u2026", "num_citations": "2\n", "authors": ["120"]}
{"title": "Weighted Multi-Factor Multi-Layer Identification of Potential Causes for Events of Interest in Software Repositories.\n", "abstract": " Change labelling is a fundamental challenge in software evolution. Certain kinds of changes can be labeled based on directly measurable characteristics. Labels for other kinds of changes, such as changes causing subsequent fixes, need to be estimated retrospectively. In this article we present a weight-based approach for identifying potential causes for events of interest based on a cause-fix graph supporting multiple factors, such as causing a fix or a refactoring, and multiple layers reflecting different levels of granularity, such as project, file, class, method. We outline different strategies that can be employed to refine the weights distribution across the different layers in order to obtain more specific labelling at finer levels of granularity.", "num_citations": "2\n", "authors": ["120"]}
{"title": "Correction: Sources of Information and Behavioral Patterns in Online Health Forums: Observational Study\n", "abstract": " Due to necessary scheduled maintenance, the JMIR Publications website will be unavailable from Wednesday, July 01, 2020 at 8: 00 PM to 10: 00 PM EST. We apologize in advance for any inconvenience this may cause you.", "num_citations": "2\n", "authors": ["120"]}
{"title": "Mining test cases: optimization possibilities\n", "abstract": " System monitors need oracles to determine whether observed traces are acceptable. One method is to compare the observed traces to a formal model of the system. Unfortunately, such models are not always available\u2014software may be developed without generating a formal model, or the implementation deviates from the original specification. In previous work, we have proposed a learning algorithm to construct a formal model of the software from its test cases, thereby providing a means to transform test cases for offline testing into an oracle for monitoring. In this paper, we refine our learning algorithm with a set of state-merging rules that help to exploit the test cases for additional information. We discuss our approach in detail and identify optimization areas. Using the additional information mined from the test cases, models can be learned from smaller test suites.", "num_citations": "2\n", "authors": ["120"]}
{"title": "Analyzing Response Inconsistencies in Test Suites\n", "abstract": " Extensive testing of modern communicating systems often involve large and complex test suites that need to be maintained throughout the life cycle of the tested system. For this purpose, quality assurance of test suites is an inevitable task that eventually has an impact on the quality of the system under test as well. In this work, we present a means to analyze response inconsistencies in test suites. We define a response consistency relation and describe a method that identifies candidates for the analysis. Using these candidates, we find response inconsistent states. The applicability of this method is discussed for local test cases, local test cases with different response orders, and distributed test cases with concurrent behavior.", "num_citations": "2\n", "authors": ["120"]}
{"title": "Component and integration level testing\n", "abstract": " The objective of integration level testing is to test the smooth interoperation of components. Some properties may be tested statically, whereas errors related to the semantics of the communication can only be tested dynamically. For example, a compiler can statically check the correct usage of interfaces provided by a component, whereas the exchange of syntactically correct but semantically incorrect information can only be detected dynamically. Integration level testing depends heavily on the integration strategy used for assembling the whole system. Well-known integration strategies are bigbang, bottom-up, top-down, and adhoc integration:\u2022 The integration of the whole system in onestep is called big-bang integration. In this case, integration level testing starts after the finalization of all components. The problems with this approach are that the integration level testing starts very late in the development process\u00a0\u2026", "num_citations": "2\n", "authors": ["120"]}
{"title": "Die Spezifikationssprachen MSC und SDL/Teil 2: Specification and Description Language (SDL)(Specification and Description Language (SDL))\n", "abstract": " Im Telekommunikationsbereich haben sich Message Sequence Chart (MSC) und Specification and Description Language (SDL) als wichtigste Modellierungssprachen etabliert. Im ersten Teil dieses Artikels wurde die Anforderungsspezifikation mit MSC erl\u00e4utert. Der vorliegende zweite Teil geht auf die Beschreibung von verteilten Systemen mit SDL ein und skizziert den Einsatz von modernen Software-Werkzeugen.", "num_citations": "2\n", "authors": ["120"]}
{"title": "A Message Sequence Chart-Profile for Graphical Test Specification, Development and Tracing\u2013Graphical Presentation Format for TTCN-3\n", "abstract": " Recently, international standard bodies approved the third evolution of the TTCN test specification language (TTCN-3) as a requirement to modernise and widen its application beyond pure OSI conformance testing. Even though TTCN-3 makes the description of complex distributed test behaviour much easier there is still a requirement from the user community to provide a visualisation means for test specification, development and tracing. Message Sequence Charts (MSC) appeared to be a particularly attractive candidate as a graphical means for visualising TTCN-3 test cases. Therefore, in addition to the pure textual core language, TTCN-3 also defines a MSC profile called the Graphical Presentation Format for TTCN (GPF). This paper describes some of the main concepts of GPF, its use and application to real test suite examples.", "num_citations": "2\n", "authors": ["120"]}
{"title": "The standardization of Core INAP CS-2 by ETSI\n", "abstract": " The development of the Core INAP CS-2 standard and the corresponding conformance test suites by expert teams at the European Telecommunications Standards Institute (ETSI) are historical breakthroughs for the use of SDL and MSC within the international telecommunications standardization process. For the rst time, the textual description of a standard has no priority over the corresponding SDL specication, which in the case of Core INAP CS-2 is published as a normative annex to the protocol standard. The power of a standard SDL speci cation has been shown by the successful application of computer aided test generation methods for the production of the necessary standard conformance test suites. This paper introduces the Core INAP CS-2 protocol speci cation and describes the test generation procedure.", "num_citations": "2\n", "authors": ["120"]}
{"title": "Quality-of-Service Testing Specifying Functional QoS Testing Requirements by using Message Sequence Charts and TTCN\n", "abstract": " With the upcoming use of multimedia systems, quality-of-service (QoS) architectures and QoS testing have become a research topic. QoS testing refers to assessing the behavior of a system performing monitoring of negotiated QoS parameter values. QoS testing extends OSI conformance testing since non-functional properties have to be tested, too. We identify requirements for a QoS test speci cation language and evaluate existing languages against these requirements. We propose the combined use of TTCN (Tree and Tabular Combined Notation) and MSC (Message Sequence Chart) for specifying functional QoS testing requirements. In our approach we use MSC for a high-level overview of the test system components and their respective interactions that are de ned in a TTCN test suite. For specifying non-functional requirements, eg, time constraints, we elaborate on a real-time version of TTCN.", "num_citations": "2\n", "authors": ["120"]}
{"title": "Combining MSCs and Data Descriptions in order to Generate Executable Test Cases for ISDN Systems\n", "abstract": " A method for the automatic implementation of test cases from their speci cation is presented. For the speci cation MSCs are used. Special care is taken over the inclusion of message de nitions and constraints. For this purpose a new concept for the reference and modi cation of constraints is introduced. The whole method has been implemented in a set of prototype tools.", "num_citations": "2\n", "authors": ["120"]}
{"title": "Die SAMSTAG Methode und ihre Rolle im OSI Konformit\u00e4tstesten\n", "abstract": " F ur das Konformit atstesten von OSI Protokollen ist die automatische Generierung von Testf allen aus formalen Spezi kationen ein bisher nur unzureichend gel ostes Problem. Von wissenschaftlichen Institutionen wurden zwar eine ganze Reihe von Methoden zur Testfallgenerierung entwickelt; sie sind jedoch in der Praxis kaum einsetzbar. Zum einen sind reale Systeme h au g so komplex, dass sie mit diesen Methoden nicht mehr testbar sind. Zum anderen hat man sich bei der Methodenentwicklung nur wenig an dem in der Praxis ublichen Vorgehen beim OSI Konformit atstesten orientiert. Dieses Vorgehen ist insbesondere im internationalen ISO/IEC Standard 9646 (ISO/IEC IS 9646) 10] beschrieben. Mit der von uns entwickelten SAMSTAG Methode lassen sich ebenfalls Testf alle f ur Konformit atstests generieren. Im Gegensatz zu anderen Methoden orientiert sich die SAMSTAG Methode eng an ISO/IEC IS 9646. In diesem Artikel werden die angesprochenen Probleme ausf uhrlich diskutiert und die SAMSTAG Methode eingef uhrt. Zus atzlich zeigen wir, wie sich ISO/IEC IS 9646, die in wissenschaftlichen Institutionen entwickelten Methoden und die SAMSTAG Methode bei einer vollst andigen formalen Erkl arung des OSI Konformit atstestens erg anzen k onnen.", "num_citations": "2\n", "authors": ["120"]}
{"title": "Investigation and prediction of open source software evolution using automated parameter mining for agent-based simulation\n", "abstract": " To guide software development, the estimation of the impact of decision making on the development process can be helpful in planning. For this estimation, often prediction models are used which can be learned from project data. In this paper, an approach for the usage of agent-based simulation for the prediction of software evolution trends is presented. The specialty of the proposed approach lies in the automated parameter estimation for the instantiation of project-specific simulation models. We want to assess how well a baseline model using average (commit) behavior of the agents (ie, the developers) performs compared to models where different amount of project-specific data is fed into the simulation model. The approach involves the interplay between the mining framework and simulation framework. Parameters to be estimated include, eg, file change probabilities of developers and the team constellation\u00a0\u2026", "num_citations": "1\n", "authors": ["120"]}
{"title": "Facilitating the Co-evolution of Standards and Models\n", "abstract": " The Information Model (IM) specified by the Network Function Virtualisation (NFV) Industry Specification Group (ISG) at the European Telecommunications Standards Institute (ETSI) provides a consolidated view of all information elements used in the various interfaces defined in the NFV standards. Its purpose is to enable quick identification of gaps and inconsistencies in the standards and in implementations of the standards. As the standards are increasing in volume, manual approaches for ensuring their consistency and their co-evolution with the IM are becoming unsustainable, especially considering the rapid release cycles. In this article, we present a model-based approach for facilitating the co-evolution of standards and models and the current state of its prototypical implementation put into place to support the work within the NFV Interfaces and Architecture (IFA) working group. The initial results\u00a0\u2026", "num_citations": "1\n", "authors": ["120"]}
{"title": "Dynamic Management of Multi-level-simulation Workflows in the Cloud\n", "abstract": " Executing dynamic simulations in a distributed environment allows saving resources and time which is a desired goal in research and industry. One example dynamic simulation is the multi-level-simulation. Here, specific parts of the simulation can be inspected on different levels of detail at runtime. To cope with the changing simulation requirements an elastic and scalable infrastructure is required, as well as an approach adjusting the infrastructure to the simulation needs. In this paper, we enhance a former approach coupling workflows with architectural needs to utilize monitored runtime information and support decision making. Moreover, we demonstrate the concept of executing dynamic simulations over a workflow based approach by dynamically choosing the levels of detail within a supply chain multi-level-simulation.", "num_citations": "1\n", "authors": ["120"]}
{"title": "Experiences with replicable experiments and replication kits for software engineering research\n", "abstract": " Replications and replicable research are currently gaining traction in the software engineering research community. Our research group made an effort in the recent years to make our own research accessible for other researchers, through the provision of replication kits that allow rerunning our experiments. Within this chapter, we present our experiences with replication kits. We first had to learn which contents are required, how to structure them, how to document them, and also how to best share them with other researchers. While this sounds very straightforward, there are many small potential mistakes, which may have a strong negative impact on the usefulness and long-term availability of replication kits. We derive best practices for the content and the sharing of replication kits based on our experiences. Moreover, we outline how platforms for replicable research may further help our community, especially with\u00a0\u2026", "num_citations": "1\n", "authors": ["120"]}
{"title": "Assessing Simulated Software Graphs Using Conditional Random Fields\n", "abstract": " In the field of software evolution, simulating the software development process is an important tool to understand the reasons why some projects fail, yet others prosper. For each simulation however, there is a need to have an assessment of the simulation results. We use Conditional Random Fields, specifically a variant form based on the Ising model from theoretical physics, to assess software graph quality. Our CRF-based assessment model works on so called Software Graphs, where each node of that graph represents a software entity of the software project. The edges are determined by immediate dependencies between the pieces of software underlying the involved nodes.               Because there is a lack of reference training data for our kind of evaluation, we engineered a special training paradigm that we call the Parsimonious Homogeneity Training. This training is not dependent on reference data\u00a0\u2026", "num_citations": "1\n", "authors": ["120"]}
{"title": "Simulating Software Refactorings Based on Graph Transformations\n", "abstract": " We aim to simulate software processes in order to predict the structural evolution of software graphs and assure higher software quality. To make our simulation and therefore the results more accurate, we need to model real world practices. In this paper, we consider the specific problem of including software refactorings in our simulation. We describe these refactorings as graph transformations and apply parameters we collected from open source projects.", "num_citations": "1\n", "authors": ["120"]}
{"title": "Automated Deployment and Parallel Execution of Legacy Applications in Cloud Environments (Short Paper)\n", "abstract": " Cloud Computing has long extended beyond the original focus of providing scalable on-demand resources for web applications and is now also ubiquitous in batch-style data processing applications. Employing Cloud services for data analysis tasks is also a viable alternative for researchers who are limited by their locally available compute power and in the need for timely execution. However, the provisioning and deployment of machines and applications at Infrastructure-as-a-Service (IaaS) providers is a non-trivial task for the average scientist. Within this paper, we propose a framework for automating the deployment and execution of existing applications in a data-parallel fashion in Cloud environments with only negligible effort by the user. Our evaluation of a real-world scientific use case exhibits a significant speedup compared to local execution.", "num_citations": "1\n", "authors": ["120"]}
{"title": "Quantifying the evolution of TTCN-3 as a language\n", "abstract": " Ten years of maintenance, nine published revisions of the standards for the Testing and Test Control Notation version 3 (TTCN-3), more than 500 change requests since 2006, and 10 years of activity on the official TTCN-3 mailing list add up to a rich history, not unlike that of many successful Open Source Software (OSS) projects. In this article, we contemplate TTCN-3 in the context of software evolution and examine its history quantitatively. We mined the changes in the textual content of the standards, the data in change requests from the past 5 years, and the mailing list archives from the past 10 years. In addition, to characterize the use of the TTCN-3 we investigated the meta-data of the contributions at the TTCN-3 User Conference, and the use of language constructs in a large-scale TTCN-3 test suite. Based on these data sets, we first analyze the amount, density, and location of changes within the\u00a0\u2026", "num_citations": "1\n", "authors": ["120"]}
{"title": "Retrospective Analysis of Software Projects using k-Means Clustering\n", "abstract": " Software projects are usually analyzed by experts based on their previous experience, their intuition and data they gather about the project. In this work, we show an approach for a purely data-driven retrospective project analysis. We plan to build on this work to make predictions about the evolution of software projects.", "num_citations": "1\n", "authors": ["120"]}
{"title": "User-Interface Testing\n", "abstract": " Most systems include at least one UI. Yet testing systems with these interfaces remains a challenge for test automation. A UI can encompass many different forms of media, for example physical (ie, buttons/switches), graphical interfaces, speech, audio, biometrics. This variation introduces a number of issues. For example, the level of abstraction that is used to control and observe a graphical UI will depend on the level of integration between the test system and the application and/or underlying graphics system. If no integration is provided, then test control and observation are based on the rendered inputs and outputs of the system, for example, captured images of the graphical interface. However, if some integration is provided, then test control and observation can be abstracted away from rendered images, thereby improving the resilience of tests and reducing test maintenance. For example, if the requirements for\u00a0\u2026", "num_citations": "1\n", "authors": ["120"]}
{"title": "Test Execution with JUnit\n", "abstract": " JUnit 4.0 is an updated version of the popular JUnit framework. The new release is a significant departure from earlier versions, which relied on naming conventions, subclassing, reflection, and similar Java constructs to enable the recognition and execution of test cases. JUnit 4.0 takes advantage of key Java 5 features (particularly annotation) to identify test cases and their related constructs. Understanding and using the concepts require an understanding of the annotation capabilities available in Java 5.0 (JUnit 4.0 requires a Java 5.0 JDK to run). Java 5.0 has a general purpose annotation facility that allows programmers to define annotation types, use annotations in declarations. It also provides APIs for reading annotations, a class-based representation for annotations, and an annotation processing facility. For further information on annotations, please see [14].In order to understand JUnit 4.0, we need to\u00a0\u2026", "num_citations": "1\n", "authors": ["120"]}
{"title": "Die Spezifikationssprachen MSC und SDL. at\n", "abstract": " In the telecommunication area, Message Sequence Chart (MSC) and Specification and Description Language (SDL) are the dominating modeling languages. In the first part of the article, requirement specification with MSC has been illustrated. In this part, the description of distributed systems with SDL and the use of modern software tools is considered.", "num_citations": "1\n", "authors": ["120"]}
{"title": "Towards the Generation of Distributed Test Cases Using Petri Nets.\n", "abstract": " Test case generation is a means to validate the implementation of a system a posteriori with respect to some given requirements imposed on the system. Current methods for the generation of test cases often rely on an interleaving model of the system, which does not fully cover the situation in reactive and distributed systems. In this paper we present an approach towards the generation of distributed test cases, using Petri nets as the class of models for system, requirements and test case.", "num_citations": "1\n", "authors": ["120"]}
{"title": "Formal Methods and Conformance Testing--or--What are we testing anyway?\n", "abstract": " In this paper, we will show the correlation between the notion of implementation relations known from formal methods and ideas of conformance testing. We will show that the implementation relations realized through the practical testing of systems come from a family of parameterized implementation relations. We will also show that for glass box testing, implementation relations parameterized by test purposes converge to the may testing preorder of DeNicola and Hennessy 7], while for black box testing, implementation relations parameterized by test cases converge to a may testing preorder of the behavior visible at the interface to the environment.", "num_citations": "1\n", "authors": ["120"]}
{"title": "Revised comparison of an automatically generated and a manually specified test suite for the B-ISDN protocol SSCOP\n", "abstract": " The test generation method SaMsTaG (SDL and MSC based test case generation) has successfully been applied to the B-ISDN ATM Adaption Layer protocol SSCOP (Service Speci c Connection Oriented Protocol). In parallel to our work the ATM Forum developed another test suite for SSCOP. Unlike the test suite generated automatically by the SaMs-TaG tool, this one was speci ed manually. Both test suites have been compared, but the results were only of restricted value because the test suites base on di erent test architectures. In order to achieve more signi cant comparison results the SaMsTaG tool has been adapted to the test method chosen by the ATM Forum, ie, the remote test method, and the test suite has been re-generated. In this paper we present a revised comparison of various aspect of the two test suites.", "num_citations": "1\n", "authors": ["120"]}
{"title": "Generating Test Cases for Infinite System Specifications.\n", "abstract": " Test case generation is a means to validate the implementation of a system a posteriori with respect to some given requirements imposed on the system. Current methods for the generation of test cases often rely on the system speci cation being given as a nite automaton, which does not fully cover the situation in systems with asynchronous communication, In this paper we present an algorithm for the computation of test cases for in nite system speci cations.", "num_citations": "1\n", "authors": ["120"]}
{"title": "An Automata based Model for the Implementation of a TTCN Simulator\n", "abstract": " This technical report provides a theoretical basis for the implementation of a TTCN test case simulator. In order to base the implementation on existing code, an automata based model has been chosen, ie, in the model a TTCN test case shows an automaton like behavior. The procedure is the following: TTCN test cases are transformed into an internal structure, which afterwards can be interpreted by using the provided enabling conditions and execution rules. The model covers control and data ow aspects of normal and concurrent TTCN.", "num_citations": "1\n", "authors": ["120"]}