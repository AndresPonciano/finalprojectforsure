{"title": "Model checking\n", "abstract": " An expanded and updated edition of a comprehensive presentation of the theory and practice of model checking, a technology that automates the analysis of complex systems. Model checking is a verification technology that provides an algorithmic means of determining whether an abstract model\u0393\u00c7\u00f6representing, for example, a hardware or software design\u0393\u00c7\u00f6satisfies a formal specification expressed as a temporal logic formula. If the specification is not satisfied, the method identifies a counterexample execution that shows the source of the problem. Today, many major hardware and software companies use model checking in practice, for verification of VLSI circuits, communication protocols, software device drivers, real-time embedded systems, and security algorithms. This book offers a comprehensive presentation of the theory and practice of model checking, covering the foundations of the key algorithms in depth. The field of model checking has grown dramatically since the publication of the first edition in 1999, and this second edition reflects the advances in the field. Reorganized, expanded, and updated, the new edition retains the focus on the foundations of temporal logic model while offering new chapters that cover topics that did not exist in 1999: propositional satisfiability, SAT-based model checking, counterexample-guided abstraction refinement, and software model checking. The book serves as an introduction to the field suitable for classroom use and as an essential guide for researchers.", "num_citations": "14360\n", "authors": ["50"]}
{"title": "Counterexample-guided abstraction refinement\n", "abstract": " We present an automatic iterative abstraction-refinement methodology in which the initial abstract model is generated by an automatic analysis of the control structures in the program to be verified. Abstract models may admit erroneous (or \u0393\u00c7\u00a3spurious\u0393\u00c7\u00a5) counterexamples. We devise new symbolic techniques which analyze such counterexamples and refine the abstract model correspondingly. The refinement algorithm keeps the size of the abstract state space small due to the use of abstraction functions which distinguish many degrees of abstraction for each program variable. We describe an implementation of our methodology in NuSMV. Practical experiments including a large Fujitsu IP core design with about 500 latches and 10000 lines of SMV code confirm the effectiveness of our approach.", "num_citations": "2295\n", "authors": ["50"]}
{"title": "Counterexample-guided abstraction refinement for symbolic model checking\n", "abstract": " The state explosion problem remains a major hurdle in applying symbolic model checking to large hardware designs. State space abstraction, having been essential for verifying designs of industrial complexity, is typically a manual process, requiring considerable creativity and insight.In this article, we present an automatic iterative abstraction-refinement methodology that extends symbolic model checking. In our method, the initial abstract model is generated by an automatic analysis of the control structures in the program to be verified. Abstract models may admit erroneous (or \"spurious\") counterexamples. We devise new symbolic techniques that analyze such counterexamples and refine the abstract model correspondingly. We describe aSMV, a prototype implementation of our methodology in NuSMV. Practical experiments including a large Fujitsu IP core design with about 500 latches and 10000 lines of SMV\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1120\n", "authors": ["50"]}
{"title": "Modular verification of software components in C\n", "abstract": " We present a new methodology for automatic verification of C programs against finite state machine specifications. Our approach is compositional, naturally enabling us to decompose the verification of large software systems into subproblems of manageable complexity. The decomposition reflects the modularity in the software design. We use weak simulation as the notion of conformance between the program and its specification. Following the counterexample guided abstraction refinement (CEGAR) paradigm, our tool MAGIC first extracts a finite model from C source code using predicate abstraction and theorem proving. Subsequently, weak simulation is checked via a reduction to Boolean satisfiability. MAGIC has been interfaced with several publicly available theorem provers and SAT solvers. We report experimental results with procedures from the Linux kernel, the OpenSSL toolkit, and several industrial\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "659\n", "authors": ["50"]}
{"title": "Handbook of model checking\n", "abstract": " This handbook is intended to give an in-depth description of the many research areas that make up the expanding field of model checking. In 32 chapters, 76 of the world\u0393\u00c7\u00d6s leading researchers in this domain present a thorough review of the origins, theory, methods, and applications of model checking. The book is meant for researchers and graduate students who are interested in the development of formalisms, algorithms, and software tools for the computer-aided verification of complex systems in general, and of hardware and software systems in particular. The idea for this handbook originated with Helmut Veith around 2006. It was clear to Helmut that a field as strong and useful as model checking needed a handbook to make its foundations broadly accessible. Helmut was in many ways the soul of this project. His untimely death in March 2016, with the project in its final phase, was a shock to all of us and we\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "308\n", "authors": ["50"]}
{"title": "Progress on the state explosion problem in model checking\n", "abstract": " Model checking is an automatic verification technique for finite state concurrent systems. In this approach to verification, temporal logic specifications are checked by an exhaustive search of the state space of the concurrent system. Since the size of the state space grows exponentially with the number of processes, model checking techniques based on explicit state enumeration can only handle relatively small examples. This phenomenon is commonly called the \u0393\u00c7\u00a3State Explosion Problem\u0393\u00c7\u00a5. Over the past ten years considerable progress has been made on this problem by (1) representing the state space symbolically using BDDs and by (2) using abstraction to reduce the size of the state space that must be searched. As a result model checking has been used successfully to find extremely subtle errors in hardware controllers and communication protocols. In spite of these successes, however, additional\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "307\n", "authors": ["50"]}
{"title": "Efficient filtering in publish-subscribe systems using binary decision diagrams\n", "abstract": " Implicit invocation or publish-subscribe has become an important architectural style for large-scale system design and evolution. The publish-subscribe style facilitates developing large-scale systems by composing separately developed components because the style permits loose coupling between various components. One of the major bottlenecks in using publish-subscribe systems for very large scale systems is the efficiency of filtering incoming messages, i.e., matching of published events with event subscriptions. This is a very challenging problem because in a realistic publish subscribe system the number of subscriptions can be large. We present an approach for matching published events with subscriptions which scales to a large number of subscriptions. Our approach uses binary decision diagrams, a compact data structure for representing Boolean functions which has been successfully used in\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "245\n", "authors": ["50"]}
{"title": "Detecting malicious code by model checking\n", "abstract": " The ease of compiling malicious code from source code in higher programming languages has increased the volatility of malicious programs: The first appearance of a new worm in the wild is usually followed by modified versions in quick succession. As demonstrated by Christodorescu and Jha, however, classical detection software relies on static patterns, and is easily outsmarted. In this paper, we present a flexible method to detect malicious code patterns in executables by model checking. While model checking was originally developed to verify the correctness of systems against specifications, we argue that it lends itself equally well to the specification of malicious code patterns. To this end, we introduce the specification language CTPL (Computation Tree Predicate Logic) which extends the well-known logic CTL, and describe an efficient model checking algorithm. Our practical experiments\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "222\n", "authors": ["50"]}
{"title": "Jakstab: A static analysis platform for binaries\n", "abstract": " For processing compiled code, model checkers require accurate model extraction from binaries. We present our fully configurable binary analysis platform Jakstab, which resolves indirect branches by multiple rounds of disassembly interleaved with dataflow analysis. We demonstrate that this iterative disassembling strategy achieves better results than the state-of-the-art tool IDA Pro.", "num_citations": "190\n", "authors": ["50"]}
{"title": "Automated abstraction refinement for model checking large state spaces using SAT based conflict analysis\n", "abstract": " We introduce a SAT based automatic abstraction refinement framework for model checking systems with several thousand state variables in the cone of influence of the specification. The abstract model is constructed by designating a large number of state variables as invisible. In contrast to previous work where invisible variables were treated as free inputs we describe a computationally more advantageous approach in which the abstract transition relation is approximated by pre-quantifying invisible variables during image computation. The abstract counterexamples obtained from model-checking the abstract model are symbolically simulated on the concrete system using a state-of-the-art SAT checker. If no concrete counterexample is found, a subset of the invisible variables is reintroduced into the system and the process is repeated. The main contribution of this paper are two new algorithms for\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "187\n", "authors": ["50"]}
{"title": "Tree-like counterexamples in model checking\n", "abstract": " Counter examples for specification violations provide engineers with important debugging information. Although counterexamples are considered one of the main advantages of model checking, state-of the art model checkers are restricted to relatively simple counterexamples, and surprisingly little research effort has been put into counterexamples. In this paper, we introduce a new general framework for counterexamples. The paper has three main contributions: (i) We determine the general form of ACTL counterexamples. To this end, we investigate the notion of counterexample and show that a large class of temporal logics beyond ACTL admits counterexamples with a simple tree-like transition relation. We show that the existence of tree-like counterexamples is related to a universal fragment of extended branching time logic based on w-regular temporal operators. (ii) We present new symbolic algorithms to\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "187\n", "authors": ["50"]}
{"title": "Secure two-party computations in ANSI C\n", "abstract": " The practical application of Secure Two-Party Computation is hindered by the difficulty to implement secure computation protocols. While recent work has proposed very simple programming languages which can be used to specify secure computations, it is still difficult for practitioners to use them, and cumbersome to translate existing source code into this format. Similarly, the manual construction of two-party computation protocols, in particular ones based on the approach of garbled circuits, is labor intensive and error-prone.", "num_citations": "146\n", "authors": ["50"]}
{"title": "An abstract interpretation-based framework for control flow reconstruction from binaries\n", "abstract": " Due to indirect branch instructions, analyses on executables commonly suffer from the problem that a complete control flow graph of the program is not available. Data flow analysis has been proposed before to statically determine branch targets in many cases, yet a generic strategy without assumptions on compiler idioms or debug information is lacking.               We have devised an abstract interpretation-based framework for generic low level programs with indirect jumps which safely combines a pluggable abstract domain with the notion of partial control flow graphs. Using our framework, we are able to show that the control flow reconstruction algorithm of our disassembly tool Jakstab produces the most precise overapproximation of the control flow graph with respect to the used abstract domain.", "num_citations": "141\n", "authors": ["50"]}
{"title": "Environment abstraction for parameterized verification\n", "abstract": " Many aspects of computer systems are naturally modeled as parameterized systems which renders their automatic verification difficult. In well-known examples such as cache coherence protocols and mutual exclusion protocols, the unbounded parameter is the number of concurrent processes which run the same distributed algorithm. In this paper, we introduce environment abstraction as a tool for the verification of such concurrent parameterized systems. Environment abstraction enriches predicate abstraction by ideas from counter abstraction; it enables us to reduce concurrent parameterized systems with unbounded variables to precise abstract finite state transition systems which can be verified by a finite state model checker. We demonstrate the feasibility of our approach by verifying the safety and liveness properties of Lamport\u0393\u00c7\u00d6s bakery algorithm and Szymanski\u0393\u00c7\u00d6s mutual exclusion algorithm. To the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "133\n", "authors": ["50"]}
{"title": "A simple and scalable static analysis for bound analysis and amortized complexity analysis\n", "abstract": " We present the first scalable bound analysis that achieves amortized complexity analysis. In contrast to earlier work, our bound analysis is not based on general purpose reasoners such as abstract interpreters, software model checkers or computer algebra tools. Rather, we derive bounds directly from abstract program models, which we obtain from programs by comparatively simple invariant generation and symbolic execution techniques. As a result, we obtain an analysis that is more predictable and more scalable than earlier approaches. We demonstrate by a thorough experimental evaluation that our analysis is fast and at the same time able to compute bounds for challenging loops in a large real-world benchmark. Technically, our approach is based on lossy vector addition systems (VASS). Our bound analysis first computes a lexicographic ranking function that proves the termination of a VASS, and\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "117\n", "authors": ["50"]}
{"title": "Decidability of parameterized verification\n", "abstract": " While the classic model checking problem is to decide whether a finite      system satisfies a specification, the goal of parameterized model      checking is to decide, given finite systems M(n)      parameterized by n \u0393\u00ea\u00ea \u0393\u00e4\u00f2, whether, for all n \u0393\u00ea\u00ea \u0393\u00e4\u00f2, the system M(n) satisfies a specification. In this book we consider the important case of M(n) being a      concurrent system, where the number of replicated processes      depends on the parameter n but each process is independent      of n. Examples are cache coherence protocols,  networks of finite-state      agents, and systems that solve mutual exclusion or scheduling      problems. Further examples are abstractions      of systems, where the processes of the original systems actually      depend on the parameter.  The literature in this area has studied a wealth of computational      models based on a variety of synchronization and communication      primitives, including token\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "114\n", "authors": ["50"]}
{"title": "Datalog LITE: A deductive query language with linear time model checking\n", "abstract": " We present Datalog LITE, a new deductive query language with a linear-time model-checking algorithm, that is, linear time data complexity and program complexity. Datalog LITE is a variant of Datalog that uses stratified negation, restricted variable occurrences and a limited form of universal quantification in rule bodies.Despite linear-time evaluation, Datalog LITE is highly expressive: It encompasses popular modal and temporal logics such as CTL or the alternation-free \u256c\u255d-calculus. In fact, these formalisms have natural presentations as fragments of Datalog LITE. Further, Datalog LITE is equivalent to the alternation-free portion of guarded fixed-point logic. Consequently, linear-time model checking algorithms for all mentioned logics are obtained in a unified way.The results are complemented by inexpressibility proofs to the effect that linear-time fragments of stratified Datalog have too limited expressive power.", "num_citations": "98\n", "authors": ["50"]}
{"title": "Verification by network decomposition\n", "abstract": " We describe a new method to verify networks of homogeneous processes which communicate by token passing. Given an arbitrary network graph and an indexed LTL \u0393\u00ea\u00fb X property, we show how to decompose the network graph into multiple constant size networks, thereby reducing one model checking call on a large network to several calls on small networks. We thus obtain cut-offs for arbitrary classes of networks, adding to previous work by Emerson and Namjoshi on the ring topology. Our results on LTL \u0393\u00ea\u00fb X are complemented by a negative result which precludes the existence of reductions for CTL \u0393\u00ea\u00fb X on general networks.", "num_citations": "87\n", "authors": ["50"]}
{"title": "Counterexamples revisited: Principles, algorithms, applications\n", "abstract": " Algorithmic counterexample generation is a central feature of model checking which sets the method apart from other approaches such as theorem proving. The practical value of counterexamples to the verification engineer is evident, and for many years, counterexample generation algorithms have been employed in model checking systems, even though they had not been subject to an adequate fundamental investigation. Recent advances in model checking technology such as counterexample-guided abstraction refinement have put strong emphasis on counterexamples, and have lead to renewed interest both in fundamental and pragmatic aspects of counterexample generation. In this paper, we survey several key contributions to the subject including symbolic algorithms, results about the graph-theoretic structure of counterexamples, and applications to automated abstraction as well as software\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "83\n", "authors": ["50"]}
{"title": "Modular logic programming and generalized quantifiers\n", "abstract": " The research on systems of logic programming with modules has followed two mainstreams, programming-in-the-large, where compositional operators are provided for combining separate and independent modules, and programming-in-the-small, which aims at enhancing logic programming with new logical connectives.             In this paper, we present a general model theoretic approach to modular logic programming which combines programming in-the-large and in-the-small in a satisfactory way. Rather than inventing completely new constructs, however, we resort to a well-known concept in formal logic: generalized quantifiers. We show how generalized quantifiers can be incorporated into logic programs, both for Horn logic programs as well as in the presence of negation. Our basic observation is then that a logic program can be seen as a generalized quantifier, and we obtain a semantics for modular\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "78\n", "authors": ["50"]}
{"title": "Complexity of t-tautologies\n", "abstract": " A t-tautology (triangular tautology) is a propositional formula which is a tautology in all fuzzy logics defined by continuous triangular norms. In this paper we show that the problem of recognizing t-tautologies is coNP complete, and thus decidable.", "num_citations": "77\n", "authors": ["50"]}
{"title": "Parameterized model checking of fault-tolerant distributed algorithms by abstraction\n", "abstract": " We introduce an automated parameterized verification method for fault-tolerant distributed algorithms (FTDA). FTDAs are parameterized by both the number of processes and the assumed maximum number of faults. At the center of our technique is a parametric interval abstraction (PIA) where the interval boundaries are arithmetic expressions over parameters. Using PIA for both data abstraction and a new form of counter abstraction, we reduce the parameterized problem to finite-state model checking. We demonstrate the practical feasibility of our method by verifying safety and liveness of several fault-tolerant broadcasting algorithms, and finding counter examples in the case where there are more faults than the FTDA was designed for.", "num_citations": "76\n", "authors": ["50"]}
{"title": "Con2colic testing\n", "abstract": " In this paper, we describe (con) 2colic testing-a systematic testing approach for concurrent software. Based on concrete and symbolic executions of a concurrent program,(con) 2colic testing derives inputs and schedules such that the execution space of the program under investigation is systematically explored. We introduce interference scenarios as key concept in (con) 2colic testing. Interference scenarios capture the flow of data among different threads and enable a unified representation of path and interference constraints. We have implemented a (con) 2colic testing engine and demonstrate the effectiveness of our approach by experiments.", "num_citations": "76\n", "authors": ["50"]}
{"title": "Interpolation in fuzzy logic\n", "abstract": " We investigate interpolation properties of many-valued propositional logics related to continuous t-norms. In case of failure of interpolation, we characterize the minimal interpolating extensions of the languages. For finite-valued logics, we count the number of interpolating extensions by Fibonacci sequences.", "num_citations": "72\n", "authors": ["50"]}
{"title": "Watermarking schemes provably secure against copy and ambiguity attacks\n", "abstract": " Protocol attacks against watermarking schemes pose a threat to modern digital rights management systems; for example, a successful attack may allow to copy a watermark between two digital objects or to forge a valid watermark. Such attacks enable a traitor to hinder a dispute resolving process or accuse an innocent party of a copyright infringement. Secure DRM systems based on watermarks must therefore prevent such protocol attacks. In this paper we introduce a formal framework that enables us to assert rigorously the security of watermarks against protocol attacks. Furthermore, we show how watermarking schemes can be secured against some protocol attacks by using a cryptographic signature of a trusted third party.", "num_citations": "71\n", "authors": ["50"]}
{"title": "Proving ptolemy right: The environment abstraction framework for model checking concurrent systems\n", "abstract": " The parameterized verification of concurrent algorithms and protocols has been addressed by a variety of recent methods. Experience shows that there is a trade-off between techniques which are widely applicable but depend on non-trivial human guidance, and fully automated approaches which are tailored for narrow classes of applications. In this spectrum, we propose a new framework based on environment abstraction which exhibits a large degree of automation and can be easily adjusted to different fields of application. Our approach is based on two insights: First, we argue that natural abstractions for concurrent software are derived from the \u0393\u00c7\u00a3Ptolemaic\u0393\u00c7\u00a5 perspective of a human engineer who focuses on a single reference process. For this class of abstractions, we demonstrate soundness of abstraction under very general assumptions. Second, most protocols in given a class of protocols \u0393\u00c7\u00f4 for instance\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "70\n", "authors": ["50"]}
{"title": "SAT based predicate abstraction for hardware verification\n", "abstract": " Predicate abstraction is an important technique for extracting compact finite state models from large or infinite state systems. Predicate abstraction uses decision procedures to compute a model which is amenable to model checking, and has been used successfully for software verification. Little work however has been done on applying predicate abstraction to large scale finite state systems, most notably, hardware, where the decision procedures are SAT solvers. We consider predicate abstraction for hardware in the framework of Counterexample-Guided Abstraction Refinement where in the course of verification, the abstract model has to be repeatedly refined. The goal of the refinement is to eliminate spurious behavior in the abstract model which is not present in the original model, and gives rise to false negatives (spurious counterexamples).             In this paper, we present two efficient SAT-based\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "68\n", "authors": ["50"]}
{"title": "On the completeness of bounded model checking for threshold-based distributed algorithms: Reachability\n", "abstract": " Counter abstraction is a powerful tool for parameterized model checking, if the number of local states of the concurrent processes is relatively small. In recent work, we introduced parametric interval counter abstraction that allowed us to verify the safety and liveness of threshold-based fault-tolerant distributed algorithms (FTDA). Due to state space explosion, applying this technique to distributed algorithms with hundreds of local states is challenging for state-of-the-art model checkers. In this paper, we demonstrate that reachability properties of FTDAs can be verified by bounded model checking. To ensure completeness, we need an upper bound on the distance between states. We show that the diameters of accelerated counter systems of FTDAs, and of their counter abstractions, have a quadratic upper bound in the number of local transitions. Our experiments show that the resulting bounds are sufficiently small to\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "64\n", "authors": ["50"]}
{"title": "A logic-based framework for verifying consensus algorithms\n", "abstract": " Fault-tolerant distributed algorithms play an important role in ensuring the reliability of many software applications. In this paper we consider distributed algorithms whose computations are organized in rounds. To verify the correctness of such algorithms, we reason about (i) properties (such as invariants) of the state, (ii) the transitions controlled by the algorithm, and (iii) the communication graph. We introduce a logic that addresses these points, and contains set comprehensions with cardinality constraints, function symbols to describe the local states of each process, and a limited form of quantifier alternation to express the verification conditions. We show its use in automating the verification of consensus algorithms. In particular, we give a semi-decision procedure for the unsatisfiability problem of the logic and identify a decidable fragment. We successfully applied our framework to verify the correctness of a\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "64\n", "authors": ["50"]}
{"title": "Proactive detection of computer worms using model checking\n", "abstract": " Although recent estimates are speaking of 200,000 different viruses, worms, and Trojan horses, the majority of them are variants of previously existing malware. As these variants mostly differ in their binary representation rather than their functionality, they can be recognized by analyzing the program behavior, even though they are not covered by the signature databases of current antivirus tools. Proactive malware detectors mitigate this risk by detection procedures that use a single signature to detect whole classes of functionally related malware without signature updates. It is evident that the quality of proactive detection procedures depends on their ability to analyze the semantics of the binary. In this paper, we propose the use of model checking-a well-established software verification technique-for proactive malware detection. We describe a tool that extracts an annotated control flow graph from the binary and\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "62\n", "authors": ["50"]}
{"title": "Parameterized model checking of rendezvous systems\n", "abstract": " Parameterized model checking is the problem of deciding if a given formula holds irrespective of the number of participating processes. A standard approach for solving the parameterized model checking problem is to reduce it to model checking finitely many finite-state systems. This work considers the theoretical power and limitations of this technique. We focus on concurrent systems in which processes communicate via pairwise rendezvous, as well as the special cases of disjunctive guards and token passing; specifications are expressed in indexed temporal logic without the next operator; and the underlying network topologies are generated by suitable formulas and graph operations. First, we settle the exact computational complexity of the parameterized model checking problem for some of our concurrent systems, and establish new decidability results for others. Second, we consider the cases where\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "60\n", "authors": ["50"]}
{"title": "A short counterexample property for safety and liveness verification of fault-tolerant distributed algorithms\n", "abstract": " Distributed algorithms have many mission-critical applications ranging from embedded systems and replicated databases to cloud computing. Due to asynchronous communication, process faults, or network failures, these algorithms are difficult to design and verify. Many algorithms achieve fault tolerance by using threshold guards that, for instance, ensure that a process waits until it has received an acknowledgment from a majority of its peers. Consequently, domain-specific languages for fault-tolerant distributed systems offer language support for threshold guards.", "num_citations": "60\n", "authors": ["50"]}
{"title": "Succinct representation, leaf languages, and projection reductions\n", "abstract": " In this article, the following results are shown: 1. For succinctly encoded problemss(A), completeness under polynomial time reductions is equivalent to completeness under projection reductions, an extremely weak reduction defined by a quantifier-free projective formula. 2. The succinct versions(Aof a computational problemAis complete under projection reductions for the class of problems characterizable with leaf languageA, but not complete undermonotoneprojections. 3. A strong conversion lemma: IfAis reducible toBin polylogarithmic time, then the succinct version ofAis monotone projection reducible to the succinct version ofB. This result strengthens previous results by Papadimitriou and Yannakakis, and Balc\u251c\u00edzar and Lozano. It allows iterated application for multiple succinct problems. 4. For all syntactic complexity classes there exist complete problems undermonotoneprojection reductions. This positively\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "53\n", "authors": ["50"]}
{"title": "CBMC-GC: an ANSI C compiler for secure two-party computations\n", "abstract": " Secure two-party computation (STC) is a computer security paradigm where two parties can jointly evaluate a program with sensitive input data, provided in parts from both parties. By the security guarantees of STC, neither party can learn any information on the other party\u0393\u00c7\u00d6s input while performing the STC task. For a long time thought to be impractical, until recently, STC has only been implemented with domain-specific languages or hand-crafted Boolean circuits for specific computations. Our open-source compiler CBMC-GC is the first ANSI C compiler for STC. It turns C programs into Boolean circuits that fit the requirements of garbled circuits, a generic STC approach based on circuits. Here, the size of the resulting circuits plays a crucial role since each STC step involves encryption and network transfer and is therefore extremely slow when compared to computations performed on modern hardware\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "51\n", "authors": ["50"]}
{"title": "Query-driven program testing\n", "abstract": " We present a new approach to program testing which enables the programmer to specify test suites in terms of a versatile query language. Our query language subsumes standard coverage criteria ranging from simple basic block coverage all the way to predicate complete coverage and multiple condition coverage, but also facilitates on-the-fly requests for test suites specific to the code structure, to external requirements, or to ad hoc needs arising in program understanding/exploration. The query language is supported by a model checking backend which employs the CBMC framework. Our main algorithmic contribution is a method called iterative constraint strengthening which enables us to solve a query for an arbitrary coverage criterion by a single call to the model checker and a novel form of incremental SAT solving: Whenever the SAT solver finds a solution, our algorithm compares this solution\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "51\n", "authors": ["50"]}
{"title": "Complexity and resource bound analysis of imperative programs using difference constraints\n", "abstract": " Difference constraints have been used for termination analysis in the literature, where they denote relational inequalities of the form , and describe that the value of x in the current state is at most the value of y in the previous state plus some constant . We believe that difference constraints are also a good choice for complexity and resource bound analysis because the complexity of imperative programs typically arises from counter increments and resets, which can be modeled naturally by difference constraints. In this article we propose a bound analysis based on difference constraints. We make the following contributions: (1) our analysis handles bound analysis problems of high practical relevance which current approaches cannot handle: we extend the range of bound analysis to a class of challenging but natural loop iteration patterns which typically appear in parsing and string-matching\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "47\n", "authors": ["50"]}
{"title": "SMT and POR beat counter abstraction: Parameterized model checking of threshold-based distributed algorithms\n", "abstract": " Automatic verification of threshold-based fault-tolerant distributed algorithms (FTDA) is challenging: they have multiple parameters that are restricted by arithmetic conditions, the number of processes and faults is parameterized, and the algorithm code is parameterized due to conditions counting the number of received messages. Recently, we introduced a technique that first applies data and counter abstraction and then runs bounded model checking (BMC). Given an FTDA, our technique computes an upper bound on the diameter of the system. This makes BMC complete: it always finds a counterexample, if there is an actual error. To verify state-of-the-art FTDAs, further improvement is needed. In this paper, we encode bounded executions over integer counters in SMT. We introduce a new form of offline partial order reduction that exploits acceleration and the structure of the FTDAs. This aggressively\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "43\n", "authors": ["50"]}
{"title": "iDQ: Instantiation-Based DQBF Solving.\n", "abstract": " iDQ: Instantiation-Based DQBF Solving Page 1 iDQ: Instantiation-Based DQBF Solving Andreas Fr\u251c\u2562hlich1, Gergely Kov\u251c\u00edsznai2, Armin Biere1, Helmut Veith2 1 Johannes Kepler University, Linz 2 Vienna University of Technology, Vienna http://fmv.jku.at http://forsyte.at POS 2014 July 13, 2014 Vienna, Austria Andreas Fr\u251c\u2562hlich, Gergely Kov\u251c\u00edsznai, Armin Biere, Helmut Veith iDQ: Instantiation-Based DQBF Solving Page 2 What is DQBF? DQBF = Dependency Quantified Boolean Formulas \u0393\u00ea\u00c7u1, u2, u3 \u0393\u00ea\u00e2e(u1, u3), f (u2) . (u2 \u0393\u00ea\u00bf u3 \u0393\u00ea\u00bf e) \u0393\u00ea\u00ba (u1 \u0393\u00ea\u00bf u2 \u0393\u00ea\u00bf e \u0393\u00ea\u00bf f ) Generalization of QBF Variable dependencies can be explicitly given Higher complexity: QBF \u0393\u00c7\u00f4 PSpace-complete DQBF \u0393\u00c7\u00f4 NExpTime-complete Andreas Fr\u251c\u2562hlich, Gergely Kov\u251c\u00edsznai, Armin Biere, Helmut Veith iDQ: Instantiation-Based DQBF Solving Page 3 Applications for DQBF Partial-information 2-player games [Peterson, Reif. Multiple-person alternation. of , 1979\u0393\u00c7\u00aa", "num_citations": "41\n", "authors": ["50"]}
{"title": "Securing symmetric watermarking schemes against protocol attacks\n", "abstract": " With the advent of the web and the creation of electronic distribution channels for multimedia objects, there is an increased risk of copyright infringements. Content providers try to alleviate this problem by using copyright protection facilities that often involve watermarking schemes as primitives. Clearly, the intention of the content provider can be subverted if the watermarking scheme is susceptible to intentional attacks, especially to attacks on the robustness of watermarks. It was noted early during the development of watermarking algorithms that the intention of resolving the copyright situation might be subverted entirely without removing any watermark contained in multimedia objects. Indeed, so-called protocol attacks try to introduce some sort of ambiguity during the copyright resolution process. After providing formal definitions for some common protocol attacks, we discuss the possibility of constructing\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "41\n", "authors": ["50"]}
{"title": "Precise static analysis of untrusted driver binaries\n", "abstract": " Most closed source drivers installed on desktop systems today have never been exposed to formal analysis. Without vendor support, the only way to make these often hastily written, yet critical programs accessible to static analysis is to directly work at the binary level. In this paper, we describe a full architecture to perform static analysis on binaries that does not rely on unsound external components such as disassemblers. To precisely calculate data and function pointers without any type information, we introduce Bounded Address Tracking, an abstract domain that is tailored towards machine code and is path sensitive up to a tunable bound assuring termination. We implemented Bounded Address Tracking in our binary analysis platform Jakstab and used it to verify API specifications on several Windows device drivers. Even without assumptions about executable layout and procedures as made by state of the art\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "40\n", "authors": ["50"]}
{"title": "Encoding treewidth into SAT\n", "abstract": " One of the most important structural parameters of graphs is treewidth, a measure for the \u0393\u00c7\u00a3tree-likeness\u0393\u00c7\u00a5 and thus in many cases an indicator for the hardness of problem instances. The smaller the treewidth, the closer the graph is to a tree and the more efficiently the underlying instance often can be solved. However, computing the treewidth of a graph is NP-hard in general. In this paper we propose an encoding of the decision problem whether the treewidth of a given graph is at most\u252c\u00e1k into the propositional satisfiability problem. The resulting SAT instance can then be fed to a SAT solver. In this way we are able to improve the known bounds on the treewidth of several benchmark graphs from the literature.", "num_citations": "40\n", "authors": ["50"]}
{"title": "State/event software verification for branching-time specifications\n", "abstract": " In the domain of concurrent software verification, there is an evident need for specification formalisms and efficient algorithms to verify branching-time properties that involve both data and communication. We address this problem by defining a new branching-time temporal logic SE-A which integrates both state-based and action-based properties. SE-A is universal, i.e., preserved by the simulation relation, and thus amenable to counterexample-guided abstraction refinement. We provide a model-checking algorithm for this logic, based upon a compositional abstraction-refinement loop which exploits the natural decomposition of the concurrent system into its components. The abstraction and refinement steps are performed over each component separately, and only the model checking step requires an explicit composition of the abstracted components. For experimental evaluation, we have integrated\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "40\n", "authors": ["50"]}
{"title": "Using verification technology to specify and detect malware\n", "abstract": " Computer viruses and worms are major threats for our computer infrastructure, and thus, for economy and society at large. Recent work has demonstrated that a model checking based approach to malware detection can capture the semantics of security exploits more accurately than traditional approaches, and consequently achieve higher detection rates. In this approach, malicious behavior is formalized using the expressive specification language CTPL based on classic CTL. This paper gives an overview of our toolchain for malware detection and presents our new system for computer assisted generation of malicious code specifications.", "num_citations": "35\n", "authors": ["50"]}
{"title": "Empirical software metrics for benchmarking of verification tools\n", "abstract": " We study empirical metrics for software source code, which can predict the performance of verification tools on specific types of software. Our metrics comprise variable usage patterns, loop patterns, as well as indicators of control-flow complexity and are extracted by simple data-flow analyses. We demonstrate that our metrics are powerful enough to devise a machine-learning based portfolio solver for software verification. We show that this portfolio solver would be the (hypothetical) overall winner of the international competition on software verification (SV-COMP) in three consecutive years (2014\u0393\u00c7\u00f42016). This gives strong empirical evidence for the predictive power of our metrics and demonstrates the viability of portfolio solvers for software verification. Moreover, we demonstrate the flexibility of our algorithm for portfolio construction in novel settings: originally conceived for SV-COMP\u0393\u00c7\u00d614, the construction\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "33\n", "authors": ["50"]}
{"title": "Towards modeling and model checking fault-tolerant distributed algorithms\n", "abstract": " Fault-tolerant distributed algorithms are central for building reliable, spatially distributed systems. In order to ensure that these algorithms actually make systems more reliable, we must ensure that these algorithms are actually correct. Unfortunately, model checking state-of-the-art fault-tolerant distributed algorithms (such as Paxos) is currently out of reach except for very small systems.             In order to be eventually able to automatically verify such fault-tolerant distributed algorithms also for larger systems, several problems have to be addressed. In this paper, we consider modeling and verification of fault-tolerant algorithms that basically only contain threshold guards to control the flow of the algorithm. As threshold guards are widely used in fault-tolerant distributed algorithms (and also in Paxos), efficient methods to handle them bring us closer to the above mentioned goal.             As a case study we use the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "33\n", "authors": ["50"]}
{"title": "Second order logic and the weak exponential hierarchies\n", "abstract": " Second order logic over finite structures is well-known to capture the levels of the polynomial hierarchy PH. Recently, it has been shown that \u256c\u00ff                                     k                                  1                \u0393\u00c7\u00f6 the first order closure of second order \u256c\u00fa                                     m                                  1                \u0393\u00c7\u00f6 captures the class \u256c\u00ff                                     k                                                     P                                 =   , a natural intermediate class of the polynomial hierarchy [12].             In this paper we show that with respect to expression complexity, second order logic characterizes the levels of the weak exponential hierarchy EH. Moreover, we extend these results to intermediate classes E\u256c\u00ff                                     k                                                     P                                 in EH which correspond to the \u256c\u00ff                                     k                                                     P                                 classes in PH.             To this end, in extending previous results, we show\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "33\n", "authors": ["50"]}
{"title": "Languages represented by Boolean formulas\n", "abstract": " A propositional problem is a problem whose instances are defined by Boolean formulas. Using quantifier free logical reductions, we give a sufficient condition under which a large class of propositional problems becomes exponentially harder than their ordinary encodings. This result extends former upgrading results which hold only for representation by Boolean circuits. It follows that all succinct circuit problems proved complete by Papadimitriou (1994) remain complete under representation by Boolean formulas.", "num_citations": "29\n", "authors": ["50"]}
{"title": "Executable protocol specification in ESL\n", "abstract": " Hardware specifications in English are frequently ambiguous and often self-contradictory.We propose a new logic ESL which facilitates formal specification of hardware protocols.Our logic is closely related to LTL but can express all regular safety properties. We have developed a protocol synthesis methodology which generates Mealy machines from ESL specifications. The Mealy machines can be automatically translated into executable code either in Verilog or SMV. Our methodology exploits the observation that protocols are naturally composed of many semantically distinct components. This structure is reflected in the syntax of ESL specifications.We use a modified LTL tableau construction to build a Mealy machine for each component. The Mealy machines are connected together in a Verilog or SMV framework. In many cases this makes it possible to circumvent the state explosion problem during code\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "26\n", "authors": ["50"]}
{"title": "Linear time Datalog and branching time logic\n", "abstract": " We survey recent results about the relation between Datalog and temporal verification logics. Datalog is a well-known database query language relying on the logic programming paradigm. We introduce Datalog LITE, a fragment of Datalog with well-founded negation, which has an easy stratified semantics and a linear time model checking algorithm. Datalog LITE subsumes temporal languages such as CTL and the alternation-free \u256c\u255d-calculus. We give easy syntactic characterizations of these temporal languages by fragments of Datalog LITE, and show that Datalog LITE has the same expressive power as the alternation-free portion of guarded fixed point logic.", "num_citations": "26\n", "authors": ["50"]}
{"title": "An axiomatization of quantified propositional G\u251c\u2562del logic using the Takeuti-Titani rule\n", "abstract": " Quantified Godel logic QGL is obtained by adding propositional quantifiers to the language of propositional Godel logic. In this paper, we give an axiomatization of QGL using an instance of the density rule by Takeuti and Titani, show decidability, and analyze its proof theoretic properties.", "num_citations": "26\n", "authors": ["50"]}
{"title": "Tutorial on parameterized model checking of fault-tolerant distributed algorithms\n", "abstract": " Recently we introduced an abstraction method for parameterized model checking of threshold-based fault-tolerant distributed algorithms. We showed how to verify distributed algorithms without fixing the size of the system a priori. As is the case for many other published abstraction techniques, transferring the theory into a running tool is a challenge. It requires understanding of several verification techniques such as parametric data and counter abstraction, finite state model checking and abstraction refinement. In the resulting framework, all these techniques should interact in order to achieve a possibly high degree of automation. In this tutorial we use the core of a fault-tolerant distributed broadcasting algorithm as a case study to explain the concepts of our abstraction techniques, and discuss how they can be\u252c\u00e1implemented.", "num_citations": "23\n", "authors": ["50"]}
{"title": "What you always wanted to know about model checking of fault-tolerant distributed algorithms\n", "abstract": " Distributed algorithms have numerous mission-critical applications in embedded avionic and automotive systems, cloud computing, computer networks, hardware design, and the internet of things. Although distributed algorithms exhibit complex interactions with their computing environment and are difficult to understand for human engineers, computer science has developed only very limited tool support to catch logical errors in distributed algorithms at design time.               In the last two decades we have witnessed a revolutionary progress in software model checking due to the development of powerful techniques such as abstract model checking, SMT solving, and partial order reduction. Still, model checking of fault-tolerant distributed algorithms poses multiple research challenges, most notably parameterized verification: verifying an algorithm for all system sizes and different combinations of faults. In\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "22\n", "authors": ["50"]}
{"title": "Parameterized vacuity\n", "abstract": " In model checking, a specification is vacuously true, if some subformula can be modified without affecting the truth value of the specification. Intuitively, this means that the property expressed in this subformula is satisfied for a trivial reason, and likely not the intended one. It has been shown by Kupferman and Vardi that vacuity detection can be reduced to model checking of simplified specifications where the subformulas of interest are replaced by constant truth\u252c\u00e1values.             In this paper, we argue that the common definition describes extreme cases of vacuity where the subformula indeed collapses to a constant truth value. We suggest a refined notion of vacuity (weak vacuity) which is parameterized by a user-defined class of vacuity causes. Under this notion, a specification is vacuously true, if a subformula collapses to a vacuity cause. Our analysis exhibits a close relationship between vacuity detection\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "22\n", "authors": ["50"]}
{"title": "Model checking: Back and forth between hardware and software\n", "abstract": " The interplay back and forth between software model checking and hardware model checking has been fruitful for both. Originally intended for the analysis of concurrent software, model checking was first used in hardware verification. The abstraction methods developed for hardware verification however have been a stepping stone for the new generation of software verification tools including SLAM, BLAST, and MAGIC which focus on control-intensive software in C. Most recently, the experience with software verification is providing new leverage for verifying hardware designs in high level languages.", "num_citations": "21\n", "authors": ["50"]}
{"title": "Proof theory of fuzzy logics: Urquhart's C and related logics\n", "abstract": " We investigate the proof theory of Urquhart's C and other logics underlying the most prominent fuzzy logics, such as G\u251c\u2562del, Product, and \u253c\u00e9ukasiewicz logic. All these logics share the property that their truth values are linearly ordered. We define hypersequent calculi for such logics, and show the following results: (1) Contraction-free counterparts of intuitionistic logic and G\u251c\u2562del logic (including C) admit cut-elimination. (2) Validity in these logics is decidable. (3) Hajek's basic fuzzy logic BL properly extends the contraction-free G\u251c\u2562del logic; the axiom for commutativity of the minimum is independent from the other axioms of BL. (4) All abovementioned logics are distinct from each other.", "num_citations": "21\n", "authors": ["50"]}
{"title": "Difference constraints: An adequate abstraction for complexity analysis of imperative programs\n", "abstract": " Difference constraints have been used for termination analysis in the literature, where they denote relational inequalities of the form x' \u0393\u00eb\u00f1 y + c, and describe that the value of x in the current state is at most the value of y in the previous state plus some constant c \u0393\u00ea\u00ea \u0393\u00e4\u00f1. In this paper, we argue that the complexity of imperative programs typically arises from counter increments and resets, which can be modeled naturally by difference constraints.We present the first practical algorithm for the analysis of difference constraint programs and describe how C programs can be abstracted to difference constraint programs. Our approach contributes to the field of automated complexity and (resource) bound analysis by enabling automated amortized complexity analysis for a new class of programs and providing a conceptually simple program model that relates invariant- and bound analysis.We demonstrate the effectiveness of\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "20\n", "authors": ["50"]}
{"title": "Temporal logic model checking\n", "abstract": " Errors in safety-critical systems such as embedded controllers may have drastic consequences and can even endanger human life. It is therefore crucially important to verify the correctness of such systems in a logically precise manner during system design itself. This chapter is an introduction to model checking\u0393\u00c7\u00f6an automated and practically successful approach for the formal verification of the correctness of hardware and software systems.               The aim of this chapter is to introduce those important lines of research which transformed model checking from a method of primarily theoretical interest into a powerful tool for the analysis of computer hardware and soft ware. We shall focus in particular on those subjects which have shaped our thinking about model checking in the verification group of Carnegie Mellon University, most notably symbolic model checking and abstraction. The development of symbolic\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "20\n", "authors": ["50"]}
{"title": "Validity of CTL queries revisited\n", "abstract": " We systematically investigate temporal logic queries in model checking, adding to the seminal paper by William Chan at CAV 2000. Chan\u0393\u00c7\u00d6s temporal logic queries are CTL specifications where one unspecified subformula is to be filled in by the model checker in such a way that the specification becomes true. Chan defined a fragment of CTL queries called  which guarantees the existence of a unique strongest solution. The starting point of our paper is a counterexample to this claim. We then show how the research agenda of Chan can be realized by modifying his fragment appropriately. To this aim, we investigate the criteria required by Chan, and define two new fragments  and  where the first is the one originally intended; the latter fragment also provides unique strongest solutions where possible but admits also cases where the set of solutions is empty.", "num_citations": "19\n", "authors": ["50"]}
{"title": "How to encode a logical structure by an OBDD\n", "abstract": " The complexity of problems whose instances are represented by ordered binary decision diagrams (OBDDs) is investigated. By using interleaved variable orders which are well-known in symbolic model checking, we derive hardness results for OBDDs from a Conversion Lemma, extending previous related work about succinct circuit problems and succinct formula problems. This method is applicable for all problems which are complete under Immerman's quantifier free reductions, in particular the problems investigated previously.", "num_citations": "19\n", "authors": ["50"]}
{"title": "On the notion of vacuous truth\n", "abstract": " The model checking community has proposed numerous definitions of vacuous satisfaction, i.e., formal criteria which tell whether a temporal logic specification holds true on a system model for the intended reason. In this paper we attempt to study the notion of vacuous satisfaction from first principles. We show that despite the apparently vague formulation of the vacuity problem, most proposed notions of vacuity for temporal logic can be cast into a uniform and simple framework, and compare previous approaches to vacuity detection from this unified point of view.", "num_citations": "18\n", "authors": ["50"]}
{"title": "Quantifier elimination in fuzzy logic\n", "abstract": " We investigate quantifier elimination of first order logic over fuzzy algebras. Fuzzy algebras are defined from continuous t-norms over the unit interval, and subsume \u253c\u00fcukasiewicz [28, 29], G\u251c\u2562del [16, 12] and Product [19] Logic as most prominent examples.               We show that a fuzzy algebra has quantifier elimination iff it is one of the abovementioned logics. Moreover, we show quantifier elimination for various extensions of these logics, and observe other model-theoretic properties of fuzzy algebras.               Further considerations are devoted to approximation of fuzzy logics by finite-valued logics.", "num_citations": "18\n", "authors": ["50"]}
{"title": "Para                                                       2 : parameterized path reduction, acceleration, and SMT for reachability in threshold-guarded distributed algorithms\n", "abstract": " Automatic verification of threshold-based fault-tolerant distributed algorithms (FTDA) is challenging: FTDAs have multiple parameters that are restricted by arithmetic conditions, the number of processes and faults is parameterized, and the algorithm code is parameterized due to conditions counting the number of received messages. Recently, we introduced a technique that first applies data and counter abstraction and then runs bounded model checking (BMC). Given an FTDA, our technique computes an upper bound on the diameter of the system. This makes BMC complete for reachability properties: it always finds a counterexample, if there is an actual error. To verify state-of-the-art FTDAs, further improvement is needed. In contrast to encoding bounded executions of a counter system over an abstract finite domain in SAT, in this paper, we encode bounded executions over integer counters in SMT. In\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "16\n", "authors": ["50"]}
{"title": "The localization reduction and counterexample-guided abstraction refinement\n", "abstract": " Automated abstraction is widely recognized as a key method for computer-aided verification of hardware and software. In this paper, we describe the evolution of counterexample-guided refinement and other iterative abstraction refinement techniques.", "num_citations": "16\n", "authors": ["50"]}
{"title": "The first order definability of graphs: upper bounds for quantifier depth\n", "abstract": " Let D (G) denote the minimum quantifier depth of a first order sentence that defines a graph G up to isomorphism in terms of the adjacency and equality relations. Call two vertices of G similar if they have the same adjacency to any other vertex and denote the maximum number of pairwise similar vertices in G by \u2567\u00e2 (G). We prove that \u2567\u00e2 (G)+ 1\u0393\u2310\u255c D (G)\u0393\u2310\u255c max {\u2567\u00e2 (G)+ 2,(n+ 5)/2}, where n denotes the number of vertices of G. In particular, D (G)\u0393\u2310\u255c(n+ 5)/2 for every G with no transposition in the automorphism group. If G is connected and has maximum degree d, we prove that D (G)\u0393\u2310\u255c c d n+ O (d 2) for a constant c d< 1 2. A linear lower bound for graphs of maximum degree 3 with no transposition in the automorphism group follows from an earlier result by Cai, F\u251c\u255drer, and Immerman [An optimal lower bound on the number of variables for graph identification, Combinatorica 12 (4)(1992) 389\u0393\u00c7\u00f4410]. Our upper bounds for D (G\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "16\n", "authors": ["50"]}
{"title": "Local linearizability for concurrent container-type data structures\n", "abstract": " The semantics of concurrent data structures is usually given by a sequential specification and a consistency condition. Linearizability is the most popular consistency condition due to its simplicity and general applicability. Nevertheless, for applications that do not require all guarantees offered by linearizability, recent research has focused on improving performance and scalability of concurrent data structures by relaxing their semantics. In this paper, we present local linearizability, a relaxed consistency condition that is applicable to container-type concurrent data structures like pools, queues, and stacks. While linearizability requires that the effect of each operation is observed by all threads at the same time, local linearizability only requires that for each thread T, the effects of its local insertion operations and the effects of those removal operations that remove values inserted by T are observed by all threads at the same time. We investigate theoretical and practical properties of local linearizability and its relationship to many existing consistency conditions. We present a generic implementation method for locally linearizable data structures that uses existing linearizable data structures as building blocks. Our implementations show performance and scalability improvements over the original building blocks and outperform the fastest existing container-type implementations.", "num_citations": "14\n", "authors": ["50"]}
{"title": "Shape and content\n", "abstract": " The verification community has studied dynamic data structures primarily in a bottom-up way by analyzing pointers and the shapes induced by them. Recent work in fields such as separation logic has made significant progress in extracting shapes from program source code. Many real world programs however manipulate complex data whose structure and content is most naturally described by formalisms from object oriented programming and databases. In this paper, we look at the verification of programs with dynamic data structures from the perspective of content representation. Our approach is based on description logic, a widely used knowledge representation paradigm which gives a logical underpinning for diverse modeling frameworks such as UML and ER. Technically, we assume that we have separation logic shape invariants obtained from a shape analysis tool, and requirements on the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "13\n", "authors": ["50"]}
{"title": "On the concept of variable roles and its use in software analysis\n", "abstract": " Human written source code in imperative programming languages exhibits typical patterns for variable use, such as flags, loop iterators, counters, indices, bitvectors, etc. Although it is widely understood by practitioners that these patterns are important for automated software analysis tools, they are not systematically studied by the formal methods community, and not well documented in the research literature. In this paper, we introduce the notion of variable roles on the example of basic types (int, float, char) in C. We propose a classification of the variables in a program by variable roles which formalises the typical usage patterns of variables. We show that classical data flow analysis lends itself naturally both as a specification formalism and an analysis paradigm for this classification problem. We demonstrate the practical applicability of our method by predicting membership of source files to the different categories\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "13\n", "authors": ["50"]}
{"title": "A syntactic characterization of distributive LTL queries\n", "abstract": " Temporal logic query solving, as introduced by Chan in 2000, is a variation of model checking where one subformula of the specification is left open and has to be filled in by the model checker in such a way that the specification becomes true. Motivated by symbolic query solving algorithms for CTL, Chan focused on a class of CTL queries which have one strongest solution, but no syntactic characterization of this class has been found yet. In this paper, we provide a syntactic characterization for the simpler case of LTL queries. We present a context-free grammar of LTL query templates\u252c\u00e1LTLQ                   x                  which guarantees that (i)\u252c\u00e1all queries in\u252c\u00e1LTLQ                   x                  have at most one strongest solution, and (ii)\u252c\u00e1all query templates not in\u252c\u00e1LTLQ                   x                  have simple instantiations with incomparable strongest solutions. While the LTL case appears to be simpler than the case of CTL, we\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "13\n", "authors": ["50"]}
{"title": "Who is afraid of model checking distributed algorithms\n", "abstract": " While distributed algorithms is a highly active area, and the correctness of distributed algorithms is usually based on very subtle mathematical arguments, there have been very limited efforts to achieve automated verification of distributed algorithms. In this note we discuss the major technical obstacles and methodological challenges. Our hope is that the collection of issues collected in this position paper will isolate the most urgent and open research questions to brave future researchers.", "num_citations": "12\n", "authors": ["50"]}
{"title": "On compiling boolean circuits optimized for secure multi-party computation\n", "abstract": " Secure multi-party computation (MPC) allows two or more distrusting parties to jointly evaluate a function over private inputs. For a long time considered to be a purely theoretical concept, MPC transitioned into a practical and powerful tool to build privacy-enhancing technologies. However, the practicality of MPC is hindered by the difficulty to implement applications on top of the underlying cryptographic protocols. This is because the manual construction of efficient applications, which need to be represented as Boolean or arithmetic circuits, is a complex, error-prone, and time-consuming task. To facilitate the development of further privacy-enhancing technology, multiple compilers have been proposed that create circuits for MPC. Yet, almost all presented compilers only support domain specific languages or provide very limited optimization methods. In this work (this is an extended and revised version of the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "11\n", "authors": ["50"]}
{"title": "On the complexity of symbolic verification and decision problems in bit-vector logic\n", "abstract": " We study the complexity of decision problems encoded in bit-vector logic. This class of problems includes word-level model checking, i.e., the reachability problem for transition systems encoded by bit-vector formulas. Our main result is a generic theorem which determines the complexity of a bit-vector encoded problem from the complexity of the problem in explicit encoding. In particular, NL-completeness of graph reachability directly implies PSpace-completeness and ExpSpace-completeness for word-level model checking with unary and binary arity encoding, respectively. In general, problems complete for a complexity class C are shown to be complete for an exponentially harder complexity class than C when represented by bit-vector formulas with unary encoded scalars, and further complete for a double exponentially harder complexity class than C with binary encoded scalars. We also show that multi\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "10\n", "authors": ["50"]}
{"title": "Local linearizability\n", "abstract": " The semantics of concurrent data structures is usually given by a sequential specification and a consistency condition. Linearizability is the most popular consistency condition due to its simplicity and general applicability. Nevertheless, for applications that do not require all guarantees offered by linearizability, recent research has focused on improving performance and scalability of concurrent data structures by relaxing their semantics. In this paper, we present local linearizability, a relaxed consistency condition that is applicable to container-type concurrent data structures like pools, queues, and stacks. While linearizability requires that the effect of each operation is observed by all threads at the same time, local linearizability only requires that for each thread T, the effects of its local insertion operations and the effects of those removal operations that remove values inserted by T are observed by all threads at the same time. We investigate theoretical and practical properties of local linearizability and its relationship to many existing consistency conditions. We present a generic implementation method for locally linearizable data structures that uses existing linearizable data structures as building blocks. Our implementations show performance and scalability improvements over the original building blocks and outperform the fastest existing container-type implementations.", "num_citations": "9\n", "authors": ["50"]}
{"title": "Role-based symmetry reduction of fault-tolerant distributed protocols with language support\n", "abstract": " Fault-tolerant (FT) distributed protocols (such as group membership, consensus, etc.) represent fundamental building blocks for many practical systems, e.g., the Google File System. Not only does one desire rigor in the protocol design but especially in its verification given the complexity and fallibility of manual proofs. The application of model checking (MC) for protocol verification is attractive with its full automation and rich property language. However, being an exhaustive exploration method, its scalable use is very much constrained by the overall number of different system states. We observe that, although FT distributed protocols usually display a very high degree of symmetry which stems from permuting different processes, MC efforts targeting their automated verification often disregard this symmetry. Therefore, we propose to leverage the framework of symmetry reduction and improve on existing\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "9\n", "authors": ["50"]}
{"title": "Generalized quantifiers in logic programs\n", "abstract": " Generalized quantifiers are an important concept in model-theoretic logic which has applications in different fields such as linguistics, philosophical logic and computer science. In this paper, we consider a novel application in the field of logic programming, which has been presented recently. The enhancement of logic programs by generalized quantifiers is a convenient tool for interfacing extra-logical functions and provides a natural framework for the definition of modular logic programs. We survey the expressive capability of syntactical classes of logic programs with generalized quantifiers over finite structures, and pay particular attention to modular logic programs. Moreover, we study the complexity of such programs. It appears that modular logic programming has the expressive power of second-order logic and captures the polynomial hierarchy, and different natural syntactical fragments capture the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "9\n", "authors": ["50"]}
{"title": "Logic for Programming, Artificial Intelligence, and Reasoning: 15th International Conference, LPAR 2008, Doha, Qatar, November 22-27, 2008, Proceedings\n", "abstract": " This book constitutes the refereed proceedings of the 15th International Conference on Logic for Programming, Artificial Intelligence, and Reasoning, LPAR 2008, which took place in Doha, Qatar, during November 22-27, 2008. The 45 revised full papers presented together with 3 invited talks were carefully revised and selected from 153 submissions. The papers address all current issues in automated reasoning, computational logic, programming languages and their applications and are organized in topical sections on automata, linear arithmetic, verification knowledge representation, proof theory, quantified constraints, as well as modal and temporal logics.", "num_citations": "7\n", "authors": ["50"]}
{"title": "An analytic calculus for quantified propositional G\u251c\u2562del logic\n", "abstract": " We define a hypersequent calculus for G\u251c\u2562del logic enhanced with (fuzzy) quantifiers over propositional variables. We prove soundness, completeness and cut-elimination for this calculus and provide a detailed investigation of the so-called Takeuti-Titani rule which expresses density of the ordering of truth values. Since this rule is critical from the point of view of proof search we characterize a fragment of the logic for which it can be eliminated.", "num_citations": "7\n", "authors": ["50"]}
{"title": "Extending ALCQIO with trees\n", "abstract": " We study the description logic ALCQIO, which extends the standard description logic ALC with nominals, inverses and counting quantifiers. ALCQIO is a fragment of first order logic and thus cannot define trees. We consider the satisfiability problem of ALCQIO over finite structures in which k relations are interpreted as forests of directed trees with unbounded out degrees. We show that the finite satisfiability problem of ALCQIO with forests is polynomial-time reducible to finite satisfiability of ALCQIO. As a consequence, we get that finite satisfiability is NEXPTIME-complete. Description logics with transitive closure constructors or fixed points have been studied before, but we give the first decidability result of the finite satisfiability problem for a description logic that contains nominals, inverse roles, and counting quantifiers and can define trees.", "num_citations": "6\n", "authors": ["50"]}
{"title": "Towards a Description Logic for Program Analysis: Extending ALCQIO with Reachability.\n", "abstract": " Shape analysis attempts to analyze and verify correctness of programs with dynamic data structures. This is a notoriously difficult task, because it necessitates efficient decision procedures for expressive logics on graphs and graph-like structures. In the last decade, model-theoretic approaches have been less prominent, and the leading approach is proof-theoretic [15]. Recent advances in finite model theory have created an opportunity for development of practical modeltheoretic approaches in shape analysis.Description Logics (DLs) are a well established family of logics for Knowledge Representation and Reasoning [2]. They model the domain of interest in terms of concepts (classes of objects) and roles (binary relations between objects). These features make DLs very useful to formally describe and reason about graph-structured information. The usefulness of DLs is witnessed eg by the W3C choosing DLs to provide the logical foundations to the standard Web Ontology Language (OWL)[14]. Another application of DLs is formalization and static analysis of UML class diagrams and ER diagrams, which are basic modeling artifacts in object-oriented software development and database design, respectively [4, 1]. In these settings, standard reasoning services provided by DLs can be used to verify eg the consistency of a diagram. To describe the memory of programs with dynamic data structures using a DL, a rather powerful DL must be chosen. The DL in question needs to allow a computationally problematic combination of constructors:(i) nominals are required to represent the program\u0393\u00c7\u00d6s variables;(ii) number restrictions are required so that the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "6\n", "authors": ["50"]}
{"title": "New Challenges in the Development of Critical Embedded Systems\u0393\u00c7\u00f6An \u0393\u00c7\u00a3aeromotive\u0393\u00c7\u00a5 Perspective\n", "abstract": " During the last decades, embedded systems have become increasingly important in highly safety-critical areas such as power plants, medical equipment, cars, and aeroplanes. The automotive and avionics domains are prominent examples of classical engineering disciplines where conflicts between costs, short product cycles and legal requirements concerning dependability, robustness, security, carbon footprint and spatial demands have become a pressing problem.", "num_citations": "6\n", "authors": ["50"]}
{"title": "Deterministic CTL query solving\n", "abstract": " Temporal logic queries provide a natural framework to extend the realm of model checking from mere verification of engineers' specifications to computing previously unknown temporal properties of a system. Formally, temporal logic queries are patterns of temporal logic specifications which contain placeholders for subformulas; a solution to a temporal logic query is an instantiation which renders the specification true. In this paper, we investigate temporal logic queries that can be solved deterministically, i.e., solving such queries can be reduced in a deterministic manner to solving their subqueries at appropriate system states. We show that this kind of determinism is intimately related to the notion of intermediate collecting queries studied by the authors in previous work. We describe a large class of deterministically solvable CTL queries and devise a BDD-based symbolic algorithm for this class.", "num_citations": "6\n", "authors": ["50"]}
{"title": "we must keep going i guess\n", "abstract": " Ensuring Media Integrity on Third-Party Infrastructures In many heterogeneous networked applications the integrity of multimedia data plays an essential role, but is not directly supported by the application. In this paper, we propose a method which enables an individual user to detect tampering with a multimedia file without changing the software application provided by the third party. Our method is based on a combination of cryptographic signatures and fragile watermarks, i.e., watermarks that are destroyed by illegitimate tampering. We show that the proposed system is provably secure under standard cryptographic assumptions.", "num_citations": "6\n", "authors": ["50"]}
{"title": "On the undecidability of some sub-classical first-order logics\n", "abstract": " A general criterion for the undecidabily of sub-classical firstorder logics and important fragments thereof is established. It is applied, among others, to Urquart\u0393\u00c7\u00d6s (original version of) C and the closely related logic C*.In addition, hypersequent systems for (first-order) C and C* are introduced and shown to enjoy cut-elimination.", "num_citations": "6\n", "authors": ["50"]}
{"title": "Verification across intellectual property boundaries\n", "abstract": " In many industries, the importance of software components provided by third-party suppliers is steadily increasing. As the suppliers seek to secure their intellectual property (IP) rights, the customer usually has no direct access to the suppliers\u0393\u00c7\u00d6 source code, and is able to enforce the use of verification tools only by legal requirements. In turn, the supplier has no means to convince the customer about successful verification without revealing the source code. This article presents an approach to resolve the conflict between the IP interests of the supplier and the quality interests of the customer. We introduce a protocol in which a dedicated server (called the \u0393\u00c7\u00a3amanat\u0393\u00c7\u00a5) is controlled by both parties: the customer controls the verification task performed by the amanat, while the supplier controls the communication channels of the amanat to ensure that the amanat does not leak information about the source code. We argue\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "5\n", "authors": ["50"]}
{"title": "A guide to quantified propositional G\u251c\u2562del logic\n", "abstract": " G\u251c\u2562del logic is a non-classical logic which naturally turns up in a number of different areas within logic and computer science. By choosing subsets of the unit interval [0, 1] as the underlying set of truthvalues many different G\u251c\u2562del logics have been defined. Unlike in classical logic, adding propositional quantifiers to G\u251c\u2562del logics in many cases increases the expressive power of the logic, and motivates thorough investigation. In a series of recent papers [8, 7, 6, 5, 4], we have started a research program to investigate quantified G\u251c\u2562del logics in a systematic manner. In this paper, we survey the results obtained so far. In the conclusion, we outline the future directions of this research program.", "num_citations": "5\n", "authors": ["50"]}
{"title": "Shape and content: Incorporating domain knowledge into shape analysis\n", "abstract": " The verification community has studied dynamic data structures primarily in a bottom-up way by analyzing pointers and the shapes induced by them. Recent work in fields such as separation logic has made significant progress in extracting shapes from program source code. Many real world programs however manipulate complex data whose structure and content is most naturally described by formalisms from object oriented programming and databases. In this paper, we look at the verification of programs with dynamic data structures from the perspective of content representation. Our approach is based on description logic, a widely used knowledge representation paradigm which gives a logical underpinning for diverse modeling frameworks such as UML and ER. Technically, we assume that we have separation logic shape invariants obtained from a shape analysis tool, and requirements on the program data in terms of description logic. We show that the two-variable fragment of first order logic with counting and trees %(whose decidability was proved at LICS 2013) can be used as a joint framework to embed suitable fragments of description logic and separation logic.", "num_citations": "4\n", "authors": ["50"]}
{"title": "Verification across intellectual property boundaries\n", "abstract": " In many industries, the share of software components provided by third-party suppliers is steadily increasing. As the suppliers seek to secure their intellectual property (IP) rights, the customer usually has no direct access to the suppliers\u0393\u00c7\u00d6 source code, and is able to enforce the use of verification tools only by legal requirements. In turn, the supplier has no means to convince the customer about successful verification without revealing the source code. This paper presents a new approach to resolve the conflict between the IP interests of the supplier and the quality interests of the customer. We introduce a protocol in which a dedicated server (called the \u0393\u00c7\u00a3amanat\u0393\u00c7\u00a5) is controlled by both parties: the customer controls the verification task performed by the amanat, while the supplier controls the communication channels of the amanat to ensure that the amanat does not leak information about the source code. We\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "4\n", "authors": ["50"]}
{"title": "On the complexity of data disjunctions\n", "abstract": " We study the complexity of data disjunctions in disjunctive deductive databases (DDDBs). A data disjunction is a disjunctive ground clause R (c \u2560\u00e4 1)\u0393\u00ef\u00bb R (c \u2560\u00e4 k), k\u0393\u2310\u255b 2, which is derived from the database such that all atoms in the clause involve the same predicate R. We consider the complexity of deciding existence and uniqueness of a minimal data disjunction, as well as actually computing one, both for propositional (data) and nonground (program) complexity of the database. Our results extend and complement previous results on the complexity of disjunctive databases, and provide newly developed tools for the analysis of the complexity of function computation.", "num_citations": "4\n", "authors": ["50"]}
{"title": "Brief announcement: parameterized model checking of fault-tolerant distributed algorithms by abstraction\n", "abstract": " We introduce an automated method for parameterized verification of fault-tolerant distribed algorithms. It rests on a novel parametric interval abstraction (PIA) technique, which works for systems with multiple parameters, for instance, where n and t are parameters describing the system size and the bound on the number of faulty processes, respectively. The PIA technique allows to map typical threshold-range intervals like [1, t+ 1) and [t+ 1, nt) to values from a finite abstract domain. Applying PIA to both the local states of the processes and the global system state, the parameterized verification problem can be reduced to finite-state model checking. We demonstrate the practical feasibility of our method by verifying several variants of the well-known consistent broadcasting algorithm by Srikanth and Toueg for different fault models. To the best of our knowledge, this is the first successful automated parameterized\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "3\n", "authors": ["50"]}
{"title": "Computer-Aided Verification\n", "abstract": " \u2568\u00fa\u2568\u255d\u2568\u255b\u2568\u255d \u2568\u00e1\u2568\u255b\u2564\u00fc\u2564\u00fc\u2568\u2555\u2564\u00c4 \u2568\u255c\u2568\u2561 \u2568\u2510\u2568\u255b\u2568\u255c\u2564\u00c5\u2564\u00e9\u2564\u00ee, \u2568\u00c9\u2564\u00c7\u2564\u00ea\u2568\u2555\u2568\u255c\u2568\u255b\u2568\u255d \u2568\u255b\u2568\u2592\u2564\u00eb\u2568\u2555\u2568\u255d \u2568\u255c\u2568\u2561 \u2568\u2555\u2568\u2556\u2568\u255d\u2568\u2561\u2564\u00c7\u2568\u2555\u2564\u00e9\u2564\u00ee: \u2568\u00fa \u2568\u255c\u2568\u2561\u2568\u2563 \u2568\u255b\u2564\u00fc\u2568\u255b\u2568\u2592\u2568\u2561\u2568\u255c\u2568\u255c\u2568\u2591\u2564\u00c5 \u2564\u00fc\u2564\u00e9\u2568\u2591\u2564\u00e9\u2564\u00ee\u0393\u00c7\u00f6\u2568\u00c6 \u2568\u00e1\u2568\u255b\u2564\u00fc\u2564\u00fc\u2568\u2555\u2564\u00c4 \u2568\u255d\u2568\u255b\u2568\u2562\u2568\u255c\u2568\u255b \u2564\u00e9\u2568\u255b\u2568\u2557\u2564\u00ee\u2568\u2551\u2568\u255b \u2568\u2593\u2568\u2561\u2564\u00c7\u2568\u2555\u2564\u00e9\u2564\u00ee.", "num_citations": "3\n", "authors": ["50"]}
{"title": "Starting a dialog between model checking and fault-tolerant distributed algorithms\n", "abstract": " Fault-tolerant distributed algorithms are central for building reliable spatially distributed systems. Unfortunately, the lack of a canonical precise framework for fault-tolerant algorithms is an obstacle for both verification and deployment. In this paper, we introduce a new domain-specific framework to capture the behavior of fault-tolerant distributed algorithms in an adequate and precise way. At the center of our framework is a parameterized system model where control flow automata are used for process specification. To account for the specific features and properties of fault-tolerant distributed algorithms for message-passing systems, our control flow automata are extended to model threshold guards as well as the inherent non-determinism stemming from asynchronous communication, interleavings of steps, and faulty processes. We demonstrate the adequacy of our framework in a representative case study where we formalize a family of well-known fault-tolerant broadcasting algorithms under a variety of failure assumptions. Our case study is supported by model checking experiments with safety and liveness specifications for a fixed number of processes. In the experiments, we systematically varied the assumptions on both the resilience condition and the failure model. In all cases, our experiments coincided with the theoretical results predicted in the distributed algorithms literature. This is giving clear evidence for the adequacy of our model. In a companion paper, we are addressing the new model checking techniques necessary for parametric verification of the distributed algorithms captured in our framework.", "num_citations": "3\n", "authors": ["50"]}
{"title": "Semantic integrity in large-scale online simulations\n", "abstract": " As large-scale online simulations such as Second Life and World of Warcraft are gaining economic significance, there is a growing incentive for attacks against such simulation software. We focus on attacks against the semantic integrity of the simulation. This class of attacks exploits the client-server architecture and is specific to online simulations which, for performance reasons, have to delegate the detailed rendering of the simulated world to the clients. Attacks against semantic integrity often compromise the physical laws of the simulated world\u0393\u00c7\u00f6enabling the user's simulation persona to fly, walk through walls, or to run faster than anybody else. We introduce the Secure Semantic Integrity Protocol (SSIP), which enables the simulation provider to audit the client computations. Then we analyze the security and scalability of SSIP. First, we show that under standard cryptographic assumptions SSIP will detect semantic\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "3\n", "authors": ["50"]}
{"title": "Closure properties and complexity of rational sets of regular languages\n", "abstract": " The test specification language FQL describes relevant test goals as regular expressions over program locations, such that each matching test case has an execution path matching this expression. To specify not only test goals but entire suites, FQL describes families of related test goals by regular expressions over extended alphabets: Herein, each symbol corresponds to a regular expression over program locations, and thus, a word in an FQL expression corresponds to a regular expression describing a single test goal. In this paper we provide a systematic foundation for FQL test specifications, which are in fact rational sets of regular languages (RSRLs). To address practically relevant problems like query optimization, we tackle open questions about RSRLs: We settle closure properties of general and finite RSRLs under common set theoretic operations. We also prove complexity results for checking equivalence\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "2\n", "authors": ["50"]}
{"title": "Monadic second order finite satisfiability and unbounded tree-width\n", "abstract": " The finite satisfiability problem of monadic second order logic is decidable only on classes of structures of bounded tree-width by the classic result of Seese (1991). We prove the following problem is decidable: Input: (i) A monadic second order logic sentence , and (ii) a sentence  in the two-variable fragment of first order logic extended with counting quantifiers. The vocabularies of  and  may intersect. Output: Is there a finite structure which satisfies  such that the restriction of the structure to the vocabulary of  has bounded tree-width? (The tree-width of the desired structure is not bounded.) As a consequence, we prove the decidability of the satisfiability problem by a finite structure of bounded tree-width of a logic extending monadic second order logic with linear cardinality constraints of the form , where the  and  are monadic second order variables. We prove the decidability of a similar extension of WS1S.", "num_citations": "2\n", "authors": ["50"]}
{"title": "Perspectives on White-Box Testing: Coverage, Concurrency, and Concolic Execution\n", "abstract": " The last years have seen a fruitful exchange of ideas between automated software verification and white-box software testing; the industrial impact of concolic testing for sequential software is the most notable result of this interdisciplinary effort. While concolic testing is very successful at finding bugs, and even achieves verification in the limit, it is often hard to quantify the progress it achieves towards verification. In this paper, we survey two recent projects which aim to remedy this situation: In the FQL project, we devise a test specification language which facilitates precise specification of coverage criteria, and a separation of concerns between test specification and test case generation. In con2colic testing, we develop a concolic testing methodology for concurrent programs where progress is measured in terms of the data flow between program threads.", "num_citations": "2\n", "authors": ["50"]}
{"title": "On the distributivity of LTL specifications\n", "abstract": " In this article, we investigate LTL specifications where \u256c\u2502[\u2567\u00e5 \u0393\u00ea\u00ba \u2567\u00ea] is equivalent to \u256c\u2502[\u2567\u00e5] \u0393\u00ea\u00ba \u256c\u2502[\u2567\u00ea] independent of \u2567\u00e5 and \u2567\u00ea. Formulas \u256c\u2502 with this property are called distributive queries because they naturally arise in Chan's seminal approach to temporal logic query solving [Chan 2000]. As recognizing distributive LTL queries is PSpace-complete, we consider distributive fragments of LTL defined by templates as in Buccafurri et al. [2001]. Our main result is a syntactic characterization of distributive LTL queries in terms of LTL templates: we construct a context-free template grammar LTLQx which guarantees that all specifications obtained from LTLQx are distributive, and all templates not obtained from LTLQx have simple nondistributive instantiations.", "num_citations": "2\n", "authors": ["50"]}
{"title": "An iterative framework for simulation conformance\n", "abstract": " MAGIC is a software verification project for C source code which verifies conformance of software components against statemachine specifications. To this aim, MAGIC extracts abstract software models using predicate abstraction, and resolves the inherent trade-off between model accuracy and scalability by an iterative abstraction refinement methodology. This paper presents the core principles implemented in the MAGIC verification engine, i.e. specification conformance using simulation and abstraction refinement. Viewing counterexamples as winning strategies in a simulation game between the implementation and the specification, we describe an algorithm where abstractions are refined on the basis of multiple winning strategies simultaneously. The refinement process is iterated until either a conformance with the specification is established, or a strategy to violate the specification is found to be\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "2\n", "authors": ["50"]}
{"title": "A novel SAT procedure for linear real arithmetic\n", "abstract": " The satisfiability of linear arithmetic logic is an important question with applications in verification and constraint satisfaction, and has been studied intensively during the last years. Most existing procedures attempt to decouple the SAT solver and the mathematical solver; this decoupled approach has the advantage that state-of-the-art solvers can be employed, but the blindness of the SAT solver with respect to arithmetic content requires subtle protocols to exploit the output of the mathsolver in the SAT DPLL procedure. In this paper, we propose to integrate mathematical constraint solving capabilities directly into the SAT solver. We introduce a new solver for linear arithmetic logic which is based on the classical SAT paradigms, but treats the SAT literals according to their arithmetic interpretation, employing stepwise Fourier-Motzkin elimination on the fly during the DPLL search. Characteristically, our solver introduces\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "2\n", "authors": ["50"]}
{"title": "Forester\u0393\u00c7\u00f6Tool for Verification of Programs with Pointers\n", "abstract": " Forester is a GCC 4.5+ plugin for verification of programs which manipulate complex dynamic data structures. It is based on the well-known CEGAR framework [1] and represents the set of program states using regular tree automata (for similar approach see [2]). Note that the tool is an early prototype which handles only a very restricted subset of program constructions.", "num_citations": "2\n", "authors": ["50"]}
{"title": "Extending ALCQIO with reachability\n", "abstract": " We introduce a description logic ALCQIO_{b,Re} which adds reachability assertions to ALCQIO, a sub-logic of the two-variable fragment of first order logic with counting quantifiers. ALCQIO_{b,Re} is well-suited for applications in software verification and shape analysis. Shape analysis requires expressive logics which can express reachability and have good computational properties. We show that ALCQIO_{b,Re} can describe complex data structures with a high degree of sharing and allows compositions such as list of trees. We show that the finite satisfiability and implication problems of ALCQIO_{b,Re}-formulae are polynomial-time reducible to finite satisfiability of ALCQIO-formulae. As a consequence, we get that finite satisfiability and finite implication in ALCQIO_{b,Re} are NEXPTIME-complete. Description logics with transitive closure constructors have been studied before, but ALCQIO_{b,Re} is the first description logic that remains decidable on finite structures while allowing at the same time nominals, inverse roles, counting quantifiers and reachability assertions,", "num_citations": "1\n", "authors": ["50"]}
{"title": "Loop patterns in C programs\n", "abstract": " Loop statements are an indispensable construct to express repetition in imperative programming languages. At the same time, their ability to express indefinite iteration entails undecidable problems, as famously demonstrated by Turing's work on the halting problem. Due to these theoretic limitations, efficacy of a termination procedure can only be demonstrated on practical problems - by Rice's theorem the same goes for any automated program analysis. Even though we have techniques to tackle some but by far not all decidable cases, so far no established notion of difficulty with regard to handling loops exists in the scientific community. Consequently, the occurrence of such \"simple\" and \"difficult\" loops in practice has not yet been investigated. We propose to conduct a systematic study that describes classes of loops and empirically determines difficulty of automated program analysis for each class. To arrive at this classification, we draw on a recent direction in software verification: Instead of treating programs merely as mathematical objects, we also consider them as human artifacts that carry important information directed at human readers of the program. This engineering information should be leveraged for automated program analysis, but is usually provided informally (e.g. as comments) or implicitly (e.g. as patterns followed by programmers). Thus, a formally sound framework to extract such information about loops is needed. A direct application of this study is the empiric evaluation of automated program analysis: As discussed, such an evaluation is of central importance to establish academic merit as well as applicability to the software\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["50"]}
{"title": "Decision Procedures in Soft, Hard and Bio-ware-Follow Up (Dagstuhl Seminar 11272)\n", "abstract": " This report documents the program and the outcomes of Dagstuhl Seminar 11272\" Decision Procedures in Soft, Hard and Bio-ware (Follow Up)\". It was held as a follow-on for a seminar 10161, of the same title, that took place in late April 2010 during the initial eruption of Eyjafjallaj\u251c\u2562kull. In spite of the travel disruptions caused by the eruption of the volcano, the original seminar received a respectable turnout by European, mainly German and Italian participants. Unfortunately, the eruption hindered participation from overseas or even more distant parts of Europe. This caused the seminar to cover only part of the original objective. The follow-on seminar focused on the remaining objectives, in particular to bio-ware and constraint solving methods.", "num_citations": "1\n", "authors": ["50"]}
{"title": "Embedding Formal Methods into Systems Engineering.\n", "abstract": " Embedding Formal Methods into Systems Engineering Page 1 1 1 Helmut Veith FORSYTE Group, TU Darmstadt School of Computer Science, Carnegie Mellon University Embedding Formal Methods into Systems Engineering H. Veith 5/2008 Power consumption ? Safety ? Certification ? ECU distribution ? Cost per car ? HW or SW ? FPGA or ASIC ? Multicore ? Weight ? Repair logistics ? Product Lines ? Variant Management ? Physical Environment ? Execution Time ? Dependability ? Obsoliscence ? Mechanical engineering process ? Component Reuse ? Experience from Collaborations with Automotive / Avionic Industry Intellectual Property ? Memory use ? Deployment strategy ? Page 2 2 3 Garmisch-Partenkirchen 1968 NATO Conference on Software Engineering mastering the complexity of system design H. Veith 5/2008 \u0393\u00c7\u00aa a clear separation of concerns emerges: we might call them the mathematical concerns .\u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["50"]}
{"title": "Brief announcement: efficient model checking of fault-tolerant distributed protocols using symmetry reduction\n", "abstract": " Motivation. Fault-tolerant (FT) distributed protocols represent fundamental building blocks behind many practical systems. A rigorous design of these protocols is desired given the complexity of manual proofs. The application of model checking (MC) [2] for protocol verification is attractive with its full automation and rich property language. However, being an exhaustive exploration method, its scalability is limited by the number of different system states. Although FT distributed protocols usually display a high degree of symmetry which stems from permuting different processes, MC efforts targeting their automated verification often disregard this symmetry. Therefore, we propose to leverage the framework of symmetry reduction [6] and improve on existing applications of it. Our secondary contribution is to define a high-level description language (called FTDP) to ease the symmetry-aware specification\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["50"]}
{"title": "Towards a systematic design of fault-tolerant asynchronous circuits\n", "abstract": " Accommodating billions of transistors on a single die, VLSI technology has reached a scale where principal physical limitations have a strong impact on design principles. Among the particular challenges are maintaining the synchronous clock abstraction in settings where wiring delays dominate over switching delays, and coping with increasing transient failure rates. In an attempt to address some of these challenges, we recently developed a clocking scheme called DARTS1. The cornerstone of this approach is a distributed fault-tolerant Tick Generation (TG) unit that implements an adaptation of a faulttolerant clock synchronization algorithm originally developed in the distributed computing context [1]. Each functional unit on the chip is augmented with a dedicated TG unit here that generates its clock signal. Since all these TG units communicate with each other (by exchanging their clock signals), all non-faulty TG units actually supply mutually synchronized clock signals to their attached functional units [2]. The implementation of the TG units [3] required a design process that was quite different from the traditional one: First, the TG circuitry dedicated to generating the chip\u0393\u00c7\u00d6s clock signals naturally mandated an implementation in asynchronous logic. Second, our fault-tolerance requirements made it necessary to cope with lost/faulty clock signal transitions, which rendered a delay-insensitive approach impossible. And last but not least, we had to bridge the gap between the highlevel distributed algorithm\u0393\u00c7\u00d6s view with its mathematical proofs, and the low-level VLSI implementation with its tool-based verification techniques. This paper (resp. its extended\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["50"]}
{"title": "Friends or foes? Communities in software verification\n", "abstract": " In contrast to hardware which is finite-state and based on relatively few ample principles, software systems generally give rise to infinite state spaces, and are described in terms of programming languages involving rich semantical concepts. The challenges of software verification can be addressed only by a combined effort of different communities including, most notably, model checking, theorem proving, symbolic computation, static analysis, compilers, and abstract interpretation. We focus on a recent family of tools which use predicate abstraction and theorem proving to extract a finite state system amenable to model checking.", "num_citations": "1\n", "authors": ["50"]}
{"title": "A Survey of Abstract BDDs\n", "abstract": " We survey and unify recent work about abstraction within BDD packages. Abstract BDDs (aBDDs) are obtained from ordinary BDDs by merging BDD nodes whose abstract values coincide. We discuss four types of abstract BDDs (called S-type, 0-type, 1-type and\u0393\u00ea\u00bf-type aBDDs) which have found applications in many CAD-related areas such as equivalence checking, variable ordering and model checking. Experimental results demonstrate the usefulness of our method.", "num_citations": "1\n", "authors": ["50"]}