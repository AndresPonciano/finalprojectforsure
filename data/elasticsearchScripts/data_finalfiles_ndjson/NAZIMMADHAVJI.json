{"title": "Software cost estimation with incomplete data\n", "abstract": " The construction of software cost estimation models remains an active topic of research. The basic premise of cost modeling is that a historical database of software project cost data can be used to develop a quantitative model to predict the cost of future projects. One of the difficulties faced by workers in this area is that many of these historical databases contain substantial amounts of missing data. Thus far, the common practice has been to ignore observations with missing data. In principle, such a practice can lead to gross biases and may be detrimental to the accuracy of cost estimation models. We describe an extensive simulation where we evaluate different techniques for dealing with missing data in the context of software cost modeling. Three techniques are evaluated: listwise deletion, mean imputation, and eight different types of hot-deck imputation. Our results indicate that all the missing data techniques\u00a0\u2026", "num_citations": "292\n", "authors": ["122"]}
{"title": "A field study of requirements engineering practices in information systems development\n", "abstract": " To make recommendations for improving requirements engineering processes, it is critical to understand the problems faced in contemporary practice. We describe a field study whose general objectives were to formulate recommendations to practitioners for improving requirements engineering processes, and to provide directions for future research on methods and tools. The results indicate that there are seven key issues of greatest concern in requirements engineering practice. These issues are discussed in terms of the problems they represent, how these problems are addressed successfully in practice, and impediments to the implementation of such good practices.", "num_citations": "220\n", "authors": ["122"]}
{"title": "The process cycle (software engineering)\n", "abstract": " Historically, the process of software development has played an important role in the field of software engineering. A number of software life-cycle models have been developed. These models do not expose myriad details that are critical in any large software development project. Recent developments, however, have unfolded many hidden aspects of the software process, giving rise to a new discipline, that called software process engineering. The author depicts software process in the context of software environments, examines recent developments in the process field and proposes the concept of process cycle, which embodies the scope of engineering and evolution of software processes. The author describes the details of the process cycle, including such issues as the role of corporate goals and policies in the engineering, management, performance and improvement of software processes; the\u00a0\u2026", "num_citations": "162\n", "authors": ["122"]}
{"title": "Software evolution and feedback: Theory and practice\n", "abstract": " Evolution of software has long been recognized as one of the most problematic and challenging areas in the field of software engineering, as evidenced by the high, often up to 60-80%, life-cycle costs attributed to this activity over the life of a software system. Studies of software evolution are central to the understanding and practice of software development. Yet it has received relatively little attention in the field of software engineering. This book focuses on topics aimed at giving a scientific insight into the aspect of software evolution and feedback. In summary, the book covers conceptual, phenomenological, empirical, technological and theoretical aspects of the field of software evolution-with contributions from the leading experts. This book delivers an up-to-date scientific understanding of what software evolution is, to show why it is inevitable for real world applications, and it demonstrates the role of feedback in software development and maintenance. The book also addresses some of the phenomenological and technological underpinnings and includes rules and guidelines for increased software evolvability and, in general, sustainability of the evolution process. Software Evolution and Feedback provides a long overdue, scientific focus on software evolution and the role of feedback in the software process, making this the indispensable guide for all software practitioners, researchers and managers in the software industry.", "num_citations": "135\n", "authors": ["122"]}
{"title": "The impact of tools on software productivity\n", "abstract": " It is common knowledge that to stay competitive, your software organization must continuously improve product quality and customer satisfaction, as well as lower software development costs and shorten delivery time. The paper considers how software tools are an effective way to improve software development variables such as productivity and product quality. It considers software tool selection and process improvement costs.", "num_citations": "120\n", "authors": ["122"]}
{"title": "Prism= methodology+ process-oriented environment\n", "abstract": " A description is given of Prism, an experimental process-oriented environment supporting methodical development, instantiation, and execution of software process models. Also described is an architecture that captures this model in its various components. The architecture has been designed to hold a product software process description, the life cycle of which is supported by an explicit representation of a higher level (or meta) process description. Also described is the nine-step Prism methodology for building and tailoring process models, and several scenarios to support this description are given. In Prism, process models are built using a hybrid process modeling language, which is based on a high-level Petri net formalism and rules. The Prism prototype has been implemented on UNIX, GRAS database for attributed graph structures, and the Sunview user interface.< >", "num_citations": "110\n", "authors": ["122"]}
{"title": "Environment evolution: The Prism model of changes\n", "abstract": " In the Prism project, a model of changes and its supporting 2 change-related environment infrastructures was developed with the following key features: 1. separation of changes to the described items from the changes to the environmental facilities encapsulating these items, 2. a facility, called the Dependency Structure, for describing various items and their interdependencies and for identifying the items affected by a given change, 3. a facility, named the Change Structure, for classifying, recording, and analyzing change-related data and for making qualitative judgments of the consequences of a change, 4. identification of the many distinct properties of a change, and 5. a built-in mechanism for providing feedback. The Prism model of changes provides a strong basis for a practical system that can be used in industrial and other environments. In particular, the model suggests the use of infrastructures without which\u00a0\u2026", "num_citations": "107\n", "authors": ["122"]}
{"title": "User participation in the requirements engineering process: An empirical study\n", "abstract": " In the development of information systems, user participation in the requirements engineering (RE) process is hypothesised to be necessary for RE success. In this paper we develop a theoretical model which predicts that the interaction between user participation in the RE process and uncertainty has an impact on RE success. This theory is empirically tested using survey data. We develop instruments to measure user participation and uncertainty. An existing instrument for measuring RE success was used. This instrument covers two dimensions of RE success: (a) the quality of RE service, and (b) the quality of RE products. The results, indicate that as uncertainty increases, greater user participation alleviates the negative influence of uncertainty on the quality of RE service, and that as uncertainty decreases, the beneficial effects on the quality of RE service of increasing user participation diminish\u00a0\u2026", "num_citations": "100\n", "authors": ["122"]}
{"title": "A reverse engineering environment based on spatial and visual software interconnection models\n", "abstract": " Reverse engineering is the process of extracting system abstractions and design information out of existing software systems. This information can then be used for subsequent development, maintenance, re-engineering, or reuse purposes. This process involves the identification of software artifacts in a particular subject system, and the aggregation of these artifacts to form more abstract system representations. This paper describes a reverse engineering environment which uses the spatial and visual information inherent in graphical representations of software systems to form the basis of a software interconnection model. This information is displayed and manipulated by the reverse engineer using an interactive graph editor to build subsystem structures out of software building blocks. The spatial component constitutes information about how a software structure looks. The coexistence of these two\u00a0\u2026", "num_citations": "100\n", "authors": ["122"]}
{"title": "Measuring the success of requirements engineering processes\n", "abstract": " Central to understanding and improving requirements engineering processes is the ability to measure requirements engineering success. The paper describes a research study whose objective was to develop an instrument to measure the success of requirements engineering processes. The instrument developed consists of 32 indicators that cover the two most important dimensions of requirements engineering success. These two dimensions were identified during the study to be: quality of requirements engineering products and quality of requirements engineering service. Evidence is presented demonstrating that the instrument has desirable psychometric properties, such as high reliability and validity.", "num_citations": "86\n", "authors": ["122"]}
{"title": "Validating object-oriented design metrics on a commercial java application\n", "abstract": " Many of the object-oriented metrics that have been developed by the research community are believed to measure some aspect of complexity. As such, they can serve as leading indicators of problematic classes, for example, those classes that are most fault-prone. If faulty classes can be detected early in the development project\u2019s life cycle, mitigating actions can be taken, such as focused inspections. Prediction models using design metrics can be used to identify faulty classes early on. In this paper, we present a cognitive theory of object-oriented metrics and an empirical study which has as objectives to formally test this theory while validating the metrics and to build a post-release fault\u2013proneness prediction model. The cognitive mechanisms which we apply in this study to object-oriented metrics are based on contemporary models of human memory. They are: familiarity, interference, and fan effects. Our empirical study was performed with data from a commercial Java application. We found that Depth of Inheritance Tree (DIT) is a good measure of familiarity and, as predicted, has a quadratic relationship with fault\u2013proneness. Our hypotheses were confirmed for Import Coupling to other classes, Export Coupling and Number of Children metrics. The Ancestor based Import Coupling metrics were not associated with fault-proneness after controlling for the confounding effect of DIT. The prediction model constructed had a good accuracy. Finally, we formulated a cost savings model and applied it to our predictive model. This demonstrated a 42% reduction in post-release costs if the prediction model is used to identify the classes that should be\u00a0\u2026", "num_citations": "70\n", "authors": ["122"]}
{"title": "Elements of software process assessment & improvement\n", "abstract": " From the Publisher: Elements of Software Process Assessment and Improvement reviews current assessment practices, experiences, and new research trends in software process improvement. Revised chapters expanded from articles in The Software Process Newsletter of the IEEE Computer Society Technical Council on Software Engineering, describe the improvement cycle in detail from diagnosing an organization, establishing a business case, and changing elements within a process to final evaluation.", "num_citations": "63\n", "authors": ["122"]}
{"title": "Elicit: A method for eliciting process models\n", "abstract": " Eliciting process models from software projects is a first significant step towards process improvement. In this paper, we present a method, called Elicit, for eliciting software process models from industrial software environments. What is significant about this method is that it has evolved from an intuitive state-the state that defines the immaturity of current elicitation methods-to a formally defined, repeatable, effective and quantified state. Over the last two years of its usage, the method has been used to elicit models from three industrial-scale processes: preliminary analysis, requirements engineering, and product planning and dependency management. The example given in the paper focuses on the requirements engineering process.< >", "num_citations": "55\n", "authors": ["122"]}
{"title": "The reliability of measuring organizational maturity\n", "abstract": " One of the recently developed classes of decision making tools for software engineering management is the organizational maturity assessment. The scores from such assessments are being applied in focusing and tracking self-improvement efforts, and as part of the contract award decision making process. However, until now, the important issue of the reliability of assessments has rarely been addressed by the developers and users of assessment methods. The extent of reliability describes the degree to which assessment scores are consistent and repeatable. In this paper we present some basic techniques for the estimation of the reliability of maturity assessments. We then demonstrate through a case study some of these techniques and how to apply the reliability estimate (s) in decision making situations. Examples of decisions are: comparing organizations' maturity scores and evaluating the relationship between maturity scores and some criterion of effectiveness. For the latter, we found a weak relationship betwen maturity and our measure of effectiveness, namely the success of the requirements engineering process.", "num_citations": "47\n", "authors": ["122"]}
{"title": "The prism model of changes\n", "abstract": " The author addresses the problem of managing changes to items of various types in a multitype software environment. Prism, a model of changes, has been designed with the following features:(1) a separation of concern between changes to the described items and changes to the environmental facilities housing these items;(2) a facility, called the dependency structure, for describing various items and their interdependencies, and for identifying the items affected by a given change;(3) a facility, called the change structure, for classifying, recording and analyzing change related data, and for making qualitative judgments of the consequences of a change;(4) identification of the many distinct properties of a change; and (5) a built-in mechanism for providing feedback. The rationale for the design of the model of changes as well as that of the dependency structure and the change structure is given.<>", "num_citations": "37\n", "authors": ["122"]}
{"title": "Implementing concepts from the Personal Software Process in an industrial setting\n", "abstract": " The Personal Software Process (PSP) has been taught at a number of universities with impressive results. If is also of interest to industry as a means for training their software engineers. While there are published reports on the teaching of PSP in classroom settings (at universities and industry), little systematic study has been conducted on the implementation of PSP in industry. Also, largely anecdotal evidence exists as to its effectiveness with real programming tasks. Effectiveness is measured in terms of the number of trained engineers who actually use PSP in their daily work, and improvements in productivity and defect removal. We report on a study of the implementation of some PSP concepts in a commercial organization. The empirical enquiry method that we employed was action research. Our results identify the problems that were encountered during the four major activities of an implementation of PSP\u00a0\u2026", "num_citations": "35\n", "authors": ["122"]}
{"title": "An instrument for measuring the success of the requirements engineering process in information systems development\n", "abstract": " There exists a strong motivation for evaluating, understanding, and improving requirements engineering practices given that a successful requirements engineering process is necessary for a successful software system. Measuring requirements engineering success is central to evaluation, understanding, and improving these practices. In this paper, a research study whose objective was to develop an instrument to measure the success of the requirements engineering process is described. The domain of this study is developing customer-specific business information systems. The main result is a subjective instrument for measuring requirements engineering success. The instrument consists of 32 indicators that cover the two most important dimensions of requirements engineering success. These two dimensions were identified during the study to be: quality of requirements engineering products and\u00a0\u2026", "num_citations": "35\n", "authors": ["122"]}
{"title": "Understanding quality requirements in the context of big data systems\n", "abstract": " While the domain of big data is anticipated to affect many aspects of human endeavour, there are numerous challenges in building big data applications among which is how to address big data characteristics in quality requirements. In this paper, we propose a novel, unified, approach for specifying big data characteristics (eg, velocity of data arrival) in quality requirements (ie, those requirements specifying attributes such as performance, reliability, availability, security, etc.). Several examples are given to illustrate the integrated specifications. As this is early work, further experimentation is needed in different big data situations and quality requirements and, beyond that, in a variety of project settings.", "num_citations": "29\n", "authors": ["122"]}
{"title": "Critical factors affecting personal software processes\n", "abstract": " Personal software process quality helps determine the success of software projects and organizations. Although encouraging, previous studies treated the Personal Software Process approach as a black-box tool for personal process improvement. We dig deeper into the factors affecting personal processes. It is concluded that A/FR (appraisal to failure ratio) and Yield (percentage of defects removed before first compile) are two critical factors affecting personal software processes. Because the findings are limited to one experiment involving subjects in a university setting, developers in an industrial environment should use them with caution. We expect readers to view this article as hopeful; we call for more of such studies in academia and industry. With additional studies, we can collectively build stronger theories underlying personal software processes.", "num_citations": "29\n", "authors": ["122"]}
{"title": "ACCA: An architecture-centric concern analysis method\n", "abstract": " The architecture of a software system is a key asset for a software business. While there are several architecting and evaluation methods, literature and practice are devoid of architecture-centric concernanalysis (ACCA) methods analogous to causal analysis methods for software defects. A concern is any aspect of an architecture considered undesirable. This paper describes an ACCA method which uses at its core a Concern Traceability map (CT-map) that captures architectural design decisions starting from software requirements and links them to identified architectural concerns. The CT-map essentially forms a net of design decisions, sandwiched between requirements and architectural concerns. Analysis of the root causes of a concern is then conducted on the CT-map. The ACCA method is empirically validated through a case study on a sizeable architecture of a banking application.", "num_citations": "27\n", "authors": ["122"]}
{"title": "Causal analysis of the requirements change process for a large system\n", "abstract": " Implementations of requirements change processes in large system projects face many difficulties. We present a method for analysing requirements change processes to identify implementation weaknesses and their causes. This method relies on prescriptive process models and tracing actual change proposals through the process model. We apply the method in the analysis of the requirements change process for a large real time system within a Canadian government agency. This allowed us to identify the process implementation problems, and the process, organisational, and people causes of these problems. Based on that experience, we draw general conclusions about the proposed method and its applicability", "num_citations": "27\n", "authors": ["122"]}
{"title": "Human-oriented improvement in the software process\n", "abstract": " By doing any task repeatedly, individuals can usually improve continuously due to the experience gained (called autonomous first-order learning). In addition, they can improve due to the injection of software development technology by the organization (called second-order learning). Organizations have studied such learning curves to make decisions regarding cost estimation and budgeting, production and labor scheduling, product pricing, etc. Such progress behavior was studied in a laboratory setting in an experiment involving a sample of 12 student software developers, who completed one small-sized project every week for ten weeks. A within-subject, repeated-measure, time-series quasi-experimental design was used as the research method. This also included the Goal/Question/Metric (GQM) paradigm with some additional validation techniques from Social Sciences/MIS/Software Engineering\u00a0\u2026", "num_citations": "27\n", "authors": ["122"]}
{"title": "Architecting-problems rooted in requirements\n", "abstract": " Requirements permeate many parts of the software development process outside the requirements engineering (RE) process. It is thus important to determine whether software developers in these other areas of software development face any requirements-oriented (RO) problems in carrying out their tasks. Feedback so obtained can be invaluable for improving both requirements and RE technologies. In this paper, we describe an exploratory case study of requirements-oriented problems experienced by 16 architecting teams designing the same banking application. The study found that there were several different types of RO problems, of varying severity, which the architects faced in using the given requirements; those architects with RE background also faced RO problems; and about a third of all problems were RO problems. There was much concurrence of our findings with software-expert opinion from a large\u00a0\u2026", "num_citations": "26\n", "authors": ["122"]}
{"title": "Does organizational maturity improve quality?\n", "abstract": " A study focused on the relationship between organizational maturity and the success of the software requirements engineering process. Two practices were found to lead to greater requirements engineering success.", "num_citations": "24\n", "authors": ["122"]}
{"title": "An exploratory study of architectural effects on requirements decisions\n", "abstract": " The question of the \u201cmanner in which an existing software architecture affects requirements decision-making\u201d is considered important in the research community; however, to our knowledge, this issue has not been scientifically explored. We do not know, for example, the characteristics of such architectural effects. This paper describes an exploratory study on this question. Specific types of architectural effects on requirements decisions are identified, as are different aspects of the architecture together with the extent of their effects. This paper gives quantitative measures and qualitative interpretation of the findings. The understanding gained from this study has several implications in the areas of: project planning and risk management, requirements engineering (RE) and software architecture (SA) technology, architecture evolution, tighter integration of RE and SA processes, and middleware in architectures\u00a0\u2026", "num_citations": "23\n", "authors": ["122"]}
{"title": "Customising software process models\n", "abstract": " A core activity in the evolution of software process models is their customisation [7]. Customisation adapts a generic process model to make it more effective in the given software project.Effectiveness of a customised process model 3 has two dimensions. The first dimension concerns implementation, which denotes the extent to which the process model is used and followed by practitioners in a particular project. The second dimension concerns software project effectiveness, which denotes the outcomes of the project (such as effort, cost, deliverables quality, predictability, etc.) if the process model is used and followed. To ensure effectiveness, one important property of a project-specific process model is congruence. Congruence is a measure of how fit a process model is in the given development environment in which it is (intended to be) used. The development environment is referred to as the process context. A\u00a0\u2026", "num_citations": "22\n", "authors": ["122"]}
{"title": "Computer software test coverage analysis\n", "abstract": " A computer implemented method, apparatus, and computer usable program code for performing software testing. A first set of traces is compressed to form a first set of compressed traces. The first set of compressed traces is compared to a plurality of additional traces to identify a set of partially matching traces within the plurality of additional traces. The first set of traces is compressed to form a second set of compressed traces. The second set of compressed traces is compressed according to a technique such that the second set of compressed traces contains more information than the first set of compressed traces. The second set of compressed traces is compared to the set of partially matching traces to identify a second set of partially matching traces within the set of partially matching traces. The second set of partially matching traces is placed in a memory.", "num_citations": "21\n", "authors": ["122"]}
{"title": "An empirical evaluation of the G/Q/M method\n", "abstract": " Improvement of the software process is a major concern for many organizations. A critical part of such an endeavor is the definition of metrics. Despite the importance of metric definition, there have been no evaluations of existing methods for achieving this. It is generally taken for granted that a method with wide acceptance is suitable. A review of metric definition methods identities Basili's G/Q/M as one of the most widely used. This paper reports on an evaluation of the G/Q/M method. The evaluation is based on an actual application of the method in a process improvement effort. The resultant metrics (and instrument) are evaluated with respect to the following criteria: interpretability, validity, reliability, effectiveness, and transportability. The causes of problems found are identified. These causes are problems with the G/Q/M method itself. The evaluation indicates that the metrics resultant from the application of the G\u00a0\u2026", "num_citations": "21\n", "authors": ["122"]}
{"title": "Characteristics of multiple-component defects and architectural hotspots: a large system case study\n", "abstract": " The architecture of a large software system is widely considered important for such reasons as: providing a common goal to the stakeholders in realising the envisaged system; helping to organise the various development teams; and capturing foundational design decisions early in the development. Studies have shown that defects originating in system architectures can consume twice as much correction effort as that for other defects. Clearly, then, scientific studies on architectural defects are important for their improved treatment and prevention. Previous research has focused on the extent of architectural defects in software systems. For this paper, we were motivated to ask the following two complementary questions in a case study: (i) How do multiple-component defects (MCDs)\u2014which are of architectural importance\u2014differ from other types of defects in terms of (a) complexity and (b) persistence across\u00a0\u2026", "num_citations": "19\n", "authors": ["122"]}
{"title": "The impact of requirements knowledge and experience on software architecting: An empirical study\n", "abstract": " While the relationship between Requirements Engineering and software architecture (SA) has been studied increasingly in the past five years in terms of methods, tools, development models, and paradigms, that in terms of the human agents conducting these processes has barely been explored. This paper describes the impact of requirements knowledge and experience (RKE) on SA tasks. Specifically, it describes an exploratory, empirical study involving a number of architecting teams, some with requirements background and others without, all architecting from the same set of requirements. The overall results of this study suggest that architects with RKE perform better than those without, and specific areas of architecting are identified where these differences manifest. We discuss the possible implications of the findings on the areas of training, education and technology.", "num_citations": "19\n", "authors": ["122"]}
{"title": "Architectural effects on requirements decisions: An exploratory study\n", "abstract": " The question of the \"manner in which an existing software architecture affects requirements decisionmaking\" is recognised as important in the research community; however, to our knowledge, this issue has not been scientifically explored. This paper describes an exploratory study on this question. Specific types of architectural effects on requirements decisions are identified, as are different aspects of the architecture together with the extent of their effects. This paper gives quantitative measures and qualitative interpretation of the findings. The understanding gained from this study has several implications in the areas of: project planning and risk management, requirements engineering and software architecture technology, architecture evolution, tighter integration of Requirements Engineering and Software Architecting processes, and middleware in architectures. The study involved six requirements engineering\u00a0\u2026", "num_citations": "17\n", "authors": ["122"]}
{"title": "An iterative, multi-level, and scalable approach to comparing execution traces\n", "abstract": " In this paper, we overview a new approach to comparing execution traces. Such comparison can be useful for purposes such as improving test coverage and profiling system's users. In our approach, traces are compressed into different levels of compaction and are then compared iteratively from highest to lowest levels, rejecting dissimilar traces in the process and eventually leaving residual, similar traces. These residual traces form an important feedback for improvement or analysis goals. The preliminary results show that the approach is scalable for industrial use.", "num_citations": "17\n", "authors": ["122"]}
{"title": "A controlled experiment to assess the impact of system architectures on new system requirements\n", "abstract": " While much research attention has been paid to transitioning from requirements to software architectures, relatively little attention has been paid to how new requirements are affected by an existing system architecture. Specifically, no scientific studies have been conducted on the \u201ccharacteristic\u201d differences between the newly elicited requirements gathered in the presence or absence of an existing software architecture. This paper describes an exploratory controlled study investigating such requirements characteristics. We identify a multitude of characteristics (e.g., end-user focus, technological focus, and importance) that were affected by the presence or absence of an SA, together with the extent of this effect. Furthermore, we identify the specific aspects of the architecture that had an impact on the characteristics. The study results have implications for RE process engineering, post-requirements analysis\u00a0\u2026", "num_citations": "16\n", "authors": ["122"]}
{"title": "A systematic, view-based approach to eliciting process models\n", "abstract": " Building a high quality descriptive process model of a software process is recognized to be important for a number of reasons, such as certification, assessment, improvement and tool insertion. Experience has shown, time and again, that such models require process information to be elicited from multiple sources. However, a problem noted with multiple sources is that, amongst other things, process information can be inconsistent in various ways. In this position paper, we describe some mechanisms for resolving this problem. In particular, the mechanisms help in: (i) drawing out information pertaining to a specific role in the process, yielding a process view; (ii) ensuring consistency within a given view; (iii) merging various views; and (iv) ensuring global coherence of the elicited process model. These mechanisms are an integral part of V-elicit, a prototype system for eliciting process models.", "num_citations": "15\n", "authors": ["122"]}
{"title": "Eliciting formal models of software engineering processes\n", "abstract": " One of the first steps in improving a process is to understand the process, for example, by building descriptive models of the process. This paper addresses the elicitation of formal process models from real-world projects, using the Elicit method and tool, developed at McGill, and Statemate. In addition, the paper illustrates how formal models can make processes visible and how they can uncover inconsistencies and incompleteness in the understanding and the documentation. The described method and tools have been used in several real-world software projects, and the paper describes some conclusions drawn from this experience. Example process models are shown, and a tool demonstration is included in the CD-ROM that is part of this publication.", "num_citations": "15\n", "authors": ["122"]}
{"title": "Impediments to regulatory compliance of requirements in contractual systems engineering projects: a case study\n", "abstract": " Large-scale contractual systems engineering projects often need to comply with myriad government regulations and standards as part of contractual obligations. A key activity in the requirements engineering (RE) process for such a project is to demonstrate that all relevant requirements have been elicited from the regulatory documents and have been traced to the contract as well as to the target system components. That is, the requirements have met regulatory compliance. However, there are impediments to achieving this level of compliance due to such complexity factors as voluminous contract, large number of regulatory documents, and multiple domains of the system. Little empirical research has been conducted in the scientific community on identifying these impediments. Knowing these impediments is a driver for change in the solutions domain (i.e., creating improved or new methods, tools, processes, etc\u00a0\u2026", "num_citations": "14\n", "authors": ["122"]}
{"title": "An empirical study on the use of mutant traces for diagnosis of faults in deployed systems\n", "abstract": " Debugging deployed systems is an arduous and time consuming task. It is often difficult to generate traces from deployed systems due to the disturbance and overhead that trace collection may cause on a system in operation. Many organizations also do not keep historical traces of failures. On the other hand earlier techniques focusing on fault diagnosis in deployed systems require a collection of passing\u2013failing traces, in-house reproduction of faults or a historical collection of failed traces. In this paper, we investigate an alternative solution. We investigate how artificial faults, generated using software mutation in test environment, can be used to diagnose actual faults in deployed software systems. The use of traces of artificial faults can provide relief when it is not feasible to collect different kinds of traces from deployed systems. Using artificial and actual faults we also investigate the similarity of function call traces\u00a0\u2026", "num_citations": "14\n", "authors": ["122"]}
{"title": "Investigation-based requirements engineering education\n", "abstract": " The norm in teaching requirements engineering to undergraduates is to follow a prescribed set of topics, eg, that proposed by IEEE/ACM task force on software engineering or that described in a suitable course text. There is no doubt that this teaches students about the established concepts in the field. In this position paper, we argue that there can be much added value to this baseline by involving students in investigative studies on innovative questions, to discover for themselves the outcome of the pursued investigations\u2013something they will not find elsewhere. The added value is in terms of creating new knowledge, gained through an empirical study in which students played a crucial role. However, there are risks in this endeavour that need to be contained; otherwise, despite good intentions, the journey can be painful and the outcome misleading.", "num_citations": "14\n", "authors": ["122"]}
{"title": "Modelling assumptions and requirements in the context of project risk\n", "abstract": " Many researchers have emphasized the importance of documenting assumptions (As) underlying software requirements (Rs). However, As and Rs can change with time for reasons such as: (i) an A or R was elicited incorrectly and subsequently needs to be changed; (ii) operational domain changes induce changes in the A and R sets; and (iii) the change in validity of an A, or desirability of an R, respectively, causes the validity of another A or desirability of an R to change. In Section 2, we describe our model and how it works. To put such a model into practice, we need to consider at least two scenarios. One is intra-release cycle-time, where invalidity risk is predicted at the start of the project for times between the inception and completion of the project. This would give us intra-release risk trends. The second scenario is prediction over multiple releases. This would give us a risk trend over a longer period of time\u00a0\u2026", "num_citations": "14\n", "authors": ["122"]}
{"title": "The impact of environmental evolution on requirements changes\n", "abstract": " We describe a case study of requirements changes due to an evolving environment. The Congruence Evaluation System was a proof of concept (CES POC) system which was part of a constantly evolving environment, and this change in the environment dictated the fitness of the CES POC system in the environment. The system served excellently for basic research purposes, but failed seriously in the long-term goal of evolvability with the set of coexisting research tools. We assessed the state of the system requirements, as well as that of the environment, at different times during system development and redevelopment. From this assessment we gained a detailed insight into how environmental evolution affects system survivability in terms of requirements changes. In this case study, we also found some empirical support for Lehman's (Lehman and Ramil, 2001) seventh law of software evolution, which has until\u00a0\u2026", "num_citations": "14\n", "authors": ["122"]}
{"title": "Adapting modules to an integrated programming environment\n", "abstract": " The design of modules in modular languages, and the model of software development activities portrayed by it, have been well received in batch environments. However, from their experience in the design of the MUPE-2 integrated programming environment, the authors hold the opinion that the model of software activities portrayed by modules in a batch environment is not entirely appropriate in an integrated programming environment. In adapting modules to the MUPE-2 environment, the authors have changed their design and implementation. In particular, they have added a module type, called the supermodule, that can encapsulate related modules, so that the architecture of a software system may be captured, and unified the separate definition and implementation parts of a module into a single module, called the DefImp module, so that some consistency problems due to textual separation can be avoided\u00a0\u2026", "num_citations": "14\n", "authors": ["122"]}
{"title": "Fragtypes: a basis for programming environments\n", "abstract": " The author introduces a novel basis for programming environments that encourages development of software in fragments of various types, called fragtypes. Fragtypes range from a simple expression type to a complete subsystem type. As a result, they are suited to the development of software in an enlarged scope that includes both programming in the small and programming in the large. The author shows how proposed operations on fragtypes can achieve unusual effects on the software development process. Fragtypes and their associated construction rules form the basis of the programming environment MUPE-2, which is currently under development at McGill University. The target and the implementation language of this environment is the programming language Modula-2.< >", "num_citations": "14\n", "authors": ["122"]}
{"title": "Analysis of pervasive multiple-component defects in a large software system\n", "abstract": " Certain software defects require corrective changes repeatedly in a few components of the system. One type of such defects spans multiple components of the system, and we call such defects pervasive multiple-component defects (PMCDs). In this paper, we describe an empirical study of six releases of a large legacy software system (of approx. size 20 million physical lines of code) to analyze PMCDs with respect to: (1) the complexity of fixing such defects and (2) the persistence of defect-prone components across phases and releases. The overall hypothesis in this study is that PMCDs inflict a greater negative impact than do other defects on defect-correction efficacy. Our findings show that the average number of changes required for fixing PMCDs is 20-30 times as much as the average for all defects. Also, over 80% of PMCD-contained defect-prone components still remain defect-prone in successive phases or\u00a0\u2026", "num_citations": "13\n", "authors": ["122"]}
{"title": "State of requirements engineering research in the context of big data applications\n", "abstract": " [Context and Motivation] Big Data applications, like traditional applications, serve end-user needs except that underlying the software system is Big Data which the system operates upon. In comparison to traditional software development where the development processes are usually well-established, the processes for the development of applications involving Big Data are not clear just yet from the scientific literature \u2013 given the nature of computing involved and data characteristics such as volume, variety, veracity, and velocity. [Question/Problem] This, uncertain situation, has given rise to new questions, that is: \u201cWhat are the early signs of the ways Big Data applications is treated in requirements engineering (RE)? What new directions in RE research are envisaged to promulgate further research?\u201d [Principal ideas/Results] This paper presents the state of the art of requirements engineering (RE\u00a0\u2026", "num_citations": "12\n", "authors": ["122"]}
{"title": "Impediments to requirements-compliance\n", "abstract": " [Context & motivation] Large contractual projects often have to comply against government regulations and standards. [Question/problem] In such a context, the contractual document can be voluminous, and there can be a large number of standards and regulations to follow. These documents typically form a complex interrelationship network. This means that in the requirements engineering (RE) process, this network needs to be analysed for deriving project requirements to be implemented. A key activity of this RE process is to demonstrate compliance by showing, through appropriate traces, that all relevant requirements have been elicited from the regulatory documents. [Principal ideas/results] [Contribution] In this problem-statement paper, we describe some key impediments to achieving requirements-compliance that we have identified in a large systems engineering project.", "num_citations": "12\n", "authors": ["122"]}
{"title": "An Approach to Requirements Encapsulation with Clustering.\n", "abstract": " Requirements encapsulation means organizing software requirements into a set of requirements clusters with tight cohesion along with external interfaces such that each cluster can be ultimately implemented by a functionality module. We propose an approach to encapsulating requirements which includes two steps: clustering requirements based on the similarity and associativity relations and then encapsulating each cluster by defining its external interface as stimulus-response pairs. The potential benefits of encapsulating requirements are reduced software development and maintenance costs.", "num_citations": "12\n", "authors": ["122"]}
{"title": "Requirements and systems architecture interaction in a prototypical project: Emerging results\n", "abstract": " [Context and Motivation] Subsequent to an exploratory laboratory study on the effects of Software Architecture (SA) on Requirements Engineering (RE), in this paper, we present preliminary results of an extension of this initial study by conducting a case study on a large-scale prototypical rail project. [Question/Problem] Specifically, we ask \u201cWhat is the role of an SA on Requirements decision-making?\u201d. [Principal Ideas/Results] Specific types of architectural effects on requirements decisions are identified. The impact of the affected requirements decisions on downstream processes and the product itself is also characterized. [Contribution] The understanding gained from this study has implications on such areas as: project planning, risk, RE, and others.", "num_citations": "11\n", "authors": ["122"]}
{"title": "The impact of non-technical factors on Software Architecture\n", "abstract": " Most of the research and pedagogical literature in Software Architecture is on technical issues. Recently, however, there has been increasing interest on the importance of non-technical factors such as leadership, communication, inter-personal skills, work habits etc. in architecting. In this paper, we continue this line of research by conducting an empirical study examining the impact of non-technical factors in Software Architecture from the viewpoint of academia. We analysed non-technical problems encountered from 15 student architecting teams to determine the types of problems students have, and also their impact on the quality of the architecture. Furthermore, we analyzed the IEEE/ACM Software Engineering and Computer Science curriculums to determine any correspondence between these curriculums and the student's architecting performance. Based on this analysis, we make recommendations for the\u00a0\u2026", "num_citations": "11\n", "authors": ["122"]}
{"title": "The Optimal Team Size for UML Design Inspections\n", "abstract": " Recent evidence indicates that the UML (Unified Modeling Language) is the most preferred and widely used modeling technique for object-oriented analysis and design. With UML becoming so popular, there is a need to have good quality assurance techniques for projects using it. Our focus in this study is on the inspections of UML design documents. The basic premise of software inspections is that they detect and remove defects before they propagate to subsequent development phases, where their detection and correction costs escalate. However, the performance of inspections can vary considerably, making it important to optimize inspections. One approach for optimizing inspections is by controlling the inspection team size. This paper presents an empirical evaluation of the optimal team size for UML design inspections. Our results show that there is no single optimal team size. Optimal team size in fact depends on various conditions such as the cost of defect detection late in the process, and inspection meeting duration. This paper quantifies these factors and proposes optimal team sizes under various conditions. Our results also indicate strongly that contemporary suggestions of only two-person inspection teams are far from optimal.", "num_citations": "11\n", "authors": ["122"]}
{"title": "Towards a requirements engineering artefact model in the context of big data software development projects: Research in progress\n", "abstract": " There is ample literature that suggests that the field of Big Data is growing rapidly. Also, there is emerging literature on the need to create end-user Big Data applications, as distinct from \u201cdata analytics\u201d that typically employs machine learning algorithms to find value in large datasets for the stakeholder. A solid foundation for creating sound applications is a thorough understanding of domain and artefact models that embody artefact types and activities involved in a software project. This paper focuses on the Requirements Engineering (RE) aspect of a Big Data software project. Currently, there are no known RE artefact models to support RE process design and project understanding. To fill this void, this paper proposes a RE artefact model for Big Data end-user applications (BD-REAM). The paper also describes a method for creating the artefact model, including the basic elements and inter-relationships involved in\u00a0\u2026", "num_citations": "10\n", "authors": ["122"]}
{"title": "Diagnosing new faults using mutants and prior faults (nier track)\n", "abstract": " Literature indicates that 20% of a program's code is responsible for 80% of the faults, and 50-90% of the field failures are rediscoveries of previous faults. Despite this, identification of faulty code can consume 30-40% time of error correction. Previous fault-discovery techniques focusing on field failures either require many pass-fail traces, discover only crashing failures, or identify faulty\" files\"(which are of large granularity) as origin of the source code. In our earlier work (the F007 approach), we identify faulty\" functions\"(which are of small granularity) in a field trace by using earlier resolved traces of the same release, which limits it to the known faulty functions. This paper overcomes this limitation by proposing a new\" strategy\" to identify new and old faulty functions using F007. This strategy uses failed traces of mutants (artificial faults) and failed traces of prior releases to identify faulty functions in the traces of\u00a0\u2026", "num_citations": "10\n", "authors": ["122"]}
{"title": "Characteristics of new requirements in the presence or absence of an existing system architecture\n", "abstract": " While much research attention has been paid to transitioning from requirements to software architectures, relatively little attention has been paid to how new requirements are affected by an existing system architecture. Specifically, no scientific studies have been conducted on the \"characteristic\" differences between the newly elicited requirements gathered in the presence or absence of an existing software architecture. This paper describes an exploratory controlled study investigating such requirements characteristics. We found that a multitude of characteristics (e.g., end-user focus, technological focus, and importance) were affected by the presence or absence of an SA, and the extent of this effect. The study results have implications for: RE process engineering, post-requirements analysis, and future empirical work in RE based on emergent hypotheses from this study.", "num_citations": "10\n", "authors": ["122"]}
{"title": "Towards a compliance meta-model for system requirements in contractual projects\n", "abstract": " In contractual systems engineering projects, the developing organization is often required to demonstrate compliance of the system's requirements against a myriad of engineering standards and government regulations. In order to satisfy this goal, the project requirements imposed by standards and regulations through the contract need to be traceable to/fro appropriate project artefacts (such as the contract, various system and sub-system requirements specifications, standards, regulatory documents, etc.). However, these artefacts form a complex interrelationship network, leading to significant challenges in demonstrating requirements compliance. Current practices dealing with such compliance are ad hoc and arduous. In this paper, we identify key artefacts, relationships and challenges that we are currently discovering from a case study on a large-scale, contractual, requirements compliance project. These\u00a0\u2026", "num_citations": "9\n", "authors": ["122"]}
{"title": "Software architecting without requirements knowledge and experience: What are the repercussions?\n", "abstract": " Whereas the relationship between Requirements Engineering and Software Architecture (SA) has been studied increasingly in recent years in terms of methods, notations, representations, tools, development paradigms and project experiences, that in terms of the human agents conducting these processes has not been explored scientifically. This paper describes the impact of requirements knowledge and experience (RKE) on software architecting tasks. Specifically, it describes an exploratory, empirical study involving 15 architecting teams, approximately evenly split between those teams with RKE and those without. Each team developed its own system architecture from the same given set of requirements in the banking domain. The subjects were all final year undergraduate or graduate students enrolled in a university-level course on software architectures. The overall results of this study suggest that architects\u00a0\u2026", "num_citations": "9\n", "authors": ["122"]}
{"title": "Identifying recurring faulty functions in field traces of a large industrial software system\n", "abstract": " Software maintainers use the traces of field failures to understand and diagnose faulty functions that cause the system to fail. Despite their usefulness, traces from the field can be quite overwhelming, especially for software systems with a vast client base. In the execution of realistic applications, many of them being millions of lines of code, there are just too many traces that are generated. In addition, traces are known to be extraordinarily large, which further complicates matters. Fortunately, not all field failures are caused by new faults. In fact, previous studies showed that 50% to 90% of field failures are due to previously known faults. In this paper, we propose a machine learning approach that automatically detects recurring faulty functions in the traces of new field failures. We achieve our goal by training decision trees on earlier resolved traces of system failures from the current and prior releases of the system\u00a0\u2026", "num_citations": "8\n", "authors": ["122"]}
{"title": "F007: finding rediscovered faults from the field using function-level failed traces of software in the field\n", "abstract": " Studies show that approximately 50% to 90% of the failures reported from the field are rediscoveries of previous faults. Also, approximately 80% of the failures originate from approximately 20% of the code. Despite this identification of the origin of the failures in system code remains an arduous activity, and consumes substantial resources. Prior fault discovery techniques for field traces either require many pass-fail traces, discover only crashing failures, or identify faulty coarse grain code such as files as the source of the fault. This paper describes a new method (F007) that focuses on identifying finer grain faulty code (faulty functions) from only failed traces of deployed software. F007 extracts patterns of function-calls from a historical collection of only function-level failed traces, and then trains decision trees on the extracted function-call patterns for each known faulty function. A ranked list of faulty functions is then\u00a0\u2026", "num_citations": "8\n", "authors": ["122"]}
{"title": "SIFT: a scalable iterative-unfolding technique for filtering execution traces\n", "abstract": " Comparing program execution traces can be useful for numerous purposes, such as software testing, system security analysis, program comprehension, software evolution and other areas of software development. Unfortunately, trace comparison techniques that operate on execution traces containing full execution details are too slow for use in large-scale production system environments. In order to speed up the comparisons, we propose a technique (called SIFT) for\" filtering-out\" irrelevant traces from a given set so that only the relevant few, residual, traces are then used for comparison. Our solution involves multiple levels of trace compression, each with a different degree of abstraction. These traces are compared iteratively while filtering out dissimilar traces. This paper describes the compression and comparison algorithms. Prototype results from a significant case study show that the SIFT approach is efficient\u00a0\u2026", "num_citations": "8\n", "authors": ["122"]}
{"title": "View\u2010based process elicitation: a user's perspective\n", "abstract": " Software process models are considered important for a number of purposes, such as assessment, improvement and customization. In order to obtain a complete model of a process, it is often necessary to gather relevant information from different sources (agents, documents, observations, etc.). One problem with this is that different sources may give inconsistent information about the same process. Resolving such conflicts can be difficult and arduous. This paper describes a user's perspective of a prototype tool, called V\u2010elicit, which helps in eliciting process models based on information from multiple sources. In V\u2010elicit, the information gathered from each source is entered separately as \u2018views\u2019, each of which is checked for internal consistency. Following this, common elements across the views are identified, and inconsistencies amongst them are flagged. As the elicitor, aided by the system, resolves these\u00a0\u2026", "num_citations": "8\n", "authors": ["122"]}
{"title": "A framework for process maintenance (software)\n", "abstract": " The authors present a framework, called the process cycle, which can assist in supporting and controlling process maintenance. The process cycle incorporates engineering management, performance, and improvement of processes by human agents subjected to desirable goals and policy constraints. Process maintenance is supported by incorporating feedback cycles so that processes, goals, and policies can be assessed and improved. In particular the authors address the identification of the reasons why processes change, the overall process change process, and the issue of policy improvement. Furthermore, they assess the applicability of the process cycle framework by relating it to current process maintenance practices. It is pointed out that an implication of using the process cycle for process maintenance is that there is a clear logical separation of concern in the various roles played by people, tools used\u00a0\u2026", "num_citations": "8\n", "authors": ["122"]}
{"title": "The IBM-McGill project on software process\n", "abstract": " Historically, the process used to develop software has played an important role in the field of software engineering. A number of software lifecycle models have been developed in the last three decades. These models, while helpful in giving general guidance to software developers, do not expose the myriad details that are critical in any large, evolving software development project. Recent developments, however, have unfolded many hidden aspects of the software process giving rise to a new discipline which we call software process engineering. This paper depicts the software process in the context of software environments, examines recent developments in the process field, proposes a set of actions which a software development team might undertake to improve their process maturity, and describes the IBM-McGill project on software process.", "num_citations": "8\n", "authors": ["122"]}
{"title": "Software construction using typed fragments\n", "abstract": " Recent research in the field of programming environments has resulted in integrated systems which demonstrate their use in the development of small programs. It is argued here that such systems are not suitable for non-trivial software development, as they support programming-in-the-small only. This paper introduces a new concept of a typed fragment called fragtype, which makes the notion of a software building block concrete. With the help of the underlying fragtype driven structured editor, and a fragment library, such building blocks can be used to construct a well-formed large software edifice.", "num_citations": "8\n", "authors": ["122"]}
{"title": "A practical look at software internationalisation\n", "abstract": " Software internationalisation is the process of developing software products that are not dependent on specific local or cultural attributes or practices. It is becoming an increasingly important topic as software gets integrated into the very fabric of our society and cultures worldwide. In this paper we present a view of some of the current issues by examining what steps had to be taken in order to carry out a particular software internationalisation task: a software system that was developed for English speakers was modified (localised) so that it could be used by speakers of Traditional Chinese. We describe the details of how this internationalisation task was carried out, what existing tools and techniques were useful, and offer suggestions concerning how the task of internationalisation may be made easier. The concluding discussion covers future directions for software internationalisation.", "num_citations": "7\n", "authors": ["122"]}
{"title": "An effort estimation model for implementing ISO 9001\n", "abstract": " A major concern for organizations who are seeking registration to ISO 9000, or seeking the implementation of any quality related process, is the ability to estimate the work/effort required for meeting the stated requirements. This is an a priori requirement in corporate decision making, regardless of the maturity level of the organization (W.S. Humphrey, 1989), the current state of the quality system, or the extent to which the organization complies to the requirements of the ISO 9000 standard. The paper presents a statistical regression model that predicts the effort required for meeting the requirements of ISO 9001. We carried out a survey in February 1995, which was sent to 1190 organizations in North America. We had a 38.8% response (462 responses). The effort estimation model we present is for ISO 9001 (all industries), and is based on the 112 responses we had from ISO 9001 registered organizations\u00a0\u2026", "num_citations": "7\n", "authors": ["122"]}
{"title": "A comprehensive process model for studying software process papers\n", "abstract": " Efficient and effective study of scientific papers is an important part of software engineering education. A comprehensive process model has been developed, enacted, improved and validated. The process model describes literature search, paper selection, reading, and group discussion of papers, as well as recording, validation, and retrieval of the data captured from the selected papers. The focus lies on the group discussion of the model. A model of this sub-process describes the systematic classification, analysis, and evaluation of the papers. It is used to guide the group discussion, and helps to ensure that pertinent information from the discussion is retained in an annotated database.< >", "num_citations": "7\n", "authors": ["122"]}
{"title": "Empirically Driven Improvement of Generic Process Models.\n", "abstract": " We are involved in an experiment to design an improvement process for a generic software process model (henceforth generic model) in collaboration with our industrial partner. Our partner's main line of business is the marketing and evolution of a generic model targeted at the development and maintenance of business information systems. Currently, the generic model has been licensed to over 100 organisations, some of whom are also taking part in the experiment.The original version of the generic model was based on the experiences of key personnel in developing large information systems. However, as the clientele base grew it was realised that the generic model was being applied to projects and organisations very di erent from those of the original experiences. This resulted in greater dependence on the skills of the company's personnel to modify the generic model as required in the eld. Furthermore, the existing generic model improvement process is operating in a reactive mode and potentially useful feedback data is not being gathered systematically. These problems necessitate the design of a new formalised improvement process that both, takes into consideration the characteristics of the clientele base and that operates in a pro-active mode.", "num_citations": "7\n", "authors": ["122"]}
{"title": "Operations for programming in the all\n", "abstract": " A primary goal of Software Engineering is to improve the process of software development. It is being recognised that recent integrated programming environments have made significant progress towards this aim. This paper describes new operations, suitable for such environments, which are applicable in a much wider scope of programming, termed here as programming in the all. Development of software in this new scope is carried out incrementally in program fragments of various types, called fragtypes. Fragtypes range from a simple Expression type to a complete Subsystem type, and therefore are suited to the development of non-trivial software. The proposed operations on fragtypes have been incorporated in the design of the programming environment MUPE-2 for Modula-2, which is currently under development at McGill University.", "num_citations": "7\n", "authors": ["122"]}
{"title": "Policy-guided software evolution\n", "abstract": " Ensuring that software systems evolve in a desired manner has thus far been an elusive goal. In a continuing effort towards this objective, in this paper we propose a new approach that monitors an evolving software system, or its evolution process, against evolutionary policies so that any feedback obtained can be used to improve the system or its process. Two key concepts that make this possible are: (1) a mechanism to detect policy violations; and (2) a contextual framework to support activities of evolving a software system beyond the next release. Together, they could provide a wide and deep scope for managing software evolution. The benefit of our approach is that it would help in: sustaining the quality of a software system as it evolves; reducing evolutionary costs; and improving evolutionary processes.", "num_citations": "6\n", "authors": ["122"]}
{"title": "Introduction to the panel session Lehman's laws of software evolution, in context\n", "abstract": " If there is anyone in our community who has been driving with the \u201chigh beams\u201d on, right from the start of the journey of software engineering, then in my mind this is, unquestionably, Professor Manny Lehman. While most of us were either in our infancy or fire-fighting software problems, Lehman had his sight set far ahead\u2013on the programming process, as evidenced by a seminal paper published in 1969 [1].For about thirty-five years now, Lehman has been concerned about, amongst other issues, software\u2019s longterm health, in effect, beyond the next release, while most others\u2013in practice and in research\u2013have had \u201clow beams\u201d on as if the next release is the final release of the software product or system. It needs no further explanation as to why we encounter surprises when we drive in the pitch-dark roads of software engineering. While it mattered little to the society at large in those early years that software systems were not so reliable and were of very limited use, this is not the case today with our society\u2019s already heavy and ever increasing dependence on software. There is no turning back, however, and we must search for ways to create, and evolve, software systems that will provide sustained quality service over long periods in our dynamic environments. Fortunately, many in practice and in research have begun to realise the importance of software evolution and that short-term thinking has an enormous economic and service quality price-tag on it. This panel session isn\u2019t about software evolution in general, although I have no doubts that some discussion might drift that way. It is about Lehman\u2019s laws of software evolution, in the context of the\u00a0\u2026", "num_citations": "6\n", "authors": ["122"]}
{"title": "Towards an emerging theory for the diagnosis of faulty functions in function-call traces\n", "abstract": " When a fault occurs in the field, developers usually collect failure reports that contain function-call traces to uncover the root causes. Fault diagnosis in failure traces is an arduous task due to the volume and size of typical traces. Previously, we have conducted several research studies to diagnose faulty functions in function-call level traces of field failures. During our studies, we have found that different faults in closely related functions occur with similar function-call traces. We also infer from existing studies (including our previous work) that a classification or clustering algorithm can be trained on the function-call traces of a fault in a function and then be used to diagnose different faults in the traces where the same function appears. In this paper, we propose an emerging descriptive theory based on the propositions grounded in these empirical findings. There is scarcity of theorizing empirical findings in software engineering research and our work is a step towards filling this gap. The emerging theory is stated as: a fault in a function can be diagnosed from a function-call trace if the traces of the same or a different fault in that function are already known to a clustering or classification algorithm. We evaluate this theory using the criteria described in the literature. We believe that this emerging theory can help reduce the time spent in diagnosing the origin of faults in field traces.", "num_citations": "5\n", "authors": ["122"]}
{"title": "Eros: an approach for ensuring regulatory compliance of process outcomes\n", "abstract": " In the service industry, such as healthcare, catering, tourism, and others, there exist regulations that require organisations to provide service outcomes that comply with the regulations. More and more regulations in the service sector are, or are aimed to be, outcome-focused regulations. An outcome prescribed in the regulation is what users should experience or achieve when the regulated business processes are compliant. Service providers need to proactively ensure that the outcomes specified in the regulations have been achieved prior to conducting the relevant part of the business or prior to inspectors discovering noncompliance. Published approaches check requirements or business processes, not outcomes, against regulations and thus this still leaves uncertain as to whether what the users actually experience is what is prescribed in the regulations. In this research preview paper., we propose a method for\u00a0\u2026", "num_citations": "5\n", "authors": ["122"]}
{"title": "View-based vs Traditional modeling approaches: which is better?\n", "abstract": " In this paper, we take a position that models of software processes elicited using a view-based approach are generally of higher quality (specially, more complete) than those elicited using traditional, non-view based, modeling approaches. This is validated empirically.", "num_citations": "5\n", "authors": ["122"]}
{"title": "Evaluating the congruence of a software process model in a given environment\n", "abstract": " The adaptation (or tailoring or improvement) of software process models is recognised to be important for software development, and there is an urgent need for tools to assist in this task. However, a key issue underlying such tools is the determination of how fit (or congruent) a process model is in its environment. Knowing this would then help drive the adaptation or improvement activity. The paper describes a system for evaluating such congruence. The underlying approach is based on an empirically derived contingency model consisting of attributes of a software process model and its environment, and their interrelationships. The system accepts, as inputs, the characteristics of a software process model and its environment. It produces, as output, an evaluation of how congruent the process model is in the given environment. The system enables changes to variables and relationships so as to improve the process\u00a0\u2026", "num_citations": "5\n", "authors": ["122"]}
{"title": "Personal'progress functions' in the software process\n", "abstract": " Individual developers can expect improvement in software productivity as a consequence of (i) a growing stock of knowledge and experience gained by repeatedly doing the same task (first-order learning) or (ii) due to technological and training programs supported by the organization (second-order learning). Organizations have used this type of progress behavior in making managerial decisions regarding cost estimation and budgeting, production and staff scheduling, product pricing, etc. Such progress is studied in productivity, product-quality and personal skills, in an experiment involving a sample of 12 software developers, who complete one project every week for ten weeks. Second-order training is provided to the subjects through Humphrey's Personal Software Process. A modified GQM method for measurement is used to execute the research method.", "num_citations": "5\n", "authors": ["122"]}
{"title": "Communications and iterations in the process cycle\n", "abstract": " In this paper, for the first time, we introduce a new end-to-end deep neural network predicting forgery masks to the image copy-move forgery detection problem. Specifically, we use a convolutional neural network to extract block-like features from an image, compute self-correlations between different blocks, use a pointwise feature extractor to locate matching points, and reconstruct a forgery mask through a deconvolutional network. Unlike classic solutions requiring multiple stages of training and parameter tuning, ranging from feature extraction to postprocessing, the proposed solution is fully trainable and can be jointly optimized for the forgery mask reconstruction loss. Our experimental results demonstrate that the proposed method achieves better forgery detection performance than classic approaches relying on different features and matching schemes, and it is more robust against various known attacks like\u00a0\u2026", "num_citations": "5\n", "authors": ["122"]}
{"title": "Modula-2/MUPE-2: Language and environment interactions\n", "abstract": " Programming environments must be considered during language design because the interactions between the two can affect software design", "num_citations": "5\n", "authors": ["122"]}
{"title": "On commands for an integrated programming environment\n", "abstract": " A command language is a user's basic vehicle for exploiting the capabilities of a software system, in order to perform a certain task. It is therefore important that the language is: simple to learn; flexible to use; knowledgeable about the structure and the semantics of the internal processes and secure. This paper treats various aspects of command languages for integrated programming environments. In particular, it shows how context can be used to tailor menu commands to a specific task, how to simplify such commands to their basic form, how the logical and physical organisation of commands can affect user interaction with the sys-tem and how the scope of the command language can be enlarged uniformly to encompass programming in the large and in the small. This work is part of the MUPE-2 project which is currently being carried out at McGill University.", "num_citations": "5\n", "authors": ["122"]}
{"title": "Visibility aspects of programmed dynamic data structures\n", "abstract": " Unlike static structures, dynamic Pascal-like data structures often suffer visibility problems due to the unrestricted use of the general pointer mechanism. By classifying these structures and identifying the different kinds of pointers, a methodology is proposed for achieving improved visibility.", "num_citations": "5\n", "authors": ["122"]}
{"title": "Cray Pascal\n", "abstract": " This paper presents an investigation of the design decisions taken in the implementation of a compiler for Pascal on the CRAY-1 computer. The structured nature of Pascal statements and data structures is contrasted with the 'powerful computing engine' nature of the CRAY-1 hardware. The accepted views of Pascal as a simple one-pass language and the CRAY-1 as a vector processor are laid aside in favour of a multi-pass approach, taking account of the machine's scalar capabilities. The project as a whole, aims to produce highly efficient run-time code for applications likely to be programmed in Pascal. Some statistics are given to indicate the nature of such applications.", "num_citations": "5\n", "authors": ["122"]}
{"title": "A Validation Study of a Requirements Engineering Artefact Model for Big Data Software Development Projects.\n", "abstract": " The elicitation, specification, analysis, prioritisation and management of system requirements for large projects are known to be challenging. It involves a number of diverse issues, such as: different types of stakeholders and their needs, relevant application domains, knowing about product and process technologies, regulatory issues, and applicable standards. The advent of \u201cBig Data\u201d and, in turn, the need for software applications involving Big Data, has further complicated requirements engineering (RE). In part, this is due to the lack of clarity in the RE literature and practices on how to treat Big Data and the \u201cV\u201d characteristics in the development of Big Data applications. Traditionally, researchers in the RE field have created domain models that help in understanding the context of the problem, and in supporting communication and analysis in a project. Analogously, for the emerging field of software applications involving Big Data, we propose an empirically derived RE artefact model. It has been validated for qualities such as: accuracy, completeness, usefulness, and generalisability by ten practitioners from Big Data software development projects in industry. The validation results indicate that the model captures the key RE elements and relationships involved in the development of Big Data software applications. The resultant artefact model is anticipated to help in such activities as: requirements elicitation and specification; definition of specific RE processes; customising and creating a common vision in Big Data RE projects; and creating traceability tools linking the artefacts.", "num_citations": "4\n", "authors": ["122"]}
{"title": "Towards a big data requirements engineering artefact model in the context of big data software development projects: Poster extended abstract\n", "abstract": " In this paper, we describe our ongoing research aimed at defining a Requirements Engineering Artefact Model (REAM) in the context of Big Data software applications. This model aims to provide a \u201cbig picture\u201d of the Requirements Engineering work products created and used in Big Data software development projects. REAM are important tools that can be used as references for the definition of domain-specific RE models, system life-cycle processes and artefact-centered processes, currently bereft in the Big Data Software Engineering research.", "num_citations": "4\n", "authors": ["122"]}
{"title": "Request-implementation ratio as an indicator for requirements prioritisation imbalance\n", "abstract": " Software development organizations strive to implement features that will bring the highest business value to existing and prospective clients; hence the importance of requirements elicited directly from clients. However, some requirements need to be put into the backlog due to time and resource constraints in the development organization. Consequently, there have been numerous prioritisation approaches published in the literature and used in practice.", "num_citations": "4\n", "authors": ["122"]}
{"title": "Does requirements clustering lead to modular design?\n", "abstract": " [Context and motivation] The clustering of system requirements groups together related requirements. In a concept paper, we had previously proposed a requirements clustering approach for the purpose of modularizing software. [Question/problem] In this short paper, we describe a preliminary study to explore the answer to the posed question: whether or not requirements clustering leads to modular design as measured by design goodness criteria. [Principal ideas/results] The study assesses the modularity of software designs developed by independent groups given the same requirements. These are then compared against the expected design resultant from implementing the requirements cluster. [Contribution] The study results are encouraging and it warrants further investigation.", "num_citations": "4\n", "authors": ["122"]}
{"title": "A method for instrumenting software evolution processes and an example application\n", "abstract": " Typical difficulties faced by contemporary information systems organizations include: expending a majority of their resources on evolving legacy systems, having a large backlog of change requests to these legacy systems, and providing inadequate service to end-users and hence contributing to their dissatisfaction. Such a state of affairs reduces the ability of user organizations to take advantage of emerging business opportunities through Information Technology. Thus, there is an urgent need for information systems organizations to address these difficulties by improving their software evolution processes.A prerequisite to improving software evolution processes is the ability to instrument these processes. In this position statement we present a method for instrumenting software evolution processes within the context of improvement. The method builds on existing approaches in software engineering, namely the Quality Improvement Paradigm and the G/Q/M method [1]. Furthermore, it draws from measurement techniques that have been employed in other scientific disciplines, namely psychology, education, marketing, and Management Information Systems.", "num_citations": "4\n", "authors": ["122"]}
{"title": "A dynamically self-adjusting structured editor\n", "abstract": " Two major characteristics shared by most program editors are that they are static, as they operate within a single framework of a full program text, and that they are strictly language based. These characteristics are considered to be less than desirable, as they appear to restrict the development of production software. As a solution, a new kind of structured editor, based on program fragments, is proposed in this paper. This kind of editor is dynamic, as it is driven by the types of different fragments, called fragtypes. As a fragment evolves during an editing session, its fragtype can change depending on the user operations. Such changes trigger off automatic adjustments in the editor capabilities. Fragtypes range from that of small granularity objects, such as an identifier, to that of large granularity objects, such as a subsystem. The dynamic nature of the editor, together with the wide range of fragtypes provided, appear to\u00a0\u2026", "num_citations": "4\n", "authors": ["122"]}
{"title": "Requirements engineering decisions in the context of an existing architecture: a case study of a prototypical project\n", "abstract": " The role of an existing systems architecture (SA) in requirements engineering (RE) is recognised as important, but under-researched. A recent exploratory study of ours investigated this issue in a laboratory setting involving student participants. While the initial findings are promising, much work still remains to solidify the results. Therefore, we conducted a replication of the study, and its significant extension, on a large-scale prototypical rail project. Specifically, we identify (i) the effects of SA on RE decisions, (ii) the characteristics of the RE decisions and (iii), the impact of such decisions on development activities and the rail system. The findings of this study have implications on tighter RE-SA integration across subsystems, impact analysis of requirements on SA, and planning and risk management. We also propose three emergent hypotheses from this case study as a driver for future empirical work in RE. This case\u00a0\u2026", "num_citations": "3\n", "authors": ["122"]}
{"title": "Transitioning from lab studies to large-scale studies: Emerging results from a literal replication\n", "abstract": " Replication of studies in Software Engineering is considered important, but is largely neglected. Because of the lack of published replicated literature, there are few established guidelines for researchers wanting to conduct replicated studies. Specifically, guidelines for transitioning from laboratory studies to large-scale studies are nonexistent. Previously, we conducted a laboratory study in the banking domain, in Canada, which we replicated by conducting an extended, large-scale case study on an innovative rail project in Germany, investigating the role of an existing systems architecture on requirements decisions. In this short paper, we present a preliminary analysis of our transitioning experiences from conducting these two studies. From our experiences, we derive a set of lessons learnt and recommendations that can be used by other researchers wanting to transition from lab studies to studies in industrial\u00a0\u2026", "num_citations": "3\n", "authors": ["122"]}
{"title": "ESDM-A Method for Developing Evolutionary Scenarios for Analysing the Impact of Historical Changes on Architectural Elements\n", "abstract": " Software maintainers need appropriate information concerning the change they are about to make to a software system so that they can make suitable choices in their decisions. In this paper, we describe a method (called ESDM) for developing evolutionary scenarios that provide information concerning the impact historical changes of different types have had on the quality of software architectural elements of interest. This information can aid in the decisions maintainers are about to make concerning the change at hand. The effectiveness of the method for developing evolutionary scenarios has been validated through an empirical study on an open source software system (Apache HTTP Server 1.3). Initial support from maintainers for the value of the developed scenarios is a major boost for further work in this area", "num_citations": "3\n", "authors": ["122"]}
{"title": "Requirements engineering education for novice software architects\n", "abstract": " We take the position that novice architects without requirements education would benefit from such education for architecting purposes. While this knowledge might be intuitive among many, our position is based on an exploratory, empirical study involving a number of architecting teams, some with requirements education and others without, all architecting from the same set of requirements. The overall results of this study show that requirements educated architects do perform better at architecting software than those without requirements education. Furthermore, this paper discusses specific areas of requirements where novice architects could benefit.", "num_citations": "3\n", "authors": ["122"]}
{"title": "A model for process congruence\n", "abstract": " In this paper, we describe a model for process congruence in which a software development process is checked against defined standards and/or organizational policies. When an inconsistency is detected, different courses of action can be taken: change the process (through its model), change the policy (if it is not current), and/or keep the\" status quo\". We also provide tool support for the specification and verification of development policies.", "num_citations": "3\n", "authors": ["122"]}
{"title": "A comprehensive process model for discussing and recording scientific papers\n", "abstract": " Efficient and effective reading of scientific papers is an important and challenging task. We have addressed it by applying principles and techniques from software process engineering, developing a comprehensive paper process model which we have enacted, improved and validated over the past one and a half years at weekly discussion sessions. The model captures paper selection, paper discussion, and the recording and retrieval of the discussion results. The focus of this paper lies on the paper discussion process which describes the systematic and detailed classification, analysis, and evaluation of research papers, provides guidance throughout the course of the group discussions, and guarantees that all relevant information of the discussions is retained in an annotated bibliography database. Our experience shows that students obtain a deep domain understanding, are trained in systematically\u00a0\u2026", "num_citations": "3\n", "authors": ["122"]}
{"title": "Software process and its support\n", "abstract": " process is not new. Ever since we began programming a computer, we have carried out software process, primarily through the labour-intensive use of automated software development tools. Especially since the 1950s, we have followed various software life-cycle models, which have given us a general understanding of how to engineer and control the evolution of software. All through this time, however, our development of software has been largely productcentred, as reflected by numerous individual and integrated software tools and repository-centred software environments. However, this is changing rapidly. Recently, we have begun to consciously investigate software life-cycle processes more deeply in order to understand the nature of software process, including its structure and dynamics, and its impact on software quality; the way we can support software process, including planning, measuring and\u00a0\u2026", "num_citations": "3\n", "authors": ["122"]}
{"title": "Dynamically structured data\n", "abstract": " While the control structures in recent programming languages are structured, the data structures are still primitive. This paper examines data structures and operations on them, and proposes some new features in programming languages. These new features are principally in the areas of data description and data usage. In data description, the emphasis is on a global view of dynamic data structures; in data usage, semantic relationships between data items are innate in the operations on these data structures. Finally, example data descriptions and algorithms using some of the new features are contrasted with those using conventional features.", "num_citations": "3\n", "authors": ["122"]}
{"title": "A Domain Model for Requirements-Driven Insight for Internal Stakeholders: A Proposal for an Exploratory Interactive Study\n", "abstract": " We propose conducting a 90-minute, interactive exploratory study at EmpiRE'18 that engages attendees in constructing a domain model for requirements-driven insight into system development for internal stakeholders. The domain model is anticipated to enhance internal stakeholders' (e.g., developers, architects, quality manager, etc.) insight into system development from the standpoint of requirements. This should improve: (i) communication among internal stakeholders; (ii) control and management of process and resources for providing requirement-driven insight to stakeholders; and (iii) reuse of the domain model across projects. The study is aimed to generate empirical data to further elaborate and enhance the preliminary model. The participants of the study will be engaged in a model building exercise, and, consequently, have insight into the domain model for internal stake-holders. The revised model will\u00a0\u2026", "num_citations": "2\n", "authors": ["122"]}
{"title": "Towards a Big Data Requirements Engineering Artefact Model in the Context of Big Data Software Development Projects\n", "abstract": " In this paper, we describe our ongoing research aimed at defining a Requirements Engineering Artefact Model (REAM) in the context of Big Data software applications. This model aims to provide a \u201cbig picture\u201d of the Requirements Engineering work products created and used in Big Data software development projects. REAM are important tools that can be used as references for the definition of domainspecific RE models, system life-cycle processes and artefactcentered processes, currently bereft in the Big Data Software Engineering research.", "num_citations": "2\n", "authors": ["122"]}
{"title": "1st International workshop on conducting empirical studies in industry (CESI 2013)\n", "abstract": " The quality of empirical studies is critical for the success of the Software Engineering (SE) discipline. More and more SE researchers are conducting empirical studies involving the software industry. While there are established empirical procedures, relatively little is known about the dynamics of conducting empirical studies in the complex industrial environments. What are the impediments and how to best handle them? This was the primary driver for organising CESI 2013. The goals of this workshop include having a dialogue amongst the participating practitioners and academics on the theme of this workshop with the aim to produce tangible output that will be summarised in a post-workshop report.", "num_citations": "2\n", "authors": ["122"]}
{"title": "A towards an extended relational algebra for software architecture\n", "abstract": " Software architecture is often structured as box-and-arrow graphs and has important implications for system development and maintenance. We propose an extended relational algebra to support presentation and manipulation of both architectural structures and implications. The core structure of this algebra is the extended architectural relation (EAR). An EAR is a mapping from an architectural relation (AR) to a multi-set of attributes (M), where the AR is an ordinary relation representing an architectural structure, and the M represents a multi-set representing a type of architectural implication. A set of EAR operations is then defined to support EAR manipulations. The main advantage of this extended algebra over ordinary relational algebras is that the architectural implications (the M part) are presented and manipulated together with the architectural structures (the AR part). This paper first discusses why we propose\u00a0\u2026", "num_citations": "2\n", "authors": ["122"]}
{"title": "Discovering\n", "abstract": " This paper proposes an automatic technique to reduce the time spent in detection of the fault origin from field traces, by discovering hidden patterns in the traces.", "num_citations": "2\n", "authors": ["122"]}
{"title": "Estimating the effort of implementing ISO 9001 in software organizations\n", "abstract": " A major concern for organizations who are seeking registration to ISO 9000 [11], or seeking the implementation of any quality software related framework [22][24], is the ability to estimate the required effort for meeting the stated requirements. This paper presents a model based on data collected using a survey of software organization that have achieved ISO 9001 registration.", "num_citations": "2\n", "authors": ["122"]}
{"title": "The role of a software process generaliser in managing a line of products\n", "abstract": " There is an urgent need for tools for understanding commonality and variability amongst a set of projects, for example, those dealing with a line of products. We describe some of the problems faced in practice by practitioners dealing with multiple projects or families of products. We then describe a prototype system, called the Generaliser, aimed to assist in the assessment of commonality and variability.", "num_citations": "2\n", "authors": ["122"]}
{"title": "Semi-structured cursor movements in MUPE-2\n", "abstract": " It has been recognised that cursor movements on a textual representation of a program present some problems when the movements are based on an underlying abstract syntax tree in a programming system. Approaches taken in many systems are to use structured or textual cursor movements, although both these approaches have some drawbacks. This paper presents a new approach, called semi-structured movements, based on the idea of cursor movement paths, or streams, that the cursor can follow in a program. The key benefits of this approach are that it eliminates the \u2018treeness\u2019 of structured movements and the flatness of textual movements, while providing an inherent support for program editing and browsing. Semi-structured cursor movements have been successfully implemented in the MUPE-2 programming environment, and this paper gives several scenarios in order to illustrate example cursor\u00a0\u2026", "num_citations": "2\n", "authors": ["122"]}
{"title": "A new approach to cursor movements in user interfaces of integrated programming environments\n", "abstract": " The design of cursor movements in the user interfaces of integrated programming environments has largely been based on the tree representation of a program. This paper proposes a new approach to the design of cursor movements. Since cursor movements are central to the use of integrated tools, such as structured editors, code browsers, and formatters, it is the requirements of these tools that should be the primary basis for the design of cursor movements. The proposed approach uses tool requirements to: (a) define precisely the parts of a program on which the cursor can be positioned, and (b) determine the paths for cursor movements, by using the defined program parts. The application of this approach to modula-2 programs is described and a comparison with other approaches is made. An implementation of cursor movements for the programming environment MUPE-2, based on the proposed approach\u00a0\u2026", "num_citations": "2\n", "authors": ["122"]}
{"title": "Some experience from the design of an integrated programming environment\n", "abstract": " Integrated programming environments have recently received considerable attention, as they seem to provide some hope in improving the process of software construction and, possibly, also improving the qual-ity of the software produced. This paper describes some experience gained by the authors in the design of the MUPE-2 programming environment. The system is aimed to support the development of", "num_citations": "2\n", "authors": ["122"]}
{"title": "QualiBD: A tool for modelling quality requirements for Big Data applications\n", "abstract": " The development of Big Data applications is not well-explored, to our knowledge. Embracing Big Data in system building, questions arise as to how to elicit, specify, analyse, model, and document Big Data quality requirements. In our ongoing research, we explore a requirements modelling language for Big Data software applications. In this paper, we introduce QualiBD, a modelling tool that implements the proposed goal-oriented requirements language that facilitates the modelling of Big Data quality requirements.", "num_citations": "1\n", "authors": ["122"]}
{"title": "Towards a Meta-Model for Requirements-Driven Information for Internal Stakeholders\n", "abstract": " [Context & Motivation] Providing requirements-driven information (e.g., requirements volatility measures, requirements-design coverage information, requirements growth rates, etc.) falls within the realm of the requirements management process. The requirements engineer must derive and present the appropriate requirements information to the right internal stakeholders (IS) in the project. [Question/Problem] This process is made complex due to project-related factors such as numerous types of ISs, varying stakeholder concerns with regard to requirements, project sizes, a plethora of software artifacts, and many affected processes. However, there is little guidance in practice as to how these factors come into play together in providing the described information to the ISs. [Principle ideas/results] Based on analyzed data from an action research (AR) study we conducted in a large systems project in the rail\u00a0\u2026", "num_citations": "1\n", "authors": ["122"]}
{"title": "The Role of Big Data Analytics in Corporate Decision-making.\n", "abstract": " Big Data Analytics results can play a major role in corporate decision-making allowing companies to achieve competitive advantage and make improved decisions. This paper describes a systematic literature review (SLR) on the role of the results of Big Data Analytics in corporate decisions. Initially, 1652 papers were identified from various sources. Filtering through the 5-step process, 20 relevant studies were selected for analysis in this SLR. The findings of this study are fourfold in the area of:(a) usage of the results of Big Data Analytics in corporate decision-making;(b) the types of business functions where analytics has been fruitfully utilised;(c) the impact of analytics on decision-making; and (d) the impediments to using Big Data Analytics in corporate decision-making. Also, on the management front, two important issues identified are:(i) aligning data-driven decision-making with business strategy and (ii) collaboration across business functions for effective flow of Big Data and information. On the technical front, big data present some challenges due to the lack of tools to process such properties of Big Data as variety, veracity, volume, and velocity. We observe from this analysis that, thus far, little scientific research has focused on understanding how to address the analytics results in corporate decision-making. This paper ends with some recommendations for further research in this area.", "num_citations": "1\n", "authors": ["122"]}
{"title": "Taming a tiger: Software engineering in the era of big data & continuous development\n", "abstract": " In this workshop, we describe software engineering challenges created by the introduction of Big Data, Data Science, and Continuous Development into corporate and research environments. Bloomberg Business reports\" Data scientists are the new superheroes\". The desire to get immediate answers to complex questions is driving software development in new directions. Basic principles like Database modeling, Object Oriented Design, and Quality Metrics are taking a back seat to demands for more answers in less time.", "num_citations": "1\n", "authors": ["122"]}
{"title": "Maps of lessons learnt in requirements engineering: a research preview\n", "abstract": " [Context and Motivation] \"Those who cannot remember the past are condemned to repeat it\" \u2013 George Santayana. From the survey we conducted of requirements engineering (RE) practitioners, over 70% seldom use RE lessons in the RE process, though 85% of these would use such lessons if readily available. Our observation, however, is that, RE lessons are scattered, mainly implicitly, in the literature and practice, which, obviously, does not help the situation. [Problem/Question] Approximately 90% of the survey participants stated that not utilising RE lessons has significant negative impact on product quality, productivity, project delays and cost overruns. [Principal Ideas] We propose \u201cmaps\u201d (or profiles) of RE lessons which, once populated, would highlight weak (dark) and strong (bright) areas of RE (and hence RE theories). Such maps would thus be: (a) a driver for research to \u201clight up\u201d the\u00a0\u2026", "num_citations": "1\n", "authors": ["122"]}
{"title": "Applying agile practices to drive continuous improvement: a measured approach\n", "abstract": " Successful businesses must sustain a competitive advantage and consistently deliver business value. In order to maximize business value, software projects must meet the challenge of delivering the right solution at the right time with high quality at the lowest cost. This requires a constant focus on eliminating waste, improving productivity and adapting to changing business needs.", "num_citations": "1\n", "authors": ["122"]}
{"title": "The Architecture-Requirements Interaction\n", "abstract": " The interaction between software architecture (SA) and requirements engineering (RE) processes is generating interest within the research community. We explored the role of SA documentation in requirements decision-making. This paper describes the findings from this study.", "num_citations": "1\n", "authors": ["122"]}
{"title": "Differentiating Web Service Offerings\n", "abstract": " The advent of Service Oriented Architecture (SOA) paradigm and increasing use of Web Services (WS) implies that the future will see a large number of services transferred between providers and consumers, using many applications or agents working on behalf of humans. Discovering and using the services is the easy part. Negotiating and selecting the best services from amongst the plethora of similar ones, depending on their cost and quality, is the challenging issue. However, existing WS-I standards neither cater to provision of Service Level Agreements (SLAs), nor their exchange between parties. These standards are confined merely to WS description (WSDL). Once WS are discovered and selected, SLAs are merely used to monitor service compliance. We propose a novel method that allows service-providers to dynamically generate the SLAs, and then transfer them to clients for selection amongst\u00a0\u2026", "num_citations": "1\n", "authors": ["122"]}
{"title": "Requirements-oriented problems while architecting: an empirical study\n", "abstract": " \u220e\u201cWhat kinds of requirements-oriented (RO) problems are being experienced while architecting a software system?\u201d", "num_citations": "1\n", "authors": ["122"]}
{"title": "Managing Requirements Invalidity Risk\n", "abstract": " The importance of dealing with various types of risk in project management has long been recognised. In this position paper, we argue for monitoring and managing the risk associated with an application\u2019s domain changes which, in turn, may necessitate requirements modification or deletion during the development stage. In other words, we are concerned with how \u201cvalid\u201d the requirements are through the development stages. We call this requirements invalidity risk. Based on the measure for calculating invalidity risk, we discuss how this risk can be managed in a typical risk management process.", "num_citations": "1\n", "authors": ["122"]}
{"title": "Centre de Recherche Informatique de Montreal\n", "abstract": " News or senior-level undergraduates. PSP courses have been introduced in a number of universities in North America, such as Carnegie Mellon University, Embry-Riddle Aeronautical University, McGill University, Bradley University, and the University of Massachusetts. PSP courses have also been used to train professional engineers in industry. PSP experiences have been reported in DEC, Motorola, Lawrence Livermore National Laboratory, and Union Switch and Signal. Experience thus far suggests that university students and professional programmers do benefit from the PSP (eg Humphrey 1994, Macke et a/. 1996). From an industrial perspective, the effectiveness of PSP should be evaluated based on what engineers do in their real programming tasks (as opposed to course exercises). If students do not continue using their PSP skills after the course, and if usage does not improve their real programming\u00a0\u2026", "num_citations": "1\n", "authors": ["122"]}