{"title": "Automated concolic testing of smartphone apps\n", "abstract": " We present an algorithm and a system for generating input events to exercise smartphone apps. Our approach is based on concolic testing and generates sequences of events automatically and systematically. It alleviates the path-explosion problem by checking a condition on program executions that identifies subsumption between different event sequences. We also describe our implementation of the approach for Android, the most popular smartphone app platform, and the results of an evaluation that demonstrates its effectiveness on five Android apps.", "num_citations": "461\n", "authors": ["663"]}
{"title": "Replicated data types: specification, verification, optimality\n", "abstract": " Geographically distributed systems often rely on replicated eventually consistent data stores to achieve availability and performance. To resolve conflicting updates at different replicas, researchers and practitioners have proposed specialized consistency protocols, called replicated data types, that implement objects such as registers, counters, sets or lists. Reasoning about replicated data types has however not been on par with comparable work on abstract data types and concurrent data types, lacking specifications, correctness proofs, and optimality results. To fill in this gap, we propose a framework for specifying replicated data types using relations over events and verifying their implementations using replication-aware simulations. We apply it to 7 existing implementations of 4 data types with nontrivial conflict-resolution strategies and optimizations (last-writer-wins register, counter, multi-value register and\u00a0\u2026", "num_citations": "237\n", "authors": ["663"]}
{"title": "'Cause i'm strong enough: reasoning about consistency choices in distributed systems\n", "abstract": " Large-scale distributed systems often rely on replicated databases that allow a programmer to request different data consistency guarantees for different operations, and thereby control their performance. Using such databases is far from trivial: requesting stronger consistency in too many places may hurt performance, and requesting it in too few places may violate correctness. To help programmers in this task, we propose the first proof rule for establishing that a particular choice of consistency guarantees for various operations on a replicated database is enough to ensure the preservation of a given data integrity invariant. Our rule is modular: it allows reasoning about the behaviour of every operation separately under some assumption on the behaviour of other operations. This leads to simple reasoning, which we have automated in an SMT-based tool. We present a nontrivial proof of soundness of our rule and\u00a0\u2026", "num_citations": "147\n", "authors": ["663"]}
{"title": "Relational separation logic\n", "abstract": " In this paper, we present a Hoare-style logic for specifying and verifying how two pointer programs are related. Our logic lifts the main features of separation logic, from an assertion to a relation, and from a property about a single program to a relationship between two programs. We show the strength of the logic, by proving that the Schorr\u2013Waite graph marking algorithm is equivalent to the depth-first traversal.", "num_citations": "142\n", "authors": ["663"]}
{"title": "Semantics for probabilistic programming: higher-order functions, continuous distributions, and soft constraints\n", "abstract": " We study the semantic foundation of expressive probabilistic programming languages, that support higher-order functions, continuous distributions, and soft constraints (such as Anglican, Church, and Venture). We define a metalanguage (an idealised version of Anglican) for probabilistic computation with the above features, develop both operational and denotational semantics, and prove soundness, adequacy, and termination. This involves measure theory, stochastic labelled transition systems, and functor categories, but admits intuitive computational readings, one of which views sampled random variables as dynamically allocated read-only variables. We apply our semantics to validate nontrivial equations underlying the correctness of certain compiler optimisations and inference algorithms such as sequential Monte Carlo simulation. The language enables defining probability distributions on higher-order\u00a0\u2026", "num_citations": "114\n", "authors": ["663"]}
{"title": "Local reasoning for stateful programs\n", "abstract": " Programs are called stateful when they manipulate the state of a computer explicitly, for example, by assignment. The main theme of the thesis is reasoning about stateful programs. In fact, for the past 30 years, significant efforts have been made for obtaining good reasoning principles to ensure that such programs work correctly. However, reasoning techniques developed so far often result in significantly more complex verification than an informal argument; consequently, they don't attract attention from real programmers. In the thesis, we develop O'Hearn's idea of local reasoning, which was recently proposed to overcome such complexity problem in reasoning. The key observation of local reasoning is that most programs show conceptual locality in their use of the store: even though a program is able to access all global variables and all heap data structures in principle, it usually uses only a few of them. Local\u00a0\u2026", "num_citations": "109\n", "authors": ["663"]}
{"title": "A convenient category for higher-order probability theory\n", "abstract": " Higher-order probabilistic programming languages allow programmers to write sophisticated models in machine learning and statistics in a succinct and structured way, but step outside the standard measure-theoretic formalization of probability theory. Programs may use both higher-order functions and continuous distributions, or even define a probability distribution on functions. But standard probability theory does not handle higher-order functions well: the category of measurable spaces is not cartesian closed. Here we introduce quasi-Borel spaces. We show that these spaces: form a new formalization of probability theory replacing measurable spaces; form a cartesian closed category and so support higher-order functions; form a well-pointed category and so support good proof principles for equational reasoning; and support continuous probability distributions. We demonstrate the use of quasi-Borel spaces\u00a0\u2026", "num_citations": "98\n", "authors": ["663"]}
{"title": "On abstraction refinement for program analyses in Datalog\n", "abstract": " A central task for a program analysis concerns how to efficiently find a program abstraction that keeps only information relevant for proving properties of interest. We present a new approach for finding such abstractions for program analyses written in Datalog. Our approach is based on counterexample-guided abstraction refinement: when a Datalog analysis run fails using an abstraction, it seeks to generalize the cause of the failure to other abstractions, and pick a new abstraction that avoids a similar failure. Our solution uses a boolean satisfiability formulation that is general, complete, and optimal: it is independent of the Datalog solver, it generalizes the failure of an abstraction to as many other abstractions as possible, and it identifies the cheapest refined abstraction to try next. We show the performance of our approach on a pointer analysis and a typestate analysis, on eight real-world Java benchmark programs.", "num_citations": "92\n", "authors": ["663"]}
{"title": "Variables as resource in separation logic\n", "abstract": " Separation logic [Reynolds, J. C., Intuitionistic reasoning about shared mutable data structure, in: J. Davies, B. Roscoe and J. Woodcock, editors, Millennial Perspectives in Computer Science, Palgrave, 2000 pp. 303\u2013321; Reynolds, J. C., Separation logic: A logic for shared mutable data structures, in: LICS '02: Proceedings of the 17th Annual IEEE Symposium on Logic in Computer Science (2002), pp. 55\u201374; O'Hearn, P., J. Reynolds and H. Yang, Local reasoning about programs that alter data structures, in: L. Fribourg, editor, CSL 2001 (2001), pp. 1\u201319, LNCS 2142] began life as an extended formalisation of Burstall's treatment of list-mutating programs [Burstall, R., Some techniques for proving correctness of programs which alter data structures, Machine Intelligence 7 (1972), pp. 23\u201350]. It rapidly became clear that there was more that it could say: O'Hearn's discovery [O'Hearn, P., Notes on separation logic for\u00a0\u2026", "num_citations": "91\n", "authors": ["663"]}
{"title": "An introduction to probabilistic programming\n", "abstract": " This document is designed to be a first-year graduate-level introduction to probabilistic programming. It not only provides a thorough background for anyone wishing to use a probabilistic programming system, but also introduces the techniques needed to design and build these systems. It is aimed at people who have an undergraduate-level understanding of either or, ideally, both probabilistic machine learning and programming languages. We start with a discussion of model-based reasoning and explain why conditioning as a foundational computation is central to the fields of probabilistic machine learning and artificial intelligence. We then introduce a simple first-order probabilistic programming language (PPL) whose programs define static-computation-graph, finite-variable-cardinality models. In the context of this restricted PPL we introduce fundamental inference algorithms and describe how they can be implemented in the context of models denoted by probabilistic programs. In the second part of this document, we introduce a higher-order probabilistic programming language, with a functionality analogous to that of established programming languages. This affords the opportunity to define models with dynamic computation graphs, at the cost of requiring inference methods that generate samples by repeatedly executing the program. Foundational inference algorithms for this kind of probabilistic programming language are explained in the context of an interface between program executions and an inference controller. This document closes with a chapter on advanced topics which we believe to be, at the time of writing, interesting\u00a0\u2026", "num_citations": "81\n", "authors": ["663"]}
{"title": "Design and implementation of probabilistic programming language anglican\n", "abstract": " Anglican is a probabilistic programming system designed to interoperate with Clojure and other JVM languages. We introduce the programming language Anglican, outline our design choices, and discuss in depth the implementation of the Anglican language and runtime, including macro-based compilation, extended CPS-based evaluation model, and functional representations for probabilistic paradigms, such as a distribution, a random process, and an inference algorithm.", "num_citations": "80\n", "authors": ["663"]}
{"title": "On nesting Monte Carlo estimators\n", "abstract": " Many problems in machine learning and statistics involve nested expectations and thus do not permit conventional Monte Carlo (MC) estimation. For such problems, one must nest estimators, such that terms in an outer estimator themselves involve calculation of a separate, nested, estimation. We investigate the statistical implications of nesting MC estimators, including cases of multiple levels of nesting, and establish the conditions under which they converge. We derive corresponding rates of convergence and provide empirical evidence that these rates are observed in practice. We further establish a number of pitfalls that can arise from naive nesting of MC estimators, provide guidelines about how these can be avoided, and lay out novel methods for reformulating certain classes of nested expectation problems into single expectations, leading to improved convergence rates. We demonstrate the applicability of our work by using our results to develop a new estimator for discrete Bayesian experimental design problems and derive error bounds for a class of variational objectives.", "num_citations": "75\n", "authors": ["663"]}
{"title": "An example of local reasoning in BI pointer logic: the Schorr-Waite graph marking algorithm\n", "abstract": " Handling pointers in a programming logic has been one of the most important and di cult problems in programming language research. The di culty does not lie in coming up with any form of programming logic for pointer programs; in fact, there exist approaches either based on the\\pointer as an index of an array\" idea 3] or on the use of semantic-based substitutions 4, 1]. Instead, the core of the problem is to obtain a formalism which matches up with an informal reasoning by programmers and algorithm designers. That is, the veri cation of a program should be only as complicated as an informal correctness argument for the program. Most of the existing formalisms fail in this criteria: when they are used in verifying a pointer program, the veri cation based on them becomes signi cantly more complicated than an informal argument. Only recently, Reynolds, Ishtiaq and O'Hearn 6, 2, 7] pointed out that one way to achieve such a match is to exploit the locality of memory access within a code fragment: usually for a given code fragment, only small number of cells on the heap are accessed. Ishtiaq and O'Hearn formulated a logic for pointer programs based on the logic of Bunched Implications 5] and proposed a rule which captures the locality:", "num_citations": "67\n", "authors": ["663"]}
{"title": "Semantics of separation-logic typing and higher-order frame rules\n", "abstract": " We show how to give a coherent semantics to programs that are well-specified in a version of separation logic for a language with higher types: idealized algol extended with heaps (but with immutable stack variables). In particular, we provide simple sound rules for deriving higher-order frame rules, allowing for local reasoning.", "num_citations": "53\n", "authors": ["663"]}
{"title": "Correctness of data representations involving heap data structures\n", "abstract": " While the semantics of local variables in programming languages is by now well-understood, the semantics of pointer-addressed heap variables is still an outstanding issue. In particular, the commonly assumed relational reasoning principles for data representations have not been validated in a semantic model of heap variables. In this paper, we define a parametricity semantics for a Pascal-like language with pointers and heap variables which gives such reasoning principles. It turns out that the correspondences between data representations cannot simply be relations between states, but more intricate correspondences that also need to keep track of visible locations whose pointers can be stored and leaked.", "num_citations": "53\n", "authors": ["663"]}
{"title": "Relational parametricity and separation logic\n", "abstract": " Separation logic is a recent extension of Hoare logic for reasoning about programs with references to shared mutable data structures. In this paper, we provide a new interpretation of the logic for a programming language with higher types. Our interpretation is based on Reynolds\u2019s relational parametricity, and it provides a formal connection between separation logic and data abstraction.", "num_citations": "52\n", "authors": ["663"]}
{"title": "Specification and complexity of collaborative text editing\n", "abstract": " Collaborative text editing systems allow users to concurrently edit a shared document, inserting and deleting elements (eg, characters or lines). There are a number of protocols for collaborative text editing, but so far there has been no precise specification of their desired behavior, and several of these protocols have been shown not to satisfy even basic expectations. This paper provides a precise specification of a replicated list object, which models the core functionality of replicated systems for collaborative text editing. We define a strong list specification, which we prove is implemented by an existing protocol, as well as a weak list specification, which admits additional protocol behaviors.", "num_citations": "51\n", "authors": ["663"]}
{"title": "Liveness-preserving atomicity abstraction\n", "abstract": " Modern concurrent algorithms are usually encapsulated in libraries, and complex algorithms are often constructed using libraries of simpler ones. We present the first theorem that allows harnessing this structure to give compositional liveness proofs to concurrent algorithms and their clients. We show that, while proving a liveness property of a client using a concurrent library, we can soundly replace the library by another one related to the original library by a generalisation of a well-known notion of linearizability. We apply this result to show formally that lock-freedom, an often-used liveness property of non-blocking algorithms, is compositional for linearizable libraries, and provide an example illustrating our proof technique.", "num_citations": "50\n", "authors": ["663"]}
{"title": "Denotational validation of higher-order Bayesian inference\n", "abstract": " We present a modular semantic account of Bayesian inference algorithms for probabilistic programming languages, as used in data science and machine learning. Sophisticated inference algorithms are often explained in terms of composition of smaller parts. However, neither their theoretical justification nor their implementation reflects this modularity. We show how to conceptualise and analyse such inference algorithms as manipulating intermediate representations of probabilistic programs using higher-order functions and inductive types, and their denotational semantics. Semantic accounts of continuous distributions use measurable spaces. However, our use of higher-order functions presents a substantial technical difficulty: it is impossible to define a measurable space structure over the collection of measurable functions between arbitrary measurable spaces that is compatible with standard operations on those functions, such as function application. We overcome this difficulty using quasi-Borel spaces, a recently proposed mathematical structure that supports both function spaces and continuous distributions. We define a class of semantic structures for representing probabilistic programs, and semantic validity criteria for transformations of these representations in terms of distribution preservation. We develop a collection of building blocks for composing representations. We use these building blocks to validate common inference algorithms such as Sequential Monte Carlo and Markov Chain Monte Carlo. To emphasize the connection between the semantic manipulation and its traditional measure theoretic origins, we use Kock's synthetic\u00a0\u2026", "num_citations": "47\n", "authors": ["663"]}
{"title": "Semantics of separation-logic typing and higher-order frame rules for Algol-like languages\n", "abstract": " We show how to give a coherent semantics to programs that are well-specified in a version of separation logic for a language with higher types: idealized algol extended with heaps (but with immutable stack variables). In particular, we provide simple sound rules for deriving higher-order frame rules, allowing for local reasoning.", "num_citations": "45\n", "authors": ["663"]}
{"title": "The CISE tool: proving weakly-consistent applications correct\n", "abstract": " Designers of a replicated database face a vexing choice between strong consistency, which ensures certain application invariants but is slow and fragile, and asynchronous replication, which is highly available and responsive, but exposes the programmer to unfamiliar behaviours. To bypass this conundrum, recent research has studied hybrid consistency models, in which updates are asynchronous by default, but synchronisation is available upon request. To help programmers exploit hybrid consistency, we propose the first static analysis tool for proving integrity invariants of applications using databases with hybrid consistency models. This allows a programmer to find minimal consistency guarantees sufficient for application correctness.", "num_citations": "44\n", "authors": ["663"]}
{"title": "Program analysis for overlaid data structures\n", "abstract": " We call a data structure overlaid, if a node in the structure includes links for multiple data structures and these links are intended to be used at the same time. In this paper, we present a static program analysis for overlaid data structures. Our analysis implements two main ideas. The first is to run multiple sub-analyses that track information about non-overlaid data structures, such as lists. Each sub-analysis infers shape properties of only one component of an overlaid data structure, but the results of these sub-analyses are later combined to derive the desired safety properties about the whole overlaid data structure. The second idea is to control the communication among the sub-analyses using ghost states and ghost instructions. The purpose of this control is to achieve a high level of efficiency by allowing only necessary information to be transferred among sub-analyses and at as few program points as\u00a0\u2026", "num_citations": "44\n", "authors": ["663"]}
{"title": "Linearizability with ownership transfer\n", "abstract": " Linearizability is a commonly accepted notion of correctness for libraries of concurrent algorithms. Unfortunately, it assumes a complete isolation between a library and its client, with interactions limited to passing values of a given data type. This is inappropriate for common programming languages, where libraries and their clients can communicate via the heap, transferring the ownership of data structures, and can even run in a shared address space without any memory protection. In this paper, we present the first definition of linearizability that lifts this limitation and establish an Abstraction Theorem: while proving a property of a client of a concurrent library, we can soundly replace the library by its abstract implementation related to the original one by our generalisation of linearizability. This allows abstracting from the details of the library implementation while reasoning about the client. We also prove that linearizability with ownership transfer can be derived from the classical one if the library does not access some of data structures transferred to it by the client.", "num_citations": "42\n", "authors": ["663"]}
{"title": "Hybrid top-down and bottom-up interprocedural analysis\n", "abstract": " Interprocedural static analyses are broadly classified into top-down and bottom-up, depending upon how they compute, instantiate, and reuse procedure summaries. Both kinds of analyses are challenging to scale: top-down analyses are hindered by ineffective reuse of summaries whereas bottom-up analyses are hindered by inefficient computation and instantiation of summaries. This paper presents a hybrid approach Swift that combines top-down and bottom-up analyses in a manner that gains their benefits without suffering their drawbacks. Swift is general in that it is parametrized by the top-down and bottom-up analyses it combines. We show an instantiation of Swift on a type-state analysis and evaluate it on a suite of 12 Java programs of size 60-250 KLOC each. Swift outperforms both conventional approaches, finishing on all the programs while both of those approaches fail on the larger programs.", "num_citations": "41\n", "authors": ["663"]}
{"title": "Verifying concurrent memory reclamation algorithms with grace\n", "abstract": " Memory management is one of the most complex aspects of modern concurrent algorithms, and various techniques proposed for it\u2014such as hazard pointers, read-copy-update and epoch-based reclamation\u2014have proved very challenging for formal reasoning. In this paper, we show that different memory reclamation techniques actually rely on the same implicit synchronisation pattern, not clearly reflected in the code, but only in the form of assertions used to argue its correctness. The pattern is based on the key concept of a grace period, during which a thread can access certain shared memory cells without fear that they get deallocated. We propose a modular reasoning method, motivated by the pattern, that handles all three of the above memory reclamation techniques in a uniform way. By explicating their fundamental core, our method achieves clean and simple proofs, scaling even to realistic\u00a0\u2026", "num_citations": "41\n", "authors": ["663"]}
{"title": "Finding optimum abstractions in parametric dataflow analysis\n", "abstract": " We propose a technique to efficiently search a large family of abstractions in order to prove a query using a parametric dataflow analysis. Our technique either finds the cheapest such abstraction or shows that none exists. It is based on counterexample-guided abstraction refinement but applies a novel meta-analysis on abstract counterexample traces to efficiently find abstractions that are incapable of proving the query. We formalize the technique in a generic framework and apply it to two analyses: a type-state analysis and a thread-escape analysis. We demonstrate the effectiveness of the technique on a suite of Java benchmark programs.", "num_citations": "37\n", "authors": ["663"]}
{"title": "Learning analysis strategies for octagon and context sensitivity from labeled data generated by static analyses\n", "abstract": " We present a method for automatically learning an effective strategy for clustering variables for the Octagon analysis from a given codebase. This learned strategy works as a preprocessor of Octagon. Given a program to be analyzed, the strategy is first applied to the program and clusters variables in it. We then run a partial variant of the Octagon analysis that tracks relationships among variables within the same cluster, but not across different clusters. The notable aspect of our learning method is that although the method is based on supervised learning, it does not require manually-labeled data. The method does not ask human to indicate which pairs of program variables in the given codebase should be tracked. Instead it uses the impact pre-analysis for Octagon from our previous work and automatically labels variable pairs in the codebase as positive or negative. We implemented our method on top of a\u00a0\u2026", "num_citations": "35\n", "authors": ["663"]}
{"title": "Automatically generating features for learning program analysis heuristics for C-like languages\n", "abstract": " We present a technique for automatically generating features for data-driven program analyses. Recently data-driven approaches for building a program analysis have been developed, which mine existing codebases and automatically learn heuristics for finding a cost-effective abstraction for a given analysis task. Such approaches reduce the burden of the analysis designers, but they do not remove it completely; they still leave the nontrivial task of designing so called features to the hands of the designers. Our technique aims at automating this feature design process. The idea is to use programs as features after reducing and abstracting them. Our technique goes through selected program-query pairs in codebases, and it reduces and abstracts the program in each pair to a few lines of code, while ensuring that the analysis behaves similarly for the original and the new programs with respect to the query. Each\u00a0\u2026", "num_citations": "31\n", "authors": ["663"]}
{"title": "Modular verification of preemptive OS kernels\n", "abstract": " Most major OS kernels today run on multiprocessor systems and are preemptive: it is possible for a process running in the kernel mode to get descheduled. Existing modular techniques for verifying concurrent code are not directly applicable in this setting: they rely on scheduling being implemented correctly, and in a preemptive kernel, the correctness of the scheduler is interdependent with the correctness of the code it schedules. This interdependency is even stronger in mainstream kernels, such as Linux, FreeBSD or XNU, where the scheduler and processes interact in complex ways. We propose the first logic that is able to decompose the verification of preemptive multiprocessor kernel code into verifying the scheduler and the rest of the kernel separately, even in the presence of complex interdependencies between the two components. The logic hides the manipulation of control by the scheduler when\u00a0\u2026", "num_citations": "28\n", "authors": ["663"]}
{"title": "Abstraction refinement guided by a learnt probabilistic model\n", "abstract": " The core challenge in designing an effective static program analysis is to find a good program abstraction--one that retains only details relevant to a given query. In this paper, we present a new approach for automatically finding such an abstraction. Our approach uses a pessimistic strategy, which can optionally use guidance from a probabilistic model. Our approach applies to parametric static analyses implemented in Datalog, and is based on counterexample-guided abstraction refinement. For each untried abstraction, our probabilistic model provides a probability of success, while the size of the abstraction provides an estimate of its cost in terms of analysis time. Combining these two metrics, probability and cost, our refinement algorithm picks an optimal abstraction. Our probabilistic model is a variant of the Erdos--Renyi random graph model, and it is tunable by what we call hyperparameters. We present a\u00a0\u2026", "num_citations": "27\n", "authors": ["663"]}
{"title": "LF-PPL: A low-level first order probabilistic programming language for non-differentiable models\n", "abstract": " We develop a new Low-level, First-order Probabilistic Programming Language (LF-PPL) suited for models containing a mix of continuous, discrete, and/or piecewise-continuous variables. The key success of this language and its compilation scheme is in its ability to automatically distinguish parameters the density function is discontinuous with respect to, while further providing runtime checks for boundary crossings. This enables the introduction of new inference engines that are able to exploit gradient information, while remaining efficient for models which are not everywhere differentiable. We demonstrate this ability by incorporating a discontinuous Hamiltonian Monte Carlo (DHMC) inference engine that is able to deliver automated and efficient inference for non-differentiable models. Our system is backed up by a mathematical formalism that ensures that any model expressed in this language has a density with measure zero discontinuities to maintain the validity of the inference engine.", "num_citations": "22\n", "authors": ["663"]}
{"title": "Algebraic laws for weak consistency\n", "abstract": " Modern distributed systems often rely on so called weakly consistent databases, which achieve scalability by weakening consistency guarantees of distributed transaction processing. The semantics of such databases have been formalised in two different styles, one based on abstract executions and the other based on dependency graphs. The choice between these styles has been made according to intended applications. The former has been used for specifying and verifying the implementation of the databases, while the latter for proving properties of client programs of the databases. In this paper, we present a set of novel algebraic laws (inequalities) that connect these two styles of specifications. The laws relate binary relations used in a specification based on abstract executions to those used in a specification based on dependency graphs. We then show that this algebraic connection gives rise to so called robustness criteria: conditions which ensure that a client program of a weakly consistent database does not exhibit anomalous behaviours due to weak consistency. These criteria make it easy to reason about these client programs, and may become a basis for dynamic or static program analyses. For a certain class of consistency models specifications, we prove a full abstraction result that connects the two styles of specifications.", "num_citations": "20\n", "authors": ["663"]}
{"title": "Transaction chopping for parallel snapshot isolation\n", "abstract": " Modern Internet services often achieve scalability and availability by relying on large-scale distributed databases that provide consistency models for transactions weaker than serialisability. We investigate the classical problem of transaction chopping for a promising consistency model in this class\u2014parallel snapshot isolation (PSI), which weakens the classical snapshot isolation to allow more efficient large-scale implementations. Namely, we propose a criterion for checking when a set of transactions executing on PSI can be chopped into smaller pieces without introducing new behaviours, thus improving efficiency. We find that our criterion is more permissive than the existing one for chopping serialisable transactions. To establish our criterion, we propose a novel declarative specification of PSI that does not refer to implementation-level concepts and, thus, allows reasoning about the behaviour of PSI\u00a0\u2026", "num_citations": "20\n", "authors": ["663"]}
{"title": "Reparameterization gradient for non-differentiable models\n", "abstract": " We present a new algorithm for stochastic variational inference that targets at models with non-differentiable densities. One of the key challenges in stochastic variational inference is to come up with a low-variance estimator of the gradient of a variational objective. We tackle the challenge by generalizing the reparameterization trick, one of the most effective techniques for addressing the variance issue for differentiable models, so that the trick works for non-differentiable models as well. Our algorithm splits the space of latent variables into regions where the density of the variables is differentiable, and their boundaries where the density may fail to be differentiable. For each differentiable region, the algorithm applies the standard reparameterization trick and estimates the gradient restricted to the region. For each potentially non-differentiable boundary, it uses a form of manifold sampling and computes the direction for variational parameters that, if followed, would increase the boundary's contribution to the variational objective. The sum of all the estimates becomes the gradient estimate of our algorithm. Our estimator enjoys the reduced variance of the reparameterization gradient while remaining unbiased even for non-differentiable models. The experiments with our preliminary implementation confirm the benefit of reduced variance and unbiasedness.", "num_citations": "17\n", "authors": ["663"]}
{"title": "Composite Replicated Data Types\n", "abstract": " Modern large-scale distributed systems often rely on eventually consistent replicated stores, which achieve scalability in exchange for providing weak semantic guarantees. To compensate for this weakness, researchers have proposed various abstractions for programming on eventual consistency, such as replicated data types for resolving conflicting updates at different replicas and weak forms of transactions for maintaining relationships among objects. However, the subtle semantics of these abstractions makes using them correctly far from trivial.               To address this challenge, we propose composite replicated data types, which formalise a common way of organising applications on top of eventually consistent stores. Similarly to an abstract data type, a composite data type encapsulates objects of replicated data types and operations used to access them, implemented using transactions.We develop\u00a0\u2026", "num_citations": "16\n", "authors": ["663"]}
{"title": "Parameterised Linearisability\n", "abstract": " Many concurrent libraries are parameterised, meaning that they implement generic algorithms that take another library as a parameter. In such cases, the standard way of stating the correctness of concurrent libraries via linearisability is inapplicable. We generalise linearisability to parameterised libraries and investigate subtle trade-offs between the assumptions that such libraries can make about their environment and the conditions that linearisability has to impose on different types of interactions with it. We prove that the resulting parameterised linearisability is closed under instantiating parameter libraries and composing several non-interacting libraries, and furthermore implies observational refinement. These results allow modularising the reasoning about concurrent programs using parameterised libraries and confirm the appropriateness of the proposed definitions. We illustrate the applicability of our\u00a0\u2026", "num_citations": "16\n", "authors": ["663"]}
{"title": "A correspondence between two approaches to interprocedural analysis in the presence of join\n", "abstract": " Many interprocedural static analyses perform a lossy join for reasons of termination or efficiency. We study the relationship between two predominant approaches to interprocedural analysis, the summary-based (or functional) approach and the call-strings (or k-CFA) approach, in the presence of a lossy join. Despite the use of radically different ways to distinguish procedure contexts by these two approaches, we prove that post-processing their results using a form of garbage collection renders them equivalent. Our result extends the classic result by Sharir and Pnueli that showed the equivalence between these two approaches in the setting of distributive analysis, wherein the join is lossless.               We also empirically compare these two approaches by applying them to a pointer analysis that performs a lossy join. Our experiments on ten Java programs of size 400K\u2013900K bytecodes show that the summary\u00a0\u2026", "num_citations": "14\n", "authors": ["663"]}
{"title": "Resource-aware program analysis via online abstraction coarsening\n", "abstract": " We present a new technique for developing a resource-aware program analysis. Such an analysis is aware of constraints on available physical resources, such as memory size, tracks its resource use, and adjusts its behaviors during fixpoint computation in order to meet the constraint and achieve high precision. Our resource-aware analysis adjusts behaviors by coarsening program abstraction, which usually makes the analysis consume less memory and time until completion. It does so multiple times during the analysis, under the direction of what we call a controller. The controller constantly intervenes in the fixpoint computation of the analysis and decides how much the analysis should coarsen the abstraction. We present an algorithm for learning a good controller automatically from benchmark programs. We applied our technique to a static analysis for C programs, where we control the degree of flow-sensitivity\u00a0\u2026", "num_citations": "13\n", "authors": ["663"]}
{"title": "Data refinement with low-level pointer operations\n", "abstract": " We present a method for proving data refinement in the presence of low-level pointer operations, such as memory allocation and deallocation, and pointer arithmetic. Surprisingly, none of the existing methods for data refinement, including those specifically designed for pointers, are sound in the presence of low-level pointer operations. The reason is that the low-level pointer operations allow an additional potential for obtaining the information about the implementation details of the module: using memory allocation and pointer comparison, a client of a module can find out which cells are internally used by the module, even without dereferencing any pointers. The unsoundness of the existing methods comes from the failure of handling this potential. In the paper, we propose a novel method for proving data refinement, called power simulation, and show that power simulation is sound even with low-level\u00a0\u2026", "num_citations": "13\n", "authors": ["663"]}
{"title": "The Beta-Bernoulli process and algebraic effects\n", "abstract": " In this paper we use the framework of algebraic effects from programming language theory to analyze the Beta-Bernoulli process, a standard building block in Bayesian models. Our analysis reveals the importance of abstract data types, and two types of program equations, called commutativity and discardability. We develop an equational theory of terms that use the Beta-Bernoulli process, and show that the theory is complete with respect to the measure-theoretic semantics, and also in the syntactic sense of Post. Our analysis has a potential for being generalized to other stochastic processes relevant to Bayesian modelling, yielding new understanding of these processes from the perspective of programming.", "num_citations": "12\n", "authors": ["663"]}
{"title": "Inference trees: Adaptive inference with exploration\n", "abstract": " We introduce inference trees (ITs), a new class of inference methods that build on ideas from Monte Carlo tree search to perform adaptive sampling in a manner that balances exploration with exploitation, ensures consistency, and alleviates pathologies in existing adaptive methods. ITs adaptively sample from hierarchical partitions of the parameter space, while simultaneously learning these partitions in an online manner. This enables ITs to not only identify regions of high posterior mass, but also maintain uncertainty estimates to track regions where significant posterior mass may have been missed. ITs can be based on any inference method that provides a consistent estimate of the marginal likelihood. They are particularly effective when combined with sequential Monte Carlo, where they capture long-range dependencies and yield improvements beyond proposal adaptation alone.", "num_citations": "11\n", "authors": ["663"]}
{"title": "Two for the price of one: Lifting separation logic assertions\n", "abstract": " Recently, data abstraction has been studied in the context of separation logic, with noticeable practical successes: the developed logics have enabled clean proofs of tricky challenging programs, such as subject-observer patterns, and they have become the basis of efficient verification tools for Java (jStar), C (VeriFast) and Hoare Type Theory (Ynot). In this paper, we give a new semantic analysis of such logic-based approaches using Reynolds's relational parametricity. The core of the analysis is our lifting theorems, which give a sound and complete condition for when a true implication between assertions in the standard interpretation entails that the same implication holds in a relational interpretation. Using these theorems, we provide an algorithm for identifying abstraction-respecting client-side proofs; the proofs ensure that clients cannot distinguish two appropriately-related module implementations.", "num_citations": "9\n", "authors": ["663"]}
{"title": "A generalization of hierarchical exchangeability on trees to directed acyclic graphs\n", "abstract": " Motivated by the problem of designing inference-friendly Bayesian nonparametric models in probabilistic programming languages, we introduce a general class of partially exchangeable random arrays which generalizes the notion of hierarchical exchangeability introduced in Austin and Panchenko (2014). We say that our partially exchangeable arrays are DAG-exchangeable since their partially exchangeable structure is governed by a collection of Directed Acyclic Graphs. More specifically, such a random array is indexed by \u2115| V| for some DAG G=(V, E), and its exchangeability structure is governed by the edge set E. We prove a representation theorem for such arrays which generalizes the Aldous-Hoover and Austin\u2013Panchenko representation theorems.", "num_citations": "7\n", "authors": ["663"]}
{"title": "Imperative lambda calculus revisited\n", "abstract": " Imperative Lambda Calculus is a type system designed to combine functional and imperative programming features in an orthogonal fashion without compromising the algebraic properties of functions. It has been noted that the original system is too restrictive and lacks the subject reduction property. We de ne a revised type system that solves these problems using ideas from Reynolds's Syntactic Control of Interference. We also extend it to handle Hindley-Milner style polymorphism and devise type reconstruction algorithms. A sophisticated constraint language is designed to formulate principal types for terms.", "num_citations": "7\n", "authors": ["663"]}
{"title": "Divide, conquer, and combine: a new inference strategy for probabilistic programs with stochastic support\n", "abstract": " Universal probabilistic programming systems (PPSs) provide a powerful framework for specifying rich probabilistic models. They further attempt to automate the process of drawing inferences from these models, but doing this successfully is severely hampered by the wide range of non\u2013standard models they can express. As a result, although one can specify complex models in a universal PPS, the provided inference engines often fall far short of what is required. In particular, we show that they produce surprisingly unsatisfactory performance for models where the support varies between executions, often doing no better than importance sampling from the prior. To address this, we introduce a new inference framework: Divide, Conquer, and Combine, which remains efficient for such models, and show how it can be implemented as an automated and generic PPS inference engine. We empirically demonstrate substantial performance improvements over existing approaches on three examples.", "num_citations": "6\n", "authors": ["663"]}
{"title": "Exchangeable random processes and data abstraction\n", "abstract": " Several Turing-complete probabilistic programming languages such as Church, Venture and Anglican support so-called exchangeable random processes (XRPs) as built-in abstract data types. Instances of these data types denote infinite sequences of random values. Every such object satisfies the so-called exchangeability property, which says that the probability distribution of the object does not change under permutations of the sequence. In fact, exchangeability is one of the core concepts in Bayesian statistics, from which these infinite sequences originate.(Extensions of the notion of exchangeability for sequences apply to other infinite random objects such as graphs, which we consider in Section 3.) We have been working on semantic foundations and reasoning principles for data types for exchangeable random processes, with special emphasis on the client programs of these data types. One slogan that summarises our efforts well is this: a data type for a random process is exchangeable if its inclusion in a probabilistic programming language does not break the following commutativity and discardability of the language, ie,(let x= M in y= M\u2019in N)=(let y= M\u2019in x= M in N)(let x= M in M\u2019)= M\u2019where x\u2208 FV (M\u2019) and y\u2208 FV (M).(See also informal discussions in [1, 5],[10, \u00a7 2.2. 1].) The specifics of a particular data type for an exchangeable random process are then captured by additional program equations, which in turn give rise to a monad for interpreting client programs of the data type. This equational view may suggest how the data-type developers should use an abstraction mechanism of a programming language (eg, abstract type) so as to hide\u00a0\u2026", "num_citations": "6\n", "authors": ["663"]}
{"title": "The semantic structure of quasi-Borel spaces\n", "abstract": " Quasi-Borel spaces are a new mathematical structure that supports higher-order probability theory, first-order iteration, and modular semantic validation of Bayesian inference algorithms with continuous distributions. Like a measurable space, a quasi-Borel space is a set with extra structure suitable for defining probability and measure distributions. But unlike measurable spaces, quasi-Borel spaces and their structure-preserving maps form a well-behaved category: they are cartesianclosed, and so suitable for higher-order semantics, and they also form a model of Kock\u2019s synthetic measure theory, and so suitable for probabilistic, and measure-theoretic, developments, such as the Metropolis-Hastings-Green theorem underlying Markov-Chain Monte-Carlo algorithms.", "num_citations": "5\n", "authors": ["663"]}
{"title": "Modularity in Lattices: A Case Study on the Correspondence Between Top-Down and Bottom-Up Analysis\n", "abstract": " Interprocedural analyses are compositional when they compute over-approximations of procedures in a bottom-up fashion. These analyses are usually more scalable than top-down analyses, which compute a different procedure summary for every calling context. However, compositional analyses are rare in practice as it is difficult to develop them with enough precision.We establish a connection between compositional analyses and modular lattices, which require certain associativity between the lattice join and meet operations, and use it to develop a compositional version of the connection analysis by Ghiya and Hendren. Our version is slightly more conservative than the original top-down analysis in order to meet our modularity requirement. When applied to real-world Java programs our analysis scaled much better than the original top-down version: The top-down analysis times out in the largest two of our five programs, while ours incurred only 2\u20135% of precision loss in the remaining programs.", "num_citations": "5\n", "authors": ["663"]}
{"title": "On the semantics of refinement calculi\n", "abstract": " Refinement calculi for imperative programs provide an integrated framework for programs and specifications and allow one to develop programs from specifications in a systematic fashion. The semantics of these calculi has traditionally been defined in terms of predicate transformers and poses several challenges in defining a state transformer semantics in the denotational style. We define a novel semantics in terms of sets of state transformers and prove it to be isomorphic to positively multiplicative predicate transformers. This semantics disagrees with the traditional semantics in some places and the consequences of the disagreement are analyzed.", "num_citations": "5\n", "authors": ["663"]}
{"title": "Computing engine, software, system and method\n", "abstract": " There is provided a computing engine (10) for use in simulating a complex system (20), controlling the complex system (20), or a combination of simulating and controlling the complex system (20), wherein the computing engine (10) includes a data processing arrangement (40) that is operable to execute one or more program instructions. The computing engine (10) includes a plurality of computational modules (50) that are operable to exchange data therebetween via a data exchange arrangement (60). The computational modules (50) are operable to execute one or more computational functions therein on data received at the computational modules (50) and to generate corresponding output data (R). The computational modules (50) are operable to receive, for input data to their one or more computational functions, at least one of: user input values, sensed data from the complex system (20). The computational\u00a0\u2026", "num_citations": "4\n", "authors": ["663"]}
{"title": "Type reconstruction for syntactic control of interference. 2\n", "abstract": " Syntactic control of interference (SCI) (J.C. Reynolds, 1978) has long been studied as a basis for interference free programming, with cleaner reasoning properties and semantics than traditional imperative languages. The paper improves upon H. Huang and U.S. Reddy's (1996) type inference system for SCI based languages in two significant ways. First, we eliminate the need for explicit coercion operators in terms. Second, we consider adding let-bound polymorphism, which appears to be nontrivial in the presence of interference control. SCI can be adapted to a wide variety of languages, and our techniques should be applicable to any such language with SCI based interference control.", "num_citations": "3\n", "authors": ["663"]}
{"title": "Spreadsheet probabilistic programming\n", "abstract": " Spreadsheet workbook contents are simple programs. Because of this, probabilistic programming techniques can be used to perform Bayesian inversion of spreadsheet computations. What is more, existing execution engines in spreadsheet applications such as Microsoft Excel can be made to do this using only built-in functionality. We demonstrate this by developing a native Excel implementation of both a particle Markov Chain Monte Carlo variant and black-box variational inference for spreadsheet probabilistic programming. The resulting engine performs probabilistically coherent inference over spreadsheet computations, notably including spreadsheets that include user-defined black-box functions. Spreadsheet engines that choose to integrate the functionality we describe in this paper will give their users the ability to both easily develop probabilistic models and maintain them over time by including actuals via a simple user-interface mechanism. For spreadsheet end-users this would mean having access to efficient and probabilistically coherent probabilistic modeling and inference for use in all kinds of decision making under uncertainty.", "num_citations": "2\n", "authors": ["663"]}
{"title": "Scale Mixtures of Neural Network Gaussian Processes\n", "abstract": " Recent works have revealed that infinitely-wide feed-forward or recurrent neural networks of any architecture correspond to Gaussian processes referred to as . While these works have extended the class of neural networks converging to Gaussian processes significantly, however, there has been little focus on broadening the class of stochastic processes that such neural networks converge to. In this work, inspired by the scale mixture of Gaussian random variables, we propose the scale mixture of  for which we introduce a prior distribution on the scale of the last-layer parameters. We show that simply introducing a scale prior on the last-layer parameters can turn infinitely-wide neural networks of any architecture into a richer class of stochastic processes. Especially, with certain scale priors, we obtain heavy-tailed stochastic processes, and we recover Student's  processes in the case of inverse gamma priors. We further analyze the distributions of the neural networks initialized with our prior setting and trained with gradient descents and obtain similar results as for . We present a practical posterior-inference algorithm for the scale mixture of  and empirically demonstrate its usefulness on regression and classification tasks.", "num_citations": "1\n", "authors": ["663"]}
{"title": "Probabilistic Programs with Stochastic Conditioning\n", "abstract": " We tackle the problem of conditioning probabilistic programs on distributions of observable variables. Probabilistic programs are usually conditioned on samples from the joint data distribution, which we refer to as deterministic conditioning. However, in many real-life scenarios, the observations are given as marginal distributions, summary statistics, or samplers. Conventional probabilistic programming systems lack adequate means for modeling and inference in such scenarios. We propose a generalization of deterministic conditioning to stochastic conditioning, that is, conditioning on the marginal distribution of a variable taking a particular form. To this end, we first define the formal notion of stochastic conditioning and discuss its key properties. We then show how to perform inference in the presence of stochastic conditioning. We demonstrate potential usage of stochastic conditioning on several case studies which involve various kinds of stochastic conditioning and are difficult to solve otherwise. Although we present stochastic conditioning in the context of probabilistic programming, our formalization is general and applicable to other settings.", "num_citations": "1\n", "authors": ["663"]}
{"title": "Specification and space complexity of collaborative text editing\n", "abstract": " Collaborative text editing systems allow users to concurrently edit a shared document, inserting and deleting elements (e.g., characters or lines). There are a number of protocols for collaborative text editing, but so far there has been no abstract, high-level specification of their desired behavior, which is decoupled from their actual implementation. Several of these protocols have been shown not to satisfy even basic expectations. This paper provides a precise specification of a replicated abstract list object, which models the core functionality of replicated systems for collaborative text editing. We define a strong list specification, which we prove is implemented by an existing protocol, as well as a weak list specification, which admits additional protocol behaviors.A major factor determining the efficiency and practical feasibility of a collaborative text editing protocol is the space overhead of the metadata that the protocol\u00a0\u2026", "num_citations": "1\n", "authors": ["663"]}
{"title": "Variational Inference for Sequential Data with Future Likelihood Estimates\n", "abstract": " The recent development of flexible and scalable variational inference algorithms has popularized the use of deep probabilistic models in a wide range of applications. However, learning and reasoning about high-dimensional models with nondifferentiable densities are still a challenge. For such a model, inference algorithms struggle to estimate the gradients of variational objectives accurately, due to high variance in their estimates. To tackle this challenge, we present a novel variational inference algorithm for sequential data, which performs well even when the density from the model is not differentiable, for instance, due to the use of discrete random variables. The key feature of our algorithm is that it estimates future likelihoods at all time steps. The estimated future likelihoods form the core of our new low-variance gradient estimator. We formally analyze our gradient estimator from the perspective of variational objective, and show the effectiveness of our algorithm with synthetic and real datasets.", "num_citations": "1\n", "authors": ["663"]}
{"title": "Some semantic issues in probabilistic programming languages (invited talk)\n", "abstract": " This is a slightly extended abstract of my talk at FSCD'19 about probabilistic programming and a few semantic issues on it. The main purpose of this abstract is to provide keywords and references on the work mentioned in my talk, and help interested audience to do follow-up study.", "num_citations": "1\n", "authors": ["663"]}
{"title": "Towards a Testable Notion of Generalization for Generative Adversarial Networks\n", "abstract": " We consider the question of how to assess generative adversarial networks, in particular with respect to whether or not they generalise beyond memorising the training data. We propose a simple procedure for assessing generative adversarial network performance based on a principled consideration of what the actual goal of generalisation is. Our approach involves using a test set to estimate the Wasserstein distance between the generative distribution produced by our procedure, and the underlying data distribution. We use this procedure to assess the performance of several modern generative adversarial network architectures. We find that this procedure is sensitive to the choice of ground metric on the underlying data space, and suggest a choice of ground metric that substantially improves performance. We finally suggest that attending to the ground metric used in Wasserstein generative adversarial network training may be fruitful, and outline a concrete pathway towards doing so.", "num_citations": "1\n", "authors": ["663"]}
{"title": "Probabilistic Programming (Invited Talk)\n", "abstract": " Probabilistic programming refers to the idea of using standard programming constructs for specifying probabilistic models from machine learning and statistics, and employing generic inference algorithms for answering various queries on these models, such as posterior inference and estimation of model evidence. Although this idea itself is not new and was, in fact, explored by several programming-language and statistics researchers in the early 2000, it is only in the last few years that probabilistic programming has gained a large amount of attention among researchers in machine learning and programming languages, and that expressive and efficient probabilistic programming systems (such as Anglican, Church, Figaro, Infer. net, PSI, PyMC, Stan, and Venture) started to appear and have been taken up by a nontrivial number of users. The primary goal of my talk is to introduce probabilistic programming to the CONCUR/QUEST/FORMATS audience. At the end of my talk, I want the audience to understand basic results and techniques in probabilistic programming and to feel that these results and techniques are relevant or at least related to what she or he studies, although they typically come from foreign research areas, such as machine learning and statistics. My talk will contain both technical materials and lessons that I learnt from my machine-learning colleagues in Oxford, who are developing a highly-expressive higher-order probabilistic programming language, called Anglican. It will also include my work on the denotational semantics of higher-order probabilistic programming languages and their inference algorithms, which are jointly\u00a0\u2026", "num_citations": "1\n", "authors": ["663"]}
{"title": "Efficient exact inference in discrete Anglican programs\n", "abstract": " The design of a general probabilistic programming system involves a balancing act between two deeply conflicting concerns. On the one hand, the system should provide as uniform and flexible an interface for specifying models as possible; on the other, it should be capable of doing efficient inference for any particular model specified. Current systems lie somewhere on a spectrum that ranges from highly expressive languages such as Church [2], Anglican [10], and Venture [3], to highly performant languages like Figaro [7], FACTORIE [4], and Infer .NET [5]. It has not yet been shown possible to optimise both these concerns simultaneously. To improve on this predicament we consider the class of discrete graphical models, for which various efficient exact inference algorithms exist. We present a technique for determining when an Anglican program expresses such a model, which allows us to achieve a substantial increase in inference performance in this case. Our approach can handle complicated language features including higher-order functions, bounded recursion, and data structures, which means that, for the discrete subset of Anglican, we do not incur any loss in expressiveness. Moreover, the resulting inference is exact, which can be useful in contexts where very high accuracy is required, or for doing nested inference inside larger models. Details of Anglican can be found in [10], but for our purposes its semantics may be understood as the Clojure programming language augmented with a sample statement and an observe statement. sample takes as argument a distribution object\u2206(which may be obtained by calling the built-in flip or dirac\u00a0\u2026", "num_citations": "1\n", "authors": ["663"]}
{"title": "Metric spaces and termination analyses\n", "abstract": " We present a framework for defining abstract interpreters for liveness properties, in particular program termination. The framework makes use of the theory of metric spaces to define a concrete semantics, relates this semantics with the usual order-theoretic semantics of abstract interpretation, and identifies a set of conditions for determining when an abstract interpreter is sound for analysing liveness properties. Our soundness proof of the framework is based on a novel relationship between unique fixpoints in metric semantics and post-fixpoints computed by abstract interpreters. We illustrate the power of the framework by providing an instance that can automatically prove the termination of programs with general (not necessarily tail) recursion.", "num_citations": "1\n", "authors": ["663"]}
{"title": "Parametric sheaves for modelling store locality\n", "abstract": " In this paper, we bring together two important ideas in the semantics of Algol-like imperative programming languages. One is that program phrases act on xed sets of storage locations. The second is that the information of local variables is hidden from client programs. This involves combining sheaf theory and parametricity to produce new classes of sheaves. We de ne the semantics of an Algol-like language using such sheaves and discuss the reasoning principles validated by the semantics.", "num_citations": "1\n", "authors": ["663"]}