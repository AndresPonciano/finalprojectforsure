{"title": "Information credibility on Twitter\n", "abstract": " We analyze the information credibility of news propagated through Twitter, a popular microblogging service. Previous research has shown that most of the messages posted on Twitter are truthful, but the service is also used to spread misinformation and false rumors, often unintentionally.", "num_citations": "2226\n", "authors": ["1177"]}
{"title": "Finding High-Quality Content in Social Media\n", "abstract": " The quality of user-generated content varies drastically from excellent to abuse and spam. As the availability of such content increases, the task of identifying high-quality content sites based on user contributions--social media sites--becomes increasingly important. Social media in general exhibit a rich variety of information sources: in addition to the content itself, there is a wide array of non-content information available, such as links between items and explicit quality ratings from members of the community. In this paper we investigate methods for exploiting such community feedback to automatically identify high quality content. As a test case, we focus on Yahoo! Answers, a large community question/answering portal that is particularly rich in the amount and types of content and social interactions available in it. We introduce a general classification framework for combining the evidence from different sources of\u00a0\u2026", "num_citations": "1829\n", "authors": ["1177"]}
{"title": "Twitter under crisis: can we trust what we RT?\n", "abstract": " In this article we explore the behavior of Twitter users under an emergency situation. In particular, we analyze the activity related to the 2010 earthquake in Chile and characterize Twitter in the hours and days following this disaster. Furthermore, we perform a preliminary study of certain social phenomenons, such as the dissemination of false rumors and confirmed news. We analyze how this information propagated through the Twitter network, with the purpose of assessing the reliability of Twitter as an information source under extreme circumstances. Our analysis shows that the propagation of tweets that correspond to rumors differs from tweets that spread news because rumors tend to be questioned more than news by the Twitter community. This result shows that it is posible to detect rumors by using aggregate analysis on tweets.", "num_citations": "1285\n", "authors": ["1177"]}
{"title": "Processing social media messages in mass emergency: A survey\n", "abstract": " Social media platforms provide active communication channels during mass convergence and emergency events such as disasters caused by natural hazards. As a result, first responders, decision makers, and the public can use this information to gain insight into the situation as it unfolds. In particular, many social media messages communicated during emergencies convey timely, actionable information. Processing social media messages to obtain such information, however, involves solving multiple challenges including: parsing brief and informal messages, handling information overload, and prioritizing different types of information found in messages. These challenges can be mapped to classical information processing operations such as filtering, classifying, ranking, aggregating, extracting, and summarizing. We survey the state of the art regarding computational methods to process social media messages\u00a0\u2026", "num_citations": "761\n", "authors": ["1177"]}
{"title": "AIDR: Artificial Intelligence for Disaster Response\n", "abstract": " We present AIDR (Artificial Intelligence for Disaster Response), a platform designed to perform automatic classification of crisis-related microblog communications. AIDR enables humans and machines to work together to apply human intelligence to large-scale data at high speed. The objective of AIDR is to classify messages that people post during disasters into a set of user-defined categories of information (eg,\" needs\",\" damage\", etc.) For this purpose, the system continuously ingests data from Twitter, processes it (ie, using machine learning classification techniques) and leverages human-participation (through crowdsourcing) in real-time. AIDR has been successfully tested to classify informative vs. non-informative tweets posted during the 2013 Pakistan Earthquake. Overall, we achieved a classification quality (measured using AUC) of 80%. AIDR is available at http://aidr. qcri. org/.", "num_citations": "497\n", "authors": ["1177"]}
{"title": "Know your neighbors: web spam detection using the web topology\n", "abstract": " Web spam can significantly deteriorate the quality of search engine results. Thus there is a large incentive for commercial search engines to detect spam pages efficiently and accurately. In this paper we present a spam detection system that combines link-based and content-based features, and uses the topology of the Web graph by exploiting the link dependencies among the Web pages. We find that linked hosts tend to belong to the same class: either both are spam or both are non-spam. We demonstrate three methods of incorporating the Web graph topology into the predictions obtained by our base classifier:(i) clustering the host graph, and assigning the label of all hosts in the cluster by majority vote,(ii) propagating the predicted labels to neighboring hosts, and (iii) using the predicted labels of neighboring hosts as new features and retraining the classifier. The result is an accurate system for detecting Web\u00a0\u2026", "num_citations": "465\n", "authors": ["1177"]}
{"title": "Extracting information nuggets from disaster-related messages in social media\n", "abstract": " Microblogging sites such as Twitter can play a vital role in spreading information during \u201cnatural\u201d or man-made disasters. But the volume and velocity of tweets posted during crises today tend to be extremely high, making it hard for disaster-affected communities and professional emergency responders to process the information in a timely manner. Furthermore, posts tend to vary highly in terms of their subjects and usefulness; from messages that are entirely off-topic or personal in nature, to messages containing critical information that augments situational awareness. Finding actionable information can accelerate disaster response and alleviate both property and human losses. In this paper, we describe automatic methods for extracting information from microblog posts. Specifically, we focus on extracting valuable \u201cinformation nuggets\u201d, brief, self-contained information items relevant to disaster response. Our methods leverage machine learning methods for classifying posts and information extraction. Our results, validated over one large disaster-related dataset, reveal that a careful design can yield an effective system, paving the way for more sophisticated data analysis and visualization systems.", "num_citations": "441\n", "authors": ["1177"]}
{"title": "What to Expect When the Unexpected Happens: Social Media Communications Across Crises\n", "abstract": " The use of social media to communicate timely information during crisis situations has become a common practice in recent years. In particular, the one-to-many nature of Twitter has created an opportunity for stakeholders to disseminate crisis-relevant messages, and to access vast amounts of information they may not otherwise have. Our goal is to understand what affected populations, response agencies and other stakeholders can expect-and not expect-from these data in various types of disaster situations. Anecdotal evidence suggests that different types of crises elicit different reactions from Twitter users, but we have yet to see whether this is in fact the case. In this paper, we investigate several crises-including natural hazards and human-induced disasters-in a systematic manner and with a consistent methodology. This leads to insights about the prevalence of different information types and sources across a\u00a0\u2026", "num_citations": "428\n", "authors": ["1177"]}
{"title": "The query-flow graph: model and applications\n", "abstract": " Query logs record the queries and the actions of the users of search engines, and as such they contain valuable information about the interests, the preferences, and the behavior of the users, as well as their implicit feedback to search engine results. Mining the wealth of information available in the query logs has many important applications including query-log analysis, user profiling and personalization, advertising, query recommendation, and more.", "num_citations": "408\n", "authors": ["1177"]}
{"title": "Information and influence propagation in social networks\n", "abstract": " Download Free Sample   Research on social networks has exploded over the last decade. To a large extent, this has been fueled by the spectacular growth of social media and online social networking sites, which continue growing at a very fast pace, as well as by the increasing availability of very large social network datasets for purposes of  research. A rich body of this research has been devoted to the analysis of the propagation of  information, influence, innovations, infections, practices and customs through networks. Can  we build models to explain the way these propagations occur? How can we validate our models  against any available real datasets consisting of a social network and propagation  traces that occurred in the past? These are just some questions  studied by researchers in this area. Information propagation models find applications in  viral marketing, outbreak detection, finding key blog posts\u00a0\u2026", "num_citations": "394\n", "authors": ["1177"]}
{"title": "Social network analysis and mining for business applications\n", "abstract": " Social network analysis has gained significant attention in recent years, largely due to the success of online social networking and media-sharing sites, and the consequent availability of a wealth of social network data. In spite of the growing interest, however, there is little understanding of the potential business applications of mining social networks. While there is a large body of research on different problems and methods for social network mining, there is a gap between the techniques developed by the research community and their deployment in real-world applications. Therefore the potential business impact of these techniques is still largely unexplored. In this article we use a business process classification framework to put the research topics in a business context and provide an overview of what we consider key problems and techniques in social network analysis and mining from the perspective of\u00a0\u2026", "num_citations": "380\n", "authors": ["1177"]}
{"title": "Predicting information credibility in time-sensitive social media\n", "abstract": " Purpose                \u2013 Twitter is a popular microblogging service which has proven, in recent years, its potential for propagating news and information about developing events. The purpose of this paper is to focus on the analysis of information credibility on Twitter. The purpose of our research is to establish if an automatic discovery process of relevant and credible news events can be achieved.                                        Design/methodology/approach                \u2013 The paper follows a supervised learning approach for the task of automatic classification of credible news events. A first classifier decides if an information cascade corresponds to a newsworthy event. Then a second classifier decides if this cascade can be considered credible or not. The paper undertakes this effort training over a significant amount of labeled data, obtained using crowdsourcing tools. The paper validates these classifiers under two settings\u00a0\u2026", "num_citations": "370\n", "authors": ["1177"]}
{"title": "Tweetcred: Real-time credibility assessment of content on Twitter\n", "abstract": " During sudden onset crisis events, the presence of spam, rumors and fake content on Twitter reduces the value of information contained on its messages (or \u201ctweets\u201d). A possible solution to this problem is to use machine learning to automatically evaluate the credibility of a tweet, i.e. whether a person would deem the tweet believable or trustworthy. This has been often framed and studied as a supervised classification problem in an off-line (post-hoc) setting.               In this paper, we present a semi-supervised ranking model for scoring tweets according to their credibility. This model is used in TweetCred, a real-time system that assigns a credibility score to tweets in a user\u2019s timeline. TweetCred, available as a browser plug-in, was installed and used by 1,127 Twitter users within a span of three months. During this period, the credibility score for about 5.4 million tweets was computed, allowing us to evaluate\u00a0\u2026", "num_citations": "359\n", "authors": ["1177"]}
{"title": "Correlating financial time series with micro-blogging activity\n", "abstract": " We study the problem of correlating micro-blogging activity with stock-market events, defined as changes in the price and traded volume of stocks. Specifically, we collect messages related to a number of companies, and we search for correlations between stock-market events for those companies and features extracted from the micro-blogging messages. The features we extract can be categorized in two groups. Features in the first group measure the overall activity in the micro-blogging platform, such as number of posts, number of re-posts, and so on. Features in the second group measure properties of an induced interaction graph, for instance, the number of connected components, statistics on the degree distribution, and other graph-based properties.", "num_citations": "354\n", "authors": ["1177"]}
{"title": "Fast shortest path distance estimation in large networks\n", "abstract": " In this paper we study approximate landmark-based methods for point-to-point distance estimation in very large networks. These methods involve selecting a subset of nodes as landmarks and computing offline the distances from each node in the graph to those landmarks. At runtime, when the distance between a pair of nodes is needed, it can be estimated quickly by combining the precomputed distances. We prove that selecting the optimal set of landmarks is an NP-hard problem, and thus heuristic solutions need to be employed. We therefore explore theoretical insights to devise a variety of simple methods that scale well in very large networks. The efficiency of the suggested techniques is tested experimentally using five real-world graphs having millions of edges. While theoretical bounds support the claim that random landmarks work well in practice, our extensive experimentation shows that smart landmark\u00a0\u2026", "num_citations": "347\n", "authors": ["1177"]}
{"title": "Effective web crawling\n", "abstract": " The key factors for the success of the World Wide Web are its large size and the lack of a centralized control over its contents. Both issues are also the most important source of problems for locating information. The Web is a context in which traditional Information Retrieval methods are challenged, and given the volume of the Web and its speed of change, the coverage of modern search engines is relatively small. Moreover, the distribution of quality is very skewed, and interesting pages are scarce in comparison with the rest of the content.", "num_citations": "337\n", "authors": ["1177"]}
{"title": "Efficient semi-streaming algorithms for local triangle counting in massive graphs\n", "abstract": " In this paper we study the problem of local triangle counting in large graphs. Namely, given a large graph G=(V; E) we want to estimate as accurately as possible the number of triangles incident to every node \u03c5\u2208 V in the graph. The problem of computing the global number of triangles in a graph has been considered before, but to our knowledge this is the first paper that addresses the problem of local triangle counting with a focus on the efficiency issues arising in massive graphs. The distribution of the local number of triangles and the related local clustering coefficient can be used in many interesting applications. For example, we show that the measures we compute can help to detect the presence of spamming activity in large-scale Web graphs, as well as to provide useful features to assess content quality in social networks.", "num_citations": "336\n", "authors": ["1177"]}
{"title": "Online team formation in social networks\n", "abstract": " We study the problem of online team formation. We consider a setting in which people possess different skills and compatibility among potential team members is modeled by a social network. A sequence of tasks arrives in an online fashion, and each task requires a specific set of skills. The goal is to form a new team upon arrival of each task, so that (i) each team possesses all skills required by the task,(ii) each team has small communication overhead, and (iii) the workload of performing the tasks is balanced among people in the fairest possible way.", "num_citations": "334\n", "authors": ["1177"]}
{"title": "CrisisLex: A Lexicon for Collecting and Filtering Microblogged Communications in Crises\n", "abstract": " Locating timely, useful information during crises and mass emergencies is critical for those forced to make potentially life-altering decisions. As the use of Twitter to broadcast useful information during such situations becomes more widespread, the problem of finding it becomes more difficult. We describe an approach toward improving the recall in the sampling of Twitter communications that can lead to greater situational awareness during crisis situations. First, we create a lexicon of crisis-related terms that frequently appear in relevant messages posted during different types of crisis situations. Next, we demonstrate how we use the lexicon to automatically identify new terms that describe a given crisis. Finally, we explain how to efficiently query Twitter to extract crisis-related messages during emergency events. In our experiments, using a crisis lexicon leads to substantial improvements in terms of recall when added to a set of crisis-specific keywords manually chosen by experts; it also helps to preserve the original distribution of message types.", "num_citations": "333\n", "authors": ["1177"]}
{"title": "Social Data: Biases, Methodological Pitfalls, and Ethical Boundaries\n", "abstract": " Social data in digital form, including user-generated content, expressed or implicit relations between people, and behavioral traces, are at the core of popular applications and platforms, driving the research agenda of many researchers. The promises of social data are many, including understanding ``what the world thinks'' about a social issue, brand, celebrity, or other entity, as well as enabling better decision-making in a variety of fields including public policy, healthcare, and economics. Many academics and practitioners have warned against the naive usage of social data. There are biases and inaccuracies occurring at the source of the data, but also introduced during processing. There are methodological limitations and pitfalls, as well as ethical boundaries and unexpected consequences that are often overlooked. This paper recognizes the rigor with which these issues are addressed by different researchers varies across a wide range. We identify a variety of menaces in the practices around social data use, and organize them in a framework that helps to identify them.", "num_citations": "301\n", "authors": ["1177"]}