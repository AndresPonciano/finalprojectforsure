{"title": "Moses: Open source toolkit for statistical machine translation\n", "abstract": " We describe an open-source toolkit for statistical machine translation whose novel contributions are (a) support for linguistically motivated factors,(b) confusion network decoding, and (c) efficient data formats for translation models and language models. In addition to the SMT decoder, the toolkit also includes a wide variety of tools for training, tuning and applying the system to many translation tasks.", "num_citations": "6305\n", "authors": ["2070"]}
{"title": "Findings of the 2014 workshop on statistical machine translation\n", "abstract": " This paper presents the results of the WMT14 shared tasks, which included a standard news translation task, a separate medical translation task, a task for run-time estimation of machine translation quality, and a metrics task. This year, 143 machine translation systems from 23 institutions were submitted to the ten translation directions in the standard translation task. An additional 6 anonymized systems were included, and were then evaluated both automatically and manually. The quality estimation task had four subtasks, with a total of 10 teams, submitting 57 entries.", "num_citations": "872\n", "authors": ["2070"]}
{"title": "Re-evaluating the role of BLEU in machine translation research\n", "abstract": " We argue that the machine translation community is overly reliant on the Bleu machine translation evaluation metric. We show that an improved Bleu score is neither necessary nor sufficient for achieving an actual improvement in translation quality, and give two significant counterexamples to Bleu\u2019s correlation with human judgments of quality. This offers new potential for research which was previously deemed unpromising by an inability to improve upon Bleu scores.", "num_citations": "802\n", "authors": ["2070"]}
{"title": "Findings of the 2009 workshop on statistical machine translation\n", "abstract": " This paper presents the results of the WMT11 shared tasks, which included a translation task, a system combination task, and a task for machine translation evaluation metrics. We conducted a large-scale manual evaluation of 148 machine translation systems and 41 system combination entries. We used the ranking of these systems to measure how strongly automatic metrics correlate with human judgments of translation quality for 21 evaluation metrics. This year featured a Haitian Creole to English task translating SMS messages sent to an emergency response service in the aftermath of the Haitian earthquake. We also conducted a pilot \u2018tunable metrics\u2019 task to test whether optimizing a fixed system to different metrics would result in perceptibly different translation quality.", "num_citations": "786\n", "authors": ["2070"]}
{"title": "PPDB: The paraphrase database\n", "abstract": " We present the 1.0 release of our paraphrase database, PPDB. Its English portion, PPDB: Eng, contains over 220 million paraphrase pairs, consisting of 73 million phrasal and 8 million lexical paraphrases, as well as 140 million paraphrase patterns, which capture many meaning-preserving syntactic transformations. The paraphrases are extracted from bilingual parallel corpora totaling over 100 million sentence pairs and over 2 billion English words. We also release PPDB: Spa, a collection of 196 million Spanish paraphrases. Each paraphrase pair in PPDB contains a set of associated scores, including paraphrase probabilities derived from the bitext data and a variety of monolingual distributional similarity scores computed from the Google n-grams and the Annotated Gigaword corpus. Our release includes pruning tools that allow users to determine their own precision/recall tradeoff.", "num_citations": "766\n", "authors": ["2070"]}
{"title": "Paraphrasing with bilingual parallel corpora\n", "abstract": " Previous work has used monolingual parallel corpora to extract and generate paraphrases. We show that this task can be done using bilingual parallel corpora, a much more commonly available resource. Using alignment techniques from phrasebased statistical machine translation, we show how paraphrases in one language can be identified using a phrase in another language as a pivot. We define a paraphrase probability that allows paraphrases extracted from a bilingual parallel corpus to be ranked using translation probabilities, and show how it can be refined to take contextual information into account. We evaluate our paraphrase extraction and ranking methods using a set of manual word alignments, and contrast the quality with paraphrases extracted from automatic alignments.", "num_citations": "712\n", "authors": ["2070"]}
{"title": "Method and apparatus for providing multilingual translation over a network\n", "abstract": " A method for electronically translating text provides an electronic language translator. Source language text is received as an input to the electronic language translator. The source language text is translated at the electronic language translator at the time of submission into one or more target language texts. A user is then provided with an option of viewing one or more of the target language texts with or without the source language texts.", "num_citations": "633\n", "authors": ["2070"]}
{"title": "Fast, cheap, and creative: Evaluating translation quality using Amazon\u2019s Mechanical Turk\n", "abstract": " Manual evaluation of translation quality is generally thought to be excessively time consuming and expensive. We explore a fast and inexpensive way of doing it using Amazon\u2019s Mechanical Turk to pay small sums to a large number of non-expert annotators. For $10 we redundantly recreate judgments from a WMT08 translation task. We find that when combined non-expert judgments have a high-level of agreement with the existing gold-standard judgments of machine translation quality, and correlate more strongly with expert judgments than Bleu does. We go on to show that Mechanical Turk can be used to calculate human-mediated translation edit rate (HTER), to conduct reading comprehension experiments with machine translation, and to create high quality reference translations.", "num_citations": "592\n", "authors": ["2070"]}
{"title": "Crowdsourcing translation: professional quality from non-professionals\n", "abstract": " Naively collecting translations by crowdsourcing the task to non-professional translators yields disfluent, low-quality results if no quality control is exercised. We demonstrate a variety of mechanisms that increase the translation quality to near professional levels. Specifically, we solicit redundant translations and edits to them, and automatically select the best output among them. We propose a set of features that model both the translations and the translators, such as country of residence, LM perplexity of the translation, edit rate from the other translations, and (optionally) calibration against professional translators. Using these features to score the collected translations, we are able to discriminate between acceptable and unacceptable translations. We recreate the NIST 2009 Urdu-to-English evaluation set with Mechanical Turk, and quantitatively show that our models are able to select translations within the range of quality that we expect from professional translators. The total cost is more than an order of magnitude lower than professional translation.", "num_citations": "467\n", "authors": ["2070"]}
{"title": "Edinburgh system description for the 2005 IWSLT speech translation evaluation\n", "abstract": " Our participation in the IWSLT 2005 speech translation task is our first effort to work on limited domain speech data. We adapted our statistical machine translation system that performed successfully in previous DARPA competitions on open domain text translations. We participated in the supplied corpora transcription track. We achieved the highest BLEU score in 2 out of 5 language pairs and had competitive results for the other language pairs.", "num_citations": "398\n", "authors": ["2070"]}
{"title": "Creating speech and language data with amazon\u2019s mechanical turk\n", "abstract": " In this paper we give an introduction to using Amazon\u2019s Mechanical Turk crowdsourcing platform for the purpose of collecting data for human language technologies. We survey the papers published in the NAACL-2010 Workshop. 24 researchers participated in the workshop\u2019s shared task to create data for speech and language applications with $100.", "num_citations": "397\n", "authors": ["2070"]}
{"title": "Improved statistical machine translation using paraphrases\n", "abstract": " Parallel corpora are crucial for training SMT systems. However, for many language pairs they are available only in very limited quantities. For these language pairs a huge portion of phrases encountered at run-time will be unknown. We show how techniques from paraphrasing can be used to deal with these otherwise unknown source language phrases. Our results show that augmenting a stateof-the-art SMT system with paraphrases leads to significantly improved coverage and translation quality. For a training corpus with 10,000 sentence pairs we increase the coverage of unique test set unigrams from 48% to 90%, with more than half of the newly covered items accurately translated, as opposed to none in current approaches.", "num_citations": "369\n", "authors": ["2070"]}
{"title": "A data-driven analysis of workers' earnings on Amazon Mechanical Turk\n", "abstract": " A growing number of people are working as part of on-line crowd work. Crowd work is often thought to be low wage work. However, we know little about the wage distribution in practice and what causes low/high earnings in this setting. We recorded 2,676 workers performing 3.8 million tasks on Amazon Mechanical Turk. Our task-level analysis revealed that workers earned a median hourly wage of only~ $2/h, and only 4% earned more than $7.25/h. While the average requester pays more than $11/h, lower-paying requesters post much more work. Our wage calculations are influenced by how unpaid work is accounted for, eg, time spent searching for tasks, working on tasks that are rejected, and working on tasks that are ultimately not submitted. We further explore the characteristics of tasks and working patterns that yield higher hourly wages. Our analysis informs platform design and worker tools to create a more\u00a0\u2026", "num_citations": "359\n", "authors": ["2070"]}
{"title": "Arabic dialect identification\n", "abstract": " The written form of the Arabic language, Modern Standard Arabic (MSA), differs in a non-trivial manner from the various spoken regional dialects of Arabic\u2014the true \u201cnative languages\u201d of Arabic speakers. Those dialects, in turn, differ quite a bit from each other. However, due to MSA's prevalence in written form, almost all Arabic data sets have predominantly MSA content. In this article, we describe the creation of a novel Arabic resource with dialect annotations. We have created a large monolingual data set rich in dialectal Arabic content called the Arabic On-line Commentary Data set (Zaidan and Callison-Burch 2011). We describe our annotation effort to identify the dialect level (and dialect itself) in each of more than 100,000 sentences from the data set by crowdsourcing the annotation task, and delve into interesting annotator behaviors (like over-identification of one's own dialect). Using this new annotated\u00a0\u2026", "num_citations": "304\n", "authors": ["2070"]}
{"title": "Optimizing statistical machine translation for text simplification\n", "abstract": " Most recent sentence simplification systems use basic machine translation models                     to learn lexical and syntactic paraphrases from a manually simplified parallel                     corpus. These methods are limited by the quality and quantity of manually                     simplified corpora, which are expensive to build. In this paper, we conduct an                     in-depth adaptation of statistical machine translation to perform text                     simplification, taking advantage of large-scale paraphrases learned from                     bilingual texts and a small amount of manual simplifications with multiple                     references. Our work is the first to design automatic metrics that are effective                     for tuning and evaluating simplification systems, which will facilitate                     iterative development for this task.", "num_citations": "297\n", "authors": ["2070"]}
{"title": "Further meta-evaluation of machine translation\n", "abstract": " This paper analyzes the translation quality of machine translation systems for 10 language pairs translating between Czech, English, French, German, Hungarian, and Spanish. We report the translation quality of over 30 diverse translation systems based on a large-scale manual evaluation involving hundreds of hours of effort. We use the human judgments of the systems to analyze automatic evaluation metrics for translation quality, and we report the strength of the correlation with human judgments at both the system-level and at the sentence-level. We validate our manual evaluation methodology by measuring intra-and inter-annotator agreement, and collecting timing information.", "num_citations": "288\n", "authors": ["2070"]}
{"title": "PPDB 2.0: Better paraphrase ranking, fine-grained entailment relations, word embeddings, and style classification\n", "abstract": " We present a new release of the Paraphrase Database. PPDB 2.0 includes a discriminatively re-ranked set of paraphrases that achieve a higher correlation with human judgments than PPDB 1.0\u2019s heuristic rankings. Each paraphrase pair in the database now also includes finegrained entailment relations, word embedding similarities, and style annotations.", "num_citations": "285\n", "authors": ["2070"]}
{"title": "Problems in current text simplification research: New data can help\n", "abstract": " Simple Wikipedia has dominated simplification research in the past 5 years. In                     this opinion paper, we argue that focusing on Wikipedia limits simplification                     research. We back up our arguments with corpus analysis and by highlighting                     statements that other researchers have made in the simplification literature. We                     introduce a new simplification dataset that is a significant improvement over                     Simple Wikipedia, and present a novel quantitative-comparative approach to study                     the quality of simplification data resources.", "num_citations": "260\n", "authors": ["2070"]}
{"title": "Findings of the 2010 joint workshop on statistical machine translation and metrics for machine translation\n", "abstract": " This paper presents the results of the WMT10 and MetricsMATR10 shared tasks, 1 which included a translation task, a system combination task, and an evaluation task. We conducted a large-scale manual evaluation of 104 machine translation systems and 41 system combination entries. We used the ranking of these systems to measure how strongly automatic metrics correlate with human judgments of translation quality for 26 metrics. This year we also investigated increasing the number of human judgments by hiring non-expert annotators through Amazon\u2019s Mechanical Turk.", "num_citations": "244\n", "authors": ["2070"]}
{"title": "Joshua: An open source toolkit for parsing-based machine translation\n", "abstract": " We describe Joshua, an open source toolkit for statistical machine translation. Joshua implements all of the algorithms required for synchronous context free grammars (SCFGs): chart-parsing, ngram language model integration, beamand cube-pruning, and k-best extraction. The toolkit also implements suffix-array grammar extraction and minimum error rate training. It uses parallel and distributed computing techniques for scalability. We demonstrate that the toolkit achieves state of the art translation performance on the WMT09 French-English translation task.", "num_citations": "225\n", "authors": ["2070"]}
{"title": "Answer extraction as sequence tagging with tree edit distance\n", "abstract": " Our goal is to extract answers from preretrieved sentences for Question Answering (QA). We construct a linear-chain Conditional Random Field based on pairs of questions and their possible answer sentences, learning the association between questions and answer types. This casts answer extraction as an answer sequence tagging problem for the first time, where knowledge of shared structure between question and source sentence is incorporated through features based on Tree Edit Distance (TED). Our model is free of manually created question and answer templates, fast to run (processing 200 QA pairs per second excluding parsing time), and yields an F1 of 63.3% on a new public dataset based on prior TREC QA evaluations. The developed system is open-source, and includes an implementation of the TED model that is state of the art in the task of ranking QA pairs.", "num_citations": "221\n", "authors": ["2070"]}
{"title": "Syntactic constraints on paraphrases extracted from parallel corpora\n", "abstract": " We improve the quality of paraphrases extracted from parallel corpora by requiring that phrases and their paraphrases be the same syntactic type. This is achieved by parsing the English side of a parallel corpus and altering the phrase extraction algorithm to extract phrase labels alongside bilingual phrase pairs. In order to retain broad coverage of non-constituent phrases, complex syntactic labels are introduced. A manual evaluation indicates a 19% absolute improvement in paraphrase quality over the baseline method.", "num_citations": "218\n", "authors": ["2070"]}
{"title": "Cheap, fast and good enough: Automatic speech recognition with non-expert transcription\n", "abstract": " Deploying an automatic speech recognition system with reasonable performance requires expensive and time-consuming in-domain transcription. Previous work demonstrated that non-professional annotation through Amazon\u2019s Mechanical Turk can match professional quality. We use Mechanical Turk to transcribe conversational speech for as little as one thirtieth the cost of professional transcription. The higher disagreement of nonprofessional transcribers does not have a significant effect on system performance. While previous work demonstrated that redundant transcription can improve data quality, we found that resources are better spent collecting more data. Finally, we describe a quality control method without needing professional transcription.", "num_citations": "191\n", "authors": ["2070"]}
{"title": "Improved statistical machine translation using monolingually-derived paraphrases\n", "abstract": " Untranslated words still constitute a major problem for Statistical Machine Translation (SMT), and current SMT systems are limited by the quantity of parallel training texts. Augmenting the training data with paraphrases generated by pivoting through other languages alleviates this problem, especially for the so-called \u201clow density\u201d languages. But pivoting requires additional parallel texts. We address this problem by deriving paraphrases monolingually, using distributional semantic similarity measures, thus providing access to larger training resources, such as comparable and unrelated monolingual corpora. We present what is to our knowledge the first successful integration of a collocational approach to untranslated words with an end-to-end, state of the art SMT system demonstrating significant translation improvements in a low-resource setting.", "num_citations": "189\n", "authors": ["2070"]}
{"title": "Machine translation of Arabic dialects\n", "abstract": " Arabic Dialects present many challenges for machine translation, not least of which is the lack of data resources. We use crowdsourcing to cheaply and quickly build Levantine-English and Egyptian-English parallel corpora, consisting of 1.1 M words and 380k words, respectively. The dialectal sentences are selected from a large corpus of Arabic web text, and translated using Amazon\u2019s Mechanical Turk. We use this data to build Dialectal Arabic MT systems, and find that small amounts of dialectal data have a dramatic impact on translation quality. When translating Egyptian and Levantine test sets, our Dialectal Arabic MT system performs 6.3 and 7.0 BLEU points higher than a Modern Standard Arabic MT system trained on a 150M-word Arabic-English parallel corpus.", "num_citations": "173\n", "authors": ["2070"]}
{"title": "The arabic online commentary dataset: an annotated dataset of informal arabic with high dialectal content\n", "abstract": " The written form of Arabic, Modern Standard Arabic (MSA), differs quite a bit from the spoken dialects of Arabic, which are the true \u201cnative\u201d languages of Arabic speakers used in daily life. However, due to MSA\u2019s prevalence in written form, almost all Arabic datasets have predominantly MSA content. We present the Arabic Online Commentary Dataset, a 52M-word monolingual dataset rich in dialectal content, and we describe our long-term annotation effort to identify the dialect level (and dialect itself) in each sentence of the dataset. So far, we have labeled 108K sentences, 41% of which as having dialectal content. We also present experimental results on the task of automatic dialect identification, using the collected labels for training and evaluation.", "num_citations": "169\n", "authors": ["2070"]}
{"title": "(Meta-) Evaluation of Machine Translation\n", "abstract": " This paper evaluates the translation quality of machine translation systems for 8 language pairs: translating French, German, Spanish, and Czech to English and back. We carried out an extensive human evaluation which allowed us not only to rank the different MT systems, but also to perform higher-level analysis of the evaluation process. We measured timing and intra-and inter-annotator agreement for three types of subjective evaluation. We measured the correlation of automatic evaluation metrics with human judgments. This meta-evaluation reveals surprising facts about the most commonly used methodologies.", "num_citations": "167\n", "authors": ["2070"]}
{"title": "Statistical machine translation with word-and sentence-aligned parallel corpora\n", "abstract": " The parameters of statistical translation models are typically estimated from sentence-aligned parallel corpora. We show that significant improvements in the alignment and translation quality of such models can be achieved by additionally including wordaligned data during training. Incorporating wordlevel alignments into the parameter estimation of the IBM models reduces alignment error rate and increases the Bleu score when compared to training the same models only on sentence-aligned data. On the Verbmobil data set, we attain a 38% reduction in the alignment error rate and a higher Bleu score with half as many training examples. We discuss how varying the ratio of word-aligned to sentencealigned data affects the expected performance gain.", "num_citations": "159\n", "authors": ["2070"]}
{"title": "Dirt cheap web-scale parallel text from the common crawl\n", "abstract": " Parallel text is the fuel that drives modern machine translation systems. The Web is a comprehensive source of preexisting parallel text, but crawling the entire web is impossible for all but the largest companies. We bring web-scale parallel text to the masses by mining the Common Crawl, a public Web crawl hosted on Amazon\u2019s Elastic Cloud. Starting from nothing more than a set of common two-letter language codes, our open-source extension of the STRAND algorithm mined 32 terabytes of the crawl in just under a day, at a cost of about $500. Our large-scale experiment uncovers large amounts of parallel text in dozens of language pairs across a variety of domains and genres, some previously unavailable in curated datasets. Even with minimal cleaning and filtering, the resulting data boosts translation performance across the board for five different language pairs in the news domain, and on open domain test sets we see improvements of up to 5 BLEU. We make our code and data available for other researchers seeking to mine this rich new data resource. 1", "num_citations": "145\n", "authors": ["2070"]}
{"title": "Constructing parallel corpora for six indian languages via crowdsourcing\n", "abstract": " Recent work has established the efficacy of Amazon\u2019s Mechanical Turk for constructing parallel corpora for machine translation research. We apply this to building a collection of parallel corpora between English and six languages from the Indian subcontinent: Bengali, Hindi, Malayalam, Tamil, Telugu, and Urdu. These languages are low-resource, under-studied, and exhibit linguistic phenomena that are difficult for machine translation. We conduct a variety of baseline experiments and analysis, and release the data to the community.", "num_citations": "142\n", "authors": ["2070"]}
{"title": "Semeval-2015 task 1: Paraphrase and semantic similarity in twitter (pit)\n", "abstract": " In this shared task, we present evaluations on two related tasks Paraphrase Identification (PI) and Semantic Textual Similarity (SS) systems for the Twitter data. Given a pair of sentences, participants are asked to produce a binary yes/no judgement or a graded score to measure their semantic equivalence. The task features a newly constructed Twitter Paraphrase Corpus that contains 18,762 sentence pairs. A total of 19 teams participated, submitting 36 runs to the PI task and 26 runs to the SS task. The evaluation shows encouraging results and open challenges for future research. The best systems scored a F1-measure of 0.674 for the PI task and a Pearson correlation of 0.619 for the SS task respectively, comparing to a strong baseline using logistic regression model of 0.589 F1 and 0.511 Pearson; while the best SS systems can often reach> 0.80 Pearson on well-formed text. This shared task also provides insights into the relation between the PI and SS tasks and suggests the importance to bringing these two research areas together. We make all the data, baseline systems and evaluation scripts publicly available. 1", "num_citations": "132\n", "authors": ["2070"]}
{"title": "Scaling phrase-based statistical machine translation to larger corpora and longer phrases\n", "abstract": " In this paper we describe a novel data structure for phrase-based statistical machine translation which allows for the retrieval of arbitrarily long phrases while simultaneously using less memory than is required by current decoder implementations. We detail the computational complexity and average retrieval times for looking up phrase translations in our suffix array-based data structure. We show how sampling can be used to reduce the retrieval time by orders of magnitude with no loss in translation quality.", "num_citations": "115\n", "authors": ["2070"]}
{"title": "Extracting lexically divergent paraphrases from Twitter\n", "abstract": " We present MultiP (Multi-instance Learning Paraphrase Model), a new                     model suited to identify paraphrases within the short messages on Twitter. We                     jointly model paraphrase relations between word and sentence pairs and assume                     only sentence-level annotations during learning. Using this principled latent                     variable model alone, we achieve the performance competitive with a                     state-of-the-art method which combines a latent space model with a feature-based                     supervised classifier. Our model also captures lexically divergent paraphrases                     that differ from yet complement previous methods; combining our model with                     previous work significantly outperforms the state-of-the-art. In addition, we                     present a novel annotation methodology that has allowed us to crowdsource a                     paraphrase corpus from Twitter. We\u00a0\u2026", "num_citations": "112\n", "authors": ["2070"]}
{"title": "A Multi-Dialect, Multi-Genre Corpus of Informal Written Arabic.\n", "abstract": " This paper presents a multi-dialect, multi-genre, human annotated corpus of dialectal Arabic. We collected utterances in five Arabic dialects: Levantine, Gulf, Egyptian, Iraqi and Maghrebi. We scraped newspaper websites for user commentary and Twitter for two distinct types of dialectal content. To the best of the authors\u2019 knowledge, this work is the most diverse corpus of dialectal Arabic in both the source of the content and the number of dialects. Every utterance in the corpus was human annotated on Amazon\u2019s Mechanical Turk; this stands in contrast to Al-Sabbagh and Girju (2012) where only a small subset was human annotated in order to train a classifier to automatically annotate the remainder of the corpus. We provide a discussion of the methodology used for the annotation in addition to the performance of the individual workers. We extend the Arabic dialect identification task to the Iraqi and Maghrebi dialects and improve the results of Zaidan and Callison-Burch (2011a) on Levantine, Gulf and Egyptian.", "num_citations": "96\n", "authors": ["2070"]}
{"title": "Constructing corpora for the development and evaluation of paraphrase systems\n", "abstract": " Automatic paraphrasing is an important component in many natural language processing tasks. In this article we present a new parallel corpus with paraphrase annotations. We adopt a definition of paraphrase based on word alignments and show that it yields high inter-annotator agreement. As Kappa is suited to nominal data, we employ an alternative agreement statistic which is appropriate for structured alignment tasks. We discuss how the corpus can be usefully employed in evaluating paraphrase systems automatically (e.g., by measuring precision, recall, and F1) and also in developing linguistically rich paraphrase models based on syntactic structure.", "num_citations": "94\n", "authors": ["2070"]}
{"title": "Improved speech-to-text translation with the Fisher and Callhome Spanish\u2013English speech translation corpus\n", "abstract": " Research into the translation of the output of automatic speech recognition (ASR) systems is hindered by the dearth of datasets developed for that explicit purpose. For Spanish-English translation, in particular, most parallel data available exists only in vastly different domains and registers. In order to support research on cross-lingual speech applications, we introduce the Fisher and Callhome Spanish-English Speech Translation Corpus, supplementing existing LDC audio and transcripts with (a) ASR 1-best, lattice, and oracle output produced by the Kaldi recognition system and (b) English translations obtained on Amazon\u2019s Mechanical Turk. The result is a four-way parallel dataset of Spanish audio, transcriptions, ASR lattices, and English translations of approximately 38 hours of speech, with defined training, development, and held-out test sets.We conduct baseline machine translation experiments using models trained on the provided training data, and validate the dataset by corroborating a number of known results in the field, including the utility of in-domain (information, conversational) training data, increased performance translating lattices (instead of recognizer 1-best output), and the relationship between word error rate and BLEU score.", "num_citations": "92\n", "authors": ["2070"]}
{"title": "Paraphrasing and translation\n", "abstract": " ResultsMeaning and Condition Grammaticality Meaning automatic alignments 49% 55%+ language model 55% 65%+ multiple corpora 57% 65%+ word sense disambiguation 62% 70% manual alignments 75% 85%", "num_citations": "89\n", "authors": ["2070"]}
{"title": "Combining bilingual and comparable corpora for low resource machine translation\n", "abstract": " Statistical machine translation (SMT) performance suffers when models are trained on only small amounts of parallel data. The learned models typically have both low accuracy (incorrect translations and feature scores) and low coverage (high out-of-vocabulary rates). In this work, we use an additional data resource, comparable corpora, to improve both. Beginning with a small bitext and corresponding phrase-based SMT model, we improve coverage by using bilingual lexicon induction techniques to learn new translations from comparable corpora. Then, we supplement the model\u2019s feature space with translation scores estimated over comparable corpora in order to improve accuracy. We observe improvements between 0.5 and 1.7 BLEU translating Tamil, Telugu, Bengali, Malayalam, Hindi, and Urdu into English.", "num_citations": "83\n", "authors": ["2070"]}
{"title": "Stream-based translation models for statistical machine translation\n", "abstract": " Typical statistical machine translation systems are trained with static parallel corpora. Here we account for scenarios with a continuous incoming stream of parallel training data. Such scenarios include daily governmental proceedings, sustained output from translation agencies, or crowd-sourced translations. We show incorporating recent sentence pairs from the stream improves performance compared with a static baseline. Since frequent batch retraining is computationally demanding we introduce a fast incremental alternative using an online version of the EM algorithm. To bound our memory requirements we use a novel data-structure and associated training regime. When compared to frequent batch retraining, our online time and space-bounded model achieves the same performance with significantly less computational overhead.", "num_citations": "78\n", "authors": ["2070"]}
{"title": "The language demographics of amazon mechanical turk\n", "abstract": " We present a large scale study of the languages spoken by bilingual workers on                     Mechanical Turk (MTurk). We establish a methodology for determining the language                     skills of anonymous crowd workers that is more robust than simple surveying. We                     validate workers\u2019 self-reported language skill claims by measuring their ability                     to correctly translate words, and by geolocating workers to see if they reside                     in countries where the languages are likely to be spoken. Rather than posting a                     one-off survey, we posted paid tasks consisting of 1,000 assignments to                     translate a total of 10,000 words in each of 100 languages. Our study ran for                     several months, and was highly visible on the MTurk crowdsourcing platform,                     increasing the chances that bilingual workers would complete it. Our study was                     useful both to\u00a0\u2026", "num_citations": "77\n", "authors": ["2070"]}
{"title": "A program for automatically selecting the best output from multiple machine translation engines\n", "abstract": " This paper describes a program that automatically selects the best translation from a set of translations produced by multiple commercial machine translation engines. The program is simplified by assuming that the most fluent item in the set is the best translation. Fluency is determined using a trigram language model. Results are provided illustrating how well the program performs for human ranked data as compared to each of its constituent engines.", "num_citations": "77\n", "authors": ["2070"]}
{"title": "Learning Sentential Paraphrases from Bilingual Parallel Corpora for Text-to-Text Generation\n", "abstract": " Previous work has shown that high quality phrasal paraphrases can be extracted from bilingual parallel corpora. However, it is not clear whether bitexts are an appropriate resource for extracting more sophisticated sentential paraphrases, which are more obviously learnable from monolingual parallel corpora. We extend bilingual paraphrase extraction to syntactic paraphrases and demonstrate its ability to learn a variety of general paraphrastic transformations, including passivization, dative shift, and topicalization. We discuss how our model can be adapted to many text generation tasks by augmenting its feature set, development data, and parameter estimation routine. We illustrate this adaptation by using our paraphrase model for the task of sentence compression and achieve results competitive with state-of-the-art compression systems.", "num_citations": "75\n", "authors": ["2070"]}
{"title": "Improving translation lexicon induction from monolingual corpora via dependency contexts and part-of-speech equivalences\n", "abstract": " This paper presents novel improvements to the induction of translation lexicons from monolingual corpora using multilingual dependency parses. We introduce a dependency-based context model that incorporates long-range dependencies, variable context sizes, and reordering. It provides a 16% relative improvement over the baseline approach that uses a fixed context window of adjacent words. Its Top 10 accuracy for noun translation is higher than that of a statistical translation model trained on a Spanish-English parallel corpus containing 100,000 sentence pairs. We generalize the evaluation to other word-types, and show that the performance can be increased to 18% relative by preserving part-of-speech equivalencies during translation. 1 Introduction Recent trends in machine translation illustrate that highly accurate word and phrase translations can be learned automatically given enough parallel training data (Koehn et al., 2003; Chiang, 2007). However, large parallel corpora exist for only a small fraction of the world\u2019s languages, leading to a bottleneck for building translation systems in low-density languages such as Swahili, Uzbek or Punjabi. While parallel training data is uncommon for such languages, more readily available resources include small translation dictionaries, comparable corpora, and large amounts of monolingual data. The marked difference in the availability of monolingual vs parallel corpora has led several researchers to develop methods for automatically learning bilingual lexicons, either by using monolingual corpora (Rapp, 1999; Koehn and Knight, 2002; Schafer and Yarowsky, 2002; Haghighi et al., 2008) or by\u00a0\u2026", "num_citations": "75\n", "authors": ["2070"]}
{"title": "Simple PPDB: A paraphrase database for simplification\n", "abstract": " We release the Simple Paraphrase Database, a subset of of the Paraphrase Database (PPDB) adapted for the task of text simplification. We train a supervised model to associate simplification scores with each phrase pair, producing rankings competitive with state-of-theart lexical simplification models. Our new simplification database contains 4.5 million paraphrase rules, making it the largest available resource for lexical simplification.", "num_citations": "74\n", "authors": ["2070"]}
{"title": "The Multilingual Paraphrase Database.\n", "abstract": " We release a massive expansion of the paraphrase database (PPDB) that now includes a collection of paraphrases in 23 different languages. The resource is derived from large volumes of bilingual parallel data. Our collection is extracted and ranked using state of the art methods. The multilingual PPDB has over a billion paraphrase pairs in total, covering the following languages: Arabic, Bulgarian, Chinese, Czech, Dutch, Estonian, Finnish, French, German, Greek, Hungarian, Italian, Latvian, Lithuanian, Polish, Portugese, Romanian, Russian, Slovak, Slovenian, and Swedish.", "num_citations": "74\n", "authors": ["2070"]}
{"title": "Bucking the trend: Large-scale cost-focused active learning for statistical machine translation\n", "abstract": " We explore how to improve machine translation systems by adding more translation data in situations where we already have substantial resources. The main challenge is how to buck the trend of diminishing returns that is commonly encountered. We present an active learning-style data solicitation algorithm to meet this challenge. We test it, gathering annotations via Amazon Mechanical Turk, and find that we get an order of magnitude increase in performance rates of improvement.", "num_citations": "71\n", "authors": ["2070"]}
{"title": "Supervised bilingual lexicon induction with multiple monolingual signals\n", "abstract": " Prior research into learning translations from source and target language monolingual texts has treated the task as an unsupervised learning problem. Although many techniques take advantage of a seed bilingual lexicon, this work is the first to use that data for supervised learning to combine a diverse set of signals derived from a pair of monolingual corpora into a single discriminative model. Even in a low resource machine translation setting, where induced translations have the potential to improve performance substantially, it is reasonable to assume access to some amount of data to perform this kind of optimization. Our work shows that only a few hundred translation pairs are needed to achieve strong performance on the bilingual lexicon induction task, and our approach yields an average relative gain in accuracy of nearly 50% over an unsupervised baseline. Large gains in accuracy hold for all 22 languages (low and high resource) that we investigate.", "num_citations": "71\n", "authors": ["2070"]}
{"title": "Evaluating sentence compression: Pitfalls and suggested remedies\n", "abstract": " This work surveys existing evaluation methodologies for the task of sentence compression, identifies their shortcomings, and proposes alternatives. In particular, we examine the problems of evaluating paraphrastic compression and comparing the output of different models. We demonstrate that compression rate is a strong predictor of compression quality and that perceived improvement over other models is often a side effect of producing longer output.", "num_citations": "64\n", "authors": ["2070"]}
{"title": "Adding semantics to data-driven paraphrasing\n", "abstract": " We add an interpretable semantics to the paraphrase database (PPDB). To date, the relationship between phrase pairs in the database has been weakly defined as approximately equivalent. We show that these pairs represent a variety of relations, including directed entailment (little girl/girl) and exclusion (nobody/someone). We automatically assign semantic entailment relations to entries in PPDB using features derived from past work on discovering inference rules from text and semantic taxonomy induction. We demonstrate that our model assigns these relations with high accuracy. In a downstream RTE task, our labels rival relations from WordNet and improve the coverage of a proof-based RTE system by 17%.", "num_citations": "63\n", "authors": ["2070"]}
{"title": "Modality and negation in SIMT use of modality and negation in semantically-informed syntactic MT\n", "abstract": " This article describes the resource- and system-building efforts of an 8-week Johns Hopkins University Human Language Technology Center of Excellence Summer Camp for Applied Language Exploration (SCALE-2009) on Semantically Informed Machine Translation (SIMT). We describe a new modality/negation (MN) annotation scheme, the creation of a (publicly available) MN lexicon, and two automated MN taggers that we built using the annotation scheme and lexicon. Our annotation scheme isolates three components of modality and negation: a trigger (a word that conveys modality or negation), a target (an action associated with modality or negation), and a holder (an experiencer of modality). We describe how our MN lexicon was semi-automatically produced and we demonstrate that a structure-based MN tagger results in precision around 86% (depending on genre) for tagging of a standard LDC data\u00a0\u2026", "num_citations": "62\n", "authors": ["2070"]}
{"title": "An algerian arabic-french code-switched corpus\n", "abstract": " Arabic is not just one language, but rather a collection of dialects in addition to Modern Standard Arabic (MSA). While MSA is used in formal situations, dialects are the language of every day life. Until recently, there was very little dialectal Arabic in written form. With the advent of social-media, however, the landscape has changed. We provide the first romanized code-switched Algerian Arabic-French corpus annotated for word-level language id. We review the history and sociological factors that make the linguistic situation in Algerian unique and highlight the value of this corpus to the natural language processing and linguistics communities. To build this corpus, we crawled an Algerian newspaper and extracted the comments from the news story. We discuss the informal nature of the language in the corpus and the challenges it will present. Additionally, we provide a preliminary analysis of the corpus. We then discuss some potential uses of our corpus of interest to the computational linguistics community.", "num_citations": "57\n", "authors": ["2070"]}
{"title": "Constraining the phrase-based, joint probability statistical translation model\n", "abstract": " The Joint Probability Model proposed by Marcu and Wong (2002) provides a probabilistic framework for modeling phrase-based statistical machine transla-tion (SMT). The model\u2019s usefulness is, however, limited by the computational complexity of estimating parameters at the phrase level. We present a method of constraining the search space of the Joint Probability Model based on statistically and linguistically motivated word align-ments. This method reduces the complexity and size of the Joint Model and allows it to display performance superior to the standard phrase-based models for small amounts of training material.", "num_citations": "57\n", "authors": ["2070"]}
{"title": "Using Mechanical Turk to build machine translation evaluation sets\n", "abstract": " Building machine translation (MT) test sets is a relatively expensive task. As MT becomes increasingly desired for more and more language pairs and more and more domains, it becomes necessary to build test sets for each case. In this paper, we investigate using Amazon's Mechanical Turk (MTurk) to make MT test sets cheaply. We find that MTurk can be used to make test sets much cheaper than professionally-produced test sets. More importantly, in experiments with multiple MT systems, we find that the MTurk-produced test sets yield essentially the same conclusions regarding system performance as the professionally-produced test sets yield.", "num_citations": "54\n", "authors": ["2070"]}
{"title": "Framenet+: Fast paraphrastic tripling of framenet\n", "abstract": " We increase the lexical coverage of FrameNet through automatic paraphrasing. We use crowdsourcing to manually filter out bad paraphrases in order to ensure a high-precision resource. Our expanded FrameNet contains an additional 22K lexical units, a 3-fold increase over the current FrameNet, and achieves 40% better coverage when evaluated in a practical setting on New York Times data.", "num_citations": "53\n", "authors": ["2070"]}
{"title": "Seeing things from a different angle: Discovering diverse perspectives about claims\n", "abstract": " One key consequence of the information revolution is a significant increase and a contamination of our information supply. The practice of fact checking won't suffice to eliminate the biases in text data we observe, as the degree of factuality alone does not determine whether biases exist in the spectrum of opinions visible to us. To better understand controversial issues, one needs to view them from a diverse yet comprehensive set of perspectives. For example, there are many ways to respond to a claim such as \"animals should have lawful rights\", and these responses form a spectrum of perspectives, each with a stance relative to this claim and, ideally, with evidence supporting it. Inherently, this is a natural language understanding task, and we propose to address it as such. Specifically, we propose the task of substantiated perspective discovery where, given a claim, a system is expected to discover a diverse set of well-corroborated perspectives that take a stance with respect to the claim. Each perspective should be substantiated by evidence paragraphs which summarize pertinent results and facts. We construct PERSPECTRUM, a dataset of claims, perspectives and evidence, making use of online debate websites to create the initial data collection, and augmenting it using search engines in order to expand and diversify our dataset. We use crowd-sourcing to filter out noise and ensure high-quality data. Our dataset contains 1k claims, accompanied with pools of 10k and 8k perspective sentences and evidence paragraphs, respectively. We provide a thorough analysis of the dataset to highlight key underlying language understanding\u00a0\u2026", "num_citations": "52\n", "authors": ["2070"]}
{"title": "Parametric: An automatic evaluation metric for paraphrasing\n", "abstract": " We present ParaMetric, an automatic evaluation metric for data-driven approaches to paraphrasing. ParaMetric provides an objective measure of quality using a collection of multiple translations whose paraphrases have been manually annotated. ParaMetric calculates precision and recall scores by comparing the paraphrases discovered by automatic paraphrasing techniques against gold standard alignments of words and phrases within equivalent sentences. We report scores for several established paraphrasing techniques.", "num_citations": "51\n", "authors": ["2070"]}
{"title": "The Gun Violence Database: A new task and data set for NLP\n", "abstract": " We argue that NLP researchers are especially well-positioned to contribute to the national discussion about gun violence. Reasoning about the causes and outcomes of gun violence is typically dominated by politics and emotion, and data-driven research on the topic is stymied by a shortage of data and a lack of federal funding. However, data abounds in the form of unstructured text from news articles across the country. This is an ideal application of NLP technologies, such as relation extraction, coreference resolution, and event detection. We introduce a new and growing dataset, the Gun Violence Database, in order to facilitate the adaptation of current NLP technologies to the domain of gun violence, thus enabling better social science research on this important and under-resourced problem.", "num_citations": "50\n", "authors": ["2070"]}
{"title": "Semi-markov phrase-based monolingual alignment\n", "abstract": " We introduce a novel discriminative model for phrase-based monolingual alignment using a semi-Markov CRF. Our model achieves stateof-the-art alignment accuracy on two phrasebased alignment datasets (RTE and paraphrase), while doing significantly better than other strong baselines in both non-identical alignment and phrase-only alignment. Additional experiments highlight the potential benefit of our alignment model to RTE, paraphrase identification and question answering, where even a naive application of our model\u2019s alignment score approaches the state of the art.", "num_citations": "50\n", "authors": ["2070"]}
{"title": "Crowd-workers: Aggregating information across turkers to help them find higher paying work\n", "abstract": " The Mechanical Turk crowdsourcing platform currently fails to provide the most basic piece of information to enable workers to make informed decisions about which tasks to undertake: what is the expected hourly pay? Mechanical Turk advertises a reward amount per assignment, but does not give any indication of how long each assignment will take. We have developed a browser plugin that tracks the length of time it takes to complete a task, and a web service that aggregates the information across many workers. Our web service, crowd-workers. com, allows workers to discovery higher paying work by sorting tasks by estimated hourly rate.", "num_citations": "49\n", "authors": ["2070"]}
{"title": "A lightweight and high performance monolingual word aligner\n", "abstract": " Fast alignment is essential for many natural language tasks. But in the setting of monolingual alignment, previous work has not been able to align more than one sentence pair per second. We describe a discriminatively trained monolingual word aligner that uses a Conditional Random Field to globally decode the best alignment with features drawn from source and target sentences. Using just part-of-speech tags and WordNet as external resources, our aligner gives state-of-the-art result, while being an order-of-magnitude faster than the previous best performing system.", "num_citations": "49\n", "authors": ["2070"]}
{"title": "Automatic detection of generated text is easiest when humans are fooled\n", "abstract": " Recent advancements in neural language modelling make it possible to rapidly generate vast amounts of human-sounding text. The capabilities of humans and automatic discriminators to detect machine-generated text have been a large source of research interest, but humans and machines rely on different cues to make their decisions. Here, we perform careful benchmarking and analysis of three popular sampling-based decoding strategies---top-, nucleus sampling, and untruncated random sampling---and show that improvements in decoding methods have primarily optimized for fooling humans. This comes at the expense of introducing statistical abnormalities that make detection easy for automatic systems. We also show that though both human and automatic detector performance improve with longer excerpt length, even multi-sentence excerpts can fool expert human raters over 30% of the time. Our findings reveal the importance of using both human and automatic detectors to assess the humanness of text generation systems.", "num_citations": "47\n", "authors": ["2070"]}
{"title": "Open source toolkit for statistical machine translation: Factored translation models and confusion network decoding\n", "abstract": " The 2006 Language Engineering Workshop Open Source Toolkit for Statistical Machine Translation had the objective to advance the current state-of-the-art in statistical machine translation through richer input and richer annotation of the training data. The workshop focused on three topics: factored translation models, confusion network decoding, and the development of an open source toolkit that incorporates this advancements. This report describes the scientific goals, the novel methods, and experimental results of the workshop. It also documents details of the implementation of the open source toolkit.", "num_citations": "46\n", "authors": ["2070"]}
{"title": "Co-training for statistical machine translation\n", "abstract": " I propose a novel co-training method for statistical machine translation. As co-training requires multiple learners trained on views of the data which are disjoint and sufficient for the labeling task, I use multiple source documents as views on translation. Co-training for statistical machine translation is therefore a type of multi-source translation. Unlike previous mutli-source methods, it improves the overall quality of translations produced by a model, rather than single translations. This is achieved by augmenting the parallel corpora on which the statistical translation models are trained. Experiments suggest that co-training is especially effective for languages with highly impoverished parallel corpora. i", "num_citations": "45\n", "authors": ["2070"]}
{"title": "A comprehensive analysis of bilingual lexicon induction\n", "abstract": " Bilingual lexicon induction is the task of inducing word translations from monolingual corpora in two languages. In this article we present the most comprehensive analysis of bilingual lexicon induction to date. We present experiments on a wide range of languages and data sizes. We examine translation into English from 25 foreign languages: Albanian, Azeri, Bengali, Bosnian, Bulgarian, Cebuano, Gujarati, Hindi, Hungarian, Indonesian, Latvian, Nepali, Romanian, Serbian, Slovak, Somali, Spanish, Swedish, Tamil, Telugu, Turkish, Ukrainian, Uzbek, Vietnamese, and Welsh. We analyze the behavior of bilingual lexicon induction on low-frequency words, rather than testing solely on high-frequency words, as previous research has done. Low-frequency words are more relevant to statistical machine translation, where systems typically lack translations of rare words that fall outside of their training data. We\u00a0\u2026", "num_citations": "43\n", "authors": ["2070"]}
{"title": "Complexity-weighted loss and diverse reranking for sentence simplification\n", "abstract": " Sentence simplification is the task of rewriting texts so they are easier to understand. Recent research has applied sequence-to-sequence (Seq2Seq) models to this task, focusing largely on training-time improvements via reinforcement learning and memory augmentation. One of the main problems with applying generic Seq2Seq models for simplification is that these models tend to copy directly from the original sentence, resulting in outputs that are relatively long and complex. We aim to alleviate this issue through the use of two main techniques. First, we incorporate content word complexities, as predicted with a leveled word complexity model, into our loss function during training. Second, we generate a large set of diverse candidate simplifications at test time, and rerank these to promote fluency, adequacy, and simplicity. Here, we measure simplicity through a novel sentence complexity model. These extensions allow our models to perform competitively with state-of-the-art systems while generating simpler sentences. We report standard automatic and human evaluation metrics.", "num_citations": "42\n", "authors": ["2070"]}
{"title": "Joshua 4.0: Packing, PRO, and paraphrases\n", "abstract": " We present Joshua 4.0, the newest version of our open-source decoder for parsing-based statistical machine translation. The main contributions in this release are the introduction of a compact grammar representation based on packed tries, and the integration of our implementation of pairwise ranking optimization, J-PRO. We further present the extension of the Thrax SCFG grammar extractor to pivot-based extraction of syntactically informed sentential paraphrases.", "num_citations": "42\n", "authors": ["2070"]}
{"title": "Reranking bilingually extracted paraphrases using monolingual distributional similarity\n", "abstract": " This paper improves an existing bilingual paraphrase extraction technique using monolingual distributional similarity to rerank candidate paraphrases. Raw monolingual data provides a complementary and orthogonal source of information that lessens the commonly observed errors in bilingual pivotbased methods. Our experiments reveal that monolingual scoring of bilingually extracted paraphrases has a significantly stronger correlation with human judgment for grammaticality than the probabilities assigned by the bilingual pivoting method does. The results also show that monolingual distribution similarity can serve as a threshold for high precision paraphrase selection.", "num_citations": "42\n", "authors": ["2070"]}
{"title": "Comparison of diverse decoding methods from conditional language models\n", "abstract": " While conditional language models have greatly improved in their ability to output high-quality natural language, many NLP applications benefit from being able to generate a diverse set of candidate sequences. Diverse decoding strategies aim to, within a given-sized candidate list, cover as much of the space of high-quality outputs as possible, leading to improvements for tasks that re-rank and combine candidate outputs. Standard decoding methods, such as beam search, optimize for generating high likelihood sequences rather than diverse ones, though recent work has focused on increasing diversity in these methods. In this work, we perform an extensive survey of decoding-time strategies for generating diverse outputs from conditional language models. We also show how diversity can be improved without sacrificing quality by over-sampling additional candidates, then filtering to the desired number.", "num_citations": "41\n", "authors": ["2070"]}
{"title": "Most \u201cbabies\u201d are \u201clittle\u201d and most \u201cproblems\u201d are \u201chuge\u201d: Compositional entailment in adjective-nouns\n", "abstract": " We examine adjective-noun (AN) composition in the task of recognizing textual entailment (RTE). We analyze behavior of ANs in large corpora and show that, despite conventional wisdom, adjectives do not always restrict the denotation of the nouns they modify. We use natural logic to characterize the variety of entailment relations that can result from AN composition. Predicting these relations depends on context and on commonsense knowledge, making AN composition especially challenging for current RTE systems. We demonstrate the inability of current stateof-the-art systems to handle AN composition in a simplified RTE task which involves the insertion of only a single word.", "num_citations": "39\n", "authors": ["2070"]}
{"title": "Toward statistical machine translation without parallel corpora\n", "abstract": " We estimate the parameters of a phrasebased statistical machine translation system from monolingual corpora instead of a bilingual parallel corpus. We extend existing research on bilingual lexicon induction to estimate both lexical and phrasal translation probabilities for MT-scale phrasetables. We propose a novel algorithm to estimate reordering probabilities from monolingual data. We report translation results for an end-to-end translation system using these monolingual features alone. Our method only requires monolingual corpora in source and target languages, a small bilingual dictionary, and a small bitext for tuning feature weights. In this paper, we examine an idealization where a phrase-table is given. We examine the degradation in translation performance when bilingually estimated translation probabilities are removed and show that 80%+ of the loss can be recovered with monolingually estimated features alone. We further show that our monolingual features add 1.5 BLEU points when combined with standard bilingually estimated phrase table features.", "num_citations": "39\n", "authors": ["2070"]}
{"title": "Joshua 3.0: Syntax-based machine translation with the Thrax grammar extractor\n", "abstract": " We present progress on Joshua, an opensource decoder for hierarchical and syntaxbased machine translation. The main focus is describing Thrax, a flexible, open source synchronous context-free grammar extractor. Thrax extracts both hierarchical (Chiang, 2007) and syntax-augmented machine translation (Zollmann and Venugopal, 2006) grammars. It is built on Apache Hadoop for efficient distributed performance, and can easily be extended with support for new grammars, feature functions, and output formats.", "num_citations": "38\n", "authors": ["2070"]}
{"title": "Incremental syntactic language models for phrase-based translation\n", "abstract": " This paper describes a novel technique for incorporating syntactic knowledge into phrase-based machine translation through incremental syntactic parsing. Bottom-up and top-down parsers typically require a completed string as input. This requirement makes it difficult to incorporate them into phrase-based translation, which generates partial hypothesized translations from left-to-right. Incremental syntactic language models score sentences in a similar left-to-right fashion, and are therefore a good mechanism for incorporating syntax into phrase-based translation. We give a formal definition of one such linear-time syntactic language model, detail its relation to phrase-based decoding, and integrate the model with the Moses phrase-based translation system. We present empirical results on a constrained Urdu-English translation task that demonstrate a significant BLEU score improvement and a large decrease in perplexity.Descriptors:", "num_citations": "38\n", "authors": ["2070"]}
{"title": "Paraphrase fragment extraction from monolingual comparable corpora\n", "abstract": " We present a novel paraphrase fragment pair extraction method that uses a monolingual comparable corpus containing different articles about the same topics or events. The procedure consists of document pair extraction, sentence pair extraction, and fragment pair extraction. At each stage, we evaluate the intermediate results manually, and tune the later stages accordingly. With this minimally supervised approach, we achieve 62% of accuracy on the paraphrase fragment pairs we collected and 67% extracted from the MSR corpus. The results look promising, given the minimal supervision of the approach, which can be further scaled up.", "num_citations": "35\n", "authors": ["2070"]}
{"title": "A compact data structure for searchable translation memories\n", "abstract": " In this paper we describe searchable translation memories, which allow translators to search their archives for possible translations of phrases. We describe how statistical machine translation can be used to align subsentential units in a translation memory, and rank them by their probability. We detail a data structure that allows for memory-efficient storage of the index. We evaluate the accuracy of translations retrieved from a searchable translation memory built from 50,000 sentence pairs, and find a precision of 86.6% for the top ranked translations.", "num_citations": "34\n", "authors": ["2070"]}
{"title": "Transliterating from all languages\n", "abstract": " Much of the previous work on transliteration has depended on resources and attributes specific to particular language pairs. In this work, rather than focus on a single language pair, we create robust models for transliterating from all languages in a large, diverse set to English. We create training data for 150 languages by mining name pairs from Wikipedia. We train 13 systems and analyze the effects of the amount of training data on transliteration performance. We also present an analysis of the types of errors that the systems make. Our analyses are particularly valuable for building machine translation systems for low resource languages, where creating and integrating a transliteration module for a language with few NLP resources may provide substantial gains in translation performance.", "num_citations": "33\n", "authors": ["2070"]}
{"title": "Feasibility of human-in-the-loop minimum error rate training\n", "abstract": " Minimum error rate training (MERT) involves choosing parameter values for a machine translation (MT) system that maximize performance on a tuning set as measured by an automatic evaluation metric, such as BLEU. The method is best when the system will eventually be evaluated using the same metric, but in reality, most MT evaluations have a human-based component. Although performing MERT with a human-based metric seems like a daunting task, we describe a new metric, RYPT, which takes human judgments into account, but only requires human input to build a database that can be reused over and over again, hence eliminating the need for human input at tuning time. In this investigative study, we analyze the diversity (or lack thereof) of the candidates produced during MERT, we describe how this redundancy can be used to our advantage, and show that RYPT is a better predictor of translation quality than BLEU.", "num_citations": "33\n", "authors": ["2070"]}
{"title": "Magnitude: A fast, efficient universal vector embedding utility package\n", "abstract": " Vector space embedding models like word2vec, GloVe, fastText, and ELMo are extremely popular representations in natural language processing (NLP) applications. We present Magnitude, a fast, lightweight tool for utilizing and processing embeddings. Magnitude is an open source Python package with a compact vector storage file format that allows for efficient manipulation of huge numbers of embeddings. Magnitude performs common operations up to 60 to 6,000 times faster than Gensim. Magnitude introduces several novel features for improved robustness like out-of-vocabulary lookups.", "num_citations": "32\n", "authors": ["2070"]}
{"title": "Edinburgh system description for the 2006 TC-STAR spoken language translation evaluation\n", "abstract": " In this paper we describe the Edinburgh University statistical machine translation system, as used for the TC-STAR 2006 evaluation campaign. We participated in the primary Final Text Edition track for the Spanish to English and English to Spanish translation tasks, using only the provided datasets for training our translation and language models. We obtained the highest WNM/Recall score in both language pairs and had competitive results for all other evaluation metrics.", "num_citations": "32\n", "authors": ["2070"]}
{"title": "Paraphrase substitution for recognizing textual entailment\n", "abstract": " We describe a method for recognizing textual entailment that uses the length of the longest common subsequence (LCS) between two texts as its decision criterion. Rather than requiring strict word matching in the common subsequences, we perform a flexible match using automatically generated paraphrases. We find that the use of paraphrases over strict word matches represents an average F-measure improvement from 0.22 to 0.36 on the CLEF 2006 Answer Validation Exercise for 7 languages.", "num_citations": "31\n", "authors": ["2070"]}
{"title": "Worker demographics and earnings on amazon mechanical turk: An exploratory analysis\n", "abstract": " Prior research reported that workers on Amazon Mechanical Turk (AMT) are underpaid, earning about $2/h. But the prior research did not investigate the difference in wage due to worker characteristics (eg, country of residence). We present the first data-driven analysis on wage gap on AMT. Using work log data and demographic data collected via online survey, we analyse the gap in wage due to different factors. We show that there is indeed wage gap; for example, workers in the US earn $3.01/h while those in India earn $1.41/h.", "num_citations": "30\n", "authors": ["2070"]}
{"title": "Joshua 5.0: Sparser, better, faster, server\n", "abstract": " We describe improvements made over the past year to Joshua, an open-source translation system for parsing-based machine translation. The main contributions this past year are significant improvements in both speed and usability of the grammar extraction and decoding steps. We have also rewritten the decoder to use a sparse feature representation, enabling training of large numbers of features with discriminative training methods.", "num_citations": "30\n", "authors": ["2070"]}
{"title": "ChatEval: A tool for chatbot evaluation\n", "abstract": " Open-domain dialog systems (ie chatbots) are difficult to evaluate. The current best practice for analyzing and comparing these dialog systems is the use of human judgments. However, the lack of standardization in evaluation procedures, and the fact that model parameters and code are rarely published hinder systematic human evaluation experiments. We introduce a unified framework for human evaluation of chatbots that augments existing tools and provides a web-based hub for researchers to share and compare their dialog systems. Researchers can submit their trained models to the ChatEval web interface and obtain comparisons with baselines and prior work. The evaluation code is open-source to ensure standardization and transparency. In addition, we introduce open-source baseline models and evaluation datasets. ChatEval can be found at https://chateval. org.", "num_citations": "28\n", "authors": ["2070"]}
{"title": "Secondary Benefits of Feedback and User Interaction in Machine Translation Tools\n", "abstract": " User feedback has often been proposed as a method for improving the accuracy of machine translation systems, but useful feedback can also serve a number of secondary benefits, including increasing user confidence in the MT technology and expanding the potential audience of users. Amikai, Inc. has produced a number of communication tools which embed translation technology and which attempt to improve the user experience by maximizing useful user interaction and feedback. As MT continues to develop, further attention needs to be paid to developing the overall user experience, which can improve the utility of translation tools even when translation quality itself plateaus.", "num_citations": "28\n", "authors": ["2070"]}
{"title": "Learning translations via images with a massively multilingual image dataset\n", "abstract": " We conduct the most comprehensive study to date into translating words via images. To facilitate research on the task, we introduce a large-scale multilingual corpus of images, each labeled with the word it represents. Past datasets have been limited to only a few high-resource languages and unrealistically easy translation settings. In contrast, we have collected by far the largest available dataset for this task, with images for approximately 10,000 words in each of 100 languages. We run experiments on a dozen high resource languages and 20 low resources languages, demonstrating the effect of word concreteness and part-of-speech on translation quality.% We find that while image features work best for concrete nouns, they are sometimes effective on other parts of speech. To improve image-based translation, we introduce a novel method of predicting word concreteness from images, which improves on a previous state-of-the-art unsupervised technique. This allows us to predict when image-based translation may be effective, enabling consistent improvements to a state-of-the-art text-based word translation system. Our code and the Massively Multilingual Image Dataset (MMID) are available at http://multilingual-images. org/.", "num_citations": "27\n", "authors": ["2070"]}
{"title": "Are two heads better than one? crowdsourced translation via a two-step collaboration of non-professional translators and editors\n", "abstract": " Crowdsourcing is a viable mechanism for creating training data for machine translation. It provides a low cost, fast turnaround way of processing large volumes of data. However, when compared to professional translation, naive collection of translations from non-professionals yields low-quality results. Careful quality control is necessary for crowdsourcing to work well. In this paper, we examine the challenges of a two-step collaboration process with translation and post-editing by non-professionals. We develop graphbased ranking models that automatically select the best output from multiple redundant versions of translations and edits, and improves translation quality closer to professionals.", "num_citations": "27\n", "authors": ["2070"]}
{"title": "Processing informal, romanized Pakistani text messages\n", "abstract": " Regardless of language, the standard character set for text messages (SMS) and many other social media platforms is the Roman alphabet. There are romanization conventions for some character sets, but they are used inconsistently in informal text, such as SMS. In this work, we convert informal, romanized Urdu messages into the native Arabic script and normalize non-standard SMS language. Doing so prepares the messages for existing downstream processing tools, such as machine translation, which are typically trained on well-formed, native script text. Our model combines information at the word and character levels, allowing it to handle out-of-vocabulary items. Compared with a baseline deterministic approach, our system reduces both word and character error rate by over 50%.", "num_citations": "27\n", "authors": ["2070"]}
{"title": "Hallucinated n-best lists for discriminative language modeling\n", "abstract": " This paper investigates semi-supervised methods for discriminative language modeling, whereby n-best lists are \u201challucinated\u201d for given reference text and are then used for training n-gram language models using the perceptron algorithm. We perform controlled experiments on a very strong baseline English CTS system, comparing three methods for simulating ASR output, and compare the results with training with \u201creal\u201d n-best list output from the baseline recognizer. We find that methods based on extracting phrasal cohorts - similar to methods from machine translation for extracting phrase tables - yielded the largest gains of our three methods, achieving over half of the WER reduction of the fully supervised methods.", "num_citations": "27\n", "authors": ["2070"]}
{"title": "Bootstrapping parallel corpora\n", "abstract": " We present two methods for the automatic creation of parallel corpora. Whereas previous work into the automatic construction of parallel corpora has focused on harvesting them from the web, we examine the use of existing parallel corpora to bootstrap data for new language pairs. First, we extend existing parallel corpora using co-training, wherein machine translations are selectively added to training corpora with multiple source texts. Retraining translation models yields modest improvements. Second, we simulate the creation of training data for a language pair for which a parallel corpus is not available. Starting with no human translations from German to English we produce a German to English translation model with 45% accuracy using parallel corpora in other languages. This suggests the method may be useful in the creation of parallel corpora for languages with scarce resources.", "num_citations": "27\n", "authors": ["2070"]}
{"title": "Unsupervised hierarchical story infilling\n", "abstract": " Story infilling involves predicting words to go into a missing span from a story. This challenging task has the potential to transform interactive tools for creative writing. However, state-of-the-art conditional language models have trouble balancing fluency and coherence with novelty and diversity. We address this limitation with a hierarchical model which first selects a set of rare words and then generates text conditioned on that set. By relegating the high entropy task of picking rare words to a word-sampling model, the second-stage model conditioned on those words can achieve high fluency and coherence by searching for likely sentences, without sacrificing diversity.", "num_citations": "26\n", "authors": ["2070"]}
{"title": "Paraphrastic sentence compression with a character-based metric: Tightening without deletion\n", "abstract": " We present a substitution-only approach to sentence compression which \u201ctightens\u201d a sentence by reducing its character length. Replacing phrases with shorter paraphrases yields paraphrastic compressions as short as 60% of the original length. In support of this task, we introduce a novel technique for re-ranking paraphrases extracted from bilingual corpora. At high compression rates1 paraphrastic compressions outperform a state-of-the-art deletion model in an oracle experiment. For further compression, deleting from oracle paraphrastic compressions preserves more meaning than deletion alone. In either setting, paraphrastic compression shows promise for surpassing deletion-only methods.", "num_citations": "26\n", "authors": ["2070"]}
{"title": "Cheap facts and counter-facts\n", "abstract": " This paper describes our experiments of using Amazon\u2019s Mechanical Turk to generate (counter-) facts from texts for certain namedentities. We give the human annotators a paragraph of text and a highlighted named-entity. They will write down several (counter-) facts about this named-entity in that context. The analysis of the results is performed by comparing the acquired data with the recognizing textual entailment (RTE) challenge dataset.", "num_citations": "25\n", "authors": ["2070"]}
{"title": "Reconciling user expectations and translation technology to create a useful real-world application\n", "abstract": " In this paper we discuss the motivations for the development of Amikai's web-based translated chat room application. Like other successful machine translation (MT) systems, Amikai's translated chat attempts to reconcile overly-optimistic user expectations with the limited capabilities of current MT technology by adjusting user expectations and limiting the scope and domain of the translation task. We explain why chat is a natural application for MT technology and briefly describe aspects of the user experience within the Amikai system.Introduction: Making Useful Machine Translation Products", "num_citations": "25\n", "authors": ["2070"]}
{"title": "Systematically adapting machine translation for grammatical error correction\n", "abstract": " n this work we adapt machine translation (MT) to grammatical error correction, identifying how components of the statistical MT pipeline can be modified for this task and analyzing how each modification impacts system performance. We evaluate the contribution of each of these components with standard evaluation metrics and automatically characterize the morphological and lexical transformations made in system output. Our model rivals the current state of the art using a fraction of the training data.", "num_citations": "24\n", "authors": ["2070"]}
{"title": "The language of place: Semantic value from geospatial context\n", "abstract": " There is a relationship between what we say and where we say it. Word embeddings are usually trained assuming that semantically-similar words occur within the same textual contexts. We investigate the extent to which semantically-similar words occur within the same geospatial contexts. We enrich a corpus of geolocated Twitter posts with physical data derived from Google Places and OpenStreetMap, and train word embeddings using the resulting geospatial contexts. Intrinsic evaluation of the resulting vectors shows that geographic context alone does provide useful information about semantic relatedness.", "num_citations": "24\n", "authors": ["2070"]}
{"title": "Edinburgh system description for the 2005 nist mt evaluation\n", "abstract": " This document describes the first NIST MT Evaluation submission of the newly formed Edinburgh University Statistical Machine Translation Group. Our entry to the 2005 DARPA/NIST MT Evaluation was largely based on the 2004 MIT system. In a two month effort we focused on adding more data and a few new features to our Arabic-English system. We also worked on preprocessing and applied some of the lessons learnt to our Chinese-English system. Our efforts resulted in improved translation performance over the previous 2004 system. Competing in the competition was also a valuable learning experience in largescale system building.This document describes the first NIST MT Evaluation submission from Edinburgh University\u2019s Statistical Machine Translation Group. Our entry to this year\u2019s evaluation was based on the 2004 system from MIT (Koehn, 2004a). This was built by Philipp Koehn, who is now a faculty member in Edinburgh\u2019s School of Informatics. In a two month effort, the group familiarised itself with the system and the data, and added a few new features to the Arabic-English system.", "num_citations": "24\n", "authors": ["2070"]}
{"title": "Decoding in joshua: Open source, parsing-based machine translation\n", "abstract": " We describe a scalable decoder for parsing-based machine translation.\ue062 e decoder is written in Java and implements all the essential algorithms described in (Chiang, 2007) and (Li and Khudanpur, 2008b): chart-parsing, n-gram language model integration, beam-and cube-pruning, and k-best extraction. Additionally, parallel and distributed computing techniques are exploited to make it scalable. We demonstrate experimentally that our decoder is more than 30 times faster than a baseline decoder written in Python.", "num_citations": "23\n", "authors": ["2070"]}
{"title": "Crowd control: effectively utilizing unscreened crowd workers for biomedical data annotation\n", "abstract": " Annotating unstructured texts in Electronic Health Records data is usually a necessary step for conducting machine learning research on such datasets. Manual annotation by domain experts provides data of the best quality, but has become increasingly impractical given the rapid increase in the volume of EHR data. In this article, we examine the effectiveness of crowdsourcing with unscreened online workers as an alternative for transforming unstructured texts in EHRs into annotated data that are directly usable in supervised learning models. We find the crowdsourced annotation data to be just as effective as expert data in training a sentence classification model to detect the mentioning of abnormal ear anatomy in radiology reports of audiology. Furthermore, we have discovered that enabling workers to self-report a confidence level associated with each annotation can help researchers pinpoint less-accurate\u00a0\u2026", "num_citations": "21\n", "authors": ["2070"]}
{"title": "Linear B system description for the 2005 NIST MT evaluation exercise\n", "abstract": " This document describes Linear B\u2019s entry for the 2005 NIST MT Evaluation exercise. Linear B examined the efficacy of human-aided statistical machine translation by looking at the improvements that could be had by involving non-Arabic speakers in the translation process. We examined two conditions: one in which non-Arabic speakers edited the output of a statistical machine translation system, and one in which they were allowed to select phrasal translations from a chart of possible translations for an Arabic sentence, and then edit the text.", "num_citations": "21\n", "authors": ["2070"]}
{"title": "End-to-end statistical machine translation with zero or small parallel texts\n", "abstract": " We use bilingual lexicon induction techniques, which learn translations from monolingual texts in two languages, to build an end-to-end statistical machine translation (SMT) system without the use of any bilingual sentence-aligned parallel corpora. We present detailed analysis of the accuracy of bilingual lexicon induction, and show how a discriminative model can be used to combine various signals of translation equivalence (like contextual similarity, temporal similarity, orthographic similarity and topic similarity). Our discriminative model produces higher accuracy translations than previous bilingual lexicon induction techniques. We reuse these signals of translation equivalence as features on a phrase-based SMT system. These monolingually estimated features enhance low resource SMT systems in addition to allowing end-to-end machine translation without parallel corpora.", "num_citations": "20\n", "authors": ["2070"]}
{"title": "Hallucinating phrase translations for low resource mt\n", "abstract": " We demonstrate that \u201challucinating\u201d phrasal translations can significantly improve the quality of machine translation in low resource conditions. Our hallucinated phrase tables consist of entries composed from multiple unigram translations drawn from the baseline phrase table and from translations that are induced from monolingual corpora. The hallucinated phrase table is very noisy. Its translations are low precision but high recall. We counter this by introducing 30 new feature functions (including a variety of monolinguallyestimated features) and by aggressively pruning the phrase table. Our analysis evaluates the intrinsic quality of our hallucinated phrase pairs as well as their impact in end-to-end Spanish-English and Hindi-English MT.", "num_citations": "20\n", "authors": ["2070"]}
{"title": "Predicting human-targeted translation edit rate via untrained human annotators\n", "abstract": " In the field of machine translation, automatic metrics have proven quite valuable in system development for tracking progress and measuring the impact of incremental changes. However, human judgment still plays a large role in the context of evaluating MT systems. For example, the GALE project uses humantargeted translation edit rate (HTER), wherein the MT output is scored against a post-edited version of itself (as opposed to being scored against an existing human reference). This poses a problem for MT researchers, since HTER is not an easy metric to calculate, and would require hiring and training human annotators to perform the editing task. In this work, we explore soliciting those edits from untrained human annotators, via the online service Amazon Mechanical Turk. We show that the collected data allows us to predict HTER-ranking of documents at a significantly higher level than the ranking obtained using automatic metrics.", "num_citations": "20\n", "authors": ["2070"]}
{"title": "Crowdsourced accessibility: Elicitation of Wikipedia articles\n", "abstract": " Mechanical Turk is useful for generating complex speech resources like conversational speech transcription. In this work, we explore the next step of eliciting narrations of Wikipedia articles to improve accessibility for low-literacy users. This task proves a useful test-bed to implement qualitative vetting of workers based on difficult to define metrics like narrative quality. Working with the Mechanical Turk API, we collected sample narrations, had other Turkers rate these samples and then granted access to full narration HITs depending on aggregate quality. While narrating full articles proved too onerous a task to be viable, using other Turkers to perform vetting was very successful. Elicitation is possible on Mechanical Turk, but it should conform to suggested best practices of simple tasks that can be completed in a streamlined workflow.", "num_citations": "20\n", "authors": ["2070"]}
{"title": "Natural language processing of Reddit data to evaluate dermatology patient experiences and therapeutics\n", "abstract": " BackgroundThere is a lack of research studying patient-generated data on Reddit, one of the world's most popular forums with active users interested in dermatology. Techniques within natural language processing, a field of artificial intelligence, can analyze large amounts of text information and extract insights.ObjectiveTo apply natural language processing to Reddit comments about dermatology topics to assess for feasibility and potential for insights and engagement.MethodsA software pipeline preprocessed Reddit comments from 2005 to 2017 from 7 popular dermatology-related subforums on Reddit, applied latent Dirichlet\u00a0allocation, and used spectral clustering to establish cohesive themes and the frequency of word representation and grouped terms within these topics.ResultsWe created a corpus of 176,000 comments and identified trends in patient engagement in spaces such as eczema and acne, among\u00a0\u2026", "num_citations": "19\n", "authors": ["2070"]}
{"title": "Affinity measures based on the graph Laplacian\n", "abstract": " Several language processing tasks can be inherently represented by a weighted graph where the weights are interpreted as a measure of relatedness between two vertices. Measuring similarity between arbitary pairs of vertices is essential in solving several language processing problems on these datasets. Random walk based measures perform better than other path based measures like shortest-path. We evaluate several random walk measures and propose a new measure based on commute time. We use the psuedo inverse of the Laplacian to derive estimates for commute times in graphs. Further, we show that this pseudo inverse based measure could be improved by discarding the least significant eigenvectors, corresponding to the noise in the graph construction process, using singular value decomposition.", "num_citations": "19\n", "authors": ["2070"]}
{"title": "Clustering paraphrases by word sense\n", "abstract": " Automatically generated databases of English paraphrases have the drawback that they return a single list of paraphrases for an input word or phrase. This means that all senses of polysemous words are grouped together, unlike WordNet which partitions different senses into separate synsets. We present a new method for clustering paraphrases by word sense, and apply it to the Paraphrase Database (PPDB). We investigate the performance of hierarchical and spectral clustering algorithms, and systematically explore different ways of defining the similarity matrix that they use as input. Our method produces sense clusters that are qualitatively and quantitatively good, and that represent a substantial improvement to the PPDB resource.", "num_citations": "18\n", "authors": ["2070"]}
{"title": "WikiTopics: What is popular on Wikipedia and why\n", "abstract": " We establish a novel task in the spirit of news summarization and topic detection and tracking (TDT): daily determination of the topics newly popular with Wikipedia readers. Central to this effort is a new public dataset consisting of the hourly page view statistics of all Wikipedia articles over the last three years. We give baseline results for the tasks of: discovering individual pages of interest, clustering these pages into coherent topics, and extracting the most relevant summarizing sentence for the reader. When compared to human judgements, our system shows the viability of this task, and opens the door to a range of exciting future work.", "num_citations": "18\n", "authors": ["2070"]}
{"title": "Edinburgh system description for the 2005 iwslt speech translation evaluation\n", "abstract": " Our participation in the IWSLT 2005 speech translation task is our first effort to work on limited domain speech data. We adapted our statistical machine trans-lation system that performed successfully in previous DARPA competitions on open domain text translations. We participated in the supplied corpora transcription track. We achieved the highest BLEU score in 2 out of 5 language pairs and had competi-tive results for the other language pairs. 1", "num_citations": "18\n", "authors": ["2070"]}
{"title": "Sentential paraphrasing as black-box machine translation\n", "abstract": " We present a simple, prepackaged solution to generating paraphrases of English sentences. We use the Paraphrase Database (PPDB) for monolingual sentence rewriting and provide machine translation language packs: prepackaged, tuned models that can be downloaded and used to generate paraphrases on a standard Unix environment. The language packs can be treated as a black box or customized to specific tasks. In this demonstration, we will explain how to use the included interactive webbased tool to generate sentential paraphrases.", "num_citations": "17\n", "authors": ["2070"]}
{"title": "Simplification using paraphrases and context-based lexical substitution\n", "abstract": " Lexical simplification involves identifying complex words or phrases that need to be simplified, and recommending simpler meaning-preserving substitutes that can be more easily understood. We propose a complex word identification (CWI) model that exploits both lexical and contextual features, and a simplification mechanism which relies on a word-embedding lexical substitution model to replace the detected complex words with simpler paraphrases. We compare our CWI and lexical simplification models to several baselines, and evaluate the performance of our simplification system against human judgments. The results show that our models are able to detect complex words with higher accuracy than other commonly used methods, and propose good simplification substitutes in context. They also highlight the limited contribution of context features for CWI, which nonetheless improve simplification compared to context-unaware models.", "num_citations": "16\n", "authors": ["2070"]}
{"title": "Cost optimization in crowdsourcing translation: Low cost translations made even cheaper\n", "abstract": " Crowdsourcing makes it possible to create translations at much lower cost than hiring professional translators. However, it is still expensive to obtain the millions of translations that are needed to train statistical machine translation systems. We propose two mechanisms to reduce the cost of crowdsourcing while maintaining high translation quality. First, we develop a method to reduce redundant translations. We train a linear model to evaluate the translation quality on a sentenceby-sentence basis, and fit a threshold between acceptable and unacceptable translations. Unlike past work, which always paid for a fixed number of translations for each source sentence and then chose the best from them, we can stop earlier and pay less when we receive a translation that is good enough. Second, we introduce a method to reduce the pool of translators by quickly identifying bad translators after they have translated only a few sentences. This also allows us to rank translators, so that we re-hire only good translators to reduce cost.", "num_citations": "16\n", "authors": ["2070"]}
{"title": "Parma: A predicate argument aligner\n", "abstract": " We introduce PARMA, a system for crossdocument, semantic predicate and argument alignment. Our system combines a number of linguistic resources familiar to researchers in areas such as recognizing textual entailment and question answering, integrating them into a simple discriminative model. PARMA achieves state of the art results on an existing and a new dataset. We suggest that previous efforts have focussed on data that is biased and too easy, and we provide a more difficult dataset based on translation data with a low baseline which we beat by 17% F1.", "num_citations": "16\n", "authors": ["2070"]}
{"title": "Improving statistical translation through editing\n", "abstract": " In this paper we introduce Linear B's statistical machine translation system. We describe how Linear B's phrase-based translation models are learned from a parallel corpus, and show how the quality of the translations produced by our system can be improved over time through editing. There are two levels at which our translations can be edited. The first is through a simple correction of the text that is produced by our system. The second is through a mechanism which allows an advanced user to examine the sentences that a particular translation was learned from. The learning process can be improved by correcting which phrases in the sentence should be considered translations of each other.", "num_citations": "16\n", "authors": ["2070"]}
{"title": "Active learning for statistical machine translation\n", "abstract": " For my PhD I propose to apply active learning to statistical machine translation. Statistical machine translation is a data-intensive way of producing translation systems. It uses machine learning algorithms to automatically create translation models from bilingual training data. Statistical translation can be used for any language provided that there is sufficient training data. However, when only small amounts training data is available statistical translation fails to produce good translations. This is a problem for so-called low density languages, which do not have extensive resources. I will examine the problem of using statistical translation for such languages, by focusing on efficient ways of creating training data through active learning. Active learning is a way to reduce the cost of creating a corpus of labeled training examples. Most machine learning takes place passively, because the statistical learner has no input on which examples it is trained on. By contrast, active learning gives the statistical learner the power to query a human annotator to label examples that will be most informative in its learning. Thus active learning minimizes the amount of training data required to achieve a certain performance level, by selectively sampling the data that needs to be annotated. This in turn reduces the amount of human effort required to create a training corpus, and reduces the associated cost of its creation. In this report I describe the theory and practice of active learning, review the relevant details of how statistical translation is learned from data, and propose a number of ways in which active learning could be applied to translation. The significance of my\u00a0\u2026", "num_citations": "16\n", "authors": ["2070"]}
{"title": "Translations of the CALLHOME Egyptian Arabic corpus for conversational speech translation\n", "abstract": " Translation of the output of automatic speech recognition (ASR) systems, also known as speech translation, has received a lot of research interest recently. This is especially true for programs such as DARPA BOLT which focus on improving spontaneous human-human conversation across languages. However, this research is hindered by the dearth of datasets developed for this explicit purpose. For Egyptian Arabic-English, in particular, no parallel speechtranscription-translation dataset exists in the same domain. In order to support research in speech translation, we introduce the Callhome Egyptian Arabic-English Speech Translation Corpus. This supplements the existing LDC corpus with four reference translations for each utterance in the transcripts. The result is a three-way parallel dataset of Egyptian Arabic Speech, transcriptions and English translations.", "num_citations": "15\n", "authors": ["2070"]}
{"title": "Semi-supervised discriminative language modeling for Turkish ASR\n", "abstract": " We present our work on semi-supervised learning of discriminative language models where the negative examples for sentences in a text corpus are generated using confusion models for Turkish at various granularities, specifically, word, sub-word, syllable and phone levels. We experiment with different language models and various sampling strategies to select competing hypotheses for training with a variant of the perceptron algorithm. We find that morph-based confusion models with a sample selection strategy aiming to match the error distribution of the baseline ASR system gives the best performance. We also observe that substituting half of the supervised training examples with those obtained in a semi-supervised manner gives similar results.", "num_citations": "15\n", "authors": ["2070"]}
{"title": "TinkerBell: Cross-lingual Cold-Start Knowledge Base Construction.\n", "abstract": " In this paper we present TinkerBell, a state-of-the-art end-to-end cold-start knowledge base construction system that extracts entity, relation, event and sentiment knowledge from three languages (English, Chinese and Spanish).", "num_citations": "14\n", "authors": ["2070"]}
{"title": "Monolingual distributional similarity for text-to-text generation\n", "abstract": " Previous work on paraphrase extraction and application has relied on either parallel datasets, or on distributional similarity metrics over large text corpora. Our approach combines these two orthogonal sources of information and directly integrates them into our paraphrasing system\u2019s log-linear model. We compare different distributional similarity feature-sets and show significant improvements in grammaticality and meaning retention on the example text-to-text generation task of sentence compression, achieving stateof-the-art quality.", "num_citations": "14\n", "authors": ["2070"]}
{"title": "Introduction to statistical machine translation\n", "abstract": " \u2022 Warren Weaver (1949)\"/have a text\u00a1 n front of me which is written in Russian but 1 am going Lo pretend LhoL iL is really written in English and Lhot iL has been coded in some strange symbols. AII 1 need to do is strip off the code in order to retrieve the information contained in the text\"", "num_citations": "14\n", "authors": ["2070"]}
{"title": "So-called non-subsective adjectives\n", "abstract": " The interpretation of adjective-noun pairs plays a crucial role in tasks such as recognizing textual entailment. Formal semantics often places adjectives into a taxonomy which should dictate adjectives\u2019 entailment behavior when placed in adjective-noun compounds. However, we show experimentally that the behavior of subsective adjectives (eg red) versus non-subsective adjectives (eg fake) is not as cut and dry as often assumed. For example, inferences are not always symmetric: while ID is generally considered to be mutually exclusive with fake ID, fake ID is considered to entail ID. We discuss the implications of these findings for automated natural language understanding.", "num_citations": "13\n", "authors": ["2070"]}
{"title": "Joshua 2.0: a toolkit for parsing-based machine translation with syntax, semirings, discriminative training and other goodies\n", "abstract": " We describe the progress we have made in the past year on Joshua (Li et al., 2009a), an open source toolkit for parsing based machine translation. The new functionality includes: support for translation grammars with a rich set of syntactic nonterminals, the ability for external modules to posit constraints on how spans in the input sentence should be translated, lattice parsing for dealing with input uncertainty, a semiring framework that provides a unified way of doing various dynamic programming calculations, variational decoding for approximating the intractable MAP decoding, hypergraph-based discriminative training for better feature engineering, a parallelized MERT module, documentlevel and tail-based MERT, visualization of the derivation trees, and a cleaner pipeline for MT experiments.", "num_citations": "13\n", "authors": ["2070"]}
{"title": "Hierarchical phrase-based grammar extraction in joshua\n", "abstract": " While example-based machine translation has long used corpus information at run-time, statistical phrase-based approaches typically include a preprocessing stage where an aligned parallel corpus is split into phrases, and parameter values are calculated for each phrase using simple relative frequency estimates. This paper describes an open source implementation of the crucial algorithms presented in (Lopez, 2008) which allow direct run-time calculation of SCFG translation rules in Joshua.", "num_citations": "13\n", "authors": ["2070"]}
{"title": "Semantically informed machine translation (SIMT)\n", "abstract": " This report describes the findings of the machine translation team from the first Summer Camp for Applied Language Exploration (SCALE) hosted at the Human Language Technology Center of Excellence located at Johns Hopkins University. This intensive, eight week workshop brought together 20 students, faculty and researchers to conduct research on the topic of Semantically Informed Machine Translation (SIMT). The type of semantics that were examined at the SIMT workshop were \u201cHigh Information Value Elements,\u201d or HIVEs, which include named entities (such as people or organizations) and modalities (indications that a statement represents something that has taken place or is a belief or an intention). These HIVEs were examined in the context of machine translation between Urdu and English. The goal of the workshop was to identify and translate HIVEs from the foreign language, and to investigate whether incorporating this sort of structured semantic information into machine translation (MT) systems could produce better translations. The SIMT SCALE differs from other efforts in MT, most notably the DARPA Global Autonomous Language Exploitation (GALE) initiative. The key differences are:1. The SIMT SCALE focused on incorporating syntax and semantics into machine translation whereas linguistically naive approaches to MT have dominated much of the GALE research. Although syntactic translation models have not shown dramatic improvements in GALE\u2019s Arabic-English translation task, we found that they dramatically improve Urdu-English translation.", "num_citations": "13\n", "authors": ["2070"]}
{"title": "Improved statistical translation through editing\n", "abstract": " In this paper we introduce Linear B's statistical machine translation system. We describe how Linear B's phrase-based translation models are learned from a parallel corpus, and show how the quality of the translations produced by our system can be improved over time through editing. There are two levels at which our translations can be edited. The first is through a simple correction of the text that is produced by our system. The second is through a mechanism which allows an advanced user to examine the sentences that a particular translation was learned from. The learning process can be improved by correcting which phrases in the sentence should be considered translations of each other.", "num_citations": "13\n", "authors": ["2070"]}
{"title": "Semantically-informed syntactic machine translation: A tree-grafting approach\n", "abstract": " We describe a unified and coherent syntactic framework for supporting a semantically-informed syntactic approach to statistical machine translation. Semantically enriched syntactic tags assigned to the target-language training texts improved translation quality. The resulting system significantly outperformed a linguistically naive baseline model (Hiero), and reached the highest scores yet reported on the NIST 2009 Urdu-English translation task. This finding supports the hypothesis (posed by many researchers in the MT community, e.g., in DARPA GALE) that both syntactic and semantic information are critical for improving translation quality---and further demonstrates that large gains can be achieved for low-resource languages with different word order than English.", "num_citations": "12\n", "authors": ["2070"]}
{"title": "Demonstration of joshua: An open source toolkit for parsing-based machine translation\n", "abstract": " We describe Joshua (Li et al., 2009a) 1, an open source toolkit for statistical machine translation. Joshua implements all of the algorithms required for translation via synchronous context free grammars (SCFGs): chart-parsing, n-gram language model integration, beam-and cubepruning, and k-best extraction. The toolkit also implements suffix-array grammar extraction and minimum error rate training. It uses parallel and distributed computing techniques for scalability. We also provide a demonstration outline for illustrating the toolkit\u2019s features to potential users, whether they be newcomers to the field or power users interested in extending the toolkit.", "num_citations": "12\n", "authors": ["2070"]}
{"title": "Effectively crowdsourcing radiology report annotations\n", "abstract": " Crowdsourcing platforms are a popular choice for researchers to gather text annotations quickly at scale. We investigate whether crowdsourced annotations are useful when the labeling task requires medical domain knowledge. Comparing a sentence classification model trained with expert-annotated sentences to the same model trained on crowd-labeled sentences, we find the crowdsourced training data to be just as effective as the manually produced dataset. We can improve the accuracy of the crowd-fueled model without collecting further labels by filtering out worker labels applied with low confidence.", "num_citations": "11\n", "authors": ["2070"]}
{"title": "Ideological perspective detection using semantic features\n", "abstract": " In this paper, we propose the use of word sense disambiguation and latent semantic features to automatically identify a person\u2019s perspective from his/her written text. We run an Amazon Mechanical Turk experiment where we ask Turkers to answer a set of constrained and open-ended political questions drawn from the American National Election Studies (ANES). We then extract the proposed features from the answers to the open-ended questions and use them to predict the answer to one of the constrained questions, namely, their preferred Presidential Candidate. In addition to this newly created dataset, we also evaluate our proposed approach on a second standard dataset of \u201cIdeological-Debates\u201d. This latter dataset contains topics from four domains: Abortion, Creationism, Gun Rights and Gay-Rights. Experimental results show that using word sense disambiguation and latentsemantics, whether separately or combined, beats the majority and random baselines on the cross-validation and held-out-test sets for both the ANES and the four domains of the \u201cIdeological Debates\u201d datasets. Moreover combining both feature sets outperforms a stronger unigram-only classification system.", "num_citations": "11\n", "authors": ["2070"]}
{"title": "Expectations of word sense in parallel corpora\n", "abstract": " Given a parallel corpus, if two distinct words in language A, a1 and a2, are aligned to the same word b1 in language B, then this might signal that b1 is polysemous, or it might signal a1 and a2 are synonyms. Both assumptions with successful work have been put forward in the literature. We investigate these assumptions, along with other questions of word sense, by looking at sampled parallel sentences containing tokens of the same type in English, asking how often they mean the same thing when they are: 1. aligned to the same foreign type; and 2. aligned to different foreign types. Results for French-English and Chinese-English parallel corpora show similar behavior: Synonymy is only very weakly the more prevalent scenario, where both cases regularly occur.", "num_citations": "11\n", "authors": ["2070"]}
{"title": "Human and automatic detection of generated text\n", "abstract": " With the advent of generative models with a billion parameters or more, it is now possible to automatically generate vast amounts of human-sounding text. This raises questions into just how human-like is the machine-generated text, and how long does a text excerpt need to be for both humans and automatic discriminators to be able reliably detect that it was machine-generated. In this paper, we conduct a thorough investigation of how choices such as sampling strategy and text excerpt length can impact the performance of automatic detection methods as well as human raters. We find that the sampling strategies which result in more human-like text according to human raters create distributional differences from human-written text that make detection easy for automatic discriminators.", "num_citations": "10\n", "authors": ["2070"]}
{"title": "PerspectroScope: A window to the world of diverse perspectives\n", "abstract": " This work presents PerspectroScope, a web-based system which lets users query a discussion-worthy natural language claim, and extract and visualize various perspectives in support or against the claim, along with evidence supporting each perspective. The system thus lets users explore various perspectives that could touch upon aspects of the issue at hand.The system is built as a combination of retrieval engines and learned textual-entailment-like classifiers built using a few recent developments in natural language understanding. To make the system more adaptive, expand its coverage, and improve its decisions over time, our platform employs various mechanisms to get corrections from the users. PerspectroScope is available at github.com/CogComp/perspectroscope.", "num_citations": "10\n", "authors": ["2070"]}
{"title": "Learning scalar adjective intensity from paraphrases\n", "abstract": " Adjectives like \u201cwarm\u201d,\u201chot\u201d, and \u201cscalding\u201d all describe temperature but differ in intensity. Understanding these differences between adjectives is a necessary part of reasoning about natural language. We propose a new paraphrase-based method to automatically learn the relative intensity relation that holds between a pair of scalar adjectives. Our approach analyzes over 36k adjectival pairs from the Paraphrase Database under the assumption that, for example, paraphrase pair \u201creally hot\u201d<\u2013>\u201cscalding\u201d suggests that \u201chot\u201d<\u201cscalding\u201d. We show that combining this paraphrase evidence with existing, complementary pattern-and lexicon-based approaches improves the quality of systems for automatically ordering sets of scalar adjectives and inferring the polarity of indirect answers to \u201cyes/no\u201d questions.", "num_citations": "10\n", "authors": ["2070"]}
{"title": "Domain-specific paraphrase extraction\n", "abstract": " The validity of applying paraphrase rules depends on the domain of the text that they are being applied to. We develop a novel method for extracting domainspecific paraphrases. We adapt the bilingual pivoting paraphrase method to bias the training data to be more like our target domain of biology. Our best model results in higher precision while retaining complete recall, giving a 10% relative improvement in AUC.", "num_citations": "10\n", "authors": ["2070"]}
{"title": "Using categorial grammar to label translation rules\n", "abstract": " Adding syntactic labels to synchronous context-free translation rules can improve performance, but labeling with phrase structure constituents, as in GHKM (Galley et al., 2004), excludes potentially useful translation rules. SAMT (Zollmann and Venugopal, 2006) introduces heuristics to create new non-constituent labels, but these heuristics introduce many complex labels and tend to add rarely-applicable rules to the translation grammar. We introduce a labeling scheme based on categorial grammar, which allows syntactic labeling of many rules with a minimal, well-motivated label set. We show that our labeling scheme performs comparably to SAMT on an Urdu\u2013English translation task, yet the label set is an order of magnitude smaller, and translation is twice as fast.", "num_citations": "10\n", "authors": ["2070"]}
{"title": "Visualizing data structures in parsing-based machine translation\n", "abstract": " As machine translation (MT) systems grow more complex and incorporate more linguistic knowledge, it becomes more difficult to evaluate independent pieces of the MT pipeline. Being able to inspect many of the intermediate data structures used during MT decoding allows a more fine-grained evaluation of MT performance, helping to determine which parts of the current process are effective and which are not. In this article, we present an overview of the visualization tools that are currently distributed with the Joshua (Li et al., 2009) MT decoder. We explain their use and present an example of how visually inspecting the decoder\u2019s data structures has led to useful improvements in the MT model.", "num_citations": "10\n", "authors": ["2070"]}
{"title": "Integrating Output from Specialized Modules in Machine Translation: Transliterations in Joshua\n", "abstract": " In many cases in SMT we want to allow specialized modules to propose translation fragments to the decoder and allow them to compete with translations contained in the phrase table. Transliteration is one module that may produce such specialized output. In this paper, as an example, we build a specialized Urdu transliteration module and integrate its output into an Urdu\u2013English MT system. The module marks-up the test text using an XML format, and the decoder allows alternate translations (transliterations) to compete.", "num_citations": "10\n", "authors": ["2070"]}
{"title": "Reasoning about goals, steps, and temporal ordering with wikihow\n", "abstract": " We propose a suite of reasoning tasks on two types of relations between procedural events: goal-step relations (\"learn poses\" is a step in the larger goal of \"doing yoga\") and step-step temporal relations (\"buy a yoga mat\" typically precedes \"learn poses\"). We introduce a dataset targeting these two relations based on wikiHow, a website of instructional how-to articles. Our human-validated test set serves as a reliable benchmark for commonsense inference, with a gap of about 10% to 20% between the performance of state-of-the-art transformer models and human performance. Our automatically-generated training set allows models to effectively transfer to out-of-domain tasks requiring knowledge of procedural events, with greatly improved performances on SWAG, Snips, and the Story Cloze Test in zero- and few-shot settings.", "num_citations": "9\n", "authors": ["2070"]}
{"title": "Toward better storylines with sentence-level language models\n", "abstract": " We propose a sentence-level language model which selects the next sentence in a story from a finite set of fluent alternatives. Since it does not need to model fluency, the sentence-level language model can focus on longer range dependencies, which are crucial for multi-sentence coherence. Rather than dealing with individual words, our method treats the story so far as a list of pre-trained sentence embeddings and predicts an embedding for the next sentence, which is more efficient than predicting word embeddings. Notably this allows us to consider a large number of candidates for the next sentence during training. We demonstrate the effectiveness of our approach with state-of-the-art accuracy on the unsupervised Story Cloze task and with promising results on larger-scale next sentence prediction tasks.", "num_citations": "9\n", "authors": ["2070"]}
{"title": "A comparison of context-sensitive models for lexical substitution\n", "abstract": " Word embedding representations provide good estimates of word meaning and give state-of-the art performance in semantic tasks. Embedding approaches differ as to whether and how they account for the context surrounding a word. We present a comparison of different word and context representations on the task of proposing substitutes for a target word in context (lexical substitution). We also experiment with tuning contextualized word embeddings on a dataset of sense-specific instances for each target word. We show that powerful contextualized word representations, which give high performance in several semantics-related tasks, deal less well with the subtle in-context similarity relationships needed for substitution. This is better handled by models trained with this objective in mind, where the inter-dependence between word and context representations is explicitly modeled during training.", "num_citations": "9\n", "authors": ["2070"]}
{"title": "Automated paraphrase lattice creation for HyTER machine translation evaluation\n", "abstract": " We propose a variant of a well-known machine translation (MT) evaluation metric, HyTER (Dreyer and Marcu, 2012), which exploits reference translations enriched with meaning equivalent expressions. The original HyTER metric relied on hand-crafted paraphrase networks which restricted its applicability to new data. We test, for the first time, HyTER with automatically built paraphrase lattices. We show that although the metric obtains good results on small and carefully curated data with both manually and automatically selected substitutes, it achieves medium performance on much larger and noisier datasets, demonstrating the limits of the metric for tuning and evaluation of current MT systems.", "num_citations": "9\n", "authors": ["2070"]}
{"title": "Crowdsourcing for NLP\n", "abstract": " Crowdsourced applications to scientific problems is a hot research area, with over 10,000 publications in the past five years. Platforms such as Amazons Mechanical Turk and CrowdFlower provide researchers with easy access to large numbers of workers. The crowds vast supply of inexpensive, intelligent labor allows people to attack problems that were previously impractical and gives potential for detailed scientific inquiry of social, psychological, economic, and linguistic phenomena via massive sample sizes of human annotated data. We introduce crowdsourcing and describe how it is being used in both industry and academia. Crowdsourcing is valuable to computational linguists both (a) as a source of labeled training data for use in machine learning and (b) as a means of collecting computational social science data that link language use to underlying beliefs and behavior. We present case studies for both categories:(a) collecting labeled data for use in natural language processing tasks such as word sense disambiguation and machine translation and (b) collecting experimental data in the context of psychology; eg finding how word use varies with age, sex, personality, health, and happiness.We will also cover tools and techniques for crowdsourcing. Effectively collecting crowdsourced data requires careful attention to the collection process, through selection of appropriately qualified workers, giving clear instructions that are understandable to non-? experts, and performing quality control on the results to eliminate spammers who complete tasks randomly or carelessly in order to", "num_citations": "9\n", "authors": ["2070"]}
{"title": "Using comparable corpora to adapt mt models to new domains\n", "abstract": " In previous work we showed that when using an SMT model trained on old-domain data to translate text in a new-domain, most errors are due to unseen source words, unseen target translations, and inaccurate translation model scores (Irvine et al., 2013a). In this work, we target errors due to inaccurate translation model scores using new-domain comparable corpora, which we mine from Wikipedia. We assume that we have access to a large olddomain parallel training corpus but only enough new-domain parallel data to tune model parameters and do evaluation. We use the new-domain comparable corpora to estimate additional feature scores over the phrase pairs in our baseline models. Augmenting models with the new features improves the quality of machine translations in the medical and science domains by up to 1.3 BLEU points over very strong baselines trained on the 150 million word Canadian Hansard dataset.", "num_citations": "9\n", "authors": ["2070"]}
{"title": "Crowdsourcing for grammatical error correction\n", "abstract": " We discuss the problem of grammatical error correction, which has gained attention for its usefulness both in the development of tools for learners of foreign languages and as a component of statistical machine translation systems. We believe the task of suggesting grammar and style corrections in writing is well suited to a crowdsourcing solution but is currently hindered by the difficulty of automatic quality control. In this proposal, we motivate the problem of grammatical error correction and outline the challenges of ensuring quality in a setting where traditional methods of aggregation (eg majority vote) fail to produce the desired results. We then propose a design for quality control and present preliminary results indicating the potential of crowd workers to provide a scalable solution.", "num_citations": "9\n", "authors": ["2070"]}
{"title": "Learning to translate with products of novices: a suite of open-ended challenge problems for teaching MT\n", "abstract": " Machine translation (MT) draws from several different disciplines, making it a complex subject to teach. There are excellent pedagogical texts, but problems in MT and current algorithms for solving them are best learned by doing. As a centerpiece of our MT course, we devised a series of open-ended challenges for students in which the goal was to improve performance on carefully constrained instances of four key MT tasks: alignment, decoding, evaluation, and reranking. Students brought a diverse set of techniques to the problems, including some novel solutions which performed remarkably well. A surprising and exciting outcome was that student solutions or their combinations fared competitively on some tasks, demonstrating that even newcomers to the field can help improve the state-of-the-art on hard NLP problems while simultaneously learning a great deal. The problems, baseline code, and results are\u00a0\u2026", "num_citations": "9\n", "authors": ["2070"]}
{"title": "Deduplicating training data makes language models better\n", "abstract": " We find that existing language modeling datasets contain many near-duplicate examples and long repetitive substrings. As a result, over 1% of the unprompted output of language models trained on these datasets is copied verbatim from the training data. We develop two tools that allow us to deduplicate training datasets -- for example removing from C4 a single 61 word English sentence that is repeated over 60,000 times. Deduplication allows us to train models that emit memorized text ten times less frequently and require fewer train steps to achieve the same or better accuracy. We can also reduce train-test overlap, which affects over 4% of the validation set of standard datasets, thus allowing for more accurate evaluation. We release code for reproducing our work and performing dataset deduplication at https://github.com/google-research/deduplicate-text-datasets.", "num_citations": "8\n", "authors": ["2070"]}
{"title": "GooAQ: Open Question Answering with Diverse Answer Types\n", "abstract": " While day-to-day questions come with a variety of answer types, the current question-answering (QA) literature has failed to adequately address the answer diversity of questions. To this end, we present GooAQ, a large-scale dataset with a variety of answer types. This dataset contains over 5 million questions and 3 million answers collected from Google. GooAQ questions are collected semi-automatically from the Google search engine using its autocomplete feature. This results in naturalistic questions of practical interest that are nonetheless short and expressed using simple language. GooAQ answers are mined from Google's responses to our collected questions, specifically from the answer boxes in the search results. This yields a rich space of answer types, containing both textual answers (short and long) as well as more structured ones such as collections. We benchmarkT5 models on GooAQ and observe that: (a) in line with recent work, LM's strong performance on GooAQ's short-answer questions heavily benefit from annotated data; however, (b) their quality in generating coherent and accurate responses for questions requiring long responses (such as 'how' and 'why' questions) is less reliant on observing annotated data and mainly supported by their pre-training. We release GooAQ to facilitate further research on improving QA with diverse response types.", "num_citations": "8\n", "authors": ["2070"]}
{"title": "Chateval: A tool for the systematic evaluation of chatbots\n", "abstract": " Open-domain dialog systems are difficult to evaluate. The current best practice for analyzing and comparing these dialog systems is the use of human judgments. However, the lack of standardization in evaluation procedures, and the fact that model parameters and code are rarely published hinder systematic human evaluation experiments. We introduce a unified framework for human evaluation of chatbots that augments existing chatbot tools, and provides a web-based hub for researchers to share and compare their dialog systems. Researchers can submit their trained models to the ChatEval web interface and obtain comparisons with baselines and prior work. The evaluation code is open-source to ensure evaluation is performed in a standardized and transparent way. In addition, we introduce open-source baseline models and evaluation datasets. ChatEval can be found at https://chateval. org.", "num_citations": "8\n", "authors": ["2070"]}
{"title": "Evaluating question answering systems using FAQ answer injection\n", "abstract": " Question answering (NLQA) systems which retrieve a textual fragment from a document collection that represents the answer to a question are an active field of research.", "num_citations": "8\n", "authors": ["2070"]}
{"title": "RESIN: A Dockerized Schema-Guided Cross-document Cross-lingual Cross-media Information Extraction and Event Tracking System\n", "abstract": " We present a new information extraction system that can automatically construct temporal event graphs from a collection of news documents from multiple sources, multiple languages (English and Spanish for our experiment), and multiple data modalities (speech, text, image and video). The system advances state-of-the-art from two aspects:(1) extending from sentence-level event extraction to cross-document cross-lingual cross-media event extraction, coreference resolution and temporal event tracking;(2) using human curated event schema library to match and enhance the extraction output. We have made the dockerlized system publicly available for research purpose at GitHub, with a demo video.", "num_citations": "7\n", "authors": ["2070"]}
{"title": "Learning antonyms with paraphrases and a morphology-aware neural network\n", "abstract": " Recognizing and distinguishing antonyms from other types of semantic relations is an essential part of language understanding systems. In this paper, we present a novel method for deriving antonym pairs using paraphrase pairs containing negation markers. We further propose a neural network model, AntNET, that integrates morphological features indicative of antonymy into a path-based relation detection algorithm. We demonstrate that our model outperforms state-of-the-art models in distinguishing antonyms from other semantic relations and is capable of efficiently handling multi-word expressions.", "num_citations": "7\n", "authors": ["2070"]}
{"title": "Tense manages to predict implicative behavior in verbs\n", "abstract": " Implicative verbs (eg manage) entail their complement clauses, while non-implicative verbs (eg want) do not. For example, while managing to solve the problem entails solving the problem, no such inference follows from wanting to solve the problem. Differentiating between implicative and non-implicative verbs is therefore an essential component of natural language understanding, relevant to applications such as textual entailment and summarization. We present a simple method for predicting implicativeness which exploits known constraints on the tense of implicative verbs and their complements. We show that this yields an effective, data-driven way of capturing this nuanced property in verbs.", "num_citations": "7\n", "authors": ["2070"]}
{"title": "Continuous space discriminative language modeling\n", "abstract": " Discriminative language modeling is a structured classification problem. Log-linear models have been previously used to address this problem. In this paper, the standard dot-product feature representation used in log-linear models is replaced by a non-linear function parameterized by a neural network. Embeddings are learned for each word and features are extracted automatically through the use of convolutional layers. Experimental results show that as a stand-alone model the continuous space model yields significantly lower word error rate (1% absolute), while having a much more compact parameterization (60%-90% smaller). If the baseline scores are combined, our approach performs equally well.", "num_citations": "7\n", "authors": ["2070"]}
{"title": "Artificial Intelligence in mental health and the biases of language based models\n", "abstract": " Background The rapid integration of Artificial Intelligence (AI) into the healthcare field has occurred with little communication between computer scientists and doctors. The impact of AI on health outcomes and inequalities calls for health professionals and data scientists to make a collaborative effort to ensure historic health disparities are not encoded into the future. We present a study that evaluates bias in existing Natural Language Processing (NLP) models used in psychiatry and discuss how these biases may widen health inequalities. Our approach systematically evaluates each stage of model development to explore how biases arise from a clinical, data science and linguistic perspective.   Design/Methods A literature review of the uses of NLP in mental health was carried out across multiple disciplinary databases with defined Mesh terms and keywords. Our primary analysis evaluated biases within \u2018GloVe\u2019 and \u2018Word2Vec\u2019 word embeddings. Euclidean distances were measured to assess relationships between psychiatric terms and demographic labels, and vector similarity functions were used to solve analogy questions relating to mental health.   Results Our primary analysis of mental health terminology in GloVe and Word2Vec embeddings demonstrated significant biases with respect to religion, race, gender, nationality, sexuality and age. Our literature review returned 52 papers, of which none addressed all the areas of possible bias that we identify in model development. In addition, only one article existed on more than one research database, demonstrating the isolation of research within disciplinary silos and inhibiting cross\u00a0\u2026", "num_citations": "6\n", "authors": ["2070"]}
{"title": "RoFT: A tool for evaluating human detection of machine-generated text\n", "abstract": " In recent years, large neural networks for natural language generation (NLG) have made leaps and bounds in their ability to generate fluent text. However, the tasks of evaluating quality differences between NLG systems and understanding how humans perceive the generated text remain both crucial and difficult. In this system demonstration, we present Real or Fake Text (RoFT), a website that tackles both of these challenges by inviting users to try their hand at detecting machine-generated text in a variety of domains. We introduce a novel evaluation task based on detecting the boundary at which a text passage that starts off human-written transitions to being machine-generated. We show preliminary results of using RoFT to evaluate detection of machine-generated news articles.", "num_citations": "6\n", "authors": ["2070"]}
{"title": "Evaluating evaluation lessons from the WMT 2007 shared task\n", "abstract": " \u2022 Method: 1. automatically word-align source with reference and system translations 2. parse source sentence3. select constituents to be judged 4. highlight source phrase and corresponding target phrases 5. rank those", "num_citations": "6\n", "authors": ["2070"]}
{"title": "A natural language question and answer system\n", "abstract": " Our project is a question and answer system that allows natural language question to be asked of a knowledge base of information. Our program is grammar-based system which maps English questions and statements onto predicate logic. User input is parsed, converted to KIF (Knowledge Interchange Format), and sent to a reasoner that does inference using a set of first order logic axioms and returns a solution. In order to implement this program we developed a large scale grammar for English questions based on recent work in Head-driven Phrase Structure Grammar. Highlights of our grammar include the facts that it:", "num_citations": "6\n", "authors": ["2070"]}
{"title": "Comparing constraints for taxonomic organization\n", "abstract": " Building a taxonomy from the ground up involves several sub-tasks: selecting terms to include, predicting semantic relations between terms, and selecting a subset of relational instances to keep, given constraints on the taxonomy graph. Methods for this final step\u2013taxonomic organization\u2013vary both in terms of the constraints they impose, and whether they enable discovery of synonymous terms. It is hard to isolate the impact of these factors on the quality of the resulting taxonomy because organization methods are rarely compared directly. In this paper, we present a head-to-head comparison of six taxonomic organization algorithms that vary with respect to their structural and transitivity constraints, and treatment of synonymy. We find that while transitive algorithms out-perform their non-transitive counterparts, the top-performing transitive algorithm is prohibitively slow for taxonomies with as few as 50 entities. We propose a simple modification to a non-transitive optimum branching algorithm to explicitly incorporate synonymy, resulting in a method that is substantially faster than the best transitive algorithm while giving complementary performance.", "num_citations": "5\n", "authors": ["2070"]}
{"title": "Introducing nieuw: Novel incentives and workflows for eliciting linguistic data\n", "abstract": " This paper introduces the NIEUW (Novel Incentives and Workflows) project funded by the United States National Science Foundation and part of the Linguistic Data Consortium\u2019s strategy to provide order of magnitude improvement in the scale, cost, variety, linguistic diversity and quality of Language Resources available for education, research and technology development. Notwithstanding decades of effort and progress in collecting and distributing Language Resources, it remains the case that demand still far exceeds supply for all of the approximately 7000 languages in the world, even the most well documented languages with global economic and political influence. The absence of Language Resources, regardless of the language, stifles teaching and technology building, inhibiting the creation of language enabled applications and, as a result, commerce and communication. Project oriented approaches which focus intensive funding and effort on problems of limited scope over short durations can only address part of the problem. The HLT community instead requires approaches that do not rely upon highly constrained resources such as project funding and can be sustained across many languages and many years. In this paper, we describe a new initiative to harness the power of alternative incentives to elicit linguistic data and annotation. We also describe changes to the workflows necessary to collect data from workforces attracted by these incentives.", "num_citations": "5\n", "authors": ["2070"]}
{"title": "The gun violence database\n", "abstract": " We describe the Gun Violence Database (GVDB), a large and growing database of gun violence incidents in the United States. The GVDB is built from the detailed information found in local news reports about gun violence, and is constructed via a large-scale crowdsourced annotation effort through our web site, http://gun-violence.org/. We argue that centralized and publicly available data about gun violence can facilitate scientific, fact-based discussion about a topic that is often dominated by politics and emotion. We describe our efforts to automate the construction of the database using state-of-the-art natural language processing (NLP) technologies, eventually enabling a fully-automated, highly-scalable resource for research on this important public health problem.", "num_citations": "5\n", "authors": ["2070"]}
{"title": "Automatically scoring freshman writing: A preliminary investigation\n", "abstract": " In this work, we explore applications of automatic essay scoring (AES) to a corpus of essays written by college freshmen and discuss the challenges we faced. While most AES systems evaluate highly constrained writing, we developed a system that handles open-ended, long-form writing. We present a novel corpus for this task, containing more than 3,000 essays and drafts written for a freshman writing course. We describe statistical analysis of the corpus and identify problems with automatically scoring this type of data. Finally, we demonstrate how to overcome grader bias by using a multi-task setup, and predict scores as well as human graders on a different dataset. Finally, we discuss how AES can help teachers assign more uniform grades.", "num_citations": "5\n", "authors": ["2070"]}
{"title": "PARADIGM: Paraphrase diagnostics through grammar matching\n", "abstract": " Paraphrase evaluation is typically done either manually or through indirect, taskbased evaluation. We introduce an intrinsic evaluation PARADIGM which measures the goodness of paraphrase collections that are represented using synchronous grammars. We formulate two measures that evaluate these paraphrase grammars using gold standard sentential paraphrases drawn from a monolingual parallel corpus. The first measure calculates how often a paraphrase grammar is able to synchronously parse the sentence pairs in the corpus. The second measure enumerates paraphrase rules from the monolingual parallel corpus and calculates the overlap between this reference paraphrase collection and the paraphrase resource being evaluated. We demonstrate the use of these evaluation metrics on paraphrase collections derived from three different data types: multiple translations of classic French novels, comparable sentence pairs drawn from different newspapers, and bilingual parallel corpora. We show that PARADIGM correlates with human judgments more strongly than BLEU on a task-based evaluation of paraphrase quality.", "num_citations": "5\n", "authors": ["2070"]}
{"title": "CSLDAMT'10: Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon's Mechanical Turk\n", "abstract": " The NAACL-2010Workshop on Creating Speech and Language DataWith Amazon's Mechanical Turk explores applications of crowdsourcing technologies for the creation and study of language data. Recent work has evaluated the effectiveness of using crowdsourcing platforms, such as Amazon's Mechanical Turk, to create annotated data for natural language processing applications. This workshop further explores this area and these proceedings contain 34 papers and an overview paper that each experiment with applications of Mechanical Turk. The diversity of applications showcases the new possibilities for annotating speech and text, and has the potential to dramatically change how we create data for human language technologies.", "num_citations": "5\n", "authors": ["2070"]}
{"title": "Upping the Ante for\u201d Best of Breed\u201d Machine Translation Providers\n", "abstract": " The notion of \u201cbest of breed\u201d among value-added machine translation technology providers is generally defined as providing access to the single best commercially available machine translation engine for each language pair. This paper describes the efforts of Amikai, Inc. to go beyond that definition of best of breed. Rather than relying on a single engine for each pair, we have written a program that automatically selects the best translation from a set of candidate translations generated by multiple commercial machine translation engines. The program is implemented using a simple statistical language modelling technique, and relies on the simplifying assumption that the most fluent item in the set is the best translation. The program was able to produce the best translation in human ranked data up to 19% more often than the single best performing engine.", "num_citations": "5\n", "authors": ["2070"]}
{"title": "A recipe for arbitrary text style transfer with large language models\n", "abstract": " In this paper, we leverage large language models (LMs) to perform zero-shot text style transfer. We present a prompting method that we call augmented zero-shot learning, which frames style transfer as a sentence rewriting task and requires only a natural language instruction, without model fine-tuning or exemplars in the target style. Augmented zero-shot learning is simple and demonstrates promising results not just on standard style transfer tasks such as sentiment, but also on arbitrary transformations such as \"make this melodramatic\" or \"insert a metaphor.\"", "num_citations": "4\n", "authors": ["2070"]}
{"title": "Cultural and Geographical Influences on Image Translatability of Words across Languages\n", "abstract": " Neural Machine Translation (NMT) models have been observed to produce poor translations when there are few/no parallel sentences to train the models. In the absence of parallel data, several approaches have turned to the use of images to learn translations. Since images of words, eg, horse may be unchanged across languages, translations can be identified via images associated with words in different languages that have a high degree of visual similarity. However, translating via images has been shown to improve upon text-only models only marginally. To better understand when images are useful for translation, we study image translatability of words, which we define as the translatability of words via images, by measuring intra-and inter-cluster similarities of image representations of words that are translations of each other. We find that images of words are not always invariant across languages, and that language pairs with shared culture, meaning having either a common language family, ethnicity or religion, have improved image translatability (ie, have more similar images for similar words) compared to its converse, regardless of their geographic proximity. In addition, in line with previous works that show images help more in translating concrete words, we found that concrete words have improved image translatability compared to abstract ones.", "num_citations": "4\n", "authors": ["2070"]}
{"title": "Mapping the paraphrase database to wordnet\n", "abstract": " WordNet has facilitated important research in natural language processing but its usefulness is somewhat limited by its relatively small lexical coverage. The Paraphrase Database (PPDB) covers 650 times more words, but lacks the semantic structure of WordNet that would make it more directly useful for downstream tasks. We present a method for mapping words from PPDB to WordNet synsets with 89% accuracy. The mapping also lays important groundwork for incorporating WordNet\u2019s relations into PPDB so as to increase its utility for semantic reasoning in applications.", "num_citations": "4\n", "authors": ["2070"]}
{"title": "Discriminative bilingual lexicon induction\n", "abstract": " Bilingual lexicon induction is the task of inducing word translations from monolingual corpora in two languages. We introduce a novel discriminative approach to bilingual lexicon induction. Our discriminative model is capable of combining a wide variety of features, which individually provide only weak indications of translation equivalence. When feature weights are discriminatively set, these signals produce dramatically higher translation quality than previous approaches that combined signals in an unsupervised fashion (eg using minimum reciprocal rank). We present experiments on a wide range of languages and data sizes. We examine translation into English from 25 foreign languages: Albanian, Azeri, Bengali, Bosnian, Bulgarian, Cebuano, Gujarati, Hindi, Hungarian, Indonesian, Latvian, Nepali, Romanian, Serbian, Slovak, Somali, Spanish, Swedish, Tamil, Telugu, Turkish, Ukrainian, Uzbek, Vietnamese and Welsh. Rather than testing solely on high frequency words, as previous research has done, we test on low frequency as well, so that our results are more relevant to statistical machine translation, where systems typically lack translations of rare words that fall outside of their training data. We systematically explore a wide range of features and phenomena that affect the quality of the translations discovered by bilingual lexicon induction. We give illustrative examples of the highest ranking translations for orthogonal signals of translation equivalence like contextual similarity and temporal similarity. We analyze the effects of frequency and burstiness, and the sizes of the seed bilingual dictionaries and the monolingual training corpora. We\u00a0\u2026", "num_citations": "4\n", "authors": ["2070"]}
{"title": "Proceedings of the Second Workshop on Statistical Machine Translation\n", "abstract": " The ACL 2007 Workshop on Statistical Machine Translation (WMT-07) took place on Saturday, June 23 in Prague, Czech Republic, immediately preceding the annual meeting of the Association for Computational Linguistics, which was hosted by Charles University. This was the second time this workshop had been held, following the first workshop at the 2006 HLT-NAACL conference. But its ancestry can be traced back farther to the ACL 2005 Workshop on Building and Using Parallel Texts (when we started our evaluation campaign on European languages), and even the ACL 2001 Workshop on Data-Driven Machine Translation (which was the first ACL workshop mostly directed at statistical machine translation).Over the last years, interest in statistical machine translation has been risen dramatically. We received an overwhelming number of full paper submission for a one-day workshop, 38 in total. Given our limited capacity, we were only able to accept 12 full papers for oral presentation and 9 papers for poster presentation, an acceptance rate of 55%. In a second poster session, 16 additional shared task papers were presented. The workshop also featured an invited talk by Jean Senellart of SYSTRAN Language Translation Technology, Paris.", "num_citations": "4\n", "authors": ["2070"]}
{"title": "Intent Detection with WikiHow\n", "abstract": " Modern task-oriented dialog systems need to reliably understand users' intents. Intent detection is most challenging when moving to new domains or new languages, since there is little annotated data. To address this challenge, we present a suite of pretrained intent detection models. Our models are able to predict a broad range of intended goals from many actions because they are trained on wikiHow, a comprehensive instructional website. Our models achieve state-of-the-art results on the Snips dataset, the Schema-Guided Dialogue dataset, and all 3 languages of the Facebook multilingual dialog datasets. Our models also demonstrate strong zero- and few-shot performance, reaching over 75% accuracy using only 100 training examples in all datasets.", "num_citations": "3\n", "authors": ["2070"]}
{"title": "Winter is here: Summarizing twitter streams related to pre-scheduled events\n", "abstract": " Pre-scheduled events, such as TV shows and sports games, usually garner considerable attention from the public. Twitter captures large volumes of discussions and messages related to these events, in real-time. Twitter streams related to pre-scheduled events are characterized by the following:(1) spikes in the volume of published tweets reflect the highlights of the event and (2) some of the published tweets make reference to the characters involved in the event, in the context in which they are currently portrayed in a subevent. In this paper, we take advantage of these characteristics to identify the highlights of pre-scheduled events from tweet streams and we demonstrate a method to summarize these highlights. We evaluate our algorithm on tweets collected around 2 episodes of a popular TV show, Game of Thrones, Season 7.", "num_citations": "3\n", "authors": ["2070"]}
{"title": "Large-Scale Paraphrasing for Natural Language Understanding\n", "abstract": " In this project, we researched and developed technologies to automatically extract large-volumes of paraphrases to aid in natural language understanding NLU tasks. We developed three core algorithms to 1 generate extremely large paraphrase databases, and 2 adapt paraphrase databases to new domains, and 3 augment paraphrase rules with fine-grained semantic entailment relations. Our work introduced the paraphrase database PPDB, the largest paraphrase resource developed to date. The resource contains over 100 million paraphrases for English. We generated paraphrase databases for 23 foreign languages.Descriptors:", "num_citations": "3\n", "authors": ["2070"]}
{"title": "Poetry of the crowd: A human computation algorithm to convert prose into rhyming verse\n", "abstract": " Poetry composition is a very complex task that requires a poet to satisfy multiple constraints concurrently. We believe that the task can be augmented by combining the creative abilities of humans with computational algorithms that efficiently constrain and permute available choices. We present a hybrid method for generating poetry from prose that combines crowdsourcing with natural language processing (NLP) machinery. We test the ability of crowd workers to accomplish the technically challenging and creative task of composing poems.", "num_citations": "3\n", "authors": ["2070"]}
{"title": "Annotation guidelines for paraphrase alignment\n", "abstract": " You will be given pairs of sentences which are paraphrases of each other because they convey the same meaning but are worded differently. Your task will be to show which parts of the sentences are in correspondence by aligning them on a word-by-word basis. Here is an example of an alignment that we would like you to produce: and him impeach towant some. down step tohim expect others", "num_citations": "3\n", "authors": ["2070"]}
{"title": "MINDS workshops machine translation working group final report\n", "abstract": " This report is one of five reports that were based on the MINDS workshops, led by Donna Harman (NIST) and sponsored by Heather McCallum-Bayliss of the Disruptive Technology Office of the Office of the Director of National Intelligence's Office of Science and Technology (ODNI/ADDNI/S&T/DTO). To find the rest of the reports, and an executive overview, please see http://www. itl. nist. gov/iaui/894.02/minds. html.", "num_citations": "3\n", "authors": ["2070"]}
{"title": "Searchable translation memories\n", "abstract": " In this paper we introduce a technique for creating searchable translation memories. Linear B\u2019s searchable translation memories allow a translator to type in a phrase and retrieve a ranked list of possible translations for that phrase, which is ordered based on the likelihood of the translations. The searchable translation memories use translation models similar to those used in statistical machine translation. In this paper we first describe the technical details of how the TMs are indexed and how translations are assigned probabilities, and then evaluate a searchable TM using precision and recall metrics.", "num_citations": "3\n", "authors": ["2070"]}
{"title": "A computer model of a grammar for English questions\n", "abstract": " This document describes my senior honors project, which is an implementation of a grammar for English questions. I have created a computer model of Ginzburg and Sag\u2019s theory of English interrogative constructions using the parsing software developed at the Center for Study of Language and Information (CSLI). In this chapter I describe the LKB parsing software, give instructions on downloading the system, and comment on the process of grammar engineering. The next chapter gives a summary of Ginzburg and Sag (2000). Chapter 3 details the discrepancies between the Ginzburg and Sag theory and my implementation. Chapter 4 provides a detailed discussion of a set of key example sentences. The appendices contain tables describing all the grammar constructions, lexical rules, types, and example lexical entries used in my implementation.", "num_citations": "3\n", "authors": ["2070"]}
{"title": "Goal-oriented script construction\n", "abstract": " The knowledge of scripts, common chains of events in stereotypical scenarios, is a valuable asset for task-oriented natural language understanding systems. We propose the Goal-Oriented Script Construction task, where a model produces a sequence of steps to accomplish a given goal. We pilot our task on the first multilingual script learning dataset supporting 18 languages collected from wikiHow, a website containing half a million how-to articles. For baselines, we consider both a generation-based approach using a language model and a retrieval-based approach by first retrieving the relevant steps from a large candidate pool and then ordering them. We show that our task is practical, feasible but challenging for state-of-the-art Transformer models, and that our methods can be readily deployed for various other datasets and domains with decent zero-shot performance.", "num_citations": "2\n", "authors": ["2070"]}
{"title": "Simple-QE: Better Automatic Quality Estimation for Text Simplification\n", "abstract": " Text simplification systems generate versions of texts that are easier to understand for a broader audience. The quality of simplified texts is generally estimated using metrics that compare to human references, which can be difficult to obtain. We propose Simple-QE, a BERT-based quality estimation (QE) model adapted from prior summarization QE work, and show that it correlates well with human quality judgments. Simple-QE does not require human references, which makes the model useful in a practical setting where users would need to be informed about the quality of generated simplifications. We also show that we can adapt this approach to accurately predict the complexity of human-written texts.", "num_citations": "2\n", "authors": ["2070"]}
{"title": "Constructing an alias list for named entities during an event\n", "abstract": " In certain fields, real-time knowledge from events can help in making informed decisions. In order to extract pertinent real-time knowledge related to an event, it is important to identify the named entities and their corresponding aliases related to the event. The problem of identifying aliases of named entities that spike has remained unexplored. In this paper, we introduce an algorithm, EntitySpike, that identifies entities that spike in popularity in tweets from a given time period, and constructs an alias list for these spiked entities. EntitySpike uses a temporal heuristic to identify named entities with similar context that occur in the same time period (within minutes) during an event. Each entity is encoded as a vector using this temporal heuristic. We show how these entity-vectors can be used to create a named entity alias list. We evaluated our algorithm on a dataset of temporally ordered tweets from a single event, the 2013 Grammy Awards show. We carried out various experiments on tweets that were published in the same time period and show that our algorithm identifies most entity name aliases and outperforms a competitive baseline.", "num_citations": "2\n", "authors": ["2070"]}
{"title": "The American Local News Corpus.\n", "abstract": " Abstract We present the American Local News Corpus (ALNC), containing over 4 billion words of text from 2, 652 online newspapers in the United States. Each article in the corpus is associated with a timestamp, state, and city. All 50 US states and 1, 924 cities are represented. We detail our method for taking daily snapshots of thousands of local and national newspapers and present two example corpus analyses. The first explores how different sports are talked about over time and geography. The second compares per capita murder rates with news coverage of murders across the 50 states. The ALNC is about the same size as the Gigaword corpus and is growing continuously. Version 1.0 is available for research use.", "num_citations": "2\n", "authors": ["2070"]}
{"title": "Bilingual lexicon induction for low-resource languages\n", "abstract": " CiteSeerX \u2014 Bilingual Lexicon Induction for Low-resource Languages Documents Authors Tables Log in Sign up MetaCart DMCA Donate CiteSeerX logo Documents: Advanced Search Include Citations Authors: Advanced Search Include Citations Tables: DMCA Bilingual Lexicon Induction for Low-resource Languages (2010) Cached Download as a PDF Download Links [hltcoe.files.wordpress.com] [hltcoe.jhu.edu] Save to List Add to Collection Correct Errors Monitor Changes by Alexandre Klementiev , Chris Callison-Burch , Ann Irvine Summary Citations Active Bibliography Co-citation Clustered Documents Version History Share Facebook Twitter Reddit Bibsonomy OpenURL Abstract Keyphrases bilingual lexicon induction low-resource language Powered by: Apache Solr About CiteSeerX Submit and Index Documents Privacy Policy Help Data Source Contact Us Developed at and hosted by The College of \u2026", "num_citations": "2\n", "authors": ["2070"]}
{"title": "Proceedings of the Third Workshop on Statistical Machine Translation\n", "abstract": " The ACL 2008 Workshop on Statistical Machine Translation (WMT-08) took place on Thursday, June 19 in Columbus, Ohio, United States, immediately following the annual meeting of the Association for Computational Linguistics, which was hosted by the Ohio State University.This is the third time this workshop has been held. It has its root in the ACL 2005 Workshop on Building and Using Parallel Texts In the following years the Workshop on Statistical Machine Translation was held at HLT-NAACL 2006 in New York City, US, and at ACL 2007 in Prague, Czech Republic.", "num_citations": "2\n", "authors": ["2070"]}
{"title": "Visual Goal-Step Inference using wikiHow\n", "abstract": " Procedural events can often be thought of as a high level goal composed of a sequence of steps. Inferring the sub-sequence of steps of a goal can help artificial intelligence systems reason about human activities. Past work in NLP has examined the task of goal-step inference for text. We introduce the visual analogue. We propose the Visual Goal-Step Inference (VGSI) task where a model is given a textual goal and must choose a plausible step towards that goal from among four candidate images. Our task is challenging for state-of-the-art muitimodal models. We introduce a novel dataset harvested from wikiHow that consists of 772,294 images representing human actions. We show that the knowledge learned from our data can effectively transfer to other datasets like HowTo100M, increasing the multiple-choice accuracy by 15% to 20%. Our task will facilitate multi-modal reasoning about procedural events.", "num_citations": "1\n", "authors": ["2070"]}
{"title": "The CLASSE GATOR (CLinical Acronym SenSE disambiGuATOR): A Method for predicting acronym sense from neonatal clinical notes\n", "abstract": " ObjectiveTo develop an algorithm for identifying acronym \u2018sense\u2019 from clinical notes without requiring a clinically annotated training set.Materials and MethodsOur algorithm is called CLASSE GATOR: Clinical Acronym SenSE disambiGuATOR. CLASSE GATOR extracts acronyms and definitions from PubMed Central (PMC). A logistic regression model is trained using words associated with specific acronym-definition pairs from PMC. CLASSE GATOR uses this library of acronym-definitions and their corresponding word feature vectors to predict the acronym \u2018sense\u2019 from Beth Israel Deaconess (MIMIC-III) neonatal notes.ResultsWe identified 1,257 acronyms and 8,287 definitions including a random definition from 31,764 PMC articles on prenatal exposures and 2,227,674 PMC open access articles. The average number of senses (definitions) per acronym was 6.6 (min = 2, max = 50). The average internal 5-fold cross\u00a0\u2026", "num_citations": "1\n", "authors": ["2070"]}
{"title": "Turkish Judge: A Peer Evaluation Framework for Crowd Work Appeals\n", "abstract": " We present our work in progress platform Turkish Judge, a crowd-driven adjudication system for rejected work on Amazon Mechanical Turk. The Mechanical Turk crowdsourcing platform allows Requesters to approve or reject assignments submitted by Workers. If the work is rejected, then Workers aren\u2019t paid, and their reputation suffers. Currently, there is no built-in mechanism for Workers to appeal rejections, other than contacting Requesters directly. The time it takes Requesters to review potentially incorrectly rejected tasks means that their costs are substantially higher than the payment amount that is in dispute. As a solution to this issue, we present an automated appeals system called Turkish Judge which employs crowd workers as judges to adjudicate whether work was fairly rejected when their peers initiate an appeal.", "num_citations": "1\n", "authors": ["2070"]}
{"title": "Bilingual is At Least Monolingual (BALM): A Novel Translation Algorithm that Encodes Monolingual Priors\n", "abstract": " State-of-the-art machine translation (MT) models do not use knowledge of any single language's structure; this is the equivalent of asking someone to translate from English to German while knowing neither language. BALM is a framework incorporates monolingual priors into an MT pipeline; by casting input and output languages into embedded space using BERT, we can solve machine translation with much simpler models. We find that English-to-German translation on the Multi30k dataset can be solved with a simple feedforward network under the BALM framework with near-SOTA BLEU scores.", "num_citations": "1\n", "authors": ["2070"]}
{"title": "Anonymization of Sensitive Information in Medical Health Records.\n", "abstract": " Due to privacy constraints, clinical records with protected health information (PHI) cannot be directly shared. De-identification, ie, the exhaustive removal, or replacement, of all mentioned PHI phrases has to be performed before making the clinical records available outside of hospitals. We have tried to identify PHI on medical records written in Spanish language. We applied two approaches for the anonymization of medical records in this paper. In the first approach, we gathered various token-level features and built a LinearSVC model which gave us F1 score of 0.861 on test data. In the other approach, we built a neural network involving an LSTM-CRF model which gave us a higher F1 score of 0.935 which is an improvement over the first approach.", "num_citations": "1\n", "authors": ["2070"]}
{"title": "Extracting structured information via automatic+ human computation\n", "abstract": " We present a system for extracting structured information from unstructured text using a combination of information retrieval, natural language processing, machine learning, and crowdsourcing. We test our pipeline by building a structured database of gun violence incidents in the United States. The results of our pilot study demonstrate that the proposed methodology is a viable way of collecting large-scale, up-to-date data for public health, public policy, and social science research.", "num_citations": "1\n", "authors": ["2070"]}
{"title": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing\n", "abstract": " Welcome to the 2015 Conference on Empirical Methods in Natural Language Processing. EMNLP is annually organized by SIGDAT, the Association for Computational Linguistics\u2019 special interest group on linguistic data and corpus-based approaches to NLP. This year the conference will be held on September 17\u201321 in the enchanting city of Lisbon, Portugal. 1EMNLP has continued to increase in prominence as one of the most important conferences in Natural Language Processing (NLP). This year the conference has experienced an unprecedented boost in submitted papers. I believe that this reflects both the growth of the NLP field and also the health and strength of the conference itself, with a history of many years of solid work. With this level of interest at submission time, we are also expecting a record attendance. The conference will span a five-day period this year, and it requires a growing organization structure.", "num_citations": "1\n", "authors": ["2070"]}
{"title": "Proceedings of the Eighth Workshop on Statistical Machine Translation\n", "abstract": " The ACL 2013 Workshop on Statistical Machine Translation (WMT 2013) took place on Thursday and Friday, August 8\u20139, 2013 in Sofia, Bulgaria, immediately following the Conference of the Association for Computational Linguistics (ACL).", "num_citations": "1\n", "authors": ["2070"]}
{"title": "Deriving conversation-based features from unlabeled speech for discriminative language modeling\n", "abstract": " The perceptron algorithm was used in [1] to estimate discriminative language models which correct errors in the output of ASR systems. In its simplest version, the algorithm simply increases the weight of n-gram features which appear in the correct (oracle) hypothesis and decreases the weight of n-gram features which appear in the 1-best hypothesis. In this paper, we show that the perceptron algorithm can be successfully used in a semi-supervised learning (SSL) framework, where limited amounts of labeled data are available. Our framework has some similarities to graph-based label propagation [2] in the sense that a graph is built based on proximity of unlabeled conversations, and then it is used to propagate confidences (in the form of features) to the labeled data, based on which perceptron trains a discriminative model. The novelty of our approach lies in the fact that the confidence \u201cflows\u201d from the unlabeled\u00a0\u2026", "num_citations": "1\n", "authors": ["2070"]}
{"title": "Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon\u2019s Mechanical Turk\n", "abstract": " The NAACL-2010 Workshop on Creating Speech and Language Data With Amazon\u2019s Mechanical Turk explores applications of crowdsourcing technologies for the creation and study of language data. Recent work has evaluated the effectiveness of using crowdsourcing platforms, such as Amazon\u2019s Mechanical Turk, to create annotated data for natural language processing applications. This workshop further explores this area and these proceedings contain 34 papers and an overview paper that each experiment with applications of Mechanical Turk. The diversity of applications showcases the new possibilities for annotating speech and text, and has the potential to dramatically change how we create data for human language technologies.Papers in the workshop also looked at best practices in creating data using Mechanical Turk. Experiments evaluated how to design Human Intelligence Tasks (HITs), how to attract users to the task, how to price annotation tasks, and how to ensure data quality. Applications include the creation of data sets for standard NLP tasks, developing entirely new tasks, and investigating new ways of integrating user feedback in the learning process.", "num_citations": "1\n", "authors": ["2070"]}
{"title": "Proceedings of the Fourth Workshop on Statistical Machine Translation\n", "abstract": " The EACL 2009 Workshop on Statistical Machine Translation (WMT09) took place on March 30 and 31 in Athens, Greece, immediately preceding the 12th Conference of the European Chapter of the Association for Computational Linguistics (EACL), which was organized by the Greek National Centre for Scientific Research, with support from Athens University of Economics and Business\u2013Department of Informatics, and the Institute for Language and Speech Processing.This is the fifth time this workshop has been held. The first time was in 2005 as part of the ACL 2005 Workshop on Building and Using Parallel Texts. In the following years, the Workshop on Statistical Machine Translation was held at HLT-NAACL 2006 in New York City, US, at ACL 2007 in Prague, Czech Republic, and at ACL 2008 in Columbus, Ohio, US.", "num_citations": "1\n", "authors": ["2070"]}
{"title": "Proceedings of the Fourth Workshop on Statistical Machine Translation, Athens, Greece\n", "abstract": " Proceedings of the Fourth Workshop on Statistical Machine Translation, Athens, Greece University of Amsterdam University of Amsterdam UvA Terms of use Contact UvA-DARE (Digital Academic Repository) Home Advanced Search Browse My selection Search UvA-DARE Author C. Callison-Burch P. Koehn C. Monz J. Schroeder Year 2009 Title Proceedings of the Fourth Workshop on Statistical Machine Translation, Athens, Greece Number of pages 269 Publisher Morristown, NJ: Association for Computational Linguistics Document type Book editing Faculty Faculty of Science (FNWI) Institute Informatics Institute (IVI) Link Link Language English Persistent Identifier https://hdl.handle.net/11245/1.320583 Disclaimer/Complaints regulations If you believe that digital publication of certain material infringes any of your rights or (privacy) interests, please let the Library know, stating your reasons. In case of a legitimate \u2026", "num_citations": "1\n", "authors": ["2070"]}
{"title": "Machine translation: Word-based models and the EM algorithm\n", "abstract": " \u2022 Incomplete data\u2013if we had complete data, would could estimate model\u2013if we had model, we could fill in the gaps in the data\u2022 Expectation Maximization (EM) in a nutshell\u2013initialize model parameters (eg uniform)\u2013assign probabilities to the missing data\u2013estimate model parameters from completed data\u2013iterate", "num_citations": "1\n", "authors": ["2070"]}
{"title": "Statistical Machine Translation\n", "abstract": " Statistical Machine Translation Page 1 ESSLLI Summer School 2008 Statistical Machine Translation Chris Callison-Burch, Johns Hopkins University Philipp Koehn, University of Edinburgh Page 2 Page 3 Intro to Statistical MT EuroMatrix MT Marathon Chris Callison-Burch Various approaches \u2022 Word-for-word translation \u2022 Syntactic transfer \u2022 Interlingual approaches \u2022 Controlled language \u2022 Example-based translation \u2022 Statistical translation 3 Page 4 Advantages of SMT \u2022 Data driven \u2022 Language independent \u2022 No need for staff of linguists of language experts \u2022 Can prototype a new system quickly and at a very low cost Statistical machine translation \u2022 Find most probable English sentence given a foreign language sentence \u2022 Automatically align words and phrases within sentence pairs in a parallel corpus \u2022 Probabilities are determined automatically by training a statistical model using the parallel corpus 4 Page 5 Parallel \u2026", "num_citations": "1\n", "authors": ["2070"]}
{"title": "Erratum to incremental syntactic language models for phrase-based translation\n", "abstract": " Schwartz et al.(2011) presented a novel technique for incorporating syntactic knowledge into phrase-based machine translation through incremental syntactic parsing, and presented empirical results on a constrained Urdu-English translation task. The work contained an error in the description of the experimental setup, which was discovered subsequent to publication. After correcting the error, no improvement in BLEU score is seen over the baseline when the syntactic language model is used on the constrained Urdu-English translation task. The error does not affect the originally reported perplexity results.", "num_citations": "1\n", "authors": ["2070"]}