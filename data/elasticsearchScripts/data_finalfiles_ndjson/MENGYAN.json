{"title": "Automated classification of software change messages by semi-supervised Latent Dirichlet Allocation\n", "abstract": " ContextTopic models such as probabilistic Latent Semantic Analysis (pLSA) and Latent Dirichlet Allocation (LDA) have demonstrated success in mining software repository tasks. Understanding software change messages described by the unstructured nature-language text is one of the fundamental challenges in mining these messages in repositories.ObjectiveWe seek to present a novel automatic change message classification method characterized by semi-supervised topic semantic analysis.MethodIn this work, we present a semi-supervised LDA based approach to automatically classify change messages. We use domain knowledge of software changes to make labeled samples which are added to build the semi-supervised LDA model. Next, we verify the cross-project analysis application of our method on three open-source projects. Our method has two advantages over existing software change\u00a0\u2026", "num_citations": "64\n", "authors": ["249"]}
{"title": "Which non-functional requirements do developers focus on? an empirical study on stack overflow using topic analysis\n", "abstract": " Programming question and answer (Q&A) websites, such as Stack Overflow, gathered knowledge and expertise of developers from all over the world, this knowledge reflects some insight into the development activities. To comprehend the actual thoughts and needs of the developers, we analyzed the non-functional requirements (NFRs) on Stack Overflow. In this paper, we acquired the textual content of Stack Overflow discussions, utilized the topic model, latent Dirichlet allocation (LDA), to discover the main topics of Stack Overflow discussions, and we used the wordlists to find the relationship between the discussions and NFRs. We focus on the hot and unresolved NFRs, the evolutions and trends of the NFRs in their discussions. We found that the most frequent topics the developers discuss are about usability and reliability while they concern few about maintainability and efficiency. The most unresolved\u00a0\u2026", "num_citations": "38\n", "authors": ["249"]}
{"title": "Automatically classifying software changes via discriminative topic model: Supporting multi-category and cross-project\n", "abstract": " Accurate classification of software changes as corrective, adaptive and perfective can enhance software decision making activities. However, a major challenge which remains is how to automatically classify multi-category changes. This paper presents a discriminative Probability Latent Semantic Analysis (DPLSA) model with a novel initialization method which initializes the word distributions for different topics using labeled samples. This method creates a one-to-one correspondence between the discovered topics and the change categories. As a result, the discriminative semantic representation of the software change messages whose largest topic entry directly corresponds to the category label of the change message which is directly used to perform single-category and multi-category change classification. In the evaluation on five open source projects, the experimental results show that the proposed approach\u00a0\u2026", "num_citations": "35\n", "authors": ["249"]}
{"title": "A component recommender for bug reports using Discriminative Probability Latent Semantic Analysis\n", "abstract": " ContextThe component field in a bug report provides important location information required by developers during bug fixes. Research has shown that incorrect component assignment for a bug report often causes problems and delays in bug fixes. A topic model technique, Latent Dirichlet Allocation (LDA), has been developed to create a component recommender for bug reports.ObjectiveWe seek to investigate a better way to use topic modeling in creating a component recommender.MethodThis paper presents a component recommender by using the proposed Discriminative Probability Latent Semantic Analysis (DPLSA) model and Jensen\u2013Shannon divergence (DPLSA-JS). The proposed DPLSA model provides a novel method to initialize the word distributions for different topics. It uses the past assigned bug reports from the same component in the model training step. This results in a correlation between the\u00a0\u2026", "num_citations": "27\n", "authors": ["249"]}
{"title": "Automated change-prone class prediction on unlabeled dataset using unsupervised method\n", "abstract": " ContextSoftware change-prone class prediction can enhance software decision making activities during software maintenance (e.g., resource allocating). Researchers have proposed many change-prone class prediction approaches and most are effective on labeled datasets (projects with historical labeled data). These approaches usually build a supervised model by learning from historical labeled data. However, a major challenge is that this typical change-prone prediction setting cannot be used for unlabeled datasets (e.g., new projects or projects with limited historical data). Although the cross-project prediction is a solution on unlabeled dataset, it needs the prior labeled data from other projects and how to select the appropriate training project is a difficult task.ObjectiveWe aim to build a change-prone class prediction model on unlabeled datasets without the need of prior labeled data.MethodWe propose to\u00a0\u2026", "num_citations": "16\n", "authors": ["249"]}
{"title": "Duplication Detection for Software Bug Reports based on Topic Model\n", "abstract": " The traditional duplicate bug reports detection approaches are usually based on vector space model. However, the experimental result is rarely satisfying since this method cannot distinguish semantic correlation among bug reports which written by natural languages. Topic model, as a method to model underlying topics of texts, can solve the problem of document similarity calculation methods used in the information retrieving. It can find the semantic topics among the texts through massive training data, and obtain semantic relatedness among documents. Therefore, this paper proposes a novel duplication detection method based on topic model. Through selecting bug reports with execution information and combing with classified information of bugs, not only does this new method overcome the problem of high dimension, sparse data and loud noise, but also avoid the problem of synonymy and ambiguity in the\u00a0\u2026", "num_citations": "8\n", "authors": ["249"]}
{"title": "TagDeepRec: Tag Recommendation for Software Information Sites Using Attention-Based Bi-LSTM\n", "abstract": " Software information sites are widely used to help developers to share and communicate their knowledge. Tags in these sites play an important role in facilitating information classification and organization. However, the insufficient understanding of software objects and the lack of relevant knowledge among developers may lead to incorrect tags. Thus, the automatic tag recommendation technique has been proposed. However, tag explosion and tag synonym are two major factors that affect the quality of tag recommendation. Prior studies have found that deep learning techniques are effective for mining software information sites. Inspired by recent deep learning researches, we propose TagDeepRec, a new tag recommendation approach for software information sites using attention-based Bi-LSTM. The attention-based Bi-LSTM model has the advantage of deep potential semantics mining, which can\u00a0\u2026", "num_citations": "6\n", "authors": ["249"]}
{"title": "Self-learning Change-prone Class Prediction.\n", "abstract": " Software change-prone class prediction can enhance software decision making activities during software maintenance (eg, resource allocating). Many change-prone class prediction approaches have been proposed and most are effective in interversion prediction within a project. These approaches usually build a supervised prediction model by learning from historical labeled dataset. However, a major challenge which remains is that this typical change-prone prediction setting cannot be used for new projects or projects with limited historical data. To address this challenge, we propose to tackle this task by adopting a novel prediction method which has not been used in changeprone prediction, namely self-learning method. The key idea of the self-learning method is to enable the change-prone prediction on new projects or projects with limited historical dataset by learning from itself. In this paper, we apply a state-of-art selflearning method, CLAMI, to change-prone prediction. In addition, we propose a novel self-learning approach CLAMI+ by extending CLAMI. The experiments among 14 open source projects show that the self-learning methods achieve comparable results to four typical inter-version baselines and the proposed CLAMI+ slightly improves the CLAMI method on average.", "num_citations": "6\n", "authors": ["249"]}
{"title": "Feature selection and embedding based cross project framework for identifying crashing fault residence\n", "abstract": " Context: The automatically produced crash reports are able to analyze the root of fault causing the crash (crashing fault for short) which is a critical activity for software quality assurance.Objective: Correctly predicting the existence of crashing fault residence in stack traces of crash report can speed up program debugging process and optimize debugging efforts. Existing work focused on the collected label information from bug-fixing logs, and the extracted features of crash instances from stack traces and source code for Identification of Crashing Fault Residence (ICFR) of newly-submitted crashes. This work develops a novel cross project ICFR framework to address the data scarcity problem by using labeled crash data of other project for the ICFR task of the project at hand. This framework removes irrelevant features, reduces distribution differences, and eases the class imbalance issue of cross project data since\u00a0\u2026", "num_citations": "5\n", "authors": ["249"]}
{"title": "Multi-Dimension Convolutional Neural Network for Bug Localization\n", "abstract": " Software bugs remain frequent in the life cycle of software development and maintenance. Automatic localization of buggy source code files is critical for timely bug fixing and improving the efficiency of software quality assurance. Various bug localization techniques have been proposed using different dimensions of features. Recent studies have shown that different dimensions of features may play different roles in bug localization. Unfortunately, how to effectively merge these dimensions of features for improving bug localization has rarely been investigated. This paper presents a Multi-Dimension Convolutional Neural Network (MD-CNN) model for bug localization automatically based on a bug report. Our approach has dual-novelty. First, we identify and extract five statistical dimensions of features. Second, we design a Convolutional Neural Network (CNN) model that takes our five statistical dimensions of features\u00a0\u2026", "num_citations": "5\n", "authors": ["249"]}
{"title": "Improving Log-Based Anomaly Detection with Component-Aware Analysis\n", "abstract": " Logs are universally available in software systems for troubleshooting. They record system run-time states and messages of system activities. Log analysis is an effective way to diagnosis system exceptions, but it will take a long time for engineers to locate anomalies accurately through logs. Many automatic approaches have been proposed for log-based anomaly detection. However, most of the prior approaches did not consider the corresponding system component of a log message. Such component records the log location, which can help detect the location-sequence-related anomalies. In this paper, we propose LogC, a new Log -based anomaly detection approach with Component-aware analysis. LogC contains two phases: (i) turning log messages into log template sequences and component sequences, (ii) feeding such two sequences to train a combined LSTM model for detecting anomalous logs. LogC\u00a0\u2026", "num_citations": "3\n", "authors": ["249"]}
{"title": "Misclassification Cost-Sensitive Software Defect Prediction\n", "abstract": " Software defect prediction helps developers focus on defective modules for efficient software quality assurance. A common goal shared by existing software defect prediction methods is to attain low classification error rates. These proposals suffer from two practical problems: (i) Most of the prediction methods rely on a large number of labeled training data. However, collecting labeled data is a difficult and expensive task. It is hard to obtain classification labels over new software projects or existing projects without historical defect data. (ii) Software defect datasets are highly imbalanced. In many real-world applications, the misclassification cost of defective modules is generally several times higher than that of non-defective ones. In this paper, we present a misclassification Cost-sensitive approach to Software Defect Prediction (CSDP). The CSDP approach is novel in two aspects: First, CSDP addresses the problem\u00a0\u2026", "num_citations": "2\n", "authors": ["249"]}
{"title": "A comprehensive investigation of the impact of feature selection techniques on crashing fault residence prediction models\n", "abstract": " Context:Software crash is a serious form of the software failure, which often occurs during the software development and maintenance process. As the stack trace reported when the software crashes contains a wealth of information about crashes, recent work utilized classification models with the collected features from stack traces and source code to predict whether the fault causing the crash resides in the stack trace. This could speed-up the crash localization task.Objective:As the quality of features can affect the performance of the constructed classification models, researchers proposed to use feature selection methods to select a representative feature subset to build models by replacing the original features. However, only limited feature selection methods and classification models were taken into consideration for this issue in previous work. In this work, we look into this topic deeply and find out the best feature\u00a0\u2026", "num_citations": "1\n", "authors": ["249"]}
{"title": "Just-in-time defect prediction for Android apps via imbalanced deep learning model\n", "abstract": " Android mobile apps have played important roles in our daily life and work. To meet new requirements from users, the mobile apps encounter frequent updates, which involves in a large quantity of code commits. Previous studies proposed to apply Just-in-Time (JIT) defect prediction for mobile apps to timely identify whether new code commits can introduce defects into apps, aiming to assure the quality of mobile apps. In general, the number of defective commit instances is much fewer than that of clean ones, in other words, the defect data is class imbalanced. In this work, we propose a novel Imbalanced Deep Learning model, called IDL, to conduct JIT defect prediction task for Android mobile apps. More specifically, we introduce a state-of-the-art cost-sensitive cross-entropy loss function into the deep neural network to learn the high-level feature representation, in which the loss function alleviates the class\u00a0\u2026", "num_citations": "1\n", "authors": ["249"]}
{"title": "Simplified Deep Forest Model based Just-In-Time Defect Prediction for Android Mobile Apps\n", "abstract": " The popularity of mobile devices has led to an explosive growth in the number of mobile apps in which Android mobile apps are the mainstream. Android mobile apps usually undergo frequent update due to new requirements proposed by users. Just-in-time (JIT) defect prediction is appropriate for this scenario for quality assurance because it can provide timely feedback by determining whether a new code commit will introduce defects into the apps. As defect-prediction performance usually relies on the quality of the data representation and the used classification model, in this work, we propose a model, called Simplified Deep Forest (SDF), to conduct JIT defect prediction for Android mobile apps. SDF modifies a state-of-the-art deep forest model by removing the multigrained scanning operation that is designed for data with a high-dimensional feature space. It uses a cascade structure with ensemble forests for\u00a0\u2026", "num_citations": "1\n", "authors": ["249"]}
{"title": "\u57fa\u4e8e\u91c7\u6837\u7684\u534a\u76d1\u7763\u652f\u6301\u5411\u91cf\u673a\u8f6f\u4ef6\u7f3a\u9677\u9884\u6d4b\u65b9\u6cd5\n", "abstract": " \u8f6f\u4ef6\u7f3a\u9677\u9884\u6d4b\u6709\u52a9\u4e8e\u63d0\u9ad8\u8f6f\u4ef6\u5f00\u53d1\u8d28\u91cf, \u4fdd\u8bc1\u6d4b\u8bd5\u8d44\u6e90\u6709\u6548\u5206\u914d. \u9488\u5bf9\u8f6f\u4ef6\u7f3a\u9677\u9884\u6d4b\u7814\u7a76\u4e2d\u7c7b\u6807\u7b7e\u6570\u636e\u96be\u4ee5\u83b7\u53d6\u548c\u7c7b\u4e0d\u5e73\u8861\u5206\u5e03\u95ee\u9898, \u63d0\u51fa\u57fa\u4e8e\u91c7\u6837\u7684\u534a\u76d1\u7763\u652f\u6301\u5411\u91cf\u673a\u9884\u6d4b\u6a21\u578b. \u8be5\u6a21\u578b\u91c7\u7528\u65e0\u76d1\u7763\u7684\u91c7\u6837\u6280\u672f, \u786e\u4fdd\u5e26\u6807\u7b7e\u6837\u672c\u6570\u636e\u4e2d\u7f3a\u9677\u6837\u672c\u6570\u91cf\u4e0d\u4f1a\u8fc7\u4f4e, \u4f7f\u7528\u534a\u76d1\u7763\u652f\u6301\u5411\u91cf\u673a\u65b9\u6cd5, \u5728\u5c11\u91cf\u5e26\u6807\u7b7e\u6837\u672c\u6570\u636e\u57fa\u7840\u4e0a\u5229\u7528\u65e0\u6807\u7b7e\u6570\u636e\u4fe1\u606f\u6784\u5efa\u9884\u6d4b\u6a21\u578b; \u4f7f\u7528\u516c\u5f00\u7684 NASA \u8f6f\u4ef6\u7f3a\u9677\u9884\u6d4b\u6570\u636e\u96c6\u8fdb\u884c\u4eff\u771f\u5b9e\u9a8c. \u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u63d0\u51fa\u7684\u65b9\u6cd5\u4e0e\u73b0\u6709\u534a\u76d1\u7763\u65b9\u6cd5\u76f8\u6bd4, \u5728\u7efc\u5408\u8bc4\u4ef7\u6307\u6807 F \u503c\u548c\u53ec\u56de\u7387\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5; \u4e0e\u6709\u76d1\u7763\u65b9\u6cd5\u76f8\u6bd4, \u80fd\u5728\u5b66\u4e60\u6837\u672c\u8f83\u5c11\u7684\u60c5\u51b5\u4e0b\u53d6\u5f97\u76f8\u5f53\u7684\u9884\u6d4b\u6027\u80fd.", "num_citations": "1\n", "authors": ["249"]}
{"title": "Software defect prediction using semi-supervised support vector machine with sampling\n", "abstract": " Software defect prediction is helpful to improve the quality of software and effectively allocate test resources. To tackle two practical yet important issues in software defect prediction: labeled data is hard to be collected and class imbalance, a sample based semi-supervised support vector machine method is proposed. This method uses an unsupervised sample approach to sample a small percentage of modules to be tested and labeled, and this sample method can ensure that the defect instances in training sets are not too few. Semi-supervised support vector machine algorithm uses few labeled data combined with unlabeled to build predictor so that the model can exploit the information of unlabeled data. In the evaluation on four NASA projects, the experimental results show that the proposed approach achieves comparable performance compared with supervised learning models, but uses little defect information. Moreover, proposed method's performance is better than other semi-supervised learning methods in terms of recall and F-measure.", "num_citations": "1\n", "authors": ["249"]}