{"title": "Automated essay scoring: A cross disciplinary perspective\n", "abstract": " This new volume is the first to focus entirely on automated essay scoring and evaluation. It is intended to provide a comprehensive overview of the evolution and state-of-the-art of automated essay scoring and evaluation technology across several disciplines, including education, testing and measurement, cognitive science, computer science, and computational linguistics. The development of this technology has led to many questions and concerns. Automated Essay Scoring attempts to address some of these questions including:* How can automated scoring and evaluation supplement classroom instruction?* How does the technology actually work?* Can it improve students' writing?* How reliable is the technology?* How can these computing methods be used to develop evaluation tools?* What are the state-of the-art essay evaluation technologies and automated scoring systems? Divided into four parts, the first part reviews the teaching of writing and how computers can contribute to it. Part II analyzes actual automated essay scorers including e-raterTM, Intellimetric, and the Intelligent Essay Assessor. The third part analyzes related psychometric issues, and the final part reviews innovations in the field. This book is ideal for researchers and advanced students interested in automated essay scoring from the fields of testing and measurement, education, cognitive science, language, and computational linguistics.", "num_citations": "597\n", "authors": ["1063"]}
{"title": "Handbook of automated essay evaluation: Current applications and new directions\n", "abstract": " This comprehensive, interdisciplinary handbook reviews the latest methods and technologies used in automated essay evaluation (AEE) methods and technologies. Highlights include the latest in the evaluation of performance-based writing assessments and recent advances in the teaching of writing, language testing, cognitive psychology, and computational linguistics. This greatly expanded follow-up to Automated Essay Scoring reflects the numerous advances that have taken place in the field since 2003 including automated essay scoring and diagnostic feedback. Each chapter features a common structure including an introduction and a conclusion. Ideas for diagnostic and evaluative feedback are sprinkled throughout the book. Highlights of the book\u2019s coverage include: The latest research on automated essay evaluation. Descriptions of the major scoring engines including the E-rater\u00ae, the Intelligent Essay Assessor, the IntellimetricTM Engine, c-raterTM, and LightSIDE. Applications of the uses of the technology including a large scale system used in West Virginia. A systematic framework for evaluating research and technological results. Descriptions of AEE methods that can be replicated for languages other than English as seen in the example from China. Chapters from key researchers in the field. The book opens with an introduction to AEEs and a review of the\" best practices\" of teaching writing along with tips on the use of automated analysis in the classroom. Next the book highlights the capabilities and applications of several scoring engines including the E-rater\u00ae, the Intelligent Essay Assessor, the IntellimetricTM engine, c-raterTM\u00a0\u2026", "num_citations": "328\n", "authors": ["1063"]}
{"title": "Anxiety sensitivity: Unitary personality trait or domain-specific appraisals?\n", "abstract": " We tested the hypothesis that the Anxiety Sensitivity Index (ASI) measures a unitary personality variable. College students (N = 840) were administered the ASI along with a questionnaire assessing panic and anxiety symptomatology. The ASI demonstrated adequate internal reliability (\u03b1 = .82) and showed modest discrimination on two of three anxiety disorder indices (i.e., anxiety medication usage and panic history). Results of a principal components analysis with varimax rotation revealed a four-factor solution which explained 53.5% of the total variance. Our findings seriously challenge previous claims that the ASI measures a single factor. Rather, our data suggest that the ASI measures several loosely-related cognitive appraisal domains concerned with the anticipated negative consequences of anxiety. The four factors that emerged from our analysis were (a) concern about physical sensations, (b) concern\u00a0\u2026", "num_citations": "234\n", "authors": ["1063"]}
{"title": "Stress-buffering factors related to adolescent coping: A path analysis\n", "abstract": " To uncover those factors that buffer the impact of stressful negative experiences on adolescent adjustment, a theoretical model of adolescent stress and coping, with social support and social problem solving proposed as moderators, was investigated using path analysis. The study was conducted with 122 ninth-and tenth-grade nonreferred high school students. Using the LISREL statistical package (J\u00f6reskog & S\u00f6rbom, 1986), it was found that a recursive loop leading from stress outcomes back to negative stressors did not allow for a successful solution to the model. However, the effects of stressful events on adjustment were mediated by coping resources, which included a combination of problem-solving abilities and social support. Overall, the findings replicated previous investigations that have demonstrated direct relationships among stressful life events, social support, problem solving, and adolescent\u00a0\u2026", "num_citations": "184\n", "authors": ["1063"]}
{"title": "A review of strategies for validating computer-automated scoring\n", "abstract": " Computer-automated scoring (CAS) is becoming a popular tool in assessment. Various studies have been conducted to assess the quality of CAS-system-generated scores. There is then a need for a systematic examination of these studies in the context of contemporary validity concepts and current practice. This article starts with a brief introduction to current CAS systems. Next, we review the current practice of validating CAS-system-generated scores. Finally, we present a conceptual framework and general recommendations for designing validation studies for CAS procedures.", "num_citations": "151\n", "authors": ["1063"]}
{"title": "State-of-the-art automated essay scoring: Competition, results, and future directions from a United States demonstration\n", "abstract": " This article summarizes the highlights of two studies: a national demonstration that contrasted commercial vendors\u2019 performance on automated essay scoring (AES) with that of human raters; and an international competition to match or exceed commercial vendor performance benchmarks. In these studies, the automated essay scoring engines performed well on five of seven measures and approximated human rater performance on the other two. With additional validity studies, it appears that automated essay scoring holds the potential to play a viable role in high-stakes writing assessments.", "num_citations": "145\n", "authors": ["1063"]}
{"title": "Automated essay scoring: Writing assessment and instruction\n", "abstract": " In 1973, the late Ellis Page and colleagues at the University of Connecticut programmed the first successful automated essay scoring engine,\u201cProject Essay Grade (PEG)\u201d(1973). The technology was foretold some six years earlier in a landmark Phi Delta Kappan article entitled,\u201cThe Imminence of Grading Essays by Computer\u201d(Page, 1966). At the time the article was provocative and a bit outrageous, though in hindsight, it can only be deemed prophetic. As a former high school English teacher, Page was convinced that students would benefit greatly by having access to technology that would provide quick feedback on their writing. He also realized that the greatest hindrance to having secondary students write more was the requirement that, ultimately, a teacher had to review stacks of papers. While PEG produced impressive results, the technology of the time was too primitive to make it a practical application. Text had to be typed on IBM 80-column punched cards and read into a mainframe computer before it could be evaluated. As a consequence, the technology sat dormant until the early 1990s and was revitalized with the confluence of two technological developments: microcomputers and the Internet. Microcomputers permitted the generation of electronic text from a regular keyboard and the internet provided a universal platform to submit text for review (Shermis, Mzumara, Olson, & Harrington, 2001).", "num_citations": "108\n", "authors": ["1063"]}
{"title": "Effects of computer-based test administrations on test anxiety and performance\n", "abstract": " This study examined the degree to which computer and test anxiety had a predictive role in performance across three computer-administered placement tests (math, reading, written English). Subjects (72 college undergraduates at a Midwestern university) were measured with the Computer Anxiety Rating Scale, the Test Anxiety Inventory, the Myers-Briggs Type Indicator (MBTI), and the three achievement areas. Age and gender were added to the prediction model for completeness. Results showed that age and test anxiety were significant predictors for math performance, with lower values on the two variables associated with better performance. When reading was the outcome variable, age and computer anxiety were statistically significant performance predictors, with older readers faring better and less anxious individuals achieving higher scores. No predictors were statistically significant for the written English\u00a0\u2026", "num_citations": "108\n", "authors": ["1063"]}
{"title": "Trait ratings for automated essay grading\n", "abstract": " This study employed an automated grader to evaluate essays, both holistically and with the rating of traits (content, organization, style, mechanics, and creativity) for Webbased student essays serving as placement tests at a large Midwestern university. The authors report the results of two combined experiments, based on random selection from 1,193 essays. In the first experiment, the essays of 807 students were used to create statistical predictions for the essay-grading software. In the second experiment, the ratings from a separate, random sample of 386 essays were used to compare the ratings of six human judges against those generated by the computer. The interjudge correlation of the human raters alone was r = .71. But the interrater reliability of all six judges in combination with computer scoring reached .83. The essay-grading software was an efficient means for evaluating the essays, with a capacity for\u00a0\u2026", "num_citations": "102\n", "authors": ["1063"]}
{"title": "Classroom assessment in action\n", "abstract": " Classroom Assessment in Action clarifies the multi-faceted roles of measurement and assessment and their applications in a classroom setting. Comprehensive in scope, Shermis and Di Vesta explain basic measurement concepts and show students how to interpret the results of standardized tests. From these basic concepts, the authors then provide clear and ordered discussions of how assessment and instruction is integrated into a functional process to enhance student learning. Guidelines are set forth for constructing various common assessments. Procedures are laid out to evaluate and improve assessments once they are constructed. Ultimately, the authors shed light on the myriad of factors that impact test score interpretation. In today's classroom, technology has become a constant companion, and Classroom Assessment in Action exposes teacher candidates to emerging technologies they might encounter in building their repertoire of assessments, whether it be automated essay scoring or electronic portfolios. Classroom Assessment in Action guides its readers to a complete and thorough understanding of assessment and measurement so that they can confidently work with students and parents in explaining results, whether they are from a high-stakes statewide assessment or the grading philosophy to which they ascribe.", "num_citations": "83\n", "authors": ["1063"]}
{"title": "On-line grading of student essays: PEG goes on the World Wide Web\n", "abstract": " This study examined the feasibility of employing Project Essay Grade (PEG) software to evaluate web-based student essays that serve as placement tests at a large Mid-Western university. The results of two experiments are reported. In the first experiment, the essays of 1293 high school and college students were used to create a statistical model for the PEG software. PEG identified 30 proxes (observed variables) that could be incorporated into an evaluation of the written work. In the second experiment, the ratings from a separate sample of 617 essays were used to compare the ratings of six human judges against those generated by the computer. The inter-judge correlation of the human raters was r = 0.62 and was r = 0.71 for the computer. Finally, the PEG software was an efficient means for grading the essays with a capacity for approximately three documents graded every second. Cycle time from the\u00a0\u2026", "num_citations": "80\n", "authors": ["1063"]}
{"title": "A comparison of survey data collected by regular mail and electronic mail questionnaires\n", "abstract": " A survey was conducted by the National Council on Measurement in Education (NCME) to examine the telecommunications needs of the organization's membership. A component of this study permitted an examination of two response modalities (regular mail and e-mail) across a number of variables. Separate samples of 585 were drawn to take the survey. The first sample consisted of all those organization members who had registered their e-mail address with the organization while the second sample was randomly selected from those members who did not list an e-mail address. The first group was sent a nine question instrument via e-mail while the second group was asked to fill out a ten question instrument via regular mail. Overall response rates were low and significantly different [30% for the e-mail group and 36% for the regular mail group, \u03c72 (1) = 10.42, p < .01], though not uncommon for\u00a0\u2026", "num_citations": "73\n", "authors": ["1063"]}
{"title": "Contrasting state-of-the-art automated scoring of essays\n", "abstract": " With the press for developing innovative assessments that can accommodate higher-order thinking and performances associated with the Common Core Standards, there is a need to systematically evaluate the benefits and features of automated essay evaluation (AEE). While the developers of AEE engines have published an impressive body of literature to suggest that the measurement technology can produce reliable and valid essay scores (when compared with trained human raters; Attali & Burstein, 2006; Shermis, Burstein, Higgins, & Zechner, 2010), comparisons across the multiple platforms have been informal, involved less-than-ideal sample essays, and were often associated with an incomplete criterion set.ABSTRACT", "num_citations": "70\n", "authors": ["1063"]}
{"title": "The incidence of stressful life events of elementary school-aged children\n", "abstract": " METHODThe potential respondents in this study were the parents of 400 schoolchildren, aged 5 through 14 years. Their names were selected randomly from a list of parents in a school district in Allegheny County, Pennsylvania. The district was selected because it seemed fairly representative of various socioeconomic groups.", "num_citations": "57\n", "authors": ["1063"]}
{"title": "Applications of computers in assessment and analysis of writing\n", "abstract": " It has been almost 40 years since Ellis \u201cBo\u201d Page prophesized the imminence of grading essays by computer. As a former high school English teacher, he envisioned a world where computers could assistin reducingtheburden of grading written English. Later, as a seminal educational researcher, his desire was to implement a system that would operationalize what 50 years of research has shown us: Students become better writers by writing more. His landmark article in Phi Delta Kappan (Page, 1966) forecast this, but it took another 7 years to produce a working model (Ajay, Tillett, & Page, 1973) using FORTRAN code and a large mainframe computer. The result was Project Essay Grade (PEG). In order to submit text to the essay grader, written documents had to be transferred to awkward IBM 80-column punched cards, an overwhelming taskforthetechnologyoftheday. Beyondthis handicap, the technology performed as well or better than the ratings assigned by humans (Ajayetal., 1973).In the early 1990s, Page refashioned PEG with a more sophisticated parser, real-time calculations, and a Web-based interface (Shermis, Mzumara, Olson, & Harrington, 2001). The details of this new format are explored further in this chapter, demonstrating the way automated essay scoring (AES) works, the kinds of software programs it uses, and some emerging applications.", "num_citations": "50\n", "authors": ["1063"]}
{"title": "From conventional to computer-adaptive testing of ESL reading comprehension\n", "abstract": " In this paper we describe the development of a computer-adaptive test of ESL reading comprehension. The computer-adaptive test was based on items from a battery of four conventional fixed-form tests and was constructed using an adaptive test development system for the Macintosh HyperCAT. The application of computer-adaptive testing (CAT) to the assessment of reading comprehension and other abilities in a second language serves as an illustration in this paper. We discuss the constraints that apply to CAT and the substantial advantages that CAT has over conventional testing modalities. The constraints on CAT include: the unidimensionality of the test, the homogeneity of the test population, and a neutral test method effect.", "num_citations": "46\n", "authors": ["1063"]}
{"title": "Contrasting state-of-the-art in the machine scoring of short-form constructed responses\n", "abstract": " This study compared short-form constructed responses evaluated by both human raters and machine scoring algorithms. The context was a public competition on which both public competitors and commercial vendors vied to develop machine scoring algorithms that would match or exceed the performance of operational human raters in a summative high-stakes testing environment. Data (N\u00a0=\u00a025,683) were drawn from three different states, employed 10 different prompts, and were drawn from two different secondary grade levels. Samples ranging in size from 2,130 to 2,999 were randomly selected from the data sets provided by the states and then randomly divided into three sets: a training set, a test set, and a validation set. Machine performance on all of the agreement measures failed to match that of the human raters. The current study concluded with recommendations on steps that might improve machine\u00a0\u2026", "num_citations": "37\n", "authors": ["1063"]}
{"title": "Introduction to automated essay evaluation\n", "abstract": " Writing in the New York Times, educator Randall Stross (2012) recently noted that his long dream of the creation of a software program to help students learn to write may be close to reality. In dreams begin responsibilities, and so this present edited collection is offered to define and explicate the promises and complexities that occur when technologies advance to the point where they appear to be able to do what humans do.ABSTRACT", "num_citations": "37\n", "authors": ["1063"]}
{"title": "Assessing behavioral responses to stress\n", "abstract": " This paper advocates the use of a stress paradigm in the assessment of children with behavior disorders. The paradigm suggests that one element of an assessment should be the behavioral pattern that the child is likely to adopt in response to stress. The Stress Response Scale, designed to assess such behavioral patterns, is presented and discussed. In order to extend the scale's clinical utility, it was necessary to obtain data on the behavioral patterns that might typically be expected to be found with children in general. Data are presented which describes the most frequently found patterns among a population of school-aged children.", "num_citations": "37\n", "authors": ["1063"]}
{"title": "The influence of word processing on English placement test results\n", "abstract": " A study was conducted to consider two issues: (a) whether differences might emerge in writing quality when students wrote examinations by hand or on a computer and (b) whether raters differed in their evaluation of essays written by hand, on a computer, or by hand and then transcribed to typed form before scoring. A total of 480 students from a large Midwestern university were randomly assigned into one of three essay groups: (a) those who composed their responses in a traditional bluebook, (b) those who wrote in a bluebook, then had their essays transcribed into a computer, and (c) those who wrote their essays on the computer. A one-way ANOVA revealed no statistically significant differences in ratings among the three groups [F(2,475) = 1.21, ns]. The discussion centers on the need for testing programs to examine the relationship between assessment and prior writing experiences, student preferences for\u00a0\u2026", "num_citations": "36\n", "authors": ["1063"]}
{"title": "The impact of automated essay scoring on writing outcomes.\n", "abstract": " This study was an expanded replication of an earlier endeavor (Shermis, Burstein, & Bliss, 2004) to document the writing outcomes associated with automated essay scoring. The focus of the current study was on determining whether exposure to multiple writing prompts facilitated writing production variables (Essay Score, Essay Length, and Number of Unique Words) and decreased writing errors (Grammar, Usage, Mechanics, Style, Organization & Development) over time. The impacts of these variables were examined in analyses of 11,685 essays written by 2,017 students at four grade levels (grades 6-8, 10). The essays, written in response to seven different prompts, were scored by automated essay scoring. The results showed significant differences across the four grades and over time for each of the eight outcome variables. Peak essay performance occurred with 8 th graders who also displayed the highest reduction of both domain errors. Specific types of error reduction were differentially associated with grade level. The implications of the results for future research incorporating writing genre are discussed.", "num_citations": "35\n", "authors": ["1063"]}
{"title": "Automated writing evaluation: An expanding body of knowledge.\n", "abstract": " The chapter on machine scoring for the first edition of this Handbook focused almost exclusively on the use of automated writing evaluation (AWE) technologies to provide information about writing ability in summative assessment contexts. Since that time, AWE has developed its own body of knowledge, building out from its origins in natural language processing and socio cognitive approaches to construct modeling (Shermis, Burstein, & Bursky, 2013). To demonstrate the depth of contemporary knowledge about AWE, in this chapter we present AWE in terms of the categories of evidence used to demonstrate that these systems are useful in the instruction and assessment of writing. Defining the documentation of such usefulness in terms of validation (Kane, 2006), Williamson, Xi, and Breyer (2012) proposed a unique conceptual framework for AWE focusing on explanation (construct, task, and scoring investigation\u00a0\u2026", "num_citations": "31\n", "authors": ["1063"]}
{"title": "Behavioral responses of children to stress An Egyptian-American cross-cultural study\n", "abstract": " The behavior a child adopts in response to stress may be viewed on a continuum from adaptive, effective coping behaviors to extreme maladaptive efforts to meet stressful demands. In order to assess children's behavioral responses to stress, a behavior rating scale was developed - the Stress Response Scale (SRS). The purpose of this study was to gather SRS data on a population of Egyptian children, so that we might examine differences between them and their US counterparts in terms of the typical behavioral responses to stress they might manifest. The results showed, with one exception, no significant differences between the samples as to the overall magnitude of behavioral deviance or the type of pattern manifested. The exception was found in the case of the dependent behavioral pattern. On this scale all three age groups of girls in the US sample and two of the three age groups of boys in that sample\u00a0\u2026", "num_citations": "31\n", "authors": ["1063"]}
{"title": "Cross-national invariance of children's temperament\n", "abstract": " Measurement of temperament is an important endeavor with international appeal; however, cross-national invariance (i.e., equivalence of test scores across countries as established by empirical comparisons) of temperament tests has not been established in published research. This study examines the cross-national invariance of school-aged children's temperament styles as measured by the Student Styles Questionnaire (SSQ). Development of the SSQ was based on Jung's theory of temperament as augmented by Myers and Briggs. A four bipolar dimension model provided an excellent fit for Australian, Chinese, Costa Rican, Philippine, United States, and Zimbabwean samples and modest fit for Gaza (Palestinian) and Nigerian samples. This study provides partial support for the conclusion that the structure of school-age children's temperament as measured by the SSQ transcends differences in languages and\u00a0\u2026", "num_citations": "30\n", "authors": ["1063"]}
{"title": "The use of the Stress Response Scale in diagnostic assessment with children\n", "abstract": " Children's experience with stress provides a useful paradigm for the assessment of their emotional status. Such an assessment must be concerned with identifying the typical behavioral pattern a child is likely to adopt in response to stress. The Stress Response Scale has been developed to assess those response patterns. This study compared the behavior patterns of a group of children referred because of possible emotional disorders with their psychiatric diagnoses. It also examined the agreement between parents and teachers as to the type of response pattern likely to be shown by the child. The results suggested that the scale may be sensitive to selected psychiatric diagnostic categories commonly assigned to children from the Diagnostic and Statistical Manual.", "num_citations": "27\n", "authors": ["1063"]}
{"title": "The Collegiate Learning Assessment: A critical perspective.\n", "abstract": " This article describes the Collegiate Learning Assessment (CLA), a postsecondary assessment tool designed to evaluate the\" value-added\" component of institutional contributions to student learning outcomes. Developed by the Council for Aid to Education (CAE), the instrument ostensibly focuses on the contributions of general education coursework that would be the foundation for most undergraduate majors. Depending on the needs of the institution, the instrument can be administered as a\" rising junior\" test, a senior exit exam, or a longitudinal measure that can track cohorts of students throughout their college years. The CLA can be used on an entire population of students or on student samples. The instrument is administered over the Internet and comprises writing and performance tasks rather than multiple-choice questions. Because scores are not provided for individual students, the CLA is intended to\u00a0\u2026", "num_citations": "26\n", "authors": ["1063"]}
{"title": "Behavioral responses to stress: profile patterns of children\n", "abstract": " We describe the development o fan instrument that identifies the behavioral patterns likely to be adopted by children attempting to respond to stress. Empirical support has been found for a theoretical model that predicts five types of response. These response types can be profiled from the scores on a behavior rating scale, the Stress Response Scale (Chandler, 1979, 1983). The purposes of this study were to identify the stress profile types among a population of randomly selected children and to compare the frequency of the profile types with those from a group of clinic-referred children. The results showed that, within the randomly selected population, a mixture of low-level responses is found in a fairly even distribution. Within the clinic-referred group, subgroups labeled for the factors Acting-Out and Repressed were identified as fairly frequent. These results have implications for behavioral classification and for\u00a0\u2026", "num_citations": "22\n", "authors": ["1063"]}
{"title": "Exit assessments: Evaluating writing ability through automated essay scoring.\n", "abstract": " The paper describes on-going work in automated essay scoring which will extend the applicability of models that are currently used for short-essay documents (ie, less than 500 words). Sponsored by the Fund for the Improvement of Post-Secondary Education(FIPSE), the project would create norms for documents that might normally be found in an electronic portfolio such as critiques, self-reflective writing, reports of empirical research, and technical reports. These norms and the software are posted on the website: http://coeweb. fiu. edu/fipsedemo and will be made available at no cost for a period of five years. The paper describes the project, the desired use of electronic portfolios, and the four major automated essay scoring programs. How this technology can help evaluate post-secondary general education/principles of undergraduate learning is also discussed.", "num_citations": "21\n", "authors": ["1063"]}
{"title": "Teacher perceptions of differences among elementary students with and without learning disabilities in referred samples\n", "abstract": " Current referral and identification procedures for students with learning disabilities (LD) have been criticized on conceptual and procedural dimensions, including difficulties in operationalizing the definition and in making eligibility decisions that are data based. Recognizing these difficulties, the Texas Education Agency appointed a task force to examine various issues associated with the identification, assessment, and programming of students with LD. Task force members recognized the need to identify classroom behaviors that differentiate students with LD from their non disabled peers. Two scales of 83 items each were devised and piloted in 70 school districts. Five significant factors or subscales were identified through discriminant factor analyses. Two subscales and 18 individual items discriminate students later classified as LD and those referred but not subsequently classified as LD. Results are discussed\u00a0\u2026", "num_citations": "21\n", "authors": ["1063"]}
{"title": "Using automated scoring to monitor reader performance and detect reader drift in essay scoring\n", "abstract": " With expediencies such as rapid turnaround time and economic benefits associated with reduced scoring costs, automated scoring is widely viewed as on a path to replace human labor, like so many prior innovative technologies. As a result, the focus of automated scoring development has tended to be on validity or measures of accuracy as compared to human efforts. This chapter is focused on a somewhat nearer-term process, the transition from human to machine scoring and how the two scoring methods can be used concurrently. Previously, the authors have referred to the use of both human and machine scoring for a single assessment program as a\u00ae blended scoring model (Lottridge, Mitzel, & Chou, 2009). The central idea is to see if two imperfect scoring methods can be mutually leveraged in some optimal way to improve the overall accuracy of scoring. Although the authors have not yet achieved that goal, the studies presented here can be taken as a progress report toward that objective. The general scenario under which these blended scoring models were investigated has some similarities to the next generation of computer-based K\u00a5 12 assessments. Specifically, this assessment program is a No Child Left Behind (NCLB) high stakes end of course (EOC) program, where a writing prompt constitutes one session of a high school English test. The prompts are scored holistically on each of two dimensions, on a 0\u00a5 4 scale. More detail is presented below. A key tenet of this program is that the\u00ae score of record, that is, the final score associated with a student response is always assigned by a human rater. This design, where final scores\u00a0\u2026", "num_citations": "20\n", "authors": ["1063"]}
{"title": "Preparing teachers for technology in the 90's: View from the top.\n", "abstract": " Teachers have been identified as the critical change agents in integrating technological instructional materials with teaching and learning styles and with classroom activities. However, many teachers do not understand the changes in their roles in a technology intensive classroom, nor do they have the proper training to use new educational technologies. Answers to these problems may be found in both preservice and inservice teacher education, the latter in the form of workshops and summer institutes after initial certification so that a maximum level of professional competence may be attained. Some problems with staff development, however, are lack of equipment, funding, and the use of quick intervention techniques in answer to the long range problem of technology integration into the classroom. In response to these problems, computer companies, corporations, and state departments of education have found\u00a0\u2026", "num_citations": "19\n", "authors": ["1063"]}
{"title": "Automated essay scoring in a high stakes testing environment\n", "abstract": " This chapter discusses the use of automated essay scoring (AES) as a possible replacement for an annual statewide high-stakes writing test. The examples provided are drawn from development work in the state of Florida, but might apply to any state in the United States. In the first section, literature associated with the frequency, costs, and consequences of high-stakes testing is reviewed. In the second section, automated essay scoring is introduced and a description of how it works as an assessment tool is provided. In the third section, an example of how AES is used as an instructional tool is given and I argue for a tighter integration of assessment with instruction. Finally, I propose that AES actually replace the high-stakes testing program for accountability (and other) purposes, and provide a list of advantages for proceeding in this fashion.", "num_citations": "18\n", "authors": ["1063"]}
{"title": "How important is content in the ratings of essay assessments?\n", "abstract": " This study was designed to examine the extent to which \u2018content\u2019 accounts for variance in scores assigned in automated essay scoring protocols. Specifically it was hypothesised that certain writing genre would emphasise content more than others. Data were drawn from 1668 essays calibrated at two grade levels (6 and 8) using e\u2010rater\u2122, an automated essay scoring engine with established validity and reliability. E\u2010rater v 2.0's scoring algorithm divides 12 variables into \u2018content\u2019 (scores assigned to essays with similar vocabulary; similarity of vocabulary to essays with the highest scores) and \u2018non\u2010content\u2019 (grammar, usage, mechanics, style, and discourse structure) related components. The essays were classified by genre: persuasive, expository, and descriptive. The analysis showed that there were significant main effects due to grade, F(1,1653) = 58.71, p < .001, and genre F(2, 1653) = 20.57, p < .001. The\u00a0\u2026", "num_citations": "18\n", "authors": ["1063"]}
{"title": "The challenges of emulating human behavior in writing assessment\n", "abstract": " This is a response to Dr. Les Perelman's critique of Phase I of the Hewlett Trials. His argument is that the construct validity of the study was undermined because there was a high correlation between word count and vendor predicted scores. The response addresses the argument by showing that correlations do not mean causation. Further the reply illustrates how predications are actually formulated in automated essay scoring. The response concludes with an appeal for more research on the underlying constructs associated with writing.", "num_citations": "17\n", "authors": ["1063"]}
{"title": "The Need\u2010Threat Analysis: A scoring system for the children's apperception test\n", "abstract": " The study of childhood stress provides a useful perspective for assessing children's emotional status. Thematic projective techniques, like the Children's Apperception Test (CAT), may be useful in exploring children's perception of stress. For this purpose, a need\u2010threat analysis is recommended to identify those underlying needs and threats that are likely to make a particular event or situation important, and hence potentially stressful, to an individual child. This paper introduces a scoring system for the CAT based on the analysis of thematic data in terms of five need\u2010threat binaries, which serve as scoring categories. Preliminary data on reliability are presented.", "num_citations": "16\n", "authors": ["1063"]}
{"title": "Assessing writing through the curriculum with automated essay scoring.\n", "abstract": " This paper provides an overview of some recent work in automated essay scoring that focuses on writing improvement at the postsecondary level. The paper illustrates the Vantage Intellimetric (tm) automated essay scorer that is being used as part of a Fund for the Improvement of Postsecondary Education (FIPSE) project that uses technology to grade electronic portfolios. The purpose of the electronic portfolio is to demonstrate a mechanism for translating the general learning goal on writing in an operational way that permits the developmental tracking of students throughout their undergraduate curriculum. Moreover, the technology can be readily incorporated into any course in which writing is a significant component.(Contains 22 references.)(Author/SLD)", "num_citations": "15\n", "authors": ["1063"]}
{"title": "All prompts are created equal, but some prompts are more equal than others.\n", "abstract": " Scores assigned to college placement essays by a computer program (PEG) showed high agreement with the evaluations of human readers (r=. 82). Further, both types of graders tended to assign higher or lower scores to essays written about particular topics. Content analyses by a second program (MCCA) indicated that themes in essays varied in terms of emphasis on\" analytic,\"\" emotional,\" or\" practical\" dimensions. Human and machine readers tended to give higher scores for analytic and practical themes, and lower scores for those involving emotion. The ranks of mean prompt-related grades were concordant with the ranks of mean analytic and practical content across topics. Such findings call for the refined standardization of prompts for future testing, and the need for care in the evaluation of existing essays.", "num_citations": "14\n", "authors": ["1063"]}
{"title": "On test and computer anxiety: Test performance under CAT and SAT conditions\n", "abstract": " This article examines the differences between computer adaptive (CAT) and self-adapted testing (SAT) along with possible differences in feedback conditions and gender. Areas of comparison include measurement precision/efficiency and student test characteristics. Participants included 623 undergraduates from a large Midwestern university who took math placement tests in a 4 (condition) \u00d7 2 (feedback) \u00d7 2 (gender) design. The four conditions included: a) CAT; b) SAT\u2014Global; c) SAT\u2014Individual; and d) SAT\u2014Placebo groups. Multivariate Analysis of Variance was used to analyze the data. The perceived control hypothesis was used as a framework to explain the differences between CAT and SAT. Results indicated that measurement efficiency is differentially affected by the type of test condition with the SAT\u2014Global condition performing worse than the others. Moreover, there were significant gender effects\u00a0\u2026", "num_citations": "14\n", "authors": ["1063"]}
{"title": "The development and evaluation of a microcomputerized adaptive placement testing system for college mathematics\n", "abstract": " A microcomputerized adaptive testing system was developed for placing students into various levels of college mathematics courses. Based on the student's math background, an initial cluster is identified for the testing. Using either a Bayesian or Maximum Likelihood decision, the testing is branched to a lower cluster or higher cluster depending on the student's performance. Based on the final estimate of math ability, appropriate courses are suggested. The feasibility of the system was examined with respect to three criteria: 1) item pool characteristics, 2) item performance, and 3) students' and advisors' reactions. Although the results show room for improvement, the system is promising as a placement instrument.", "num_citations": "13\n", "authors": ["1063"]}
{"title": "Computerized adaptive math tests for elementary talent development selection\n", "abstract": " A computerized adaptive test was developed for placing fifth grade elementary school children in a middle school mathematics talent development program. Three study cohorts were used for test construction, validation, and piloting. The 240 items that comprised the item bank were based on 60 objectives teachers identified as critical for successful completion of a sixth grade math talent development program. The items were partitioned into eight forms (average a = .76) with 10 anchor items and equated using Item Response Theory (IRT). A factor analysis of the 10 anchor items suggested that the item bank was essentially unidimensional (first factor explained 41% of the total test variance). A comparison between sixth grade children enrolled in a math talent development program and those enrolled in regular math classes provided additional evidence of construct validity. The scores generated by the\u00a0\u2026", "num_citations": "12\n", "authors": ["1063"]}
{"title": "Computerized adaptive testing through the World Wide Web.\n", "abstract": " A computerized adaptive testing package was developed that permitted remote placement testing of high school students via the World Wide Web. The project was designed to alert secondary students to the rigors of college work, and to let them explore their post-secondary schooling options. This paper describes the rationale for developing the project, how the programming was formulated, implementation and security issues, and presents some preliminary data on student performance/reactions to the testing.", "num_citations": "11\n", "authors": ["1063"]}
{"title": "Web applications in assessment\n", "abstract": " The principal problem with writing about technological innova\u2014tions in assessment is that the developments keep changing. This is a double\u2014edged sword. On the \u201cdull\u201d edge is the comfort and sat\u2014isfaction that there will always be something interesting about which to write. On the \u201cbleeding\u201d edge is the realization that the contribution one makes will never be lasting because some newer innovation will replace a tool that was just illustrated as a \u201cbest practice.\u201d Even though there is no perfect resolution to these two tensions, our hope for this chapter is to identify the current state of the art with regard to Web\u2014based assessments. Although it would be impossible to describe all the creative endeavors in this area, we will highlight a few of the newest developments, indicate how they might be used in a comprehensive assessment program, and iden\u2014tify additional resources that the interested reader can pursue. If there is a lasting component to this work, it is that scholars everywhere are engaged in fusing technology and measurement with the goal of creating assessments that meet the scienti\ufb01c stan\u2014dards of validity and reliability. An assessment has validity\u2014the cor\u2014nerstone of any assessment procedure\u2014if one measures that which is supposed to be measured, whether one employs authen\u2014tic assessment techniques or some sort of standardized test. An assessment possesses reliability\u2014a prerequisite condition for estab\u2014lishing validity\u2014if one measures a construct in a consistent man\u2014ner. Our hope is that technology will help transform practices from", "num_citations": "10\n", "authors": ["1063"]}
{"title": "The use of item response theory (IRT) to investigate the hierarchical nature of a college mathematics curriculum\n", "abstract": " The purpose of this study was to investigate the degree to which an undergraduate math curriculum matched the item difficulty levels of representative mathematics problems based on that sequence. Following a systematic process outlined by Hsu and Shermis, an item bank of 120 items was developed for placement testing at the undergraduate levels. Ranks for the final 62 item values were calculated and correlated with the rank order of the math curriculum sequence. Results suggest a congruence between curricular sequence and item difficulty levels of problems based on that sequence. This finding indicates that the linear or hierarchical assumption concerning the undergraduate-level sequence appears to be reasonable. The discussion addresses items that were exceptions to this trend and summarizes implications for computerized adaptive testing.", "num_citations": "9\n", "authors": ["1063"]}
{"title": "Construction of a Faking Detector Scale for a Bioda Survey Instrument\n", "abstract": " This article describes the development and empirical validation of the Faking Detector scale, a component of the Leadership Effectiveness Assessment Profile (LEAP). Based on an unlikely virtues model, an initial pool of 30 items was developed and, after review, formal pretesting, and piloting, was reduced to 12 items. The final scale was then administered as a part of the LEAP instrument to a total of 425 Reserve Officer Training Corps (ROTC) students at five different ROTC summer encamp- ments. A confirmatory factor analysis using the Linear Structural Relations (LISREL) technique revealed a unidimensional scale accounting for 59% of the total test variance. One month later, the scale was readministered to the ROTC students. Test-retest reliability was calculated at r = .65. The covariance structures of the two tests were compared to see if they were essentially the same. Although the model of equivalent\u00a0\u2026", "num_citations": "9\n", "authors": ["1063"]}
{"title": "Computerized adaptive testing for reading placement and diagnostic assessment\n", "abstract": " Computerized Adaptive Testing for Reading Placement and Diagnostic Assessment By Mark D. Shermis, Mary Wolting, and Danielle Lombard It is not only important to examine the reader's pre-existing schemata but also to include an assessment of reformulated knowledge structures. Mark D. Shermis Department of Psychology Mary Wolting School of Education Danielle Lombard Department of Psychology Indiana University Purdue University Indianapolis Indianapolis, IN 46202 ABSTRACT: The purpose of the study was to develop a reading placement test for entering col-lege undergraduates that would eventually be admin-istered in a computerized adaptive testing environ-ment. The assessment instrument was based on an integrative cognitive-processing model that identified 11 skill strands which students had to master in or-der to be exempt from formal reading instruction. An item bank of 153 items was\u00a0\u2026", "num_citations": "8\n", "authors": ["1063"]}
{"title": "Computerized adaptive skill assessment in a statewide testing program\n", "abstract": " The purpose of this article was to provide a pilot study of computerized adaptive testing (CAT) in the Michigan Educational Assessment Program\u2019s 10th-grade mathematics problem-solving and applications subtests. More than five hundred volunteers in Grades 9-12 were used to calibrate 97 unique items in the test bank. A separate sample of 122 student volunteers was tested on the operational version of the CAT. Results indicated that the item bank spanned a wide ability range and performed well. Students answered an average of 19 items in approximately 16 min. The results produced a reduction of about 25% compared to its paper-and-pencil counterpart. Although the average ability estimate was high, about 20% of the sample scored below the 10th-grade (targeted) level. It is speculated that these students were better assessed with the CAT version because it contained 4th- and 7th-grade items in the item\u00a0\u2026", "num_citations": "8\n", "authors": ["1063"]}
{"title": "Using microcomputers in social science research\n", "abstract": " Shows how microcomputer technology can be applied to social science research in psychology, education, sociology, political science, health sciences and information science. The book uses a research cycle model and deals with meta analysis, power analysis and the choice of appropriate statistics.", "num_citations": "8\n", "authors": ["1063"]}
{"title": "An extension of the norms for the Stress Response Scale for Children\n", "abstract": " This article reports an attempt to develop experimental norms on the Stress Response Scale (SRS) for adolescents aged 15 to 18 years. The SRS is a 40-item behavior rating scale designed to measure the impact of stress on the child's behavioral adjustment and to provide scores for five subscales: Acting Out, Passive-Aggressive, Overactive, Dependent, and Repressed. Subjects (N = 167) were selected randomly from a central Texas high school, and ratings on the SRS for each child were provided by a parent along with demographic information. Results of a confirmatory factor analysis replicated three of the five SRS subscales, x2(474) = 1721.23, p < .05. Two of the scales (Repressed and Dependent) did not fit well with the older children although the overall GFI was .88, an indication of a fair fit. While the original factor structure was not replicated entirely, there was enough of a relationship to proceed with\u00a0\u2026", "num_citations": "7\n", "authors": ["1063"]}
{"title": "The impact of anonymization for automated essay scoring\n", "abstract": " This study investigated the impact of anonymizing text on predicted scores made by two kinds of automated scoring engines: one that incorporates elements of natural language processing (NLP) and one that does not. Eight data sets (N = 22,029) were used to form both training and test sets in which the scoring engines had access to both text and human rater scores for training, but only the text for the test set. Machine ratings were applied under three conditions: (a) both the training and test were conducted with the original data, (b) the training was modeled on the anonymized data, but the predictions were made on the original data, and (c) both the training and test were conducted on the anonymized text. The first condition served as the baseline for subsequent comparisons on the mean, standard deviation, and quadratic weighted kappa. With one exception, results on scoring scales in the range of 1\u20136 were not\u00a0\u2026", "num_citations": "6\n", "authors": ["1063"]}
{"title": "Multitrait-multimethod analysis of FCAT Reading and Writing: Or Is It Writing and Reading?\n", "abstract": " This study investigated the convergent and discriminant validity of the high-stakes Florida Comprehensive Assessment Test (FCAT) in both reading and writing at grade levels 4, 8, and 10. The data from the 2006 FCAT administration were analyzed via traditional multitrait-multimethod (MTMM) analysis to identify the factor structure and structural equation models (SEMs), to determine the weights of the influential variables underlying the tests. The MTMM analyses suggested that across all grade levels, the correlation between the multiple-choice reading and multiple-choice writing tests of the FCAT approached the reliability coefficients for each test separately. These correlations were higher than the correlations between the multiple choice and performance sections for each trait. The SEM analyses, however, provided support for both the convergent and discriminant validity of the test scores. The fit for the SEMs\u00a0\u2026", "num_citations": "6\n", "authors": ["1063"]}
{"title": "Recent Innovations in Machine Scoring of Student-and Test Taker\u2013Written and\u2013Spoken Responses\n", "abstract": " Machine scoring of constructed responses, both text and speech, is an emerging subdomain in the field of computational linguistics that has strong application in the world of assessment. This field was first developed in the area of automated essay scoring (Ajay, Tillett & Page, 1973; Page, 1966), expanded to short-form constructed responses (Burstein, Wolff & Lu, 1999; Kaplan & Bennett, 1994; Leacock & Chodorow, 2003; Martinez & Bennett, 1992) and more recently extended to the evaluation of speech (Bernstein, 1999; Xi, Higgins, Zechner & Williamson, 2008). It also includes the evaluation of constructed responses to a performance assessment (eg, making a medical diagnosis; Clauser, Kane & Swanson, 2002).ABSTRACT", "num_citations": "5\n", "authors": ["1063"]}
{"title": "Scaling and norming for automated essay scoring\n", "abstract": " In this chapter, we provide an overview of different methods used in scaling and norming essay scores starting with a general comparison of holistic and analytic rubrics. Next, scales applied for rating the quality of writing samples are reviewed with a focus on standardized writing development scales established for application in automated essay evaluation (AEE) providing for an examinee\u2019s writing ability to be meaningfully tracked over time and across essay prompts. Methods for formation of scores in AEE are then overviewed. Common standard-setting methods are summarized and, finally, differential item functioning methods are discussed. Throughout the chapter, we raise validity issues that continue to exist today, and make recommendations for future work in the effort of producing meaningful scores in the process of scaling and norming essay scores in general and in AES.ABSTRACT", "num_citations": "5\n", "authors": ["1063"]}
{"title": "Predictive validity of placement test scores for course placement at IUPUI: Summer and fall 2000\n", "abstract": " Renewed interest in placement testing has emerged on the IUPUI Campus with the adoption of two tests from the commercial COMPASS series produced by ACT. The move to COMPASS Math was predicated on an expressed dissatisfaction with math placements at the upper end of the ability spectrum by the web-based computerized adaptive math test (Hsu & Shermis, 1989). Moreover there was the additional attraction in aligning placement testing practices with those adopted by Ivy Tech State College since there are a number of students who move between Ivy Tech and IUPUI through the Passport Program. The math faculty examined two commercial systems and concluded that the COMPASS system provided the best overall package in terms of its predictive validity and compatibility with other schools that might refer students to IUPUI. Additional advantages of the COMPASS Math test include a larger item\u00a0\u2026", "num_citations": "5\n", "authors": ["1063"]}
{"title": "Facing off on automated scoring\n", "abstract": " Assessment Update\u2022 March\u2013April 2003\u2022 Volume 15, Number 2 5 want them to. However, at Keystone College, where Strain works, the student enrollment is 1,450 and boasts a 12: 1 student-to-faculty ratio. I\u2019ve never been to La Plume, Pennsylvania, but I suspect that most faculty at Keystone College have declared teaching as their number one priority. And although the tuition is about $14,000 per year at Keystone College, I imagine that students receive as much individual attention as they could possibly want. By contrast, at 34,000 students, my institution is the fourteenth largest in the United States, and the student-to-faculty ratio is two and a half times that of Keystone College. Minority students make up the majority at my institution, where the tuition is about $3,000 per year for in-state students. As faculty at a research university, those at my institution are committed to teaching, but know that their research\u00a0\u2026", "num_citations": "4\n", "authors": ["1063"]}
{"title": "Norming and scaling for automated essay scoring\n", "abstract": " Depending on how the scores are derived, the numbers may reflect assignments given at the ordinal, interval, or ratio scale. With ordinally-scaled numbers, the higher the number, the more of the underlying trait or characteristic one possesses. However, the distances between the numbers on the scale are not assumed to be equal. For example, if one uses a writing rubric on a 5-point scale, it likely that the trait discrepancies in performance between a\" 1\" and a\" 2\" are different than between a\" 2\" and a\" 3,\" although the numeric result in both cases reflects 1 point. The exception to this definition comes in the form of\" rank-ordered numbers,\" in which the lower the number, the more of the trait or characteristic one possesses. So, for instance, being ranked first in your high school is better than being ranked 16th.With intervally-scaled numbers, the differences among the numbers are assumed to be equal, though there is no true zero point. So, although one might like to characterize an uncooperative colleague as having\" no intelligence,\" this would technically be inaccurate. What we say is that our intelligence measure is insensitive at the extremes of the cognitive ability spectrum. With ratio-scaled numbers, however, there is a true zero point. For example, it is possible to have\" no money.\" Therefore, money is considered to be on a ratio-scale. Sometimes the true zero point cannot be obtained under normal conditions. One can obtain a state of weightlessness in space, but not on earth. If the assignment of numbers is arbitrary, but consistently applied to categories of performance, then the measurement is said to be at the nominal scale. In this situation\u00a0\u2026", "num_citations": "4\n", "authors": ["1063"]}
{"title": "Where did all the data go? Internet security for web-based assessments.\n", "abstract": " The purpose of this paper is to enumerate a series of security steps that might be taken by those individuals or organizations that are contemplating Web-based tests and performance assessments. From a security viewpoint, much of what goes on with Web-based transactions is similar to other general computer activity, but the recommendations focus on what can be done to avoid data compromise and loss or to resurrect, such information should it be modified. Some very specific advice is offered. An appendix lists Web sites to visit about security.(Contains 12 references.)", "num_citations": "4\n", "authors": ["1063"]}
{"title": "Assessing the reliability of computer adaptive testing branching algorithms using hyperCAT\n", "abstract": " The reliability of four branching algorithms commonly used in computer adaptive testing (CAT) was examined. These algorithms were:(1) maximum likelihood (MLE);(2) Bayesian;(3) modal Bayesian; and (4) crossover. Sixty-eight undergraduate college students were randomly assigned to one of the four conditions using the HyperCard-based CAT program, HyperCAT. As a way to control for order effects, half of the students were randomly assigned to take the paper-and-pencil test first, followed 3 weeks later by the CAT, while the other half took the CAT first. Investigative analyses showed no initial group differences by algorithm for the paper-and-pencil test and for CAT-estimated ability. In addition, there was no order effect. The internal consistency coefficient for the paper-and-pencil test was 0.73. The marginal reliability for the CAT was 0.97. Correlations between the paper-and-pencil scores and theta estimates of\u00a0\u2026", "num_citations": "4\n", "authors": ["1063"]}
{"title": "Use of automated scoring features to generate hypotheses regarding language-based DIF\n", "abstract": " This study uses the feature sets employed by two automated scoring engines to determine if a \u201clinguistic profile\u201d could be formulated that would help identify items that are likely to exhibit differential item functioning (DIF) based on linguistic features. Sixteen items were administered to 1200 students where demographic information was collected on gender and socioeconomic status (SES). Textual features were extracted and analyzed using Differential Item Functioning Analysis System (DIFAS) and the Mantel-Haenszel chi-square, the Liu-Agresti cumulative common log-odds ratio, and Cox's noncentrality parameter estimator to assess the probability that the focal groups (e.g., females) differed significantly from the reference groups (e.g., males). Two of the 14 items were flagged for possible DIF on gender and four were flagged for possible DIF on SES. The responses from the focal and reference groups were then\u00a0\u2026", "num_citations": "3\n", "authors": ["1063"]}
{"title": "Using prizes to facilitate change in educational assessment\n", "abstract": " In this chapter, we discuss one approach to fostering change in educational assessment, the use of incentives to stimulate solutions for assessment challenges that are not adequately solved. We document three studies, two involving prizes that examine the performance of machine scoring with that of trained human raters for both essay and short-answer performance assessments. The results from the three studies suggest that well-managed incentives can permit the leveraging of resources to spur innovation at an efficient cost. Moreover, the use of carefully designed prizes can result in more transparent and open communication in which competitors actually help each other find optimal solutions. The result here might be viewed as a case study on how to foster innovation in educational assessment. To begin with, we examine the histories of two different measurement technologies\u2014computerized adaptive testing (CAT) and automated essay scoring (AES)\u2014both of which were \u201cideas\u201d in the mid 1960s and had working models by the mid 1970s. The two technologies then diverged. Due to its open-source nature, CAT was extensively researched by academic investigators during the 1980s and 1990s, whereas the proprietary nature of AES limited research and its general acceptability. Today CAT (or its variations) is a common approach to testing, whereas AES has had only limited application. We then discuss why prizes might work in the context of educational assessment.", "num_citations": "3\n", "authors": ["1063"]}
{"title": "Controlling test and computer anxiety: Test performance under CAT and SAT conditions\n", "abstract": " Controlling test and computer anxiety: Test performance under CAT and SAT conditions | IACAT Home My Account About Us Contact Us Conferences Facebook Twitter Linkedin Home What is CAT? News & Events Event Calendar Adaptive Testing in the News IACAT Conferences 2021 Conference 2019 IACAT Conference 2017 IACAT Conference 2015 IACAT Conference 2014 IACAT Conference 2012 IACAT Conference 2011 IACAT Conference 2010 First IACAT Conference 2009 CAT Conference 2007 CAT Conference Resources Journal of Computerized Adaptive Testing CAT Software Operational CAT Programs Forums Bibliography Authors Keywords Research Strategies in CAT About US Mission By-Laws IACAT Board Present & Past Contact Us Home Controlling test and computer anxiety: Test performance under CAT and SAT conditions Submitted by daveweiss on Thu, 08/25/2011 - 17:15 Title Controlling \u2026", "num_citations": "3\n", "authors": ["1063"]}
{"title": "Item bias in mathematics achievement: the Progressive Achievement Tests for Mathematics.\n", "abstract": " This study examined differential item performance in the Progressive Achievement Tests for Mathematics (PAT Math) by both Pakeha (reference-majority) and Maori (focal-minority) students in New Zealand using both Item Response Theory (IRT) and non-IRT techniques. The PAT Math (Form A), a 50-item standardised test, used in part to guide within-class and school placement, was suspected of containing problems which might be biased in favour of the reference group. The sample consisted of the 726 Pakeha and 63 Maori students who had taken the test in 1983. After testing for unidimensionality, the data were calibrated using ASCAL. Items were classified by patterns of ICCs that were generated using the two-parameter logistic model. Seven patterns were discovered, indicating different response patterns for the two groups. Results were compared with the Mantel-Haenszel chi-square tests and the Z2 test\u00a0\u2026", "num_citations": "3\n", "authors": ["1063"]}
{"title": "Where did all the data Go? Internet security for web\u2010based assessments\n", "abstract": " The purpose of this article is to enumerate a series of security steps that might be taken by those researchers or organizations that are contemplating web\u2010based tests and performance assessments. From a security viewpoint, much of what goes on with web\u2010based transactions is similar to other general computer activity, but the recommendations here focus on what can be done to avoid the loss, compromising, or modification of data collected by or stored through the Internet.", "num_citations": "2\n", "authors": ["1063"]}
{"title": "Predictive validity of the IUPUI web-based placement test scores for course placement at IUPUI: 1998-1999\n", "abstract": " This annual report addresses a variety of issues regarding the undergraduate placement tests that are mandated for all entering students at IUPUI. Several changes to the placement testing process were implemented by the academic departments recently, cutoff score modifications to the computerized adaptive math placement test and an experimental procedure for the computerized assessment of writing samples to be used with the high school testing activities (Shermis, Mzumara, Harrington, & Olson, 1998). Based on last year\u2019s data, revised placement graphs have been incorporated that indicate the probability of success for a student who achieves a given placement test score (Noble & Sawyer, 1997). We hope that these interpretational aids will provide some additional help to counselors and other academic advisors looking to use the placement tests as one source of information in guiding the student to an appropriate course.The evaluation of the computerized adaptive math test involved samples drawn from fall of 1998 and spring of 1999. A computerized adaptive test is one that conforms to the ability level of the examinee. Several studies have shown this type of test to be just as reliable as the older non-adaptive test, even though it averages to be 30-50% shorter. Our own exit surveys continue to suggest that students like the adaptive tests because they are neither too difficult nor too easy. Moreover, the current computerized adaptive math test, which consists of an item bank of 168 items, addresses security concerns in that each student essentially takes a different form of the test. The placement validity coefficients for this test\u00a0\u2026", "num_citations": "2\n", "authors": ["1063"]}
{"title": "An Assessment of the Concurrent Validity and Reliability of the Merkler Style Preference Inventory (MSPI\n", "abstract": " This study examines three aspects of a new vocational interest assessment, the Merkler Style Preference Inventory (MSPI). The MSPI is a 60-item computer-administered instrument that makes vocational pursuit recommendations based on an interest profile. It also produces a list of recommended college majors. Seventy-eight undergraduates from a large midwestern university participated in a study in which they took the MSPI and either the Strong Interest Inventory-Strong Vocational Interest Battery (SII-SVIB) or the Kuder Preference Record (Kuder) during one evaluation session. In a follow- up 2 weeks later, the participants took the MSPI once again and the inventory that was not administered during the first session. Order of SII-SVIB and Kuder form completion was randomly assigned. Test-retest reliability across the six subscales of the MSPI ranged from .70 to .86 with a median reliability of .79. Internal\u00a0\u2026", "num_citations": "2\n", "authors": ["1063"]}
{"title": "Validity of the IUPUI placement test scores for course placement: 1997-1998\n", "abstract": " This annual report addresses a variety of issues regarding the undergraduate placement tests that are mandated for all entering students at IUPUI. Several improvements to the placement testing process were implemented by the academic departments recently, cutoff score modifications to the computerized adaptive math placement test and an experimental procedure for the computerized assessment of writing samples to be used with the high school testing activities (Shermis, Mzumara, Harrington, & Olson, 1998). Moreover, beginning with last year's report, new graphs have been incorporated that indicate the probability of success for a student who achieves a given placement test score (Noble & Sawyer, 1997). Our hope is that these new interpretational aids will provide some additional help to counselors and other academic advisors looking to use the placement tests as one source of information in guiding the student to an appropriate course. The evaluation of the computerized adaptive math test involved samples drawn from fall of 1997. An adaptive test is one that conforms to the ability level of the examinee. Several studies have shown this type of test to be just as reliable as the older non-adaptive test, even though it averages to be 30-50% shorter. Our own exit surveys also suggest that students like the adaptive tests because they are neither too difficult or too easy. Moreover, the computerized adaptive test, which consists of an item bank of 168 items, addresses security concerns in that each student essentially takes a different form of the test. The placement validity coefficients for this test, calculated on the relationship between the\u00a0\u2026", "num_citations": "2\n", "authors": ["1063"]}
{"title": "Factor structure of the sociocultural scales\n", "abstract": " The measurement of sociocultural characteristics is an accepted component of research as well as clinical practice; however, few standardized measures exist to assist scholars and clinicians in its assessment. The Sociocultural Scales, an important component of the System of Multicultural Pluralistic Assessment (Mercer & Lewis, 1977), is one of the few instruments designed for this use. This study examined the factor structure of the Sociocultural Scales, utilizing data on 436 Anglo, Black, and Mexican-American children from middle- and lower-class homes. Both iterated principal axis and principal components factor analyses were used. Modest support is found for the factor structure of the four major Sociocultural Scales, although only 38% of the total test variance is accounted for under this model, and estimates of internal consistency are generally low. A number of test items seem to detract from the Scales'\u00a0\u2026", "num_citations": "2\n", "authors": ["1063"]}
{"title": "International Applications of Automated Scoring\n", "abstract": " When Page (1966b) prophesied the advent of \u201cautomated essay grading\u201d(ie, machine scoring of writing), he viewed the technology as a \u201cteacher\u2019s helper\u201d based on his belief that the evaluation of writing was a major impediment for teachers assigning more writing in the classroom. For the last 10 years, researchers have been developing automated essay scoring (AES) engines that can be used for languages other than English. In this chapter we present a survey of AES engines developed in both English and other languages in order to provide a benchmark of developments in machines. By having knowledge of work in other languages, the field may be able to address some of the challenges facing machine scoring in English, both for essays and for responses other than essays.ABSTRACT", "num_citations": "1\n", "authors": ["1063"]}
{"title": "Communicating to the Public About Machine Scoring: What Works, What Doesn\u2019t\n", "abstract": " This paper documents six case studies about how to, and how not to, communicate a testing entity\u2019s transition to machine scoring. Data are drawn from four US state-, one Canadian Province-, and one Country\u2019s testing programs. Based on the analysis of the six cases, several tentative recommendations can be made.", "num_citations": "1\n", "authors": ["1063"]}
{"title": "Establishing a crosswalk between the Common European Framework for Languages (CEFR) and writing domains scored by automated essay scoring\n", "abstract": " This article employs the Common European Framework Reference for Language Acquisition (CEFR) as a basis for evaluating writing in the context of machine scoring. The CEFR was designed as a framework for evaluating proficiency levels of speaking for the 49 languages comprising the European Union. The intent was to impact language instruction so that \u201cmastery\u201d of one language has the same meaning as it does in another. A second objective is to provide a crosswalk for what one automated writing evaluation (AWE) system does in attending to the dimensions of the framework. The CEFR Framework is divided into five traits and different proficiency levels. The question then becomes: Does the AWE system attempt to measure these dimensions of writing? And, if so, how is this operationalized? Is it measuring aspects of communication that are not specified? The goal here is to create a common vocabulary\u00a0\u2026", "num_citations": "1\n", "authors": ["1063"]}
{"title": "Using automated scoring to monitor reader performance and detect reader drift in essay scoring\n", "abstract": " With expediencies such as rapid turnaround time and economic benefits associated with reduced scoring costs, automated scoring is widely viewed as on a path to replace human labor, like so many prior innovative technologies. As a result, the focus of automated scoring development has tended to be on validity or measures of accuracy as compared to human efforts. This chapter is focused on a somewhat nearer-term process, the transition from human to machine scoring and how the two scoring methods can be used concurrently. Previously, the authors have referred to the use of both human and machine scoring for a single assessment program as a \u201cblended\u201d scoring model (Lottridge, Mitzel, & Chou, 2009). The central idea is to see if two imperfect scoring methods can be mutually leveraged in some optimal way to improve the overall accuracy of scoring. Although the authors have not yet achieved that goal\u00a0\u2026", "num_citations": "1\n", "authors": ["1063"]}
{"title": "Overview of intelliMetric\n", "abstract": " IntelliMetric\u2122 is an intelligent scoring system that emulates the process carried out by human scorers and is theoretically grounded in the traditions of cognitive processing, computational linguistics, and classification. The system must be \u201ctrained\u201d with a set of previously scored responses containing \u201cknown score\u201d marker papers for each score point These papers are used as a basis for the system to infer the rubric and the pooled judgments of the human scorers. Relying on Vantage Learning\u2019s proprietary CogniSearch\u2122 and Quantum Reasoning\u2122 technologies, the IntelliMetric\u2122 system internalizes the characteristics of the responses associated with each score point and applies this intelligence in subsequent scoring (Vantage Learning, 2001d). The approach is consistent with the procedure underlying holistic scoring. IntelliMetric\u2122 creates a unique solution for each stimulus or prompt This is conceptually similar to prompt-specific training for human scorers. For this reason, IntelliMetric\u2122 is able to achieve both high correlations with the scores of human readers and matching percentages with scores awarded by humans. IntelliMetric\u2122 is based on a blend of artificial intelligence (AI), natural language processing and statistical technologies. It is essentially a learning engine that internalizes the characteristics of the score scale through an iterative learning process. In essence, IntelliMetric\u2122 internalizes the pooled wisdom of many expert scorers. It is important to note that AI is widely believed to better handle \u201cnoisy\u201d data and develop a more sophisticated internalization of complex relations among features. IntelliMetric\u2122 was first commercially\u00a0\u2026", "num_citations": "1\n", "authors": ["1063"]}
{"title": "The Effects of Questionnaire Modification on Response Rates: A Field Experiment.\n", "abstract": " ABSTRACT The Michigan Department of Education conducts an annual follow-up survey to determine the employment and continuing educational status of students who have graduated or withdrawn from its secondary vocational education programs. This standard survey form was modified by presenting a range of values and increasing the question length to improve the rate and the quality of the data collected. Six participating districts were randomly assigned to data collection (mail, phone, or mixed) and were then randomly half-sampled. The overall response rate for both the modified mail and phone format was significantly higher than for either of the standard formats. The lack of a screening question for employment confounds whatever effects the format modifications might have made. The results indicate no pronounaed pattern in the mailed item response rates. Further, it is only if. phone interviewers react positively to the form that one might expect to see improvement in overall response rates for the modified phone group. However, no systemati'c measures of interviewer attitudes towards the standard or modified forms were attempted. The appendices include the standard and modified forms for both the mailed questionnaire and the phone interview.(PN)", "num_citations": "1\n", "authors": ["1063"]}