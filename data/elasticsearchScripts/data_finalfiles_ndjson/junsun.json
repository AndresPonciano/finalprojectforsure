{"title": "Anomaly detection for a water treatment system using unsupervised machine learning\n", "abstract": " In this paper, we propose and evaluate the application of unsupervised machine learning to anomaly detection for a Cyber-Physical System (CPS). We compare two methods: Deep Neural Networks (DNN) adapted to time series data generated by a CPS, and one-class Support Vector Machines (SVM). These methods are evaluated against data from the Secure Water Treatment (SWaT) testbed, a scaled-down but fully operational raw water purification plant. For both methods, we first train detectors using a log generated by SWaT operating under normal conditions. Then, we evaluate the performance of both methods using a log generated by SWaT operating under 36 different attack scenarios. We find that our DNN generates fewer false positives than our one-class SVM while our SVM detects slightly more anomalies. Overall, our DNN has a slightly better F measure than our SVM. We discuss the characteristics of\u00a0\u2026", "num_citations": "152\n", "authors": ["1634"]}
{"title": "Social-Loc: Improving indoor localization with social sensing\n", "abstract": " Location-based services, such as targeted advertisement, geo-social networking and emergency services, are becoming increasingly popular for mobile applications. While GPS provides accurate outdoor locations, accurate indoor localization schemes still require either additional infrastructure support (eg, ranging devices) or extensive training before system deployment (eg, WiFi signal fingerprinting). In order to help existing localization systems to overcome their limitations or to further improve their accuracy, we propose Social-Loc, a middleware that takes the potential locations for individual users, which is estimated by any underlying indoor localization system as input and exploits both social encounter and non-encounter events to cooperatively calibrate the estimation errors. We have fully implemented Social-Loc on the Android platform and demonstrated its performance on two underlying indoor localization\u00a0\u2026", "num_citations": "97\n", "authors": ["1634"]}
{"title": "Adversarial sample detection for deep neural network through model mutation testing\n", "abstract": " Deep neural networks (DNN) have been shown to be useful in a wide range of applications. However, they are also known to be vulnerable to adversarial samples. By transforming a normal sample with some carefully crafted human imperceptible perturbations, even highly accurate DNN make wrong decisions. Multiple defense mechanisms have been proposed which aim to hinder the generation of such adversarial samples. However, a recent work show that most of them are ineffective. In this work, we propose an alternative approach to detect adversarial samples at runtime. Our main observation is that adversarial samples are much more sensitive than normal samples if we impose random mutations on the DNN. We thus first propose a measure of 'sensitivity' and show empirically that normal samples and adversarial samples have distinguishable sensitivity. We then integrate statistical hypothesis testing and\u00a0\u2026", "num_citations": "95\n", "authors": ["1634"]}
{"title": "Learning from mutants: Using code mutation to learn and monitor invariants of a cyber-physical system\n", "abstract": " Cyber-physical systems (CPS) consist of sensors, actuators, and controllers all communicating over a network; if any subset becomes compromised, an attacker could cause significant damage. With access to data logs and a model of the CPS, the physical effects of an attack could potentially be detected before any damage is done. Manually building a model that is accurate enough in practice, however, is extremely difficult. In this paper, we propose a novel approach for constructing models of CPS automatically, by applying supervised machine learning to data traces obtained after systematically seeding their software components with faults (\"mutants\"). We demonstrate the efficacy of this approach on the simulator of a real-world water purification plant, presenting a framework that automatically generates mutants, collects data traces, and learns an SVM-based model. Using cross-validation and statistical model\u00a0\u2026", "num_citations": "83\n", "authors": ["1634"]}
{"title": "Model checking linearizability via refinement\n", "abstract": " Linearizability is an important correctness criterion for implementations of concurrent objects. Automatic checking of linearizability is challenging because it requires checking that 1) all executions of concurrent operations be serializable, and 2) the serialized executions be correct with respect to the sequential semantics. This paper describes a new method to automatically check linearizability based on refinement relations from abstract specifications to concrete implementations. Our method avoids the often difficult task of determining linearization points in implementations, but can also take advantage of linearization points if they are given. The method exploits model checking of finite state systems specified as concurrent processes with shared variables. Partial order reduction is used to effectively reduce the search space. The approach is built into a toolset that supports a rich set of concurrent operators\u00a0\u2026", "num_citations": "77\n", "authors": ["1634"]}
{"title": "Auditing anti-malware tools by evolving android malware and dynamic loading technique\n", "abstract": " Although a previous paper shows that existing anti-malware tools (AMTs) may have high detection rate, the report is based on existing malware and thus it does not imply that AMTs can effectively deal with future malware. It is desirable to have an alternative way of auditing AMTs. In our previous paper, we use malware samples from android malware collection Genome to summarize a malware meta-model for modularizing the common attack behaviors and evasion techniques in reusable features. We then combine different features with an evolutionary algorithm, in which way we evolve malware for variants. Previous results have shown that the existing AMTs only exhibit detection rate of 20%-30% for 10 000 evolved malware variants. In this paper, based on the modularized attack features, we apply the dynamic code generation and loading techniques to produce malware, so that we can audit the AMTs at\u00a0\u2026", "num_citations": "62\n", "authors": ["1634"]}
{"title": "scompile: Critical path identification and analysis for smart contracts\n", "abstract": " Ethereum smart contracts are an innovation built on top of the blockchain technology, which provides a platform for automatically executing contracts in an anonymous, distributed, and trusted way. The problem is magnified by the fact that smart contracts, unlike ordinary programs, cannot be patched easily once deployed. It is important for smart contracts to be checked against potential vulnerabilities. In this work, we propose an alternative approach to automatically identify critical program paths (with multiple function calls including inter-contract function calls) in a smart contract, rank the paths according to their criticalness, discard them if they are infeasible or otherwise present them with user friendly warnings for user inspection. We identify paths which involve monetary transaction as critical paths, and prioritize those which potentially violate important properties. For scalability, symbolic execution\u00a0\u2026", "num_citations": "52\n", "authors": ["1634"]}
{"title": "Model checking hierarchical probabilistic systems\n", "abstract": " Probabilistic modeling is important for random distributed algorithms, bio-systems or decision processes. Probabilistic model checking is a systematic way of analyzing finite-state probabilistic models. Existing probabilistic model checkers have been designed for simple systems without hierarchy. In this paper, we extend the PAT toolkit to support probabilistic model checking of hierarchical complex systems. We propose to use PCSP#, a combination of Hoare\u2019s CSP with data and probability, to model such systems. In addition to temporal logic, we allow complex safety properties to be specified by non-probabilistic PCSP# model. Validity of the properties (with probability) is established by refinement checking. Furthermore, we show that refinement checking can be applied to verify probabilistic systems against safety/co-safety temporal logic properties efficiently. We demonstrate the usability and scalability of\u00a0\u2026", "num_citations": "44\n", "authors": ["1634"]}
{"title": "Detection and classification of malicious JavaScript via attack behavior modelling\n", "abstract": " Existing malicious JavaScript (JS) detection tools and commercial anti-virus tools mostly use feature-based or signature-based approaches to detect JS malware. These tools are weak in resistance to obfuscation and JS malware variants, not mentioning about providing detailed information of attack behaviors. Such limitations root in the incapability of capturing attack behaviors in these approches. In this paper, we propose to use Deterministic Finite Automaton (DFA) to abstract and summarize common behaviors of malicious JS of the same attack type. We propose an automatic behavior learning framework, named JS*, to learn DFAs from dynamic execution traces of JS malware, where we implement an effective online teacher by combining data dependency analysis, defense rules and trace replay mechanism. We evaluate JS* using real world data of 10000 benign and 276 malicious JS samples to cover 8 most\u00a0\u2026", "num_citations": "39\n", "authors": ["1634"]}
{"title": "Detecting adversarial samples for deep neural networks through mutation testing\n", "abstract": " Recently, it has been shown that deep neural networks (DNN) are subject to attacks through adversarial samples. Adversarial samples are often crafted through adversarial perturbation, i.e., manipulating the original sample with minor modifications so that the DNN model labels the sample incorrectly. Given that it is almost impossible to train perfect DNN, adversarial samples are shown to be easy to generate. As DNN are increasingly used in safety-critical systems like autonomous cars, it is crucial to develop techniques for defending such attacks. Existing defense mechanisms which aim to make adversarial perturbation challenging have been shown to be ineffective. In this work, we propose an alternative approach. We first observe that adversarial samples are much more sensitive to perturbations than normal samples. That is, if we impose random perturbations on a normal and an adversarial sample respectively, there is a significant difference between the ratio of label change due to the perturbations. Observing this, we design a statistical adversary detection algorithm called nMutant (inspired by mutation testing from software engineering community). Our experiments show that nMutant effectively detects most of the adversarial samples generated by recently proposed attacking methods. Furthermore, we provide an error bound with certain statistical significance along with the detection.", "num_citations": "37\n", "authors": ["1634"]}
{"title": "Satisfiability modulo heap-based programs\n", "abstract": " In this work, we present a semi-decision procedure for a fragment of separation logic with user-defined predicates and Presburger arithmetic. To check the satisfiability of a formula, our procedure iteratively unfolds the formula and examines the derived disjuncts. In each iteration, it searches for a proof of either satisfiability or unsatisfiability. Our procedure is further enhanced with automatically inferred invariants as well as detection of cyclic proof. We also identify a syntactically restricted fragment of the logic for which our procedure is terminating and thus complete. This decidable fragment is relatively expressive as it can capture a range of sophisticated data structures with non-trivial pure properties, such as size, sortedness and near-balanced. We have implemented the proposed solver and a new system for verifying heap-based programs. We have evaluated our system on benchmark programs from a\u00a0\u2026", "num_citations": "37\n", "authors": ["1634"]}
{"title": "Verification of functional and non-functional requirements of web service composition\n", "abstract": " Web services have emerged as an important technology nowadays. There are two kinds of requirements that are crucial to web service composition, which are functional and non-functional requirements. Functional requirements focus on functionality of the composed service, e.g., given a booking service, an example of functional requirements is that a flight ticket with price higher than $2000 will never be purchased. Non-functional requirements are concerned with the quality of service (QoS), e.g., an example of the booking service\u2019s non-functional requirements is that the service will respond to the user within 5 seconds. Non-functional requirements are important to web service composition, and are often an important clause in service-level agreements (SLAs). Even though the functional requirements are satisfied, a slow or unreliable service may still not be adopted. In our paper, we propose an\u00a0\u2026", "num_citations": "36\n", "authors": ["1634"]}
{"title": "IBED: Combining IBEA and DE for optimal feature selection in software product line engineering\n", "abstract": " Software configuration, which aims to customize the software for different users (e.g., Linux kernel configuration), is an important and complicated task. In software product line engineering (SPLE), feature oriented domain analysis is adopted and feature model is used to guide the configuration of new product variants. In SPLE, product configuration is an optimal feature selection problem, which needs to find a set of features that have no conflicts and meanwhile achieve multiple design objectives (e.g., minimizing cost and maximizing the number of features). In previous studies, several multi-objective evolutionary algorithms (MOEAs) were used for the optimal feature selection problem and indicator-based evolutionary algorithm (IBEA) was proven to be the best MOEA for this problem. However, IBEA still suffers from the issues of correctness and diversity of found solutions. In this paper, we propose a dual\u00a0\u2026", "num_citations": "34\n", "authors": ["1634"]}
{"title": "Tzuyu: Learning stateful typestates\n", "abstract": " Behavioral models are useful for various software engineering tasks. They are, however, often missing in practice. Thus, specification mining was proposed to tackle this problem. Existing work either focuses on learning simple behavioral models such as finite-state automata, or relies on techniques (e.g., symbolic execution) to infer finite-state machines equipped with data states, referred to as stateful typestates. The former is often inadequate as finite-state automata lack expressiveness in capturing behaviors of data-rich programs, whereas the latter is often not scalable. In this work, we propose a fully automated approach to learn stateful typestates by extending the classic active learning process to generate transition guards (i.e., propositions on data states). The proposed approach has been implemented in a tool called TzuYu and evaluated against a number of Java classes. The evaluation results show that\u00a0\u2026", "num_citations": "32\n", "authors": ["1634"]}
{"title": "Double negative metamaterial sensor based on microring resonator\n", "abstract": " Metamaterials are artificial media structured on a size scale smaller than the wavelength of external stimuli, and may provide novel tools to significantly enhance the sensitivity and resolution of the sensors. In this paper, we derive the dispersion relation of cylindrical dielectric waveguide loaded on double negative metamaterial (DNM) layer, and compute the resonant frequencies and electric field distribution of the corresponding Whispering-Gallery-Modes (WGM). Theoretical results of resonant frequency and electric field distribution are in good agreement with the simulation results. We show that the DNM sensor based on microring resonator possesses higher sensitivity than the traditional microring sensor since the amplification of evanescent wave, and with the increase of metamaterial layer thickness, the sensitivity will be increased greatly. It might open an avenue for designing perfect sensors.", "num_citations": "31\n", "authors": ["1634"]}
{"title": "Synthesis of distributed processes from scenario-based specifications\n", "abstract": " Given a set of sequence diagrams, the problem of synthesis is of deciding whether there exists a satisfying object system and if so, synthesize one automatically. It is crucial in the development of complex systems, since sequence diagrams serve as the manifestation of use cases and if synthesizable they could lead directly to implementation. It is even more interesting (and harder) if the synthesized object system is distributed. In this paper, we propose a systematic way of synthesizing distributed processes from Live Sequence Charts. The basic idea is to first construct a CSP specification from the LSC specification, and then use CSP algebraic laws to group the behaviors of each object effectively. The key point is that the behaviors of each object can be decided locally without constructing the global state machine.", "num_citations": "30\n", "authors": ["1634"]}
{"title": "A decidable fragment in separation logic with inductive predicates and arithmetic\n", "abstract": " We consider the satisfiability problem for a fragment of separation logic including inductive predicates with shape and arithmetic properties. We show that the fragment is decidable if the arithmetic properties can be represented as semilinear sets. Our decision procedure is based on a novel algorithm to infer a finite representation for each inductive predicate which precisely characterises its satisfiability. Our analysis shows that the proposed algorithm runs in exponential time in the worst case. We have implemented our decision procedure and integrated it into an existing verification system. Our experiment on benchmarks shows that our procedure helps to verify the benchmarks effectively.", "num_citations": "29\n", "authors": ["1634"]}
{"title": "Verifying linearizability via optimized refinement checking\n", "abstract": " Linearizability is an important correctness criterion for implementations of concurrent objects. Automatic checking of linearizability is challenging because it requires checking that: (1) All executions of concurrent operations are serializable, and (2) the serialized executions are correct with respect to the sequential semantics. In this work, we describe a method to automatically check linearizability based on refinement relations from abstract specifications to concrete implementations. The method does not require that linearization points in the implementations be given, which is often difficult or impossible. However, the method takes advantage of linearization points if they are given. The method is based on refinement checking of finite-state systems specified as concurrent processes with shared variables. To tackle state space explosion, we develop and apply symmetry reduction, dynamic partial order reduction, and\u00a0\u2026", "num_citations": "29\n", "authors": ["1634"]}
{"title": "Model checking live sequence charts\n", "abstract": " Live sequence charts (LSCs) are a broad extension to message sequence charts (MSCs) to capture complex inter-object communication rigorously. A tool support for LSCs, named PlayEngine, is developed to interactively \"play-in\" and \"play-out\" scenarios. However, PlayEngine cannot automatically expose system design inconsistencies, e.g., conflicts between universal charts and etc. CSP is a formal language to specify sequential behaviors of a process and communication between processes, which has powerful tool supports, e.g., FDR. Semantically, system behaviors specified by LSCs correspond to CSP's traces and failures. This close semantic correspondence makes FDR a potential model checker for LSCs. The challenge is to discover a systematic way of constructing semantic preserving CSP models from LSCs. In this work, we investigate theoretical relations between LSCs and CSP. LSCs are formalized\u00a0\u2026", "num_citations": "29\n", "authors": ["1634"]}
{"title": "Scalable multi-core model checking fairness enhanced systems\n", "abstract": " Rapid development in hardware industry has brought the prevalence of multi-core systems with shared-memory, which enabled the speedup of various tasks by using parallel algorithms. The Linear Temporal Logic (LTL) model checking problem is one of the difficult problems to be parallelized or scaled up to multi-core. In this work, we propose an on-the-fly parallel model checking algorithm based on the Tarjan\u2019s strongly connected components (SCC) detection algorithm. The approach can be applied to general LTL model checking or with different fairness assumptions. Further, it is orthogonal to state space reduction techniques like partial order reduction. We enhance our PAT model checker with the technique and show its usability via the automated verification of several real-life systems. Experimental results show that our approach is scalable, especially when a system search space contains many\u00a0\u2026", "num_citations": "28\n", "authors": ["1634"]}
{"title": "A scalable approach to multi-style architectural modeling and verification\n", "abstract": " Software Architecture represents the high level description of a system in terms of components, external properties and communication. Despite its importance in the software engineering process, the lack of formal description and verification support limits the value of developing architectural models. Automated formal engineering methods can provide an effective means to precisely describe and rigorously verify intended structures and behaviors of software systems. In this paper, we present an approach to support the design and verification of software architectural models using the Alloy analyzer. Based on our earlier work, we propose a fundamental library for specifying system structures in terms of different architectural styles. We illustrate use of the architecture style library in modeling and verifying a complex system that utilizes multi-style structures. To promote scalability, we use model decomposition to\u00a0\u2026", "num_citations": "28\n", "authors": ["1634"]}
{"title": "Veriws: a tool for verification of combined functional and non-functional requirements of web service composition\n", "abstract": " Web service composition is an emerging technique to develop Web applications by composing existing Web services. Web service composition is subject to two important classes of requirements, ie, functional and non-functional requirements. Both are crucial to Web service composition. Therefore, it is desirable to verify combined functional and non-functional requirements for Web service composition. We present VeriWS, a tool to verify combined functional and non-functional requirements of Web service composition. VeriWS captures the semantics of Web service composition and verifies it directly based on the semantics. We also show how to describe Web service composition and properties using VeriWS. The YouTube video for demonstration of VeriWS is available at https://sites. google. com/site/veriwstool/.", "num_citations": "27\n", "authors": ["1634"]}
{"title": "Dynamic synthesis of local time requirement for service composition\n", "abstract": " Service composition makes use of existing service-based applications as components to achieve a business goal. In time critical business environments, the response time of a service is crucial, which is also reflected as a clause in service level agreements (SLAs) between service providers and service users. To allow the composite service to fulfill the response time requirement as promised, it is important to find a feasible set of component services, such that their response time could collectively allow the satisfaction of the response time of the composite service. In this work, we propose a fully automated approach to synthesize the response time requirement of component services, in the form of a constraint on the local response times, that guarantees the global response time requirement. Our approach is based on parameter synthesis techniques for real-time systems. It has been implemented and evaluated\u00a0\u2026", "num_citations": "27\n", "authors": ["1634"]}
{"title": "Learning-guided network fuzzing for testing cyber-physical system defences\n", "abstract": " The threat of attack faced by cyber-physical systems (CPSs), especially when they play a critical role in automating public infrastructure, has motivated research into a wide variety of attack defence mechanisms. Assessing their effectiveness is challenging, however, as realistic sets of attacks to test them against are not always available. In this paper, we propose smart fuzzing, an automated, machine learning guided technique for systematically finding 'test suites' of CPS network attacks, without requiring any knowledge of the system's control programs or physical processes. Our approach uses predictive machine learning models and metaheuristic search algorithms to guide the fuzzing of actuators so as to drive the CPS into different unsafe physical states. We demonstrate the efficacy of smart fuzzing by implementing it for two real-world CPS testbeds-a water purification plant and a water distribution system-finding\u00a0\u2026", "num_citations": "26\n", "authors": ["1634"]}
{"title": "An adaptive Markov strategy for defending smart grid false data injection from malicious attackers\n", "abstract": " We present a novel defending strategy, adaptive Markov strategy (AMS), to protect a smart-grid system from being attacked by unknown attackers with unpredictable and dynamic behaviors. One significant merit of deploying AMS to defend the system is that it is theoretically guaranteed to converge to a best response strategy against any stationary attacker, and converge to a Nash equilibrium (NE) in case of self-play (the attacker is intelligent enough to use AMS to attack). The effectiveness of AMS is evaluated by considering the class of the data integrity attacks in which an attacker manages to inject false voltage information into the intelligent voltage controller in a substation. This kind of attack may cause load shedding and potentially a blackout. We perform extensive simulations using a number of IEEE standard test cases of different scales (different number of buses). Our simulation results indicate that AMS\u00a0\u2026", "num_citations": "26\n", "authors": ["1634"]}
{"title": "Automatic early defects detection in use case documents\n", "abstract": " Use cases, as the primary techniques in the user requirement analysis, have been widely adopted in the requirement engineering practice. As developed early, use cases also serve as the basis for function requirement development, system design and testing. Errors in the use cases could potentially lead to problems in the system design or implementation. It is thus highly desirable to detect errors in use cases. Automatically analyzing use case documents is challenging primarily because they are written in natural languages. In this work, we aim to achieve automatic defect detection in use case documents by leveraging on advanced parsing techniques. In our approach, we first parse the use case document using dependency parsing techniques. The parsing results of each use case are further processed to form an activity diagram. Lastly, we perform defect detection on the activity diagrams. To evaluate our\u00a0\u2026", "num_citations": "26\n", "authors": ["1634"]}
{"title": "An efficient algorithm for learning event-recording automata\n", "abstract": " In inference of untimed regular languages, given an unknown language to be inferred, an automaton is constructed to accept the unknown language from answers to a set of membership queries each of which asks whether a string is contained in the unknown language. One of the most well-known regular inference algorithms is the L* algorithm, proposed by Angluin in 1987, which can learn a minimal deterministic finite automaton (DFA) to accept the unknown language. In this work, we propose an efficient polynomial time learning algorithm, TL*, for timed regular language accepted by event-recording automata. Given an unknown timed regular language, TL* first learns a DFA accepting the untimed version of the timed language, and then passively refines the DFA by adding time constraints. We prove the correctness, termination, and minimality of the proposed TL* algorithm.", "num_citations": "26\n", "authors": ["1634"]}
{"title": "Model-based Methods for Linking Web Service Choreography and Orchestration\n", "abstract": " In recent years, many Web service composition languages have been proposed. Web service choreography describes collaboration protocols of cooperating Web service participants from a global view. Web service orchestration describes collaboration of the Web services in predefined patterns based on local decision about their interactions with one another at the message/execution level. In this work, we present model-based methods to close the gap between the two views. Building on the strength of model checking techniques, Web service choreography and orchestration are verified against temporal properties or against each other (to show that they are consistent). Specialized optimization techniques are developed to handle large Web service models. Furthermore, we propose a method to mechanically synthesize a prototype Web service orchestration from choreography, by repairing the choreography if\u00a0\u2026", "num_citations": "26\n", "authors": ["1634"]}
{"title": "Parameter synthesis for hierarchical concurrent real-time systems\n", "abstract": " Modeling and verifying complex real-time systems, involving timing delays, are notoriously difficult problems. Checking the correctness of a system for one particular value for each delay does not give any information for other values. It is thus interesting to reason parametrically, by considering that the delays are parameters (unknown constants) and synthesizing a constraint guaranteeing a correct behavior. We present here Parametric Stateful Timed Communicating Sequential Processes, a language capable of specifying and verifying parametric hierarchical real-time systems with complex data structures. Although we prove that the synthesis is undecidable in general, we present several semi-algorithms for efficient parameter synthesis, which behave well in practice. This work has been implemented in a real-time model checker, PSyHCoS, and validated on a set of case studies.", "num_citations": "24\n", "authors": ["1634"]}
{"title": "White-box fairness testing through adversarial sampling\n", "abstract": " Although deep neural networks (DNNs) have demonstrated astonishing performance in many applications, there are still concerns on their dependability. One desirable property of DNN for applications with societal impact is fairness (ie, non-discrimination). In this work, we propose a scalable approach for searching individual discriminatory instances of DNN. Compared with state-of-the-art methods, our approach only employs lightweight procedures like gradient computation and clustering, which makes it significantly more scalable than existing methods. Experimental results show that our approach explores the search space more effectively (9 times) and generates much more individual discriminatory instances (25 times) using much less time (half to 1/7).", "num_citations": "23\n", "authors": ["1634"]}
{"title": "Bounded model checking of compositional processes\n", "abstract": " Verification techniques like SAT-based bounded model checking have been successfully applied to a variety of system models. Applying bounded model checking to compositional process algebras is, however, not a trivial task. One challenge is that the number of system states for process algebra models is not statically known, whereas exploring the full state space is computationally expensive. This paper presents a compositional encoding of hierarchical processes as SAT problems and then applies state-of-the-art SAT solvers for bounded model checking. The encoding avoids exploring the full state space for complex systems so as to deal with state space explosion. We developed an automated analyzer which combines complementing model checking techniques (i.e., bounded model checking and explicit on-the-fly model checking) to validate system models against event-based temporal properties. The\u00a0\u2026", "num_citations": "23\n", "authors": ["1634"]}
{"title": "Automatic loop-invariant generation anc refinement through selective sampling\n", "abstract": " Automatic loop-invariant generation is important in program analysis and verification. In this paper, we propose to generate loop-invariants automatically through learning and verification. Given a Hoare triple of a program containing a loop, we start with randomly testing the program, collect program states at run-time and categorize them based on whether they satisfy the invariant to be discovered. Next, classification techniques are employed to generate a candidate loop-invariant automatically. Afterwards, we refine the candidate through selective sampling so as to overcome the lack of sufficient test cases. Only after a candidate invariant cannot be improved further through selective sampling, we verify whether it can be used to prove the Hoare triple. If it cannot, the generated counterexamples are added as new tests and we repeat the above process. Furthermore, we show that by introducing a path-sensitive\u00a0\u2026", "num_citations": "22\n", "authors": ["1634"]}
{"title": "XML-based static type checking and dynamic visualization for TCOZ\n", "abstract": " Timed Communicating Object Z(TCOZ) combines Object-Z\u2019s strengths in modelling complex data and state with TCSP\u2019s strengths in modeling real-time concurrency. Based on our previous work on the XML environment for TCOZ, this paper firstly demonstrates the development of a type checker for detecting static semantic errors of the TCOZ specification, then illustrates a transformation tool to automatically project TCOZ models into UML statechart diagrams for visualising the dynamic system behaviour.", "num_citations": "22\n", "authors": ["1634"]}
{"title": "Sequential schemes for frequentist estimation of properties in statistical model checking\n", "abstract": " Statistical Model Checking (SMC) is an approximate verification method that overcomes the state space explosion problem for probabilistic systems by Monte Carlo simulations. Simulations might be however costly if many samples are required. It is thus necessary to implement efficient algorithms to reduce the sample size while preserving precision and accuracy. In the literature, some sequential schemes have been provided for the estimation of property occurrence based on predefined confidence and absolute or relative error. Nevertheless, these algorithms remain conservative and may result in huge sample sizes if the required precision standards are demanding. In this article, we compare some useful bounds and some sequential methods based on frequentist estimations. We propose outperforming and rigorous alternative schemes, based on Massart bounds and robust confidence intervals. Our\u00a0\u2026", "num_citations": "21\n", "authors": ["1634"]}
{"title": "Battery-aware mobile data service\n", "abstract": " Significant research has been devoted to reduce the energy consumption of mobile devices, but how to increase their energy supply has received far less attention. Moreover, reducing the energy consumption alone does not always extend the device operation time due to a unique battery property - the capacity it delivers hinges critically upon how it is discharged. In this paper, we propose B-MODS, a novel design of battery-aware mobile data service on mobile devices. B-MODS constructs battery-friendly discharge patterns utilizing the recovery effect so as to increase the capacity delivered from batteries while meeting data service requirements. We implement B-MODS as an application layer library on the Android platform. Our experiments with diverse mobile devices under various application scenarios have shown that B-MODS increases the capacity delivery from the battery by up to 49.5 percent, with which an\u00a0\u2026", "num_citations": "21\n", "authors": ["1634"]}
{"title": "All your sessions are belong to us: Investigating authenticator leakage through backup channels on android\n", "abstract": " Security of authentication protocols heavily relies on the confidentiality of credentials (or authenticators) like passwords and session IDs. However, unlike browser-based web applications for which highly evolved browsers manage the authenticators, Android apps have to construct their own management. We find that most apps simply locate their authenticators into the persistent storage and entrust underlying Android OS for mediation. Consequently, these authenticators can be leaked through compromised backup channels. In this work, we conduct the first systematic investigation on this previously overlooked attack vector. We find that nearly all backup apps on Google Play inadvertently expose backup data to any app with internet and SD card permissions. With this exposure, the malicious apps can steal other apps' authenticators and obtain complete control over the authenticated sessions. We show that this\u00a0\u2026", "num_citations": "21\n", "authors": ["1634"]}
{"title": "A formal semantic model of the semantic web service ontology (WSMO)\n", "abstract": " Semantic Web services, one of the most significant research areas within the semantic Web vision, has attracted increasing attention from both the research community and industry. The Web service modelling ontology (WSMO) has recently been proposed as an enabling framework for the total/partial automation of the tasks (e.g., discovery, selection, composition, mediation, execution, monitoring, etc.) involved in both intra- and inter-enterprise integration of Web services. To support the standardization and tool support of WSMO, a formal semantics of the language is highly desirable. As there are a few variants of WSMO and it is still under development, the semantics of WSMO needs to be formally defined to facilitate easy reuse and future development. In this paper, we present a formal object-Z semantics of WSMO. Different aspects of the language have been precisely defined within one unified framework. This\u00a0\u2026", "num_citations": "21\n", "authors": ["1634"]}
{"title": "Efficient and robust emergence of norms through heuristic collective learning\n", "abstract": " In multiagent systems, social norms serves as an important technique in regulating agents\u2019 behaviors to ensure effective coordination among agents without a centralized controlling mechanism. In such a distributed environment, it is important to investigate how a desirable social norm can be synthesized in a bottom-up manner among agents through repeated local interactions and learning techniques. In this article, we propose two novel learning strategies under the collective learning framework, collective learning EV-l and collective learning EV-g, to efficiently facilitate the emergence of social norms. Extensive simulations results show that both learning strategies can support the emergence of desirable social norms more efficiently and be applicable in a wider range of multiagent interaction scenarios compared with previous work. The influence of different topologies is investigated, which shows that the\u00a0\u2026", "num_citations": "20\n", "authors": ["1634"]}
{"title": "Optimizing selection of competing services with probabilistic hierarchical refinement\n", "abstract": " Recently, many large enterprises (e.g., Netflix, Amazon) have decomposed their monolithic application into services, and composed them to fulfill their business functionalities. Many hosting services on the cloud, with different Quality of Service (QoS) (e.g., availability, cost), can be used to host the services. This is an example of competing services. QoS is crucial for the satisfaction of users. It is important to choose a set of services that maximize the overall QoS, and satisfy all QoS requirements for the service composition. This problem, known as optimal service selection, is NP-hard. Therefore, an effective method for reducing the search space and guiding the search process is highly desirable. To this end, we introduce a novel technique, called Probabilistic Hierarchical Refinement (PROHR). PROHR effectively reduces the search space by removing competing services that cannot be part of the selection. PROHR\u00a0\u2026", "num_citations": "20\n", "authors": ["1634"]}
{"title": "Semantic understanding of smart contracts: Executable operational semantics of solidity\n", "abstract": " Bitcoin has been a popular research topic recently. Ethereum (ETH), a second generation of cryptocurrency, extends Bitcoin's design by offering a Turing-complete programming language called Solidity to develop smart contracts. Smart contracts allow creditable execution of contracts on EVM (Ethereum Virtual Machine) without third parties. Developing correct and secure smart contracts is challenging due to the decentralized computation nature of the blockchain. Buggy smart contracts may lead to huge financial loss. Furthermore, smart contracts are very hard, if not impossible, to patch once they are deployed. Thus, there is a recent surge of interest in analyzing and verifying smart contracts. While most of the existing works either focus on EVM bytecode or translate Solidity smart contracts into programs in intermediate languages, we argue that it is important and necessary to understand and formally define the\u00a0\u2026", "num_citations": "19\n", "authors": ["1634"]}
{"title": "Towards learning and verifying invariants of cyber-physical systems by code mutation\n", "abstract": " Cyber-physical systems (CPS), which integrate algorithmic control with physical processes, often consist of physically distributed components communicating over a network. A malfunctioning or compromised component in such a CPS\u00a0can lead to costly consequences, especially in the context of public infrastructure. In this short paper, we argue for the importance of constructing invariants (or models) of the physical behaviour exhibited by CPS, motivated by their applications to the control, monitoring, and attestation of components. To achieve this despite the inherent complexity of CPS, we propose a new technique for learning invariants that combines machine learning with ideas from mutation testing. We present a preliminary study on a water treatment system that suggests the efficacy of this approach, propose strategies for establishing confidence in the correctness of invariants, then summarise some\u00a0\u2026", "num_citations": "19\n", "authors": ["1634"]}
{"title": "More anti-chain based refinement checking\n", "abstract": " Refinement checking plays an important role in system verification. It establishes properties of an implementation by showing a refinement relationship between the implementation and a specification. Recently, it has been shown that anti-chain based approaches increase the efficiency of trace refinement checking significantly. In this work, we study the problem of adopting anti-chain for stable failures refinement checking, failures-divergence refinement checking and probabilistic refine checking (i.e., a probabilistic implementation against a non-probabilistic specification). We show that the first two problems can be significantly improved, because the state space of the product model may be reduced dramatically. Though applying anti-chain for probabilistic refinement checking is more complicated, we manage to show improvements in some cases. We have integrated these techniques into the PAT model\u00a0\u2026", "num_citations": "19\n", "authors": ["1634"]}
{"title": "Improved BDD-based discrete analysis of timed systems\n", "abstract": " Model checking timed systems through digitization is relatively easy, compared to zone-based approaches. The applicability of digitization, however, is limited mainly for two reasons, i.e., it is only sound for closed timed systems; and clock ticks cause state space explosion. The former is mild as many practical systems are subject to digitization. It has been shown that BDD-based techniques can be used to tackle the latter to some extent. In this work, we significantly improve the existing approaches by keeping the ticks simple in the BDD encoding. Taking advantage of the \u2018simple\u2019 nature of clock ticks, we fine-tune the encoding of ticks and are able to verify systems with many ticks. Furthermore, we develop a BDD library which supports not only encoding/verifying of timed state machines (through digitization) but also composing timed components using a rich set of composition functions. The usefulness and\u00a0\u2026", "num_citations": "19\n", "authors": ["1634"]}
{"title": "PRTS: An Approach for Model Checking Probabilistic Real-Time Hierarchical Systems\n", "abstract": " Model Checking real-life systems is always difficult since such systems usually have quantitative timing factors and work in unreliable environment. The combination of real-time and probability in hierarchical systems presents a unique challenge to system modeling and analysis. In this work, we develop an automated approach for verifying probabilistic, real-time, hierarchical systems. Firstly, a modeling language called PRTS is defined, which combines data structures, real-time and probability. Next, a zone-based method is used to build a finite-state abstraction of PRTS models so that probabilistic model checking could be used to calculate the probability of a system satisfying certain property. We implemented our approach in the PAT model checker and conducted experiments with real-life case studies.", "num_citations": "19\n", "authors": ["1634"]}
{"title": "GPU accelerated counterexample generation in LTL model checking\n", "abstract": " Strongly Connected Component (SCC) based searching is one of the most popular LTL model checking algorithms. When the SCCs are huge, the counterexample generation process can be time-consuming, especially when dealing with fairness assumptions. In this work, we propose a GPU accelerated counterexample generation algorithm, which improves the performance by parallelizing the Breadth First Search (BFS) used in the counterexample generation. BFS work is irregular, which means it is hard to allocate resources and may suffer from imbalanced load. We make use of the features of latest CUDA Compute Architecture-NVIDIA Kepler GK110 to achieve the dynamic parallelism and memory hierarchy so as to handle the irregular searching pattern in BFS. We build dynamic queue management, task scheduler and path recording such that the counterexample generation process can be completely\u00a0\u2026", "num_citations": "18\n", "authors": ["1634"]}
{"title": "Diamonds are a girl\u2019s best friend: Partial order reduction for timed automata with abstractions\n", "abstract": " A major obstacle for using partial order reduction in the context of real time verification is that the presence of clocks and clock constraints breaks the usual diamond structure of otherwise independent transitions. This is especially true when information of the relative values of clocks is preserved in the form of diagonal constraints. However, when diagonal constraints are relaxed by a suitable abstraction, some diamond structure is re-introduced in the zone graph. In this article, we introduce a variant of the stubborn set method for reducing an abstracted zone graph. Our method works with all abstractions, but especially targets situations where one abstract execution can simulate several permutations of the corresponding concrete execution, even though it might not be able to simulate the permutations of the abstract execution. We define independence relations that capture this \u201chidden\u201d diamond structure\u00a0\u2026", "num_citations": "18\n", "authors": ["1634"]}
{"title": "Model checking software architecture design\n", "abstract": " Software Architecture plays an essential role in the high level description of a system design. Despite its importance in the software engineering practice, the lack of formal description and verification support hinders the development of quality architectural models. In this paper, we present an automated approach to the modeling and verification of software architecture designs using the Process Analysis Toolkit (PAT). We present the formal syntax of the Wright# architecture description language together with its operational semantics in Labeled Transition System (LTS). A dedicated model checking module for Wright# is implemented in the PAT verification framework based on the proposed formalism. The module - ADL supports verification and simulation of software architecture models in PAT. We advance our work via defining an architecture style library that embodies commonly used architecture patterns to\u00a0\u2026", "num_citations": "18\n", "authors": ["1634"]}
{"title": "Verification of population ring protocols in PAT\n", "abstract": " The population protocol model has emerged as an elegant paradigm for describing mobile ad hoc networks, consisting of a number of nodes that interact with each other to carry out a computation. One essential property of self-stabilizing population protocols is that all nodes must eventually converge to the correct output value, with respect to all possible initial configurations. It has been shown that fairness constraints play a crucial role in designing population protocols. The Process Analysis Toolkit (PAT) has been developed to perform verifications under different fairness constraints efficiently. In particular, it can handle global fairness, which is required for the correctness of most of population protocols. It is an ideal candidate for automatically verifying population protocols. In this paper, we summarize our latest empirical evaluation of PAT on a set of self-stabilizing population protocols for ring networks. We report\u00a0\u2026", "num_citations": "18\n", "authors": ["1634"]}
{"title": "Enhancing symbolic execution of heap-based programs with separation logic for test input generation\n", "abstract": " Symbolic execution is a well established method for test input generation. Despite of having achieved tremendous success over numerical domains, existing symbolic execution techniques for heap-based programs are limited due to the lack of a succinct and precise description for symbolic values over unbounded heaps. In this work, we present a new symbolic execution method for heap-based programs based on separation logic. The essence of our proposal is context-sensitive lazy initialization, a novel approach for efficient test input generation. Our approach differs from existing approaches in two ways. Firstly, our approach is based on separation logic, which allows us to precisely capture preconditions of heap-based programs so that we avoid generating invalid test inputs. Secondly, we generate only fully initialized test inputs, which are more useful in practice compared to those partially initialized test inputs\u00a0\u2026", "num_citations": "17\n", "authors": ["1634"]}
{"title": "GPU accelerated on-the-fly reachability checking\n", "abstract": " Model checking suffers from the infamous state space explosion problem. In this paper, we propose an approach, named GPURC, to utilize the Graphics Processing Units (GPUs) to speed up the reachability verification. The key idea is to achieve a dynamic load balancing so that the many cores in GPUs are fully utilized during the state space exploration. To this end, we firstly construct a compact data encoding of the input transition systems to reduce the memory cost and fit the calculation in GPUs. To support a large number of concurrent components, we propose a multi-integer encoding with conflict-release accessing approach. We then develop a BFS-based state space generation algorithm in GPUs, which makes full use of the GPU memory hierarchy and the latest dynamic parallelism feature in CUDA to achieve a high parallelism. GPURC also supports a parallel collaborative event synchronization approach\u00a0\u2026", "num_citations": "17\n", "authors": ["1634"]}
{"title": "Are timed automata bad for a specification language? language inclusion checking for timed automata\n", "abstract": " Given a timed automaton  modeling an implementation and a timed automaton  as a specification, language inclusion checking is to decide whether the language of  is a subset of that of . It is known that this problem is undecidable and \u201cthis result is an obstacle in using timed automata as a specification language\u201d [2]. This undecidability result, however, does not imply that all timed automata are bad for specification. In this work, we propose a zone-based semi-algorithm for language inclusion checking, which implements simulation reduction based on Anti-Chain and LU-simulation. Though it is not guaranteed to terminate, we show that it does in many cases through both theoretical and empirical analysis. The semi-algorithm has been incorporated into the PAT model checker, and applied to multiple systems to show its usefulness and scalability.", "num_citations": "17\n", "authors": ["1634"]}
{"title": "Automatic compositional verification of timed systems\n", "abstract": " Specification and verification of real-time systems are important research topics with crucial applications; however, the so-called state space explosion problem often prevents model checking to be used in practice for large systems. In this work, we present a self-contained toolkit to analyze real-time systems specified using event-recording automata (ERAs), which supports system modeling, animated simulation, and fully automatic compositional verification based on learning techniques. Experimental results show that our tool outperforms the state-of-the-art timed model checker.", "num_citations": "17\n", "authors": ["1634"]}
{"title": "Formal analysis of pervasive computing systems\n", "abstract": " Pervasive computing systems are heterogenous and complex as they usually involve human activities, various sensors and actuators as well as middleware for system controlling. Therefore, analyzing such systems is highly non-trivial. In this work, we propose to use formal methods for analyzing pervasive computing systems. Firstly, a formal modeling framework is proposed to cover main characteristics of pervasive computing systems (e.g., context-awareness, concurrent communications, layered architectures). Secondly, we identify the safety requirements (e.g., free of deadlock and conflicts etc.) and propose their specifications as safety and liveness properties. Finally, we demonstrate our ideas using a case study of a smart nursing home system. Experimental results show the effectiveness of our approach in exploring system behaviors and revealing system design flaws such as information inconsistency and\u00a0\u2026", "num_citations": "17\n", "authors": ["1634"]}
{"title": "Model checking a model checker: A code contract combined approach\n", "abstract": " Model checkers, like any complex software, are subject to bugs. Unlike ordinary software, model checkers are often used to verify safety critical systems. Their correctness is thus vital. Verifying model checkers is extremely challenging because they are always complicated in logic and highly optimized. In this work, we propose a code contract combined approach for checking model checkers and apply it to a home-grown model checker PAT. In this approach, we firstly embed programming contracts (i.e., pre/post-conditions and invariants) into its source code, which can capture correctness of model checking algorithms, underlying data structures, consistency between different model checking parameters, etc. Then, interface models of complicated data structures and graphical user interfaces (GUI) are built and model checked. By linking the interface models with actual source codes and exhausting all\u00a0\u2026", "num_citations": "17\n", "authors": ["1634"]}
{"title": "Pfix: fixing concurrency bugs based on memory access patterns\n", "abstract": " Concurrency bugs of a multi-threaded program may only manifest with certain scheduling, ie, they are heisenbugs which are observed only from time to time if we execute the same program with the same input multiple times. They are notoriously hard to fix. In this work, we propose an approach to automatically fix concurrency bugs. Compared to previous approaches, our key idea is to systematically fix concurrency bugs by inferring locking policies from failure inducing memory-access patterns. That is, we automatically identify memory-access patterns which are correlated with the manifestation of the bug, and then conjecture what is the intended locking policy of the program. Afterwards, we fix the program by implementing the locking policy so that the failure inducing memory-access patterns are made impossible. We have implemented our approach in a toolkit called PFix which supports Java programs. We\u00a0\u2026", "num_citations": "15\n", "authors": ["1634"]}
{"title": "USMMC: a self-contained model checker for UML state machines\n", "abstract": " UML diagrams are gaining increasing usage in Object-Oriented system designs. UML state machines are specifically used in modeling dynamic behaviors of classes. It has been widely agreed that verification of system designs at an early stage will dramatically reduce the development cost. Tool support for verification UML designs can also encourage consistent usage of UML diagrams throughout the software development procedure. In this work, we present a tool, named USMMC, which turns model checking of UML state machines into practice. USMMC is a self-contained toolkit, which provides editing, interactive simulation as well as powerful model checking support for UML state machines. The evaluation results show the effectiveness and scalability of our tool.", "num_citations": "15\n", "authors": ["1634"]}
{"title": "Combining model checking and testing with an application to reliability prediction and distribution\n", "abstract": " Testing provides a probabilistic assurance of system correctness. In general, testing relies on the assumptions that the system under test is deterministic so that test cases can be sampled. However, a challenge arises when a system under test behaves non-deterministiclly in a dynamic operating environment because it will be unknown how to sample test cases.", "num_citations": "15\n", "authors": ["1634"]}
{"title": "Frame inference for inductive entailment proofs in separation logic\n", "abstract": " Given separation logic formulae  and , frame inference is the problem of checking whether  entails  and simultaneously inferring residual heaps. Existing approaches on frame inference do not support inductive proofs with general inductive predicates. In this work, we present an automatic frame inference approach for an expressive fragment of separation logic. We further show how to strengthen the inferred frame through predicate normalization and arithmetic inference. We have integrated our approach into an existing verification system. The experimental results show that our approach helps to establish a number of non-trivial inductive proofs which are beyond the capability of all existing tools.", "num_citations": "14\n", "authors": ["1634"]}
{"title": "FiB: Squeezing loop invariants by interpolation between forward/backward predicate transformers\n", "abstract": " Loop invariant generation is a fundamental problem in program analysis and verification. In this work, we propose a new approach to automatically constructing inductive loop invariants. The key idea is to aggressively squeeze an inductive invariant based on Craig interpolants between forward and backward reachability analysis. We have evaluated our approach by a set of loop benchmarks, and experimental results show that our approach is promising.", "num_citations": "14\n", "authors": ["1634"]}
{"title": "Interpolation guided compositional verification (t)\n", "abstract": " Model checking suffers from the state space explosion problem. Compositional verification techniques such as assume-guarantee reasoning (AGR) have been proposed to alleviate the problem. However, there are at least three challenges in applying AGR. Firstly, given a system M1 ? M2, how do we automatically construct and refine (in the presence of spurious counterexamples) an assumption A2, which must be an abstraction of M2? Previous approaches suggest to incrementally learn and modify the assumption through multiple invocations of a model checker, which could be often time consuming. Secondly, how do we keep the state space small when checking M1 ? A2 = f if multiple refinements of A2 are necessary? Lastly, in the presence of multiple parallel components, how do we partition the components? In this work, we propose interpolation-guided compositional verification. The idea is to tackle three\u00a0\u2026", "num_citations": "14\n", "authors": ["1634"]}
{"title": "Complexity of the soundness problem of workflow nets\n", "abstract": " Classical workflow nets (WF-nets for short) are an important subclass of Petri nets that are widely used to model and analyze workflow systems. Soundness is a crucial property of workflow systems and guarantees that these systems are deadlock-free and bounded. Aalst et al. proved that the soundness problem is decidable for WF-nets and can be polynomially solvable for free-choice WF-nets. This paper proves that the soundness problem is PSPACE-hard for WF-nets. Furthermore, it is proven that the soundness problem is PSPACE-complete for bounded WF-nets. Based on the above conclusion, it is derived that the soundness problem is also PSPACE-complete for bounded WF-nets with reset or inhibitor arcs (ReWF-nets and InWF-nets for short, resp.). ReWF-and InWF-nets are two extensions to WF-nets and their soundness problems were proven by Aalst et al. to be undecidable. Additionally, we prove that the\u00a0\u2026", "num_citations": "14\n", "authors": ["1634"]}
{"title": "Verification of orchestration systems using compositional partial order reduction\n", "abstract": " Orc is a computation orchestration language which is designed to specify computational services, such as distributed communication and data manipulation, in a concise and elegant way. Four concurrency primitives allow programmers to orchestrate site calls to achieve a goal, while managing timeouts, priorities, and failures. To guarantee the correctness of Orc model, effective verification support is desirable. Orc has a highly concurrent semantics which introduces the problem of state-explosion to search-based verification methods like model checking. In this paper, we present a new method, called Compositional Partial Order Reduction (CPOR), which aims to provide greater state-space reduction than classic partial order reduction methods in the context of hierarchical concurrent processes. Evaluation shows that CPOR is more effective in reducing the state space than classic partial order\u00a0\u2026", "num_citations": "14\n", "authors": ["1634"]}
{"title": "Poster: Testing heap-based programs with Java StarFinder\n", "abstract": " We present Java StarFinder (JSF), a tool for automated test case generation and error detection for Java programs having inputs in the form of complex heap-manipulating data structures. The core of JSF is a symbolic execution engine that uses separation logic with existential quantifiers and inductively-defined predicates to precisely represent the (unbounded) symbolic heap. The feasibility of a heap con figuration is checked by a satisfiability solver for separation logic. At the end of each feasible path, a concrete model of the symbolic heap (returned by the solver) is used to generate a test case, e.g., a linked list or an AVL tree, that exercises that path. We show the effectiveness of JSF by applying it on non-trivial heap-manipulating programs and evaluated it against JBSE, a state-of-the-art symbolic execution engine for heap-based programs. Experimental results show that our tool significantly reduces the number\u00a0\u2026", "num_citations": "13\n", "authors": ["1634"]}
{"title": "TLV: abstraction through testing, learning, and validation\n", "abstract": " A (Java) class provides a service to its clients (ie, programs which use the class). The service must satisfy certain specifications. Different specifications might be expected at different levels of abstraction depending on the client's objective. In order to effectively contrast the class against its specifications, whether manually or automatically, one essential step is to automatically construct an abstraction of the given class at a proper level of abstraction. The abstraction should be correct (ie, over-approximating) and accurate (ie, with few spurious traces). We present an automatic approach, which combines testing, learning, and validation, to constructing an abstraction. Our approach is designed such that a large part of the abstraction is generated based on testing and learning so as to minimize the use of heavy-weight techniques like symbolic execution. The abstraction is generated through a process of abstraction\u00a0\u2026", "num_citations": "13\n", "authors": ["1634"]}
{"title": "SCC-based improved reachability analysis for Markov decision processes\n", "abstract": " Markov decision processes (MDPs) are extensively used to model systems with both probabilistic and nondeterministic behavior. The problem of calculating the probability of reaching certain system states (hereafter reachability analysis) is central to the MDP-based system analysis. It is known that existing approaches on reachability analysis for MDPs are often inefficient when a given MDP contains a large number of states and loops, especially with the existence of multiple probability distributions. In this work, we propose a method to eliminate strongly connected components (SCCs) in an MDP using a divide-and-conquer algorithm, and actively remove redundant probability distributions in the MDP based on the convex property. With the removal of loops and parts of probability distributions, the probabilistic reachability analysis can be accelerated, as evidenced by our experiment results.", "num_citations": "13\n", "authors": ["1634"]}
{"title": "A model checker for hierarchical probabilistic real-time systems\n", "abstract": " Real-life systems are usually hard to control, due to their complicated structures, quantitative time factors and even stochastic behaviors. In this work, we present a model checker to analyze hierarchical probabilistic real-time systems. A modeling language called PRTS is used to specify such systems, and automatic zone-abstraction approach, which is probability preserving, is used to generate finite state MDP. We have implemented PRTS in model checking framework PAT so that friendly user interface can be used to edit, simulate and verify PRTS models. Some experiments are conducted to show our tool\u2019s efficiency.", "num_citations": "13\n", "authors": ["1634"]}
{"title": "Complexity of the soundness problem of bounded workflow nets\n", "abstract": " Classical workflow nets (WF-nets) are an important class of Petri nets that are widely used to model and analyze workflow systems. Soundness is a crucial property that guarantees these systems are deadlock-free and bounded. Aalst et al. proved that the soundness problem is decidable, and proposed (but not proved) that the soundness problem is EXPSPACE-hard. In this paper, we show that the satisfiability problem of Boolean expression is polynomial time reducible to the liveness problem of bounded WF-nets, and soundness and liveness are equivalent for bounded WF-nets. As a result, the soundness problem of bounded WF-nets is co-NP-hard.               Workflow nets with reset arcs (reWF-nets) are an extension to WF-nets, which enhance the expressiveness of WF-nets. Aalst et al. proved that the soundness problem of reWF-nets is undecidable. In this paper, we show that for bounded reWF-nets, the\u00a0\u2026", "num_citations": "13\n", "authors": ["1634"]}
{"title": "SeVe: automatic tool for verification of security protocols\n", "abstract": " Security protocols play more and more important roles with wide use in many applications nowadays. Currently, there are many tools for specifying and verifying security protocols such as Casper/FDR, ProVerif, or AVISPA. In these tools, the intruder\u2019s ability, which either needs to be specified explicitly or set by default, is not flexible in some circumstances. Moreover, whereas most of the existing tools focus on secrecy and authentication properties, few supports privacy properties like anonymity, receipt freeness, and coercion resistance, which are crucial in many applications such as in electronic voting systems or anonymous online transactions.               In this paper, we introduce a framework for specifying security protocols in the labeled transition system (LTS) semantics model, which embeds the knowledge of the participants and parameterizes the ability of an attacker. Using this model, we give the formal\u00a0\u2026", "num_citations": "13\n", "authors": ["1634"]}
{"title": "Generative API usage code recommendation with parameter concretization\n", "abstract": " Many programming languages and development frameworks have extensive libraries (e.g., JDK and Android libraries) that ease the task of software engineering if used effectively. With numerous library classes and sometimes intricate API (application programming interface) usage constraints, programmers often have difficulty remembering the library APIs and/or using them correctly. This study addresses this problem by developing an engine called DeepAPIRec, which automatically recommends the API usage code. Compared to the existing proposals, our approach distinguishes itself in two ways. First, it is based on a tree-based long short-term memory (LSTM) neural network inspired by recent developments in the machine-learning community. A tree-based LSTM neural network allows us to model and reason about variable-length, preceding and succeeding code contexts, and to make precise\u00a0\u2026", "num_citations": "12\n", "authors": ["1634"]}
{"title": "Assertion generation through active learning\n", "abstract": " Program assertions are useful for many program analysis tasks. They are however often missing in practice. Many approaches have been developed to generate assertions automatically. Existing methods are either based on generalizing from a set of test cases (e.g., Daikon), or based on some forms of symbolic execution. In this work, we develop a novel approach for generating likely assertions automatically based on active learning. Our targets are complex Java programs which are challenging for symbolic execution. Our key idea is to generate candidate assertions based on test cases and then apply active learning techniques to iteratively improve them. We evaluate our approach using two sets of programs, i.e., 425 methods from three popular Java projects from GitHub and 10 programs from the SVComp repository. We evaluate the \u2018correctness\u2019 of the assertions either by comparing them with\u00a0\u2026", "num_citations": "12\n", "authors": ["1634"]}
{"title": "A formal specification and verification framework for timed security protocols\n", "abstract": " Nowadays, protocols often use time to provide better security. For instance, critical credentials are often associated with expiry dates in system designs. However, using time correctly in protocol design is challenging, due to the lack of time related formal specification and verification techniques. Thus, we propose a comprehensive analysis framework to formally specify as well as automatically verify timed security protocols. A parameterized method is introduced in our framework to handle timing parameters whose values cannot be decided in the protocol design stage. In this work, we first propose timed applied p-calculus as a formal language for specifying timed security protocols. It supports modeling of continuous time as well as application of cryptographic functions. Then, we define its formal semantics based on timed logic rules, which facilitates efficient verification against various authentication and secrecy\u00a0\u2026", "num_citations": "12\n", "authors": ["1634"]}
{"title": "Designing minimal effective normative systems with the help of lightweight formal methods\n", "abstract": " Normative systems (ie, a set of rules) are an important approach to achieving effective coordination among (often an arbitrary number of) agents in multiagent systems. A normative system should be effective in ensuring the satisfaction of a desirable system property, and minimal (ie, not containing norms that unnecessarily over-constrain the behaviors of agents). Designing or even automatically synthesizing minimal effective normative systems is highly non-trivial. Previous attempts on synthesizing such systems through simulations often fail to generate normative systems which are both minimal and effective. In this work, we propose a framework that facilitates designing of minimal effective normative systems using lightweight formal methods. Given a minimal effective normative system which coordinates many agents must be minimal and effective for a small number of agents, we start with automatically\u00a0\u2026", "num_citations": "12\n", "authors": ["1634"]}
{"title": "Formalizing and verifying stochastic system architectures using Monterey Phoenix\n", "abstract": " The analysis of software architecture plays an important role in understanding the system structures and facilitate proper implementation of user requirements. Despite its importance in the software engineering practice, the lack of formal description and verification support in this domain hinders the development of quality architectural models. To tackle this problem, in this work, we develop an approach for modeling and verifying software architectures specified using Monterey Phoenix (MP) architecture description language. MP is capable of modeling system and environment behaviors based on event traces, as well as supporting different architecture composition operations and views. First, we formalize the syntax and operational semantics for MP; therefore, formal verification of MP models is feasible. Second, we extend MP to support shared variables and stochastic characteristics, which not only\u00a0\u2026", "num_citations": "12\n", "authors": ["1634"]}
{"title": "Towards verification of computation orchestration\n", "abstract": " Recently, a promising programming model called Orc has been proposed to support a structured way of orchestrating distributed Web Services. Orc is intuitive because it offers concise constructors to manage concurrent communication, time-outs, priorities, failure of Web Services or communication and so forth. The semantics of Orc is precisely defined. However, there is no automatic verification tool available to verify critical properties against Orc programs. Our goal is to verify the orchestration programs (written in Orc language) which invoke web services to achieve certain goals. To investigate this problem and build useful tools, we explore in two directions. Firstly, we define a Timed Automata semantics for the Orc language, which we prove is semantically equivalent to the operational semantics of Orc. Consequently, Timed Automata models are systematically constructed from Orc programs. The practical\u00a0\u2026", "num_citations": "12\n", "authors": ["1634"]}
{"title": "A verification system for interval-based specification languages\n", "abstract": " Interval-based specification languages have been used to formally model and rigorously reason about real-time computing systems. This usually involves logical reasoning and mathematical computation with respect to continuous or discrete time. When these systems are complex, analyzing their models by hand becomes error-prone and difficult. In this article, we develop a verification system to facilitate the formal analysis of interval-based specification languages with machine-assisted proof support. The verification system is developed using a generic theorem prover, Prototype Verification System (PVS). Our system elaborately encodes a highly expressive set-based notation, Timed Interval Calculus (TIC), and can rigorously carry out the verification of TIC models at an interval level. We validated all TIC reasoning rules and discovered subtle flaws in the original rules. We also apply TIC to model Duration\u00a0\u2026", "num_citations": "12\n", "authors": ["1634"]}
{"title": "A Formal Model of Semantic Web Service Ontology (WSMO) Execution\n", "abstract": " Semantic Web services have been one of the most significant research areas within the semantic Web vision, and have been recognized as a promising technology that exhibits huge commercial potential. Current semantic Web service research focuses on defining models and languages for the semantic markup of all relevant aspects of services, which are accessible through a Web service interface. The Web service modelling ontology (WSMO) is one of the most significant semantic Web service framework proposed to date. To support the standardization and tool support of WSMO, a formal semantics of the language is highly desirable. As there are a few variants of WSMO and it is still under development, the semantics of WSMO needs to be formally defined to facilitate easy reuse and future development. In this paper, we present a formal object-Z semantics of WSMO. Different aspects of the language have been\u00a0\u2026", "num_citations": "12\n", "authors": ["1634"]}
{"title": "A tools environment for developing and reasoning about ontologies\n", "abstract": " Started in the beginning of 2001, the semantic Web is regarded by many as the next generation of the Web. Ontology languages are the building blocks of semantic Web as they provide basic vocabularies for data markups: the ontologies. The correctness of shared ontologies is crucial to the proper functioning of agents. Hence ensuring the consistency of ontologies is a central issue in both the design and deployment phases of any semantic Web-aware application. Our experiences show that semantic Web is a novel and fruitful application domain for formal languages and their mature reasoning tool support. In order to ease the application of formal methods and tools to the semantic Web, we developed an integrated tools environment to support systematic development of OWL ontologies and then transformation, reasoning assistance and querying of them.", "num_citations": "12\n", "authors": ["1634"]}
{"title": "There is limited correlation between coverage and robustness for deep neural networks\n", "abstract": " Deep neural networks (DNN) are increasingly applied in safety-critical systems, e.g., for face recognition, autonomous car control and malware detection. It is also shown that DNNs are subject to attacks such as adversarial perturbation and thus must be properly tested. Many coverage criteria for DNN since have been proposed, inspired by the success of code coverage criteria for software programs. The expectation is that if a DNN is a well tested (and retrained) according to such coverage criteria, it is more likely to be robust. In this work, we conduct an empirical study to evaluate the relationship between coverage, robustness and attack/defense metrics for DNN. Our study is the largest to date and systematically done based on 100 DNN models and 25 metrics. One of our findings is that there is limited correlation between coverage and robustness, i.e., improving coverage does not help improve the robustness. Our dataset and implementation have been made available to serve as a benchmark for future studies on testing DNN.", "num_citations": "11\n", "authors": ["1634"]}
{"title": "Should we learn probabilistic models for model checking? A new approach and an empirical study\n", "abstract": " Many automated system analysis techniques (e.g., model checking, model-based testing) rely on first obtaining a model of the system under analysis. System modeling is often done manually, which is often considered as a hindrance to adopt model-based system analysis and development techniques. To overcome this problem, researchers have proposed to automatically \u201clearn\u201d models based on sample system executions and shown that the learned models can be useful sometimes. There are however many questions to be answered. For instance, how much shall we generalize from the observed samples and how fast would learning converge? Or, would the analysis result based on the learned model be more accurate than the estimation we could have obtained by sampling many system executions within the same amount of time? In this work, we investigate existing algorithms for learning\u00a0\u2026", "num_citations": "11\n", "authors": ["1634"]}
{"title": "vTRUST: a formal modeling and verification framework for virtualization systems\n", "abstract": " Virtualization is widely used for critical services like Cloud computing. It is desirable to formally verify virtualization systems. However, the complexity of the virtualization system makes the formal analysis a difficult task, e.g., sophisticated programs to manipulate low-level technologies, paged memory management, memory mapped I/O and trusted computing. In this paper, we propose a formal framework, vTRUST, to formally describe virtualization systems with a carefully designed abstraction. vTRUST includes a library to model configurable hardware components and technologies commonly used in virtualization. The system designer can thus verify virtualization systems on critical properties (e.g., confidentiality, verifiability, isolation and PCR consistency) with respect to certain adversary models. We demonstrate the effectiveness of vTRUST by automatically verifying a real-world Cloud implementation with\u00a0\u2026", "num_citations": "11\n", "authors": ["1634"]}
{"title": "Symbolic model-checking of stateful timed csp using bdd and digitization\n", "abstract": " Stateful Timed CSP has been recently proposed to model (and verify) hierarchical real-time systems. It is an expressive modeling language which combines data structure/operations, complicated control flows (modeled using compositional process operators adopted from Timed CSP), and real-time requirements like deadline and within. It has been shown that Stateful Timed CSP is equivalent to closed timed automata with silent transitions, which implies that the timing constraints of Stateful Timed CSP can be captured using explicit tick events, through digitization. In order to tackle the state space explosion problem, we develop a BDD-based symbolic model checking approach to verify Stateful Timed CSP models. Due to the rich language features, BDD-based system encoding and verification is highly nontrivial. In this work, we show how to systematically encode Stateful Timed CSP models in BDD. Our\u00a0\u2026", "num_citations": "11\n", "authors": ["1634"]}
{"title": "Concolic testing heap-manipulating programs\n", "abstract": " Concolic testing is a test generation technique which works effectively by integrating random testing generation and symbolic execution. Existing concolic testing engines focus on numeric programs. Heap-manipulating programs make extensive use of complex heap objects like trees and lists. Testing such programs is challenging due to multiple reasons. Firstly, test inputs for such programs are required to satisfy non-trivial constraints which must be specified precisely. Secondly, precisely encoding and solving path conditions in such programs are challenging and often expensive. In this work, we propose the first concolic testing engine called CSF for heap-manipulating programs based on separation logic. CSF effectively combines specification-based testing and concolic execution for test input generation. It is evaluated on a set of challenging heap-manipulating programs. The results show that CSF generates\u00a0\u2026", "num_citations": "10\n", "authors": ["1634"]}
{"title": "Gasfuzz: Generating high gas consumption inputs to avoid out-of-gas vulnerability\n", "abstract": " The out-of-gas error occurs when smart contract programs are provided with inputs that cause excessive gas consumption, and would be easily exploited to make the DoS attack. Multiple approaches have been proposed to estimate the gas limit of a function in smart contracts to avoid such error. However, under estimation often happens when the contract is complicated. In this work, we propose GasFuzz, which could automatically generate inputs that maximizes the gas cost and reduce the under estimation cases. GasFuzz is designed based on feedback-directed mutational fuzz testing. First, GasFuzz builds the gas weighted control flow graph (CFG) of functions in smart contracts. Then, GasFuzz develops gas consumption guided selection and mutation strategies to generate the input that maximize the gas consumption. For evaluation, we implement GasFuzz based on js-evm, a widely used ethereum virtual machine written in javascript, and conduct experiments on 736 real-world transactions recorded on Ethereum. 44.02% of the transactions would have out-of-gas errors under the estimation results given by solc, means that the recorded real gas consumption for those recorded transactions is larger than the gas limit value estimated by solc. While GasFuzz could reduce the under estimation ratio to 13.86%. Compared with other well-known feedback-directed fuzzing engines such as PerFuzz and SlowFuzz, GasFuzz can generate a same or higher gas estimation value on 97.8% of the recorded transactions with less time, usually within 5 minutes. Furthermore, GasFuzz has exposed 25 previously unknown out-of-gas vulnerabilities in those\u00a0\u2026", "num_citations": "10\n", "authors": ["1634"]}
{"title": "Heuristic collective learning for efficient and robust emergence of social norms\n", "abstract": " In multiagent systems, social norms is a useful technique in regulating agents\u2019 behaviors to achieve coordination or cooperation among agents. One important research question is to investigate how a desirable social norm can be evolved in a bottom-up manner through local interactions. In this paper, we propose two novel learning strategies under the collective learning framework: collective learning EV-l and collective learning EV-g, to efficiently facilitate the emergence of social norms. Experimental results show that both learning strategies can support the emergence of desirable social norms more efficiently in a much broader range of multiagent interaction scenarios than previous work, and also are robust across different network topologies.", "num_citations": "10\n", "authors": ["1634"]}
{"title": "Practical analysis framework for software-based attestation scheme\n", "abstract": " An increasing number of \u201dsmart\u201d embedded devices are employed in our living environment nowadays. Unlike traditional computer systems, these devices are often physically accessible to the attackers. It is therefore almost impossible to guarantee that they are un-compromised, i.e., that indeed the devices are executing the intended software. In such a context, software-based attestation is deemed as a promising solution to validate their software integrity. It guarantees that the software running on the embedded devices are un-compromised without any hardware support. However, designing software-based attestation protocols are shown to be error-prone. In this work, we develop a framework for design and analysis of software-based attestation protocols. We first propose a generic attestation scheme that captures most existing software-based attestation protocols. After formalizing the security criteria for\u00a0\u2026", "num_citations": "10\n", "authors": ["1634"]}
{"title": "Probabilistic model checking multi-agent behaviors in dispersion games using counter abstraction\n", "abstract": " Accurate analysis of the stochastic dynamics of multi-agent system is important but challenging. Probabilistic model checking, a formal technique for analysing a system which exhibits stochastic behaviors, can be a natural solution to analyse multi-agent systems. In this paper, we investigate this problem in the context of dispersion games focusing on two strategies: basic simple strategy (BSS) and extended simple strategies (ESS). We model the system using discrete-time Markov chain (DTMC) and reduce the state space of the models by applying counter abstraction technique. Two important properties of the system are considered: convergence and convergence rate. We show that these kinds of properties can be automatically analysed and verified using probabilistic model checking techniques. Better understanding of the dynamics of the strategies can be obtained compared with empirical evaluations\u00a0\u2026", "num_citations": "10\n", "authors": ["1634"]}
{"title": "On combining state space reductions with global fairness assumptions\n", "abstract": " Model checking has established itself as an effective system analysis method, as it is capable of proving/dis-proving properties automatically. Its application to practical systems is however limited by state space explosion. Among effective state reduction techniques are symmetry reduction and partial order reduction. Global fairness often plays a vital role in designing self-stabilizing population protocols. It is known that combining fairness and symmetry reduction is nontrivial. In this work, we first show that global fairness, unlike weak/strong fairness, can be combined with symmetry reduction. We extend the PAT model checker with the technique and demonstrate its usability by verifying recently proposed population protocols. Second, we show that partial order reduction is not property-preserving with global fairness.", "num_citations": "10\n", "authors": ["1634"]}
{"title": "Context Awareness Systems Design and Reasoning\n", "abstract": " This paper reports a recent research investigation on an integrated formal approach to model and verify sensor constraints and relations in the context awareness systems.", "num_citations": "10\n", "authors": ["1634"]}
{"title": "Extracting FSMs from Object-Z specifications with history invariants\n", "abstract": " Object-Z with history invariants can present precise and abstract models for complex systems. The system behavior patterns are often implicitly embedded within various state/operational constraints and history invariants. Without explicit system behavior representations, it is difficult to implement those abstract models. In this paper, we present a sound and systematic approach to automatically extract explicit system behaviors (as FSMs) from the abstract Object-Z specifications. Safety and liveness and additional crucial requirements for open systems are ensured.", "num_citations": "10\n", "authors": ["1634"]}
{"title": "Symbolic verification of message passing interface programs\n", "abstract": " Message passing is the standard paradigm of programming in high-performance computing. However, verifying Message Passing Interface (MPI) programs is challenging, due to the complex program features (such as non-determinism and non-blocking operations). In this work, we present MPI symbolic verifier (MPI-SV), the first symbolic execution based tool for automatically verifying MPI programs with non-blocking operations. MPI-SV combines symbolic execution and model checking in a synergistic way to tackle the challenges in MPI program verification. The synergy improves the scalability and enlarges the scope of verifiable properties. We have implemented MPI-SV 1 and evaluated it with 111 real-world MPI verification tasks. The pure symbolic execution-based technique successfully verifies 61 out of the 111 tasks (55%) within one hour, while in comparison, MPI-SV verifies 100 tasks (90%). On average\u00a0\u2026", "num_citations": "9\n", "authors": ["1634"]}
{"title": "Executable operational semantics of Solidity\n", "abstract": " Bitcoin has attracted everyone's attention and interest recently. Ethereum (ETH), a second generation cryptocurrency, extends Bitcoin's design by offering a Turing-complete programming language called Solidity to develop smart contracts. Smart contracts allow creditable execution of contracts on EVM (Ethereum Virtual Machine) without third parties. Developing correct smart contracts is challenging due to its decentralized computation nature. Buggy smart contracts may lead to huge financial loss. Furthermore, smart contracts are very hard, if not impossible, to patch once they are deployed. Thus, there is a recent surge of interest on analyzing/verifying smart contracts. While existing work focuses on EVM opcode, we argue that it is equally important to understand and define the semantics of Solidity since programmers program and reason about smart contracts at the level of source code. In this work, we develop the structural operational semantics for Solidity, which allows us to identify multiple design issues which underlines many problematic smart contracts. Furthermore, our semantics is executable in the K framework, which allows us to verify/falsify contracts automatically.", "num_citations": "9\n", "authors": ["1634"]}
{"title": "Towards concolic testing for hybrid systems\n", "abstract": " Hybrid systems exhibit both continuous and discrete behavior. Analyzing hybrid systems is known to be hard. Inspired by the idea of concolic testing (of programs), we investigate whether we can combine random sampling and symbolic execution in order to effectively verify hybrid systems. We identify a sufficient condition under which such a combination is more effective than random sampling. Furthermore, we analyze different strategies of combining random sampling and symbolic execution and propose an algorithm which allows us to dynamically switch between them so as to reduce the overall cost. Our method has been implemented as a web-based checker named HyChecker. HyChecker has been evaluated with benchmark hybrid systems and a water treatment system in order to test its effectiveness.", "num_citations": "9\n", "authors": ["1634"]}
{"title": "Regular symmetry patterns\n", "abstract": " Symmetry reduction is a well-known approach for alleviating the state explosion problem in model checking. Automatically identifying symmetries in concurrent systems, however, is computationally expensive. We propose a symbolic framework for capturing symmetry patterns in parameterised systems (i.e. an infinite family of finite-state systems): two regular word transducers to represent, respectively, parameterised systems and symmetry patterns. The framework subsumes various types of \u201csymmetry relations\u201d ranging from weaker notions (e.g. simulation preorders) to the strongest notion (i.e. isomorphisms). Our framework enjoys two algorithmic properties: (1) symmetry verification: given a transducer, we can automatically check whether it is a symmetry pattern of a given system, and (2) symmetry synthesis: we can automatically generate a symmetry pattern for a given system in the form of a transducer\u00a0\u2026", "num_citations": "9\n", "authors": ["1634"]}
{"title": "PSyHCoS: Parameter synthesis for hierarchical concurrent real-time systems\n", "abstract": " Real-time systems are often hard to control, due to their complicated structures, quantitative time factors and even unknown delays. We present here PSyHCoS, a tool for analyzing parametric real-time systems specified using the hierarchical modeling language PSTCSP. PSyHCoS supports several algorithms for parameter synthesis and model checking, as well as state space reduction techniques. Its architecture favors reusability in terms of syntax, semantics, and algorithms. It comes with a friendly user interface that can be used to edit, simulate and verify PSTCSP models. Experiments show its efficiency and applicability.", "num_citations": "9\n", "authors": ["1634"]}
{"title": "State space reduction for sensor networks using two-level partial order reduction\n", "abstract": " Wireless sensor networks may be used to conduct critical tasks like fire detection or surveillance monitoring. It is thus important to guarantee the correctness of such systems by systematically analyzing their behaviors. Formal verification of wireless sensor networks is an extremely challenging task as the state space of sensor networks is huge, e.g., due to interleaving of sensors and intra-sensor interrupts. In this work, we develop a method to reduce the state space significantly so that state space exploration methods can be applied to a much smaller state space without missing a counterexample. Our method explores the nature of networked NesC programs and uses a novel two-level partial order reduction approach to reduce interleaving among sensors and intra-sensor interrupts. We define systematic rules for identifying dependence at sensor and network levels so that partial order reduction can be\u00a0\u2026", "num_citations": "9\n", "authors": ["1634"]}
{"title": "Using monterey phoenix to formalize and verify system architectures\n", "abstract": " Modeling and analyzing software architectures are useful for helping to understand the system structures and facilitate proper implementation of user requirements. Despite its importance in the software engineering practice, the lack of formal description and verification support hinders the development of quality architectural models. In this work, we develop an approach for modeling and verifying software architectures specified using Monterey Phoenix (MP) architecture description language. Firstly, we formalize the syntax and operational semantics for MP. This language is capable of modeling system and environment behaviors based on event traces, as well as supporting different architecture composition operations and views. Secondly, a dedicated model checker for MP is developed based on PAT verification framework. Finally, several case studies are presented to evaluate the usability and effectiveness of\u00a0\u2026", "num_citations": "9\n", "authors": ["1634"]}
{"title": "Analyzing hierarchical complex real-time systems\n", "abstract": " Specification and verification of real-time systems are important research topics which have practical implications. In this work, we present a self-contained toolkit to analyze real-time systems, which supports system modeling, animated simulation and automatic verification (based on advanced model checking techniques like dynamic zone abstraction). In this tool, we adopt an event-based modeling language for describing real-time systems with hierarchical structure. Experiments show that our tool has compatible performance with the state-of-the-art verifiers, and complement them with additional capabilities like LTL model checking, timed refinement checking.", "num_citations": "9\n", "authors": ["1634"]}
{"title": "Holistic combination of structural and textual code information for context based API recommendation\n", "abstract": " Context based API recommendation is an important way to help developers find the needed APIs effectively and efficiently. For effective API recommendation, we need not only a joint view of both structural and textual code information, but also a holistic view of correlated API usage in control and data flow graph as a whole. Unfortunately, existing API recommendation methods exploit structural or textual code information separately. In this work, we propose a novel API recommendation approach called APIRec-CST (API Recommendation by Combining Structural and Textual code information). APIRec-CST is a deep learning model that combines the API usage with the text information in the source code based on an API Context Graph Network and a Code Token Network that simultaneously learn structural and textual features for API recommendation. We apply APIRec-CST to train a model for JDK library based\u00a0\u2026", "num_citations": "8\n", "authors": ["1634"]}
{"title": "Towards using concurrent java api correctly\n", "abstract": " Concurrent Programs are hard to analyze or debug due to the complex program logic and unpredictable execution environment. In practice, ordinary programmers often adopt existing well-designed concurrency related API (e.g., those in java.util.concurrent) so as to avoid dealing with these issues. These API can however often be used incorrectly, which results in hardto-debug concurrent bugs. In this work, we propose an approach for enforcing the correct usage of concurrency-related Java API. Our idea is to annotate concurrency-related Java classes with annotations related to misuse of these API and develop lightweight type checker to detect concurrent API misuse based on the annotations. To automate this process, we need to solve two problems: (1) how do we obtain annotations of the relevant API; and (2) how do we systematically detect concurrent API misuse based on the annotations? We solve the first\u00a0\u2026", "num_citations": "8\n", "authors": ["1634"]}
{"title": "A hybrid model of connectors in cyber-physical systems\n", "abstract": " Compositional coordination models and languages play an important role in cyber-physical systems (CPSs). In this paper, we introduce a formal model for describing hybrid behaviors of connectors in CPSs. We extend the constraint automata model, which is used as the semantic model for the exogenous channel-based coordination language Reo, to capture the dynamic behavior of connectors in CPSs where the discrete and continuous dynamics co-exist and interact with each other. In addition to the formalism, we also provide a theoretical compositional approach for constructing the product automata for a Reo circuit, which is typically obtained by composing several primitive connectors in Reo.", "num_citations": "8\n", "authors": ["1634"]}
{"title": "A UTP semantics for communicating processes with shared variables\n", "abstract": " CSP# (Communicating Sequential Programs) is a modelling language designed for specifying concurrent systems by integrating CSP-like compositional operators with sequential programs updating shared variables. In this paper, we define an observation-oriented denotational semantics in an open environment for the CSP# language based on the UTP framework. To deal with shared variables, we lift traditional event-based traces into hybrid traces which consist of event-state pairs for recording process behaviours. We also define refinement to check process equivalence and present a set of algebraic laws which are established based on our denotational semantics. Our approach thus provides a rigorous means for reasoning about the correctness of CSP# process behaviours. We further derive a closed semantics by focusing on special types of hybrid traces; this closed semantics can be linked with\u00a0\u2026", "num_citations": "8\n", "authors": ["1634"]}
{"title": "Improved reachability analysis in DTMC via divide and conquer\n", "abstract": " Discrete Time Markov Chains (DTMCs) are widely used to model probabilistic systems in many domains, such as biology, network and communication protocols. There are two main approaches for probability reachability analysis of DTMCs, i.e., solving linear equations or using value iteration. However, both approaches have drawbacks. On one hand, solving linear equations can generate accurate results, but it can be only applied to relatively small models. On the other hand, value iteration is more scalable, but often suffers from slow convergence. Furthermore, it is unclear how to parallelize (i.e., taking advantage of multi-cores or distributed computers) these two approaches. In this work, we propose a divide-and-conquer approach to eliminate loops in DTMC and hereby speed up probabilistic reachability analysis. A DTMC is separated into several partitions according to our proposed cutting criteria\u00a0\u2026", "num_citations": "8\n", "authors": ["1634"]}
{"title": "Analyzing multi-agent systems with probabilistic model checking approach\n", "abstract": " Multi-agent systems, which are composed of autonomous agents, have been successfully employed as a modeling paradigm in many scenarios. However, it is challenging to guarantee the correctness of their behaviors due to the complex nature of the autonomous agents, especially when they have stochastic characteristics. In this work, we propose to apply probabilistic model checking to analyze multi-agent systems. A modeling language called PMA is defined to specify such kind of systems, and LTL property and logic of knowledge combined with probabilistic requirements are supported to analyze system behaviors. Initial evaluation indicates the effectiveness of our current progress; meanwhile some challenges and possible solutions are discussed as our ongoing work.", "num_citations": "8\n", "authors": ["1634"]}
{"title": "A verification system for timed interval calculus\n", "abstract": " Timed Interval Calculus (TIC) is a highly expressive set-based notation for specifying and reasoning about embedded real-time systems. However, it lacks mechanical proving support, as its verification usually involves infinite time intervals and continuous dynamics. In this paper, we develop a system based on a generic theorem prover, Prototype Verification System (PVS), to assist formal verification of TIC at a high grade of automation. TIC semantics has been constructed by the PVS typed higher-order logic. Based on the encoding, we have checked all TIC reasoning rules and discovered subtle flaws. A translator has been implemented in Java to automatically transform TIC models into PVS specifications. A collection of supplementary rules and PVS strategies has been defined to facilitate the rigorous reasoning of TIC models with functional and non-functional (for example, real-time) requirements at the interval\u00a0\u2026", "num_citations": "8\n", "authors": ["1634"]}
{"title": "Validating semistructured data using OWL\n", "abstract": " Semistructured data has become prevalent in both web applications and database systems. This rapid growth in use makes the design of good semistructured data essential. Formal semantics and automated reasoning tools enable us to reveal the inconsistencies in a semistructured data model and its instances. The Object Relationship Attribute model for Semistructured data (ORA-SS) is a graphical notation for designing and representing semistructured data. This paper presents a methodology of encoding the semantics of ORA-SS in the Web Ontology Language (OWL) and automatically validating the semistructured data design using the OWL reasoning tool \u2013 RACER. Our methodology provides automated consistency checking of an ORA-SS data model at both the schema and instance levels.", "num_citations": "8\n", "authors": ["1634"]}
{"title": "Active fuzzing for testing and securing cyber-physical systems\n", "abstract": " Cyber-physical systems (CPSs) in critical infrastructure face a pervasive threat from attackers, motivating research into a variety of countermeasures for securing them. Assessing the effectiveness of these countermeasures is challenging, however, as realistic benchmarks of attacks are difficult to manually construct, blindly testing is ineffective due to the enormous search spaces and resource requirements, and intelligent fuzzing approaches require impractical amounts of data and network access. In this work, we propose active fuzzing, an automatic approach for finding test suites of packet-level CPS network attacks, targeting scenarios in which attackers can observe sensors and manipulate packets, but have no existing knowledge about the payload encodings. Our approach learns regression models for predicting sensor values that will result from sampled network packets, and uses these predictions to guide a\u00a0\u2026", "num_citations": "7\n", "authors": ["1634"]}
{"title": "Verifying complex systems probabilistically through learning, abstraction and refinement\n", "abstract": " Precisely modeling complex systems like cyber-physical systems is often challenging, which may render model-based system verification techniques like model checking infeasible. To overcome this challenge, we propose a method called LAR to \u2018verify\u2019such complex systems through a combination of learning, abstraction and refinement. Instead of starting with system modeling, our method takes a set of concrete system traces as input. The output is either a counterexample with a bounded probability of being a spurious counterexample, or a probabilistic model based on which the given property is \u2018verified\u2019. The model could be viewed as a proof obligation, ie, the property is verified if the model is correct. It can also be used for subsequent system analysis activities like runtime monitoring. Our method has been implemented as a self-contained software toolkit. The evaluation on multiple benchmark systems as well as a real-world water purification system show promising results.", "num_citations": "7\n", "authors": ["1634"]}
{"title": "Verifying parameterized timed security protocols\n", "abstract": " Quantitative timing is often explicitly used in systems for better security, e.g., the credentials for automatic website logon often has limited lifetime. Verifying timing relevant security protocols in these systems is very challenging as timing adds another dimension of complexity compared with the untimed protocol verification. In our previous work, we proposed an approach to check the correctness of the timed authentication in security protocols with fixed timing constraints. However, a more difficult question persists, i.e., given a particular protocol design, whether the protocol has security flaws in its design or it can be configured secure with proper parameter values? In this work, we answer this question by proposing a parameterized verification framework, where the quantitative parameters in the protocols can be intuitively specified as well as automatically analyzed. Given a security protocol, our verification\u00a0\u2026", "num_citations": "7\n", "authors": ["1634"]}
{"title": "Adaptive defending strategy for smart grid attacks\n", "abstract": " One active area of research in smart grid security focuses on applying game-theoretic frameworks to analyze interactions between a system and an attacker and formulate effective defense strategies. In previous work, a Nash equilibrium (NE) solution is chosen as the optimal defense strategy, which [7, 9] implies that the attacker has complete knowledge of the system and would also employ the corresponding NE strategy. In practice, however, the attacker may have limited knowledge and resources, and thus employ an attack which is less than optimal, allowing the defender to devise more efficient strategies.", "num_citations": "7\n", "authors": ["1634"]}
{"title": "Symbolic analysis of an electric vehicle charging protocol\n", "abstract": " In this paper, we describe our analysis of a recently proposed electric vehicle charing protocol. The protocol builds on complicated cryptographic primitives such as commitment, zero-knowledge proofs, BBS+ signature and etc. Moreover, interesting properties such as secrecy, authentication, anonymity, and location privacy are claimed on this protocol. It thus presents a challenge for formal verification, as existing tools for security protocol analysis lack support for all the required features. In our analysis, we employ and combine the strength of two state-of-the-art symbolic verifiers, Tamarin and Prove if, to check all important properties of the protocol.", "num_citations": "7\n", "authors": ["1634"]}
{"title": "CELL: A compositional verification framework\n", "abstract": " This paper presents CELL, a comprehensive and extensible framework for compositional verification of concurrent and real-time systems based on commonly used semantic models. For each semantic model, CELL offers three libraries, i.e., compositional verification paradigms, learning algorithms and model checking methods to support various state-of-the-art compositional verification approaches. With well-defined APIs, the framework could be applied to build customized model checkers. In addition, each library could be used independently for verification and program analysis purposes. We have built three model checkers with CELL. The experimental results show that the performance of these model checkers can offer similar or often better performance compared to the state-of-the-art verification tools.", "num_citations": "7\n", "authors": ["1634"]}
{"title": "An analytical and experimental comparison of CSP extensions and tools\n", "abstract": " Communicating Sequential Processes (CSP) has been widely applied to modeling and analyzing concurrent systems. There have been considerable efforts on enhancing CSP by taking data and other system aspects into account. For instance, CSP                   M                  combines CSP with a functional programming language whereas CSP# integrates high-level CSP-like process operators with low-level procedure code. Little work has been done to systematically compare these CSP extensions, which may have subtle and substantial differences. In this paper, we compare CSP                   M                  and CSP# not only on their syntax, but also operational semantics as well as their supporting tools such as FDR, ProB, and PAT. We conduct extensive experiments to compare the performance of these tools in different settings. Our comparison can be used to guide users to choose the appropriate CSP\u00a0\u2026", "num_citations": "7\n", "authors": ["1634"]}
{"title": "Formal verification of scalable nonzero indicators\n", "abstract": " Concurrent algorithms are notoriously difficult to design correctly, and high performance algorithms that make little or no use of locks even more so. In this paper, we describe a formal verification of a recent concurrent data structure Scalable NonZero Indicators. The algorithm supports incrementing, decrementing, and querying the shared counter in an efficient and linearizable way without blocking. The algorithm is highly non-trivial and it is challenging to prove the correctness. We have proved that the algorithm satisfies linearizability, by showing a trace refinement relation from the concrete implementation to its abstract specification. These models are specified in CSP and verified automatically using the model checking toolkit PAT.", "num_citations": "7\n", "authors": ["1634"]}
{"title": "RobOT: Robustness-oriented testing for deep learning systems\n", "abstract": " Recently, there has been a significant growth of interest in applying software engineering techniques for the quality assurance of deep learning (DL) systems. One popular direction is deep learning testing, where adversarial examples (a.k. a. bugs) of DL systems are found either by fuzzing or guided search with the help of certain testing metrics. However, recent studies have revealed that the commonly used neuron coverage metrics by existing DL testing approaches are not correlated to model robustness. It is also not an effective measurement on the confidence of the model robustness after testing. In this work, we address this gap by proposing a novel testing framework called Robustness-Oriented Testing (RobOT). A key part of RobOT is a quantitative measurement on 1) the value of each test case in improving model robustness (often via retraining), and 2) the convergence quality of the model robustness\u00a0\u2026", "num_citations": "6\n", "authors": ["1634"]}
{"title": "Global PAC Bounds for Learning Discrete Time Markov Chains\n", "abstract": " Learning models from observations of a system is a powerful tool with many applications. In this paper, we consider learning Discrete Time Markov Chains (DTMC), with different methods such as frequency estimation or Laplace smoothing. While models learnt with such methods converge asymptotically towards the exact system, a more practical question in the realm of trusted machine learning is how accurate a model learnt with a limited time budget is. Existing approaches provide bounds on how close the model is to the original system, in terms of bounds on local (transition) probabilities, which has unclear implication on the global behavior.                 In this work, we provide global bounds on the error made by such a learning process, in terms of global behaviors formalized using temporal logic. More precisely, we propose a learning process ensuring a bound on the error in the probabilities of these\u00a0\u2026", "num_citations": "6\n", "authors": ["1634"]}
{"title": "A Generalized Formal Semantic Framework for Smart Contracts.\n", "abstract": " Smart contracts can be regarded as one of the most popular blockchain-based applications. The decentralized nature of the blockchain introduces vulnerabilities absent in other programs. Furthermore, it is very difficult, if not impossible, to patch a smart contract after it has been deployed. Therefore, smart contracts must be formally verified before they are deployed on the blockchain to avoid attacks exploiting these vulnerabilities. There is a recent surge of interest in analyzing and verifying smart contracts. While most of the existing works either focus on EVM bytecode or translate Solidity contracts into programs in intermediate languages for analysis and verification, we believe that a direct executable formal semantics of the high-level programming language of smart contracts is necessary to guarantee the validity of the verification. In this work, we propose a generalized formal semantic framework based on a general semantic model of smart contracts. Furthermore, this framework can directly handle smart contracts written in different high-level programming languages through semantic extensions and facilitates the formal verification of security properties with the generated semantics.", "num_citations": "6\n", "authors": ["1634"]}
{"title": "Towards Interpreting Recurrent Neural Networks through Probabilistic Abstraction\n", "abstract": " Neural networks are becoming a popular tool for solving many real-world problems such as object recognition and machine translation, thanks to its exceptional performance as an end-to-end solution. However, neural networks are complex black-box models, which hinders humans from interpreting and consequently trusting them in making critical decisions. Towards interpreting neural networks, several approaches have been proposed to extract simple deterministic models from neural networks. The results are not encouraging (e.g., low accuracy and limited scalability), fundamentally due to the limited expressiveness of such simple models. In this work, we propose an approach to extract probabilistic automata for interpreting an important class of neural networks, i.e., recurrent neural networks. Our work distinguishes itself from existing approaches in two important ways. One is that probability is used to compensate for the loss of expressiveness. This is inspired by the observation that human reasoning is often `probabilistic'. The other is that we adaptively identify the right level of abstraction so that a simple model is extracted in a request-specific way. We conduct experiments on several real-world datasets using state-of-the-art architectures including GRU and LSTM. The result shows that our approach significantly improves existing approaches in terms of accuracy or scalability. Lastly, we demonstrate the usefulness of the extracted models through detecting adversarial texts.", "num_citations": "6\n", "authors": ["1634"]}
{"title": "Learning probabilistic models for model checking: an evolutionary approach and an empirical study\n", "abstract": " Many automated system analysis techniques (e.g., model checking, model-based testing) rely on first obtaining a model of the system under analysis. System modeling is often done manually, which is often considered as a hindrance to adopt model-based system analysis and development techniques. To overcome this problem, researchers have proposed to automatically \u201clearn\u201d models based on sample system executions and shown that the learned models can be useful sometimes. There are however many questions to be answered. For instance, how much shall we generalize from the observed samples and how fast would learning converge? Or, would the analysis result based on the learned model be more accurate than the estimation we could have obtained by sampling many system executions within the same amount of time? Moreover, how well does learning scale to real-world applications? If\u00a0\u2026", "num_citations": "6\n", "authors": ["1634"]}
{"title": "A UTP semantics for communicating processes with shared variables and its formal encoding in PVS\n", "abstract": " CSP#(communicating sequential programs) is a modelling language designed for specifying concurrent systems by integrating CSP-like compositional operators with sequential programs updating shared variables. In this work, we define an observation-oriented denotational semantics in an open environment for the CSP# language based on the UTP framework. To deal with shared variables, we lift traditional event-based traces into mixed traces which consist of state-event pairs for recording process behaviours. To capture all possible concurrency behaviours between action/channel-based communications and global shared variables, we construct a comprehensive set of rules on merging traces from processes which run in parallel/interleaving. We also define refinement to check process equivalence and present a set of algebraic laws which are established based on our denotational semantics. We further encode our proposed denotational semantics into the PVS theorem prover. The encoding not only ensures the semantic consistency, but also builds up a theoretic foundation for machine-assisted verification of CSP# specifications.", "num_citations": "6\n", "authors": ["1634"]}
{"title": "Compositional reasoning for shared-variable concurrent programs\n", "abstract": " Scalable and automatic formal verification for concurrent systems is always demanding. In this paper, we propose a verification framework to support automated compositional reasoning for concurrent programs with shared variables. Our framework models concurrent programs as succinct automata and supports the verification of multiple important properties. Safety verification and simulations of succinct automata are parallel compositional, and safety properties of succinct automata are preserved under refinements. We generate succinct automata from infinite state concurrent programs in an automated manner. Furthermore, we propose the first automated approach to checking rely-guarantee based simulations between infinite state concurrent programs. We have prototyped our algorithms and applied our tool to the verification of multiple refinements.", "num_citations": "6\n", "authors": ["1634"]}
{"title": "Parametric model checking timed automata under non-Zenoness assumption\n", "abstract": " Real-time systems often involve hard timing constraints and concurrency, and are notoriously hard to design or verify. Given a model of a real-time system and a property, parametric model-checking aims at synthesizing timing valuations such that the model satisfies the property. However, the counter-example returned by such a procedure may be Zeno (an infinite number of discrete actions occurring in a finite time), which is unrealistic. We show here that synthesizing parameter valuations such that at least one counterexample run is non-Zeno is undecidable for parametric timed automata (PTAs). Still, we propose a semi-algorithm based on a transformation of PTAs into Clock Upper Bound PTAs to derive all valuations whenever it terminates, and some of them otherwise.", "num_citations": "6\n", "authors": ["1634"]}
{"title": "Tauth: Verifying timed security protocols\n", "abstract": " Quantitative timing is often relevant to the security of systems, like web applications, cyber-physical systems, etc. Verifying timed security protocols is however challenging as both arbitrary attacking behaviors and quantitative timing may lead to undecidability. In this work, we develop a service framework to support intuitive modeling of the timed protocol, as well as automatic verification with an unbounded number of sessions. The partial soundness and completeness of our verification algorithms are formally defined and proved. We implement our method into a tool called TAuth and the experiment results show that our approach is efficient and effective in both finding security flaws and giving proofs.", "num_citations": "6\n", "authors": ["1634"]}
{"title": "A systematic study on explicit-state non-Zenoness checking for timed automata\n", "abstract": " Zeno runs, where infinitely many actions occur within finite time, may arise in Timed Automata models. Zeno runs are not feasible in reality and must be pruned during system verification. Thus it is necessary to check whether a run is Zeno or not so as to avoid presenting Zeno runs as counterexamples during model checking. Existing approaches on non-Zenoness checking include either introducing an additional clock in the Timed Automata models or additional accepting states in the zone graphs. In addition, there are approaches proposed for alternative timed modeling languages, which could be generalized to Timed Automata. In this work, we investigate the problem of non-Zenoness checking in the context of model checking LTL properties, not only evaluating and comparing existing approaches but also proposing a new method. To have a systematic evaluation, we develop a software toolkit to support\u00a0\u2026", "num_citations": "6\n", "authors": ["1634"]}
{"title": "Build your own model checker in one month\n", "abstract": " Model checking has established as an effective method for automatic system analysis and verification. It is making its way into many domains and methodologies. Applying model checking techniques to a new domain (which probably has its own dedicated modeling language) is, however, far from trivial. Translation-based approach works by translating domain specific languages into input languages of a model checker. Because the model checker is not designed for the domain (or equivalently, the language), translation-based approach is often ad hoc. Ideally, it is desirable to have an optimized model checker for each application domain. Implementing one with reasonable efficiency, however, requires years of dedicated efforts. In this tutorial, we will briefly survey a variety of model checking techniques. Then we will show how to develop a model checker for a language combining real-time and probabilistic\u00a0\u2026", "num_citations": "6\n", "authors": ["1634"]}
{"title": "Automatic generation of provably correct embedded systems\n", "abstract": " With the demand for new and complicated features, embedded systems are becoming more and more difficult to design and verify. Even if the design of a system is verified, how to guarantee the consistency between the design and its implementation remains a big issue. As a solution, we propose a framework that can help a system designer to model his or her embedded system using a high-level modeling language, verify the design of the system, and automatically generate executable software codes whose behavior semantics are consistent with that of the high-level model. We use two case studies to demonstrate the effectiveness of our framework.", "num_citations": "6\n", "authors": ["1634"]}
{"title": "Planning as model checking tasks\n", "abstract": " Model checking provides a way to automatically verify hardware and software systems, whereas the goal of planning is to produce a sequence of actions that leads from the initial state to the desired goal states. Recently research indicates that there is a strong connection between model checking and planning problem solving. In this paper, we investigate the feasibility of using different model checking tools and techniques for solving classic planning problems. To achieve this, we carried out a number of experiments on different planning domains in order to compare the performance and capabilities of various tools. Our experimental results indicate that the performance of some model checkers is comparable to that of state-of-theart planners for certain categories of problems. In particular, a new planning module with specifically designed searching algorithm is implemented on top of the established model\u00a0\u2026", "num_citations": "6\n", "authors": ["1634"]}
{"title": "Machine-assisted proof support for validation beyond Simulink\n", "abstract": " Simulink is popular in industry for modeling and simulating embedded systems. It is deficient to handle requirements of high-level assurance and timing analysis. Previously, we showed the idea of applying Timed Interval Calculus (TIC) to complement Simulink. In this paper, we develop machine-assisted proof support for Simulink models represented in TIC. The work is based on a generic theorem prover, Prototype Verification System (PVS). The TIC specifications of both Simulink models and requirements are transformed to PVS specifications automatically. Verification can be carried out at interval level with a high level of automation. Analysis of continuous and discrete behaviors is supported. The work enhances the applicability of applying TIC to cope with complex Simulink models.", "num_citations": "6\n", "authors": ["1634"]}
{"title": "Careful-packing: A practical and scalable anti-tampering software protection enforced by trusted computing\n", "abstract": " Ensuring the correct behaviour of an application is a critical security issue. One of the most popular ways to modify the intended behaviour of a program is to tamper its binary. Several solutions have been proposed to solve this problem, including trusted computing and anti-tampering techniques. Both can substantially increase security, and yet both have limitations. In this work, we propose an approach which combines trusted computing technologies and anti-tampering techniques, and that synergistically overcomes some of their inherent limitations. In our approach critical software regions are protected by leveraging on trusted computing technologies and cryptographic packing, without introducing additional software layers. To illustrate our approach we implemented a secure monitor which collects user activities, such as keyboard and mouse events for insider attack detection. We show how our solution\u00a0\u2026", "num_citations": "5\n", "authors": ["1634"]}
{"title": "Towards \u2018verifying\u2019a water treatment system\n", "abstract": " Modeling and verifying real-world cyber-physical systems is challenging, which is especially so for complex systems where manually modeling is infeasible. In this work, we report our experience on combining model learning and abstraction refinement to analyze a challenging system, i.e., a real-world Secure Water Treatment system (SWaT). Given a set of safety requirements, the objective is to either show that the system is safe with a high probability (so that a system shutdown is rarely triggered due to safety violation) or not. As the system is too complicated to be manually modeled, we apply latest automatic model learning techniques to construct a set of Markov chains through abstraction and refinement, based on two long system execution logs (one for training and the other for testing). For each probabilistic safety property, we either report it does not hold with a certain level of probabilistic confidence\u00a0\u2026", "num_citations": "5\n", "authors": ["1634"]}
{"title": "Classification-based parameter synthesis for parametric timed automata\n", "abstract": " Parametric timed automata are designed to model timed systems with unknown parameters, often representing design uncertainties of external environments. In order to design a robust system, it is crucial to synthesize constraints on the parameters, which guarantee the system behaves according to certain properties. Existing approaches suffer from scalability issues. In this work, we propose to enhance existing approaches through classification-based learning. We sample multiple concrete values for parameters and model check the corresponding non-parametric models. Based on the checking results, we form conjectures on the constraint through classification techniques, which can be subsequently confirmed by existing model checkers for parametric timed automata. In order to limit the number of model checker invocations, we actively identify informative parameter values so as to help the\u00a0\u2026", "num_citations": "5\n", "authors": ["1634"]}
{"title": "Improved EGT-Based robustness analysis of negotiation strategies in multiagent systems via model checking\n", "abstract": " Automated negotiations play an important role in various domains modeled as multiagent systems, where agents represent human users and adopt different negotiation strategies. Generally, given a multiagent system, a negotiation strategy should be robust in the sense that most agents in the system have the incentive to choose it rather than other strategies. Empirical game-theoretic (EGT) analysis is a game-theoretic analysis approach to investigate the robustness of different strategies based on a set of empirical results. In this study, we propose that model-checking techniques can be adopted to improve EGT analysis for negotiation strategies. The dynamics of strategy profiles can be modeled as a labeled transition system using the counter abstraction technique. We define single-agent best deviation to represent the strategy deviations during negotiation, which focuses on each agent's best deviation benefit\u00a0\u2026", "num_citations": "5\n", "authors": ["1634"]}
{"title": "Constraint-based automatic symmetry detection\n", "abstract": " We present an automatic approach to detecting symmetry relations for general concurrent models. Despite the success of symmetry reduction in mitigating state explosion problem, one essential step towards its soundness and effectiveness, i.e., how to discover sufficient symmetries with least human efforts, is often either overlooked or oversimplified. In this work, we show how a concurrent model can be viewed as a constraint satisfaction problem (CSP), and present an algorithm capable of detecting symmetries arising from the CSP which induce automorphisms of the model. To the best of our knowledge, our method is the first approach that can automatically detect both process and data symmetries as demonstrated via a number of systems.", "num_citations": "5\n", "authors": ["1634"]}
{"title": "Translating pddl into csp#-the pat approach\n", "abstract": " Model checking provides a way to automatically verify hardware and software systems, whereas the goal of planning is to produce a sequence of actions that leads from the initial state to the desired goal state. Recently research indicates that there is a strong connection between model checking and planning problem solving. In this paper, we investigate the feasibility of using a newly developed model checking framework, Process Analysis Toolkit (PAT), to serve as a planning solution provider for upper layer applications. We first carried out a number of experiments on different planning tools in order to compare their performance and capabilities. Our experimental results showed that the performance of the PAT model checker is comparable to that of state-of-art planners for certain categories of problems. We further propose a set of translation rules for mapping from a commonly used planning notation - PDDL\u00a0\u2026", "num_citations": "5\n", "authors": ["1634"]}
{"title": "Realizing live sequence charts in SystemVerilog\n", "abstract": " The design of an embedded control system starts with an investigation of properties and behaviors of the process evolving within its environment, and an analysis of the requirement for its safety performance. In early stages, system requirements are often specified as scenarios of behavior using sequence charts for different use cases. This specification must be precise, intuitive and expressive enough to capture different aspects of embedded control systems. As a rather rich and useful extension to the classical message sequence charts, live sequence charts (LSC), which provide a rich collection of constructs for specifying both possible and mandatory behaviors, are very suitable for designing an embedded control system. However, it is not a trivial task to realize a high-level design model in executable program codes effectively and correctly. This paper tackles the challenging task by providing a mapping\u00a0\u2026", "num_citations": "5\n", "authors": ["1634"]}
{"title": "Visualizing and simulating semantic web services ontologies\n", "abstract": " The development of Web Services has transformed the World Wide Web into a more application-aware information portal. The various standards ensure that Web Services are interpretable and extensible, opening up possibilities for simple services to be combined to build complex ones. The Semantic Web presents a new mechanism for users and software agents to discover, describe, invoke, compose and monitor Web services. For these purposes the Semantic Web Services (OWL-S) ontologies have been developed to provide vocabularies to describe Web Services in a precise and machine-understandable way. It is necessary to ensure the ontological descriptions of the services capture the intended meaning as erroneous description may cause invocation of wrong services, with wrong parameters, resulting in undesired outcome. In this paper, we propose to apply software engineering method and\u00a0\u2026", "num_citations": "5\n", "authors": ["1634"]}
{"title": "Attack as Defense: Characterizing Adversarial Examples using Robustness\n", "abstract": " As a new programming paradigm, deep learning has expanded its application to many real-world problems. At the same time, deep learning based software are found to be vulnerable to adversarial attacks. Though various defense mechanisms have been proposed to improve robustness of deep learning software, many of them are ineffective against adaptive attacks. In this work, we propose a novel characterization to distinguish adversarial examples from benign ones based on the observation that adversarial examples are significantly less robust than benign ones. As existing robustness measurement does not scale to large networks, we propose a novel defense framework, named attack as defense (A2D), to detect adversarial examples by effectively evaluating an example's robustness. A2D uses the cost of attacking an input for robustness evaluation and identifies those less robust examples as adversarial since less robust examples are easier to attack. Extensive experiment results on MNIST, CIFAR10 and ImageNet show that A2D is more effective than recent promising approaches. We also evaluate our defence against potential adaptive attacks and show that A2D is effective in defending carefully designed adaptive attacks, e.g., the attack success rate drops to 0% on CIFAR10.", "num_citations": "4\n", "authors": ["1634"]}
{"title": "Parametric timed model checking for guaranteeing timed opacity\n", "abstract": " Information leakage can have dramatic consequences on systems security. Among harmful information leaks, the timing information leakage is the ability for an attacker to deduce internal information depending on the system execution time. We address the following problem: given a timed system, synthesize the execution times for which one cannot deduce whether the system performed some secret behavior. We solve this problem in the setting of timed automata (TAs). We first provide a general solution, and then extend the problem to parametric TAs, by synthesizing internal timings making the TA secure. We study decidability, devise algorithms, and show that our method can also apply to program analysis.", "num_citations": "4\n", "authors": ["1634"]}
{"title": "Practical static analysis of context leaks in Android applications\n", "abstract": " Android native applications, written in Java and distributed in APK format, are widely used in mobile devices. Their specific pattern of use lets the operating system control the creation and destruction of resources, such as activities and services (contexts). Programmers are not supposed to interfere with such life cycle events. Otherwise, contexts might be leaked, ie, they will never be deallocated from memory, or be deallocated late, leading to memory exhaustion and frozen applications. In practice, it is easy to write incorrect code, which hinders garbage collection of contexts and leads to context leakages. In this work, we present a novel static analysis method that finds context leaks in Android code. We apply this analysis to APKs translated into Java bytecode. We provide a formal analysis of our algorithms and suggest further research directions for improving precision by combining different approaches. We\u00a0\u2026", "num_citations": "4\n", "authors": ["1634"]}
{"title": "On the sequential Massart algorithm for statistical model checking\n", "abstract": " Several schemes have been provided in Statistical Model Checking (SMC) for the estimation of property occurrence based on predefined confidence and absolute or relative error. Simulations might be however costly if many samples are required and the usual algorithms implemented in statistical model checkers tend to be conservative. Bayesian and rare event techniques can be used to reduce the sample size but they can not be applied without prerequisite or knowledge about the system under scrutiny. Recently, sequential algorithms based on Monte Carlo estimations and Massart bounds have been proposed to reduce the sample size while providing guarantees on error bounds which has been shown to outperform alternative frequentist approaches\u00a0[15]. In this work, we discuss some features regarding the distribution and the optimisation of these algorithms.", "num_citations": "4\n", "authors": ["1634"]}
{"title": "Importance sampling of interval markov chains\n", "abstract": " In real-world systems, rare events often characterize critical situations like the probability that a system fails within some time bound and they are used to model some potentially harmful scenarios in dependability of safety-critical systems. Probabilistic Model Checking has been used to verify dependability properties in various types of systems but is limited by the state space explosion problem. An alternative is the recourse to Statistical Model Checking (SMC) that relies on Monte Carlo simulations and provides estimates within predefined error and confidence bounds. However, rare properties require a large number of simulations before occurring at least once. To tackle the problem, Importance Sampling, a rare event simulation technique, has been proposed in SMC for different types of probabilistic systems. Importance Sampling requires the full knowledge of probabilistic measure of the system, e.g. Markov\u00a0\u2026", "num_citations": "4\n", "authors": ["1634"]}
{"title": "Improving probability estimation through active probabilistic model learning\n", "abstract": " It is often necessary to estimate the probability of certain events occurring in a system. For instance, knowing the probability of events triggering a shutdown sequence allows us to estimate the availability of the system. One approach is to run the system multiple times and then construct a probabilistic model to estimate the probability. When the probability of the event to be estimated is low, many system runs are necessary in order to generate an accurate estimation. For complex cyber-physical systems, each system run is costly and time-consuming, and thus it is important to reduce the number of system runs while providing accurate estimation. In this work, we assume that the user can actively tune the initial configuration of the system before the system runs and answer the following research question: how should the user set the initial configuration so that a better estimation can be learned with fewer\u00a0\u2026", "num_citations": "4\n", "authors": ["1634"]}
{"title": "Improving quality of use case documents through learning and user interaction\n", "abstract": " Use cases are widely used to capture user requirements based on interactions between different roles in the system. They are mostly documented in natural language and sometimes aided with graphical illustrations in the form of use case diagrams. Use cases serve as an important means to communicate among stakeholders, requirement engineers and system engineers as they are easy to understand and are produced early in the software development process. Having high quality use cases are beneficial in many ways, e.g., in avoiding inconsistency/incompleteness in requirements, in guiding system design, in generating test cases. In this work, we propose an approach to improve the quality of use cases using techniques including natural language processing and machine learning. The central idea is to discover potential problems in use cases through active learning and human interaction and provide\u00a0\u2026", "num_citations": "4\n", "authors": ["1634"]}
{"title": "Towards formal modelling and verification of pervasive computing systems\n", "abstract": " Smart systems equipped with emerging pervasive computing technologies enable people with limitations to live in their homes independently. However, lack of guarantees for correctness prevent such system to be widely used. Analysing the system with regard to correctness requirements is a challenging task due to the complexity of the system and its various unpredictable faults. In this work, we propose to use formal methods to analyse pervasive computing (PvC) systems. Firstly, a formal modelling framework is proposed to cover the main characteristics of such systems (e.g., context-awareness, concurrent communications, layered architectures). Secondly, we identify the safety requirements (e.g., free of deadlocks and conflicts) and specify them as safety and liveness properties. Furthermore, based on the modelling framework, we propose an approach of verifying reasoning rules which are used in the\u00a0\u2026", "num_citations": "4\n", "authors": ["1634"]}
{"title": "Improving model checking stateful timed csp with non-zenoness through clock-symmetry reduction\n", "abstract": " Real-time system verification must deal with a special notion of \u2018fairness\u2019, i.e., clocks must always be able to progress. A system run which prevents clocks from progressing unboundedly is known as Zeno. Zeno runs are infeasible in reality and thus must be pruned during system verification. Though zone abstraction is an effective technique for model checking real-time systems, it is known that zone graphs (e.g., those generated from Timed Automata models) are too abstract to directly infer time progress and hence non-Zenoness. As a result, model checking with non-Zenoness (i.e., existence of a non-Zeno counterexample) based on zone graphs only is infeasible. In our previous work\u00a0[23], we show that model checking Stateful Timed CSP with non-Zenoness based on zone graphs only is feasible, due to the difference between Stateful Timed CSP and Timed Automata. Nonetheless, the algorithm\u00a0\u2026", "num_citations": "4\n", "authors": ["1634"]}
{"title": "Differencing labeled transition systems\n", "abstract": " Concurrent programs often use Labeled Transition Systems (LTSs) as their operational semantic models, which provide the basis for automatic system analysis and verification. System behaviors (generated from the operational semantics) evolve as programs evolve for fixing bugs or implementing new user requirements. Even when a program remains unchanged, its LTS models explored by a model checker or analyzer may be different due to the application of different exploration methods. In this paper, we introduce a novel approach (named SpecDiff) to computing the differences between two LTSs, representing the evolving behaviors of a concurrent program. SpecDiff considers LTSs as Typed Attributed Graphs (TAGs), in which states and transitions are encoded in finite dimensional vector spaces. It then computes a maximum common subgraph of two TAGs, which represents an optimal matching of\u00a0\u2026", "num_citations": "4\n", "authors": ["1634"]}
{"title": "Towards expressive specification and efficient model checking\n", "abstract": " We share the views that specifications are preferably executable. In this tutorial, we introduce our latest effort on combining the expressiveness of integrated formal specification languages with the power of mechanical system analysis method like model checking. We present a process analysis toolkit (PAT) which is a self-contained framework for system specification, simulation and verification. PAT supports a modeling language named CSP# (short for communicating sequential programs).", "num_citations": "4\n", "authors": ["1634"]}
{"title": "Towards a Toolkit for Flexible and Efficient Verification under Fairness\n", "abstract": " Recent development on distributed systems has shown that a variety of fairness constraints (some of which are only recently defined) play vital roles in designing self-stabilizing population protocols. Current practice of system analysis is, however, deficient under fairness constraints. In this work, we present a Process Analysis Toolkit (PAT) for flexible and efficient system analysis under fairness constraints. A unified algorithm is proposed to model check systems with different fairness (eg, weak fairness, strong local fairness, strong global fairness) effectively. Partial order reduction which is effective for distributed system verification is extended to fair systems whenever possible. We show through empirical evaluation (on recent population protocols as well as benchmark systems) that PAT significantly outperforms the current practice of verification with fairness. We report that previously unknown bugs have been revealed using PAT against systems functioning under strong global fairness.", "num_citations": "4\n", "authors": ["1634"]}
{"title": "Route coverage testing for autonomous vehicles via map modeling\n", "abstract": " Autonomous vehicles (AVs) play an important role in transforming our transportation systems and relieving traffic congestion. To guarantee their safety, AVs must be sufficiently tested before they are deployed to public roads. Existing testing often focuses on AVs\u2019 collision avoidance on a given route. There is little work on the systematic testing for AVs\u2019 route planning and tracking on a map. In this paper, we propose CROUTE, a novel testing method based on a new AV testing criterion called route coverage. First, the map is modeled as a labeled Petri net, where roads, junctions, and traffic signs are modeled as places, transitions, and labels, respectively. Second, based on the Petri net, we define junctions\u2019 topology features and route features for junction classification. The topology feature describes the topology of roads forming the junction, and the route feature identifies the actions that a vehicle can take to follow a\u00a0\u2026", "num_citations": "3\n", "authors": ["1634"]}
{"title": "Improving neural network verification through spurious region guided refinement\n", "abstract": " We propose a spurious region guided refinement approach for robustness verification of deep neural networks. Our method starts with applying the DeepPoly abstract domain to analyze the network. If the robustness property cannot be verified, the result is inconclusive. Due to the over-approximation, the computed region in the abstraction may be spurious in the sense that it does not contain any true counterexample. Our goal is to identify such spurious regions and use them to guide the abstraction refinement. The core idea is to make use of the obtained constraints of the abstraction to infer new bounds for the neurons. This is achieved by linear programming techniques. With the new bounds, we iteratively apply DeepPoly, aiming to eliminate spurious regions. We have implemented our approach in a prototypical tool DeepSRGR. Experimental results show that a large amount of regions can be identified as\u00a0\u2026", "num_citations": "3\n", "authors": ["1634"]}
{"title": "An Empirical Study on Correlation between Coverage and Robustness for Deep Neural Networks\n", "abstract": " Deep neural networks (DNN) are increasingly applied in safety-critical systems, e.g., for face recognition, autonomous car control and malware detection. It is also shown that DNNs are subject to attacks such as adversarial perturbation and thus must be properly tested. Many coverage criteria for DNN since have been proposed, inspired by the success of code coverage criteria for software programs. The expectation is that if a DNN is well tested (and retrained) according to such coverage criteria, it is more likely to be robust. In this work, we conduct an empirical study to evaluate the relationship between coverage, robustness and attack/defense metrics for DNN. Our study is the largest to date and systematically done based on 100 DNN models and 25 metrics. One of our findings is that there is limited correlation between coverage and robustness, i.e., improving coverage does not help improve the robustness. Our\u00a0\u2026", "num_citations": "3\n", "authors": ["1634"]}
{"title": "MAP-Coverage: a novel coverage criterion for testing thread-safe classes\n", "abstract": " Concurrent programs must be thoroughly tested, as concurrency bugs are notoriously hard to detect. Code coverage criteria can be used to quantify the richness of a test suite (e.g., whether a program has been tested sufficiently) or provide practical guidelines on test case generation (e.g., as objective functions used in program fuzzing engines). Traditional code coverage criteria are, however, designed for sequential programs and thus ineffective for concurrent programs. In this work, we introduce a novel code coverage criterion for testing thread-safe classes called MAP-coverage (short for memory-access patterns). The motivation is that concurrency bugs are often correlated with certain memory-access patterns, and thus it is desirable to comprehensively cover all memory-access patterns. Furthermore, we propose a testing method for maximizing MAP-coverage. Our method has been implemented as a self\u00a0\u2026", "num_citations": "3\n", "authors": ["1634"]}
{"title": "Adaptive randomized scheduling for concurrency bug detection\n", "abstract": " Multi-threaded programs often exhibit erroneous behaviours due to unintended interactions among threads. Those bugs are often difficult to find because they typically manifest under very specific thread schedules. The traditional randomized algorithms increase the probability of exploring infrequent interleavings using randomized scheduling and improve the chances of detecting concurrency defects. However, they may generate many redundant trials, especially for those hard-to-detect defects, and thus their performance is often not stable. In this work, we propose an adaptive randomized scheduling algorithm~(ARS), which adaptively explores the search space and detects concurrency bugs more efficiently with less efforts. We compare ARS with random searching and the state-of-the-art maximal causality reduction method on 27 concurrent Java programs. The evaluation results show that ARS shows a more\u00a0\u2026", "num_citations": "3\n", "authors": ["1634"]}
{"title": "Automatically \u2018Verifying\u2019Discrete-Time Complex Systems through Learning, Abstraction and Refinement\n", "abstract": " Precisely modeling complex systems like cyber-physical systems is challenging, which often render model-based system verification techniques like model checking infeasible. To overcome this challenge, we propose a method called LAR to automatically \u2018verify\u2019 such complex systems through a combination of learning, abstraction and refinement from a set of system log traces. We assume that log traces and sampling frequency are adequate to capture \u2018enough\u2019 behaviour of the system. Given a safety property and the concrete system log traces as input, LAR automatically learns and refines system models, and produces two kinds of outputs. One is a counterexample with a bounded probability of being spurious. The other is a probabilistic model based on which the given property is \u2018verified\u2019. The model can be viewed as a proof obligation, i.e., the property is verified if the model is correct. It can also be used for\u00a0\u2026", "num_citations": "3\n", "authors": ["1634"]}
{"title": "The miles before formal methods-a case study on modeling and analyzing a passenger lift system\n", "abstract": " Cyber-Physical Systems (CPS) pervade our everyday lives. As users, we need assurances that such systems satisfy requirements on safety, reliability, security and interoperability. CPS presents a major challenge for formal analysis because of their complexity, physical dependencies and non-linearity, and for smart CPS - the ability to improve their behavior over time. Existing approaches on analyzing CPS (e.g., model checking and model-based testing) often assume the existence of a system model. Such approaches have limited application in practice as the models often do not exist. In this work, we report our experience on applying a three-step approach to analyzing a practical CPS: a passenger lift system in a commercial building. The three steps are (1) determining the right level of system abstraction, (2) building the model automatically using grammatical inference, and (3) analyzing the model\u00a0\u2026", "num_citations": "3\n", "authors": ["1634"]}
{"title": "SUN Jun\n", "abstract": " Fault analysis and debugging of microservice systems: Industrial survey, benchmark system, and empirical study, by ZHOU, Xiang; PENG, Xin; XIE, Tao; SUN, Jun; JI, Chao; LI, Wenhai; DING, Dan.(2018). IEEE Transactions on Software Engineering, 14 (8), 1-18. https://doi. org/10.1109/TSE. 2018.2887384 (Published)", "num_citations": "3\n", "authors": ["1634"]}
{"title": "An adaptive Markov strategy for effective network intrusion detection\n", "abstract": " Network monitoring is an important way to ensure the security of hosts from being attacked by malicious attackers. One challenging problem for network operators is how to distribute the limited monitoring resources (e.g., intrusion detectors) among the network to detect attacks in a cost-effective manner, especially when the attacking strategies can be changing dynamically and unpredictable. To this end, we adopt Markov game to model the interactions between the network operator and the attacker and propose an adaptive Markov strategy (AMS) to determine how the detectors should be placed on the network against possible attacks to minimize the network's accumulated cost over time. The AMS is guaranteed to converge to the best response strategy when the attacker's strategy is fixed (rationality), converge to a fixed strategy under self-play (convergence) and obtain a payoff no less than that under the\u00a0\u2026", "num_citations": "3\n", "authors": ["1634"]}
{"title": "An extensive model checking framework for multi-agent systems\n", "abstract": " In this work, we propose a novel probabilistic modeling language PML-MAS to capture the stochastic characteristics of multi-agent systems (MASs). Moreover, we design a model checking framework for MAS, which is highly extensible. It provides powerful modeling editor, interactive simulator and automatic verifier for MASs. In addition, it can support various MAS model languages via extracting their semantic models and verification algorithms.", "num_citations": "3\n", "authors": ["1634"]}
{"title": "Stateful security protocol verification\n", "abstract": " A long-standing research problem in security protocol design is how to efficiently verify security protocols with tamper-resistant global states. In this paper, we address this problem by first proposing a protocol specification framework, which explicitly represents protocol execution states and state transformations. Secondly, we develop an algorithm for verifying security properties by utilizing the key ingredients of the first-order reasoning for reachability analysis, while tracking state transformation and checking the validity of newly generated states. Our verification algorithm is proven to be (partially) correct, if it terminates. We have implemented the proposed framework and verification algorithms in a tool named SSPA, and evaluate it using a number of stateful security protocols. The experimental results show that our approach is not only feasible but also practically efficient. In particular, we have found a security flaw on the digital envelope protocol, which could not be detected by existing security protocol verifiers.", "num_citations": "3\n", "authors": ["1634"]}
{"title": "Compositional encoding for bounded model checking\n", "abstract": " Verification techniques like SAT-based bounded model checking have been successfully applied to a variety of system models. Applying bounded model checking to compositional process algebras is, however, a highly non-trivial task. One challenge is that the number of system states for process algebra models is not statically known, whereas exploring the full state space is computationally expensive. This paper presents a compositional encoding of hierarchical processes as SAT problems and then applies state-of-the-art SAT solvers for bounded model checking. The encoding avoids exploring the full state space for complex systems so as to deal with state space explosion. We developed an automated analyzer which combines complementing model checking techniques (i.e., bounded model checking and explicit onthe-fly model checking) to validate system models against event-based temporal\u00a0\u2026", "num_citations": "3\n", "authors": ["1634"]}
{"title": "Reasoning about timed csp models\n", "abstract": " HORAE is an interactive system which supports composing and reasoning of Timed CSP process descriptions. Timed CSP extends CSP by introducing the capability to quantify temporal aspects of sequencing and synchronization. It is a powerful language to model real-time reactive systems. However, there is no verification tool support for proving critical properties over systems modeled using Timed CSP. HORAE, using Constraint Logic Programming (CLP) as the underlying reasoning support, is capable to prove traditional safety properties and beyond, such as the reachability, deadlock-freeness, timewise refinement relationship, lower or upper bound of variables, and etc.", "num_citations": "3\n", "authors": ["1634"]}
{"title": "Sensor Based Designs for Smart Space\n", "abstract": " The challenge for specifying and designing complex sensor based smart space is how to precisely capture communicating behaviors, sensor constraints, and real-time system properties in a highly abstract, modular and integrated way. In particular, the high level design models for the smart systems need to capture not only concurrent interactions between various software control units and physical sensing devices, but also environmental and requirement constraints on sensors and sensor relations. In this paper, we demonstrate that a sensor based formalism can be succinctly applied to the design of smart space with multiple functionalities. Based on the formal model, essential properties within a particular domain or across different domains can also be verified.", "num_citations": "3\n", "authors": ["1634"]}
{"title": "TCOZ to Timed Automata\n", "abstract": " The integrated logic-based modeling language, Timed Communicating Object Z (TCOZ), is well suited for presenting more complete and coherent requirement models for complex real-time systems. However, the challenge is how to check the TCOZ models with tool support, especially for analyzing timing properties. Specialized graph-based modeling technique, Timed Automata (TA), has powerful mechanisms for designing real-time models using multiple clocks and has well developed automatic tool support. One weakness of TA is the lack of high level composable graphical patterns to support the systematic design for complex systems. The investigation of the possible links between TCOZ and TA may benefit both techniques. For TCOZ, TA's tool support can be reused to check timing properties (rather than developing a new TCOZ tool from scratch). For TA, a possible set of composable graphical patterns can be defined based on the semantics of the TCOZ constructs (so that those patterns can be used as a library in an engineering way). This paper firstly defines the composable TA graphical patterns, then presents sound transformation rules and a tool for projecting TCOZ specifications into TA. A case study of a railroad crossing is demonstrated.", "num_citations": "3\n", "authors": ["1634"]}
{"title": "Collision avoidance testing for autonomous driving systems on complete maps\n", "abstract": " Collision avoidance is one of the crucial functions of autonomous driving systems (ADSs) to guarantee the safety of autonomous vehicles (AVs). It requires extensive testing before an AV is deployed to public roads. Most of the current ADS testing methods generate test cases either from real traffic data or manually designed for some specific scenarios. There is little work on systematic methods to generate test cases from a complete map where an AV operates. Systematic testing on such a map is challenging due to the enormous scenarios. In this paper, we propose a collision-avoidance testing method for ADSs running on a map, which aims to reduce the scenario space while maintaining scenario diversity. The method consists of test case classification and test case generation. First, we build the topology structure of a map, based on which we classify possible scenarios into different classes. Second, we divide\u00a0\u2026", "num_citations": "2\n", "authors": ["1634"]}
{"title": "Adversarial Attacks and Mitigation for Anomaly Detectors of Cyber-Physical Systems\n", "abstract": " The threats faced by cyber-physical systems\u00a0(CPSs) in critical infrastructure have motivated research into a multitude of attack detection mechanisms, including anomaly detectors based on neural network models. The effectiveness of anomaly detectors can be assessed by subjecting them to test suites of attacks, but less consideration has been given to adversarial attackers that craft noise specifically designed to deceive them. While successfully applied in domains such as images and audio, adversarial attacks are much harder to implement in CPSs due to the presence of other built-in defence mechanisms such as rule checkers (or invariant checkers). In this work, we present an adversarial attack that simultaneously evades the anomaly detectors and rule checkers of a CPS. Inspired by existing gradient-based approaches, our adversarial attack crafts noise over the sensor and actuator values, then uses a\u00a0\u2026", "num_citations": "2\n", "authors": ["1634"]}
{"title": "sGUARD: Towards Fixing Vulnerable Smart Contracts Automatically\n", "abstract": " Smart contracts are distributed, self-enforcing programs executing on top of blockchain networks. They have the potential to revolutionize many industries such as financial institutes and supply chains. However, smart contracts are subject to code-based vulnerabilities, which casts a shadow on its applications. As smart contracts are unpatchable (due to the immutability of blockchain), it is essential that smart contracts are guaranteed to be free of vulnerabilities. Unfortunately, smart contract languages such as Solidity are Turing-complete, which implies that verifying them statically is infeasible. Thus, alternative approaches must be developed to provide the guarantee. In this work, we develop an approach which automatically transforms smart contracts so that they are provably free of 4 common kinds of vulnerabilities. The key idea is to apply runtime verification in an efficient and provably correct manner. Experiment results with 5000 smart contracts show that our approach incurs minor run-time overhead in terms of time (i.e., 14.79%) and gas (i.e., 0.79%).", "num_citations": "2\n", "authors": ["1634"]}
{"title": "Learning Fault Models of Cyber Physical Systems\n", "abstract": " Cyber Physical Systems (CPSs) comprise sensors and actuators which interact with the physical environment over a computer network to achieve some control objective. Bugs in CPSs can have severe consequences as CPSs are increasingly deployed in safety-critical applications. Debugging CPSs is therefore an important real world problem. Traces from a CPS can be lengthy and are usually linked to different parts of the system, making debugging CPSs a complex and time-consuming undertaking. It is challenging to isolate a component without running the whole CPS. In this work, we propose a model-based approach to debugging a CPS. For each CPS property, active automata learning is applied to learn a fault model, which is a Deterministic Finite Automata (DFA) of the violation of the property. The L* algorithm (L*) will find a minimum DFA given the queries and counterexamples. Short test cases\u00a0\u2026", "num_citations": "2\n", "authors": ["1634"]}
{"title": "Static analysis of context leaks in android applications\n", "abstract": " Android native applications, written in Java and distributed in APK format, are widely used in mobile devices. Their specific pattern of use lets the operating system control the creation and destruction of key resources, such as activities and services (contexts). Programmers are not supposed to interfere with such lifecycle events. Otherwise contexts might be leaked, ie they will never be deallocated from memory, or be deallocated too late, leading to memory exhaustion and frozen applications. In practice, it is easy to write incorrect code, which hinders garbage collection of contexts and subsequently leads to context leakage.", "num_citations": "2\n", "authors": ["1634"]}
{"title": "Learning likely invariants to explain why a program fails\n", "abstract": " Debugging is difficult. Recent studies show that automatic bug localization techniques have limited usefulness. One of the reasons is that programmers typically have to understand why the program fails before fixing it. In this work, we aim to help programmers understand a bug by automatically generating likely invariants which are violated in the failed tests. Given a program with an initial assertion and at least one test case failing the assertion, we first generate random test cases, identify potential bug locations through bug localization, and then generate program state mutation based on active learning techniques to identify a predicate \"explaining\" the cause of the bug. The predicate is a classifier for the passed test cases and failed test cases. Our main contribution is the application of invariant learning for bug explanation, as well as a novel approach to overcome the problem of lack of test cases in practice. We\u00a0\u2026", "num_citations": "2\n", "authors": ["1634"]}
{"title": "Jsfox: integrating static and dynamic type analysis of javascript programs\n", "abstract": " JavaScript is a dynamic programming language that has been widely used nowadays. The dynamism has become a hindrance of type analysis for JavaScript. Existing works use either static or dynamic type analysis to infer variable types for JavaScript. Static type analysis of JavaScript is difficult since it is hard to predict the behavior of the language without execution. Dynamic type analysis is usually incomplete as it might not cover all paths of a JavaScript program. In this work, we propose jsFox, a browser-agnostic approach that provides integrated type analysis, based on both static and dynamic type analysis, which enables us to gain the merits of both types of analysis. We have made use of the integrated type analysis for finding type issues that could potentially lead to erroneous results. jsFox discovers 23 type issues in existing benchmark suites and real-world Web applications.", "num_citations": "2\n", "authors": ["1634"]}
{"title": "Automated Verification of Timed Security Protocols with Clock Drift\n", "abstract": " Time is frequently used in security protocols to provide better security. For instance, critical credentials often have limited lifetime which improves the security against brute-force attacks. However, it is challenging to correctly use time in protocol design, due to the existence of clock drift in practice. In this work, we develop a systematic method to formally specify as well as automatically verify timed security protocols with clock drift. We first extend the previously proposed timed applied          -calculus as a formal specification language for timed protocols with clock drift. Then, we define its formal semantics based on timed logic rules, which facilitates efficient verification against various security properties. Clock drift is encoded as parameters in the rules. The verification result shows the constraints associated with clock drift that are required for the security of the protocol, e.g., the maximum drift should be less\u00a0\u2026", "num_citations": "2\n", "authors": ["1634"]}
{"title": "Formalizing and verifying stochastic system architectures using Monterey Phoenix (SoSyM abstract)\n", "abstract": " The analysis of software architecture plays an important role in understanding the system structures and facilitate proper implementation of user requirements. Despite its importance in the software engineering practice, the lack of formal description and verification support in this domain hinders the development of quality architectural models. To tackle this problem, in this work, we develop an approach for modeling and verifying software architectures specified using Monterey Phoenix (MP) architecture description language. MP is capable of modeling system and environment behaviors based on event traces, as well as supporting different architecture composition operations and views. First, we formalize the syntax and operational semantics for MP; therefore, formal verification of MP models is feasible. Second, we extend MP to support shared variables and stochastic characteristics, which not only increases the\u00a0\u2026", "num_citations": "2\n", "authors": ["1634"]}
{"title": "Event analytics\n", "abstract": " The process analysis toolkit (PAT) integrates the expressiveness of state, event, time, and probability-based languages with the power of model checking. PAT is a self-contained reasoning system for system specification, simulation, and verification. PAT currently supports a wide range of 12 different expressive modeling languages with many application domains and has attracted thousands of registered users from hundreds of organizations. In this invited talk, we will present the PAT system and its vision on \u201cEvent Analytics\u201d (EA) which is beyond \u201cData Analytics\u201d. The EA research is based on applying model checking to event planning, scheduling, prediction, strategy analysis and decision making. Various new EA research directions will be discussed.", "num_citations": "2\n", "authors": ["1634"]}
{"title": "Symmetry Detection for Model Checking\n", "abstract": " We present an automatic approach to detecting symmetry relations for general concurrent models. Despite the success of symmetry reduction in mitigating state explosion problem, one essential step towards its soundness and effectiveness, ie, how to discover sufficient symmetries with least human effort, is often either overlooked or oversimplified. In this work, we show how a concurrent model can be viewed as a constraint satisfaction problem (CSP), and present an algorithm capable of detecting arbitrary symmetries arising from the CSP which induce automorphisms of the model. Unlike previous approaches, our method can automatically detect both various process and data symmetries as demonstrated via a number of systems. Further, we propose an inductive approach to inferring symmetries in a parameterized system from the symmetries detected over a small set of its instances.", "num_citations": "2\n", "authors": ["1634"]}
{"title": "Improving indoor localization with social interactions\n", "abstract": " In this paper, we propose Social-Loc, which uniquely utilizes social interactions in addition to common on-board sensors such as accelerometer and gyroscope on modern smartphones, to localize indoor mobile users. Specifically, Social-Loc takes the potential locations for individual users estimated by a novel particle filter tailored for indoor localization as input, and exploits both social encounter and non-encounter events to further improve the localization accuracy. We have implemented Social-Loc on the Android platform and extensively evaluated its performance. The simulation results demonstrate that Social-Loc improves the accuracy of the particle-filter-only scheme by as much as 560% on average and is able to achieve accuracy of few meters without any external ranging device or system training.", "num_citations": "2\n", "authors": ["1634"]}
{"title": "PRTS: Specification and Model Checking\n", "abstract": " With the development of computing and sensing technology, information process and control software are integrated into everyday objects and activities. Design and development of control software for safety-critical systems are notoriously difficult problems. Real-life systems often have complex data components or complicated hierarchical control flows. Furthermore, control software often interacts with physical environment and therefore depends on quantitative timing. It is a challenging task to verify hierarchical complex real-time systems. In addition, probability exhibits itself commonly in the form of statistical estimates regarding the environment in which control software is embedded. Requiring a system always function perfectly within any environment is often overwhelming.In last 3 years, we aim to develop a useful tool for verifying hierarchical complex probabilistic real-time systems. In the first phase, we proposed a language called CSP# for system modeling. CSP# is an expressive language, combining Hoare\u2019s CSP [7] and data structures; after that, we extended this language to support probabilistic choices and we named it as PCSP#. It extended previous work on combining CSP with probabilistic choice [8] or on combining CSP with data structures [11]. Now we integrate real-time into this language and get PRTS. PRTS combines low-level programs, eg, sequence programs defined in a simple imperative language or any C# program, with high-level specifications (with process constructs like parallel, choice, hiding, etc.), as well as timed transitions and probabilistic choices. It supports shared variables as well as abstract events, making it\u00a0\u2026", "num_citations": "2\n", "authors": ["1634"]}
{"title": "Algorithmic design using object-z for twig xml queries evaluation\n", "abstract": " Web technologies based on XML, e.g. Semantic Web and Web Services, facilitate Web automation and universally accessible content. One of the key factors for the success of XML-based technologies is of finding an efficient query evaluation algorithm for XML-based data models. An XML twig query is a complex selection predicate on both structure and content of a labelled XML document. Several novel twig query evaluation algorithms have been proposed recently. However, these algorithms are difficult to understand and hence implement due to high complexity. In this work, we present an algorithmic design for XML queries evaluation system using Object-Z. An Object-Z specification is developed to give a concise and logical description of the XML data model and the twig queries. It makes the twig query evaluation straight-forward, and allows different evaluation algorithms to be constructed easily and\u00a0\u2026", "num_citations": "2\n", "authors": ["1634"]}
{"title": "From Live Sequence Charts to Distributed Implementations\n", "abstract": " Mechanized generation of prototypes from high-level specifications has long been an ultimate challenge for software engineering. One high-level specification of great interest is scenario-based sequence diagrams. Sequence diagrams serve as the manifestation of use cases and if synthesizable they could lead directly to implementation. In this work, we propose a method to generate prototypes all the way from a variant of sequence diagrams, namely Live Sequence Charts (LSC). The idea is of using mature theories and tool support of Communicating Sequential Process (CSP) to offer practical solutions to the distributed synthesis and verification problem of LSC. Our approach starts with defining a formal semantics for LSC, based on which a sound CSP interpretation of LSC is developed. CSP algebraic laws are then applied to synthesize the distributed behaviors of each object without constructing the global state machine. Another implication is that tool support for CSP can be reused to verify LSC specifications.", "num_citations": "2\n", "authors": ["1634"]}
{"title": "Generating MSCs from an integrated formal specification language\n", "abstract": " The requirements capture of complex systems requires powerful mechanisms for specifying system state, structure and interactive behaviors. Integrated formal specification languages are well suited for presenting more complete and coherent requirement models for complex systems. Given an integrated model, one can project it into multiple views for specialized analysis. Message Sequence Charts (MSCs) is a popular graphical notation for presenting interactive viewpoints of a system. In this paper, we investigate the semantic based transformation from an integrated formal specification language TCOZ to MSCs. An automated tool has also been developed for generating MSCs from TCOZ models. Furthermore, by inserting operation constraints (as assertions) into the generated MSCs, system testing requirements can be obtained.", "num_citations": "2\n", "authors": ["1634"]}
{"title": "Probabilistic verification of neural networks against group fairness\n", "abstract": " Fairness is crucial for neural networks which are used in applications with important societal implication. Recently, there have been multiple attempts on improving fairness of neural networks, with a focus on fairness testing (e.g., generating individual discriminatory instances) and fairness training (e.g., enhancing fairness through augmented training). In this work, we propose an approach to formally verify neural networks against fairness, with a focus on independence-based fairness such as group fairness. Our method is built upon an approach for learning Markov Chains from a user-provided neural network (i.e., a feed-forward neural network or a recurrent neural network) which is guaranteed to facilitate sound analysis. The learned Markov Chain not only allows us to verify (with Probably Approximate Correctness guarantee) whether the neural network is fair or not, but also facilities sensitivity analysis\u00a0\u2026", "num_citations": "1\n", "authors": ["1634"]}
{"title": "Automatic Fairness Testing of Neural Classifiers through Adversarial Sampling\n", "abstract": " Although deep learning has demonstrated astonishing performance in many applications, there are still concerns on their dependability. One desirable property of deep learning applications with societal impact is fairness (i.e., non-discrimination). Unfortunately, discrimination might be intrinsically embedded into the models due to discrimination in the training data. As a countermeasure, fairness testing systemically identifies discriminative samples, which can be used to retrain the model and improve its fairness. Existing fairness testing approaches however have two major limitations. First, they only work well on traditional machine learning models and have poor performance (e.g., effectiveness and efficiency) on deep learning models. Second, they only work on simple tabular data and are not applicable for domains such as text. In this work, we bridge the gap by proposing a scalable and effective approach for\u00a0\u2026", "num_citations": "1\n", "authors": ["1634"]}
{"title": "Towards Repairing Neural Networks Correctly\n", "abstract": " Neural networks are increasingly applied to support decision making in safety-critical applications (like autonomous cars, unmanned aerial vehicles and face recognition based authentication). While many impressive static verification techniques have been proposed to tackle the correctness problem of neural networks, it is possible that static verification may never be sufficiently scalable to handle real-world neural networks. In this work, we propose a runtime verification method to ensure the correctness of neural networks. Given a neural network and a desirable safety property, we adopt state-of-the-art static verification techniques to identify strategically locations to introduce additional gates which \"correct\" neural network behaviors at runtime. Experiment results show that our approach effectively generates neural networks which are guaranteed to satisfy the properties, whilst being consistent with the original neural network most of the time.", "num_citations": "1\n", "authors": ["1634"]}
{"title": "SOCRATES: Towards a Unified Platform for Neural Network Verification\n", "abstract": " Studies show that neural networks, not unlike traditional programs, are subject to bugs, eg, adversarial samples that cause classification errors and discriminatory instances that demonstrate the lack of fairness. Given that neural networks are increasingly applied in critical applications (eg, self-driving cars, face recognition systems and personal credit rating systems), it is desirable that systematic methods are developed to verify or falsify neural networks against desirable properties. Recently, a number of approaches have been developed to verify neural networks. These efforts are however scattered (ie, each approach tackles some restricted classes of neural networks against certain particular properties), incomparable (ie, each approach has its own assumptions and input format) and thus hard to apply, reuse or extend. In this project, we aim to build a unified framework for developing verification techniques for\u00a0\u2026", "num_citations": "1\n", "authors": ["1634"]}
{"title": "Compositional verification of heap-manipulating programs through property-guided learning\n", "abstract": " Analyzing and verifying heap-manipulating programs automatically is challenging. A key for fighting the complexity is to develop compositional methods. For instance, many existing verifiers for heap-manipulating programs require user-provided specification for each function in the program in order to decompose the verification problem. The requirement, however, often hinders the users from applying such tools. To overcome the issue, we propose to automatically learn heap-related program invariants in a property-guided way for each function call. The invariants are learned based on the memory graphs observed during test execution and improved through memory graph mutation. We implemented a prototype of our approach and integrated it with two existing program verifiers. The experimental results show that our approach enhances existing verifiers effectively in automatically verifying complex heap\u00a0\u2026", "num_citations": "1\n", "authors": ["1634"]}
{"title": "A verification framework for stateful security protocols\n", "abstract": " A long-standing research problem is how to efficiently verify security protocols with tamper-resistant global states, especially when the global states evolve unboundedly. We propose a protocol specification framework, which facilitates explicit modeling of states and state transformations. On the basis of that, we develop an algorithm for verifying security properties of protocols with unbounded state-evolving, by tracking state transformation and checking the validity of the state-evolving traces. We prove the correctness of the verification algorithm, implement both of the specification framework and the algorithm, and evaluate our implementation using a number of stateful security protocols. The experimental results show that our approach is both feasible and practically efficient. Particularly, we have found a security flaw on the digital envelope protocol, which cannot be detected with existing security protocol\u00a0\u2026", "num_citations": "1\n", "authors": ["1634"]}
{"title": "Language Inclusion Checking of Timed Automata with Non-Zenoness\n", "abstract": " Given a timed automaton P modeling an implementation and a timed automaton S as a specification, the problem of language inclusion checking is to decide whether the language of P is a subset of that of S. It is known to be undecidable. The problem gets more complicated if non-Zenoness is taken into consideration. A run is Zeno if it permits infinitely many actions within finite time. Otherwise it is non-Zeno. Zeno runs might present in both P and S. It is necessary to check whether a run is Zeno or not so as to avoid presenting Zeno runs as counterexamples of language inclusion checking. In this work, we propose a zone-based semi-algorithm for language inclusion checking with non-Zenoness. It is further improved with simulation reduction based on LU-simulation. Though our approach is not guaranteed to terminate, we show that it does in many cases through empirical study. Our approach has been\u00a0\u2026", "num_citations": "1\n", "authors": ["1634"]}
{"title": "Service adaptation with probabilistic partial models\n", "abstract": " Web service composition makes use of existing Web services to build complex business processes. Non-functional requirements are crucial for the Web service composition. In order to satisfy non-functional requirements when composing a Web service, one needs to rely on the estimated quality of the component services. However, estimation is seldom accurate especially in the dynamic environment. Hence, we propose a framework, ADFlow, to monitor and adapt the workflow of the Web service composition when necessary to maximize its ability to satisfy the non-functional requirements automatically. To reduce the monitoring overhead, ADFlow relies on asynchronous monitoring. ADFlow has been implemented and the evaluation has shown the effectiveness and efficiency of our approach. Given a composite service, ADFlow achieves 25\u00a0%\u201332\u00a0% of average improvement in the conformance of non\u00a0\u2026", "num_citations": "1\n", "authors": ["1634"]}
{"title": "Scaling BDD-based Timed Verification with Simulation Reduction\n", "abstract": " Digitization is a technique that has been widely used in real-time model checking. With the assumption of digital clocks, symbolic model checking techniques (like those based on BDDs) can be applied for real-time systems. The problem of model checking real-time systems based on digitization is that the number of tick transitions increases rapidly with the increment of clock upper bounds. In this paper, we propose to improve BDD-based verification for real-time systems using simulation reduction. We show that simulation reduction allows us to verify timed automata with large clock upper bounds and to converge faster to the fixpoint. The presented approach is applied to reachability and LTL verification for real-time systems. Finally, we compare our approach with existing tools such as Rabbit, Uppaal, and CTAV and show that our approach outperforms them and achieves a significant speedup.", "num_citations": "1\n", "authors": ["1634"]}
{"title": "Regular symmetry patterns (technical report)\n", "abstract": " Symmetry reduction is a well-known approach for alleviating the state explosion problem in model checking. Automatically identifying symmetries in concurrent systems, however, is computationally expensive. We propose a symbolic framework for capturing symmetry patterns in parameterised systems (i.e. an infinite family of finite-state systems): two regular word transducers to represent, respectively, parameterised systems and symmetry patterns. The framework subsumes various types of symmetry relations ranging from weaker notions (e.g. simulation preorders) to the strongest notion (i.e. isomorphisms). Our framework enjoys two algorithmic properties: (1) symmetry verification: given a transducer, we can automatically check whether it is a symmetry pattern of a given system, and (2) symmetry synthesis: we can automatically generate a symmetry pattern for a given system in the form of a transducer. Furthermore, our symbolic language allows additional constraints that the symmetry patterns need to satisfy to be easily incorporated in the verification/synthesis. We show how these properties can help identify symmetry patterns in examples like dining philosopher protocols, self-stabilising protocols, and prioritised resource-allocator protocol. In some cases (e.g. Gries's coffee can problem), our technique automatically synthesises a safety-preserving finite approximant, which can then be verified for safety solely using a finite-state model checker.", "num_citations": "1\n", "authors": ["1634"]}
{"title": "Event and strategy analytics\n", "abstract": " Model checking has been pervasive and successful in finding bugs in hardware and software systems, including real-time and probabilistic systems. Applying model checking to decision making is relative new and has an excellent potential to be compliment to data analytics and other Artificial Intelligent (AI) or Operational Research (OR) based decision making techniques. Our last 8 years research has focused on the development of PAT (Process Analysis Toolkit) [18] whichsupports modelling languages that combine the expressiveness of event, state, time and probability based modeling techniques to which model checking can be directly applied. The next direction for PAT is to move from verification to analytics, we call it \"Event Analytics\" with a special focus on \"Strategy Analytics\".", "num_citations": "1\n", "authors": ["1634"]}
{"title": "Towards a combination of cafeobj and pat\n", "abstract": " In the quest for tractable formal methods to improve the practice of software engineering, both CafeOBJ [7] and PAT [12] have made great achievements based on different formal techniques. CafeOBJ has an evident advantage in specifying concurrent systems with object-oriented methods and proving behavioral properties based on reusability of proof. However, it is difficult to be applied to automatically verify some LTL based properties which involve complex state updates and finite path of states. Conversely, PAT offers great flexibility to simulate system behaviors and support modeling checking various properties, but it is difficult to prove behavioral properties directly, the definition of which is based on the structure of contexts. In the paper, we attempt to combine the two approaches by modeling specifications and verifying properties in CafeOBJ and PAT. A keyless car system is provided to illustrate our\u00a0\u2026", "num_citations": "1\n", "authors": ["1634"]}
{"title": "Multi-core Model Checking Algorithms for LTL Verification with Fairness Assumptions\n", "abstract": " The main challenge in model checking is the state space explosion. With developments in hardware today, most processors have many cores inside. To leverage on the advances in hardware, we can increase the performance of verifying large models by designing parallel algorithms to run efficiently on multi-core architecture. This work focuses on this problem in the context of Linear Temporal Logic (LTL) model checking, which can be seen as finding accepting cycles in a graph. Recently, there are some parallel algorithms based on Nested Depth First Search (NDFS). In this work, we propose two new parallel algorithms based on strongly connected component (SCC) searching algorithm (i.e., Tarjan's algorithm). By finding all the SCCs in the graph, our approaches can not only check LTL properties, but also handle fairness assumptions all together. The experiments show that our new algorithms are comparable\u00a0\u2026", "num_citations": "1\n", "authors": ["1634"]}
{"title": "TTM/PAT: Specifying and Verifying Timed Transition Models\n", "abstract": " Timed Transition Models (TTMs) are event-based descriptions for specifying real-time systems in a discrete setting. We propose a convenient and expressive event-based textual syntax for TTMs and a corresponding operational semantics using labelled transition systems. A system is specified as a composition of module instances. Each module has a clean interface for declaring input, output, and shared variables. Events in a module can be specified, individually, as spontaneous, fair or real-time. An event action specifies a before-after predicate by a set of (possibly non-deterministic) assignments and nested conditionals. The TTM assertion language, linear-time temporal logic (LTL), allows references to event occurrences, including clock ticks (thus allowing for a check that the behaviour is non-Zeno). We implemented a model checker for the TTM notation (using the PAT framework) that includes an editor\u00a0\u2026", "num_citations": "1\n", "authors": ["1634"]}
{"title": "Parameter synthesis for hierarchical concurrent real-time systems (full version)\n", "abstract": " Modeling and verifying complex real-time systems, involving timing delays, are notoriously difficult problems. Checking the correctness of a system for one particular value for each delay does not give any information for other values. It is hence interesting to reason parametrically, by considering that the delays are parameters (unknown constants) and synthesize a constraint guaranteeing a correct behavior. We present here Parametric Stateful Timed CSP, viz., a parameterization of Stateful Timed CSP, a language capable of specifying hierarchical real-time systems with complex data structures. Although we prove that the synthesis is undecidable in general, we present an algorithm for efficient parameter synthesis that behaves well in practice.", "num_citations": "1\n", "authors": ["1634"]}
{"title": "SpecDiff: Debugging formal specifications\n", "abstract": " This paper presents our SpecDiff tool that exploits the model differencing technique for debugging and understanding evolving behaviors of formal specifications. SpecDiff has been integrated in the Process Analysis Toolkit (PAT), a framework for formal specification, verification and simulation. SpecDiff is able to assist in diagnosing system faults, understanding the impacts of specification optimization techniques, and revealing the system change patterns.", "num_citations": "1\n", "authors": ["1634"]}
{"title": "Enhanced Specification and Verification for Timed Planning\n", "abstract": " In this project, the PI introduced a specification language named Timed Planning, which is an extension of Timed CSP with the capability of stating more complicated timing behaviors for processes and events. They also developed a reasoning mechanism for Timed Planning based on Constraint Logic Programming. They model the Pearl Harbor Attack plan to demonstrate the capability of their approach for modeling time based military plans with critical timing constraints. Their approach is capable to handle the extended job-shop scheduling problems. In their work, the job shop scheduling problems with extensions can be naturally modeled as Timed Planning processes, whose complete executions correspond to feasible schedules. By using CLP based reasoning mechanism, the optimal scheduler which is an execution with the minimum execution time, can be found.Descriptors:", "num_citations": "1\n", "authors": ["1634"]}
{"title": "Debugging evolving system behaviors with SpecDiff\n", "abstract": " Modern specification languages are getting more and more sophisticated in order to capture intricate system behaviors. Like programs, specifications may evolve for a variety of reasons, eg, amendment of user requirements, bug fixing, or change of application contexts. Even when a specification remains unmodified, its behavior may change due to change of language semantics or the underlying behavior exploration system (eg a model checker). When an evolving specification behaves unexpectedly, it is often challenging to figure out what is wrong and why. Since specifications describe systems at a higher level than programs, syntax-based analysis and existing program analysis and debugging techniques may not be effective in locating faults in specifications. In this paper, we present SpecDiff, which exploits model differencing technique for debugging and understanding the evolving system behaviors of\u00a0\u2026", "num_citations": "1\n", "authors": ["1634"]}
{"title": "Event-based fairness\n", "abstract": " Liveness plays an important role in system requirements. Fairness conditions are an effective way of expressing liveness properties. Previous studies on liveness and fairness have been state-based, for instance, using the notion of B\\\"{u}chi automata. Traditional event-based specification languages like CSP and CCS deal with safety properties only. In this work, we propose a notion of event-based fairness, i.e., an intuitive yet sound way of associating fairness constraints with individual events. Our approach is demonstrated in the setting of the classic CSP.", "num_citations": "1\n", "authors": ["1634"]}
{"title": "Reasoning about ORA-SS data models using the semantic web\n", "abstract": " There has been a rapid growth in the use of semistructured data in both web applications and database systems. Consequently, the design of a good semistructured data model is essential. In the relational database community, algorithms have been defined to transform a relational schema from one normal form to a more suitable normal form. These algorithms have been shown to preserve certain semantics during the transformation. The work presented in this paper is the first step towards representing such algorithms for semistructured data, namely formally defining the semantics necessary for achieving this goal. Formal semantics and automated reasoning tools enable us to reveal the inconsistencies in a semistructured data model and its instances. The Object Relationship Attribute model for Semistructured data (ORA-SS) is a graphical notation for designing and representing semistructured data. This\u00a0\u2026", "num_citations": "1\n", "authors": ["1634"]}
{"title": "Automatically partition software into least privilege components using dynamic data dependency analysis.(2013)\n", "abstract": " The principle of least privilege requires that software components should be granted only necessary privileges, so that compromising one component does not lead to compromising others. However, writing privilege separated software is difficult and as a result, a large number of software is monolithic, ie, it runs as a whole without separation. Manually rewriting monolithic software into privilege separated software requires significant effort and can be error prone. We propose ProgramCutter, a novel approach to automatically partitioning monolithic software using dynamic data dependency analysis. ProgramCutter works by constructing a data dependency graph whose nodes are functions and edges are data dependencies between functions. The graph is then partitioned into subgraphs where each subgraph represents a least privilege component. The privilege separated software runs each component in a separated process with confined system privileges. We evaluate it by applying it on four open source software. We can reduce the privileged part of the program from 100% to below 22%, while having a reasonable execution time overhead. Since ProgramCutter does not require any expert knowledge of the software, it not only can be used by its developers for software refactoring, but also by end users or system administrators. Our contributions are threefold:(i) we define a quantitative measure of the security and performance of privilege separation;(ii) we propose a graph-based approach to compute the optimal separation based on dynamic information flow analysis; and (iii) the separation process is automatic and does not require expert\u00a0\u2026", "num_citations": "1\n", "authors": ["1634"]}
{"title": "Automated synthesis of local time requirement for service composition\n", "abstract": " Service composition aims at achieving a business goal by composing existing service-based applications or components. The response time of a service is crucial especially in time critical business environments, which is often stated as a clause in service level agreements between service providers and service users. To meet the guaranteed response time requirement of a composite service, it is important to select a feasible set of component services such that their response time will collectively satisfy the response time requirement of the composite service. In this work, we propose a fully automated approach to synthesize the response time requirement of component services, in the form of a constraint on the local response times. The synthesized requirement will guarantee the satisfaction of the global response time requirement, statically or dynamically. We implemented our work into a tool SELAMAT, and performed several experiments to evaluate the validity of our approach.", "num_citations": "1\n", "authors": ["1634"]}