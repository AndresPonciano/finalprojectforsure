{"title": "The model checker SPIN\n", "abstract": " SPIN is an efficient verification system for models of distributed software systems. It has been used to detect design errors in applications ranging from high-level descriptions of distributed algorithms to detailed code for controlling telephone exchanges. The paper gives an overview of the design and structure of the verifier, reviews its theoretical foundation, and gives an overview of significant practical applications.", "num_citations": "9932\n", "authors": ["659"]}
{"title": "Design and validation of computer protocols\n", "abstract": " 3.10 Arithmetic Checksum 63 3.11 Summary 64 Exercises 64 Bibliographic Notes 65 4. Flow Control 4.1 Introduction 66 4.2 Window Protocols 70 4.3 Sequence Numbers 74 4.4 Negative Acknowledgments 80 4.5 Congestion Avoidance 83 4.6 Summary 86 Exercises 87 Bibliographic Notes 88", "num_citations": "3031\n", "authors": ["659"]}
{"title": "An improvement in formal verification\n", "abstract": " Critical safety and liveness properties of a concurrent system can often be proven with the help of a reachability analysis of a finite state model. This type of analysis is usually implemented as a depth-first search of the product state-space of all components in the system, with each (finite state) component modeling the behavior of one asynchronously executing process. Formal verification is achieved by coupling the depth-first search with a method for identifying those states or sequences of states that violate the correctness requirements.", "num_citations": "490\n", "authors": ["659"]}
{"title": "An analysis of bitstate hashing\n", "abstract": " The bitstate hashing, or supertrace, technique was introduced in 1987 as a method to increase the quality of verification by reachability analyses for applications that defeat analysis by traditional means because of their size. Since then, the technique has been included in many research verification tools, and was adopted in tools that are marketed commercially. It is therefore important that we understand well how and why the method works, what its limitations are, and how it compares with alternative methods over a broad range of problem sizes.               The original motivation for the bitstate hashing technique was based on empirical evidence of its effectiveness. In this paper we provide an analytical argument. We compare the technique with two alternatives that have been proposed in the recent literature. We also describe a sequential bitstate hashing technique that can be of value when confronted\u00a0\u2026", "num_citations": "347\n", "authors": ["659"]}
{"title": "Software model checking: Extracting verification models from source code\n", "abstract": " To formally verify a large software application, the standard method is to invest a considerable amount of time and expertise into the manual construction of an abstract model, which is then analysed for its properties by either a mechanized or a human prover. There are two problems with this approach. The first problem is that this verification method can be no more reliable than the humans that perform the manual steps. If the average rate of error for human work is a function of the problem size, this holds not only for the construction of the original application, but also for the construction of the model. The standard verification trajectory therefore tends to become less reliable for larger applications. The second problem is one of timing and relevance. Software applications built by teams of programmers can change rapidly, often daily. Manually constructing an accurate abstraction of any one version of the\u00a0\u2026", "num_citations": "253\n", "authors": ["659"]}
{"title": "Optimizing b\u00fcchi automata\n", "abstract": " We describe a family of optimizations implemented in a translation from a linear temporal logic to B\u00fcchi automata. Such optimized automata can enhance the efficiency of model checking, as practiced in tools such as Spin. Some of our optimizations are applied during preprocessing of temporal formulas, while other key optimizations are applied directly to the resulting B\u00fcchi automata independent of how they arose. Among these latter optimizations we apply a variant of fair simulation reduction based on color refinement. We have implemented our optimizations in a translation of an extension to LTL described in [Ete99]. Inspired by this work, a subset of the optimizations outlined here has been added to a recent version of Spin. Both implementations begin with an underlying algorithm of [GPVW95]. We describe the results of tests we have conducted, both to see how the optimizations improve the sizes of\u00a0\u2026", "num_citations": "243\n", "authors": ["659"]}
{"title": "Implementing statecharts in PROMELA/SPIN\n", "abstract": " We translate statecharts into PROMELA, the input language of the SPIN verification system, using extended hierarchical automata as an intermediate format. We discuss two possible frameworks for this translation, leading to either sequential or parallel code. We show that in this context the sequential code can be verified more efficiently than the parallel code. We conclude with a discussion of an application of the resulting translator to a well-known case study, which demonstrates the feasibility of linear temporal logic model checking of statecharts.", "num_citations": "240\n", "authors": ["659"]}
{"title": "An improved protocol reachability analysis technique\n", "abstract": " An automated analysis of all reachable states in a distributed system can be used to trace obscure logical errors that would be very hard to find manually. This type of validation is traditionally performed by the symbolic execution of a finite state machine (FSM) model of the system studied. The application of this method to systems of a practical size, though, is complicated by time and space requirements. If a system is larger, more space is needed to store the state descriptions and more time is needed to compare and analyze these states. This paper shows that if the FSM model is abandoned and replaced by a state vector model significant gains in performance are feasible, for the first time making it possible to perform effective validations of large systems.", "num_citations": "206\n", "authors": ["659"]}
{"title": "The early history of data networks\n", "abstract": " The exhaustive research for this book took ten years. The authors have pursued the subject of data networks, or telecommunications, back to its beginnings in ancient Greece, Rome, Persia, and the Mongol empire, and later in England, Spain, France, and other nations. The authors describe the many attempts at establishing signaling methods. All of them were only marginally successful, but they formed the basis for later attempts. Most of the space is devoted to semaphore telegraphs, which were introduced in France by Claude Chappe and in Sweden by Abraham Clewberg, later renamed Edelcrantz, around 1800. They were widely used, mostly for official correspondence. Without this book, American readers would probably never have heard about these two men and their epoch-making achievements. Pehrson's description of the Swedish telegraph system is evidently a labor of love. He provides an English\u00a0\u2026", "num_citations": "191\n", "authors": ["659"]}
{"title": "State compression in SPIN: Recursive indexing and compression training runs\n", "abstract": " The verification algorithm of SPIN is based on an explicit enumeration of a subset of the reachable state-space of a system that is obtained through the formalization of a correctness requirement as an \u03c9-automaton. This \u03c9-automaton restricts the state-space to precisely the subset that may contain the counter-examples to the original correctness requirement, if they exist. This method of verification conforms to the method for automata-theoretic verification outlined in [VW86]. SPIN derives much of its power from an efficient implementation of the explicit state enumeration method in this context. The maximum number of reachable states that can be stored in main memory with this method, either explicitly or implicitly, then determines the maximum problem size that can be solved. We review some methods that have been tried to reduce the amount memory used per state, and describe some new methods that achieve relatively high compression rates.", "num_citations": "184\n", "authors": ["659"]}
{"title": "Logic verification of ANSI-C code with SPIN\n", "abstract": " We describe a tool, called AX, that can be used in combination with the model checker Spin to efficiently verify logical properties of distributed software systems implemented in ANSI-standard C [18]. AX, short for Automaton eXtractor, can extract verification models from C code at a user defined level of abstraction. Target applications include telephone switching software, distributed operating systems code, protocol implementations, concurrency control methods, and client-server applications. This paper discusses how AX is currently implemented, and how we plan to extend it. The tool was used in the formal verification of two substantial software applications: a commercial checkpoint management system and the call processing code for a new telephone switch.", "num_citations": "177\n", "authors": ["659"]}
{"title": "The design of a multicore extension of the SPIN model checker\n", "abstract": " We describe an extension of the SPIN model checker for use on multicore shared-memory systems and report on its performance. We show how, with proper load balancing, the time requirements of a verification run can, in some cases, be reduced close to N-fold when N processing cores are used. We also analyze the types of verification problems for which multicore algorithms cannot provide relief. The extensions discussed here require only relatively small changes in the SPIN source code and are compatible with most existing verification modes such as partial order reduction, the verification of temporal logic formulas, bitstate hashing, and hash-compact compression.", "num_citations": "168\n", "authors": ["659"]}
{"title": "A practical method for verifying event-driven software\n", "abstract": " Formal verification methods are used only sparingly in software development. The most successful methods to date are based on the use of model checking tools. To use such tools, the user must first define a faithful abstraction of the application (the model), specify how the application interacts with its environment, and then formulate the properties that it should satisfy. Each step in this process can become an obstacle. To complete the verification process successfully often requires specialized knowledge of verification techniques and a considerable investment of time. In this paper we describe a verification method that requires little or no specialized knowledge in model construction. It allows us to extract models mechanically from the source of software applications, securing accuracy. Interface definitions and property specifications have meaningful defaults that can be adjusted when the checking process\u00a0\u2026", "num_citations": "161\n", "authors": ["659"]}
{"title": "Model-driven software verification\n", "abstract": " In the classic approach to logic model checking, software verification requires a manually constructed artifact (the model) to be written in the language that is accepted by the model checker. The construction of such a model typically requires good knowledge of both the application being verified and of the capabilities of the model checker that is used for the verification. Inadequate knowledge of the model checker can limit the scope of verification that can be performed; inadequate knowledge of the application can undermine the validity of the verification experiment itself.               In this paper we explore a different approach to software verification. With this approach, a software application can be included, without substantial change, into a verification test-harness and then verified directly, while preserving the ability to apply data abstraction techniques. Only the test-harness is written in the language of the\u00a0\u2026", "num_citations": "159\n", "authors": ["659"]}
{"title": "Automating software feature verification\n", "abstract": " A significant part of the call processing software for Lucent's new PathStar\u2122 Access Server was checked with formal verification techniques. The verification system we built for this purpose, named FeaVer, is accessed via a standard Web browser. The system maintains a database of feature requirements, together with the results of the most recently performed verifications. Via the browser the user can invoke new verification runs, which are performed in the background with the help of a logic model checking tool. Requirement violations are reported either as high-level message sequence charts or as detailed execution traces of the system source. A main strength of the system is its capability to detect potential feature interaction problems at an early stage of systems design. This type of problem is difficult to detect with traditional testing techniques. Error reports are typically generated by the system within minutes\u00a0\u2026", "num_citations": "156\n", "authors": ["659"]}
{"title": "The power of 10: Rules for developing safety-critical code\n", "abstract": " Existing coding guidelines therefore offer limited benefit, even for critical applications. A verifiable set of well-chosen coding rules could, however, assist in analyzing critical software components for properties that go well beyond compliance with the set of rules itself. To be effective, though, the set of rules must be small, and it must be clear enough that users can easily understand and remember it. In addition, the rules must be specific enough that users can check them thoroughly and mechanically. To put an upper bound on the number of rules, the set is restricted to no more than 10 rules that will provide an effective guideline. Although such a small set of rules cannot be all-encompassing, following it can achieve measurable effects on software reliability and verifiability", "num_citations": "148\n", "authors": ["659"]}
{"title": "Static source code checking for user-defined properties\n", "abstract": " Only a small fraction of the output generated by typical static analysis tools tends to reveal serious software defects. There are two main causes for this phenomenon. The first is that the typical static analyzer casts its nets too broadly, reporting everything reportable, rather than what is likely to be a true bug. The second cause is that most static analyzers can check the code for only a fixed set of flaws. We describe a simple source code analyzer, UNO, that tries to remedy these problems. The default properties searched for by UNO are restricted to the most common types of error in C programs: use of uninitialized variables, nil-pointer dereferencing, and out-of-bound array indexing. The checking capabilities of UNO can be extended by the user with the definition of applicationdependent properties, which are written as ANSI-C functions.", "num_citations": "148\n", "authors": ["659"]}
{"title": "The state of SPIN\n", "abstract": " The number of installations of the Spin model checking tool is steadily increasing. There are well over two thousand installations today, divided roughly evenly over academic and industrial sites. The tool itself also continues to evolve; it has more than doubled in size, and hopefully at least equally so in functionality, since it was first distributed in early 1991. The tool runs on most standard workstations, and starting with version 2.8 also on standard PCs. In this overview, we summarize the design principles of the tool, and review its current state.", "num_citations": "144\n", "authors": ["659"]}
{"title": "Randomized differential testing as a prelude to formal verification\n", "abstract": " Most flight software testing at the Jet Propulsion Laboratory relies on the use of hand-produced test scenarios and is executed on systems as similar as possible to actual mission hardware. We report on a flight software development effort incorporating large-scale (biased) randomized testing on commodity desktop hardware. The results show that use of a reference implementation, hardware simulation with fault injection, a testable design, and test minimization enabled a high degree of automation in fault detection and correction. Our experience will be of particular interest to developers working in domains where on-time delivery of software is critical (a strong argument for randomized automated testing) but not at the expense of correctness and reliability (a strong argument for model checking, theorem proving, and other heavyweight techniques). The effort spent in randomized testing can prepare the way for\u00a0\u2026", "num_citations": "139\n", "authors": ["659"]}
{"title": "Automated protocol validation in argos: Assertion proving and scatter searching\n", "abstract": " Argos is a validation language for data communication protocols. To validate a protocol, a model in Argos is constructed consisting of a control flow specification and a formal description of the correctness requirements. This model can be compiled into a minimized lower level description that is based on a formal model of communicating finite state machines. An automated protocol validator trace uses these minimized descriptions to perform a partial symbolic execution of the protocol to establish its correctness for the given requirements.", "num_citations": "138\n", "authors": ["659"]}
{"title": "Algorithms for automated protocol verification\n", "abstract": " This paper studies the four basic types of algorithm that, over the last 10 years, have been developed for the automated verification of the logical consistency of data communication protocols. The algorithms are compared on memory usage, CPU time requirements, and the quality of the search for errors. It is shown that the best algorithm, according to above criteria, can be improved further in a significant way, by avoiding a known performance bottleneck. The algorithm derived in this manner works in a fixed-size memory arena (it will never run out of memory), it is up to 2 orders of magnitude faster than the previous methods, and it has superior coverage of the state space when analyzing large protocol systems. The algorithm is the first for which the search efficiency (the number of states analyzed per second) does not depend on the size of the state space: there is no time penalty for analyzing very large state\u00a0\u2026", "num_citations": "134\n", "authors": ["659"]}
{"title": "Design and validation of protocols: a tutorial\n", "abstract": " It can be remarkably hard to design a good communications protocol, much harder than it is to write a sequential program. Unfortunately, when the design of a new protocol is complete, we usually have little trouble convincing ourselves that it is trivially correct. It can be a unreasonably hard to prove those facts formally and to convince also others. Faced with that dilemma, a designer usually decides to trust his or her instincts and forgo the formal proofs. The subtle logical flaws in a design thus get a chance to hide, and inevitably find the worst possible moment in the lifetime of the protocol to reveal themselves. Though few will admit it, most people design protocols by trial and error. There is a known set of trusted protocol standards, whose descriptions are faithfully copied in most textbooks, but there is little understanding of why some designs are correct and why others are not. To design and to analyze protocols you\u00a0\u2026", "num_citations": "128\n", "authors": ["659"]}
{"title": "On limits and possibilities of automated protocol analysis\n", "abstract": " It is not likely that many traveling salesmen can be discouraged from their job by a lecture on its complexity [9]. Not surprisingly, writers of automated protocol analyzers are much the same. The problem of determining whether an arbitrary message passing system contains deadlocks is PSPACE-complete at best (for bounded queue lengths)[7-9]. Yet for any given formal analysis model it is easy to derive state space exploration routines that can find such errors with certainty\u2014given a sufficient amount of time and space. In practice, therefore, one of the main problems is to optimize the speed and memory usage of an automated validator. To phrase it differently: it is not hard to validate protocols, it is hard to do it (sufficiently) fast. In reachability analyses, the limits of what can be analyzed in practice can be moved substantially if the traditional finite state machine model is abandoned. To illustrate this, we introduce a simple symbolic execution method based on vector addition. It is extended into a full protocol validator, carefully avoiding known performance bottlenecks. Compared with previous methods the performance of this validator is roughly two orders of magnitude in speed faster and allows validation of protocol systems up to 10 6 states in only minutes of CPU time on a medium size computer.", "num_citations": "125\n", "authors": ["659"]}
{"title": "An automated verification method for distributed systems software based on model extraction\n", "abstract": " Software verification methods are used only sparingly in industrial software development today. The most successful methods are based on the use of model checking. There are, however, many hurdles to overcome before the use of model checking tools can truly become mainstream. To use a model checker, the user must first define a formal model of the application, and to do so requires specialized knowledge of both the application and of model checking techniques. For larger applications, the effort to manually construct a formal model can take a considerable investment of time and expertise, which can rarely be afforded. Worse, it is hard to secure that a manually constructed model can keep pace with the typical software application, as it evolves from the concept stage to the product stage. We describe a verification method that requires far less specialized knowledge in model construction. It allows us to\u00a0\u2026", "num_citations": "123\n", "authors": ["659"]}
{"title": "Events and constraints: A graphical editor for capturing logic requirements of programs\n", "abstract": " A logic model checker can be an effective tool for debugging software applications. A stumbling block can be that model-checking tools expect the user to supply a formal statement of the correctness requirements to be checked in temporal logic. Expressing non-trivial requirements in logic, however, can be challenging. To address this problem, we developed a graphical tool, called the TimeLine Editor, that simplifies the formalization of certain kinds of requirements. A series of events and required system responses are placed on a timeline. The user converts the timeline specification automatically into a test automaton that can be used directly by a logic model checker or for traditional test-sequence generation. We have used the TimeLine Editor to verify the call processing code for Lucent's PathStar access server against the TelCordia LSSGR [LATA (local access and transport area) Switching Systems Generic\u00a0\u2026", "num_citations": "116\n", "authors": ["659"]}
{"title": "Tracing protocols\n", "abstract": " Automated protocol validation tools are by necessity often based on some form of symbolic execution. The complexity of the analysis problem however imposes restrictions on the scope of these tools. The paper studies the nature of these restrictions and explicitly addresses the problem of finding errors in data communication protocols of which the size precludes analysis by traditional means. The protocol tracing method described here allows one to locate design errors in protocols relatively quickly by probing a partial state space. This scatter searching method was implemented in a portable program called Trace. Specifications for the tracer are written in a higher-level language and are compiled into a minimized finite state machine model, which is then used to perform either partial or exhaustive symbolic executions. The user of the tracer can control the scope of each search. The tracer can be used as a fast\u00a0\u2026", "num_citations": "112\n", "authors": ["659"]}
{"title": "The logic of bugs\n", "abstract": " Real-life bugs are successful because of their unfailing ability to adapt. In particular this applies to their ability to adapt to strategies that are meant to eradicate them as a species. Software bugs have some of these same traits. We will discuss these traits, and consider what we can do about them.", "num_citations": "111\n", "authors": ["659"]}
{"title": "A minimized automaton representation of reachable states\n", "abstract": " We consider the problem of storing a set S\u2282\u03a3kas a deterministic finite automaton (DFA). We show that inserting a new string \u03c3\u2208\u03a3k or deleting a string from the set S represented as a minimized DFA can be done in expected time O(k|\u03a3|), while preserving the minimality of the DFA. The method can be applied to reduce the memory requirements of model checkers that are based on explicit state enumeration. As an example, we discuss an implementation of the method for the model checker Spin.", "num_citations": "108\n", "authors": ["659"]}
{"title": "A mini challenge: build a verifiable filesystem\n", "abstract": " We propose tackling a \u201cmini challenge\u201d problem: a nontrivial verification effort that can be completed in 2\u20133\u00a0years, and will help establish notational standards, common formats, and libraries of benchmarks that will be essential in order for the verification community to collaborate on meeting Hoare\u2019s 15-year verification grand challenge. We believe that a suitable candidate for such a mini challenge is the development of a filesystem that is verifiably reliable and secure. The paper argues why we believe a filesystem is the right candidate for a mini challenge and describes a project in which we are building a small embedded filesystem for use with flash memory.", "num_citations": "107\n", "authors": ["659"]}
{"title": "Swarm verification techniques\n", "abstract": " The range of verification problems that can be solved with logic model checking tools has increased significantly in the last few decades. This increase in capability is based on algorithmic advances and new theoretical insights, but it has also benefitted from the steady increase in processing speeds and main memory sizes on standard computers. The steady increase in processing speeds, though, ended when chip-makers started redirecting their efforts to the development of multicore systems. For the near-term future, we can anticipate the appearance of systems with large numbers of CPU cores, but without matching increases in clock-speeds. We will describe a model checking strategy that can allow us to leverage this trend and that allows us to tackle significantly larger problem sizes than before.", "num_citations": "103\n", "authors": ["659"]}
{"title": "Apparatus and method for communicating data between elements of a distributed system using a general protocol\n", "abstract": " Apparatus and methods for communicating using protocols. The apparatus and methods employ protocol descriptions written in a device-independent protocol description language. A protocol is executed by employing a protocol description language interpreter to interpret the protocol description. Communication using any protocol for which there is a protocol description may be done by means of a general protocol. The general protocol includes a first general protocol message which includes a protocol description for a specific protocol. The protocol apparatus which receives the first protocol message employs a protocol description language interpreter to interpret the included protocol description and thereby to execute the specific protocol. The protocol apparatus may also be made to adapt to its environment by encaching protocol descriptions which were received in an earlier first general protocol message\u00a0\u2026", "num_citations": "102\n", "authors": ["659"]}
{"title": "Using SPIN model checking for flight software verification\n", "abstract": " Flight software is the central nervous system of modern spacecraft. Verifying spacecraft flight software to assure that it operates correctly and safely is presently an intensive and costly process. A multitude of scenarios and tests must be devised, executed and reviewed to provide reasonable confidence that the software will perform as intended and not endanger the spacecraft. Undetected software defects on spacecraft and launch vehicles have caused embarrassing and costly failures in recent years. Model checking is a technique for software verification that can detect concurrency defects that are otherwise difficult to discover. Within appropriate constraints, a model checker can perform an exhaustive state-space search on a software design or implementation and alert the implementing organization to potential design deficiencies. Unfortunately, model checking of large software systems requires an often-too\u00a0\u2026", "num_citations": "89\n", "authors": ["659"]}
{"title": "Parallelizing the spin model checker\n", "abstract": " We describe an extension of the Spin model checker that allows us to take advantage of the increasing number of cpu-cores available on standard desktop systems. Our main target is to speed up the verification process for safety properties, the mode used most frequently, but we also describe a small modification of the parallel search algorithm, called the piggyback algorithm, that is remarkably effective in catching violations for an interesting class of liveness properties at little cost.", "num_citations": "83\n", "authors": ["659"]}
{"title": "Mars code\n", "abstract": " Redundant software (and hardware) ensured Curiosity reached its destination and functioned as its designers intended.", "num_citations": "81\n", "authors": ["659"]}
{"title": "Design and validation of protocols\n", "abstract": " It can be remarkably hard to design a good communications protocol, much harder than it is to write a sequential program. Unfortunately, when the design of a new protocol is complete, we usually have little trouble convincing ourselves that it is trivially correct. It can be a unreasonably hard to prove those facts formally and to convince also others. Faced with that dilemma, a designer usually decides to trust his or her instincts and forgo the formal proofs. The subtle logical flaws in a design thus get a chance to hide, and inevitably find the worst possible moment in the lifetime of the protocol to reveal themselves. Though few will admit it, most people design protocols by trial and error. There is a known set of trusted protocol standards, whose descriptions are faithfully copied in most textbooks, but there is little understanding of why some designs are correct and why others are not. To design and to analyze protocols you need tools. Until recently the right tools were simply not generally available. But that has changed. In this tutorial we introduce a state-of-the-art tool called SPIN and the specification language PROMELA. Weshow how the language and the tool can be used to design reliable protocols. The tool itself is available by anonymous ftp from research. att. com, or by email from the author.", "num_citations": "69\n", "authors": ["659"]}
{"title": "Validating SDL Specifications: an Experiment.\n", "abstract": " This paper describes a method for validating specifications written in the CCITT language SDL. The method has been implemented as part of an experimental validation system. With the experimental system we have been able to perform exhaustive analyses of systems with over 250 million reachable composite system states. The practicality of the tool for the analysis of substantial portions of AT&T\u2019s 5ESS\u00ae Switch code is now being studied.", "num_citations": "64\n", "authors": ["659"]}
{"title": "Basic spin manual\n", "abstract": " Spin is a tool for analyzing the logical consistency of concurrent systems, specifically of data communication protocols. The system is described in a modeling language called PROMELA. The language allows for the dynamic creation of concurrent processes. Communication via message channels can be defined to be synchronous (ie, rendez\u00b1vous), or asynchronous (ie, buffered).Given a model system specified in PROMELA, spin can either perform random simulations of the system\u2019s execution or it can generate a C program that performs an efficient online verification of the system\u2019s correctness properties. During simulation and verification spin checks for the absence of deadlocks, unspecified receptions, and unexecutable code. The verifier can also be used to verify the correctness of system invariants, it can find non\u00b1progress execution cycles, and it can verify correctness properties expressed in next\u00b1time free linear temporal logic formulae.", "num_citations": "63\n", "authors": ["659"]}
{"title": "The Engineering of a Model Checker: the Gnu i-Protocol Case Study Revisited.\n", "abstract": " In a recent study a series of model checkers, among which Spin [5], SMV [9], and a newer system called XMC [10], were compared on performance. The measurements used for this comparison focused on a model of the i-protocol from GNU uucp version 1.04. Eight versions of this iprotocol model were obtained by varying window size, assumptions about the transmission channel, and the presence or absence of a patch for a known livelock error. The results as published in [1] show the XMC system to outperform the other model checking systems on most of the tests. It also contains a challenge to the builders of the other model checkers to match the results. This paper answers that challenge for the Spin model checker. We show that with either default Spin verification runs, or a reasonable choice of parameter settings, the version of Spin that was used for the tests in [1] (Spin 2.9.7) can outperform the\u00a0\u2026", "num_citations": "62\n", "authors": ["659"]}
{"title": "Swarm verification\n", "abstract": " Reportedly, supercomputer designer Seymour Cray once said that he would sooner use two strong oxen to plow afield than a thousand chickens. Although this is undoubtedly wise when it comes to plowing afield, it is not so clear for other types of tasks. Model checking problems are of the proverbial \"search the needle in a haystack\" type. Such problems can often be parallelized easily. Alas, none of the usual divide and conquer methods can be used to parallelize the working of a model checker. Given that it has become easier than ever to gain access to large numbers of computers to perform even routine tasks it is becoming more and more attractive to find alternate ways to use these resources to speed up model checking tasks. This paper describes one such method, called swarm verification.", "num_citations": "61\n", "authors": ["659"]}
{"title": "From code to models\n", "abstract": " One of the corner stones of formal methods is the notion that abstraction enables analysis. By the construction of an abstract model we can trade implementation detail for analytical power. The intent of a model is to preserve selected characteristics of real-world artifact, while suppressing others. Unfortunately, practitioners are less likely to use a modeling tool if it cannot handle real-world artifacts in their native format. The requirement to build a model to enable analysis is often seen as a verdict to design a system twice: once in a verification language. and once in an implementation language. Because the. implementation phase cannot be skipped, verification is often sacrificed. In this paper we will consider a way to avoid this problem by automating the extraction of verification models from implementation level code. The user now provides only model extraction rules, or abstractions, rather than full-scale models.", "num_citations": "61\n", "authors": ["659"]}
{"title": "The Theory and Practice of A Formal Method: NewCoRe.\n", "abstract": " We discuss what the ideal characteristics of a formal design method should be, and evaluate how the existing methods measure up. We then look at a recent attempt within AT&T to apply formal methods based on design verification techniques, and evaluate it in the same context.", "num_citations": "61\n", "authors": ["659"]}
{"title": "Software model checking with SPIN\n", "abstract": " The aim of this chapter is to give an overview of the theoretical foundation and the practical application of logic model checking techniques for the verification of multi-threaded software (rather than hardware) systems. The treatment is focused on the logic model checker Spin, which was designed for this specific domain of application. Spin implements an automata-theoretic method of verification. Although the tool has been available for over 15 years, it continues to evolve, adopting new optimization strategies from time to time to help it tackle larger verification problems. This chapter explains how the tool works, and which types of software verification problems it is designed to handle.", "num_citations": "58\n", "authors": ["659"]}
{"title": "Protocol design: Redefining the state of the art\n", "abstract": " The application of formal methods to high-level protocol design is addressed. A formal method is considered to be one that has the capability of rendering correctness proofs. The traditional and formal design processes are described and compared. The framework for proving logical correctness in protocol engineering is then discussed.< >", "num_citations": "57\n", "authors": ["659"]}
{"title": "Design tools for requirements engineering\n", "abstract": " Industrial software design projects often begin with a requirements capture and analysis phase. During this phase, the main architectural and behavioral requirements for a new system are collected, documented, and validated. To date, however, requirements engineers have had few reliable tools to guide and support this work. We show that a significant portion of the design requirements can be captured in formalized message sequence charts (MSCs) using a set of tools that we built to reliably create, organize, and analyze such charts.", "num_citations": "55\n", "authors": ["659"]}
{"title": "Beyond Photography\n", "abstract": " BEYOND PHOTOGRAPHY Page 1 BEYOND PHOTOGRAPHY THE DIGITAL DARKROOM Gerard J. Holzmann Page 2 BEYOND PHOTOGRAPHY \u2014 THE DIGITAL DARKROOM \u2014 Gerard J. Holzmann AT&T Bell Laborator ies Murray Hill, New Jersey 07974 PRENTICE-HALL Englewood Cliffs, New Jersey 07632 Page 3 Librar y of Congress Catalog Card Number: 88-5992 Prentice Hall Software Series Br ian W. Ker nighan, Advisor Copyr ight \u00a9 1988 by Bell Telephone Laborator ies, Incor porated. All rights reserved. No par t of this publication may be reproduced, stored in a retrieval system, or transmitted, in any for m or by any means, electronic, mechanical, photocopying, recording, or otherwise, without the prior written permission of the publisher. Pr inted in the United States of America. This book was typeset in Helvetica by the author, using an Autologic APS-5 phototypesetter and a DEC VAX 8550 running the 9th \u2026", "num_citations": "54\n", "authors": ["659"]}
{"title": "Tackling large verification problems with the swarm tool\n", "abstract": " The range of verification problems that can be solved with logic model checking tools has increased significantly in the last few decades. This increase in capability is based on algorithmic advances, but in no small measure it is also made possible by increases in processing speed and main memory sizes on standard desktop systems. For the time being, though, the increase in CPU speeds has mostly ended as chip-makers are redirecting their efforts to the development of multi-core systems. In the coming years we can expect systems with very large memory sizes, and increasing numbers of CPU cores, but with each core running at a relatively low speed. We will discuss the implications of this important trend, and describe how we can leverage these developments with new tools.", "num_citations": "53\n", "authors": ["659"]}
{"title": "Economics of software verification\n", "abstract": " How can we determine the added value of software verification techniques over the more readily available conventional testing techniques? Formal verification techniques introduce both added costs and potential benefits. Can we show objectively when the benefits will outweigh the cost?", "num_citations": "52\n", "authors": ["659"]}
{"title": "A theory for protocol validation\n", "abstract": " This paper introduces a simple algebra for the validation of communication protocols in message passing systems. The behavior of each process participating in a communication is first modeled in a finite state machine. The symbol sequences that can be accepted by these machines are then expressed in \"protocol expressions,\" which are defined as regular expressions extended with two new operators: division and multiplication. The interactions of the machines can be analyzed by combining protocol expressions via multiplication and algebraically manipulating the terms.", "num_citations": "50\n", "authors": ["659"]}
{"title": "Partial Order Methods in Verification: DIMACS Workshop, July 24-26, 1996\n", "abstract": " This book presents surveys on the theory and practice of modeling, specifying, and validating concurrent systems. It contains surveys of techniques used in tools developed for automatic validation of systems. Other papers present recent developments in concurrency theory, logics of programs, model-checking, automata, and formal languages theory. The volume contains the proceedings from the workshop, Partial Order Methods in Verification, which was held in Princeton, NJ, in July 1996. The workshop focused on both the practical and the theoretical aspects of using partial order models, including automata and formal languages, category theory, concurrency theory, logic, process algebra, program semantics, specification and verification, topology, and trace theory. The book also includes a lively e-mail debate that took place about the importance of the partial order dichotomy in modeling concurrency.", "num_citations": "48\n", "authors": ["659"]}
{"title": "Early fault detection tools\n", "abstract": " The traditional software development cycle relies mostly on informal methods to capture design errors in its initial phases, and on more rigorous testing methods during the later phases. It is well understood, though, that those bugs that slip through the early design phases tend to cause the most damage to a design. The anomaly of traditional design is therefore that it excels at catching bugs at the worst possible point in a design cycle: at the end.             In this paper we consider what it would take to develop a suite of tools that has the opposite characteristic: excelling at catching bugs at the start, rather than the end of the design cycle. Such early fault detection tools differ from standard formal verification techniques in the sense that they must be able to deal with incomplete, informal design specifications, with possibly ill-defined requirements. They do not aim to replace either testing or formal verification\u00a0\u2026", "num_citations": "48\n", "authors": ["659"]}
{"title": "Method and apparatus for automatically extracting verification models\n", "abstract": " A verification system for verifying that a software system satisfies a property by extracting a verification model from implementation level source code. The extraction proceeds by translating source strings generated from the code subject to an externally-defined abstraction filter (a so-called conversion table). Standard logic model checking is performed on the extracted verification model to check that the system satisfies (or can possibly violate) any given explicit or implicit logic system property.", "num_citations": "46\n", "authors": ["659"]}
{"title": "Software analysis and model checking\n", "abstract": " Most software developers today rely on only a small number of techniques to check their code for defects: peer review, code walkthroughs, and testing. Despite a rich literature on these subjects, the results often leave much to be desired. The current software testing process consumes a significant fraction of the overall resources in industrial software development, yet it cannot promise zero-defect code. There is reason to hope that the process can be improved. A range of tools and techniques has become available in the last few years that can asses the quality of code with considerably more rigor than before, and often also with more ease. Many of the new tools can be understood as applications of automata theory, and can readily be combined with logic model checking techniques.", "num_citations": "45\n", "authors": ["659"]}
{"title": "Method and apparatus for testing event driven software\n", "abstract": " A technique for testing event driven software. In accordance with the technique, the source code of the event driven software is directly converted to an automation based model useful in verifying that the program code complies with the desired properties defined by the user. More particularly, the event driven system program code is translated into a target language for a particular model checker. Such a translation results in a model which contains statements directed at whether execution of the program code will affect the behavior of the event driven system. Thus, this model extraction process can be used as input to a logic model checker for determining whether event driven system complies with the desired correctness properties specified by the user. Advantageously, the model extraction process and application of the model checker occurs in a direct and dynamic fashion from the subject event driven system\u00a0\u2026", "num_citations": "45\n", "authors": ["659"]}
{"title": "v-Promela: A visual, object-oriented language for Spin\n", "abstract": " Describes the design of VIP (Visual Interface for Promela), a graphical front-end to the model checker SPIN. VIP supports a visual formalism, called v-Promela, that connects the model checker to modern hierarchical notations for the specification of object-oriented, reactive systems. The formalism is comparable to formalisms such as UML-RT (Unified Modeling Language for Real-Time systems), ROOM (Real-time Object-Oriented Modeling) and Statecharts, but is presented in this paper in a framework that allows us to combine the benefits of a visual, hierarchical specification method with the power of LTL (linear temporal logic) model checking provided by SPIN. Like comparable formalisms, VIP can describe hierarchies of behaviour and of system structure. The formalism is designed to be transparent to the SPIN model checker itself, by allowing all central constructs to be translated mechanically into basic Promela\u00a0\u2026", "num_citations": "43\n", "authors": ["659"]}
{"title": "On-the-fly model checking with partial-order state space reduction\n", "abstract": " An on-the-fly verification system which employs statically-available information to reduce the size of the state space required to verify liveness and safety properties of a target system consisting of asynchronous communicating processes. The verification system generates a verifier from a description of the target system and a specification of the property to be verified. The verifier models the target system as a set of finite state machines, constructs a state space containing a graph of nodes representing states of the target system and transitions between the states, and uses the state space to verify the property. The size of the state space is reduced by using information from the description and the specification to divide transitions from a node into per-process bundles and to determine which bundles of transitions must be included in the state space and which may be left out of the state space. The state space\u00a0\u2026", "num_citations": "43\n", "authors": ["659"]}
{"title": "Landing a spacecraft on Mars\n", "abstract": " How much software does it take to land a spacecraft safely on Mars, and how do you make all that code reliable? This column describes such a software development process. The first Web extra at http://mars.jpl.nasa.gov/multimedia/videos/movies/miam20121218/miam20121218-1280.mov is a 60-second video from NASA's Jet Propulsion Laboratory that shows how NASA's Mars rover drivers operate the vehicles from millions of miles away. The second Web extra at http://mars.jpl.nasa.gov/multimedia/videos/movies/msl20120827_curiositycommunicates/msl20120827_curiositycommunicates-1280.mov is an animated video showing how NASA's Curiosity rover communicates with Earth via two of NASA's Mars orbiters, Mars Reconnaissance Orbiter (MRO) and Odyssey, and the European Space Agency's Mars Express. The third Web extra at http://mars.jpl.nasa.gov/multimedia/videos/movies/CoM20121207\u00a0\u2026", "num_citations": "42\n", "authors": ["659"]}
{"title": "Improving spin\u2019s partial-order reduction for breadth-first search\n", "abstract": " We describe an improvement of the partial-order reduction algorithm for breadth-first search which was introduced in Spin version 4.0. Our improvement is based on the algorithm by Alur et al. for symbolic state model checking for local safety properties [1]. The crux of the improvement is an optimization in the context of explicit state model checking of the condition that prevents action ignoring, also known as the cycle proviso. There is an interesting duality between the cycle provisos for the breadth-first search (BFS) and depth first search (DFS) exploration of the state space, which is reflected in the role of the BFS queue and the DFS stack, respectively. The improved version of the algorithm is supported in the current version of Spin and can be shown to perform significantly better than the initial version.", "num_citations": "41\n", "authors": ["659"]}
{"title": "Establishing flight software reliability: Testing, model checking, constraint-solving, monitoring and learning\n", "abstract": " In this paper we discuss the application of a range of techniques to the verification of mission-critical flight software at NASA\u2019s Jet Propulsion Laboratory. For this type of application we want to achieve a higher level of confidence than can be achieved through standard software testing. Unfortunately, given the current state of the art, especially when efforts are constrained by the tight deadlines and resource limitations of a flight project, it is not feasible to produce a rigorous formal proof of correctness of even a well-specified stand-alone module such as a file system (much less more tightly coupled or difficult-to-specify modules). This means that we must look for a practical alternative in the area between traditional testing and proof, as we attempt to optimize rigor and coverage. The approaches we describe here are based on testing, model checking, constraint-solving, monitoring, and finite-state machine\u00a0\u2026", "num_citations": "40\n", "authors": ["659"]}
{"title": "Automatic generation and regeneration of a covering test case set from a model\n", "abstract": " A method and apparatus for generating a covering set of test cases from a directed graph is provided. The directed graph includes nodes and edges connecting the nodes, and a test case is a path through the directed graph. To generate a partial set of test cases, a set of selected test cases is received. These test cases can be manually selected or they can be a maintained test case set. The edges or nodes on the directed graph (or requirements linked to nodes or edges) that are covered by the selected test cases are marked with an identifier. Test cases are then generated from the directed graph according to a coverage algorithm. Marked graph elements may, but need not, be included in the generated test cases. The resulting partial test case set together with the selected test cases satisfy the coverage criterion.", "num_citations": "39\n", "authors": ["659"]}
{"title": "Model driven code checking\n", "abstract": " Model checkers were originally developed to support the formal verification of high-level design models of distributed system designs. Over the years, they have become unmatched in precision and performance in this domain. Research in model checking has meanwhile moved towards methods that allow us to reason also about implementation level artifacts (e.g., software code) directly, instead of hand-crafted representations of those artifacts. This does not mean that there is no longer a place for the use of high-level models, but it does mean that such models are used in a different way today. In the approach that we describe here, high-level models are used to represent the environment for which the code is to be verified, but not the application itself. The code of the application is now executed as is by the model checker, while using powerful forms of abstraction on-the-fly to build the abstract state space\u00a0\u2026", "num_citations": "37\n", "authors": ["659"]}
{"title": "Advanced spin tutorial\n", "abstract": " Spin\u00a0[9] is a model checker for the verification of distributed systems software. The tool is freely distributed, and often described as one of the most widely used verification systems. The Advanced Spin Tutorial is a sequel to [7] and is targeted towards intermediate to advanced Spin users.", "num_citations": "37\n", "authors": ["659"]}
{"title": "Trends in software verification\n", "abstract": " With the steady increase in computational power of general purpose computers, our ability to analyze routine software artifacts is also steadily increasing. As a result, we are witnessing a shift in emphasis from the verification of abstract hand-built models of code, towards the direct verification of implementation level code. This change in emphasis poses a new set of challenges in software verification. We explore some of them in this paper.", "num_citations": "37\n", "authors": ["659"]}
{"title": "SPIN model checking: An introduction\n", "abstract": " A long-standing and elusive problem in software engineering is to devise reliable means that would allow us to check the correctness of distributed systems code mechanically. Writing reliable distributed code is notoriously difficult; locating the inevitable bugs in such code is therefore important. As is well-known and often repeated, traditional testing methods are of little use in this context, because the most pernicious bugs typically depend on subtle race conditions that produce peculiar and unexpected interleavings of events. Well-known are the deadlock and starvation problems that plagued the designers of the first distributed systems code in the 1960s and 1970s (see for instance [16] p. 155). In simple cases, a small set of strictly enforced rules can preserve the sanity in a system. One such rule is the requirement that frequently used resources in an operating system can only be allocated in a fixed order, to prevent circular waiting. But the simple rules only cover the known problems. Each new system builds a new context, with its own peculiarities and hazards. This is illustrated by the well-publicized description of the hangup problem in the control software of the Mars Pathfinder a few years ago [11]. In retrospect, the hangup scenario could be understood in very simple terms, yet it was missed even in the long and unusually thorough (but traditional) software testing process that had been used.", "num_citations": "37\n", "authors": ["659"]}
{"title": "Conquering complexity\n", "abstract": " In safety-critical systems, the potential impact of each separate failure is normally studied in detail and remedied by adding backups. Failure combinations, though, are rarely studied exhaustively; there are just too many of them, and most have a low probability of occurrence. Defect detection in software development is usually understood to be a best effort at rigorous testing just before deployment. But defects can be introduced in all phases of software design, not just in the final coding phase. Defect detection therefore shouldn't be limited to the end of the process, but practiced from the very beginning. In a rigorous model-based engineering process, each phase is based on the construction of verifiable models that capture the main decisions.", "num_citations": "36\n", "authors": ["659"]}
{"title": "Multi-core model checking with SPIN\n", "abstract": " We present the first experimental results on the implementation of a multi-core model checking algorithm for the SPIN model checker. These algorithms specifically target shared-memory systems, and are initially restricted to dual-core systems. The extensions we have made require only small changes in the SPIN source code, and preserve virtually all existing verification modes and optimization techniques supported by SPIN, including the verification of both safety and liveness properties and the verification of SPIN models with embedded C code fragments.", "num_citations": "36\n", "authors": ["659"]}
{"title": "Method and apparatus for automatic verification of properties of a concurrent software system\n", "abstract": " A verification system for verifying that a software system satisfies a property by extracting a verification model from implementation level source code is provided. The extraction proceeds by translating source strings generated from the source code into strings belonging to a target language, and generating a verification model in the target language. The translation is guided by explicit mappings in a lookup table, optional user-defined data restrictions, and default type rules. Standard logic model checking is performed on the extracted verification model to check that the system satisfies (or can possibly violate) any given explicit or implicit logic system property.", "num_citations": "33\n", "authors": ["659"]}
{"title": "On-the-fly model checking\n", "abstract": " Significant advances have been made since people first started building automated verification tools in the mid-seventies. This note briefly summarizes some of the steps that were taken, and considers what we need to do to ensure that the tools we have today can become a permanent part of the software designer's toolkit.", "num_citations": "33\n", "authors": ["659"]}
{"title": "Designing executable abstractions\n", "abstract": " It is well-known that in general the problem of deciding whether a program halts (or can deadlock) is undecidable. Model checkers, therefore, cannot be applied to arbitrary programs, but work with well-defined abstractions of programs. The feasibility of a verification often depends on the type of abstraction that is made. Abstraction is indeed the most powerful tool that the user of a model checking tool can apply, yet it is often perceived as a temporary inconvenience.", "num_citations": "32\n", "authors": ["659"]}
{"title": "A stack-slicing algorithm for multi-core model checking\n", "abstract": " The broad availability of multi-core chips on standard desktop PCs provides strong motivation for the development of new algorithms for logic model checkers that can take advantage of the additional processing power. With a steady increase in the number of available processing cores, we would like the performance of a model checker to increase as well \u2013 ideally linearly. The new trend implies a change of focus away from cluster computers towards shared memory systems. In this paper we discuss the multi-core algorithms that are in development for the SPIN model checker.", "num_citations": "31\n", "authors": ["659"]}
{"title": "Practical methods for the formal validation of SDL specifications\n", "abstract": " Formal design and validation methods have achieved most of their successes on problems of a relatively modest size, involving no more than one or two designers and no more than a few hundred lines of code. The serious application of formal methods to larger software development projects remains a formidable challenge. In this paper we report on some initial experience with the application of a formal validation system to SDL design projects involving more than ten people, producing tens of thousands of lines of high-level code over several years. The problems encountered here are large enough for most formal methods to break down, for both technical and nontechnical reasons.", "num_citations": "30\n", "authors": ["659"]}
{"title": "Graphics image editor\n", "abstract": " An interactive image editor. Editing commands are interactively imputted to a computer by a user to form an image transformation function. The commands define how to alter the pixels of the image and the portions of the image to alter. The editor parses the commands and generates a program for performing the image transformation. The program is then executed, either by interpreting it or by first compiling it on-the-fly. In either case, each affected pixel of the image is transformed in accordance with the command statements.", "num_citations": "29\n", "authors": ["659"]}
{"title": "Tutorial: Proving properties of concurrent systems with SPIN\n", "abstract": " SPIN is an on-the-fly model checking system for finite state system. s, that is optimized for the verification of linear time temporal logic (LTL) properties? SPIN's input language, PROMELA, can be used to specify concurrent systems with dynamically changing numbers of interacting processes, where process interactions can be either synchronous (rendez-vous) or asynchronous (buffered). In the tutorial we will examine some of the algorithm.~ that determine SPIN's functionality and performance. After a brief summary of the automata theoretic foundation of SPIN, we consider the methodology for LTL model checking, the recognition of B chi acceptance conditions, cycle detection, and the handling Of very large verification problems.Automata Theoretic Framework The semantics of PROMEI.~ is defined in terms of a standard labeled transition system. The language is founded on the notion of executability. A first rule\u00a0\u2026", "num_citations": "28\n", "authors": ["659"]}
{"title": "SCRUB: a tool for code reviews\n", "abstract": " This paper describes a tool called Source Code Review User Browser (SCRUB) that was developed to support a more effective and tool-based code review process. The tool was designed to support a large team-based software development effort of mission critical software at JPL, but can also be used for individual software development on small projects. The tool combines classic peer code review with machine-generated analyses from a customizable range of source code analyzers. All reports, whether generated by humans or by background tools, are accessed through a single uniform interface provided by SCRUB.", "num_citations": "21\n", "authors": ["659"]}
{"title": "Model checking with bounded context switching\n", "abstract": " We discuss the implementation of a bounded context switching algorithm in the Spin model checker. The algorithm allows us to find counter-examples that are often simpler to understand, and that may be more likely to occur in practice. We discuss extensions of the algorithm that allow us to use this new algorithm in combination with most other search modes supported in Spin, including partial order reduction and bitstate hashing. We show that, other than often assumed, the enforcement of a bounded context switching discipline does not decrease but increases the complexity of the model checking procedure. We discuss the performance of the algorithm on a range of applications.", "num_citations": "20\n", "authors": ["659"]}
{"title": "The SPIN Verification System: The Second Workshop on the SPIN Verification System: Proceedings of a DIMACS Workshop, August 5, 1996\n", "abstract": " What is Spin? Spin is a general tool for the specification and formal verification of software for distributed systems. It has been used to detect design errors in a wide range of applications, such as abstract distributed algorithms, data communications protocols, operating systems code, and telephone switching code. The verifier can check for basic correctness properties, such as absence of deadlock and race conditions, logical completeness, or unwarranted assumptions about the relative speeds of correctness properties expressed in the syntax of Linear-time Temporal Logic (LTL). The tool translates LTL formulae automatically into automata representations, which can be used in an efficient on-the-fly verifications procedure. This DIMACS volume presents the papers contributed to the second international workshop that was held on the Spin verification system at Rutgers University in August 1996. The work covers theoretical and foundational studies of formal verification, empirical studies of the effectiveness of different types of algorithms, significant practical applications of the Spin verifier, and discussions of extensions and revisions of the basic code.", "num_citations": "19\n", "authors": ["659"]}
{"title": "Outline for an operational semantics of Promela.\n", "abstract": " PROMELA is a high-level specification language for modeling interactions in distributed systems, and for expressing logical correctness requirements about such interactions. The model checker SPIN accepts specifications written in this language, and it can produce automated proofs for each type of property. SPIN either proves that a property is valid in the given system, or it generates a counter-example that shows that it is not. This paper contains the outline for an operationalsemantics definition of PROMELA.", "num_citations": "19\n", "authors": ["659"]}
{"title": "Process sleep and wakeup on a shared-memory multiprocessor\n", "abstract": " The problem of enabling a sleeping process on a sharedVmemory multiprocessor is a difficult one, especially if the process is to be awak ened by an interruptVtime event. We present here the code for sleep and wakeup primitives that we use in our multiprocessor system. The code has been exercised by years of active use and by a verification system.Our problem is to synchronise processes on a symmetric sharedVmemory multiproces sor. Processes suspend execution, or sleep, while awaiting an enabling event such as an I/O interrupt. When the event occurs, the process is issued a wakeup to resume its exe cution. During these events, other processes may be running and other interrupts occurring on other processors.", "num_citations": "19\n", "authors": ["659"]}
{"title": "A multi-paradigm language for reactive synthesis\n", "abstract": " This paper proposes a language for describing reactive synthesis problems that integrates imperative and declarative elements. The semantics is defined in terms of two-player turn-based infinite games with full information. Currently, synthesis tools accept linear temporal logic (LTL) as input, but this description is less structured and does not facilitate the expression of sequential constraints. This motivates the use of a structured programming language to specify synthesis problems. Transition systems and guarded commands serve as imperative constructs, expressed in a syntax based on that of the modeling language Promela. The syntax allows defining which player controls data and control flow, and separating a program into assumptions and guarantees. These notions are necessary for input to game solvers. The integration of imperative and declarative paradigms allows using the paradigm that is most appropriate for expressing each requirement. The declarative part is expressed in the LTL fragment of generalized reactivity(1), which admits efficient synthesis algorithms, extended with past LTL. The implementation translates Promela to input for the Slugs synthesizer and is written in Python. The AMBA AHB bus case study is revisited and synthesized efficiently, identifying the need to reorder binary decision diagrams during strategy construction, in order to prevent the exponential blowup observed in previous work.", "num_citations": "18\n", "authors": ["659"]}
{"title": "Proving the value of formal methods\n", "abstract": " The record of successful applications of formal verification techniques is slowly growing. Our ultimate aim, however, is not to perform small pilot projects that show that verification is sometimes feasible in an industrial setting; our aim must be to integrate verification techniques into the software design cycle as a non-negotiable part of quality control.", "num_citations": "18\n", "authors": ["659"]}
{"title": "Software model checking\n", "abstract": " In thes notes we will review the automata-theoretic verification method and propositional linear temporal logic, with specific emphasis on their potential application to distributed software verification. An important issue in software verification is the establishment of a formal relation between the concrete, implementation-level, software application and the abstract, derived, automata-model that is the subject of the actual verification. In principle one can either attempt to derive an implementation from a verified abstract model, using refinement techniques, or one can attempt to derive a verification model from an implementation, using systematic abstraction techniques. The former method has long been advocated, but has not received much attention in industrial practice. The latter method, deriving abstract models from concrete implementations guided by explicitly stated correctness requirements, has recently begun to show consider-able promise. We will discuss it in detail.", "num_citations": "17\n", "authors": ["659"]}
{"title": "Formal methods after 15 years: Status and trends a paper based on contributions of the panelists at the FORmal TEchnique'95 Conference, Montreal, october 1995\n", "abstract": " The FORTE 1995 conference was held in Montreal, organized by Gregor von Bochmann, Rachida Dssouli, and Omar Rafiq. One of the activities that took place was a panel presentation and discussion among the coauthors of this paper. This exercise lasted about two hours and elicited several remarkable contributions from the panelists and the audience. It was thought worthwhile to record the panelists\u2019 contributions in the form of a joint paper, this one.", "num_citations": "17\n", "authors": ["659"]}
{"title": "Tutorial: Design and validation of protocols\n", "abstract": " It can be remarkably hard to design a good communications protocol, much harder even than it is to write a normal sequential program. Unfortunately, when the design of a new protocol is complete, we usually have little trouble convincing ourselves that it is trivially correct. It can be a unreasonably hard to prove those facts formally and to convince also others. Faced with that dilemma, a designer usually decides to trust his or her instincts and forgo the formal proofs. The subtle logical flaws in a design thus get a chance to hide, and inevitably find the worst possible moment in the lifetime of the protocol to reveal themselves.Though few will admit it, most people design protocols by trial and error. There is a known set of trusted protocol standards, whose descriptions are faithfully copied in most textbooks, but there is little understanding of why some designs are correct and why others are not. To design and to analyze protocols you need tools. Until recently the right tools were simply not generally available. But that has changed. In this tutorial we introduce a state-of-the-art tool called SPIN and a specification language called PROMELA, and we show how these can be used to design reliable protocols.", "num_citations": "17\n", "authors": ["659"]}
{"title": "The Pandora System: an interactive system for the design of data communication protocols\n", "abstract": " PANDORA is an interactive system for the analysis, synthesis, and real-time assessment of data communication protocols. The Pandora system is being developed at the Delft University of Technology in cooperation with the Dr. Neher Laboratories of the Netherlands PTT. This paper gives an overview of the structure of the system and discusses the main design goals.", "num_citations": "17\n", "authors": ["659"]}
{"title": "PAN: a protocol specification analyzer\n", "abstract": " Pan is a program that can analyze the consistency of protocol specification for up to ten interacting processes. The validation method is based on a special algebra for an extended type of regular expressions, named protocol expressions. A protocol specification is written in a CSP-like language that includes concatenation, selection, and looping (but no variables). Pan analyzes an average protocol faster than troff prepares an average paper. Unlike troff though, the execution time of pan is more sensitive to the quality than to the size of its victim.", "num_citations": "17\n", "authors": ["659"]}
{"title": "Code Inflation.\n", "abstract": " If you compare the state of software development tools today with those that were used in, say, the sixties, you do of course see many signs of improvement. Compilers are faster and better, we have powerful new integrated program development environments, and there are quite a few effective static source code analysis and logic model checking tools around today that help us catch bugs. All this is true, and it would have made a fabulous difference if our software applications still looked like they did in the sixties. But they don\u2019t.Many of my colleagues at JPL are astronomers or cosmologists. To explain how rapidly things are changing in software development I have often been tempted to make an analogy with their field. One of the first things you learn about in cosmology is the theory of inflation. The details of this theory don\u2019t matter too much here, but in a nutshell it postulates that the universe started expanding exponentially fast in the first few moments after the Big Bang, and continues to expand even today. The parallel with software development is easily made.", "num_citations": "16\n", "authors": ["659"]}
{"title": "Designing bug-free protocols with SPIN\n", "abstract": " SPIN is an efficient, automated verification tool that can be used to design robust software for distributed systems in general, and bug-free communications protocols in particular. This paper outlines the use of the tool to address protocol design problems. As an example we consider the verification of a published protocol for implementing synchronous rendezvous operations in a distributed system. We also briefly review some of the techniques that SPIN employs to address the computational complexity of larger verification problems.", "num_citations": "16\n", "authors": ["659"]}
{"title": "The first data networks\n", "abstract": " GERARD J. HOLZMANN and BJ\u00d6RN PEHRSON studied the history of telecommunications methods independently for many years. They discovered their common interest in 1989 and have collaborated since then. Holzmann works at AT&T Bell Laboratories in Murray Hill, NJ, where he does research in the design and verification of communications protocols, distributed computing and computer graphics. He received his Ph. D. from the University of Technology in Delft, the Netherlands, in 1979. Pehrson is chair of the department of teleinformatics at the Royal Institute of Technology in Stockholm and a member of the board of the Swedish Institute of Computer Science. He received his Ph. D. from Uppsala University in 1975.SEMAPHORE TOWER at Marcy-sur-Anse (opposite page) was built about 1804 as part of a line extending from Paris to Lyons. The tower and its semaphore have recently been reconstructed\u00a0\u2026", "num_citations": "16\n", "authors": ["659"]}
{"title": "An improvement of the piggyback algorithm for parallel model checking\n", "abstract": " This paper extends the piggyback algorithm to enlarge the set of liveness properties it can verify. Its extension is motivated by an attempt to express in logic the counterexamples it can detect and relate them to bounded liveness. The original algorithm is based on parallel breadth-first search and piggybacking of accepting states that are deleted after counting a fixed number of transitions. The main improvement is obtained by renewing the counter of transitions when the same accepting states are visited in the negated property automaton. In addition, we describe piggybacking of multiple states in either sets (exact) or Bloom filters (lossy but conservative), and use of local searches that attempt to connect cycles fragmented among processing cores. Finally it is proved that accepting cycle detection is in NC in the size of the product automaton's entire state space, including unreachable states.", "num_citations": "15\n", "authors": ["659"]}
{"title": "New challenges in model checking\n", "abstract": " In the last 25 years, the notion of performing software verification with logic model checking techniques has evolved from intellectual curiosity to accepted technology with significant potential for broad practical application. In this paper we look back at the main steps in this evolution and illustrate how the challenges have changed over the years, as we sharpened our theories and tools. Next we discuss a typical challenge in software verification that we face today \u2013 and that perhaps we can look back on in another 25 years as having inspired the next logical step towards a broader integration of model checking into the software development process.", "num_citations": "15\n", "authors": ["659"]}
{"title": "Model Checking Autonomous Planners: Even the B est L aid P lans M ust be V erified\n", "abstract": " Automated planning systems (APS) are gaining acceptance for use on NASA missions as evidenced by APS flown on missions such as Earth Orbiter 1 and Deep Space 1, both of which were commanded by onboard planning systems. The planning system takes high level goals and expands them onboard into a detailed plan of action that the spacecraft executes. The system must be verified to ensure that the automatically generated plans achieve the goals as expected and do not generate actions that would harm the spacecraft or mission. These systems are typically tested using empirical methods. Formal methods, such as model checking, offer exhaustive or measurable test coverage which leads to much greater confidence in correctness. This paper describes a formal method based on the SPIN model checker. This method guarantees that possible plans meet certain desirable properties. We express the input\u00a0\u2026", "num_citations": "15\n", "authors": ["659"]}
{"title": "Formal methods for early fault detection\n", "abstract": " A traditional formal verification method becomes an effective weapon in the arsenal of a designer only after sufficient insight into a design problem has been developed for a draft solution to be formalized. In the initial phases of a design the designers can therefore perceive formal methods to be more of a hindrance than an assistance. Since formal methods are meant to be problem solving tools, we would like to find ways to make them both effective and attractive from the moment that a design process begins.", "num_citations": "15\n", "authors": ["659"]}
{"title": "Cobra: a light-weight tool for static and dynamic program analysis\n", "abstract": " Static source code analysis tools have become indispensable for the development of reliable software applications. The best analyzers can reveal subtle flaws in a code base, but they can also be slow. In part this is due to the collection of detailed information about the possible data and control flow of an application to support the broadest possible range of analyses. For larger code bases it is not unusual that even the best of breed static analyzers can take an hour or more to complete an analysis. In this paper we describe a framework for a much faster, but more light-weight type of static analysis that can support interactive use for standard types of queries. The Cobra tool we designed for this purpose can scale to explore millions of lines of code interactively. The tool is mostly language agnostic, and can therefore easily be configured to resolve even dynamic program analysis queries.", "num_citations": "13\n", "authors": ["659"]}
{"title": "Cloud-based verification of concurrent software\n", "abstract": " Logic model checkers are unparalleled in their ability to reveal subtle bugs in multi-threaded software systems. The underlying verification procedure is based on a systematic search of potentially faulty system behaviors, which can be computationally expensive for larger problem sizes. In this paper we consider if it is possible to significantly reduce the runtime requirements of a verification with cloud computing techniques. We explore the use of large numbers of CPU-cores, that each perform small, fast, independent, and randomly different searches to achieve the same problem coverage as a much slower stand-alone run on a single CPU. We present empirical results to demonstrate what is achievable.", "num_citations": "13\n", "authors": ["659"]}
{"title": "Effective bug hunting with spin and modex\n", "abstract": " This tutorial consists of two parts. In the first part we present an advanced overview of Spin [1][4], and illustrate its practical application to logic model checking problems. In the second part of the tutorial we present an overview of a related tool called Modex [2,3]. Modex can be used to extract Spin verification models directly from C source code. It supports the definition of user-defined abstractions, and cleverly exploits the capability in Spin version 4 to include embedded C code inside abstract verification models. We will show how to use Spin and Modex, separately and combined, in an effective way when searching for design errors in distributed software applications. Both Spin and Modex are written in ANSI-C and can freely be used on research projects.", "num_citations": "13\n", "authors": ["659"]}
{"title": "Pico\u2014a picture editor\n", "abstract": " PICO is an interactive editor for digitized graphic images. Editing operations are defined in a simple expression language based on the C language. The editor treats images as an ordered set of pixel structures stored in two\u2010dimensional arrays. PICO checks editing commands for syntax, translates them into programs, optimizes and then executes them, all within a few seconds of run time. The command structure is similar to that of conventional multifile text editors with options for reading, writing, and transforming digitized images.", "num_citations": "13\n", "authors": ["659"]}
{"title": "Backward Symbolic Execution of Protocols.\n", "abstract": " A traditional method to validate protocols by state space exploration is to use forward symbolic execution. One of the main problems of this approach is that to find all undesirable system states one has to generate all reachable states and evaluate all desirable system states as well. The paper discusses an alternative search strategy based on backward symbolic execution. This time we start with a state that we know to be undesirable and execute the protocol backwards, evaluating only undesirable states in an effort to show that they are unreachable.", "num_citations": "13\n", "authors": ["659"]}
{"title": "Message sequence chart analyzer\n", "abstract": " Apparatus and methods for editing message sequence charts and determining whether a message sequence chart is consistent with a semantic of the system which the message sequence chart represents. As an editor, the apparatus maintains an internal representation of the message sequence chart as a set of processes and events, displays an image of the message sequence chart, and modifies the internal representation in response to modifications of the image by the user. The internal representation can be used to produce further representations of the message sequence chart. One of the representations is an event list which lists send events and receive events in the message sequence chart in a visual order. The event list is used together with a semantic provided by the user of the apparatus to determine whether there is an inconsistency between the message sequence chart and the semantic.", "num_citations": "11\n", "authors": ["659"]}
{"title": "Data Communications: The First 2500 Years.\n", "abstract": " It is usually assumed that data\u2212 networks are a 20th Century phenomenon. Evidence of efforts to build data communications systems, however, can be found throughout history. Before the electrical telegraph was introduced, many countries in Europe already had fully operational nationwide data\u2212 networks in place. We briefly recount the long history of pre\u2212 electric communication methods and devices.", "num_citations": "11\n", "authors": ["659"]}
{"title": "Explicit-state model checking\n", "abstract": " In this chapter we discuss the methodology used in explicit-state logic model checking, specifically as applied to asynchronous software systems. As the name indicates, in an explicit-state model checker the state descriptor for a system is maintained in explicit, and not symbolic, form, as are all state transitions. Abstraction techniques and partial-order reduction algorithms are used to reduce the search space to a minimum, and advanced storage techniques can be used to extend the reach of this form of verification to very large system sizes. The basic algorithms for explicit-state model checking date from the late 1970s and early 1980s. More advanced versions of these algorithms remain an active area of research.", "num_citations": "10\n", "authors": ["659"]}
{"title": "Standardized protocol interfaces\n", "abstract": " A traditional protocol implementation typically consists of at least two distinct parts, a sender and a receiver. Each part runs on a distinct machine, with the implementation provided by a local expert. At best, the two machines are of the same type and the protocol implementations are provided by the same person. More likely, however, the machines are not of the same type and the implementations of the two halves of the protocol are provided by two different people, working from an often loosely defined protocol specification. It seems almost unavoidable that the two implementations are not quite compatible. In this paper we consider an alternative technique. With this method, one of the two implementors can design, formally validate, and implement all the relevant protocol parts, including those parts that are to be executed remotely. Each communication channel is now terminated on the receiving side, by a single\u00a0\u2026", "num_citations": "9\n", "authors": ["659"]}
{"title": "Code clarity\n", "abstract": " Naming conventions affect the readability of your code and the ease with which you can find your way around when you're reviewing that code. Naming conventions aren't meant to help the compiler. A compiler has no trouble distinguishing names, no matter how long, short, or obscure they are. But to us humans, they can matter a great deal.", "num_citations": "8\n", "authors": ["659"]}
{"title": "Automated testing of planning models\n", "abstract": " Automated planning systems (APS) are maturing to the point that they have been used in experimental mode on both the NASA Deep Space 1 spacecraft and the NASA Earth Orbiter 1 satellite. One challenge is to improve the test coverage of APS to ensure that no unsafe plans can be generated. Unsafe plans can cause wasted resources or damage to hardware. Model checkers can be used to increase test coverage for large complex distributed systems and to prove the absence of certain types of errors. In this work we have built a generalized tool to convert the input models of an APS to Promela, the modeling language of the Spin model checker. We demonstrate on a mission sized APS input model, that we with Spin can explore a large part of the space of possible plans and verify with high probability the absence of unsafe plans.", "num_citations": "8\n", "authors": ["659"]}
{"title": "Reliable software systems design: Defect prevention, detection, and containment\n", "abstract": " The grand challenge that is the focus of this conference targets the development of a practical methodology for software verification: a methodology that can help us to reduce the number of residual defects in software products. Reducing residual defects is of course not in itself the objective of this exercise; the true objective is to reduce the number of failures in the use of software products. Or in other words: the objective is the development of a methodology for \u201creliable software systems design.\u201d", "num_citations": "8\n", "authors": ["659"]}
{"title": "Out of bounds\n", "abstract": " Writing reliable code means understanding bounds. Only a finite amount of memory is available for computation, only a finite amount of time exists to do it, and every object we store and modify must be finite. Resources are similarly bounded. Stacks are bounded, queues are bounded, file system capacity is bounded, and even numbers are bounded. This makes the world of computer science very different from the world of mathematics, but too few people take this into account when they write code.", "num_citations": "7\n", "authors": ["659"]}
{"title": "Software verification at Bell Labs: one line of development\n", "abstract": " Collectors often greet the first report of a new type of minting error for commonly circulating coins with enthusiasm. A coin is a rare example of an object that can increase, rather than decrease, in value when it is faulty. In software design we are not so fortunate. Software faults are often intriguing, but they rarely increase the value of a product. Since the early days of computers, programmers have sought effective ways to defend against software bugs. Software verification techniques are meant to help the user locate possible defects in a software product reliably and, preferably, mechanically. In this paper we examine a line of research that has led to one of the most widely used verification systems for distributed software today.", "num_citations": "7\n", "authors": ["659"]}
{"title": "The Pandora Protocol Development System.\n", "abstract": " In a joint project with the Netherlands PTT, the Delft University of Technology is developing an interactive protocol design and analysis system named \u2018Pandora\u2019. The system provides users with a controlled environment for protocol synthesis and formal analysis, and offers both software and hardware tools for protocol assessment. Pandora can assist the user in the documentation of protocol designs by autonomously extracting SDL\u2212 diagrams, and has a set of tools for the generation of executable protocol implementations from abstract specifications.", "num_citations": "7\n", "authors": ["659"]}
{"title": "Proving properties of concurrent programs\n", "abstract": " How do you prove the correctness of multi-threaded code? This question has been asked since at least the mid-sixties, and it has inspired researchers ever since. Many approaches have been tried, based on mathematical theories, the use of annotations, or the construction of abstractions. An ideal solution would be a tool that one can point at an arbitrary piece of concurrent code, and that can resolve correctness queries in real-time. We describe one possible method for achieving this capability with a logic model checker.", "num_citations": "6\n", "authors": ["659"]}
{"title": "Automatic generation of invariants in SPIN\n", "abstract": " Spin takes a model to be verified and a property, and outputs true if the model satisfies the property or otherwise false, with a counterexample. In the first case, the user has no indication about why the property is satisfied. It may be the case that the model does not really represent what the user meant to express, and the property is void. It can therefore be useful if the tool could provide some additional information about the behavior of a model, regardless of a property's validity.A way of providing such information is to have Spin discover invariants of the model automatically. In this paper we focus on generating invariants of the form a rel b, where a and b are integer variables, and rel is an ordering relation. We give an algorithm with a matching implementation in Spin, including a graphical user interface.", "num_citations": "6\n", "authors": ["659"]}
{"title": "On checking model checkers\n", "abstract": " It has become good practice to expect authors of new model checking algorithms to provide not only rigorous evidence of the algorithms correctness, but also evidence of their practical significance. Though the rules for determining what is and what is not a good proof of correctness are clear, no comparable rules are usually enforced for determining the soundness of the data that is used to support the claim for practical significance. We consider here how we can flag the more common types of omission.", "num_citations": "6\n", "authors": ["659"]}
{"title": "Debate'90: An electronic discussion on true concurrency\n", "abstract": " Debate'90 | Proceedings of the DIMACS workshop on Partial order methods in verification ACM Digital Library home ACM home Google, Inc. (search) Advanced Search Browse About Sign in Register Advanced Search Journals Magazines Proceedings Books SIGs Conferences People More Search ACM Digital Library SearchSearch Advanced Search Browse Browse Digital Library Collections More HomeBrowse by TitleProceedingsPOMIV '96Debate'90: an electronic discussion on true concurrency ARTICLE Debate'90: an electronic discussion on true concurrency Share on Editors: Vaughan Ronald Pratt profile image Vaughan Pratt View Profile , Doron A Peled profile image Doron A. Peled View Profile , GJ Holzmann profile image Gerard J. Holzmann View Profile Authors Info & Affiliations Publication: POMIV '96: Proceedings of the DIMACS workshop on Partial order methods in verificationFebruary 1997 Pages \u2026", "num_citations": "6\n", "authors": ["659"]}
{"title": "Cobra: fast structural code checking (keynote)\n", "abstract": " In software analysis most research has traditionally been focused on the development of tools and techniques that can be used to formally prove key correctness properties of a software design. Design errors can be hard to catch without the right tools, and even after decades of development, the right tools can still be frustratingly hard to use.", "num_citations": "5\n", "authors": ["659"]}
{"title": "The value of doubt\n", "abstract": " Doubt is key to becoming a good programmer. If you don't doubt the correctness of your work, you have no incentive to look for the hidden spoilers that are always there.", "num_citations": "5\n", "authors": ["659"]}
{"title": "Logic model checking of time-periodic real-time systems\n", "abstract": " In this paper we report on the work we performed to extend the logic model checker SPIN with builtin support for the verification of periodic, real-time embedded software systems, as commonly used in aircraft, automobiles, and spacecraft. We first extended the SPIN verification algorithms to model priority based scheduling policies. Next, we added a library to support the modeling of periodic tasks. This library was used in a recent application of the SPIN model checker to verify the engine control software of an automobile, to study the feasibility of software triggers for unintended acceleration events.", "num_citations": "5\n", "authors": ["659"]}
{"title": "Software certification: coding, code, and coders\n", "abstract": " We describe a certification approach for software development that has been adopted at our organization. JPL develops robotic spacecraft for the exploration of the solar system. The flight software that controls these spacecraft is considered to be mission critical. We argue that the goal of a software certification process cannot be the development of\" perfect\" software, ie, software that can be formally proven to be correct under all imaginable and unimaginable circumstances. More realistically, the goal is to guarantee a software development process that is conducted by knowledgeable engineers, who follow generally accepted procedures to control known risks, while meeting agreed upon standards of workmanship. We target three specific issues that must be addressed in such a certification procedure: the coding process, the code that is developed, and the skills of the coders. The coding process is driven by\u00a0\u2026", "num_citations": "5\n", "authors": ["659"]}
{"title": "Method and apparatus for deriving image width and height from raw graphical image data\n", "abstract": " The present invention involves a method of deriving the height and width of a graphic image from raw graphical image data stored in a computer system. It includes commencing execution of a height-width derivation program in the computer system and, at least once, operating the height-width derivation program to perform a series of steps. These include calculating the area of the image size, eg using the total number of bytes, and calculating the square root of the area to obtain a first value, and then determining by assumption a first assumed width and a first assumed height. For example, these assumptions may be achieved by calculating one half of the first value to obtain a first assumed width value and dividing the area by the first assumed width value to obtain a first assumed height value. Next, the first assumed height value and first assumed width value are rounded to the nearest whole number, further\u00a0\u2026", "num_citations": "5\n", "authors": ["659"]}
{"title": "Reliable software development: Analysis-aware design\n", "abstract": " The application of formal methods in software development does not have to be an all-or-nothing proposition. Progress can be made with the introduction of relatively unobtrusive techniques that simplify analysis. This approach is meant replace traditional analysis-agnostic coding with an analysis-aware style of software development.", "num_citations": "4\n", "authors": ["659"]}
{"title": "Formal software verification: how close are we?\n", "abstract": " Spin and its immediate predecessors were originally designed for the verification of data communication protocols. It didn\u2019t take long, though, for us to realize that a data communications protocol is just a special case of a general distributed process system, with asynchronously executing and interacting concurrent processes. This covers both multi-threaded software systems with shared memory, and physically distri- buted systems, interacting via network channels.               The tool tries to provide a generic capability to prove (or as the case may be, to disprove) the correctness of interactions in complex software systems. This means a reliable and easy-to-use method to discover the types of things that are virtually impossible to detect reliably with traditional software test methods, such as race conditions and deadlocks.               As initially primarily a research tool, Spin has been remarkably successful, with\u00a0\u2026", "num_citations": "4\n", "authors": ["659"]}
{"title": "Methods and apparatus for generating passive testers from properties\n", "abstract": " Techniques and testers for testing a system U including the steps of (a) defining a formal specification of a logical property P that system U is required not to satisfy;(b) generating a passive testing module T based upon property P to monitor system U;(c) invoking a function F at specific invocation points during the execution of system U to compute an abstract representation of the state of system U at the current point of execution;(d) passing the abstract representation computed by function F to passive testing module T in order to determine whether the abstract representation of the execution of system U to the current point matches illegal property P; and (e) declaring a\" fail\" result if the abstract representation of the execution of system U to the current point matches illegal property P and declaring a\" pass\" result if the abstract representation of the execution of system U to the current point does not match illegal\u00a0\u2026", "num_citations": "4\n", "authors": ["659"]}
{"title": "using SPIN\n", "abstract": " SPIN can be used for proving or disproving properties of concurrent systems. To render the proofs, a concurrent system is first modeled in a formal specification language called PROMELA2. The language allows one to specify the behaviors of asynchronously executing concurrent processes that may interact through synchronous or asynchronous message passing, or through direct access to shared variables.", "num_citations": "4\n", "authors": ["659"]}
{"title": "Algebraic Validation Methods-A Comparison of Three Techniques.\n", "abstract": " Protocol verification methods are commonly subdivided into two main groups. A distinction is then made between state-transition models and program-proving theories, with in between a small group of hybrid techniques [eg 3, 7]. Our aim in this paper is to show that there is yet a third group of verification techniques that is slowly evolving into a main new discipline in its own right. This third group consists of the methods based on special validation algebras for communicating systems. Here we will briefly review three examples of techniques that belong to this emerging class. Though we restrict the discussion to just these three techniques, there are also other validation methods [12, 13, 16], and indeed some specification methods [1, 2, 15] that share important concepts with them.In section 2 we will consider Milner\u2019s calculus for communicating systems, CCS [10, 11]. In section 3 we will discuss the Flow Expression Language introduced by Shaw [14], and finally in section 4 we will review the approach suggested in [4, 5, 6]. Section 5 summarizes our findings.", "num_citations": "4\n", "authors": ["659"]}
{"title": "Software Components.\n", "abstract": " AT THE 1968 Conference on Software Engineering, mathematician and software engineer Doug McIlroy, alarmed by the sorry state of software development, made a strong pitch for the industrial production of software components. 1 Software systems, like bridges, houses, and cars, are built from parts. McIlroy noted that it didn\u2019t make much sense for every organization and developer to keep having to reinvent what\u2019s basically a common set of core components for software design. McIlroy envisioned an industry that could provide programmers a selection of mass-produced software parts, differing in accuracy, performance, and cost, to fit a broad range of possible applications.An inspiration for McIlroy\u2019s presentation was the way in which the electronics industry had evolved. Electronics were commonly designed as sets of circuit boards populated with standardized components. There were, and still are, catalogs of resistors, capacitors, diodes, and transistors, with each item documented and marked with the intended range of use. For instance, resistors are marked with a standard color code that indicates their nominal value and the percentage by which their actual value can differ from that nominal value. The user can then make the tradeoff between paying somewhat more for greater precision or less when the highest level of", "num_citations": "3\n", "authors": ["659"]}
{"title": "Points of truth\n", "abstract": " The SPOT (Single Point of Truth) principle says that developers should specify key pieces of information in one and only one place in their code. Unfortunately, they frequently violate this principle.", "num_citations": "3\n", "authors": ["659"]}
{"title": "Plan 9: The early papers\n", "abstract": " This report reprints half a dozen early but still current papers on Plan 9 from Bell Labs, a distributed computing system being developed at the Computing Science Research Center of AT&T Bell Laboratories.", "num_citations": "3\n", "authors": ["659"]}
{"title": "Trace\u2013performance measurements\n", "abstract": " Tr ace is a protocol validation program that can locate design errors in data communication protocols by performing a reduced symbolic execution of a finite state machine model described in a higher level language Argos. This memo documents the results of measurements of the effect of different search methods, search depth restrictions, channel sizes, cache sizes, and caching disciplines on the performance of the validater.", "num_citations": "3\n", "authors": ["659"]}
{"title": "Tiny tools\n", "abstract": " Gerard Holzmann offers simple tools for developers who don't use IDEs but prefer to write code using their own screen editor and who do everything else with command-line tools.", "num_citations": "2\n", "authors": ["659"]}
{"title": "Assertive Testing [Reliable Code]\n", "abstract": " Standard software testing might not catch important defects, and formal methods can be difficult to use. But, there's a middle ground between the two. This middle ground involves adding five steps to standard testing and employing test randomization, model-based testing, and a more aggressive use of assertions (also called self-tests).", "num_citations": "2\n", "authors": ["659"]}
{"title": "Synthesis from multi-paradigm specifications\n", "abstract": " This work proposes a language for describing reactive synthesis problems that integrates imperative and declarative elements. The semantics is defined in terms of two-player turn-based infinite games with full information. Currently, synthesis tools accept linear temporal logic (LTL) as input, but this description is less structured and does not facilitate the expression of sequential constraints. This motivates the use of a structured programming language to specify synthesis problems. Transition systems and guarded commands serve as imperative constructs, expressed in a syntax based on that of the modeling language Promela. The syntax allows defining which player controls data and control flow, and separating a program into assumptions and guarantees. These notions are necessary for input to game solvers. The integration of imperative and declarative paradigms allows using the paradigm that is most appropriate for expressing each requirement. The declarative part is expressed in the LTL fragment of generalized reactivity(1), which admits efficient synthesis algorithms. The implementation translates Promela to input for the Slugs synthesizer and is written in Python.", "num_citations": "2\n", "authors": ["659"]}
{"title": "Fault intolerance [reliable code]\n", "abstract": " The author considers what it takes to develop truly reliable software systems, and what the role is of program verification in all this. One problem he focuses on is the difficulty of writing good specifications, particularly in making sure that those specifications are complete. Reality can be surprisingly good in showing that our painfully constructed software design requirements are incomplete or even incorrect.", "num_citations": "2\n", "authors": ["659"]}
{"title": "OOPSLA keynote: Scrub and SPIN: Stealth use of formal methods in software development\n", "abstract": " OOPSLA keynote: Scrub and Spin | Proceedings of the 24th ACM SIGPLAN conference companion on Object oriented programming systems languages and applications ACM Digital Library Logo ACM Logo Google, Inc. (search) Advanced Search Browse About Sign in Register Advanced Search Journals Magazines Proceedings Books SIGs Conferences People More Search ACM Digital Library SearchSearch Advanced Search splash Conference Proceedings Upcoming Events Authors Affiliations Award Winners More HomeConferencesSPLASHProceedingsOOPSLA '09OOPSLA keynote: Scrub and Spin: Stealth Use of Formal Methods in Software Development keynote OOPSLA keynote: Scrub and Spin: Stealth Use of Formal Methods in Software Development Share on Author: Gerard Holzmann profile image Gerard Holzmann JPL Fellow / Adjunct Faculty Caltech CS JPL Fellow / Adjunct Faculty Caltech CS \u2026", "num_citations": "2\n", "authors": ["659"]}
{"title": "Reliable software systems design\n", "abstract": " The grand challenge that is the focus of this conference targets the development of a practical methodology for software verification: a methodology that can help us to reduce the number of residual defects in software products. Reducing residual defects is of course not in itself the objective of this exercise; the true objective is to reduce the number of failures in the use of software products. Or in other words: the objective is the development of a methodology for \u201creliable software systems design.\u201d It has often been argued that with the right training, discipline, and tools it should be possible to produce zero-defect code. Very few things in life, though, are zero-defect\u2013not even the things that can be considered life critical. If you practice sky-diving, you are probably acutely aware that your main parachute could fail to open, no matter how carefully you check it before each jump. The parachutist would also be wise not to\u00a0\u2026", "num_citations": "2\n", "authors": ["659"]}
{"title": "Spin\u2014A protocol analyzer\n", "abstract": " Spin is a tool for analyzing the logical consistency of concurrent systems, specifically of data communication protocols, The system is described in a modeling language cafled PROMELA. he language allows for the dynamic creation of concurrent processes. Communication via fessage channels can be defined to be synchronous (ie, rendez-vous), or asynchronous (ie, buffered).", "num_citations": "2\n", "authors": ["659"]}
{"title": "Manual for the Protocol Analyzer'Trace'\n", "abstract": " Tr ace is a program that can be used to analyze the consistency of data communication protocols. A protocol is specified in the nondeterministic guarded command language Argos that includes case selection, do-loops, variables, expressions, value transfer, procedures, and macros. The analyzer traces deadlocks, unspecified receptions, timing problems, and errors caused by value passing.", "num_citations": "2\n", "authors": ["659"]}
{"title": "Code overload\n", "abstract": " I spend much of my time analyzing code. In most cases that code was written by someone else, but it could just as easily have been written by an evil earlier version of me from many years ago. Today, any nontrivial code base is typically hundreds of thousands of lines of code, and for large companies it often reaches into the megamillions. This makes the code-review process feel like you're exploring caves with myriads of little passages leading nowhere in particular.", "num_citations": "1\n", "authors": ["659"]}
{"title": "Code mining\n", "abstract": " Machine-learning techniques are starting to achieve some impressive feats. The successes are due, in part, to the availability of astonishingly large amounts of data that can be used for training. Show a suitably equipped system a billion images of cats and a billion images of lawn mowers, and then see whether it can figure out what the common patterns are. If this succeeds, we can then show the same system a picture of our own cat or lawn mower, and maybe it can tell which is which. Of course, if you show the same system an image of a house, it wouldn't have a clue beyond saying that it's neither a cat nor a lawn mower. Basically, what we're leveraging are statistics, not intelligence, but, nonetheless, the results can be impressive.", "num_citations": "1\n", "authors": ["659"]}
{"title": "Formalizing Requirements Is $$\\Diamond\\Box $$ Hard\n", "abstract": " The use of formal methods in software engineering requires that the user adopt a suitable level of precision in the description of both design artifacts and the properties that should hold for those artifacts. The level of precision must be sufficiently high that the logical consistency of the design and the logic properties can be verified mechanically.                 The source code of any well-defined program is itself a formal object, although it typically contains more detail than desirable for effective analysis. But, practitioners often have no problem producing or recognizing an abstracted version of the key features of a design, expressed in the modeling language of a verification tool.                 The real problem preventing a broader acceptance of formal methods is that there are no intuitive formalisms that practitioners can use to express logic requirements at the level of precision that is required for formal verification\u00a0\u2026", "num_citations": "1\n", "authors": ["659"]}
{"title": "Code Craft.\n", "abstract": " Which is harder: overcoming the many physical challenges of autonomously landing a spacecraft on the surface of a distant planet, building the many interacting pieces of hardware to execute such a mission, or writing the software that controls the whole process? To me, the first two tasks look impossibly hard. But, if we look at a series of failed attempts to accomplish this feat, it may well be that writing the software correctly is still the harder problem.The recent loss of the Schiaparelli spacecraft from the European Space Agency, now thought to have been caused by a software defect, is a powerful reminder of this fact, though it is certainly not the only one, nor is it likely to be the last. Why is it so hard to write bug-free code? Over half a century ago we hopefully started calling our discipline \u2018software engineering,\u2019but how much do we really have in common with more traditional engineering disciplines?", "num_citations": "1\n", "authors": ["659"]}
{"title": "Brace yourself\n", "abstract": " The author states that to reduce the risk of hard-to-spot coding mistakes, you can do one simple thing: correctly use parentheses.", "num_citations": "1\n", "authors": ["659"]}
{"title": "Code evasion\n", "abstract": " Programs sometimes tend to lose their structure and clarity through the addition of error handling. Often, more than half of a code base ends up dedicated to various types of error detection and recovery, obscuring the nominal control flow that defines the basic structure. The challenge in writing reliable code is to find ways to remove code from an application by simplifying and generalizing, rather than continuing to add more.", "num_citations": "1\n", "authors": ["659"]}
{"title": "NASA Formal Methods: 7th International Symposium, NFM 2015, Pasadena, CA, USA, April 27-29, 2015, Proceedings\n", "abstract": " This book constitutes the refereed proceedings of the 7th International Symposium on NASA Formal Methods, NFM 2015, held in Pasadena, CA, USA, in April 2015. The 24 revised regular papers presented together with 9 short papers were carefully reviewed and selected from 108 submissions. The topics include model checking, theorem proving; SAT and SMT solving; symbolic execution; static analysis; runtime verification; systematic testing; program refinement; compositional verification; security and intrusion detection; modeling and specification formalisms; model-based development; model-based testing; requirement engineering; formal approaches to fault tolerance; and applications of formal methods.", "num_citations": "1\n", "authors": ["659"]}
{"title": "To Code Is Human.\n", "abstract": " Programmers have found creative ways around programming rules. However, such tactics have a cost.", "num_citations": "1\n", "authors": ["659"]}
{"title": "A three-step program for recovering hackers\n", "abstract": " Following a three-step program can help software developers rely less on users to catch their bugs.", "num_citations": "1\n", "authors": ["659"]}
{"title": "Logic Model Checking of Unintended Acceleration Claims in the 2005 Toyota Camry Electronic Throttle Control System\n", "abstract": " Part of the US DOT investigation of Toyota SUA involved analysis of the throttle control software. JPL LaRS applied several techniques, including static analysis and logic model checking, to the software. A handful of logic models were built. Some weaknesses were identified; however, no cause for SUA was found. The full NASA report includes numerous other analyses", "num_citations": "1\n", "authors": ["659"]}
{"title": "NASA Formal Methods: Third International Symposium, NFM 2011, Pasadena, CA, USA, April 18-20, 2011, Proceedings\n", "abstract": " This book constitutes the refereed proceedings of the Third International Symposium on NASA Formal Methods, NFM 2011, held in Pasadena, CA, USA, in April 2011. The 26 revised full papers presented together with 12 tool papers, 3 invited talks, and 2 invited tutorials were carefully reviewed and selected from 141 submissions. The topics covered by NFM 2011 included but were not limited to: theorem proving, logic model checking, automated testing and simulation, model-based engineering, real-time and stochastic systems, SAT and SMT solvers, symbolic execution, abstraction and abstraction refinement, compositional verification techniques; static and dynamic analysis techniques, fault protection, cyber security, specification formalisms, requirements analysis, and applications of formal techniques.", "num_citations": "1\n", "authors": ["659"]}
{"title": "Software analysis and formal verification\n", "abstract": " CiNii \u8ad6\u6587 - Software analysis and formal verification CiNii \u56fd\u7acb\u60c5\u5831\u5b66\u7814\u7a76\u6240 \u5b66\u8853\u60c5\u5831\u30ca\u30d3\u30b2\u30fc\u30bf [\u30b5\u30a4\u30cb\u30a3] \u65e5\u672c\u306e\u8ad6\u6587\u3092\u3055\u304c\u3059 \u5927\u5b66\u56f3\u66f8\u9928\u306e\u672c\u3092\u3055\u304c\u3059 \u65e5\u672c\u306e\u535a\u58eb\u8ad6\u6587\u3092\u3055\u304c\u3059 \u65b0\u898f\u767b\u9332 \u30ed\u30b0\u30a4\u30f3 English \u691c\u7d22 \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u9589\u3058\u308b \u30bf\u30a4\u30c8\u30eb \u8457\u8005\u540d \u8457\u8005ID \u8457\u8005\u6240\u5c5e \u520a\u884c\u7269\u540d ISSN \u5dfb\u53f7\u30da\u30fc\u30b8 \u51fa\u7248\u8005 \u53c2\u8003\u6587\u732e \u51fa\u7248\u5e74 \u5e74\u304b\u3089 \u5e74\u307e\u3067 \u691c\u7d22 \u691c\u7d22 \u691c\u7d22 CiNii\u7a93\u53e3\u696d\u52d9\u306e\u518d\u958b \u306b\u3064\u3044\u3066 Software analysis and formal verification HOLZMANN G. \u88ab\u5f15\u7528\u6587\u732e: 1\u4ef6 \u8457\u8005 HOLZMANN G. \u53ce\u9332\u520a\u884c\u7269 Proc. Computer Aided Verification, 2002 Proc. Computer Aided Verification, 2002, 2002 \u88ab\u5f15\u7528\u6587\u732e: 1\u4ef6\u4e2d 1-1\u4ef6\u3092 \u8868\u793a 1 Checking Liveness Properties of Concurrent Systems by Using Reinforcement Learning Araragi Tadashi , Cho Seung Mo \u96fb\u5b50 \u60c5\u5831\u901a\u4fe1\u5b66\u4f1a\u6280\u8853\u7814\u7a76\u5831\u544a. AI, \u4eba\u5de5\u77e5\u80fd\u3068\u77e5\u8b58\u51e6\u7406 105(361), 25-30, 2005-10-21 \u53c2\u8003\u6587\u732e9\u4ef6 \u5927\u5b66\u9662\u8aac\u660e\u4f1a Tweet \u5404\u7a2e\u30b3\u30fc\u30c9 NII\u8ad6\u6587ID(NAID) 10016836663 \u8cc7\u6599\u7a2e\u5225 \u4f1a\u8b70\u8cc7\u6599 \u30c7\u30fc\u30bf\u63d0\u4f9b\u5143 \u2026", "num_citations": "1\n", "authors": ["659"]}
{"title": "Interval reduction through requirements analysis\n", "abstract": " A significant number of the delays encountered in system design projects are caused by logical inconsistencies that are inadvertently introduced during the early phases of software design. Many of these inconsistencies are ultimately detected in design and code reviews or during system testing. In the current design process, though, these errors are rarely caught before implementation of the system nears completion. We show that modeling and verifying the requirements separately, before actual system design begins, can help to intercept the ambiguities and inconsistencies that might otherwise not be detected until testing or field use. By doing so, one can prevent a class of errors from entering the development phase and thereby reduce time to market while at the same time improving overall design quality. We demonstrate this approach by using the Bell Labs model checking system Spin to mechanically verify\u00a0\u2026", "num_citations": "1\n", "authors": ["659"]}
{"title": "SPIN Verification Examples and Exercises\n", "abstract": " Included in this memo are some verification exercises that can help new users to get acquainted with SPIN. A few example models for standard verification problems are included at the end. All examples are also available as PROMELA files in the Test directory of the SPIN distribution (they are bundled in the file Test/examples).", "num_citations": "1\n", "authors": ["659"]}
{"title": "What's New in SPIN Version 2.0 (Draft)\n", "abstract": " SPIN is a general verification tool for proving correctness properties of distributed or concurrent systems. The systems can interact through shared memory, through rendezvous operations, or through buffered message exchanges. The coordination problems that these interactions may create can effectively be debugged with the SPIN system. Once a correct design of the system has been obtained, a rigorous proof of its correctness can be provided.SPIN version 2.0 is the most recent release of a verification system that is based on the paradigm of on\u2212 the\u2212 fly verification. The first version of SPIN was released through netlib in January 1991. This note gives an overview of the changes that have been made in SPIN itself, in the validation modeling language PROMELA, and in SPIN\u2019s graphical user interface XSPIN.", "num_citations": "1\n", "authors": ["659"]}
{"title": "\u2018Algebraic validation methods\n", "abstract": " Though we restrict the discussion to just these three techniques, there are other validation methods (4, 15, 16, 19) and indeed some specification methods (1, 2, 18] that share important concepts with them. In section 2 we will consider Milner's calculus for communicating systems, CCS (12, 13). In section 3 we take a brief look at the flow expression language intro-duced by Shaw (17), and finally in section 4 we will review the validation algebra described in (5, 6, 7). Section 5 summarizes our findings., mu mteractions. The proof rules from com2. CCS-A Calculus for Communicating Systems (10-13] Processes in CCS are modeled by\" communicating agents\" whose behaviors are described in algebraic expressions. CCS combines operators resembling regular expression constructs like concatenation (time ordering), summation (ambiguity), and repetition or recursion (Kleene star) with a small set of operators that can be used to model concurrency and interactions. The proof rules from CCS are, how-ever, quite different from the rules that hold for regular expressions. One of the major divergencies from these identities is the rejection of the distributive law for time orderings over summations in CCS. We will illustrate this with an example. Consider the following two behavior expressions in CCS:", "num_citations": "1\n", "authors": ["659"]}
{"title": "Concise description of a protocol validation algebra\n", "abstract": " The following description of a protocol validation algebra is a simplified and purified version of the one discussed in [1]. The current description will be used for the implementation of the Pandora system [2]. For amore gentle introduction to the theory developed here the reader is referred to [1].", "num_citations": "1\n", "authors": ["659"]}
{"title": "Coordination problems in multiprocessing systems\n", "abstract": " They are, however, not always recognized as such, and if they are recognized they are often solved in haphazard ways. In practice, this presents few difficulties. The probability of the emergence of serious coordination errors is in general comparatively low.", "num_citations": "1\n", "authors": ["659"]}