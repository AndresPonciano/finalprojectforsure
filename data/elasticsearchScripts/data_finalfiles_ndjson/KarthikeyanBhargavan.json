{"title": "Imperfect forward secrecy: How Diffie-Hellman fails in practice\n", "abstract": " We investigate the security of Diffie-Hellman key exchange as used in popular Internet protocols and find it to be less secure than widely believed. First, we present Logjam, a novel flaw in TLS that lets a man-in-the-middle downgrade connections to\" export-grade\" Diffie-Hellman. To carry out this attack, we implement the number field sieve discrete log algorithm. After a week-long precomputation for a specified 512-bit group, we can compute arbitrary discrete logs in that group in about a minute. We find that 82% of vulnerable servers use a single 512-bit group, allowing us to compromise connections to 7% of Alexa Top Million HTTPS sites. In response, major browsers are being changed to reject short groups. We go on to consider Diffie-Hellman with 768-and 1024-bit groups. We estimate that even in the 1024-bit case, the computations are plausible given nation-state resources. A small number of fixed or\u00a0\u2026", "num_citations": "542\n", "authors": ["1798"]}
{"title": "On the practical (in-) security of 64-bit block ciphers: Collision attacks on HTTP over TLS and OpenVPN\n", "abstract": " While modern block ciphers, such as AES, have a block size of at least 128 bits, there are many 64-bit block ciphers, such as 3DES and Blowfish, that are still widely supported in Internet security protocols such as TLS, SSH, and IPsec. When used in CBC mode, these ciphers are known to be susceptible to collision attacks when they are used to encrypt around 2 32 blocks of data (the so-called birthday bound). This threat has traditionally been dismissed as impractical since it requires some prior knowledge of the plaintext and even then, it only leaks a few secret bits per gigabyte. Indeed, practical collision attacks have never been demonstrated against any mainstream security protocol, leading to the continued use of 64-bit ciphers on the Internet.", "num_citations": "186\n", "authors": ["1798"]}
{"title": "HACL*: A verified modern cryptographic library\n", "abstract": " HACL* is a verified portable C cryptographic library that implements modern cryptographic primitives such as the ChaCha20 and Salsa20 encryption algorithms, Poly1305 and HMAC message authentication, SHA-256 and SHA-512 hash functions, the Curve25519 elliptic curve, and Ed25519 signatures.", "num_citations": "155\n", "authors": ["1798"]}
{"title": "Transcript collision attacks: Breaking authentication in TLS, IKE, and SSH\n", "abstract": " In response to high-profile attacks that exploit hash function collisions, software vendors have started to phase out the use of MD5 and SHA-1 in third-party digital signature applications such as X.509 certificates. However, weak hash constructions continue to be used in various cryptographic constructions within mainstream protocols such as TLS, IKE, and SSH, because practitioners argue that their use in these protocols relies only on second preimage resistance, and hence is unaffected by collisions. This paper systematically investigates and debunks this argument. We identify a new class of transcript collision attacks on key exchange protocols that rely on efficient collision-finding algorithms on the underlying hash constructions. We implement and demonstrate concrete credential-forwarding attacks on TLS 1.2 client authentication, TLS 1.3 server authentication, and TLS channel bindings. We describe almost-practical impersonation and downgrade attacks in TLS 1.1, IKEv2 and SSH-2. As far as we know, these are the first collision-based attacks on the cryptographic constructions used in these popular protocols. Our practical attacks on TLS were responsibly disclosed (under the name SLOTH) and have resulted in security updates to several TLS libraries. Our analysis demonstrates the urgent need for disabling all uses of weak hash functions in mainstream protocols, and our recommendations have been incorporated in the upcoming Token Binding and TLS 1.3 protocols.", "num_citations": "116\n", "authors": ["1798"]}
{"title": "A constraint-based formalism for consistency in replicated systems\n", "abstract": " We present a formalism for modeling replication in a distributed system with concurrent users sharing information. It is based on actions, which represent operations requested by independent users, and constraints, representing scheduling relations between actions. The formalism encompasses semantics of shared data, such as commutativity or conflict between actions, and user intents such as causal dependence or atomicity. It enables us to reason about the consistency properties of a replication protocol or of classes of protocols. It supports weak consistency (optimistic protocols) as well as the stronger pessimistic protocols. Our approach clarifies the requirements and assumptions common to all replication systems. We are able to prove a number of common properties. For instance consistency properties that appear different operationally are proved equivalent under suitable liveness assumptions\u00a0\u2026", "num_citations": "47\n", "authors": ["1798"]}
{"title": "A verified extensible library of elliptic curves\n", "abstract": " In response to increasing demand for elliptic curve cryptography, and specifically for curves that are free from the suspicion of influence by the NSA, new elliptic curves such as Curve25519 and Curve448 are currently being standardized, implemented, and deployed in major protocols such as Transport Layer Security. As with all new cryptographic code, the correctness of these curve implementations is of concern, because any bug or backdoor in this code can potentially compromise the security of important Internet protocols. We present a principled approach towards the verification of elliptic curve implementations by writing them in the dependently-typed programming language F* and proving them functionally correct against a readable mathematical specification derived from a previous Coq development. A key technical innovation in our work is the use of templates to write and verify arbitrary precision\u00a0\u2026", "num_citations": "33\n", "authors": ["1798"]}
{"title": "Content delivery over TLS: a cryptographic analysis of keyless SSL\n", "abstract": " The Transport Layer Security (TLS) protocol is designed to allow two parties, a client and a server, to communicate securely over an insecure network. However, when TLS connections are proxied through an intermediate middlebox, like a Content Delivery Network (CDN), the standard endto- end security guarantees of the protocol no longer apply. In this paper, we investigate the security guarantees provided by Keyless SSL, a CDN architecture currently deployed by CloudFlare that composes two TLS 1.2 handshakes to obtain a proxied TLS connection. We demonstrate new attacks that show that Keyless SSL does not meet its intended security goals. These attacks have been reported to CloudFlare and we are in the process of discussing fixes. We argue that proxied TLS handshakes require a new, stronger, 3-party security definition. We present 3(S)ACCEsecurity, a generalization of the 2-party ACCE security\u00a0\u2026", "num_citations": "26\n", "authors": ["1798"]}
{"title": "Noise Explorer: Fully automated modeling and verification for arbitrary Noise protocols\n", "abstract": " The Noise Protocol Framework, introduced recently, allows for the design and construction of secure channel protocols by describing them through a simple, restricted language from which complex key derivation and local state transitions are automatically inferred. Noise \"Handshake Patterns\" can support mutual authentication, forward secrecy, zero round-trip encryption, identity hiding and other advanced features. Since the framework's release, Noise-based protocols have been adopted by WhatsApp, WireGuard and other high-profile applications. We present Noise Explorer, an online engine for designing, reasoning about, formally verifying and implementing arbitrary Noise Handshake Patterns. Based on our formal treatment of the Noise Protocol Framework, Noise Explorer can validate any Noise Handshake Pattern and then translate it into a model ready for automated verification and also into a production\u00a0\u2026", "num_citations": "23\n", "authors": ["1798"]}
{"title": "Imperfect forward secrecy: How Diffie-Hellman fails in practice\n", "abstract": " We investigate the security of Diffie-Hellman key exchange as used in popular Internet protocols and find it to be less secure than widely believed. First, we present Logjam, a novel flaw in TLS that lets a man-in-the-middle downgrade connections to \"export-grade\" Diffie-Hellman. To carry out this attack, we implement the number field sieve discrete logarithm algorithm. After a week-long precomputation for a specified 512-bit group, we can compute arbitrary discrete logarithms in that group in about a minute. We find that 82% of vulnerable servers use a single 512-bit group, and that 8.4% of Alexa Top Million HTTPS sites are vulnerable to the attack. In response, major browsers have changed to reject short groups. We go on to consider Diffie-Hellman with 768- and 1024-bit groups. We estimate that even in the 1024-bit case, the computations are plausible given nation-state resources. A small number of fixed or\u00a0\u2026", "num_citations": "23\n", "authors": ["1798"]}
{"title": "Fault origin adjudication\n", "abstract": " When a program P fails to satisfy a requirement R supposedly ensured by a detailed specification S that was used to implement P, there is a question about whether the problem arises in S or in P. We call this determination fault origin adjudication and illustrate its significance in various software engineering contexts. The primary contribution of this paper is a framework for formal fault origin adjudication for network protocols using the NS simulator and the SPIN model checker. We describe our architecture and illustrate its use in a case study involving a standard specification for packet radio routing.", "num_citations": "21\n", "authors": ["1798"]}
{"title": "Requirements for a practical network event recognition language\n", "abstract": " We propose a run-time monitoring and checking architecture for network protocols called Network Event Recognition. Our framework is based on passively monitoring the packet trace produced by a protocol implementation and checking it for properties written in a formal specification language, NERL. In this paper, we describe the design requirements for NERL. We show how the unique requirements of network protocol monitoring impact design and implementation options. Finally we outline our prototype implementation of NERL and discuss two case studies: checking the correctness of network protocol simulations and privacy issues in packet-mode surveillance.", "num_citations": "19\n", "authors": ["1798"]}
{"title": "Formally verified cryptographic web applications in webassembly\n", "abstract": " After suffering decades of high-profile attacks, the need for formal verification of security-critical software has never been clearer. Verification-oriented programming languages like F* are now being used to build high-assurance cryptographic libraries and implementations of standard protocols like TLS. In this paper, we seek to apply these verification techniques to modern Web applications, like WhatsApp, that embed sophisticated custom cryptographic components. The problem is that these components are often implemented in JavaScript, a language that is both hostile to cryptographic code and hard to reason about. So we instead target WebAssembly, a new instruction set that is supported by all major JavaScript runtimes. We present a new toolchain that compiles Low*, a low-level subset of the F* programming language, into WebAssembly. Unlike other WebAssembly compilers like Emscripten, our\u00a0\u2026", "num_citations": "18\n", "authors": ["1798"]}
{"title": "The village telephone system: A case study in formal software engineering\n", "abstract": " In this paper we illustrate the use of formal methods in the development of a benchmark application we call the Village Telephone System which is characteristic of a class of network and telecommunication protocols. The aim is to show an effective integration of methodology and tools in a software engineering task that proceeds from user-level requirements to an implementation. In particular, we employ a general methodology which we advocate for requirements capture and refinement based on a treatment of designated terminology, domain knowledge, requirements, specifications, and implementation. We show how a general-purpose theorem prover (HOL) can provide formal support for all of these components and how a model checker (Mocha) can provide formal support for the specifications and implementation. We develop a new HOL theory of inductive sequences that is suited to modelling reactive\u00a0\u2026", "num_citations": "18\n", "authors": ["1798"]}
{"title": "Identifying website users by TLS traffic analysis: New attacks and effective countermeasures\n", "abstract": " Websites commonly use HTTPS to protect their users' private data from network-based attackers. By combining public social network profiles with TLS traffic analysis, we present a new attack that reveals the precise identities of users accessing major websites. As a countermeasure, we propose a novel length-hiding scheme that leverages standard TLS padding to enforce website-specific privacy policies. We present several implementations of this scheme, notably a patch for GnuTLS that offers a rich length-hiding API and an Apache module that uses this API to enforce an anonymity policy for sensitive user files. Our implementations are the first to fully exercise the length-hiding features of TLS and our work uncovers hidden timing assumptions in recent formal proofs of these features. Compared to previous work, we offer the first countermeasure that is standards-based, provably secure, and experimentally effective, yet pragmatic, offering websites a precise trade-off between user privacy and bandwidth efficiency.", "num_citations": "16\n", "authors": ["1798"]}
{"title": "The Actions-Constraints approach to replication: Definitions and proofs\n", "abstract": " Replicated information raises the major issue of consistency. We have developped a simple, formal framework, in order to better understand and compare consistency properties of replication protocols. The framework is both formal and implementable. Our language is simple enough to prove interesting properties, yet sufficiently powerful to specify diverse systems. In our model, each site maintains its local view of data, of actions to execute, and of the constraints that define legal execution schedules. Adding actions increases the number of possible schedules; adding constraints reduces scheduling non-determinism. We exhibit significant subsets of actions that are progressively more determined and show a number of useful properties. The system is consistent if every action is eventually scheduled and local executions converge. We compare different possible formulations of the consistency property and prove\u00a0\u2026", "num_citations": "16\n", "authors": ["1798"]}
{"title": "Network event recognition\n", "abstract": " Network protocols can be tested by capturing communication packets, assembling them into the high-level events, and comparing these to a finite state machine that describes the protocol standard. This process, which we call Network Event Recognition (NER), faces a number of challenges only partially addressed by existing systems. These include the ability to provide precise conformance with specifications, achieve adequate performance, admit analysis of the correctness of recognizers, provide useful diagnostics to enable the analysis of errors, and provide reasonable fidelity by distinguishing application errors from network errors. We introduce a special-purpose Network Event Recognition Language (NERL) and associated tools to address these issues. We validate the design using case studies on protocols at application and transport layers. These studies show that our system can efficiently find\u00a0\u2026", "num_citations": "12\n", "authors": ["1798"]}
{"title": "A formalism for consistency and partial replication\n", "abstract": " Replication protocols are complex and it is difficult to compare their consistency properties. To this effect, we propose a formalism where a replica executes actions subject to constraints in its local view or multilog. Schedules are selected non-deterministically from the set of sound schedules. This set grows with the number of actions and shrinks as the number of constraints increases. If the size of the set is one, the site has converged; if the size becomes zero, the site has detected an unrecoverable conflict. If every site runs the same schedule, they are consistent. We formalise this intuitive concept of consistency in four different ways, which generalise classical consistency criteria and which expose different aspects of consistency protocols. We prove them equivalent. We provide the first formal definition of consistency for partially replicated data. In general, achieving consistency entails global consensus; we exhibit sufficient conditions for deciding locally and derive a new decentralised protocol. In a separate technical report we prove the consistency of some published consistency protocols; this underscores the deep commonalities between them.", "num_citations": "12\n", "authors": ["1798"]}
{"title": "Managing Server Farms\n", "abstract": " Manual management of server farms is expensive. Low-level tools and the sheer complexity of the task make it prone to human error. By providing a typed interface using service combinators for managing server farms it is possible to improve automated server farm management. Metadata about a server farm is obtained, for example, from disk images, and this is used to generate a typed environment interface for accessing server farm resources. Scripts are received, from a human operator or automated process, which use the environment interface and optionally also pre-specified service combinators. The scripts are executed to assemble and link together services in the server farm to form and manage a running server farm application. By using typechecking server farm construction errors can be caught before implementation.", "num_citations": "11\n", "authors": ["1798"]}
{"title": "Towards unified authorization for android\n", "abstract": " Android applications that manage sensitive data such as email and files downloaded from cloud storage services need to protect their data from malware installed on the phone. While prior security analyses have focused on protecting system data such as GPS locations from malware, not much attention has been given to the protection of application data. We show that many popular commercial applications incorrectly use Android authorization mechanisms leading to attacks that steal sensitive data. We argue that formal verification of application behaviors can reveal such errors and we present a formal model in ProVerif that accounts for a variety of Android authorization mechanisms and system services. We write models for four popular applications and analyze them with ProVerif to point out attacks. As a countermeasure, we propose Authzoid, a sample standalone application that lets applications\u00a0\u2026", "num_citations": "9\n", "authors": ["1798"]}
{"title": "DY*: a modular symbolic verification framework for executable cryptographic protocol code\n", "abstract": " We present DY*, a new formal verification framework for the symbolic security analysis of cryptographic protocol code written in the F* programming language. Unlike automated symbolic provers, our framework accounts for advanced protocol features like unbounded loops and mutable recursive data structures, as well as low-level implementation details like protocol state machines and message formats, which are often at the root of real-world attacks. Our work extends a long line of research on using dependent type systems for this task, but takes a fundamentally new approach by explicitly modeling the global trace-based semantics within the framework, hence bridging the gap between trace-based and type-based protocol analyses. This approach enables us to uniformly, precisely, and soundly model, for the first time using dependent types, long-lived mutable protocol state, equational theories, fine-grained dynamic corruption, and trace-based security properties like forward secrecy and post-compromise security. DY* is built as a library of F* modules that includes a model of low-level protocol execution, a Dolev-Yao symbolic attacker, and generic security abstractions and lemmas, all verified using F*. The library exposes a high-level API that facilitates succinct security proofs for protocol code. We demonstrate the effectiveness of this approach through a detailed symbolic security analysis of the Signal protocol that is based on an interoperable implementation of the protocol from prior work, and is the first mechanized proof of Signal to account for forward and post-compromise security over an unbounded number of protocol rounds.", "num_citations": "8\n", "authors": ["1798"]}
{"title": "HACLxN: Verified Generic SIMD Crypto (for all your favourite platforms)\n", "abstract": " We present a new methodology for building formally verified cryptographic libraries that are optimized for multiple architectures. In particular, we show how to write and verify generic crypto code in the F* programming language that exploits single-instruction multiple data (SIMD) parallelism. We show how this code can be compiled to platforms that support vector instructions, including ARM Neon and Intel AVX, AVX2, and AVX512. We apply our methodology to obtain verified vectorized implementations on all these platforms for the ChaCha20 encryption algorithm, the Poly1305 one-time MAC, and the SHA-2 and Blake2 families of hash algorithms.", "num_citations": "8\n", "authors": ["1798"]}
{"title": "Service combinators for farming virtual machines\n", "abstract": " Management is one of the main expenses of running the server farms that implement enterprise services, and operator errors can be costly. Our goal is to develop type-safe programming mechanisms for combining and managing enterprise services, and we achieve this goal in the particular setting of farms of virtual machines. We assume each server is service-oriented, in the sense that the services it provides, and the external services it depends upon, are explicitly described in metadata. We describe the design, implementation, and formal semantics of a library of combinators whose types record and respect server metadata. We describe a series of programming examples run on our implementation, based on existing server code for a typical web application.", "num_citations": "8\n", "authors": ["1798"]}
{"title": "A compositional theory for STM Haskell\n", "abstract": " We address the problem of reasoning about Haskell programs that use Software Transactional Memory (STM). As a motivating example, we consider Haskell code for a concurrent non-deterministic tree rewriting algorithm implementing the operational semantics of the ambient calculus. The core of our theory is a uniform model, in the spirit of process calculi, of the run-time state of multi-threaded STM Haskell programs. The model was designed to simplify both local and compositional reasoning about STM programs. A single reduction relation captures both pure functional computations and also effectful computations in the STM and I/O monads. We state and prove liveness, soundness, completeness, safety, and termination properties relating source processes and their Haskell implementation. Our proof exploits various ideas from concurrency theory, such as the bisimulation technique, but in the setting of a\u00a0\u2026", "num_citations": "7\n", "authors": ["1798"]}
{"title": "Routing information protocol in HOL/SPIN\n", "abstract": " We provide a proof using HOL and SPIN of convergence for the Routing Information Protocol (RIP), an internet protocol based on distance vector routing. We also calculate a sharp realtime bound for this convergence. This extends existing results to deal with the RIP standard itself, which has complexities not accounted for in theorems about abstract versions of the protocol. Our work also provides a case study in the combined use of a higher-order theorem prover and a model checker. The former is used to express abstraction properties and inductions, and structure the high-level proof, while the latter deals efficiently with case analysis of finitary properties.", "num_citations": "7\n", "authors": ["1798"]}
{"title": "Formal Models and Verified Protocols for Group Messaging: Attacks and Proofs for IETF MLS\n", "abstract": " Group conversations are supported by most modern messaging applications, but the security guarantees they offer are significantly weaker than those for two-party protocols like Signal. The problem is that mechanisms that are efficient for two parties do not scale well to large dynamic groups where members may be regularly added and removed. Further, group messaging introduces subtle new security requirements that require new solutions. The IETF Messaging Layer Security (MLS) working group is standardizing a new asynchronous group messaging protocol that aims to achieve strong guarantees like forward secrecy and post-compromise security for large dynamic groups. In this paper, we define a formal framework for group messaging in the F language and use it to compare the security and performance of several candidate MLS protocols up to draft 7. We present a succinct, executable, formal specification and symbolic security proof for TreeKEMB, the group key establishment protocol in MLS draft 7. Our analysis finds new attacks and we propose verified fixes, which are now being incorporated into MLS. Ours is the first mechanically checked proof for MLS, and our analysis technique is of independent interest, since it accounts for groups of unbounded size, stateful recursive data structures, and fine-grained compromise.", "num_citations": "6\n", "authors": ["1798"]}
{"title": "ProScript TLS: Building a TLS 1.3 Implementation with a Verifiable Protocol Model\n", "abstract": " ProScript TLS: Building a TLS 1.3 Implementation with a Verifiable Protocol Model Page 1 ProScript TLS: Building a TLS 1.3 Implementation with a Verifiable Protocol Model Karthikeyan Bhargavan Nadim Kobeissi Bruno Blanchet Page 2 miTLS: reference implementation of TLS 1.0-1.2 flexTLS: specification-based testing for TLS Page 3 Goal: get developers to run light-weight analysis A reference implementation of TLS in JavaScript \u2022 Not a cryptographic proof! Page 4 Current implementation status Current verification status Page 5 Draft 11 specification Complex key schedule Page 6 Client knows S\u2019s semi-static key gs Client auth block 0-RTT data Page 7 Server Auth Block Server Semi-Static 1.5-RTT Data Page 8 Client auth block (again) 1-RTT Data Composite Data Stream Page 9 Handshake messages processed in flights Client Server send_client_hello recv_client_hello recv_server_hello send_server_finished \u2026", "num_citations": "6\n", "authors": ["1798"]}
{"title": "Network event recognition for packet-mode surveillance\n", "abstract": " \u00cburveillance of packet-mode communications can draw on ideas from firewalls and network intrusion detection systems but has features that raise distinct software engineering challenges. We propose an architecture, C\u00cbF, for composable separation functions that can enhance privacy, clarity of specifications, and assurance. We introduce a language, NERL, for network event recognition and use it to build an opensource surveillance system, OpenWarrants, based on C\u00cbF. We demonstrate how NERL can be used as a basis for formally analyzing privacy protections and how C\u00cbF can be used to provide new capabilities within formally-specified privacy policies.", "num_citations": "5\n", "authors": ["1798"]}
{"title": "An assessment of tools used in the verinet project\n", "abstract": " The Verinet project aims to develop methodology and tool support for the formal analysis of internetwork routing protocols. We show how di erent tools have been used for protocol speci cation, verication, and testing, and assess their e ectiveness and performance. SPIN is used extensively in our analyses, and we demonstrate e ective ways of combining the SPIN veri cation system with other analysis tools for network protocols.", "num_citations": "5\n", "authors": ["1798"]}
{"title": "Protecting Transport Layer Security from Legacy Vulnerabilities\n", "abstract": " The Transport Layer Security protocol (TLS) is the most widely-used secure channel protocol on the Web. After 20 years of evolution, TLS has grown to include five protocol versions, dozens of extensions, and hundreds of ciphersuites. The success of TLS as an open standard is at least partially due its protocol agility: clients and servers can implement different subsets of protocol features and still interoperate, as long as they can negotiate a common version and ciphersuite. Hence, software vendors can seamlessly deploy newer cryptographic mechanisms while still supporting older algorithms for backwards compatibility. An undesirable consequence of this agility is that obsolete and broken ciphers can stay enabled in TLS clients and servers for years after cryptographers have explicitly warned against their use. Practitioners consider this relatively safe for two reasons. First, the TLS key exchange protocol\u00a0\u2026", "num_citations": "4\n", "authors": ["1798"]}
{"title": "Getting Operations Logic Right Types, Service-Orientation, and Static Analysis\n", "abstract": " The human operators of datacentres work from a manual, sometimes known as the run book, that lists how to perform operating procedures such as the provisioning, deployment, monitoring, and upgrading of servers. To improve failure and recovery rates, it is attractive to replace human intervention by software, known as operations logic, that automates such operating procedures. We advocate a declarative programming model for operations logic, and the use of static analysis to detect programming errors, such as the potential for misconfiguration. 1. Background: The Datacentre is the Computer A datacentre is some housing (typically a room or a building) that physically contains a cluster of commodity servers, and provides them with power, cooling, and networking. Datacentres are the computers that run applications such as websites (search and mail", "num_citations": "4\n", "authors": ["1798"]}
{"title": "An in-depth symbolic security analysis of the ACME standard\n", "abstract": " The ACME certificate issuance and management protocol, standardized as IETF RFC 8555, is an essential element of the web public key infrastructure (PKI). It has been used by Let's Encrypt and other certification authorities to issue over a billion certificates, and a majority of HTTPS connections are now secured with certificates issued through ACME. Despite its importance, however, the security of ACME has not been studied at the same level of depth as other protocol standards like TLS 1.3 or OAuth. Prior formal analyses of ACME only considered the cryptographic core of early draft versions of ACME, ignoring many security-critical low-level details that play a major role in the 100 page RFC, such as recursive data structures, long-running sessions with asynchronous sub-protocols, and the issuance for certificates that cover multiple domains.", "num_citations": "3\n", "authors": ["1798"]}
{"title": "TreeKEM: asynchronous decentralized key management for large dynamic groups a protocol proposal for messaging layer security (MLS)\n", "abstract": " The Messaging Layer Security (MLS) architecture envisions a protocol that can establish a key shared by a group of members, where each member controls a number of clients (devices). Each client is identified by its own long-term key, and can participate in the protocol asynchronously, that is, without needing any other client to be online. Notably, each client can issue asynchronous group modification requests to add new members, remove members, and update its own keys, etc. The architecture document also states a series of security goals for the protocol. We begin this document by stating the desired functionality and security goals of MLS in our own notation. We then propose a new protocol that seeks to achieve the confidentiality goals of the MLS architecture. (This proposal was first posted on the IETF MLS Mailing List on May 3rd, 2019. See: https://mailarchive.ietf.org/arch/msg/mls/e3ZKNzPC7Gxrm3Wf0q96dsLZoD8)", "num_citations": "3\n", "authors": ["1798"]}
{"title": "Noise Explorer\n", "abstract": " Noise Explorer Page 1 Noise Explorer Fully automated modeling, analysis and verification for arbitrary Noise protocols Nadim Kobeissi Karthikeyan Bhargavan IACR Real World Crypto Symposium 2019 San Jose, California Page 2 Noise Protocol Framework: What is it? Example Noise Handshake Pattern NK: <- s ... -> e, es <- e, ee A Framework for Secure Channel Protocols \u2022 Based on Diffie-Hellman key agreement. \u2022 Simple language for describing messages. \u2022 From message description, complex state transformations are derived. \u2022 Author: Trevor Perrin. 2 Page 3 Trevor Perrin\u2019s Talk at RWC2018 3 https://youtu.be/3gipxdJ22iM Page 4 Understanding the Notation Example Noise Handshake Pattern IK: <- s ... -> e, es, s, ss <- e, ee, se Handshake Pattern Notation \u2022 s, e: local static and ephemeral key pairs. Automatically generated when they appear in a message. \u2022 ss, se, es, ee: Diffie-Hellman operations. \u2026", "num_citations": "3\n", "authors": ["1798"]}
{"title": "Brief announcement: Exploring the consistency problem space\n", "abstract": " We study formally the consistency problem, for replicated shared data, in the Action-Constraint framework (ACF). ACF can describe a large range of application semantics and replication protocols, including optimistic and/or partial replication. ACF is used to decompose the consistency problem into simpler sub-problems. Each is easily understood. Existing algorithms from the literature can be explained as combinations of concrete sub-problem implementations. Using ACF, we design a new serialisation algorithm that does not cause aborts and only needs pairwise agreement (not global consensus).", "num_citations": "3\n", "authors": ["1798"]}
{"title": "Brief Announcement: A formalism for consistency and partial replication\n", "abstract": " Replicating data in a distributed system improves availability at the cost of maintaining consistency, since each site\u2019s view may be partial or stale. Although a number of protocols have been proposed to achieve various degrees of consistency [14], we lack a common framework for understanding and comparing them. This paper presents such a framework. In our model, a replicated system consists of a database, replicated at several sites, and a replication protocol that reacts to an environment consisting of users and the underlying network. Users issue actions to query or update the database. Each site receives these actions in arbitrary order and executes a schedule, a sequence of received actions. The chosen schedule completely determines the state of the replica. Not all schedules are sound, however, since an actions may conflict with another, or causally depend on another, or users may require some actions to occur before others. These relationships between actions can be represented as binary constraints. We introduce a simple language of constraints between actions. These constraints are adequate to encode many kinds of user requirements, application semantics, and protocol decisions. A replication protocol takes as input a set of actions and constraints, and ensures that, eventually, the schedules executed at all sites satisfy the constraints and are equivalent. This is the well-known notion of eventual consistency. However, eventual consistency says nothing about the intermediate schedules at the replicas. For instance, it allows a site to execute an intermediate schedule that violates some user constraints. Instead, we want the\u00a0\u2026", "num_citations": "3\n", "authors": ["1798"]}
{"title": "hacspec: Towards Verifiable Crypto Standards\n", "abstract": " We present hacspec, a collaborative effort to design a formal specification language for cryptographic primitives. Specifications (specs) written in hacspec are succinct, easy to read and implement, and lend themselves to formal verification using a variety of existing tools. The syntax of hacspec is similar to the pseudocode used in cryptographic standards but is equipped with a static type system and syntax checking tools that can find errors. Specs written in hacspec are executable and can hence be tested against test vectors taken from standards and specified in a common format. Finally, hacspec is designed to be compilable to other formal specification languages like F, EasyCrypt, Coq, and cryptol, so that it can be used as the basis for formal proofs of functional correctness and cryptographic security using various verification frameworks. This paper presents the syntax, design, and tool architecture of\u00a0\u2026", "num_citations": "2\n", "authors": ["1798"]}
{"title": "A tutorial-style introduction to DY\n", "abstract": " DY* is a recently proposed formal verification framework for the symbolic security analysis of cryptographic protocol code written in the F* programming language. Unlike automated symbolic provers, DY* accounts for advanced protocol features like unbounded loops and mutable recursive data structures as well as low-level implementation details like protocol state machines and message formats, which are often at the root of real-world attacks. Protocols modeled in DY* can be executed, and hence, tested, and they can even interoperate with real-world counterparts. DY* extends a long line of research on using dependent type systems but takes a fundamentally new approach by explicitly modeling the global trace-based semantics within the framework, hence bridging the gap between trace-based and type-based protocol analyses. With this, one can uniformly, precisely, and soundly model, for the first time using\u00a0\u2026", "num_citations": "1\n", "authors": ["1798"]}