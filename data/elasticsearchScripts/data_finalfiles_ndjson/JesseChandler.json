{"title": "Estimating the reproducibility of psychological science\n", "abstract": " INTRODUCTION Reproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. Scientific claims should not gain credence because of the status or authority of their originator but by the replicability of their supporting evidence. Even research of exemplary quality may have irreproducible empirical findings because of random or systematic error.RATIONALE There is concern about the rate and predictors of reproducibility, but limited evidence. Potentially problematic practices include selective reporting, selective analysis, and insufficient specification of the conditions necessary or sufficient to obtain the results. Direct replication is the attempt to recreate the conditions believed sufficient for obtaining a previously observed finding and is the means of establishing reproducibility of a finding with new data. We conducted a large-scale, collaborative effort to obtain an\u00a0\u2026", "num_citations": "6703\n", "authors": ["2069"]}
{"title": "Inside the Turk: Understanding Mechanical Turk as a participant pool\n", "abstract": " Mechanical Turk (MTurk), an online labor market created by Amazon, has recently become popular among social scientists as a source of survey and experimental data. The workers who populate this market have been assessed on dimensions that are universally relevant to understanding whether, why, and when they should be recruited as research participants. We discuss the characteristics of MTurk as a participant pool for psychology and other social sciences, highlighting the traits of the MTurk samples, why people become MTurk workers and research participants, and how data quality on MTurk compares to that from other pools and depends on controllable and uncontrollable factors.", "num_citations": "2426\n", "authors": ["2069"]}
{"title": "Using Mechanical Turk to study clinical populations\n", "abstract": " Although participants with psychiatric symptoms, specific risk factors, or rare demographic characteristics can be difficult to identify and recruit for participation in research, participants with these characteristics are crucial for research in the social, behavioral, and clinical sciences. Online research in general and crowdsourcing software in particular may offer a solution. However, no research to date has examined the utility of crowdsourcing software for conducting research on psychopathology. In the current study, we examined the prevalence of several psychiatric disorders and related problems, as well as the reliability and validity of participant reports on these domains, among users of Amazon\u2019s Mechanical Turk. Findings suggest that crowdsourcing software offers several advantages for clinical research while providing insight into potential problems, such as misrepresentation, that researchers should address\u00a0\u2026", "num_citations": "1187\n", "authors": ["2069"]}
{"title": "Nonna\u00efvet\u00e9 among Amazon Mechanical Turk workers: Consequences and solutions for behavioral researchers\n", "abstract": " Crowdsourcing services\u2014particularly Amazon Mechanical Turk\u2014have made it easy for behavioral scientists to recruit research participants. However, researchers have overlooked crucial differences between crowdsourcing and traditional recruitment methods that provide unique opportunities and challenges. We show that crowdsourced workers are likely to participate across multiple related experiments and that researchers are overzealous in the exclusion of research participants. We describe how both of these problems can be avoided using advanced interface features that also allow prescreening and longitudinal data collection. Using these techniques can minimize the effects of previously ignored drawbacks and expand the scope of crowdsourcing as a tool for psychological research.", "num_citations": "1097\n", "authors": ["2069"]}
{"title": "Investigating variation in replicability\n", "abstract": " Although replication is a central tenet of science, direct replications are rare in psychology. This research tested variation in the replicability of 13 classic and contemporary effects across 36 independent samples totaling 6,344 participants. In the aggregate, 10 effects replicated consistently. One effect\u2013imagined contact reducing prejudice\u2013showed weak support for replicability. And two effects\u2013flag priming influencing conservatism and currency priming influencing system justification\u2013did not replicate. We compared whether the conditions such as lab versus online or US versus international sample predicted effect magnitudes. By and large they did not. The results of this small sample of effects suggest that replicability is more dependent on the effect itself than on the sample and setting used to investigate the effect.", "num_citations": "990\n", "authors": ["2069"]}
{"title": "Conducting clinical research using crowdsourced convenience samples\n", "abstract": " Crowdsourcing has had a dramatic impact on the speed and scale at which scientific research can be conducted. Clinical scientists have particularly benefited from readily available research study participants and streamlined recruiting and payment systems afforded by Amazon Mechanical Turk (MTurk), a popular labor market for crowdsourcing workers. MTurk has been used in this capacity for more than five years. The popularity and novelty of the platform have spurred numerous methodological investigations, making it the most studied nonprobability sample available to researchers. This article summarizes what is known about MTurk sample composition and data quality with an emphasis on findings relevant to clinical psychological research. It then addresses methodological issues with using MTurk\u2014many of which are common to other nonprobability samples but unfamiliar to clinical science researchers\u00a0\u2026", "num_citations": "852\n", "authors": ["2069"]}
{"title": "An open, large-scale, collaborative effort to estimate the reproducibility of psychological science\n", "abstract": " Reproducibility is a defining feature of science. However, because of strong incentives for innovation and weak incentives for confirmation, direct replication is rarely practiced or published. The Reproducibility Project is an open, large-scale, collaborative effort to systematically examine the rate and predictors of reproducibility in psychological science. So far, 72 volunteer researchers from 41 institutions have organized to openly and transparently replicate studies published in three prominent psychological journals in 2008. Multiple methods will be used to evaluate the findings, calculate an empirical rate of replication, and investigate factors that predict reproducibility. Whatever the result, a better understanding of reproducibility will ultimately improve confidence in scientific methodology and findings.", "num_citations": "578\n", "authors": ["2069"]}
{"title": "Many Labs 2: Investigating variation in replicability across samples and settings\n", "abstract": " We conducted preregistered replications of 28 classic and contemporary published findings, with protocols that were peer reviewed in advance, to examine variation in effect magnitudes across samples and settings. Each protocol was administered to approximately half of 125 samples that comprised 15,305 participants from 36 countries and territories. Using the conventional criterion of statistical significance (p < .05), we found that 15 (54%) of the replications provided evidence of a statistically significant effect in the same direction as the original finding. With a strict significance criterion (p < .0001), 14 (50%) of the replications still provided such evidence, a reflection of the extremely high-powered design. Seven (25%) of the replications yielded effect sizes larger than the original ones, and 21 (75%) yielded effect sizes smaller than the original ones. The median comparable Cohen\u2019s ds were 0.60 for the original\u00a0\u2026", "num_citations": "551\n", "authors": ["2069"]}
{"title": "The average laboratory samples a population of 7,300 Amazon Mechanical Turk workers\n", "abstract": " Using capture-recapture analysis we estimate the effective size of the active Amazon Mechanical Turk (MTurk) population that a typical laboratory can access to be about 7,300 workers. We also estimate that the time taken for half of the workers to leave the MTurk pool and be replaced is about 7 months. Each laboratory has its own population pool which overlaps, often extensively, with the hundreds of other laboratories using MTurk. Our estimate is based on a sample of 114,460 completed sessions from 33,408 unique participants and 689 sessions across seven laboratories in the US, Europe, and Australia from January 2012 to March 2015.", "num_citations": "338\n", "authors": ["2069"]}
{"title": "Use does not wear ragged the fabric of friendship: Thinking of objects as alive makes people less willing to replace them\n", "abstract": " Anthropomorphic beliefs about objects lead people to treat them as if they were alive. Two experiments test how anthropomorphic thought affects consumers' product replacement intentions. Consumers induced to think about their car in anthropomorphic terms (i) were less willing to replace it and (ii) gave less weight to its quality when making replacement decisions. Instead, they (iii) attended to (experimentally induced connotations of) the car's \u201cwarmth,\u201d a feature usually considered relevant in the interpersonal domain. While anthropomorphic beliefs about brands are often seen as advantageous by marketers because they increase brand loyalty, similar beliefs about products may be less desirable.", "num_citations": "318\n", "authors": ["2069"]}
{"title": "Using nonnaive participants can reduce effect sizes\n", "abstract": " Although researchers often assume their participants are naive to experimental materials, this is not always the case. We investigated how prior exposure to a task affects subsequent experimental results. Participants in this study completed the same set of 12 experimental tasks at two points in time, first as a part of the Many Labs replication project and again a few days, a week, or a month later. Effect sizes were markedly lower in the second wave than in the first. The reduction was most pronounced when participants were assigned to a different condition in the second wave. We discuss the methodological implications of these findings.", "num_citations": "220\n", "authors": ["2069"]}
{"title": "Lie for a dime: When most prescreening responses are honest but most study participants are impostors\n", "abstract": " The Internet has enabled recruitment of large samples with specific characteristics. However, when researchers rely on participant self-report to determine eligibility, data quality depends on participant honesty. Across four studies on Amazon Mechanical Turk, we show that a substantial number of participants misrepresent theoretically relevant characteristics (e.g., demographics, product ownership) to meet eligibility criteria explicit in the studies, inferred by a previous exclusion from the study or inferred in previous experiences with similar studies. When recruiting rare populations, a large proportion of responses can be impostors. We provide recommendations about how to ensure that ineligible participants are excluded that are applicable to a wide variety of data collection efforts, which rely on self-report.", "num_citations": "193\n", "authors": ["2069"]}
{"title": "Crowdsourcing samples in cognitive science\n", "abstract": " Crowdsourcing data collection from research participants recruited from online labor markets is now common in cognitive science. We review who is in the crowd and who can be reached by the average laboratory. We discuss reproducibility and review some recent methodological innovations for online experiments. We consider the design of research studies and arising ethical issues. We review how to code experiments for the web, what is known about video and audio presentation, and the measurement of reaction times. We close with comments about the high levels of experience of many participants and an emerging tragedy of the commons.", "num_citations": "166\n", "authors": ["2069"]}
{"title": "Online panels in social science research: Expanding sampling methods beyond Mechanical Turk\n", "abstract": " Amazon Mechanical Turk (MTurk) is widely used by behavioral scientists to recruit research participants. MTurk offers advantages over traditional student subject pools, but it also has important limitations. In particular, the MTurk population is small and potentially overused, and some groups of interest to behavioral scientists are underrepresented and difficult to recruit. Here we examined whether online research panels can avoid these limitations. Specifically, we compared sample composition, data quality (measured by effect sizes, internal reliability, and attention checks), and the non-naivete of participants recruited from MTurk and Prime Panels\u2014an aggregate of online research panels. Prime Panels participants were more diverse in age, family composition, religiosity, education, and political attitudes. Prime Panels participants also reported less exposure to classic protocols and produced larger effect\u00a0\u2026", "num_citations": "163\n", "authors": ["2069"]}
{"title": "Common concerns with MTurk as a participant pool: Evidence and solutions.\n", "abstract": " Crowdsourced online samples, particularly those recruited from Amazon Mechanical Turk (MTurk), have become a popular source of research participants in the social sciences. If anything, consumer research has been leading the move towards crowdsourced samples. The popularity of crowdsourcing platforms among consumer researchers is driven by the advantages they have over other means of recruiting participants. Crowdsourcing platforms are simple to use and can recruit large samples of diverse participants quickly and at low cost. This chapter discusses common concerns that researcher have with MTurk, reviewing the evidence that bears upon each concern. In general, the available evidence shows both that high-quality data can be collected from MTurk and that high-quality data are by no means guaranteed. There are many pitfalls to collecting data from online respondents that researchers\u00a0\u2026", "num_citations": "151\n", "authors": ["2069"]}
{"title": "How extending your middle finger affects your perception of others: Learned movements influence concept accessibility\n", "abstract": " Body movements both express and influence how people feel and think. Conceptualizations of this bidirectional influence assume that movement\u2013concept associations can be innate or learned, although evidence for learned associations remained ambiguous. Providing a conservative test of learned movement\u2013concept associations, two studies investigate the influence of culture-specific body movements, which involve an arbitrary relationship between movements and associated concepts. Paralleling the influence of hostility primes, extending the middle finger influenced the interpretation of ambiguously aggressive behaviors as hostile, but did not influence unrelated trait judgments (Study 1). Paralleling the effects of global evaluative primes, upward extension of the thumb resulted in more positive evaluations of the same target along all trait dimensions and higher liking of the target (Study 2).", "num_citations": "144\n", "authors": ["2069"]}
{"title": "To judge a book by its weight you need to know its content: Knowledge moderates the use of embodied cues\n", "abstract": " Participants evaluated a book as more important when it weighed heavily in their hands (due to a concealed weight), but only when they had substantive knowledge about the book. Those who had read a synopsis (Study 1), had read the book (Study 2) and knew details about its plot (Study 3) were influenced by its weight, whereas those unfamiliar with the book were not. This contradicts the widely shared assumption that metaphorically related perceptual inputs serve as heuristic cues that people primarily use in the absence of more diagnostic information. Instead, perceptual inputs may increase the accessibility of metaphorically congruent knowledge or may suggest an initial hypothesis that is only endorsed when supporting information is accessible.", "num_citations": "117\n", "authors": ["2069"]}
{"title": "The pipeline project: Pre-publication independent replications of a single laboratory's research pipeline\n", "abstract": " This crowdsourced project introduces a collaborative approach to improving the reproducibility of scientific research, in which findings are replicated in qualified independent laboratories before (rather than after) they are published. Our goal is to establish a non-adversarial replication process with highly informative final results. To illustrate the Pre-Publication Independent Replication (PPIR) approach, 25 research groups conducted replications of all ten moral judgment effects which the last author and his collaborators had \u201cin the pipeline\u201d as of August 2014. Six findings replicated according to all replication criteria, one finding replicated but with a significantly smaller effect size than the original, one finding replicated consistently in the original culture but not outside of it, and two findings failed to find support. In total, 40% of the original findings failed at least one major replication criterion. Potential ways to\u00a0\u2026", "num_citations": "106\n", "authors": ["2069"]}
{"title": "Intertemporal differences among MTurk workers: Time-based sample variations and implications for online data collection\n", "abstract": " The online labor market Amazon Mechanical Turk (MTurk) is an increasingly popular source of respondents for social science research. A growing body of research has examined the demographic composition of MTurk workers as compared with that of other populations. While these comparisons have revealed the ways in which MTurk workers are and are not representative of the general population, variations among samples drawn from MTurk have received less attention. This article focuses on whether MTurk sample composition varies as a function of time. Specifically, we examine whether demographic characteristics vary by (a) time of day, (b) day of week, and serial position (i.e., earlier or later in data collection), both (c) across the entire data collection and (d) within specific batches. We find that day of week differences are minimal, but that time of day and serial position are associated with small but\u00a0\u2026", "num_citations": "78\n", "authors": ["2069"]}
{"title": "Warmer hearts, warmer rooms\n", "abstract": " Conceptual representations of warmth have been shown to be related to people\u2019s perceptions of ambient temperature. Based on this premise, we hypothesized that merely thinking about personality traits related to communion (but not agency) influences physical experience of warmth. Specifically, the three studies revealed that (a) perceptions of temperature are influenced by both positive and negative attributes within the communion but not agency dimension,(b) the effect is stronger when traits indicate sociability rather than morality subdimension of communion, and (c) communion activation affects temperature perceptions independently of target\u2019s or self-perceptions.", "num_citations": "67\n", "authors": ["2069"]}
{"title": "Risks and rewards of crowdsourcing marketplaces\n", "abstract": " Crowdsourcing has become an increasingly popular means of flexibly deploying large amounts of human computational power. The present chapter investigates the role of microtask labor marketplaces in managing human and hybrid human machine computing. Labor marketplaces offer many advantages that in combination allow human intelligence to be allocated across projects rapidly and efficiently and information to be transmitted effectively between market participants. Human computation comes with a set of challenges that are distinct from machine computation, including increased unsystematic error (e.g. mistakes) and systematic error (e.g. cognitive biases), both of which can be exacerbated when motivation is low, incentives are misaligned, and task requirements are poorly communicated. We provide specific guidance about how to ameliorate these issues through task design, workforce\u00a0\u2026", "num_citations": "66\n", "authors": ["2069"]}
{"title": "Fast Thought Speed Induces Risk Taking\n", "abstract": " In two experiments, we tested for a causal link between thought speed and risk taking. In Experiment 1, we manipulated thought speed by presenting neutral-content text at either a fast or a slow pace and having participants read the text aloud. In Experiment 2, we manipulated thought speed by presenting fast-, medium-, or slow-paced movie clips that contained similar content. Participants who were induced to think more quickly took more risks with actual money in Experiment 1 and reported greater intentions to engage in real-world risky behaviors, such as unprotected sex and illegal drug use, in Experiment 2. These experiments provide evidence that faster thinking induces greater risk taking.", "num_citations": "54\n", "authors": ["2069"]}
{"title": "Data from investigating variation in replicability: A \u201cmany labs\u201d replication project\n", "abstract": " This dataset is from the Many Labs Replication Project in which 13 effects were replicated across 36 samples and over 6,000 participants. Data from the replications are included, along with demographic variables about the participants and contextual information about the environment in which the replication was conducted. Data were collected in-lab and online through a standardized procedure administered via an online link. The dataset is stored on the Open Science Framework website. These data could be used to further investigate the results of the included 13 effects or to study replication and generalizability more broadly.", "num_citations": "44\n", "authors": ["2069"]}
{"title": "Lost in the crowd: Entitative group membership reduces mind attribution\n", "abstract": " This research examined how and why group membership diminishes the attribution of mind to individuals. We found that mind attribution was inversely related to the size of the group to which an individual belonged (Experiment 1). Mind attribution was affected by group membership rather than the total number of entities perceived at once (Experiment 2). Moreover, mind attribution to an individual varied with the perception that the individual was a group member. Participants attributed more mind to an individual that appeared distinct or distant from other group members than to an individual that was perceived to be similar or proximal to a cohesive group (Experiments 3 and 4). This effect occurred for both human and nonhuman targets, and was driven by the perception of the target as an entitative group member rather than by the knowledge that the target was an entitative group member (Experiment 5).", "num_citations": "42\n", "authors": ["2069"]}
{"title": "In the \u201cI\u201d of the storm: Shared initials increase disaster donations\n", "abstract": " People prefer their own initials to other letters, influencing preferences in many domains. The \u201cname letter effect\u201d(Nuttin, 1987) may not apply to negatively valenced targets if people are motivated to downplay or distance themselves from negative targets associated with the self, as previous research has shown (eg, Finch & Cialdini, 1989). In the current research we examine the relationship between same initial preferences and negatively valenced stimuli. Specifically, we examined donations to disaster relief after seven major hurricanes to test the influence of the name letter effect with negatively valenced targets. Individuals who shared an initial with the hurricane name were overrepresented among hurricane relief donors relative to the baseline distribution of initials in the donor population. This finding suggests that people may seek to ameliorate the negative effects of a disaster when there are shared characteristics between the disaster and the self.", "num_citations": "36\n", "authors": ["2069"]}
{"title": "Participant carelessness and fraud: Consequences for clinical research and potential solutions.\n", "abstract": " Clinical psychological research studies often require individuals with specific characteristics. The Internet can be used to recruit broadly, enabling the recruitment of rare groups such as people with specific psychological disorders. However, Internet-based research relies on participant self-report to determine eligibility, and thus, data quality depends on participant honesty. For those rare groups, even low levels of participant dishonesty can lead to a substantial proportion of fraudulent survey responses, and all studies will include careless respondents who do not pay attention to questions, do not understand them, or provide intentionally wrong responses. Poor-quality responses should be thought of as categorically different from high-quality responses. Including these responses will lead to the overestimation of the prevalence of rare groups and incorrect estimates of scale reliability, means, and correlations\u00a0\u2026", "num_citations": "32\n", "authors": ["2069"]}
{"title": "Online and on my mind: Temporary and chronic accessibility moderate the influence of media figures\n", "abstract": " To investigate the influence of media figures on self-perception, online gamers reported how central their main videogame character (avatar) is to their own identity and answered questions about their avatar's body size either before or after questions about their own body size. When the avatar was not central to the gamer's identity, the avatar's body size influenced gamer's own body judgments only when the avatar was brought to mind by preceding questions. When the avatar was central to the gamer's identity, it influenced gamers' own body judgments independent of question order. In both cases, accessible avatars elicited assimilation effects on self-judgment. We conclude that media figures exert a chronic influence on self-judgment when they are central to the self.", "num_citations": "32\n", "authors": ["2069"]}
{"title": "Theory building through replication: Response to commentaries on the \u201cMany Labs\u201d replication project.\n", "abstract": " Responds to the comments made by Monin and Oppenheimer (see record 2014-37961-001), Ferguson et al.(see record 2014-38072-001), Crisp et al.(see record 2014-38072-002), and Schwarz & Strack (see record 2014-38072-003) on the current authors original article (see record 2014-20922-002). The current authors thank the commentators for their productive discussion of the Many Labs project. They entirely agree with the main theme across the commentaries: direct replication does not guarantee that the same effect was tested. As noted by Nosek and Lakens (2014, p. 137),\u2018\u2018direct replication is the attempt to duplicate the conditions and procedure that existing theory and evidence anticipate as necessary for obtaining the effect.\u2019\u2019Attempting to do so does not guarantee success, but it does provide substantial opportunity for theoretical development building on empirical evidence.(PsycINFO Database Record\u00a0\u2026", "num_citations": "20\n", "authors": ["2069"]}
{"title": "Likeableness and meaningfulness ratings of 555 (+ 487) person-descriptive words\n", "abstract": " The present study renorms and expands upon a list of person descriptive words originally compiled by Anderson (1968). Anderson observed that person descriptive words had a bimodal and slightly negative distribution. Averill (1980) localized this negativity to emotion words, finding no difference for non-emotional words. Likeability ratings observed in the original study and the present study were highly correlated. Despite these similarities, significant differences in likability were observed across a large proportion of words. There was some evidence that words describing emotions and temporary states were unusually negative suggesting that either negative behavior is categorized differently or that the granularity of negative behavior differs across kinds of person descriptive words.", "num_citations": "17\n", "authors": ["2069"]}
{"title": "The choice architecture of school choice websites\n", "abstract": " We conducted a randomized factorial experiment to determine how displaying school information to parents in different ways affects what schools they choose for their children in a hypothetical school district. In a sample of 3,500 low-income parents of school-aged children, a small design manipulation, such as changing the default order in which schools were presented, induced meaningful changes in the types of schools selected. Other design choices such as using icons to represent data, instead of graphs or just numbers, or presenting concise summaries instead of detailed displays, also led parents to choose schools with higher academic performance. We also examined effects on parents\u2019 understanding of the information and their self-reported satisfaction and ease of use. In some cases, there were trade-offs. For example, representing data using only numbers maximized understanding, but adding graphs\u00a0\u2026", "num_citations": "11\n", "authors": ["2069"]}
{"title": "Do violent media numb our consciences?\n", "abstract": " Then shall we allow our children to listen to any story anyone happens to make up, and so receive into their minds ideas often the very opposite of those we shall think they ought to have when they are grown up?(Plato, 427\u2013347 BC)Throughout time, parents have worried about the stories their children might hear, fearing that some of these stories might have a negative influence on them. By listening to stories, children learn many things, including how to deal with conflict. In the past, many of the stories children heard came from parents, teachers, religious leaders, and media available at the time\u2013poems, songs, and stories. Today, many of the stories come from the mass media\u2013television, movies, video games, computer games, books and magazines, the Internet, and musical recordings.", "num_citations": "9\n", "authors": ["2069"]}
{"title": "Data from a pre-publication independent replication initiative examining ten moral judgement effects\n", "abstract": " We present the data from a crowdsourced project seeking to replicate findings in independent laboratories before (rather than after) they are published. In this Pre-Publication Independent Replication (PPIR) initiative, 25 research groups attempted to replicate 10 moral judgment effects from a single laboratory\u2019s research pipeline of unpublished findings. The 10 effects were investigated using online/lab surveys containing psychological manipulations (vignettes) followed by questionnaires. Results revealed a mix of reliable, unreliable, and culturally moderated findings. Unlike any previous replication project, this dataset includes the data from not only the replications but also from the original studies, creating a unique corpus that researchers can use to better understand reproducibility and irreproducibility in science.", "num_citations": "8\n", "authors": ["2069"]}
{"title": "Emailing workers using Python\n", "abstract": " While the Mechanical Turk web interface allows researchers to contact research participants individually, it is frequently necessary to email participants en masse. In this working paper we describe how to email up to 100 workers at a time using boto.", "num_citations": "6\n", "authors": ["2069"]}
{"title": "Nudging parents to choose better schools: The importance of school choice architecture\n", "abstract": " We conducted a randomized factorial experiment to determine how displaying school information to parents in different ways might affect what schools they choose for their children. In a sample of 3,500 low-income parents of school-aged children, we found that a small nudge, such as changing the default order in which schools were presented, could induce meaningful changes in the types of schools selected. Specifically, changing the default sort order from distance-from-home to academic performance resulted in parents choosing schools with higher academic performance. The academic performance of the average school selected was 5 percentile points higher, equivalent to 0.20 standard deviations. The change in sort order also led parents to choose schools that were more than half a mile farther from home (2.3 versus 1.7 miles, on average). Other design choices such as using icons to represent data, instead of graphs or just numbers, or presenting concise summaries instead of detailed displays, also led parents to choose schools with higher academic performance. We also examined effects of information display strategies on parents\u2019 understanding of the information and their self-reported satisfaction and ease of use. In some cases, there were trade-offs. For example, representing data using only numbers maximized understanding, but adding graphs maximized satisfaction at the expense of understanding.Many people contributed to this study. Eric Johnson provided important design advice early in the project. Phil Gleason read drafts and provided helpful comments. Daniel Kassler provided expert research assistance and Mariel\u00a0\u2026", "num_citations": "5\n", "authors": ["2069"]}
{"title": "Using Behavioral Science to Improve Survey Response: An Experiment with the National Beneficiary Survey\n", "abstract": " Much of the data on which policy and program decisions are based come from stakeholder surveys. We use these surveys to increase our understanding of characteristics, experiences, and behaviors when administrative data sources don\u2019t provide the kind of rich data we need. Unfortunately, convincing people to cooperate is increasingly difficult with the rise of do-not-call lists and the abandonment of landlines. Research firms have attempted to compensate for this decline in cooperation by investing in efforts to locate and engage potential respondents, but this increases the cost of survey data collection. However, design choices may improve cooperation without increasing cost. Findings from an experiment conducted as part of the National Beneficiary Survey (NBS) suggest one strategy to help on this front.The NBS, conducted for the Social Security Administration, collects periodic data on a nationally representative sample of Social Security program beneficiaries. Findings from this large-scale study help policymakers better understand beneficiaries\u2019 experiences with Social Security programs and identify areas for improvement. Mathematica has been conducting the NBS since its inception in 2004.", "num_citations": "5\n", "authors": ["2069"]}
{"title": "The cognitive and emotional consequences of anthropomorphic thought\n", "abstract": " Six experiments test how accessible anthropomorphic concepts affect thoughts and feelings about a variety of different objects (robots, vehicles and computers). Across these studies, people induced to think about objects in anthropomorphic terms (i) give less weight to product quality when making purchase and replacement decisions. Instead, they (ii) attend to features usually considered relevant in the interpersonal domain (such as neonatal features or connotations of\" warmth\"). Additionally,(iii) although people do not prefer new anthropomorphic products over non-anthropomorphized objects, they are generally more reluctant to replace anthropomorphized products that they already own, especially if they have an anxious interpersonal attachment style. Finally,(iv) people report experiencing more interpersonal emotions such as love and anger when thinking about anthropomorphized objects than when thinking\u00a0\u2026", "num_citations": "5\n", "authors": ["2069"]}
{"title": "Uniformity: The effects of organizational attire on judgments and attributions\n", "abstract": " Despite their prevalence in the marketplace, little empirical attention has been paid to how employee uniforms affect consumer reactions to service experiences. We propose that employee uniforms facilitate the shared categorization of employees and their organization in the mind of the customer, which affects many of the inferences that customers draw following service encounters. Study 1 shows that uniforms lead to greater attribution of responsibility to the company for employee behavior, especially following poor service. Studies 2 and 3 show that uniforms also lead to more assimilation of judgments across employees, increasing the impact of one employee's behavior on judgments of other employees of the same organization. Study 3 shows that employee uniforms lead to more extreme judgments of the company following service encounters. It also shows that bad (good) service from a uniformed employee\u00a0\u2026", "num_citations": "4\n", "authors": ["2069"]}
{"title": "Speaking on data\u2019s behalf: What researchers say and how audiences choose\n", "abstract": " Background:Bayesian statistics have become popular in the social sciences, in part because they are thought to present more useful information than traditional frequentist statistics. Unfortunately, little is known about whether or how interpretations of frequentist and Bayesian results differ.Objectives:We test whether presenting Bayesian or frequentist results based on the same underlying data influences the decisions people made.Research design:Participants were randomly assigned to read Bayesian and frequentist interpretations of hypothetical evaluations of new education technologies of various degrees of uncertainty, ranging from posterior probabilities of 99.8% to 52.9%, which have equivalent frequentist p values of .001 and .65, respectively.Subjects:Across three studies, 933 U.S. adults were recruited from Amazon Mechanical Turk.Measures:The primary outcome was the proportion of participants who\u00a0\u2026", "num_citations": "3\n", "authors": ["2069"]}
{"title": "Non-naivety among experimental participants on Amazon Mechanical Turk\n", "abstract": " Online labor markets allow \u201crequesters\u201d to recruit \u201cworkers\u201d for the completion of computer-based tasks. One such market, Amazon Mechanical Turk (AMT), offers a convenient means of accessing a relatively diverse population. The speed and ease with which data can be collected on AMT has led to considerable interest in using it to collect experimental data, as indicated by the large and growing number of publications that rely on AMT data over the past few years (> 400 in the social sciences alone) and self-reports by researchers subscribed to the major mailing lists in social psychology and decision-making (> 50% have used AMT). Initial evaluations of AMT as a source of data have emphasized its compelling strengths, notably the comparatively diversity of workers and the possibility of conducting research on a common population. In a nutshell, these studies have found that AMT workers produce quality data, and are more representative than other convenience samples.Fewer efforts have been made to explore and quantify potential unique drawbacks and limitations of using AMT to collect social science data. This special session focuses on some of the issues that threaten experimental validity on AMT and on providing easily implementable solutions to avoid these problems. The four papers included in the session deal with diverse issues. Joe Goodman discusses differences between AMT workers and more traditional subject populations that are of high relevance to consumer behavior research. Julie Downs discusses strategies for restricting data collection and data retention to attentive participants, together with their implications for the\u00a0\u2026", "num_citations": "3\n", "authors": ["2069"]}
{"title": "Online experimentation: amazon mechanical turk\n", "abstract": " Amazon Mechanical Turk has become increasingly popular among social scientists as a source of experimental data. Behavioral scientists have noticed the ease with which online workers can be recruited and paid using crowdsourcing services and have begun using Mechanical Turk intensively. We report the results of a comparative study involving classic experiments in judgment and decision-making, which found no differences in the magnitude of effects obtained using AMT and using traditional subject pools. We further discuss the advantages and drawbacks of AMT, with particular attention to those that seem to be overlooked by the academic community.", "num_citations": "3\n", "authors": ["2069"]}
{"title": "Direct replications in the era of open sampling\n", "abstract": " Data collection in psychology increasingly relies on \u201copen populations\u201d of participants recruited online, which presents both opportunities and challenges for replication. Reduced costs and the possibility to access the same populations allows for more informative replications. However, researchers should ensure the directness of their replications by dealing with the threats of participant nonnaivet\u00e9 and selection effects.", "num_citations": "2\n", "authors": ["2069"]}
{"title": "Responsibility, intent, and donor behavior: Commentary on who helps natural-disaster victims? Assessment of trait and situational predictors\n", "abstract": " Intentional harms are perceived as more painful and more deserving of compensation than unintentional harms. In conjunction with research demonstrating that people are poor judges of intent, this observation may explain why people are more willing to help victims whose suffering appears to be caused by others. This account further explains the authors\u2019 finding that people high in cognitive empathy are especially sensitive to other-caused harm, and aligns well with existing attributional accounts of why perceived victim responsibility reduces helping behavior. Finally, this account suggests a number of novel predictions about the determinants of donor behavior.Marjanovic, Struthers, and Greenglass (this issue) demonstrate that people are more likely to help disaster victims when other agents are judged at least partially responsible for their suffering. The authors explain their finding by suggesting that people feel that human relief is more effective in alleviating human-caused disasters than natural disasters. This may or may not be true\u2014one could just as easily argue that people think that those who fail to prepare for disasters are also likely to squander relief money. However, this explanation by itself seems incomplete for both theoretical and empirical reasons. Theoretically, this account ignores the attributional framework that is typically used to explain why people are less likely to help victims perceived as the cause of their own misfortunes (Rudolph, Roesch, Greitemeyer, & Weiner, 2004; Weiner, 1995). Empirically,", "num_citations": "2\n", "authors": ["2069"]}
{"title": "Surveying vocational rehabilitation applicants online: A feasibility study\n", "abstract": " Web surveys enable efficient data collection, but their usefulness is potentially limited when studying people with disabilities, who often lack Internet access. We test the feasibility of collecting web survey data from a sample of state vocational rehabilitation (VR) applicants, inviting nonrespondents to complete a telephone interview instead. People who lacked Internet access were provided with a mobile device and wireless access and were as likely to complete the web surveys as people who already had Internet access. Respondents who elected to complete the survey online versus by telephone differed in level of education and VR experience. These findings suggest that for disability studies, web surveys are an important supplement to, but not a replacement for, traditional data collection efforts.", "num_citations": "1\n", "authors": ["2069"]}
{"title": "Using Behavioral Science to Improve Survey Response: An Experiment with the National Beneficiary Survey (In Focus Brief)\n", "abstract": " This brief showcases how behavioral science can be used to boost call-in rates for surveys and increase our understanding of characteristics, experiences, and behaviors when administrative data sources don\u00e2\u20ac\u2122t provide the kind of rich data we need.", "num_citations": "1\n", "authors": ["2069"]}