{"title": "Encyclopedia of machine learning\n", "abstract": " This comprehensive encyclopedia, with over 250 entries in an AZ format, provides easy access to relevant information for those seeking entry into any aspect within the broad field of machine learning. Most entries in this preeminent work include useful literature references. Topics for the Encyclopedia of Machine Learning were selected by a distinguished international advisory board. These peer-reviewed, highly-structured entries include definitions, illustrations, applications, bibliographies and links to related literature, providing the reader with a portal to more detailed information on any given topic. The style of the entries in the Encyclopedia of Machine Learning is expository and tutorial, making the book a practical resource for machine learning experts, as well as professionals in other fields who need to access this vital information but may not have the time to work their way through an entire text on their topic of interest. The authoritative reference is published both in print and online. The print publication includes an index of subjects and authors. The online edition supplements this index with hyperlinks as well as internal hyperlinks to related entries in the text, CrossRef citations, and links to additional significant research.", "num_citations": "1343\n", "authors": ["1067"]}
{"title": "Multiboosting: A technique for combining boosting and wagging\n", "abstract": " MultiBoosting is an extension to the highly successful AdaBoost technique for forming decision committees. MultiBoosting can be viewed as combining AdaBoost with wagging. It is able to harness both AdaBoost's high bias and variance reduction with wagging's superior variance reduction. Using C4.5 as the base learning algorithm, MultiBoosting is demonstrated to produce decision committees with lower error than either AdaBoost or wagging significantly more often than the reverse over a large representative cross-section of UCI data sets. It offers the further advantage over AdaBoost of suiting parallel execution.", "num_citations": "832\n", "authors": ["1067"]}
{"title": "Not so naive Bayes: Aggregating one-dependence estimators\n", "abstract": " Of numerous proposals to improve the accuracy of naive Bayes by weakening its attribute independence assumption, both LBR and Super-Parent TAN have demonstrated remarkable error performance. However, both techniques obtain this outcome at a considerable computational cost. We present a new approach to weakening the attribute independence assumption by averaging all of a constrained class of classifiers. In extensive experiments this technique delivers comparable prediction accuracy to LBR and Super-Parent TAN with substantially improved computational efficiency at test time relative to the former and at training time relative to the latter. The new algorithm is shown to have low variance and is suited to incremental learning.", "num_citations": "805\n", "authors": ["1067"]}
{"title": "Machine learning for user modeling\n", "abstract": " At first blush, user modeling appears to be a prime candidate for straightforward application of standard machine learning techniques. Observations of the user's behavior can provide training examples that a machine learning system can use to form a model designed to predict future actions. However, user modeling poses a number of challenges for machine learning that have hindered its application in user modeling, including: the need for large data sets; the need for labeled data; concept drift; and computational complexity. This paper examines each of these issues and reviews approaches to resolving them.", "num_citations": "559\n", "authors": ["1067"]}
{"title": "Supervised descriptive rule discovery: A unifying survey of contrast set, emerging pattern and subgroup mining.\n", "abstract": " This paper gives a survey of contrast set mining (CSM), emerging pattern mining (EPM), and subgroup discovery (SD) in a unifying framework named supervised descriptive rule discovery. While all these research areas aim at discovering patterns in the form of rules induced from labeled data, they use different terminology and task definitions, claim to have different goals, claim to use different rule learning heuristics, and use different means for selecting subsets of induced patterns. This paper contributes a novel understanding of these subareas of data mining by presenting a unified terminology, by explaining the apparent differences between the learning tasks as variants of a unique supervised descriptive rule discovery task and by exploring the apparent differences between the approaches. It also shows that various rule learning heuristics used in CSM, EPM and SD algorithms all aim at optimizing a trade off between rule coverage and precision. The commonalities (and differences) between the approaches are showcased on a selection of best known variants of CSM, EPM and SD algorithms. The paper also provides a critical survey of existing supervised descriptive rule discovery visualization methods.", "num_citations": "473\n", "authors": ["1067"]}
{"title": "Encyclopedia of machine learning and data mining\n", "abstract": " This authoritative, expanded and updated second edition of Encyclopedia of Machine Learning and Data Mining provides easy access to core information for those seeking entry into any aspect within the broad field of Machine Learning and Data Mining. A paramount work, its 800 entries - about 150 of them newly updated or added - are filled with valuable literature references, providing the reader with a portal to more detailed information on any given topic. Topics for the Encyclopedia of Machine Learning and Data Mining include Learning and Logic, Data Mining, Applications, Text Mining, Statistical Learning, Reinforcement Learning, Pattern Mining, Graph Mining, Relational Mining, Evolutionary Computation, Information Theory, Behavior Cloning, and many others. Topics were selected by a distinguished international advisory board. Each peer-reviewed, highly-structured entry includes a definition, key words\u00a0\u2026", "num_citations": "378\n", "authors": ["1067"]}
{"title": "Lazy learning of Bayesian rules\n", "abstract": " The naive Bayesian classifier provides a simple and effective approach to classifier learning, but its attribute independence assumption is often violated in the real world. A number of approaches have sought to alleviate this problem. A Bayesian tree learning algorithm builds a decision tree, and generates a local naive Bayesian classifier at each leaf. The tests leading to a leaf can alleviate attribute inter-dependencies for the local naive Bayesian classifier. However, Bayesian tree learning still suffers from the small disjunct problem of tree learning. While inferred Bayesian trees demonstrate low average prediction error rates, there is reason to believe that error rates will be higher for those leaves with few training examples. This paper proposes the application of lazy learning techniques to Bayesian tree induction and presents the resulting lazy Bayesian rule learning algorithm, called LBR. This algorithm\u00a0\u2026", "num_citations": "347\n", "authors": ["1067"]}
{"title": "Discovering significant patterns\n", "abstract": " Pattern discovery techniques, such as association rule discovery, explore large search spaces of potential patterns to find those that satisfy some user-specified constraints. Due to the large number of patterns considered, they suffer from an extreme risk of type-1 error, that is, of finding patterns that appear due to chance alone to satisfy the constraints on the sample data. This paper proposes techniques to overcome this problem by applying well-established statistical practices. These allow the user to enforce a strict upper limit on the risk of experimentwise error. Empirical studies demonstrate that standard pattern discovery techniques can discover numerous spurious patterns when applied to random data and when applied to real-world data result in large numbers of patterns that are rejected when subjected to sound statistical evaluation. They also reveal that a number of pragmatic choices about how\u00a0\u2026", "num_citations": "326\n", "authors": ["1067"]}
{"title": "OPUS: An efficient admissible algorithm for unordered search\n", "abstract": " OPUS is a branch and bound search algorithm that enables efficient admissible search through spaces for which the order of search operator application is not significant. The algorithm's search efficiency is demonstrated with respect to very large machine learning search spaces. The use of admissible search is of potential value to the machine learning community as it means that the exact learning biases to be employed for complex learning tasks can be precisely specified and manipulated. OPUS also has potential for application in other areas of artificial intelligence, notably, truth maintenance.", "num_citations": "289\n", "authors": ["1067"]}
{"title": "Discretization for naive-Bayes learning: managing discretization bias and variance\n", "abstract": " Quantitative attributes are usually discretized in Naive-Bayes learning. We establish simple conditions under which discretization is equivalent to use of the true probability density function during naive-Bayes learning. The use of different discretization techniques can be expected to affect the classification bias and variance of generated naive-Bayes classifiers, effects we name discretization bias and variance. We argue that by properly managing discretization bias and variance, we can effectively reduce naive-Bayes classification error. In particular, we supply insights into managing discretization bias and variance by adjusting the number of intervals and the number of training instances contained in each interval. We accordingly propose proportional discretization and fixed frequency discretization, two efficient unsupervised discretization methods that are able to effectively manage discretization bias and\u00a0\u2026", "num_citations": "278\n", "authors": ["1067"]}
{"title": "Efficient search for association rules\n", "abstract": " This paper argues that for some applications direct search for association rules can be more efficient than the tw o stage process of the Apriori algorithm which first finds large itemsets whic hare then used to iden tifyassociations. In particular, it is argued, Apriori can impose large computational overheads when the number of frequen titemsets is very large. This will often be the case when association rule analysis is performed on domains other than basket analysis or when it is performed for basket analysis with basket information augmented by other customer information. An algorithm is presented that is computationally efficient for association rule analyses during which the n umber of rules to be found can be constrained and all data can be maintained in memory.", "num_citations": "259\n", "authors": ["1067"]}
{"title": "iFeature: a Python package and web server for features extraction and selection from protein and peptide sequences\n", "abstract": " Summary           Structural and physiochemical descriptors extracted from sequence data have been widely used to represent sequences and predict structural, functional, expression and interaction profiles of proteins and peptides as well as DNAs/RNAs. Here, we present iFeature, a versatile Python-based toolkit for generating various numerical feature representation schemes for both protein and peptide sequences. iFeature is capable of calculating and extracting a comprehensive spectrum of 18 major sequence encoding schemes that encompass 53 different types of feature descriptors. It also allows users to extract specific amino acid properties from the AAindex database. Furthermore, iFeature integrates 12 different types of commonly used feature clustering, selection and dimensionality reduction algorithms, greatly facilitating training, analysis and benchmarking of machine-learning models. The\u00a0\u2026", "num_citations": "254\n", "authors": ["1067"]}
{"title": "PROSPER: an integrated feature-based tool for predicting protease substrate cleavage sites\n", "abstract": " The ability to catalytically cleave protein substrates after synthesis is fundamental for all forms of life. Accordingly, site-specific proteolysis is one of the most important post-translational modifications. The key to understanding the physiological role of a protease is to identify its natural substrate(s). Knowledge of the substrate specificity of a protease can dramatically improve our ability to predict its target protein substrates, but this information must be utilized in an effective manner in order to efficiently identify protein substrates by in silico approaches. To address this problem, we present PROSPER, an integrated feature-based server for in silico identification of protease substrates and their cleavage sites for twenty-four different proteases. PROSPER utilizes established specificity information for these proteases (derived from the MEROPS database) with a machine learning approach to predict protease cleavage sites by using different, but complementary sequence and structure characteristics. Features used by PROSPER include local amino acid sequence profile, predicted secondary structure, solvent accessibility and predicted native disorder. Thus, for proteases with known amino acid specificity, PROSPER provides a convenient, pre-prepared tool for use in identifying protein substrates for the enzymes. Systematic prediction analysis for the twenty-four proteases thus far included in the database revealed that the features we have included in the tool strongly improve performance in terms of cleavage site prediction, as evidenced by their contribution to performance improvement in terms of identifying known cleavage sites in substrates for these\u00a0\u2026", "num_citations": "247\n", "authors": ["1067"]}
{"title": "A comparative study of discretization methods for naive-Bayes classifiers\n", "abstract": " Discretization is a popular approach to handling numeric attributes in machine learning. We argue that the requirements for effective discretization differ between naive-Bayes learning and many other learning algorithms. We evaluate the effectiveness with naive-Bayes classifiers of nine discretization methods, equal width discretization (EWD), equal frequency discretization (EFD), fuzzy discretization (FD), entropy minimization discretization (EMD), iterative discretization (ID), proportional k-interval discretization (PKID), lazy discretization (LD), nondisjoint discretization (NDD) and weighted proportional k-interval discretization (WPKID). It is found that in general naive-Bayes classifiers trained on data preprocessed by LD, NDD or WPKID achieve lower classification error than those trained on data preprocessed by the other discretization methods. But LD can not scale to large data. This study leads to a new discretization method, weighted non-disjoint discretization (WNDD) that combines WPKID and NDD\u2019s advantages. Our experiments show that among all the rival discretization methods, WNDD best helps naive-Bayes classifiers reduce average classification error.", "num_citations": "235\n", "authors": ["1067"]}
{"title": "Multistrategy ensemble learning: Reducing error by combining ensemble learning techniques\n", "abstract": " Ensemble learning strategies, especially boosting and bagging decision trees, have demonstrated impressive capacities to improve the prediction accuracy of base learning algorithms. Further gains have been demonstrated by strategies that combine simple ensemble formation approaches. We investigate the hypothesis that the improvement in accuracy of multistrategy approaches to ensemble learning is due to an increase in the diversity of ensemble members that are formed. In addition, guided by this hypothesis, we develop three new multistrategy ensemble learning techniques. Experimental results in a wide variety of natural domains suggest that these multistrategy ensemble learning techniques are, on average, more accurate than their component ensemble learning techniques.", "num_citations": "207\n", "authors": ["1067"]}
{"title": "Accurate in silico identification of species-specific acetylation sites by integrating protein sequence-derived and functional features\n", "abstract": " Lysine acetylation is a reversible post-translational modification, playing an important role in cytokine signaling, transcriptional regulation and apoptosis. To fully understand acetylation mechanisms, identification of substrates and specific acetylation sites is crucial. Experimental identification is often time-consuming and expensive. Alternative bioinformatics methods are cost-effective and can be used in a high-throughput manner to generate relatively precise predictions. Here we develop a method termed as SSPKA for species-specific lysine acetylation prediction, using random forest classifiers that combine sequence-derived and functional features with two-step feature selection. Feature importance analysis indicates functional features, applied for lysine acetylation site prediction for the first time, significantly improve the predictive performance. We apply the SSPKA model to screen the entire human proteome\u00a0\u2026", "num_citations": "204\n", "authors": ["1067"]}
{"title": "Alleviating naive Bayes attribute independence assumption by attribute weighting\n", "abstract": " Despite the simplicity of the Naive Bayes classifier, it has continued to perform well against more sophisticated newcomers and has remained, therefore, of great interest to the machine learning community. Of numerous approaches to refining the naive Bayes classifier, attribute weighting has received less attention than it warrants. Most approaches, perhaps influenced by attribute weighting in other machine learning algorithms, use weighting to place more emphasis on highly predictive attributes than those that are less predictive. In this paper, we argue that for naive Bayes attribute weighting should instead be used to alleviate the conditional independence assumption. Based on this premise, we propose a weighted naive Bayes algorithm, called WANBIA, that selects weights to minimize either the negative conditional log likelihood or the mean squared error objective functions. We perform extensive evaluations and find that WANBIA is a competitive alternative to state of the art classifiers like Random Forest, Logistic Regression and A1DE.", "num_citations": "190\n", "authors": ["1067"]}
{"title": "iProt-Sub: a comprehensive package for accurately mapping and predicting protease-specific substrates and cleavage sites\n", "abstract": " Regulation of proteolysis plays a critical role in a myriad of important cellular processes. The key to better understanding the mechanisms that control this process is to identify the specific substrates that each protease targets. To address this, we have developed iProt-Sub, a powerful bioinformatics tool for the accurate prediction of protease-specific substrates and their cleavage sites. Importantly, iProt-Sub represents a significantly advanced version of its successful predecessor, PROSPER. It provides optimized cleavage site prediction models with better prediction performance and coverage for more species-specific proteases (4 major protease families and 38 different proteases). iProt-Sub integrates heterogeneous sequence and structural features and uses a two-step feature selection procedure to further remove redundant and irrelevant features in an effort to improve the cleavage site prediction accuracy\u00a0\u2026", "num_citations": "175\n", "authors": ["1067"]}
{"title": "Cascleave: towards more accurate prediction of caspase substrate cleavage sites\n", "abstract": " Motivation: The caspase family of cysteine proteases play essential roles in key biological processes such as programmed cell death, differentiation, proliferation, necrosis and inflammation. The complete repertoire of caspase substrates remains to be fully characterized. Accordingly, systematic computational screening studies of caspase substrate cleavage sites may provide insight into the substrate specificity of caspases and further facilitating the discovery of putative novel substrates.                    Results: In this article we develop an approach (termed Cascleave) to predict both classical (i.e. following a P1 Asp) and non-typical caspase cleavage sites. When using local sequence-derived profiles, Cascleave successfully predicted 82.2% of the known substrate cleavage sites, with a Matthews correlation coefficient (MCC) of 0.667. We found that prediction performance could be further improved by\u00a0\u2026", "num_citations": "158\n", "authors": ["1067"]}
{"title": "On detecting differences between groups\n", "abstract": " Understanding the differences between contrasting groups is a fundamental task in data analysis. This realization has led to the development of a new special purpose data mining technique, contrast-set mining. We undertook a study with a retail collaborator to compare contrast-set mining with existing rule-discovery techniques. To our surprise we observed that straightforward application of an existing commercial rule-discovery system, Magnum Opus, could successfully perform the contrast-set-mining task. This led to the realization that contrast-set mining is a special case of the more general rule-discovery task. We present the results of our study together with a proof of this conclusion.", "num_citations": "153\n", "authors": ["1067"]}
{"title": "Further experimental evidence against the utility of Occam's razor\n", "abstract": " This paper presents new experimental evidence against the utility of Occam's razor. A~ systematic procedure is presented for post-processing decision trees produced by C4. 5. This procedure was derived by rejecting Occam's razor and instead attending to the assumption that similar objects are likely to belong to the same class. It increases a decision tree's complexity without altering the performance of that tree on the training data from which it is inferred. The resulting more complex decision trees are demonstrated to have, on average, for a variety of common learning tasks, higher predictive accuracy than the less complex original decision trees. This result raises considerable doubt about the utility of Occam's razor as it is commonly applied in modern machine learning.", "num_citations": "149\n", "authors": ["1067"]}
{"title": "Proportional k-interval discretization for naive-Bayes classifiers\n", "abstract": " This paper argues that two commonly-used discretization approaches, fixed k-interval discretization and entropy-based discretization have sub-optimal characteristics for naive-Bayes classification. This analysis leads to a new discretization method, Proportional k-Interval Discretization (PKID), which adjusts the number and size of discretized intervals to the number of training instances, thus seeks an appropriate trade-off between the bias and variance of the probability estimation for naive-Bayes classifiers. We justify PKID in theory, as well as test it on a wide cross-section of datasets. Our experimental results suggest that in comparison to its alternatives, PKID provides naive-Bayes classifiers competitive classification performance for smaller datasets and better classification performance for larger datasets.", "num_citations": "148\n", "authors": ["1067"]}
{"title": "Discovering associations with numeric variables\n", "abstract": " This paper further develops Aumann and Lindell's [3] proposal for a variant of association rules for which the consequent is a numeric variable. It is argued that these rules can discover useful interactions with numeric data that cannot be discovered directly using traditional association rules with discretization. Alternative measures for identifying interesting rules are proposed. Efficient algorithms are presented that enable these rules to be discovered for dense data sets for which application of Auman and Lindell's algorithm is infeasible.", "num_citations": "139\n", "authors": ["1067"]}
{"title": "K-optimal rule discovery\n", "abstract": " K-optimal rule discovery finds the K rules that optimize a user-specified measure of rule value with respect to a set of sample data and user-specified constraints. This approach avoids many limitations of the frequent itemset approach of association rule discovery. This paper presents a scalable algorithm applicable to a wide range of K-optimal rule discovery tasks and demonstrates its efficiency.", "num_citations": "136\n", "authors": ["1067"]}
{"title": "iLearn: an integrated platform and meta-learner for feature engineering, machine-learning analysis and modeling of DNA, RNA and protein sequence data\n", "abstract": " With the explosive growth of biological sequences generated in the post-genomic era, one of the most challenging problems in bioinformatics and computational biology is to computationally characterize sequences, structures and functions in an efficient, accurate and high-throughput manner. A number of online web servers and stand-alone tools have been developed to address this to date; however, all these tools have their limitations and drawbacks in terms of their effectiveness, user-friendliness and capacity. Here, we present iLearn, a comprehensive and versatile Python-based toolkit, integrating the functionality of feature extraction, clustering, normalization, selection, dimensionality reduction, predictor construction, best descriptor/model selection, ensemble learning and results visualization for DNA, RNA and protein sequences. iLearn was designed for users that only want to upload their data set and\u00a0\u2026", "num_citations": "134\n", "authors": ["1067"]}
{"title": "Discretization methods\n", "abstract": " Data-mining applications often involve quantitative data. However, learning from quantitative data is often less effective and less efficient than learning from qualitative data. Discretization addresses this issue by transforming quantitative data into qualitative data. This chapter presents a comprehensive introduction to discretization. It clarifies the definition of discretization. It provides a taxonomy of discretization methods together with a survey of major discretization methods. It also discusses issues that affect the design and application of discretization methods.", "num_citations": "132\n", "authors": ["1067"]}
{"title": "Decision tree grafting from the all-tests-but-one partition\n", "abstract": " Decision tree grafting adds nodes to an existing decision tree with the objective of reducing prediction error. A new grafting algorithm is presented that considers one set of training data only for each leaf of the initial decision tree, the set of cases that fail at most one test on the path to the leaf. This new technique is demonstrated to retain the error reduction power of the original grafting algorithm while dramatically reducing compute time and the complexity of the inferred tree. Bias/variance analyses reveal that the original grafting technique operated primarily by variance reduction while the new technique reduces both bias and variance.", "num_citations": "131\n", "authors": ["1067"]}
{"title": "GlycoMine: a machine learning-based approach for predicting N-, C-and O-linked glycosylation in the human proteome\n", "abstract": " Motivation: Glycosylation is a ubiquitous type of protein post-translational modification (PTM) in eukaryotic cells, which plays vital roles in various biological processes (BPs) such as cellular communication, ligand recognition and subcellular recognition. It is estimated that >50% of the entire human proteome is glycosylated. However, it is still a significant challenge to identify glycosylation sites, which requires expensive/laborious experimental research. Thus, bioinformatics approaches that can predict the glycan occupancy at specific sequons in protein sequences would be useful for understanding and utilizing this important PTM.                    Results: In this study, we present a novel bioinformatics tool called GlycoMine, which is a comprehensive tool for the systematic in silico identification of C-linked, N-linked, and O-linked glycosylation sites in the human proteome. GlycoMine was developed using the\u00a0\u2026", "num_citations": "127\n", "authors": ["1067"]}
{"title": "PROSPERous: high-throughput prediction of substrate cleavage sites for 90 proteases with improved accuracy\n", "abstract": " Summary           Proteases are enzymes that specifically cleave the peptide backbone of their target proteins. As an important type of irreversible post-translational modification, protein cleavage underlies many key physiological processes. When dysregulated, proteases\u2019 actions are associated with numerous diseases. Many proteases are highly specific, cleaving only those target substrates that present certain particular amino acid sequence patterns. Therefore, tools that successfully identify potential target substrates for proteases may also identify previously unknown, physiologically relevant cleavage sites, thus providing insights into biological processes and guiding hypothesis-driven experiments aimed at verifying protease\u2013substrate interaction. In this work, we present PROSPERous, a tool for rapid in silico prediction of protease-specific cleavage sites in substrate sequences. Our tool is based on\u00a0\u2026", "num_citations": "125\n", "authors": ["1067"]}
{"title": "Discovering significant rules\n", "abstract": " In many applications, association rules will only be interesting if they represent non-trivial correlations between all constituent items. Numerous techniques have been developed that seek to avoid false discoveries. However, while all provide useful solutions to aspects of this problem, none provides a generic solution that is both flexible enough to accommodate varying definitions of true and false discoveries and powerful enough to provide strict control over the risk of false discoveries. This paper presents generic techniques that allow definitions of true and false discoveries to be specified in terms of arbitrary statistical hypothesis tests and which provide strict control over the experiment wise risk of false discoveries.", "num_citations": "122\n", "authors": ["1067"]}
{"title": "On why discretization works for naive-Bayes classifiers\n", "abstract": " We investigate why discretization can be effective in naive-Bayes learning. We prove a theorem that identifies particular conditions under which discretization will result in naive-Bayes classifiers delivering the same probability estimates as would be obtained if the correct probability density functions were employed. We discuss the factors that might affect naive-Bayes classification error under discretization. We suggest that the use of different discretization techniques can affect the classification bias and variance of the generated classifiers. We argue that by properly managing discretization bias and variance, we can effectively reduce naive-Bayes classification error.", "num_citations": "122\n", "authors": ["1067"]}
{"title": "Adjusted probability naive Bayesian induction\n", "abstract": " Naive Bayesian classifiers utilise a simple mathematical model for induction. While it is known that the assumptions on which this model is based are frequently violated, the predictive accuracy obtained in discriminate classification tasks is surprisingly competitive in comparison to more complex induction techniques. Adjusted probability naive Bayesian induction adds a simple extension to the naive Bayesian classifier. A numeric weight is inferred for each class. During discriminate classification, the naive Bayesian probability of a class is multiplied by its weight to obtain an adjusted value. The use of this adjusted value in place of the naive Bayesian probability is shown to significantly improve predictive accuracy.", "num_citations": "122\n", "authors": ["1067"]}
{"title": "PREvaIL, an integrative approach for inferring catalytic residues using sequence, structural, and network features in a machine-learning framework\n", "abstract": " Determining the catalytic residues in an enzyme is critical to our understanding the relationship between protein sequence, structure, function, and enhancing our ability to design novel enzymes and their inhibitors. Although many enzymes have been sequenced, and their primary and tertiary structures determined, experimental methods for enzyme functional characterization lag behind. Because experimental methods used for identifying catalytic residues are resource- and labor-intensive, computational approaches have considerable value and are highly desirable for their ability to complement experimental studies in identifying catalytic residues and helping to bridge the sequence\u2013structure\u2013function gap. In this study, we describe a new computational method called PREvaIL for predicting enzyme catalytic residues. This method was developed by leveraging a comprehensive set of informative features\u00a0\u2026", "num_citations": "118\n", "authors": ["1067"]}
{"title": "Learning by extrapolation from marginal to full-multivariate probability distributions: Decreasingly naive Bayesian classification\n", "abstract": " Averaged n-Dependence Estimators (AnDE) is an approach to probabilistic classification learning that learns by extrapolation from marginal to full-multivariate probability distributions. It utilizes a single parameter that transforms the approach between a low-variance high-bias learner (Naive Bayes) and a high-variance low-bias learner with Bayes optimal asymptotic error. It extends the underlying strategy of Averaged One-Dependence Estimators (AODE), which relaxes the Naive Bayes independence assumption while retaining many of Naive Bayes\u2019 desirable computational and theoretical properties. AnDE further relaxes the independence assumption by generalizing AODE to higher-levels of dependence. Extensive experimental evaluation shows that the bias-variance trade-off for Averaged 2-Dependence Estimators results in strong predictive accuracy over a wide range of data sets. It has training time\u00a0\u2026", "num_citations": "115\n", "authors": ["1067"]}
{"title": "On the application of ROC analysis to predict classification performance under varying class distributions\n", "abstract": " We counsel caution in the application of ROC analysis for prediction of classifier performance under varying class distributions. We argue that it is not reasonable to expect ROC analysis to provide accurate prediction of model performance under varying distributions if the classes contain causally relevant subclasses whose frequencies may vary at different rates or if there are attributes upon which the classes are causally dependent.", "num_citations": "113\n", "authors": ["1067"]}
{"title": "Na\u00efve Bayes.\n", "abstract": " Na\u00efve Bayes is a simple learning algorithm that utilizes Bayes\u2019 rule together with a strong assumption that the attributes are conditionally independent given the class. While this independence assumption is often violated in practice, na\u00efve Bayes nonetheless often delivers competitive classification accuracy. Coupled with its computational efficiency and many other desirable features, this leads to na\u00efve Bayes being widely applied in practice.", "num_citations": "109\n", "authors": ["1067"]}
{"title": "A comparative study of semi-naive Bayes methods in classification learning\n", "abstract": " Numerous techniques have sought to improve the accuracy of Naive Bayes (NB) by alleviating the attribute interdependence problem. This paper summarizes these semi-naive Bayesian methods into two groups: those that apply conventional NB with a new attribute set, and those that alter NB by allowing inter-dependencies between attributes. We review eight typical semi-naive Bayesian learning algorithms and perform error analysis using the bias-variance decomposition on thirty-six natural domains from the UCI Machine Learning Repository. In analysing the results of these experiments we provide general recommendations for selection between methods.", "num_citations": "109\n", "authors": ["1067"]}
{"title": "POSSUM: a bioinformatics toolkit for generating numerical sequence feature descriptors based on PSSM profiles\n", "abstract": " Summary           Evolutionary information in the form of a Position-Specific Scoring Matrix (PSSM) is a widely used and highly informative representation of protein sequences. Accordingly, PSSM-based feature descriptors have been successfully applied to improve the performance of various predictors of protein attributes. Even though a number of algorithms have been proposed in previous studies, there is currently no universal web server or toolkit available for generating this wide variety of descriptors. Here, we present POSSUM (Position-Specific Scoring matrix-based feature generator for machine learning), a versatile toolkit with an online web server that can generate 21 types of PSSM-based feature descriptors, thereby addressing a crucial need for bioinformaticians and computational biologists. We envisage that this comprehensive toolkit will be widely used as a powerful tool to facilitate feature\u00a0\u2026", "num_citations": "106\n", "authors": ["1067"]}
{"title": "Self-sufficient itemsets: An approach to screening potentially interesting associations between items\n", "abstract": " Self-sufficient itemsets are those whose frequency cannot be explained solely by the frequency of either their subsets or of their supersets. We argue that itemsets that are not self-sufficient will often be of little interest to the data analyst, as their frequency should be expected once that of the itemsets on which their frequency depends is known. We present tests for statistically sound discovery of self-sufficient itemsets, and computational techniques that allow those tests to be applied as a post-processing step for any itemset discovery algorithm. We also present a measure for assessing the degree of potential interest in an itemset that complements these statistical measures.", "num_citations": "100\n", "authors": ["1067"]}
{"title": "On the effect of data set size on bias and variance in classification learning\n", "abstract": " With the advent of data mining, machine learning has come of age and is now a critical technology in many businesses. However, machine learning evolved in a different research context to that in which it now finds itself employed. A particularly important problem in the data mining world is working effectively with large data sets. However, most machine learning research has been conducted in the context of learning from very small data sets. To date most approaches to scaling up machine learning to large data sets have attempted to modify existing algorithms to deal with large data sets in a more computationally efficient and effective manner. But is this necessarily the best method? This paper explores the possibility of designing algorithms specifically for large data sets. Specifically, the paper looks at how increasing data set size affects bias and variance error decompositions for classification algorithms. Preliminary results of experiments to determine these effects are presented, showing that, as hypothesised variance can be expected to decrease as training set size increases. No clear effect of training set size on bias was observed. These results have profound implications for data mining from large data sets, indicating that developing effective learning algorithms for large data sets is not simply a matter of finding computationally efficient variants of existing learning algorithms.", "num_citations": "99\n", "authors": ["1067"]}
{"title": "Large-scale comparative assessment of computational predictors for lysine post-translational modification sites\n", "abstract": " Lysine post-translational modifications (PTMs) play a crucial role in regulating diverse functions and biological processes of proteins. However, because of the large volumes of sequencing data generated from genome-sequencing projects, systematic identification of different types of lysine PTM substrates and PTM sites in the entire proteome remains a major challenge. In recent years, a number of computational methods for lysine PTM identification have been developed. These methods show high diversity in their core algorithms, features extracted and feature selection techniques and evaluation strategies. There is therefore an urgent need to revisit these methods and summarize their methodologies, to improve and further develop computational techniques to identify and characterize lysine PTMs from the large amounts of sequence data. With this goal in mind, we first provide a comprehensive survey on a\u00a0\u2026", "num_citations": "77\n", "authors": ["1067"]}
{"title": "Efficient lazy elimination for averaged one-dependence estimators\n", "abstract": " Semi-naive Bayesian classifiers seek to retain the numerous strengths of naive Bayes while reducing error by relaxing the attribute independence assumption. Backwards Sequential Elimination (BSE) is a wrapper technique for attribute elimination that has proved effective at this task. We explore a new technique, Lazy Elimination (LE), which eliminates highly related attribute-values at classification time without the computational overheads inherent in wrapper techniques. We analyze the effect of LE and BSE on a state-of-the-art semi-naive Bayesian algorithm Averaged One-Dependence Estimators (AODE). Our experiments show that LE significantly reduces bias and error without undue computation, while BSE significantly reduces bias but not error, with high training time complexity. In the context of AODE, LE has a significant advantage over BSE in both computational efficiency and error.", "num_citations": "77\n", "authors": ["1067"]}
{"title": "Feature Based Modelling: A methodology for producing coherent, consistent, dynamically changing models of agents' competencies\n", "abstract": " Feature Based Modelling uses attribute value machine learning techniques to model an agent's competency. This is achieved by creating a model describing the relationships between the features of the agent's actions and of the contexts in which those actions are performed. This paper describes techniques that have been developed for creating these models and for extracting key information therefrom. An overview is provided of previous studies that have evaluated the application of Feature Based Modelling in a number of educational contexts including piano keyboard playing, the unification of Prolog terms and elementary subtraction. These studies have demonstrated that the approach is applicable to a wide spectrum of domains. Classroom use has demonstrated the low computational overheads of the technique. A new study of the application of the approach to modelling elementary subtraction\u00a0\u2026", "num_citations": "74\n", "authors": ["1067"]}
{"title": "Layered critical values: a powerful direct-adjustment approach to discovering significant patterns\n", "abstract": " Standard pattern discovery techniques, such as association rules, suffer an extreme risk of finding very large numbers of spurious patterns for many knowledge discovery tasks. The direct-adjustment approach to controlling this risk applies a statistical test during the discovery process, using a critical value adjusted to take account of the size of the search space. However, a problem with the direct-adjustment strategy is that it may discard numerous true patterns. This paper investigates the assignment of different critical values to different areas of the search space as an approach to alleviating this problem, using a variant of a technique originally developed for other purposes. This approach is shown to be effective at increasing the number of discoveries while still maintaining strict control over the risk of false discoveries.", "num_citations": "71\n", "authors": ["1067"]}
{"title": "Using decision trees for agent modeling: improving prediction performance\n", "abstract": " A modeling system may be required to predict an agent's future actions under constraints of inadequate or contradictory relevant historical evidence. This can result in low prediction accuracy, or otherwise, low prediction rates, leaving a set of cases for which no predictions are made. A previous study that explored techniques for improving prediction rates in the context of modeling students' subtraction skills using Feature Based Modeling showed a tradeoff between prediction rate and predication accuracy. This paper presents research that aims to improve prediction rates without affecting prediction accuracy. The FBM-C4.5 agent modeling system was used in this research. However, the techniques explored are applicable to any Feature Based Modeling system, and the most effective technique developed is applicable to most agent modeling systems. The default FBM-C4.5 system models agents'\u00a0\u2026", "num_citations": "71\n", "authors": ["1067"]}
{"title": "The need for low bias algorithms in classification learning from large data sets\n", "abstract": " This paper reviews the appropriateness for application to large data sets of standard machine learning algorithms, which were mainly developed in the context of small data sets. Sampling and parallelisation have proved useful means for reducing computation time when learning from large data sets. However, such methods assume that algorithms that were designed for use with what are now considered small data sets are also fundamentally suitable for large data sets. It is plausible that optimal learning from large data sets requires a different type of algorithm to optimal learning from small data sets. This paper investigates one respect in which data set size may affect the requirements of a learning algorithm \u2014 the bias plus variance decomposition of classification error. Experiments show that learning from large data sets may be more effective when using an algorithm that places greater emphasis on\u00a0\u2026", "num_citations": "70\n", "authors": ["1067"]}
{"title": "A novel selective na\u00efve Bayes algorithm\n", "abstract": " Na\u00efve Bayes is one of the most popular data mining algorithms. Its efficiency comes from the assumption of attribute independence, although this might be violated in many real-world data sets. Many efforts have been done to mitigate the assumption, among which attribute selection is an important approach. However, conventional efforts to perform attribute selection in na\u00efve Bayes suffer from heavy computational overhead. This paper proposes an efficient selective na\u00efve Bayes algorithm, which adopts only some of the attributes to construct selective na\u00efve Bayes models. These models are built in such a way that each one is a trivial extension of another. The most predictive selective na\u00efve Bayes model can be selected by the measures of incremental leave-one-out cross validation. As a result, attributes can be selected by efficient model selection. Empirical results demonstrate that the selective na\u00efve Bayes shows\u00a0\u2026", "num_citations": "69\n", "authors": ["1067"]}
{"title": "Twenty years of bioinformatics research for protease-specific substrate and cleavage site prediction: a comprehensive revisit and benchmarking of existing methods\n", "abstract": " The roles of proteolytic cleavage have been intensively investigated and discussed during the past two decades. This irreversible chemical process has been frequently reported to influence a number of crucial biological processes (BPs), such as cell cycle, protein regulation and inflammation. A number of advanced studies have been published aiming at deciphering the mechanisms of proteolytic cleavage. Given its significance and the large number of functionally enriched substrates targeted by specific proteases, many computational approaches have been established for accurate prediction of protease-specific substrates and their cleavage sites. Consequently, there is an urgent need to systematically assess the state-of-the-art computational approaches for protease-specific cleavage site prediction to further advance the existing methodologies and to improve the prediction performance. With this goal in\u00a0\u2026", "num_citations": "69\n", "authors": ["1067"]}
{"title": "Computational analysis and prediction of lysine malonylation sites by exploiting informative features in an integrative machine-learning framework\n", "abstract": " As a newly discovered post-translational modification (PTM), lysine malonylation (Kmal) regulates a myriad of cellular processes from prokaryotes to eukaryotes and has important implications in human diseases. Despite its functional significance, computational methods to accurately identify malonylation sites are still lacking and urgently needed. In particular, there is currently no comprehensive analysis and assessment of different features and machine learning (ML) methods that are required for constructing the necessary prediction models. Here, we review, analyze and compare 11 different feature encoding methods, with the goal of extracting key patterns and characteristics from residue sequences of Kmal sites. We identify optimized feature sets, with which four commonly used ML methods (random forest, support vector machines, K-nearest neighbor and logistic regression) and one recently proposed\u00a0\u2026", "num_citations": "67\n", "authors": ["1067"]}
{"title": "PhosphoPredict: A bioinformatics tool for prediction of human kinase-specific phosphorylation substrates and sites by integrating heterogeneous feature selection\n", "abstract": " Protein phosphorylation is a major form of post-translational modification (PTM) that regulates diverse cellular processes. In silico methods for phosphorylation site prediction can provide a useful and complementary strategy for complete phosphoproteome annotation. Here, we present a novel bioinformatics tool, PhosphoPredict, that combines protein sequence and functional features to predict kinase-specific substrates and their associated phosphorylation sites for 12 human kinases and kinase families, including ATM, CDKs, GSK-3, MAPKs, PKA, PKB, PKC, and SRC. To elucidate critical determinants, we identified feature subsets that were most informative and relevant for predicting substrate specificity for each individual kinase family. Extensive benchmarking experiments based on both five-fold cross-validation and independent tests indicated that the performance of PhosphoPredict is competitive with that of\u00a0\u2026", "num_citations": "65\n", "authors": ["1067"]}
{"title": "Weighted proportional k-interval discretization for naive-Bayes classifiers\n", "abstract": " The use of different discretization techniques can be expected to affect the classification bias and variance of naive-Bayes classifiers. We call such an effect discretization bias and variance. Proportional k-interval discretization (PKID) tunes discretization bias and variance by adjusting discretized interval size and number proportional to the number of training instances. Theoretical analysis suggests that this is desirable for naive-Bayes classifiers. However PKID is sub-optimal when learning from training data of small size. We argue that this is because PKID equally weighs bias reduction and variance reduction. But for small data, variance reduction can contribute more to lower learning error and thus should be given greater weight than bias reduction. Accordingly we propose weighted proportional k-interval discretization (WPKID), which establishes a more suitable bias and variance trade-off for small data\u00a0\u2026", "num_citations": "63\n", "authors": ["1067"]}
{"title": "Extremely fast decision tree\n", "abstract": " We introduce a novel incremental decision tree learning algorithm, Hoeffding Anytime Tree, that is statistically more efficient than the current state-of-the-art, Hoeffding Tree. We demonstrate that an implementation of Hoeffding Anytime Tree---\" Extremely Fast Decision Tree'', a minor modification to the MOA implementation of Hoeffding Tree---obtains significantly superior prequential accuracy on most of the largest classification datasets from the UCI repository. Hoeffding Anytime Tree produces the asymptotic batch tree in the limit, is naturally resilient to concept drift, and can be used as a higher accuracy replacement for Hoeffding Tree in most scenarios, at a small additional computational cost.", "num_citations": "62\n", "authors": ["1067"]}
{"title": "Implementation of Lazy Bayesian Rules in the Weka System\n", "abstract": " The implementation of lazy Bayesian rule in Weka system - DRO Home Library DRO home Submit research Contact DRO DRO The implementation of lazy Bayesian rule in Weka system Wang, Zhihai, Webb, Geoffrey and Dai, Honghua 2001, The implementation of lazy Bayesian rule in Weka system, in ISFST 2001 : Proceedings of the International Symposium on Future Software Technology, International Symposium on Future Software Technology, [ZhengZhou City, China], pp. 204-208. Attached Files Name Description MIMEType Size Downloads Title The implementation of lazy Bayesian rule in Weka system Author(s) Wang, Zhihai Webb, Geoffrey Dai, Honghua ORCID iD for Dai, Honghua orcid.org/0000-0001-9899-7029 Conference name International Symposium on Future Software Technology ISFST (2001: ZhengZhou City, China) Conference location ZhengZhou City, China Conference dates 5-8 Nov. 2001 \u2026", "num_citations": "60\n", "authors": ["1067"]}
{"title": "Lazy Bayesian Rules: A lazy semi-naive Bayesian learning technique competitive to boosting decision trees\n", "abstract": " Lbr is a lazy semi-naive Bayesian classifier learning technique, designed to alleviate the attribute interdependence problem of naive Bayesian classification. To classify a test example, it creates a conjunctive rule that selects a most appropriate subset of training examples and induces a local naive Bayesian classifier using this subset. Lbr can significantly improve the performance of the naive Bayesian classifier. A bias and variance analysis of Lbr reveals that it significantly reduces the bias of naive Bayesian classification at a cost of a slight increase in variance. It is interesting to compare this lazy technique with boosting and bagging, two well-known state-of-the-art non-lazy learning techniques. Empirical comparison of Lbr with boosting decision trees on discrete valued data shows that Lbr has, on average, significantly lower variance and higher bias. As a result of the interaction of these effects, the average prediction error of Lbr over a range of learning tasks is at...", "num_citations": "59\n", "authors": ["1067"]}
{"title": "GlycoMine struct: a new bioinformatics tool for highly accurate mapping of the human N-linked and O-linked glycoproteomes by incorporating structural features\n", "abstract": " Glycosylation plays an important role in cell-cell adhesion, ligand-binding and subcellular recognition. Current approaches for predicting protein glycosylation are primarily based on sequence-derived features, while little work has been done to systematically assess the importance of structural features to glycosylation prediction. Here, we propose a novel bioinformatics method called GlycoMine struct (http://glycomine. erc. monash. edu/Lab/GlycoMine_Struct/) for improved prediction of human N-and O-linked glycosylation sites by combining sequence and structural features in an integrated computational framework with a two-step feature-selection strategy. Experiments indicated that GlycoMine struct outperformed NGlycPred, the only predictor that incorporated both sequence and structure features, achieving AUC values of 0.941 and 0.922 for N-and O-linked glycosylation, respectively, on an independent test\u00a0\u2026", "num_citations": "57\n", "authors": ["1067"]}
{"title": "Subsumption resolution: an efficient and effective technique for semi-naive Bayesian learning\n", "abstract": " Semi-naive Bayesian techniques seek to improve the accuracy of naive Bayes (NB) by relaxing the attribute independence assumption. We present a new type of semi-naive Bayesian operation, Subsumption Resolution (SR), which efficiently identifies occurrences of the specialization-generalization relationship and eliminates generalizations at classification time. We extend SR to Near-Subsumption Resolution (NSR) to delete near\u2013generalizations in addition to generalizations. We develop two versions of SR: one that performs SR during training, called eager SR (ESR), and another that performs SR during testing, called lazy SR (LSR). We investigate the effect of ESR, LSR, NSR and conventional attribute elimination (BSE) on NB and Averaged One-Dependence Estimators (AODE), a powerful alternative to NB. BSE imposes very high training time overheads on NB and AODE accompanied by varying\u00a0\u2026", "num_citations": "57\n", "authors": ["1067"]}
{"title": "To select or to weigh: A comparative study of linear combination schemes for superparent-one-dependence estimators\n", "abstract": " We conduct a large-scale comparative study on linearly combining superparent-one-dependence estimators (SPODEs), a popular family of seminaive Bayesian classifiers. Altogether, 16 model selection and weighing schemes, 58 benchmark data sets, and various statistical tests are employed. This paper's main contributions are threefold. First, it formally presents each scheme's definition, rationale, and time complexity and hence can serve as a comprehensive reference for researchers interested in ensemble learning. Second, it offers bias-variance analysis for each scheme's classification error performance. Third, it identifies effective schemes that meet various needs in practice. This leads to accurate and fast classification algorithms which have an immediate and significant impact on real-world applications. Another important feature of our study is using a variety of statistical tests to evaluate multiple learning\u00a0\u2026", "num_citations": "54\n", "authors": ["1067"]}
{"title": "Feature-subspace aggregating: ensembles for stable and unstable learners\n", "abstract": " This paper introduces a new ensemble approach, Feature-Subspace Aggregating (Feating), which builds local models instead of global models. Feating is a generic ensemble approach that can enhance the predictive performance of both stable and unstable learners. In contrast, most existing ensemble approaches can improve the predictive performance of unstable learners only. Our analysis shows that the new approach reduces the execution time to generate a model in an ensemble through an increased level of localisation in Feating. Our empirical evaluation shows that Feating performs significantly better than Boosting, Random Subspace and Bagging in terms of predictive accuracy, when a stable learner SVM is used as the base learner. The speed up achieved by Feating makes feasible SVM ensembles that would otherwise be infeasible for large data sets. When SVM is the preferred base\u00a0\u2026", "num_citations": "53\n", "authors": ["1067"]}
{"title": "Positive-unlabelled learning of glycosylation sites in the human proteome\n", "abstract": " As an important type of post-translational modification (PTM), protein glycosylation plays a crucial role in protein stability and protein function. The abundance and ubiquity of protein glycosylation across three domains of life involving Eukarya, Bacteria and Archaea demonstrate its roles in regulating a variety of signalling and metabolic pathways. Mutations on and in the proximity of glycosylation sites are highly associated with human diseases. Accordingly, accurate prediction of glycosylation can complement laboratory-based methods and greatly benefit experimental efforts for characterization and understanding of functional roles of glycosylation. For this purpose, a number of supervised-learning approaches have been proposed to identify glycosylation sites, demonstrating a promising predictive performance. To train a conventional supervised-learning model, both reliable positive and negative samples are\u00a0\u2026", "num_citations": "52\n", "authors": ["1067"]}
{"title": "Filtered\u2010top\u2010k association discovery\n", "abstract": " Association mining has been one of the most intensively researched areas of data mining. However, direct uptake of the resulting technologies has been relatively low. This paper examines some of the reasons why the dominant paradigms in association mining have not lived up to their promise, and argues that a powerful alternative is provided by top\u2010k techniques coupled with appropriate statistical and other filtering. \u00a9 2011 John Wiley & Sons, Inc. WIREs Data Mining Knowl Discov 2011 1 183\u2013192 DOI: 10.1002/widm.28 This article is categorized under:  Algorithmic Development > Association Rules Algorithmic Development > Statistics Technologies > Association Rules", "num_citations": "52\n", "authors": ["1067"]}
{"title": "Ensemble selection for superparent-one-dependence estimators\n", "abstract": " SuperParent-One-Dependence Estimators (SPODEs) loosen Naive-Bayes\u2019 attribute independence assumption by allowing each attribute to depend on a common single attribute (superparent) in addition to the class. An ensemble of SPODEs is able to achieve high classification accuracy with modest computational cost. This paper investigates how to select SPODEs for ensembling. Various popular model selection strategies are presented. Their learning efficacy and efficiency are theoretically analyzed and empirically verified. Accordingly, guidelines are investigated for choosing between selection criteria in differing contexts.", "num_citations": "52\n", "authors": ["1067"]}
{"title": "An experimental evaluation of integrating machine learning with knowledge acquisition\n", "abstract": " Machine learning and knowledge acquisition from experts have distinct capabilities that appear to complement one another. We report a study that demonstrates the integration of these approaches can both improve the accuracy of the developed knowledge base and reduce development time. In addition, we found that users expected the expert systems created through the integrated approach to have higher accuracy than those created without machine learning and rated the integrated approach less difficult to use. They also provided favorable evaluations of both the specific integrated software, a system called The Knowledge Factory, and of the general value of machine learning for knowledge acquisition.", "num_citations": "52\n", "authors": ["1067"]}
{"title": "DeepCleave: a deep learning predictor for caspase and matrix metalloprotease substrates and cleavage sites\n", "abstract": " Motivation           Proteases are enzymes that cleave target substrate proteins by catalyzing the hydrolysis of peptide bonds between specific amino acids. While the functional proteolysis regulated by proteases plays a central role in the \u2018life and death\u2019 cellular processes, many of the corresponding substrates and their cleavage sites were not found yet. Availability of accurate predictors of the substrates and cleavage sites would facilitate understanding of proteases\u2019 functions and physiological roles. Deep learning is a promising approach for the development of accurate predictors of substrate cleavage events.                             Results           We propose DeepCleave, the first deep learning-based predictor of protease-specific substrates and cleavage sites. DeepCleave uses protein substrate sequence data as input and employs convolutional neural networks with transfer learning to train accurate predictive\u00a0\u2026", "num_citations": "51\n", "authors": ["1067"]}
{"title": "Efficient Discovery of the Most Interesting Associations\n", "abstract": " Self-sufficient itemsets have been proposed as an effective approach to summarizing the key associations in data. However, their computation appears highly demanding, as assessing whether an itemset is self-sufficient requires consideration of all pairwise partitions of the itemset into pairs of subsets as well as consideration of all supersets. This article presents the first published algorithm for efficiently discovering self-sufficient itemsets. This branch-and-bound algorithm deploys two powerful pruning mechanisms based on upper bounds on itemset value and statistical significance level. It demonstrates that finding top-k productive and nonredundant itemsets, with postprocessing to identify those that are not independently productive, can efficiently identify small sets of key associations. We present extensive evaluation of the strengths and limitations of the technique, including comparisons with alternative\u00a0\u2026", "num_citations": "50\n", "authors": ["1067"]}
{"title": "Comprehensive assessment and performance improvement of effector protein predictors for bacterial secretion systems III, IV and VI\n", "abstract": " Bacterial effector proteins secreted by various protein secretion systems play crucial roles in host\u2013pathogen interactions. In this context, computational tools capable of accurately predicting effector proteins of the various types of bacterial secretion systems are highly desirable. Existing computational approaches use different machine learning (ML) techniques and heterogeneous features derived from protein sequences and/or structural information. These predictors differ not only in terms of the used ML methods but also with respect to the used curated data sets, the features selection and their prediction performance. Here, we provide a comprehensive survey and benchmarking of currently available tools for the prediction of effector proteins of bacterial types III, IV and VI secretion systems (T3SS, T4SS and T6SS, respectively). We review core algorithms, feature selection techniques, tool availability and\u00a0\u2026", "num_citations": "49\n", "authors": ["1067"]}
{"title": "Scalable learning of Bayesian network classifiers\n", "abstract": " Ever increasing data quantity makes ever more urgent the need for highly scalable learners that have good classification performance. Therefore, an out-of-core learner with excellent time and space complexity, along with high expressivity (that is, capacity to learn very complex multivariate probability distributions) is extremely desirable. This paper presents such a learner. We propose an extension to the k-dependence Bayesian classifier (KDB) that discriminatively selects a sub-model of a full KDB classifier. It requires only one additional pass through the training data, making it a three-pass learner. Our extensive experimental evaluation on 16 large data sets reveals that this out-of-core algorithm achieves competitive classification performance, and substantially better training and classification time than state-of-the-art in-core learners such as random forest and linear and non-linear logistic regression.", "num_citations": "49\n", "authors": ["1067"]}
{"title": "Classifying under computational resource constraints: anytime classification using probabilistic estimators\n", "abstract": " In many online applications of machine learning, the computational resources available for classification will vary from time to time. Most techniques are designed to operate within the constraints of the minimum expected resources and fail to utilize further resources when they are available. We propose a novel anytime classification algorithm, anytime averaged probabilistic estimators (AAPE), which is capable of delivering strong prediction accuracy with little CPU time and utilizing additional CPU time to increase classification accuracy. The idea is to run an ordered sequence of very efficient Bayesian probabilistic estimators (single improvement steps) until classification time runs out. Theoretical studies and empirical validations reveal that by properly identifying, ordering, invoking and ensembling single improvement steps, AAPE is able to accomplish accurate classification whenever it is interrupted. It is\u00a0\u2026", "num_citations": "49\n", "authors": ["1067"]}
{"title": "The efficacy of a low-level program visualization tool for teaching programming concepts to novice C programmers\n", "abstract": " It is widely agreed that learning to program is difficult. Program visualization tools make visible aspects of program execution which are often hidden from the user. While several program visualization tools aimed at novice programmers have been developed over the past decade there is little empirical evidence showing that novices actually benefit from their use [1]. In this article we describe a \u201cGlass-box Interpreter\u201d called Bradman. An experiment is presented which tests the efficacy of Bradman in assisting novice programmers learn programming concepts. We show that students that used the glass-box interpreter achieved greater understanding of some programming concepts than those without access. We also give evidence that the student's ability to assimilate new concepts was enhanced by exposure to the glass-box interpreter. This is experimental confirmation that such tools are beneficial in helping\u00a0\u2026", "num_citations": "48\n", "authors": ["1067"]}
{"title": "Decision tree grafting\n", "abstract": " This paper extends recent work on decision tree grafting. Grafting is an inductive process that adds nodes to inferred decision trees. This process is demonstrated to frequently improve predictive accuracy. Superficial analysis might suggest that decision tree grafting is the direct reverse of pruning. To the contrary, it is argued that the two processes are complementary. This is because, like standard tree growing techniques, pruning uses only local information, whereas grafting uses non-local information. The use of both pruning and grafting in conjunction is demonstrated to provide the best general predictive accuracy over a representative selection of learning tasks.", "num_citations": "47\n", "authors": ["1067"]}
{"title": "Comprehensive review and assessment of computational methods for predicting RNA post-transcriptional modification sites from RNA sequences\n", "abstract": " RNA post-transcriptional modifications play a crucial role in a myriad of biological processes and cellular functions. To date, more than 160 RNA modifications have been discovered; therefore, accurate identification of RNA-modification sites is fundamental for a better understanding of RNA-mediated biological functions and mechanisms. However, due to limitations in experimental methods, systematic identification of different types of RNA-modification sites remains a major challenge. Recently, more than 20 computational methods have been developed to identify RNA-modification sites in tandem with high-throughput experimental methods, with most of these capable of predicting only single types of RNA-modification sites. These methods show high diversity in their dataset size, data quality, core algorithms, features extracted and feature selection techniques and evaluation strategies. Therefore, there is an\u00a0\u2026", "num_citations": "46\n", "authors": ["1067"]}
{"title": "Systematic analysis and prediction of type IV secreted effector proteins by machine learning approaches\n", "abstract": " In the course of infecting their hosts, pathogenic bacteria secrete numerous effectors, namely, bacterial proteins that pervert host cell biology. Many Gram-negative bacteria, including context-dependent human pathogens, use a type IV secretion system (T4SS) to translocate effectors directly into the cytosol of host cells. Various type IV secreted effectors (T4SEs) have been experimentally validated to play crucial roles in virulence by manipulating host cell gene expression and other processes. Consequently, the identification of novel effector proteins is an important step in increasing our understanding of host\u2013pathogen interactions and bacterial pathogenesis. Here, we train and compare six machine learning models, namely, Na\u00efve Bayes (NB), K-nearest neighbor (KNN), logistic regression (LR), random forest (RF), support vector machines (SVMs) and multilayer perceptron (MLP), for the identification of T4SEs\u00a0\u2026", "num_citations": "43\n", "authors": ["1067"]}
{"title": "RCPdb: An evolutionary classification and codon usage database for repeat-containing proteins\n", "abstract": " Over 3% of human proteins contain single amino acid repeats (repeat-containing proteins, RCPs). Many repeats (homopeptides) localize to important proteins involved in transcription, and the expansion of certain repeats, in particular poly-Q and poly-A tracts, can also lead to the development of neurological diseases. Previous studies have suggested that the homopeptide makeup is a result of the presence of G+C-rich tracts in the encoding genes and that expansion occurs via replication slippage. Here, we have performed a large-scale genomic analysis of the variation of the genes encoding RCPs in 13 species and present these data in an online database (http://repeats.med.monash.edu.au/genetic_analysis/). This resource allows rapid comparison and analysis of RCPs, homopeptides, and their underlying genetic tracts across the eukaryotic species considered. We report three major findings. First, there is a\u00a0\u2026", "num_citations": "42\n", "authors": ["1067"]}
{"title": "Stochastic attribute selection committees\n", "abstract": " Classifier committee learning methods generate multiple classifiers to form a committee by repeated application of a single base learning algorithm. The committee members vote to decide the final classification. Two such methods, Bagging and Boosting, have shown great success with decision tree learning. They create different classifiers by modifying the distribution of the training set. This paper studies a different approach: Stochastic Attribute Selection Committee learning of decision trees. It generates classifier committees by stochastically modifying the set of attributes but keeping the distribution of the training set unchanged. An empirical evaluation of a variant of this method, namely Sasc, in a representative collection of natural domains shows that the SASC method can significantly reduce the error rate of decision tree learning. On average Sasc is more accurate than Bagging and less accurate than\u00a0\u2026", "num_citations": "42\n", "authors": ["1067"]}
{"title": "Prodepth: predict residue depth by support vector regression approach from protein sequences only\n", "abstract": " Residue depth (RD) is a solvent exposure measure that complements the information provided by conventional accessible surface area (ASA) and describes to what extent a residue is buried in the protein structure space. Previous studies have established that RD is correlated with several protein properties, such as protein stability, residue conservation and amino acid types. Accurate prediction of RD has many potentially important applications in the field of structural bioinformatics, for example, facilitating the identification of functionally important residues, or residues in the folding nucleus, or enzyme active sites from sequence information. In this work, we introduce an efficient approach that uses support vector regression to quantify the relationship between RD and protein sequence. We systematically investigated eight different sequence encoding schemes including both local and global sequence characteristics and examined their respective prediction performances. For the objective evaluation of our approach, we used 5-fold cross-validation to assess the prediction accuracies and showed that the overall best performance could be achieved with a correlation coefficient (CC) of 0.71 between the observed and predicted RD values and a root mean square error (RMSE) of 1.74, after incorporating the relevant multiple sequence features. The results suggest that residue depth could be reliably predicted solely from protein primary sequences: local sequence environments are the major determinants, while global sequence features could influence the prediction performance marginally. We highlight two examples as a comparison in order to\u00a0\u2026", "num_citations": "40\n", "authors": ["1067"]}
{"title": "Solving regression problems using competitive ensemble models\n", "abstract": " The use of ensemble models in many problem domains has increased significantly in the last fewyears. The ensemble modeling, in particularly boosting, has shown a great promise in improving predictive performance of a model. Combining the ensemble members is normally done in a co-operative fashion where each of the ensemble members performs the same task and their predictions are aggregated to obtain the improved performance. However, it is also possible to combine the ensemble members in a competitive fashion where the best prediction of a relevant ensemble member is selected for a particular input. This option has been previously somewhat overlooked. The aim of this article is to investigate and compare the competitive and co-operative approaches to combining the models in the ensemble. A comparison is made between a competitive ensemble model and that of MARS with bagging\u00a0\u2026", "num_citations": "40\n", "authors": ["1067"]}
{"title": "Crysalis: an integrated server for computational analysis and design of protein crystallization\n", "abstract": " The failure of multi-step experimental procedures to yield diffraction-quality crystals is a major bottleneck in protein structure determination. Accordingly, several bioinformatics methods have been successfully developed and employed to select crystallizable proteins. Unfortunately, the majority of existing in silico methods only allow the prediction of crystallization propensity, seldom enabling computational design of protein mutants that can be targeted for enhancing protein crystallizability. Here, we present Crysalis, an integrated crystallization analysis tool that builds on support-vector regression (SVR) models to facilitate computational protein crystallization prediction, analysis, and design. More specifically, the functionality of this new tool includes:(1) rapid selection of target crystallizable proteins at the proteome level,(2) identification of site non-optimality for protein crystallization and systematic analysis of all\u00a0\u2026", "num_citations": "39\n", "authors": ["1067"]}
{"title": "TANGLE: two-level support vector regression approach for protein backbone torsion angle prediction from primary sequences\n", "abstract": " Protein backbone torsion angles (Phi) and (Psi) involve two rotation angles rotating around the C\u03b1-N bond (Phi) and the C\u03b1-C bond (Psi). Due to the planarity of the linked rigid peptide bonds, these two angles can essentially determine the backbone geometry of proteins. Accordingly, the accurate prediction of protein backbone torsion angle from sequence information can assist the prediction of protein structures. In this study, we develop a new approach called TANGLE (Torsion ANGLE predictor) to predict the protein backbone torsion angles from amino acid sequences. TANGLE uses a two-level support vector regression approach to perform real-value torsion angle prediction using a variety of features derived from amino acid sequences, including the evolutionary profiles in the form of position-specific scoring matrices, predicted secondary structure, solvent accessibility and natively disordered region as well as other global sequence features. When evaluated based on a large benchmark dataset of 1,526 non-homologous proteins, the mean absolute errors (MAEs) of the Phi and Psi angle prediction are 27.8\u00b0 and 44.6\u00b0, respectively, which are 1% and 3% respectively lower than that using one of the state-of-the-art prediction tools ANGLOR. Moreover, the prediction of TANGLE is significantly better than a random predictor that was built on the amino acid-specific basis, with the p-value<1.46e-147 and 7.97e-150, respectively by the Wilcoxon signed rank test. As a complementary approach to the current torsion angle prediction algorithms, TANGLE should prove useful in predicting protein structural properties and assisting protein fold\u00a0\u2026", "num_citations": "38\n", "authors": ["1067"]}
{"title": "Structural and dynamic properties that govern the stability of an engineered fibronectin type III domain\n", "abstract": " Consensus protein design is a rapid and reliable technique for the improvement of protein stability, which relies on the use of homologous protein sequences. To enhance the stability of a fibronectin type III (FN3) domain, consensus design was employed using an alignment of 2123 sequences. The resulting FN3 domain, FN3con, has unprecedented stability, with a melting temperature >100\u00b0C, a \u0394GD\u2212N of 15.5 kcal mol\u22121 and a greatly reduced unfolding rate compared with wild-type. To determine the underlying molecular basis for stability, an X-ray crystal structure of FN3con was determined to 2.0 \u00c5 and compared with other FN3 domains of varying stabilities. The structure of FN3con reveals significantly increased salt bridge interactions that are cooperatively networked, and a highly optimized hydrophobic core. Molecular dynamics simulations of FN3con and comparison structures show the cooperative\u00a0\u2026", "num_citations": "37\n", "authors": ["1067"]}
{"title": "Incremental discretization for naive-Bayes classifier\n", "abstract": " Na\u00efve-Bayes classifiers (NB) support incremental learning. However, the lack of effective incremental discretization methods has been hindering NB\u2019s incremental learning in face of quantitative data. This problem is further compounded by the fact that quantitative data are everywhere, from temperature readings to share prices. In this paper, we present a novel incremental discretization method for NB, incremental flexible frequency discretization (IFFD). IFFD discretizes values of a quantitative attribute into a sequence of intervals of flexible sizes. It allows online insertion and splitting operation on intervals. Theoretical analysis and experimental test are conducted to compare IFFD with alternative methods. Empirical evidence suggests that IFFD is efficient and effective. NB coupled with IFFD achieves a rapport between high learning efficiency and high classification accuracy in the context of incremental\u00a0\u2026", "num_citations": "37\n", "authors": ["1067"]}
{"title": "Mining negative rules using GRD\n", "abstract": " GRD is an algorithm for k-most interesting rule discovery. In contrast to association rule discovery, GRD does not require the use of a minimum support constraint. Rather, the user must specify a measure of interestingness and the number of rules sought (k). This paper reports efficient techniques to extend GRD to support mining of negative rules. We demonstrate that the new approach provides tractable discovery of both negative and positive rules.", "num_citations": "36\n", "authors": ["1067"]}
{"title": "Non-disjoint discretization for naive-Bayes classifiers\n", "abstract": " Previous discretization techniques have discretized numeric attributes into disjoint intervals. We argue that this is neither necessary nor appropriate for naive-Bayes classifiers. The analysis leads to a new discretization method, Non-Disjoint Discretization (NDD). NDD forms overlapping intervals for a numeric attribute, always locating a value toward the middle of an interval to obtain more reliable probability estimation. It also adjusts the number and size of discretized intervals to the number of training instances, seeking an appropriate trade-off between bias and variance of probability estimation. We justify NDD in theory and test it on a wide cross-section of datasets. Our experimental results suggest that for naive-Bayes classifiers, NDD works better than alternative discretization approaches.", "num_citations": "34\n", "authors": ["1067"]}
{"title": "Systematic search for categorical attribute-value data-driven machine learning\n", "abstract": " Optimal Pruning for Unordered Search is a search algorithm that enables complete search through the space of possible disjuncts at the inner level of a covering algorithm. This algorithm takes as inputs an evaluation function, e, a training set, t, and a set of specialisation operators, o. It outputs a set of operators from o that creates a classifier that maximises e with respect to t. While OPUS has exponential worst case time complexity, the algorithm is demonstrated to reach solutions for complex real world domains within reasonable time frames. Indeed, for some domains, the algorithm exhibits greater computational efficiency than common heuristic search algorithms.", "num_citations": "33\n", "authors": ["1067"]}
{"title": "SecretEPDB: a comprehensive web-based resource for secreted effector proteins of the bacterial types III, IV and VI secretion systems\n", "abstract": " Bacteria translocate effector molecules to host cells through highly evolved secretion systems. By definition, the function of these effector proteins is to manipulate host cell biology and the sequence, structural and functional annotations of these effector proteins will provide a better understanding of how bacterial secretion systems promote bacterial survival and virulence. Here we developed a knowledgebase, termed SecretEPDB (Bacterial Secreted Effector Protein DataBase), for effector proteins of type III secretion system (T3SS), type IV secretion system (T4SS) and type VI secretion system (T6SS). SecretEPDB provides enriched annotations of the aforementioned three classes of effector proteins by manually extracting and integrating structural and functional information from currently available databases and the literature. The database is conservative and strictly curated to ensure that every effector protein entry\u00a0\u2026", "num_citations": "32\n", "authors": ["1067"]}
{"title": "Bioinformatic approaches for predicting substrates of proteases\n", "abstract": " Proteases have central roles in \"life and death\" processes due to their important ability to catalytically hydrolyze protein substrates, usually altering the function and/or activity of the target in the process. Knowledge of the substrate specificity of a protease should, in theory, dramatically improve the ability to predict target protein substrates. However, experimental identification and characterization of protease substrates is often difficult and time-consuming. Thus solving the \"substrate identification\" problem is fundamental to both understanding protease biology and the development of therapeutics that target specific protease-regulated pathways. In this context, bioinformatic prediction of protease substrates may provide useful and experimentally testable information about novel potential cleavage sites in candidate substrates. In this article, we provide an overview of recent advances in developing bioinformatic\u00a0\u2026", "num_citations": "32\n", "authors": ["1067"]}
{"title": "Comparison of lazy Bayesian rule, and tree-augmented Bayesian learning\n", "abstract": " The naive Bayes classifier is widely used in interactive applications due to its computational efficiency, direct theoretical base, and competitive accuracy. However its attribute independence assumption can result in sub-optimal accuracy. A number of techniques have explored simple relaxations of the attribute independence assumption in order to increase accuracy. Among these, the lazy Bayesian rule (LBR) and the tree-augmented naive Bayes (TAN) have demonstrated strong prediction accuracy. However their relative performance has never been evaluated. The paper compares and contrasts these two techniques, finding that they have comparable accuracy and hence should be selected according to computational profile. LBR is desirable when small numbers of objects are to be classified while TAN is desirable when large numbers of objects are to be classified.", "num_citations": "32\n", "authors": ["1067"]}
{"title": "Identifying markers of pathology in SAXS data of malignant tissues of the brain\n", "abstract": " Conventional neuropathological analysis for brain malignancies is heavily reliant on the observation of morphological abnormalities, observed in thin, stained sections of tissue. Small Angle X-ray Scattering (SAXS) data provide an alternative means of distinguishing pathology by examining the ultra-structural (nanometer length scales) characteristics of tissue. To evaluate the diagnostic potential of SAXS for brain tumors, data was collected from normal, malignant and benign tissues of the human brain at station 2.1 of the Daresbury Laboratory Synchrotron Radiation Source and subjected to data mining and multivariate statistical analysis. The results suggest SAXS data may be an effective classifier of malignancy.", "num_citations": "31\n", "authors": ["1067"]}
{"title": "Efficient large-scale protein sequence comparison and gene matching to identify orthologs and co-orthologs\n", "abstract": " Broadly, computational approaches for ortholog assignment is a three steps process: (i) identify all putative homologs between the genomes, (ii) identify gene anchors and (iii) link anchors to identify best gene matches given their order and context. In this article, we engineer two methods to improve two important aspects of this pipeline [specifically steps (ii) and (iii)]. First, computing sequence similarity data [step (i)] is a computationally intensive task for large sequence sets, creating a bottleneck in the ortholog assignment pipeline. We have designed a fast and highly scalable sort-join method (afree) based on              k             -mer counts to rapidly compare all pairs of sequences in a large protein sequence set to identify putative homologs. Second, availability of complex genomes containing large gene families with prevalence of complex evolutionary events, such as duplications, has made the task of\u00a0\u2026", "num_citations": "30\n", "authors": ["1067"]}
{"title": "A comparative study of bandwidth choice in kernel density estimation for naive Bayesian classification\n", "abstract": " Kernel density estimation (KDE) is an important method in nonparametric learning. While KDE has been studied extensively in the context of accuracy of distribution estimation, it has not been studied extensively in the context of classification. This paper studies nine bandwidth selection schemes for kernel density estimation in Naive Bayesian classification context, using 52 machine learning benchmark datasets. The contributions of this paper are threefold. First, it shows that some commonly used and very sophisticated bandwidth selection schemes do not give good performance in Naive Bayes. Surprisingly, some very simple bandwidth selection schemes give statistically significantly better performance. Second, it shows that kernel density estimation can achieve statistically significantly better classification performance than a commonly used discretization method in Naive Bayes, but only when\u00a0\u2026", "num_citations": "30\n", "authors": ["1067"]}
{"title": "Identifying approximate itemsets of interest in large databases\n", "abstract": " This paper presents a method for discovering approximate frequent itemsets of interest in large scale databases. This method uses the central limit theorem to increase efficiency, enabling us to reduce the sample size by about half compared to previous approximations. Further efficiency is gained by pruning from the search space uninteresting frequent itemsets. In addition to improving efficiency, this measure also reduces the number of itemsets that the user need consider. The model and algorithm have been implemented and evaluated using both synthetic and real-world databases. Our experimental results demonstrate the efficiency of the approach.", "num_citations": "30\n", "authors": ["1067"]}
{"title": "A tutorial on statistically sound pattern discovery\n", "abstract": " Statistically sound pattern discovery harnesses the rigour of statistical hypothesis testing to overcome many of the issues that have hampered standard data mining approaches to pattern discovery. Most importantly, application of appropriate statistical tests allows precise control over the risk of false discoveries\u2014patterns that are found in the sample data but do not hold in the wider population from which the sample was drawn. Statistical tests can also be applied to filter out patterns that are unlikely to be useful, removing uninformative variations of the key patterns in the data. This tutorial introduces the key statistical and data mining theory and techniques that underpin this fast developing field. We concentrate on two general classes of patterns: dependency rules that express statistical dependencies between condition and consequent parts and dependency sets that express mutual dependence between\u00a0\u2026", "num_citations": "29\n", "authors": ["1067"]}
{"title": "Estimating bias and variance from data\n", "abstract": " The bias-variance decomposition of error provides useful insights into the error performance of a classifier as it is applied to different types of learning task. Most notably, it has been used to explain the extraordinary effectiveness of ensemble learning techniques. It is important that the research community have effective tools for assessing such explanations. To this end, techniques have been developed for estimating bias and variance from data. The most widely deployed of these uses repeated sub-sampling with a holdout set. We argue, with empirical support, that this approach has serious limitations. First, it provides very little flexibility in the types of distributions of training sets that may be studied. It requires that the training sets be relatively small and that the degree of variation between training sets be very circumscribed. Second, the approach leads to bias and variance estimates that have high statistical variance and hence low reliability. We develop an alternative method that is based on cross-validation. We show that this method allows far greater flexibility in the types of distribution that are examined and that the estimates derived are much more stable. Finally, we show that changing the distributions of training sets from which bias and variance estimates are drawn can alter substantially the bias and variance estimates that are derived.", "num_citations": "29\n", "authors": ["1067"]}
{"title": "A knowledge-based approach to computer-aided learning\n", "abstract": " This paper describes a methodology for the creation of knowledge-based computer-aided learning lessons. Unlike previous approaches, the knowledge base is utilized only for restricted aspects of the lesson\u2014both for the management of flow of control through a body of instructional materials and for the evaluation of the student's understanding of the subject matter. This has many advantages. While the approach has lower developmental and operational overheads than alternatives it is also able to perform far more flexible evaluations of the student's performance. As flow of control is managed by a knowledge-based component with reference to a detailed analysis of the student's understanding of the subject matter, lessons adapt to each student's individual understanding and aptitude within a domain.", "num_citations": "28\n", "authors": ["1067"]}
{"title": "Survey of distance measures for quantifying concept drift and shift in numeric data\n", "abstract": " Deployed machine learning systems are necessarily learned from historical data and are often applied to current data. When the world changes, the learned models can lose fidelity. Such changes to the statistical properties of data over time are known as concept drift. Similarly, models are often learned in one context, but need to be applied in another. This is called concept shift. Quantifying the magnitude of drift or shift, especially in the context of covariate drift or shift, or unsupervised learning, requires use of measures of distance between distributions. In this paper, we survey such distance measures with respect to their suitability for estimating drift and shift magnitude between samples of numeric data.", "num_citations": "27\n", "authors": ["1067"]}
{"title": "Association rules\n", "abstract": " Association rules \u2014 Monash University Skip to main navigation Skip to search Skip to main content Monash University Logo Help & FAQ Home Profiles Research Units Equipment Projects Research Output Prizes Activities Press / Media Association rules Geoffrey Webb Research output: Book/Report \u203a Commissioned Report \u203a Research Overview Original language English Place of Publication Mahwah New Jersey USA Publisher Lawrence Erlbaum Associates Number of pages 15 Edition 1 ISBN (Print) 0805840818 Publication status Published - 2003 Access to Document tns:text Cite this APA Author BIBTEX Harvard Standard RIS Vancouver Webb, G. (2003). Association rules. (1 ed.) Lawrence Erlbaum Associates. Webb, Geoffrey. / Association rules. 1 ed. Mahwah New Jersey USA : Lawrence Erlbaum Associates, 2003. 15 p. Webb, G 2003, Association rules. 1 edn, Lawrence Erlbaum Associates, Mahwah New \u2026", "num_citations": "27\n", "authors": ["1067"]}
{"title": "Learning decision lists by prepending inferred rules\n", "abstract": " This paper describes a new algorithm for learning decision lists that operates by prepending successive rules to front of the list under construction. This contrasts with the original decision list induction algorithm which operates by appending successive rules to end of the list under construction.. The new algorithm is demonstrated in the majority of cases to produce smaller classifiers that provide improved predictive accuracy than those produced by the original decision list induction algorithm.", "num_citations": "27\n", "authors": ["1067"]}
{"title": "Averaged One-Dependence Estimators: Preliminary Results.\n", "abstract": " Naive Bayes is a simple, computationally efficient and remarkably accurate approach to classification learning. These properties have led to its wide deployment in many online applications. However, it is based on an assumption that all attributes are conditionally independent given the class. This assumption leads to decreased accuracy in some applications. AODE overcomes the attribute independence assumption of naive Bayes by averaging over all models in which all attributes depend upon the class and a single other attribute. The resulting classification learning algorithm for nominal data is computationally efficient and achieves very low error rates.", "num_citations": "26\n", "authors": ["1067"]}
{"title": "Integrating machine learning with knowledge acquisition through direct interaction with domain experts\n", "abstract": " Knowledge elicitation from experts and empirical machine learning are two distinct approaches to knowledge acquisition with differing and mutually complementary capabilities. Learning apprentices have provided environments in which a knowledge engineer may collaborate with a machine learning system allowing for a synergy between the complementary approaches. The Knowledge Factory is a knowledge acquisition environment that allows a domain expert to collaborate directly with a machine learning system without the need for assistance from a knowledge engineer. This requires a different form of environment to the learning apprentice. The paper describes techniques for supporting such interactions and their implementation in a knowledge acquisition environment called The Knowledge Factory.", "num_citations": "26\n", "authors": ["1067"]}
{"title": "Cost-sensitive specialization\n", "abstract": " Cost-sensitive specialization is a generic technique for mis-classification cost sensitive induction. This technique involves specializing aspects of a classifier associated with high misclassification costs and generalizing those associated with low misclassification costs. It is widely applicable and simple to implement. It could be used to augment the effect of standard cost-sensitive induction techniques. It should directly extend to test application cost sensitive induction tasks. Experimental evaluation demonstrates consistent positive effects over a range of misclassification cost sensitive learning tasks.", "num_citations": "25\n", "authors": ["1067"]}
{"title": "A multiple test correction for streams and cascades of statistical hypothesis tests.\n", "abstract": " Statistical hypothesis testing is a popular and powerful tool for inferring knowledge from data. For every such test performed, there is always a non-zero probability of making a false discovery, ie~ rejecting a null hypothesis in error. Familywise error rate (FWER) is the probability of making at least one false discovery during an inference process. The expected FWER grows exponentially with the number of hypothesis tests that are performed, almost guaranteeing that an error will be committed if the number of tests is big enough and the risk is not managed; a problem known as the multiple testing problem. State-of-the-art methods for controlling FWER in multiple comparison settings require that the set of hypotheses be predetermined. This greatly hinders statistical testing for many modern applications of statistical inference, such as model selection, because neither the set of hypotheses that will be tested, nor even\u00a0\u2026", "num_citations": "24\n", "authors": ["1067"]}
{"title": "Contrary to popular belief incremental discretization can be sound, computationally efficient and extremely useful for streaming data\n", "abstract": " Discretization of streaming data has received surprisingly little attention. This might be because streaming data require incremental discretization with cut points that may vary over time and this is perceived as undesirable. We argue, to the contrary, that it can be desirable for a discretization to evolve in synchronization with an evolving data stream, even when the learner assumes that attribute values' meanings remain invariant over time. We examine the issues associated with discretization in the context of distribution drift and develop computationally efficient incremental discretization algorithms. We show that discretization can reduce the error of a classical incremental learner and that allowing a discretization to drift in synchronization with distribution drift can further reduce error.", "num_citations": "24\n", "authors": ["1067"]}
{"title": "Improving an inverse model of sheet metal forming by neural network based regression\n", "abstract": " An inverse model for a sheet metal forming process aims to determine the initial parameter levels required to form the final formed shape. This is a difficult problem that is usually approached by traditional methods such as finite element analysis. Formulating the problem as a classification problem makes it possible to use well established classification algorithms, such as decision trees. Classification is, however, generally based on a winner-takes-all approach when associating the output value with the corresponding class. On the other hand, when formulating the problem as a regression task, all the output values are combined to produce the corresponding class value. For a multi-class problem, this may result in very different associations compared with classification between the output of the model and the corresponding class. Such formulation makes it possible to use well known regression algorithms, such as\u00a0\u2026", "num_citations": "23\n", "authors": ["1067"]}
{"title": "Generality is predictive of prediction accuracy\n", "abstract": " During knowledge acquisition it frequently occurs that multiple alternative potential rules all appear equally credible. This paper addresses the dearth of formal analysis about how to select between such alternatives. It presents two hypotheses about the expected impact of selecting between classification rules of differing levels of generality in the absence of other evidence about their likely relative performance on unseen data. We argue that the accuracy on unseen data of the more general rule will tend to be closer to that of a default rule for the class than will that of the more specific rule. We also argue that in comparison to the more general rule, the accuracy of the more specific rule on unseen cases will tend to be closer to the accuracy obtained on training data. Experimental evidence is provided in support of these hypotheses. These hypotheses can be useful for selecting between rules in order to achieve specific knowledge acquisition objectives.", "num_citations": "22\n", "authors": ["1067"]}
{"title": "Critical evaluation of bioinformatics tools for the prediction of protein crystallization propensity\n", "abstract": " X-ray crystallography is the main tool for structural determination of proteins. Yet, the underlying crystallization process is costly, has a high attrition rate and involves a series of trial-and-error attempts to obtain diffraction-quality crystals. The Structural Genomics Consortium aims to systematically solve representative structures of major protein-fold classes using primarily high-throughput X-ray crystallography. The attrition rate of these efforts can be improved by selection of proteins that are potentially easier to be crystallized. In this context, bioinformatics approaches have been developed to predict crystallization propensities based on protein sequences. These approaches are used to facilitate prioritization of the most promising target proteins, search for alternative structural orthologues of the target proteins and suggest designs of constructs capable of potentially enhancing the likelihood of successful\u00a0\u2026", "num_citations": "21\n", "authors": ["1067"]}
{"title": "Mining significant association rules from uncertain data\n", "abstract": " In association rule mining, the trade-off between avoiding harmful spurious rules and preserving authentic ones is an ever critical barrier to obtaining reliable and useful results. The statistically sound technique for evaluating statistical significance of association rules is superior in preventing spurious rules, yet can also cause severe loss of true rules in presence of data error. This study presents a new and improved method for statistical test on association rules with uncertain erroneous data. An original mathematical model was established to describe data error propagation through computational procedures of the statistical test. Based on the error model, a scheme combining analytic and simulative processes was designed to correct the statistical test for distortions caused by data error. Experiments on both synthetic and real-world data show that the method significantly recovers the loss in true rules\u00a0\u2026", "num_citations": "21\n", "authors": ["1067"]}
{"title": "Comparative evaluation of alternative induction engines for Feature Based Modelling\n", "abstract": " Feature Based Modelling has demonstrated the ability to produce agent models with high accuracy in predicting an agent\u2019s future actions. There are a number of respects in which this modelling technique is novel. However, there has been no previous analysis of which aspects of the approach are responsible for its performance. One distinctive feature of the approach is a purpose built induction module. This paper presents a study in which the original custom built Feature Based Modelling induction module was replaced by the C4. 5 machine learning system. Comparative evaluation shows that the use of C4. 5 increases the number of predictions made without significantly altering the accuracy of those predictions. This suggests that it is the general input-output agent modelling methodology used with both systems that has primary responsibility for the high predictive accuracy previously reported for Feature Based Modelling, rather than its initial idiosyncratic induction technique.", "num_citations": "21\n", "authors": ["1067"]}
{"title": "Integrating boosting and stochastic attribute selection committees for further improving the performance of decision tree learning\n", "abstract": " Techniques for constructing classifier committees including boosting and bagging have demonstrated great success, especially boosting for decision tree learning. This type of technique generates several classifiers to form a committee by repeated application of a single base learning algorithm. The committee members vote to decide the final classification. Boosting and bagging create different classifiers by modifying the distribution of the training set. SASC (Stochastic Attribute Selection Committees) uses an alternative approach to generating classifier committees by stochastic manipulation of the set of attributes considered at each node during tree induction, but keeping the distribution of the training set unchanged. We propose a method for improving the performance of boosting. This technique combines boosting and SASC. It builds classifier committees by manipulating both the distribution of the training set\u00a0\u2026", "num_citations": "20\n", "authors": ["1067"]}
{"title": "Using C4. 5 as an induction engine for agent modelling: An experiment of optimisation\n", "abstract": " Input-Output Agent Modelling (IOAM) is an approach to modelling an agent in terms of relationships between the inputs and outputs of the cognitive system. This approach, together with one of the leading inductive learning algorithm, C4. 5, has been adopted to build a C4. 5-IOAM subtraction modeller, which aims to model students\u2019 competencies on elementary subtraction skills. Results showed that C4. 5-IOAM could achieved reasonably high predictive power for this purpose. Very little attempt has been made for optimising the current system that improvement of its performance could be achieved by employing strategies and techniques for this purpose. This paper reports an experiment that studied how the system\u2019s performance could be improved with techniques of confining training examples and resolving conflicting predictions. Results show that these strategies improve the system\u2019s performance in the aspects of capturing more student errors and achieving higher prediction rate.", "num_citations": "20\n", "authors": ["1067"]}
{"title": "PRISMOID: a comprehensive 3D structure database for post-translational modifications and mutations with functional impact\n", "abstract": " Post-translational modifications (PTMs) play very important roles in various cell signaling pathways and biological process. Due to PTMs\u2019 extremely important roles, many major PTMs have been studied, while the functional and mechanical characterization of major PTMs is well documented in several databases. However, most currently available databases mainly focus on protein sequences, while the real 3D structures of PTMs have been largely ignored. Therefore, studies of PTMs 3D structural signatures have been severely limited by the deficiency of the data. Here, we develop PRISMOID, a novel publicly available and free 3D structure database for a wide range of PTMs. PRISMOID represents an up-to-date and interactive online knowledge base with specific focus on 3D structural contexts of PTMs sites and mutations that occur on PTMs and in the close proximity of PTM sites with functional impact. The\u00a0\u2026", "num_citations": "19\n", "authors": ["1067"]}
{"title": "Procleave: predicting protease-specific substrate cleavage sites by combining sequence and structural information\n", "abstract": " Proteases are enzymes that cleave and hydrolyse the peptide bonds between two specific amino acid residues of target substrate proteins. Protease-controlled proteolysis plays a key role in the degradation and recycling of proteins, which is essential for various physiological processes. Thus, solving the substrate identification problem will have important implications for the precise understanding of functions and physiological roles of proteases, as well as for therapeutic target identification and pharmaceutical applicability. Consequently, there is a great demand for bioinformatics methods that can predict novel substrate cleavage events with high accuracy by utilizing both sequence and structural information. In this study, we present Procleave, a novel bioinformatics approach for predicting protease-specific substrates and specific cleavage sites by taking into account both their sequence and 3D structural\u00a0\u2026", "num_citations": "19\n", "authors": ["1067"]}
{"title": "Instance-dependent pu learning by bayesian optimal relabeling\n", "abstract": " When learning from positive and unlabelled data, it is a strong assumption that the positive observations are randomly sampled from the distribution of  conditional on , where X stands for the feature and Y the label. Most existing algorithms are optimally designed under the assumption. However, for many real-world applications, the observed positive examples are dependent on the conditional probability  and should be sampled biasedly. In this paper, we assume that a positive example with a higher  is more likely to be labelled and propose a probabilistic-gap based PU learning algorithms. Specifically, by treating the unlabelled data as noisy negative examples, we could automatically label a group positive and negative examples whose labels are identical to the ones assigned by a Bayesian optimal classifier with a consistency guarantee. The relabelled examples have a biased domain, which is remedied by the kernel mean matching technique. The proposed algorithm is model-free and thus do not have any parameters to tune. Experimental results demonstrate that our method works well on both generated and real-world datasets.", "num_citations": "19\n", "authors": ["1067"]}
{"title": "Adjusting dependence relations for semi-lazy TAN classifiers\n", "abstract": " The naive Bayesian classifier is a simple and effective classification method, which assumes a Bayesian network in which each attribute has the class label as its only one parent. But this assumption is not obviously hold in many real world domains. Tree-Augmented Naive Bayes (TAN) is a state-of-the-art extension of the naive Bayes, which can express partial dependence relations among attributes. In this paper, we analyze the implementations of two different TAN classifiers and their tree structures. Experiments show how different dependence relations impact on accuracy of TAN classifiers. We present a kind of semi-lazy TAN classifier, which builds a TAN identical to the original TAN at training time, but adjusts the dependence relations for a new test instance at classification time. Our extensive experimental results show that this kind of semi-lazy classifier delivers lower error than the original TAN and is\u00a0\u2026", "num_citations": "19\n", "authors": ["1067"]}
{"title": "Reinforcing a generic computer model for novice programmers\n", "abstract": " Novices often find learning their first programming language to be a frustrating and difficult process. They have difficulties in developing and debugging their programs. One of their problems is that their mental model of how the computer works is inadequate.In this paper we discuss a programming assistant, called Bradman, which we are currently developing. It is aimed at novice programmers and designed to reinforce a concrete mental model of how the computer works as a program is executed. It shows explicitly how program states change as statements in the procedural language C are executed. It does this by means of graphical display together with contextualised verbal explanations of each statement.", "num_citations": "19\n", "authors": ["1067"]}
{"title": "Removing trivial associations in association rule discovery\n", "abstract": " Association rule discovery has become one of the most widely applied data mining strategies. Techniques for association rule discovery have been dominated by the frequent itemset strategy as exemplified by the Apriori algorithm. One limitation of this approach is that it provides little opportunity to detect and remove association rules on the basis of relationships between rules. As a result, the association rules discovered are frequently swamped with large numbers of spurious rules that are of little interest to the user. This paper presents association rule discovery techniques that can detect and discard one form of spurious association rule: trivial associations.", "num_citations": "18\n", "authors": ["1067"]}
{"title": "Preface to UMUAI special issue on machine learning for user modeling\n", "abstract": " It can be argued that every interactive software system utilizes a user model, albeit, in many cases, an implicit model of the user\u2019s objectives, and capabilities. Rather than such implicit models, research on user modeling has concentrated on explicit models that provide some form of assessment of specific attributes of the user. There are three main ways in which the content of such a model might be generated and maintained.\u2013it might be specified by an external source, either through pre-session configuration, or by externally specified update;\u2013it might be specified by the user; or\u2013it might be specified by the software, usually on the basis of observation of the user\u2019s performance.The preferences settings to be found in many modern software packages serve to illustrate the first two methods. Such preferences are usually pre-set to reflect the manufacturer\u2019s assumptions about the software\u2019s community of users. The\u00a0\u2026", "num_citations": "18\n", "authors": ["1067"]}
{"title": "Recent progress in learning decision lists by prepending inferred rules\n", "abstract": " This paper describes a new algorithm for learning decision lists that operates by prepending successive rules to the front of the list under construction. By contrast, the classic algorithm operates by appending successive rules to the end of the decision list under construction. The new algorithm is demonstrated in the majority of cases to produce smaller classifiers that provide improved predictive accuracy in less time than the classic algorithm.", "num_citations": "18\n", "authors": ["1067"]}
{"title": "Learning crew scheduling constraints from historical schedules\n", "abstract": " For most airlines, there are numerous policies, agreements and regulations that govern the workload of airline crew. Although some constraints are formally documented, there are many others based on established practice and tacit understanding. Consequently, the task of developing a formal representation of the constraints that govern the working conditions of an airline\u2019s crew requires extensive time and effort involving interviews with the airline\u2019s crew schedulers and detailed analysis of historical schedules. We have developed a system that infers crew scheduling constraints from historical crew schedules with the assistance of a domain expert. This system implements the ComCon algorithm developed to learn constraints that prescribe the limits of certain aspects of crew schedules. The algorithm induces complex multivariate constraints based on a set of user provided templates that outline the general\u00a0\u2026", "num_citations": "17\n", "authors": ["1067"]}
{"title": "Advances in Artificial Intelligence--IBERAMIA 2004: 9th Ibero-American Conference on AI, Puebla, Mexico, November 22-26, 2004, Proceedings\n", "abstract": " This book constitutes the refereed proceedings of the 9th Ibero-American Conference on Artificial Intelligence, IBERAMIA 2004, held in Puebla, Mexico in November 2004. The 97 revised full papers presented were carefully reviewed and selected from 304 submissions. The papers are organized in topical sections on distributed AI and multi-agent systems, knowledge engineering and case-based reasoning, planning and scheduling, machine learning and knowledge acquisition, natural language processing, knowledge representation and reasoning, knowledge discovery and data mining, robotics, computer vision, uncertainty and fuzzy systems, genetic algorithms and neural networks, AI in education, and miscellaneous topics.", "num_citations": "17\n", "authors": ["1067"]}
{"title": "Multiple boosting: A combination of boosting and bagging\n", "abstract": " Classifier committee learning approaches have demonstrated great success in increasing the prediction accuracy of classifier learning, which is a key technique for datamining. These approaches generate several classifiers to form a committee by repeated application of a single base learning algorithm. The committee members vote to decide the final classification. It has been shown that Boosting and Bagging, as two representative methods of this type, can significantly decrease the error rate of decision tree learning. Boosting is generally more accurate than Bagging, but the former is more variable than the latter. In addition, Bagging is amenable to parallel or distributed processing, while Boosting is not. In this paper, we study a new committee learning algorithm, namely MB (Multiple Boosting). It creates multiple subcommittees by combining Boosting and Bagging. Experimental results in a representative collection of natural domains show that MB is, on average, more accurate than eit...", "num_citations": "17\n", "authors": ["1067"]}
{"title": "A comparison of first-order and zeroth-order induction for Input-Output Agent Modelling\n", "abstract": " Most student modelling systems seek to develop a model of the internal operation of the cognitive system. In contrast, Input-Output Agent Modelling (IOAM) models an agent in terms of relationships between the inputs and outputs of the cognitive system. Previous IOAM systems have demonstrated high predictive accuracy in the domain of elementary subtraction. These systems use zeroth-order induction. Many of the predicates used, however, represent relations. This suggests that first-order induction might perform well in this domain. This paper reports a study in which zeroth-order and first-order induction engines were used to build models of student subtraction skills. Comparative evaluation shows that zeroth-order induction performs better than first-order in detecting regularities indicating misconceptions while first-order induction leads zeroth-order in detecting regularities indicating correct concepts\u00a0\u2026", "num_citations": "17\n", "authors": ["1067"]}
{"title": "Inducing diagnostic rules for glomerular disease with the DLG machine learning algorithm\n", "abstract": " A pilot study has applied the DLG machine learning algorithm to create expert systems for the assessment and interpretation of clinical and laboratory data in glomerular disease. Despite the limited size of the data-set and major deficiencies in the information recorded therein, promising results have been obtained. On average, 100 expert systems developed from different subsets of the database, had a diagnostic accuracy of 54.70% when applied to cases that had not been used in their development. This compares with an average diagnostic accuracy of 48.31% obtained by four expert clinicians and of 3.23% obtained by random diagnosis. The expert systems demonstrated increased accuracy (62.90% on average) when cases of diseases represented by less than twenty examples were discarded. These results suggest that database expansion may enable the induction of diagnostic rules that provide accurate\u00a0\u2026", "num_citations": "17\n", "authors": ["1067"]}
{"title": "Selective AnDE for large data learning: a low-bias memory constrained approach\n", "abstract": " Learning from data that are too big to fit into memory poses great challenges to currently available learning approaches. Averaged n-Dependence Estimators (AnDE) allows for a flexible learning from out-of-core data, by varying the value of n (number of super parents). Hence, AnDE is especially appropriate for learning from large quantities of data. Memory requirement in AnDE, however, increases combinatorially with the number of attributes and the parameter n. In large data learning, number of attributes is often large and we also expect high n to achieve low-bias classification. In order to achieve the lower bias of AnDE with higher n but with less memory requirement, we propose a memory constrained selective AnDE algorithm, in which two passes of learning through training examples are involved. The first pass performs attribute selection on super parents according to available memory, whereas the second\u00a0\u2026", "num_citations": "16\n", "authors": ["1067"]}
{"title": "Smoothing a rugged protein folding landscape by sequence-based redesign\n", "abstract": " The rugged folding landscapes of functional proteins puts them at risk of misfolding and aggregation. Serine protease inhibitors, or serpins, are paradigms for this delicate balance between function and misfolding. Serpins exist in a metastable state that undergoes a major conformational change in order to inhibit proteases. However, conformational labiality of the native serpin fold renders them susceptible to misfolding, which underlies misfolding diseases such as \u03b1 1-antitrypsin deficiency. To investigate how serpins balance function and folding, we used consensus design to create conserpin, a synthetic serpin that folds reversibly, is functional, thermostable and polymerization resistant. Characterization of its structure, folding and dynamics suggest that consensus design has remodeled the folding landscape to reconcile competing requirements for stability and function. This approach may offer general benefits\u00a0\u2026", "num_citations": "16\n", "authors": ["1067"]}
{"title": "Naive-Bayes inspired effective pre-conditioner for speeding-up logistic regression\n", "abstract": " We propose an alternative parameterization of Logistic Regression (LR) for the categorical data, multi-class setting. LR optimizes the conditional log-likelihood over the training data and is based on an iterative optimization procedure to tune this objective function. The optimization procedure employed may be sensitive to scale and hence an effective pre-conditioning method is recommended. Many problems in machine learning involve arbitrary scales or categorical data (where simple standardization of features is not applicable). The problem can be alleviated by using optimization routines that are invariant to scale such as (second-order) Newton methods. However, computing and inverting the Hessian is a costly procedure and not feasible for big data. Thus one must often rely on first-order methods such as gradient descent (GD), stochastic gradient descent (SGD) or approximate second-order such as quasi\u00a0\u2026", "num_citations": "16\n", "authors": ["1067"]}
{"title": "Finding the right family: parent and child selection for averaged one-dependence estimators\n", "abstract": " Averaged One-Dependence Estimators (AODE) classifies by uniformly aggregating all qualified one-dependence estimators (ODEs). Its capacity to significantly improve naive Bayes\u2019 accuracy without undue time complexity has attracted substantial interest. Forward Sequential Selection and Backwards Sequential Elimination are effective wrapper techniques to identify and repair harmful interdependencies which have been profitably applied to naive Bayes. However, their straightforward application to AODE has previously proved ineffective. We investigate novel variants of these strategies. Our extensive experiments show that elimination of child attributes from within the constituent ODEs results in a significant improvement in probability estimate and reductions in bias and error relative to unmodified AODE. In contrast, elimination of complete constituent ODEs and the four types of attribute addition are\u00a0\u2026", "num_citations": "16\n", "authors": ["1067"]}
{"title": "To select or to weigh: A comparative study of model selection and model weighing for spode ensembles\n", "abstract": " An ensemble of Super-Parent-One-Dependence Estimators (SPODEs) offers a powerful yet simple alternative to naive Bayes classifiers, achieving significantly higher classification accuracy at a moderate cost in classification efficiency. Currently there exist two families of methodologies that ensemble candidate SPODEs for classification. One is to select only helpful SPODEs and uniformly average their probability estimates, a type of model selection. Another is to assign a weight to each SPODE and linearly combine their probability estimates, a methodology named model weighing. This paper presents a theoretical and empirical study comparing model selection and model weighing for ensembling SPODEs. The focus is on maximizing the ensemble\u2019s classification accuracy while minimizing its computational time. A number of representative selection and weighing schemes are studied, providing a\u00a0\u2026", "num_citations": "16\n", "authors": ["1067"]}
{"title": "Stochastic attribute selection committees with multiple boosting: Learning more accurate and more stable classifier committees\n", "abstract": " Classifier learning is a key technique for KDD. Approaches to learning classifier committees, including Boosting, Bagging, Sasc, and SascB, have demonstrated great success in increasing the prediction accuracy of decision trees. Boosting and Bagging create different classifiers by modifying the distribution of the training set. Sasc adopts a different method. It generates committees by stochastic manipulation of the set of attributes considered at each node during tree induction, but keeping the distribution of the training set unchanged. SascB, a combination of Boosting and Sasc, has shown the ability to further increase, on average, the prediction accuracy of decision trees. It has been found that the performance of SascB and Boosting is more variable than that of Sasc, although SascB is more accurate than the others on average. In this paper, we present a novel method to reduce variability of SascB and\u00a0\u2026", "num_citations": "16\n", "authors": ["1067"]}
{"title": "Knowledge-transfer learning for prediction of matrix metalloprotease substrate-cleavage sites\n", "abstract": " Matrix Metalloproteases (MMPs) are an important family of proteases that play crucial roles in key cellular and disease processes. Therefore, MMPs constitute important targets for drug design, development and delivery. Advanced proteomic technologies have identified type-specific target substrates; however, the complete repertoire of MMP substrates remains uncharacterized. Indeed, computational prediction of substrate-cleavage sites associated with MMPs is a challenging problem. This holds especially true when considering MMPs with few experimentally verified cleavage sites, such as for MMP-2,-3,-7, and-8. To fill this gap, we propose a new knowledge-transfer computational framework which effectively utilizes the hidden shared knowledge from some MMP types to enhance predictions of other, distinct target substrate-cleavage sites. Our computational framework uses support vector machines combined\u00a0\u2026", "num_citations": "15\n", "authors": ["1067"]}
{"title": "Sample-based Attribute Selective AnDE for Large Data\n", "abstract": " More and more applications have come with large data sets in the past decade. However, existing algorithms cannot guarantee to scale well on large data. Averaged n-Dependence Estimators (AnDE) allows for flexible learning from out-of-core data, by varying the value of n (number of super parents). Hence, AnDE is especially appropriate for large data learning. In this paper, we propose a sample-based attribute selection technique for AnDE. It needs one more pass through the training data, in which a multitude of approximate AnDE models are built and efficiently assessed by leave-one-out cross validation. The use of a sample reduces the training time. Experiments on 15 large data sets demonstrate that the proposed technique significantly reduces AnDE's error at the cost of a modest increase in training time. This efficient and scalable out-of-core approach delivers superior or comparable performance to\u00a0\u2026", "num_citations": "15\n", "authors": ["1067"]}
{"title": "Candidate elimination criteria for lazy Bayesian rules\n", "abstract": " Lazy Bayesian Rules modifies naive Bayesian classification to undo elements of the harmful attribute independence assumption. It has been shown to provide classification error comparable to boosting decision trees. This paper explores alternatives to the candidate elimination criterion employed within Lazy Bayesian Rules. Improvements over naive Bayes are consistent so long as the candidate elimination criteria ensures there is sufficient data for accurate probability estimation. However, the original candidate elimination criterion is demonstrated to provide better overall error reduction than the use of a minimum data subset size criterion.", "num_citations": "15\n", "authors": ["1067"]}
{"title": "Educational Evaluation of Feature Based Modelling in a Problem Solving Domain\n", "abstract": " Feature-Based Modelling is a machine learning based cognitive modelling methodology. An intelligent educational system has been implemented, for the purpose of evaluating the methodology, which helps students learn about the unification of terms from the Prolog programming language. The system has been used by Third Year Computer Science students at La Trobe University during September 1989. Students were randomly allocated to an Experimental condition, in which FBM modelling was used to select tasks, and give extra comments, or to a Control condition in which similar tasks and comments were given, but without FBM tailoring to the individual. Ratings of task appropriateness, and comment usefulness, were collected on-line as the students worked with the tutor; overall ratings were obtained by questionnaire at the end; and semester exam results were examined. Despite the fact that only a minority of students showed sufficient misunderstanding for FBM to have potential value, of the ten comparisons chat relate most directly to the aims of the Tutor, while in no case reaching significance, seven were in favour of the Tutor, and only two against. These preliminary results are very encouraging for the FBM principles of the Tutor.", "num_citations": "15\n", "authors": ["1067"]}
{"title": "Periscope: quantitative prediction of soluble protein expression in the periplasm of Escherichia coli\n", "abstract": " Periplasmic expression of soluble proteins in Escherichia coli not only offers a much-simplified downstream purification process, but also enhances the probability of obtaining correctly folded and biologically active proteins. Different combinations of signal peptides and target proteins lead to different soluble protein expression levels, ranging from negligible to several grams per litre. Accurate algorithms for rational selection of promising candidates can serve as a powerful tool to complement with current trial-and-error approaches. Accordingly, proteomics studies can be conducted with greater efficiency and cost-effectiveness. Here, we developed a predictor with a two-stage architecture, to predict the real-valued expression level of target protein in the periplasm. The output of the first-stage support vector machine (SVM) classifier determines which second-stage support vector regression (SVR) classifier to be used\u00a0\u2026", "num_citations": "14\n", "authors": ["1067"]}
{"title": "Why Gaussian Distribution?\n", "abstract": " In this paper, we proved three theorems based on Gaussian distribution. Gaussian probability distribution is perhaps the most used distribution in all of science. The Gaussian distribution is a continuous function which approximates the exact binomial distribution of events. This paper traces the origins of the Gaussian probability distribution is summarized.", "num_citations": "14\n", "authors": ["1067"]}
{"title": "A case study in feature invention for breast cancer diagnosis using X-ray scatter images\n", "abstract": " X-ray mammography is the current clinical method for screening for breast cancer, and like any technique, has its limitations. Several groups have reported differences in the X-ray scattering patterns of normal and tumour tissue from the breast. This gives rise to the hope that X-ray scatter analysis techniques may lead to a more accurate and cost effective method of diagnosing beast cancer which lends itself to automation. This is a particularly challenging exercise due to the inherent complexity of the information content in X-ray scatter patterns from complex hetrogenous tissue samples. We use a simple na\u00efve Bayes classier as our classification system. High-level features are extracted from the low-level pixel data. This paper reports some preliminary results in the ongoing development of this classification method that can distinguish between the diffraction patterns of normal and cancerous tissue, with\u00a0\u2026", "num_citations": "14\n", "authors": ["1067"]}
{"title": "The problem of missing values in decision tree grafting\n", "abstract": " Decision tree grafting adds nodes to inferred decision trees. Previous research has demonstrated that appropriate grafting techniques can improve predictive accuracy across a wide cross-selection of domains. However, previous decision tree grafting systems are demonstrated to have a serious deficiency for some data sets containing missing values. This problem arises due to the method for handling missing values employed by C4.5, in which the grafting systems have been embedded. This paper provides an explanation of and solution to the problem. Experimental evidence is presented of the efficacy of this solution.", "num_citations": "14\n", "authors": ["1067"]}
{"title": "Transparency Debugging with Explanations for Novice Programmers.\n", "abstract": " Novice programmers often find programming to be a difficult and frustrating task. Because of their lack of experience in programming novices have different needs to experts when it comes to debugging assistants. One way a debugging assistant could be tailored to novices, as proposed by Eisenstadt, is to provide them with an explic it model of how their program works and, hence encourage them to find errors for themselves. We discuss such a transparency debugger, Bradman, that we have been developing to assist novice programmers understand and debug their C programs. We also present the results of an experiment, conducted on volunteer novice programmers, in which approximately half of the students had access to an explanation of each statement as it was executed and the other half did not. We show that access to such explanations provided beneficial results for a significant number of students.", "num_citations": "14\n", "authors": ["1067"]}
{"title": "Evaluation of feature based modelling in subtraction\n", "abstract": " One aim of intelligent tutoring systems is to tailor lessons to each individual student's needs. To do this a tutoring system requires a model of the student's knowledge. Cognitive modelling aims to produce a detailed explanation of the student's progress. Feature Based Modelling forms a cognitive model of the student by creating aspects of problem descriptions and of students' responses. This paper will discuss Feature Based Modelling and show the results of an evaluation carried out in the domain of elementary subtraction.", "num_citations": "14\n", "authors": ["1067"]}
{"title": "MetalExplorer, a bioinformatics tool for the improved prediction of eight types of metal-binding sites using a random forest algorithm with two-step feature selection\n", "abstract": " Background: Metalloproteins are highly involved in many biological processes, including catalysis, recognition, transport, transcription, and signal transduction. The metal ions they bind usually play enzymatic or structural roles in mediating these diverse functional roles. Thus, the systematic analysis and prediction of metal-binding sites using sequence and/or structural information are crucial for understanding their sequence-structure-function relationships. Objective: The objective of this work is to develop a new computational algorithm for improved prediction of major types of metal-binding sites. Method: We propose MetalExplorer (http://metalexplorer.erc.monash.edu.au/), a new machine learning-based method for predicting eight different types of metal-binding sites (Ca, Co, Cu, Fe, Ni, Mg, Mn, and Zn) in proteins. Our approach combines heterogeneous sequence-, structure-, and residue contact network-based\u00a0\u2026", "num_citations": "13\n", "authors": ["1067"]}
{"title": "EGM: encapsulated gene-by-gene matching to identify gene orthologs and homologous segments in genomes\n", "abstract": " Motivation: Identification of functionally equivalent genes in different species is essential to understand the evolution of biological pathways and processes. At the same time, identification of strings of conserved orthologous genes helps identify complex genomic rearrangements across different organisms. Such an insight is particularly useful, for example, in the transfer of experimental results between different experimental systems such as Drosophila and mammals.                    Results: Here, we describe the Encapsulated Gene-by-gene Matching (EGM) approach, a method that employs a graph matching strategy to identify gene orthologs and conserved gene segments. Given a pair of genomes, EGM constructs a global gene match for all genes taking into account gene context and family information. The Hungarian method for identifying the maximum weight matching in bipartite graphs is employed\u00a0\u2026", "num_citations": "13\n", "authors": ["1067"]}
{"title": "Supervised Descriptive Rule Induction.\n", "abstract": " Supervised descriptive rule induction (SDRI) is a machine learning task in which individual patterns in the form of rules (see classification rule) intended for interpretation are induced from data, labeled by a predefined property of interest. In contrast to standard supervised rule induction, which aims at learning a set of rules defining a classification/prediction model, the goal of SDRI is to induce individual descriptive patterns. In this respect, SDRI is similar to association rule discovery, but the consequents of the rules are restricted to a single variable\u2013the property of interest\u2013and, except for the discrete target attribute, the data is not necessarily assumed to be discrete.Supervised descriptive rule induction assumes a set of training examples, described by attributes and their values and a selected attribute of interest (called the target attribute). Supervised descriptive rule induction induces rules that may each be interpreted independently of the others. Each rule is a local model, covering a subset of training examples, that captures a local relationship between the target attribute and the other attributes. Induced descriptive rules are mainly aimed at human interpretation. More specifically, the purposes of supervised descriptive rule induction are to allow the user to gain insights into the data domain and to better understand the phenomena underlying the data.", "num_citations": "13\n", "authors": ["1067"]}
{"title": "iLearnPlus: a comprehensive and automated machine-learning platform for nucleic acid and protein sequence analysis, prediction and visualization\n", "abstract": " Sequence-based analysis and prediction are fundamental bioinformatic tasks that facilitate understanding of the sequence(-structure)-function paradigm for DNAs, RNAs and proteins. Rapid accumulation of sequences requires equally pervasive development of new predictive models, which depends on the availability of effective tools that support these efforts. We introduce iLearnPlus, the first machine-learning platform with graphical- and web-based interfaces for the construction of machine-learning pipelines for analysis and predictions using nucleic acid and protein sequences. iLearnPlus provides a comprehensive set of algorithms and automates sequence-based feature extraction and analysis, construction and deployment of models, assessment of predictive performance, statistical analysis, and data visualization; all without programming. iLearnPlus includes a wide range of feature sets which encode\u00a0\u2026", "num_citations": "12\n", "authors": ["1067"]}
{"title": "Adaptive online extreme learning machine by regulating forgetting factor by concept drift map\n", "abstract": " In online-learning, the data is incrementally received and the distributions from which it is drawn may keep changing over time. This phenomenon is widely known as concept drift. Such changes may affect the generalization of a learned model to future data. This problem may be exacerbated by the form of the drift itself changing over time. Quantitative measures to describe and analyze the concept drift have been proposed in previous work. A description composed from these measures is called a concept drift map. We believe that these maps could be useful for guiding how much knowledge in the old model should be forgotten. Therefore, this paper presents an adaptive online learning model that uses a concept drift map to regulate the forgetting factor of an extreme learning machine. Specifically, when a batch of new instances are labeled, the distribution of each class on each attribute is firstly estimated, and\u00a0\u2026", "num_citations": "12\n", "authors": ["1067"]}
{"title": "Lazy Learning.\n", "abstract": " DiscussionLazy learning can be computationally advantageous when predictions using a single training set will only be made for few objects. This is because it is only necessary to model the immediate areas of the instance space that are occupied by objects to be classified. In consequence, no computation is expended modeling areas of the instance space that are irrelevant to the predictions that need to be made. This can also be an advantage when a training set is frequently updated, as can be the case in online learning, as it is not necessary to crease a complete global model before making a prediction subsequent to new training examples becoming available.Lazy learning can help improve prediction accuracy, by allowing a system to concentrate on deriving the best possible decision for the exact points of the instance space for which predictions are to be made. In contrast, eager learning can sometimes result in suboptimal predictions for some specific areas of the instance space as a result of trade-offs during the process of deriving a single model that seeks to minimize average error over the entire instance space.", "num_citations": "12\n", "authors": ["1067"]}
{"title": "AI* IA 2005: Advances in Artificial Intelligence: 9th Congress of the Italian Association for Artificial Intelligence Milan, Italy, September 21-23, 2005, Proceedings\n", "abstract": " This volume collects the papers selected for presentation at the IX Congress of the Italian Association for Arti? cial Intelligence (AI* IA), held in Milan at the University of Milano\u2013Bicocca (September 21\u201323, 2005). On the one hand this congress continues the tradition of AI* IA in organizing its biannual s-enti? c meeting from 1989; on the other hand, this edition is a landmark in the involvement of the international community of arti? cial intelligence (AI), directly involving a broad number of experts from several countries in the P-gramCommittee. Moreover, the peculiar nature of scienti? c researchin arti? cial intelligence (which is intrinsically international) and several consolidated int-national collaborations in projects and mobility programs allowed the collection and selection of papers from many di? erent countries, all around the world, enlarging the visibility of the Italian contribution within this research? eld. Arti? cial intelligence is today a growing complex set of conceptual, theor-ical, methodological, and technological frameworks, o? ering innovative com-tational solutions in the design and development of computer-based systems. Within this perspective, researchers working in this area must tackle a broad range of knowledge about methods, results, and solutions coming from di? erent classical areas of this discipline. The congress was designed as a forum allowing researchers to present and discuss specialized results as general contributions to AI growth.", "num_citations": "12\n", "authors": ["1067"]}
{"title": "Feature based modelling\n", "abstract": " Feature Based Modelling uses attribute value machine learning techniques to model the relationships between the cognitive system\u2019s inputs and outputs. This paper describes techniques that have been developed for creating these models and for extracting key information therefrom. An overview is provided of studies that have evaluated the application of feature based modelling in a number of educational contexts including piano keyboard playing, the unification of Prolog terms and elementary subtraction. These studies have demonstrated that the approach is applicable to a wide spectrum of domains. Classroom use has demonstrated the low computational overheads of the technique. In the domain of elementary subtraction, the approach has demonstrated accuracies in excess of 90% when predicting student solutions.", "num_citations": "12\n", "authors": ["1067"]}
{"title": "Highly scalable attribute selection for averaged one-dependence estimators\n", "abstract": " Averaged One-Dependence Estimators (AODE) is a popular and effective approach to Bayesian learning. In this paper, a new attribute selection approach is proposed for AODE. It can search in a large model space, while it requires only a single extra pass through the training data, resulting in a computationally efficient two-pass learning algorithm. The experimental results indicate that the new technique significantly reduces AODE\u2019s bias at the cost of a modest increase in training time. Its low bias and computational efficiency make it an attractive algorithm for learning from big data.", "num_citations": "11\n", "authors": ["1067"]}
{"title": "Beyond association rules: Generalized rule discovery\n", "abstract": " Generalized rule discovery is a rule discovery framework that subsumes association rule discovery and the type of search employed to find individual rules in classification rule discovery. This new rule discovery framework escapes the limitations of the support-confidence framework inherent in association rule discovery. This empowers data miners to identify the types of rules that they wish to discover and develop efficient algorithms for discovering those rules. This paper presents a scalable algorithm applicable to a wide range of generalized rule discovery task and demonstrates its efficiency.", "num_citations": "11\n", "authors": ["1067"]}
{"title": "Evaluation of data aging: A technique for discounting old data during student modeling\n", "abstract": " Student modeling systems must operate in an environment in which a student\u2019s mastery of a subject matter is likely to change as a lesson progresses. A student model is formed from evaluation of evidence about the student\u2019s mastery of the domain. However, given that such mastery will change, older evidence is likely to be less valuable than recent evidence. Data aging addresses this issue by discounting the value of older evidence. This paper provides experimental evaluation of the effects of data aging. While it is demonstrated that data aging can result in statistically significant increases in both the number and accuracy of predictions that a modeling system makes, it is also demonstrated that the reverse can be true. Further, the effects experienced are of only small magnitude. It is argued that these results demonstrate some potential for data aging as a general strategy, but do not warrant employing\u00a0\u2026", "num_citations": "11\n", "authors": ["1067"]}
{"title": "On the effectiveness of discretizing quantitative attributes in linear classifiers\n", "abstract": " Linear models in machine learning are extremely computational efficient but they have high representation bias due to non-linear nature of many real-world datasets. In this article, we show that this representation bias can be greatly reduced by discretization. Discretization is a common procedure in machine learning that is used to convert a quantitative attribute into a qualitative one. It is often motivated by the limitation of some learners to handle qualitative data. Since discretization looses information (as fewer distinctions among instances are possible using discretized data relative to undiscretized data) \u2013 where discretization is not essential, it might appear desirable to avoid it, and typically, it is avoided. However, in the past, it has been shown that discretization can leads to superior performance on generative linear models, e.g., naive Bayes. This motivates a systematic study of the effects of discretizing\u00a0\u2026", "num_citations": "10\n", "authors": ["1067"]}
{"title": "Mining significant crisp-fuzzy spatial association rules\n", "abstract": " Spatial association rule mining (SARM) is an important data mining task for understanding implicit and sophisticated interactions in spatial data. The usefulness of SARM results, represented as sets of rules, depends on their reliability: the abundance of rules, control over the risk of spurious rules, and accuracy of rule interestingness measure (RIM) values. This study presents crisp-fuzzy SARM, a novel SARM method that can enhance the reliability of resultant rules. The method firstly prunes dubious rules using statistically sound tests and crisp supports for the patterns involved, and then evaluates RIMs of accepted rules using fuzzy supports. For the RIM evaluation stage, the study also proposes a Gaussian-curve-based fuzzy data discretization model for SARM with improved design for spatial semantics. The proposed techniques were evaluated by both synthetic and real-world data. The synthetic data was\u00a0\u2026", "num_citations": "10\n", "authors": ["1067"]}
{"title": "Designing a more efficient, effective and safe Medical Emergency Team (MET) service using data analysis\n", "abstract": " Introduction Hospitals have seen a rise in Medical Emergency Team (MET) reviews. We hypothesised that the commonest MET calls result in similar treatments. Our aim was to design a pre-emptive management algorithm that allowed direct institution of treatment to patients without having to wait for attendance of the MET team and to model its potential impact on MET call incidence and patient outcomes.   Methods Data was extracted for all MET calls from the hospital database. Association rule data mining techniques were used to identify the most common combinations of MET call causes, outcomes and therapies.   Results There were 13,656 MET calls during the 34-month study period in 7936 patients. The most common MET call was for hypotension [31%, (2459/7936)]. These MET calls were strongly associated with the immediate administration of intra-venous fluid (70% [1714/2459] v 13% [739/5477] p<0.001), unless the patient was located on a respiratory ward (adjusted OR 0.41 [95%CI 0.25\u20130.67] p<0.001), had a cardiac cause for admission (adjusted OR 0.61 [95%CI 0.50\u20130.75] p<0.001) or was under the care of the heart failure team (adjusted OR 0.29 [95%CI 0.19\u20130.42] p<0.001). Modelling the effect of a pre-emptive management algorithm for immediate fluid administration without MET activation on data from a test period of 24 months following the study period, suggested it would lead to a 68.7% (2541/3697) reduction in MET calls for hypotension and a 19.6% (2541/12938) reduction in total METs without adverse effects on patients.   Conclusion Routinely collected data and analytic techniques can be used to develop a pre-emptive\u00a0\u2026", "num_citations": "10\n", "authors": ["1067"]}
{"title": "Statistically sound pattern discovery.\n", "abstract": " STATISTICALLY SOUND PATTERN DISCOVERY Page 1 Tutorial KDD\u201914 New York STATISTICALLY SOUND PATTERN DISCOVERY Wilhelmiina H\u00e4m\u00e4l\u00e4inen University of Eastern Finland whamalai@cs.uef.fi Geoff Webb Monash University Australia geoff.webb@monash.edu http://www.cs.joensuu.fi/pages/whamalai/kdd14/ sspdtutorial.html SSPD tutorial KDD\u201914 \u2013 p. 1 Page 2 Statistically sound pattern discovery: Problem 0000000000000 0000000000000 0000000000000 1111111111111 1111111111111 1111111111111 1111111111111 1111111111111 POPULATION clean and accurate usually infinite SAMPLE may contain noise REAL PATTERNS (with some tool) PATTERNS FOUND FROM THE SAMPLE ? REAL WORLD IDEAL \u2026", "num_citations": "10\n", "authors": ["1067"]}
{"title": "Anytime classification for a pool of instances\n", "abstract": " In many real-world applications of classification learning, such as credit card transaction vetting or classification embedded in sensor nodes, multiple instances simultaneously require classification under computational resource constraints such as limited time or limited battery capacity. In such a situation, available computational resources should be allocated across the instances in order to optimize the overall classification efficacy and efficiency. We propose a novel anytime classification framework, Scheduling Anytime Averaged Probabilistic Estimators (SAAPE), which is capable of classifying a pool of instances, delivering accurate results whenever interrupted and optimizing the collective classification performance. Following the practice of our previous anytime classification system AAPE, SAAPE runs a sequence of very efficient Bayesian probabilistic classifiers to classify each single instance\u00a0\u2026", "num_citations": "10\n", "authors": ["1067"]}
{"title": "Selective augmented Bayesian network classifiers based on rough set theory\n", "abstract": " The naive Bayes classifier is widely used in interactive applications due to its computational efficiency, direct theoretical base, and competitive accuracy. However, its attribute independence assumption can result in sub-optimal accuracy. A number of techniques have explored simple relaxations of the attribute independence assumption in order to increase accuracy. TAN is a state-of-the-art extension of naive Bayes, that can express limited forms of inter-dependence among attributes. Rough sets theory provides tools for expressing inexact or partial dependencies within dataset. In this paper, we present a variant of TAN using rough sets theory and compare their tree classifier structures, which can be thought of as a selective restricted trees Bayesian classifier. It delivers lower error than both pre-existing TAN-based classifiers, with substantially less computation than is required by the SuperParent approach.", "num_citations": "10\n", "authors": ["1067"]}
{"title": "Incorporating canonical discriminant attributes in classification learning\n", "abstract": " This paper describes a method for incorporating canonical discriminant attributes in classification machine learning. Though decision trees and rules have semantic appeal when building expert systems, the merits of discriminant analysis are well documented. For data sets on which discriminant analysis obtains significantly better predictive accuracy than symbolic machine learning, the incorporation of canonical discriminant attributes can benefit machine learning. The process starts by applying canonical discriminant analysis to the training set. The canonical discriminant attributes are included as additional attributes. The expanded data set is then subjected to machine learning. This enables linear combinations of numeric attributes to be incorporated in the classifiers that are learnt. Evaluation on the data sets on which discriminant analysis performs better than most machine learning systems, such as the Iris flowers and Waveform data sets, shows that incorporating the power of discriminant analysis in machine classification learning can significantly improve the predictive accuracy and reduce the complexity of classifiers induced by machine learning systems.", "num_citations": "10\n", "authors": ["1067"]}
{"title": "Generality is more significant than complexity: toward an alternative to Occam's Razor\n", "abstract": " The principle of Occam\u2019s Razor is widely employed in machine learning to select between classifiers with equal empirical support. This paper presents the theorem of decreasing inductive power: that, in the absence of other evidence to the contrary, if two classifiers a and b cover identical cases from the training set and a is a generalisation of b, there is a higher probability of a misclassifying previously unsighted cases. This theorem suggests that, to the contrary of Occam\u2019s Razor, generality, not complexity should be the determining factor in selecting between classifiers with equal empirical support. Two studies are presented. The first study demonstrates that the theorem of decreasing inductive power holds for a number of commonly studied learning problems and for a number of different means of manipulating classifier generality. The second study demonstrates that generality provides a more consistent indicator of predictive accuracy in the context of a default rule than does complexity. These results suggest that the theorem of decreasing predictive power provides a suitable theoretical framework for the development of learning biases for use in selecting between classifiers with identical empirical support.", "num_citations": "10\n", "authors": ["1067"]}
{"title": "DLGref'2: techniques for inductive knowledge relinement\n", "abstract": " This paper describes and evaluates machine learning techniques for knowledge-base refinement. These techniques are central to Einstein, a knowledge acquisition system that enables a human expert to collaborate with a machine learning system at all stages of the knowledge-acquisition cycle. Experimental evaluation demonstrates that the knowledge-base refinement techniques are able to significantly increase the accuracy of nontrivial expert systems in a wide variety of domains.", "num_citations": "10\n", "authors": ["1067"]}
{"title": "Minirocket: A very fast (almost) deterministic transform for time series classification\n", "abstract": " Rocket achieves state-of-the-art accuracy for time series classification with a fraction of the computational expense of most existing methods by transforming input time series using random convolutional kernels, and using the transformed features to train a linear classifier. We reformulate Rocket into a new method, MiniRocket. MiniRocket is up to 75 times faster than Rocket on larger datasets, and almost deterministic (and optionally, fully deterministic), while maintaining essentially the same accuracy. Using this method, it is possible to train and test a classifier on all of 109 datasets from the UCR archive to state-of-the-art accuracy in under 10 minutes. MiniRocket is significantly faster than any other method of comparable accuracy (including Rocket), and significantly more accurate than any other method of remotely similar computational expense.", "num_citations": "9\n", "authors": ["1067"]}
{"title": "A deep learning-based method for identification of bacteriophage-host interaction\n", "abstract": " Multi-drug resistance (MDR) has become one of the greatest threats to human health worldwide, and novel treatment methods of infections caused by MDR bacteria are urgently needed. Phage therapy is a promising alternative to solve this problem, to which the key is correctly matching target pathogenic bacteria with the corresponding therapeutic phage. Deep learning is powerful for mining complex patterns to generate accurate predictions. In this study, we develop PredPHI (Predicting Phage-Host Interactions), a deep learning-based tool capable of predicting the host of phages from sequence data. We collect >3000 phage-host pairs along with their protein sequences from PhagesDB and GenBank databases and extract a set of features. Then we select high-quality negative samples based on the K-Means clustering method and construct a balanced training set. Finally, we employ a deep convolutional neural\u00a0\u2026", "num_citations": "9\n", "authors": ["1067"]}
{"title": "Data Preparation.\n", "abstract": " Before data can be analyzed they must be organized into an appropriate form. Data preparation is the process of manipulating and organizing data prior to analysis.Data preparation is typically an iterative process of manipulating raw data, which is often unstructured and messy, into a more structured and useful form that is ready for further analysis. The whole preparation process consists of a series of major activities (or tasks) including data profiling, cleansing, integration and transformation.", "num_citations": "9\n", "authors": ["1067"]}
{"title": "Fast and effective single pass Bayesian learning\n", "abstract": " The rapid growth in data makes ever more urgent the quest for highly scalable learning algorithms that can maximize the benefit that can be derived from the information implicit in big data. Where data are too big to reside in core, efficient learning requires minimal data access. Single pass learning accesses each data point once only, providing the most efficient data access possible without resorting to sampling. The AnDE family of classifiers are effective single pass learners. We investigate two extensions to A2DE, subsumption resolution and MI-weighting. Neither of these techniques require additional data access. Both reduce A2DE\u2019s learning bias, improving its effectiveness for big data. Furthermore, we demonstrate that the techniques are complementary. The resulting combined technique delivers computationally efficient low-bias learning well suited to learning from big data.", "num_citations": "9\n", "authors": ["1067"]}
{"title": "Semi-naive Bayesian classification\n", "abstract": " The success and popularity of naive Bayes (NB) has led to a field of research exploring algorithms that seek to retain its numerous strengths while reducing error by alleviating the attribute interdependence problem. These algorithms can be categorized into five groups: those that apply conventional NB to a subset of attributes, those that alter NB by allowing interdependencies between attributes, those that apply NB to a subset of the training sample, those that calibrate NB\u2019s probability estimates and those that introduce hidden variables to NB. Eighteen key algorithms are analyzed in detail. We provide comparative analysis of thirteen algorithms\u2019 features and benchmark them using error analysis based on the bias-variance decomposition and probabilistic prediction analysis based on the quadratic loss function on sixty natural domains from the UCI Machine Learning Repository. To provide a baseline for comparison, we also present comprehensive experimental results for Logistic Regression and LibSVM, a popular SVM implementation. In analyzing the results of these experiments we provide general recommendations for selection between semi-naive Bayesian methods based on the characteristics of the application to which they are applied.", "num_citations": "9\n", "authors": ["1067"]}
{"title": "Inclusive pruning: a new class of pruning rule for unordered search and its application to classification learning\n", "abstract": " This paper presents a new class of pruning rule for unordered search. Previous pruning rules for unordered search identify operators that should not be applied in order to prune nodes reached via those operators. In contrast, the new pruning rules identify operators that should be applied and prune nodes that are not reached via those operators. Specific pruning rules employing both these approaches are identified for classification learning. Experimental results demonstrate that application of the new pruning rules can reduce by more than 60% the number of states from the search space that are considered during classification learning.", "num_citations": "9\n", "authors": ["1067"]}
{"title": "SIMLIN: a bioinformatics tool for prediction of S-sulphenylation in the human proteome based on multi-stage ensemble-learning models\n", "abstract": " S-sulphenylation is a ubiquitous protein post-translational modification (PTM) where an S-hydroxyl (\u2212SOH) bond is formed via the reversible oxidation on the Sulfhydryl group of cysteine (C). Recent experimental studies have revealed that S-sulphenylation plays critical roles in many biological functions, such as protein regulation and cell signaling. State-of-the-art bioinformatic advances have facilitated high-throughput in silico screening of protein S-sulphenylation sites, thereby significantly reducing the time and labour costs traditionally required for the experimental investigation of S-sulphenylation. In this study, we have proposed a novel hybrid computational framework, termed SIMLIN, for accurate prediction of protein S-sulphenylation sites using a multi-stage neural-network based ensemble-learning model integrating both protein sequence derived and protein structural features. Benchmarking experiments\u00a0\u2026", "num_citations": "8\n", "authors": ["1067"]}
{"title": "Pruning derivative partial rules during impact rule discovery\n", "abstract": " Because exploratory rule discovery works with data that is only a sample of the phenomena to be investigated, some resulting rules may appear interesting only by chance. Techniques are developed for automatically discarding statistically insignificant exploratory rules that cannot survive a hypothesis with regard to its ancestors. We call such insignificant rules derivative extended rules. In this paper, we argue that there is another type of derivative exploratory rules, which is derivative with regard to their children. We also argue that considerable amount of such derivative partial rules can not be successfully removed using existing rule pruning techniques. We propose a new technique to address this problem. Experiments are done in impact rule discovery to evaluate the effect of this derivative partial rule filter. Results show that the inherent problem of too many resulting rules in exploratory rule discovery is\u00a0\u2026", "num_citations": "8\n", "authors": ["1067"]}
{"title": "Discretization for data mining\n", "abstract": " Discretization is a process that transforms quantitative data into qualitative data. Quantitative data are commonly involved in data mining applications. However, many learning algorithms are designed primarily to handle qualitative data. Even for algorithms that can directly deal with quantitative data, learning is often less efficient and less effective. Hence research on discretization has long been active in data mining.", "num_citations": "8\n", "authors": ["1067"]}
{"title": "A systemic approach to the database marketing process\n", "abstract": " The role of database marketing (DBM) has become increasingly important for organisations that have large databases of information on customers with whom they deal directly. At the same time, DBM models used in practice have increased in sophistication. This paper examines a systemic view of DBM and the role of analytical techniques within DBM. It extends existing process models to develop a systemic model that encompasses the increased complexity of DBM in practice. The systemic model provides a framework to integrate data mining, experimental design and prioritisation decisions. This paper goes on to identify opportunities for research in DBM, including DBM process models used in practice, the use of evolutionary operations techniques in DBM, prioritisation decisions, and the factors that surround the uptake of DBM.", "num_citations": "8\n", "authors": ["1067"]}
{"title": "A heuristic covering algorithm has higher predictive accuracy than learning all rules\n", "abstract": " The induction of classification rules has been dominated by a single generic technique--the covering algorithm. This approach employs a simple hill-climbing search to learn sets of rules. Such search is subject to numerous widely known deficiencies. Further, there is a growing body of evidence that learning redundant sets of rules can improve predictive accuracy. The ultimate end-point of a move toward learning redundant rule sets would appear to be to learn and employ all possible rules. This paper presents a learning system that does this. An empirical investigation shows that, while the approach often achieves higher predictive accuracy than a covering algorithm, the covering algorithm outperforms induction of all rules significantly more frequently. Preliminary analysis suggests that learning all rules performs well when the training set clearly defines the decision surfaces but that the heuristic covering\u00a0\u2026", "num_citations": "8\n", "authors": ["1067"]}
{"title": "Recent progress in machine-expert collaboration for knowledge acquisition\n", "abstract": " Knowledge acquisition remains one of the primary constraints on the development of expert systems. A number of researchers have explored methods for allowing a machine learning system to assist a knowledge engineer in knowledge acquisition. In contrast, we are exploring methods for enabling an expert to directly interact with a machine learning system to collaborate during knowledge acquisition. We report recent extensions to our methodology encompassing a revised model of the role of machine learning in knowledge acquisition; techniques for communication between a machine learning system and a domain expert and novel forms of assistance that a machine learning system may provide to an expert.", "num_citations": "8\n", "authors": ["1067"]}
{"title": "A machine learning approach to student modelling\n", "abstract": " This paper describes an application of established machine learning principles to student modelling. Unlike previous machine learning based approaches to student modelling, the new approach is based on attributevalue machine learning. In contrast to many previous approaches it is not necessary for the lesson author to identify all forms of error that may be detected. Rather, the lesson author need only identify the relevant attributes both of the tasks to be performed by the student and of the student\u2019s actions. The values of these attributes are automatically processed by the student modeler to produce the student model.", "num_citations": "8\n", "authors": ["1067"]}
{"title": "PROSPECT: a web server for predicting protein histidine phosphorylation sites\n", "abstract": " Background: Phosphorylation of histidine residues plays crucial roles in signaling pathways and cell metabolism in prokaryotes such as bacteria. While evidence has emerged that protein histidine phosphorylation also occurs in more complex organisms, its role in mammalian cells has remained largely uncharted. Thus, it is highly desirable to develop computational tools that are able to identify histidine phosphorylation sites. Result: Here, we introduce PROSPECT that enables fast and accurate prediction of proteome-wide histidine phosphorylation substrates and sites. Our tool is based on a hybrid method that integrates the outputs of two convolutional neural network (CNN)-based classifiers and a random forest-based classifier. Three features, including the one-of-K coding, enhanced grouped amino acids content (EGAAC) and composition of k-spaced amino acid group pairs (CKSAAGP) encoding, were taken\u00a0\u2026", "num_citations": "7\n", "authors": ["1067"]}
{"title": "SimUSF: an efficient and effective similarity measure that is invariant to violations of the interval scale assumption\n", "abstract": " Similarity measures are central to many machine learning algorithms. There are many different similarity measures, each catering for different applications and data requirements. Most similarity measures used with numerical data assume that the attributes are interval scale. In the interval scale, it is assumed that a unit difference has the same meaning irrespective of the magnitudes of the values separated. When this assumption is violated, accuracy may be reduced. Our experiments show that removing the interval scale assumption by transforming data to ranks can improve the accuracy of distance-based similarity measures on some tasks. However the rank transform has high time and storage overheads. In this paper, we introduce an efficient similarity measure which does not consider the magnitudes of inter-instance distances. We compare the new similarity measure with popular similarity measures\u00a0\u2026", "num_citations": "7\n", "authors": ["1067"]}
{"title": "Learning lazy rules to improve the performance of classifiers\n", "abstract": " Based on an earlier study on lazy Bayesian rule learning, this paper introduces a general lazy learning framework, called LazyRule, that begins to learn a rule only when classifying a test case. The objective of the framework is to improve the performance of a base learning algorithm. It has the potential to be used for different types of base learning algorithms. LazyRule performs attribute elimination and training case selection using cross-validation to generate the most appropriate rule for each test case. At the consequent of the rule, it applies the base learning algorithm on the selected training subset and the remaining attributes to construct a classifier to make a prediction. This combined action seeks to build a better performing classifier for each test case than the classifier trained using all attributes and all training cases. We show empirically that LazyRule improves the performances of naive Bayesian\u00a0\u2026", "num_citations": "7\n", "authors": ["1067"]}
{"title": "Overview of a low-level program visualisation tool for novice c programmers\n", "abstract": " As a programming novice attempts to attain expertise in programming she must develop adequate mental models and knowledge structures of the programming process. Unfortunately, many of the computerised tools to which novice programmers have access are designed by expert programmers for experts and as such do not meet the needs of novices. Low-level program visualisation tools make explicit the internal workings of program execution and as such can serve as conceptual models onto which novices can assimilate information about programming. This paper discusses the need for such a tool, what features such a tool may include and gives a brief description of an evaluation of a low-level program visualisation tool developed at Deakin University.", "num_citations": "7\n", "authors": ["1067"]}
{"title": "Man-machine collaboration for knowledge acquisition\n", "abstract": " Both machine learning and knowledge elicitation from human experts have unique strengths and weaknesses. Man-machine collaboration for knowledge acquisition allows both knowledge acquisition techniques to be employed handin-hand. The strengths of each can alleviate the other's weaknesses. This has the potential to both reduce the time taken to develop an expert system while increasing the quality of the finished product. This paper discusses techniques for man-machine collaboration for knowledge acquisition and describes Einstein, a computer system that implements those techniques.", "num_citations": "7\n", "authors": ["1067"]}
{"title": "ECCLES: an \u201cExpert System\u201d for CAL\n", "abstract": " An authoring and lesson management system is described for Computer Assisted Learning in which lesson questioning and control flow arising from student response are generated at lesson time from an internal model of the lesson topic area. This approach permits the rapid authoring of conceptually complex lesson material. A languageless menu\u2014driven authoring system rninimises the system familiarization time for lesson authors and enforces the construction of logically complete lessons. From the student\u2019s point of view, the system appears as an expert system in the lesson subject area, with precise and detailed knowledge of the topic taught and of the causes of the student\u2019s own errors.", "num_citations": "7\n", "authors": ["1067"]}
{"title": "PCA-based drift and shift quantification framework for multidimensional data\n", "abstract": " Concept drift is a serious problem confronting machine learning systems in a dynamic and ever-changing world. In order to manage concept drift it may be useful to first quantify it by measuring the distance between distributions that generate data before and after a drift. There is a paucity of methods to do so in the case of multidimensional numeric data. This paper provides an in-depth analysis of the PCA-based change detection approach, identifies shortcomings of existing methods and shows how this approach can be used to measure a drift, not merely detect it.", "num_citations": "6\n", "authors": ["1067"]}
{"title": "Robust Bayesian kernel machine via Stein variational gradient descent for big data\n", "abstract": " Kernel methods are powerful supervised machine learning models for their strong generalization ability, especially on limited data to effectively generalize on unseen data. However, most kernel methods, including the state-of-the-art LIBSVM, are vulnerable to the curse of kernelization, making them infeasible to apply to large-scale datasets. This issue is exacerbated when kernel methods are used in conjunction with a grid search to tune their kernel parameters and hyperparameters which brings in the question of model robustness when applied to real datasets. In this paper, we propose a robust Bayesian Kernel Machine (BKM)-a Bayesian kernel machine that exploits the strengths of both the Bayesian modelling and kernel methods. A key challenge for such a formulation is the need for an efficient learning algorithm. To this end, we successfully extended the recent Stein variational theory for Bayesian inference\u00a0\u2026", "num_citations": "6\n", "authors": ["1067"]}
{"title": "Dual-model: An architecture for utilizing temporal information in student modeling\n", "abstract": " A modeling system may be required to predict an agent\u2019s future actions even when confronted by inadequate or contradictory relevant evidence from observations of past actions. This can result in low prediction accuracy, or otherwise, low prediction rates, leaving a set of cases for which no predictions are made. This raises two issues. First, when maximizing prediction rate is preferable, what mechanisms can be employed such that a system can make more predictions without severely degrading prediction accuracy? Second, for contexts in which accuracy is of primary importance, how can we further improve prediction accuracy? A recently proposed Dual-model approach, which takes models\u2019 temporal characteristics into account, suggests a solution to the first problem, but leaves room for further improvement. This paper presents two classes of Dual-model variant. Each aims to achieve one of the above objectives. With the performance of the original system as a baseline, which does not utilize the temporal information, empirical evaluations in the domain of elementary subtraction show that one class of variant outperforms the baseline in prediction rate while the other does so in prediction accuracy, without significantly affecting other overall measures of the original performance.", "num_citations": "6\n", "authors": ["1067"]}
{"title": "Empirical function attribute construction in classification learning\n", "abstract": " The merits of incorporating feature construction to assist selective induction in learning hard concepts are well documented. This paper introduces the notion of function attributes and reports a method of incorporating functional regularities in classifiers. Training sets are preprocessed with this method before submission to a selective induction classification learning system. The method, referred to as FAFA (function attribute finding), is characterised by finding bivariate functions that contribute to the discrimination between classes and then transforming them to function attributes as additional attributes of the data set. The value of each function attribute equals the deviation of each example from the value obtained by applying that function to the example. The expanded data set is then submitted to classification learning. Evaluation with published and artificial data shows that this method can improve classifiers in terms of predictive accuracy and complexity.", "num_citations": "6\n", "authors": ["1067"]}
{"title": "Application of machine learning to a renal biopsy database\n", "abstract": " This pilot study has applied machine learning (artificial intelligence derived qualitative analysis procedures) to yield non-invasive techniques for the assessment and interpretation of clinical and laboratory data in glomerular disease. To evaluate the appropriateness of these techniques, they were applied to subsets of a small database of 284 case histories and the resulting procedures evaluated against the remaining cases. Over such evaluations, the following average diagnostic accuracies were obtained: microscopic polyarteritis, 95.37% minimal lesion nephrotic syndrome, 96.50% immunoglobulin A nephropathy, 81.26% minor changes, 93.66% lupus nephritis, 96.27% focal glomerulosclerosis, 92.06% mesangial proliferative glomerulonephritis, 92.56% and membranous nephropathy, 92.56%. Although in general the new diagnostic system is not yet as accurate as the histological evaluation of renal\u00a0\u2026", "num_citations": "6\n", "authors": ["1067"]}
{"title": "The Unification Tutor-An intelligent educational system in the classroom\n", "abstract": " The Unification Tutor is experimental Intelligent Tutoring System for the domain of the unification of Prolog terms. It demonstrates the interactive use of Feature-Based Modelling-an approach to cognitive modelling that has been presented at previous ASCILITE Conferences (Webb, 1988b.) The Unification Tutor has been used by Third Year Computer Science students at La Trobe University during September 1989. This paper describes the Unification Tutor and evaluates its performance at La Trobe.", "num_citations": "6\n", "authors": ["1067"]}
{"title": "Ensembles of localised models for time series forecasting\n", "abstract": " With large quantities of data typically available nowadays, forecasting models that are trained across sets of time series, known as Global Forecasting Models (GFM), are regularly outperforming traditional univariate forecasting models that work on isolated series. As GFMs usually share the same set of parameters across all time series, they often have the problem of not being localised enough to a particular series, especially in situations where datasets are heterogeneous. We study how ensembling techniques can be used with generic GFMs and univariate models to solve this issue. Our work systematises and compares relevant current approaches, namely clustering series and training separate submodels per cluster, the so-called ensemble of specialists approach, and building heterogeneous ensembles of global and local models. We fill some gaps in the existing GFM localisation approaches, in particular by\u00a0\u2026", "num_citations": "5\n", "authors": ["1067"]}
{"title": "Specious rules: an efficient and effective unifying method for removing misleading and uninformative patterns in association rule mining\n", "abstract": " We present theoretical analysis and a suite of tests and procedures for addressing a broad class of redundant and misleading association rules we call specious rules. Specious dependencies, also known as spurious, apparent, or illusory associations, refer to a well-known phenomenon where marginal dependencies are merely products of interactions with other variables and disappear when conditioned on those variables. The most extreme example is Yule-Simpson's paradox where two variables present positive dependence in the marginal contingency table but negative in all partial tables defined by different levels of a confounding factor. It is accepted wisdom that in data of any nontrivial dimensionality it is infeasible to control for all of the exponentially many possible confounds of this nature. In this paper, we consider the problem of specious dependencies in the context of statistical association rule mining\u00a0\u2026", "num_citations": "5\n", "authors": ["1067"]}
{"title": "A new restricted Bayesian network classifier\n", "abstract": " On the basis of examining the existing restricted Bayesian network classifiers, a new Bayes-theorem-based and more strictly restricted Bayesian-network-based classification model DLBAN is proposed, which can be viewed as a double-level Bayesian network augmented naive Bayes classification. The experimental results show that the DLBAN classifier is better than the TAN classifier in the most cases.", "num_citations": "5\n", "authors": ["1067"]}
{"title": "Discriminant attribute finding in classification learning\n", "abstract": " This paper describes a method for extending domain models in classification learning by deriving new attributes from existing ones. The process starts by examining examples of different classes which have overlapping ranges in all of their numeric attribute values. Based on existing attributes, new attributes which enhance the distinguishability of a class are created. These additional attributes are then used in the subsequent classification learning process. The research revealed that this method can enable relationships between attributes to be incorporated in the classification procedures and, depending on the nature of data, significantly increase the coverage of class descriptions, improve the accuracy of classifying novel instances and reduce the number of clauses in class description when compared to classification learning alone. Evaluation with the data on iris flower classification showed that the classification accuracy is slightly improved and the number of clauses in the class description is significantly reduced.", "num_citations": "5\n", "authors": ["1067"]}
{"title": "Techniques for efficient empirical induction\n", "abstract": " This paper describes the LEI algorithm for empirical induction. The LEI algorithm provides efficient empirical induction for discrete attribute value data. It derives a classification procedure in the form of a set of predicate logic classification rules. This contrasts with the only other efficient approach to exhaustive empirical induction, the derivatives of the CLS algorithm, which present their classification procedures in the form of a decision tree. The LEI algorithm will always find the simplest non-disjunctive rule that correctly classifies all examples of a single class where such a rule exists.", "num_citations": "5\n", "authors": ["1067"]}
{"title": "Assessing the performance of computational predictors for estimating protein stability changes upon missense mutations\n", "abstract": " Understanding how a mutation might affect protein stability is of significant importance to protein engineering and for understanding protein evolution genetic diseases. While a number of computational tools have been developed to predict the effect of missense mutations on protein stability protein stability upon mutations, they are known to exhibit large biases imparted in part by the data used to train and evaluate them. Here, we provide a comprehensive overview of predictive tools, which has provided an evolving insight into the importance and relevance of features that can discern the effects of mutations on protein stability. A diverse selection of these freely available tools was benchmarked using a large mutation-level blind dataset of 1342 experimentally characterised mutations across 130 proteins from ThermoMutDB, a second test dataset encompassing 630 experimentally characterised mutations\u00a0\u2026", "num_citations": "4\n", "authors": ["1067"]}
{"title": "Anthem: a user customised tool for fast and accurate prediction of binding between peptides and HLA class I molecules\n", "abstract": " Neopeptide-based immunotherapy has been recognised as a promising approach for the treatment of cancers. For neopeptides to be recognised by CD8+ T cells and induce an immune response, their binding to human leukocyte antigen class I (HLA-I) molecules is a necessary first step. Most epitope prediction tools thus rely on the prediction of such binding. With the use of mass spectrometry, the scale of naturally presented HLA ligands that could be used to develop such predictors has been expanded. However, there are rarely efforts that focus on the integration of these experimental data with computational algorithms to efficiently develop up-to-date predictors. Here, we present Anthem for accurate HLA-I binding prediction. In particular, we have developed a user-friendly framework to support the development of customisable HLA-I binding prediction models to meet challenges associated with the rapidly\u00a0\u2026", "num_citations": "4\n", "authors": ["1067"]}
{"title": "Non-disjoint discretization for aggregating one-dependence estimator classifiers\n", "abstract": " There is still lack of clarity about the best manner in which to handle numeric attributes when applying Bayesian network classifiers. Discretization methods entail an unavoidable loss of information. Nonetheless, a number of studies have shown that appropriate discretization can outperform straightforward use of common, but often unrealistic parametric distribution (e.g. Gaussian). Previous studies have shown the Averaged One-Dependence Estimators (AODE) classifier and its variant Hybrid AODE (HAODE, which deals with numeric and discrete variables) to be robust towards the discretization method applied. However, all the discretization techniques taken into account so far formed non-overlapping intervals for a numeric attribute. We argue that the idea of non-disjoint discretization, already justified in Naive Bayes classifiers, can also be profitably extended to AODE and HAODE, albeit with some\u00a0\u2026", "num_citations": "4\n", "authors": ["1067"]}
{"title": "Discovery of Amino Acid Motifs for Thrombin Cleavage and Validation Using a Model Substrate\n", "abstract": " Understanding the active site preferences of an enzyme is critical to the design of effective inhibitors and to gaining insights into its mechanisms of action on substrates. While the subsite specificity of thrombin is understood, it is not clear whether the enzyme prefers individual amino acids at each subsite in isolation or prefers to cleave combinations of amino acids as a motif. To investigate whether preferred peptide motifs for cleavage could be identified for thrombin, we exposed a phage-displayed peptide library to thrombin. The resulting preferentially cleaved substrates were analyzed using the technique of association rule discovery. The results revealed that thrombin selected for amino acid motifs in cleavage sites. The contribution of these hypothetical motifs to substrate cleavage efficiency was further investigated using the B1 IgG-binding domain of streptococcal protein G as a model substrate. Introduction of a\u00a0\u2026", "num_citations": "4\n", "authors": ["1067"]}
{"title": "Averaged One-Dependence Estimators.\n", "abstract": " Averaged One-Dependence Estimators is a classification learning technique that forms an ensemble of a specific form of Bayesian network classifier called a 1-dependence classifier. Each 1-dependence classifier makes a weaker attribute independence assumption than naive Bayes, allowing each attribute to depend on the class and one common non-class attribute. AODE performs classification by aggregating the predictions of all 1-dependence classifiers in which all attributes depend on the same single parent attribute as well as the class and that parent attribute satisfies a minimum frequency constraint.", "num_citations": "4\n", "authors": ["1067"]}
{"title": "Model Evaluation.\n", "abstract": " When assessing the predictive efficacy of a model learned from data, to obtain a reliable estimate of its likely performance on new data, it is essential that it not be assessed by considering its performance on the data from which it was learned. A learning algorithm must interpolate appropriate predictions for regions of the instance space that are not included in the training data. It is probable that the inferred model will be more accurate for those regions represented in the training data than for those that are not, and hence predictions are likely to be less accurate for instances that were not included in the training data. Estimates that have been computed on the training data, are called resubstitution estimates. For example, the error of a model on the training data from which it was learned is called resubstitution error.", "num_citations": "4\n", "authors": ["1067"]}
{"title": "Alternative strategies for decision list construction\n", "abstract": " Alternative strategies for decision list construction DA~ ewlands'& GI Webb2 l Deakin University, Victoria 321 7, Australia\" onash University, Victoria 3800, Australia Abstract This work surveys well-known approaches to building decision lists. Some novel variations to strategies based on default rules for the most common class and insertion of new rules before the default rule are presented. These are expected to offer speed up in the construction of the decision list as well as compression of the length of the list. These strategies and a testing regime have been implemented and some empirical studies done to compare the strategies. Experimental results are presented and interpreted. We show that all strategies deliver decision lists of comparable accuracy. However, two techniques are shown to deliver this accuracy with lists composed of significantly fewer rules than alternative strategies. Of these, one also demonstrates significant computational advantages. The prepending strategy", "num_citations": "4\n", "authors": ["1067"]}
{"title": "Analysis of stamping production data with view towards quality management\n", "abstract": " A quality analysis trial was undertaken at Ford Geelong Stamping Plant on a press line that was fitted with standard press sensors to measure press and binder force over the stamping cycle for each panel. The quality of randomly sampled panels was measured by obtaining the panel thicknesses at five points, for 135 panels in total. These points were chosen such that they exhibited different forming modes. This paper analyses the input force data and the output quality data from the trial to determine any potential relationships. The analysis of the production data was performed using statistical correlation techniques to determine initial potential relationships between input and output variables. An Active Shape Model was used to extract features when identifying the major sources of variation within the input data. However, the initial analysis of the data elicited no direct relationship between the input variables measured and the panel thicknesses. This result is significant as the data collected is from a standard sensor configuration found in many press lines through-out the world. The reason for the lack of a direct relationship is believed to come from the lack of sensitivity in the force measurements which are not able to identify small changes in the process, whereas gross geometric variations have in previous studies shown an obvious relationship with changes in the force press profile. This means that existing force sensors require augmentation by additional sensors if a detailed automatic quality control system for the press lines based on input sensors alone.", "num_citations": "4\n", "authors": ["1067"]}
{"title": "Evaluation of Low-Level Program Visualisation for Teaching Novice C Programmers\n", "abstract": " While several program visualisation tools aimed at novice programmers have been developed over the past decade there is little empirical evidence showing that novices actually benefit from their use [3]. Bradman [7] is a low-level program visualisation tool. We present an experiment that tests the efficacy of Bradman in assisting novice programmers learn programming concepts. We show that students with access to this low-level program visualisation tool achieved greater understanding of some programming concepts than those without access.", "num_citations": "4\n", "authors": ["1067"]}
{"title": "An attribute-value machine learning approach to student modelling\n", "abstract": " This paper describes an application of machine learning to student modelling. Unlike previous machine learning approaches to student modelling, the new approach is based on attribute-value machine learning. In contrast to many previous approaches it is not necessary for the lesson author to identify all forms of error that may be detected or to identify the possible approaches to problem solving in the domain that may be adopted. Rather, the lesson author need only identify the relevant attributes both of the tasks to be performed by the student and of the student\u2019s actions. The values of these attributes are automatically processed by the student modeler to produce the student model.", "num_citations": "4\n", "authors": ["1067"]}
{"title": "Monash time series forecasting archive\n", "abstract": " Many businesses and industries nowadays rely on large quantities of time series data making time series forecasting an important research area. Global forecasting models that are trained across sets of time series have shown a huge potential in providing accurate forecasts compared with the traditional univariate forecasting models that work on isolated series. However, there are currently no comprehensive time series archives for forecasting that contain datasets of time series from similar sources available for the research community to evaluate the performance of new global forecasting algorithms over a wide variety of datasets. In this paper, we present such a comprehensive time series forecasting archive containing 20 publicly available time series datasets from varied domains, with different characteristics in terms of frequency, series lengths, and inclusion of missing values. We also characterise the datasets, and identify similarities and differences among them, by conducting a feature analysis. Furthermore, we present the performance of a set of standard baseline forecasting methods over all datasets across eight error metrics, for the benefit of researchers using the archive to benchmark their forecasting algorithms.", "num_citations": "3\n", "authors": ["1067"]}
{"title": "A strong baseline for weekly time series forecasting\n", "abstract": " Many businesses and industries require accurate forecasts for weekly time series nowadays. The forecasting literature however does not currently provide easy-to-use, automatic, reproducible and accurate approaches dedicated to this task. We propose a forecasting method that can be used as a strong baseline in this domain, leveraging state-of-the-art forecasting techniques, forecast combination, and global modelling. Our approach uses four base forecasting models specifically suitable for forecasting weekly data: a global Recurrent Neural Network model, Theta, Trigonometric Box-Cox ARMA Trend Seasonal (TBATS), and Dynamic Harmonic Regression ARIMA (DHR-ARIMA). Those are then optimally combined using a lasso regression stacking approach. We evaluate the performance of our method against a set of state-of-the-art weekly forecasting models on six datasets. Across four evaluation metrics, we show that our method consistently outperforms the benchmark methods by a considerable margin with statistical significance. In particular, our model can produce the most accurate forecasts, in terms of mean sMAPE, for the M4 weekly dataset.", "num_citations": "3\n", "authors": ["1067"]}
{"title": "AI for monitoring the Sustainable Development Goals and supporting and promoting action and policy development\n", "abstract": " The United Nations sustainable development goals (SDGs) were ratified with much enthusiasm by all UN member states in 2015. However, subsequent progress to meet these goals has been hampered by a lack of data available to measure the SDG indicators (SDIs), and a lack of evidence-based insights to inform effective policy responses. We outline an interdisciplinary program of research into the use of artificial intelligence techniques to support measurement of the SDIs, using both machine learning methods to model SDI measurements and explainable AI techniques to present the outputs in a human-friendly manner. As well as addressing the technical concerns, we will investigate the governance issues of what forms of evidence, methods of collecting that evidence and means of its communication will most usefully inform effective policy development. By addressing these fundamental challenges, we aim to\u00a0\u2026", "num_citations": "3\n", "authors": ["1067"]}
{"title": "A fast trust-region newton method for softmax logistic regression\n", "abstract": " With the emergence of big data, there has been a growing interest in optimization routines that lead to faster convergence of Logistic Regression (LR). Among many optimization methods such as Gradient Descent, Quasi-Newton, Conjugate Gradient, etc., the Trust-region based truncated Newton method (TRON) algorithm has been shown to converge the fastest. The TRON algorithm also forms an important component of the highly efficient and widely used liblinear package. It has been shown that the WANBIA-C trick of scaling with the log of the naive Bayes conditional probabilities can greatly accelerate the convergence of LR trained using (first-order) Gradient Descent and (approximate second-order) Quasi-Newton optimization. In this work we study the applicability of the WANBIA-C trick to TRON. We first devise a TRON algorithm optimizing the softmax objective function and then demonstrate that WANBIA-C\u00a0\u2026", "num_citations": "3\n", "authors": ["1067"]}
{"title": "Scalable learning of Bayesian network classifiers\n", "abstract": " Scalable learning of Bayesian network classifiers \u2013 Projects \u2014 Monash University Skip to main navigation Skip to search Skip to main content Monash University Home Monash University Logo Help & FAQ Home Profiles Research Units Equipment Projects Research output Prizes Activities Press / Media Search by expertise, name or affiliation Scalable learning of Bayesian network classifiers Ana M. Mart\u00ednez, Geoffrey I. Webb, Shenglei Chen, Nayyar A. Zaidi Research output: Contribution to journal \u203a Article \u203a Research \u203a peer-review 28 Citations (Scopus) Overview Projects (1) Projects Projects per year 2014 2016 1 Finished 1 results Status, start date (descending) Title Start date End date Type Status, start date(ascending) Filter Finished Search results Finished Combining generative and discriminative strategies to facilitate efficient and effective learning from big data Webb, G. Australian Research Council (ARC), \u2026", "num_citations": "3\n", "authors": ["1067"]}
{"title": "Efficiently identifying exploratory rules\u2019 significance\n", "abstract": " How to efficiently discard potentially uninteresting rules in exploratory rule discovery is one of the important research foci in data mining. Many researchers have presented algorithms to automatically remove potentially uninteresting rules utilizing background knowledge and user-specified constraints. Identifying the significance of exploratory rules using a significance test is desirable for removing rules that may appear interesting by chance, hence providing the users with a more compact set of resulting rules. However, applying statistical tests to identify significant rules requires considerable computation and data access in order to obtain the necessary statistics. The situation gets worse as the size of the database increases. In this paper, we propose two approaches for improving the efficiency of significant exploratory rule discovery. We also evaluate the experimental effect in impact rule discovery which\u00a0\u2026", "num_citations": "3\n", "authors": ["1067"]}
{"title": "Experimental evaluation of integrating machine learning with knowledge acquisition through direct interaction with domain experts\n", "abstract": " Machine learning and knowledge acquisition from experts have distinct and apparently complementary knowledge acquisition capabilities. This study demonstrates that the integration of these approaches can both improve the accuracy of the knowledge base that is developed and reduce the time taken to develop it. The system studied, called The Knowledge Factory is distinguished by the manner in which it supports direct interaction with domain experts with little or no knowledge engineering expertise. The benefits reported relate to use by such users. In addition to the improved quality of the knowledge base, in questionnaire responses the users provided favourable evaluations of the integration of machine learning with knowledge acquisition within the system.", "num_citations": "3\n", "authors": ["1067"]}
{"title": "Control, Capabilities and Communication: Three Key Issues for Machine-expert Collaborative Knowledge-acquisition\n", "abstract": " Machine learning and knowledge elicitation are different but complementary approaches to knowledge acquisition. On the face of it there are large potential gains to be reaped from the integration of these two knowledge acquisition techniques. Machine-expert collaborative knowledge acquisition combines these approaches by placing the machine learning system and the human expert as partners in the knowledge-acquisition task. This paper examines three key issues facing machine-expert collaborative knowledge-acquisition\u2014where should control reside, what capabilities should each partner bring to the task and how should the partners communicate?", "num_citations": "3\n", "authors": ["1067"]}
{"title": "OPUS: A systematic search algorithm and its application to categorical attribute-value datadriven machine learning.\n", "abstract": " OPUS is a branch and bound search algorithm that enables efficient systematic search through spaces in which the order in which the search operators are applied is not significant. OPUS achieves this by maximising the effect of each pruning action. While it is not possible to guarantee in the general case that any pruning shall occur, when pruning is possible, its effect is maximised. Experimental application of OPUS in data-driven machine learning demonstrates that NP hard search problems in which it is not possible to guarantee a solution in reasonable time can be solved for real world data within acceptable time frames. Indeed, OPUS is demonstrated to enable systematic search of extremely large search spaces in less time than is taken by common heuristic machine learning search algorithms. The use of systematic search in concept learning enables better experimental comparison of alternative inductive biases than was previously possible as the precise inductive bias can be described...", "num_citations": "3\n", "authors": ["1067"]}
{"title": "Modelling elementary subtraction: Intelligent warfare against bugs\n", "abstract": " This paper discusses an intelligent system. that uses Input/Output Cognitive Modelling (IOCM) techniques to form a model of the student. The paper describes FBM, an IOCM system that uses features to represent the inputs and outputs of the tasks being presented to the student and forms a relationship which describes in essence the knowledge the student has in the domain. Also presented is ASPMoRe, an intelligent tool that takes the model of the student and adapts the lesson to both refine the model and give the student practice in weak areas of his knowledge. Results have shown that the system can be an effective tool for educational purposes.", "num_citations": "3\n", "authors": ["1067"]}
{"title": "Rule optimisation and theory optimisation: Heuristic search strategies for data-driven machine learning\n", "abstract": " Previous implementations of the Aq algorithm have used rule optimisation search strategies to attempt to develop optimal classification procedures. These strategies involve generating successive characteristic descriptions each of which is individually of maximal value. This is contrasted with theory optimisation search strategies which, instead, generate successive complete classification procedures from which those with the maximal value are selected. These two strategies have been applied to the domain of the diagnosis of Immunoglobulin A Nephropathy disease. The theory optimisation strategy was observed to out perform the rule optimisation strategy.", "num_citations": "3\n", "authors": ["1067"]}
{"title": "Object-oriented control for intelligent computer assisted learning systems\n", "abstract": " This paper investigates an approach to providing a general-purpose authoring/tutoring shell for intelligent computer assisted learning systems. The approach is to outline an object-oriented representation of task/goal hierarchies, then to consider the ways in which domain expertise, student information and teacher expertise can be made to interact with such hierarchies. The result is a skeleton in terms of which exploratory lessons are being constructed; and in terms of which further research on the domain expert system, student modelling, educational expertise modelling, and user interfacing can more concretely be developed.", "num_citations": "3\n", "authors": ["1067"]}
{"title": "The Domain-Analysis Based Instruction System\n", "abstract": " At the past two CALITE conferences I have described a methodology for creating knowledgebased CAL. This paper describes how that methodology has evolved. The Domain-Analysis Based Instruction System, a CAL system that utilises the methodology is then described in detail.", "num_citations": "3\n", "authors": ["1067"]}
{"title": "Early abandoning and pruning for elastic distances including dynamic time warping\n", "abstract": " Nearest neighbor search under elastic distances is a key tool for time series analysis, supporting many applications. However, straightforward implementations of distances require space and time complexities, preventing these applications from scaling to long series. Much work has been devoted to speeding up the NN search process, mostly with the development of lower bounds, allowing to avoid costly distance computations when a given threshold is exceeded. This threshold, provided by the similarity search process, also allows to early abandon the computation of a distance itself. Another approach, is to prune parts of the computation. All these techniques are orthogonal to each other. In this work, we develop a new generic strategy,\u201cEAPruned\u201d, that tightly integrates pruning with early abandoning. We apply it to six elastic distance measures: DTW, CDTW, WDTW, ERP, MSM and TWE, showing\u00a0\u2026", "num_citations": "2\n", "authors": ["1067"]}
{"title": "HEAL: an automated deep learning framework for cancer histopathology image analysis\n", "abstract": " Motivation           Digital pathology supports analysis of histopathological images using deep learning methods at a large-scale. However, applications of deep learning in this area have been limited by the complexities of configuration of the computational environment and of hyperparameter optimization, which hinder deployment and reduce reproducibility.                             Results           Here, we propose HEAL, a deep learning-based automated framework for easy, flexible and multi-faceted histopathological image analysis. We demonstrate its utility and functionality by performing two case studies on lung cancer and one on colon cancer. Leveraging the capability of Docker, HEAL represents an ideal end-to-end tool to conduct complex histopathological analysis and enables deep learning in a broad range of applications for cancer image analysis\u00a0\u2026", "num_citations": "2\n", "authors": ["1067"]}
{"title": "Advances in Knowledge Discovery and Data Mining: 22nd Pacific-Asia Conference, PAKDD 2018, Melbourne, VIC, Australia, June 3-6, 2018, Proceedings, Part II\n", "abstract": " This three-volume set, LNAI 10937, 10938, and 10939, constitutes the thoroughly refereed proceedings of the 22nd Pacific-Asia Conference on Advances in Knowledge Discovery and Data Mining, PAKDD 2018, held in Melbourne, VIC, Australia, in June 2018. The 164 full papers were carefully reviewed and selected from 592 submissions. The volumes present papers focusing on new ideas, original research results and practical development experiences from all KDD related areas, including data mining, data warehousing, machine learning, artificial intelligence, databases, statistics, knowledge engineering, visualization, decision-making systems and the emerging applications.", "num_citations": "2\n", "authors": ["1067"]}
{"title": "Proceedings of the 10th IEEE International Conference on Data Mining (ICDM)\n", "abstract": " Proceedings of the 10th IEEE International Conference on Data Mining (ICDM) \u2014 Monash University Skip to main navigation Skip to search Skip to main content Monash University Logo Help & FAQ Home Profiles Research Units Equipment Projects Research Output Prizes Activities Press / Media Proceedings of the 10th IEEE International Conference on Data Mining (ICDM) Geoffrey Webb, Bing Liu, Chengqi Zhang, Dimitrios Gunopulos, Xindong Wu Research output: Book/Report \u203a Edited Book \u203a Other Overview Original language English Place of Publication Los Alamitos CA USA Publisher IEEE Computer Society Number of pages 1226 ISBN (Print) 9780769542560 Publication status Published - 2010 Access to Document tns:text Cite this APA Author BIBTEX Harvard Standard RIS Vancouver Webb, G., Liu, B., Zhang, C., Gunopulos, D., & Wu, X. (2010). Proceedings of the 10th IEEE International Conference on \u2026", "num_citations": "2\n", "authors": ["1067"]}
{"title": "Occam's Razor.\n", "abstract": " Most attempts to learn a model from data confront the problem that there will be many models that are consistent with the data. In order to learn a single model, a choice must be made between the available models. The factors taken into account by a learner in choosing between models are called its learning biases [2]. A preference for simple models is a common learning bias and is embodied in many learning techniques including pruning, minimum message length and minimum description length. Regularization is also sometimes viewed as an application of Occams\u2019 Razor.Occam\u2019s Razor is an imperative, rather than a proposition. That is, it is neither true nor false. Rather, it is a call to act in a particular way without making any claim about the consequences of doing so. In machine learning the so called Occam thesis is sometimes assumed, that", "num_citations": "2\n", "authors": ["1067"]}
{"title": "Machine Learning Techniques\n", "abstract": " Machine learning is a hot topic of artificial intelligence, and the interests of researchers on the same have grown fast in recent years. Especially, the great potential of machine learning techniques in classification makes it more popular in the research areas where pattern recognition is involved, including the development of condition monitoring and fault diagnosis for machines and structures. This article gives an outline to machine learning including the background and some key algorithms and theories that form the core of machine learning. The use of machine learning techniques in condition monitoring and fault diagnosis has also been discussed along with a basic framework of the machine\u2010learning\u2010based fault diagnosis system. More efforts have been made to introduce support vector machine (SVM), which is a significant development in machine learning. The basic procedure of the machine\u2010learning\u00a0\u2026", "num_citations": "2\n", "authors": ["1067"]}
{"title": "Finding the real patterns\n", "abstract": " Pattern discovery typically explores a massive space of potential patterns to identify those that satisfy some user-specified set of criteria. This process entails a huge risk (in many cases a near certainty) that many patterns will be false discoveries. These are patterns that satisfy the specified criteria with respect to the sample data but do not satisfy those criteria with respect to the population from which those data are drawn. This talk discusses the problem of false discoveries, and presents techniques for avoiding them.", "num_citations": "2\n", "authors": ["1067"]}
{"title": "Experimentation and self learning in continuous database marketing\n", "abstract": " We present a method for continuous database marketing that identifies target customers for a number of marketing offers using predictive models. The algorithm then selects the appropriate offer for the customer. Experimental design principles are encapsulated to capture more information that will be used to monitor and refine the predictive models. The updated predictive models are then used for the next round of marketing offers.", "num_citations": "2\n", "authors": ["1067"]}
{"title": "Fault detection in a cold forging process through feature extraction with a neural network\n", "abstract": " This paper investigates the application of neural networks to the recognition of lubrication defects typical to an industrial cold forging process employed by fastener manufacturers. The accurate recognition of lubrication errors, such as coating not being applied properly or damaged during material handling, is very important to the quality of the final product in fastener manufacture. Lubrication errors lead to increased forging loads and premature tool failure, as well as to increased defect sorting and the re-processing of the coated rod. The lubrication coating provides a barrier between the work material and the die during the drawing operation; moreover it needs be sufficiently robust to remain on the wire during the transfer to the cold forging operation. In the cold forging operation the wire undergoes multi-stage deformation without the application of any additional lubrication. Four types of lubrication errors, typical to\u00a0\u2026", "num_citations": "2\n", "authors": ["1067"]}
{"title": "Using decision trees for agent modelling: A study on resolving conflicting predictions\n", "abstract": " Input-Output Agent Modelling (IOAM) is an approach to modelling an agent in terms of relationships. between the inputs and outputs of the cognitive system. This approach, together with a leading inductive learning algorithm, C4.5, has been adopted to build a subtraction skill modeller, C4.5-IOAM. It models agents' competencies with a set of decision trees. C4.5-IOAM makes no prediction when predictions from different decision trees are contradictory. This paper proposes three techniques for resolving such situations. Two techniques involve selecting the more reliable prediction from a set of competing predictions using a tree quality measure and a leaf quality measure. The other technique merges multiple decision trees into a single tree. This has the additional advantage of producing more comprehensible models. Experimental results, in the domain of modelling elementary subtraction skills, showed that\u00a0\u2026", "num_citations": "2\n", "authors": ["1067"]}
{"title": "Education for cancer nurses: a European priority.\n", "abstract": " Education for cancer nurses: a European priority. - Abstract - Europe PMC Sign in or create an account https://orcid.org Europe PMC Menu About About Europe PMC Preprints in Europe PMC Funders Joining Europe PMC Governance Roadmap Outreach Tools Tools overview ORCID article claiming Journal list Grant finder External links service RSS feeds Annotations Annotations submission service Developers Developer resources Articles RESTful API Grants RESTful API API use cases SOAP web service Annotations API OAI service Bulk downloads Developers Forum Help Help using Europe PMC Search syntax reference Contact us Contact us Helpdesk Feedback Twitter Blog Tech blog Developer Forum Europe PMC plus Search worldwide, life-sciences literature Search Advanced Search Recent history Saved searches Abstract Full text Education for cancer nurses: a European priority. Webb P European \u2026", "num_citations": "2\n", "authors": ["1067"]}
{"title": "Debugging Using Partial Models\n", "abstract": " We present initial work on an expert system that will help users to debug their Pascal programs. This system asks the user questions concerning attempts to build a \u2018partial model\u2019of the program-a model of those aspects of the program likely to relate to the error. This contrasts with previous systems in which a complete model of the user\u2019s program is built and compared to templates of correct versions of the program. The advantages of this approach are greater flexibility, greater student involvement in the debugging process and lower computational overheads.", "num_citations": "2\n", "authors": ["1067"]}
{"title": "Cognitive diagnosis using student attributions\n", "abstract": " This paper details an approach to cognitive diagnosis that enables the inference of detailed models of a student\u2019s conceptualisation of a domain. This model is constructed be examining the attributes of the problems that the student has tackled and the student\u2019s performance while tackling those problems. A feature network is used to represent educationally relevant domain knowledge.\u2022 This approach has low implementation and operational overheads.\u2022 It provides a detailed model of the student\u2019s conceptualisation of the subject domain in terms of elements of knowledge from that domain;\u2022 Student models are not restricted to overlays of predefined correct and/or incorrect knowledge.", "num_citations": "2\n", "authors": ["1067"]}
{"title": "DeepBL: a deep learning-based approach for in silico discovery of beta-lactamases\n", "abstract": " Beta-lactamases (BLs) are enzymes localized in the periplasmic space of bacterial pathogens, where they confer resistance to beta-lactam antibiotics. Experimental identification of BLs is costly yet crucial to understand beta-lactam resistance mechanisms. To address this issue, we present DeepBL, a deep learning-based approach by incorporating sequence-derived features to enable high-throughput prediction of BLs. Specifically, DeepBL is implemented based on the Small VGGNet architecture and the TensorFlow deep learning library. Furthermore, the performance of DeepBL models is investigated in relation to the sequence redundancy level and negative sample selection in the benchmark dataset. The models are trained on datasets of varying sequence redundancy thresholds, and the model performance is evaluated by extensive benchmarking tests. Using the optimized DeepBL model, we perform\u00a0\u2026", "num_citations": "1\n", "authors": ["1067"]}
{"title": "OCTID: a one-class learning-based Python package for tumor image detection\n", "abstract": " Motivation Tumor tile selection is a necessary prerequisite in patch-based cancer whole slide image analysis, which is labor-intensive and requires expertise. Whole slides are annotated as tumor or tumor free, but tiles within a tumor slide are not. As all tiles within a tumor free slide are tumor free, these can be used to capture tumor-free patterns using the one-class learning strategy.   Results We present a Python package, termed OCTID, which combines a pre-trained convolutional neural network (CNN) model, UMAP, and one-class SVM to achieve accurate tumor tile classification using a training set of tumor free tiles. Benchmarking experiments on four H&E image datasets achieved remarkable performance in terms of F1-Score (0.90 \u00b1 0.06), Matthews correlation coefficient (0.93 \u00b1 0.05), and Accuracy (0.94 \u00b1 0.03).  Availability Detailed information can be found in the supplementary file\u00a0\u2026", "num_citations": "1\n", "authors": ["1067"]}
{"title": "MultiRocket: Multiple pooling operators and transformations for fast and effective time series classification\n", "abstract": " We propose MultiRocket, a fast time series classification (TSC) algorithm that achieves state-of-the-art performance with a tiny fraction of the time and without the complex ensembling structure of many state-of-the-art methods. MultiRocket improves on MiniRocket, one of the fastest TSC algorithms to date, by adding multiple pooling operators and transformations to improve the diversity of the features generated. In addition to processing the raw input series, MultiRocket also applies first order differences to transform the original series. Convolutions are applied to both representations, and four pooling operators are applied to the convolution outputs. When benchmarked using the University of California Riverside TSC benchmark datasets, MultiRocket is significantly more accurate than MiniRocket, and competitive with the best ranked current method in terms of accuracy, HIVE-COTE 2.0, while being orders of\u00a0\u2026", "num_citations": "1\n", "authors": ["1067"]}
{"title": "Cell graph neural networks enable digital staging of tumour microenvironment and precisely predict patient survival in gastric cancer\n", "abstract": " Gastric cancer is one of the deadliest cancers worldwide. Accurate prognosis is essential for effective clinical assessment and treatment. Spatial patterns in the tumor microenvironment (TME) are conceptually indicative of the staging and progression of gastric cancer patients. Using spatial patterns of the TME by integrating and transforming the multiplexed immunohistochemistry (mIHC) images as Cell-Graphs, we propose a novel graph neural network-based approach, termed Cell-Graph Signature or CGSignature, powered by artificial intelligence, for digital staging of TME and precise prediction of patient survival in gastric cancer.   In this study, patient survival prediction is formulated as either a binary (short-term and long-term) or ternary(short-term, medium-term, and long-term) classification task. Extensive benchmarking experiments demonstrate that the CGSignature achieves outstanding model performance, with Area Under the Receiver-Operating Characteristic curve (AUROC) of 0.960\u00b10.01, and 0.771\u00b10.024 to 0.904\u00b10.012 for the binary- and ternary-classification, respectively. Moreover, Kaplan-Meier survival analysis indicates that the \u2032digital-grade\u2032 cancer staging produced by CGSignature provides a remarkable capability in discriminating both binary and ternary classes with statistical significance (P-value< 0.0001), significantly outperforming the AJCC 8th edition Tumor-Node-Metastasis staging system.  Using mIHC-based Cell-Graphs, our method improves theassessment of the link between the TME spatial patterns and patient prognosis. Our study suggests the feasibility and benefits of such artificial intelligence-powered digital\u00a0\u2026", "num_citations": "1\n", "authors": ["1067"]}
{"title": "Structural capacitance in protein evolution and human diseases\n", "abstract": " Canonical mechanisms of protein evolution include the duplication and diversification of pre-existing folds through genetic alterations that include point mutations, insertions, deletions, and copy number amplifications, as well as post-translational modifications that modify processes such as folding efficiency and cellular localization. Following a survey of the human mutation database, we have identified an additional mechanism that we term \u201cstructural capacitance,\u201d which results in the de novo generation of microstructure in previously disordered regions. We suggest that the potential for structural capacitance confers select proteins with the capacity to evolve over rapid timescales, facilitating saltatory evolution as opposed to gradualistic canonical Darwinian mechanisms. Our results implicate the elements of protein microstructure generated by this distinct mechanism in the pathogenesis of a wide variety of human\u00a0\u2026", "num_citations": "1\n", "authors": ["1067"]}
{"title": "A data scientist's guide to start-ups\n", "abstract": " In August 2013, we held a panel discussion at the KDD 2013 conference in Chicago on the subject of data science, data scientists, and start-ups. KDD is the premier conference on data science research and practice. The panel discussed the pros and cons for top-notch data scientists of the hot data science start-up scene. In this article, we first present background on our panelists. Our four panelists have unquestionable pedigrees in data science and substantial experience with start-ups from multiple perspectives (founders, employees, chief scientists, venture capitalists). For the casual reader, we next present a brief summary of the experts' opinions on eight of the issues the panel discussed. The rest of the article presents a lightly edited transcription of the entire panel discussion.", "num_citations": "1\n", "authors": ["1067"]}
{"title": "Uncommon presentation of common problem: pneumomediastinum in a child with diabetic ketoacidosis\n", "abstract": " DiscussionPneumomediastinum is a rare but recognised complication of DKA. Most cases have been reported in adult patients. The presence of pneumomediastinum has been associated with DKA since 1937. 1 Vomiting and Kussmaul\u2019s breathing associated with severe DKA can cause alveolar rupture due to increased intra-alveolar pressures with subsequent air leakage along the perivascular sheaths toward the mediastinum. 2 Because some of the signs and symptoms of pneumomediastinum may be confused with those of the patient's primary disease process, this complication may be present more frequently than has been previously described. 3 Oesophageal rupture requires surgical intervention and has a mortality of up to 70%. However, for spontaneous pneumomediastinum the prognosis is excellent and there is prompt regression of the pneumomediastinum following correction of the ketoacidosis. 4 The condition was noticed to be self limiting in a systematic review of 56 cases. 5 Evidence-based guidelines are currently not available to help in choosing the best diagnostic approach.", "num_citations": "1\n", "authors": ["1067"]}
{"title": "Techniques for efficient learning without search\n", "abstract": " Averaged n-Dependence Estimators (AnDE) is a family of learning algorithms that range from low variance coupled with high bias through to high variance coupled with low bias. The asymptotic error of the lowest bias variant is the Bayes optimal. The AnDE family of algorithms have a training time that is linear with respect to the training examples, learn in a single pass through the data, support incremental learning, handle missing values directly and are robust in the face of noise. These characteristics make the algorithms particularly well suited to learning from large data. However, for higher orders of n they are very computationally demanding. This paper presents data structures and algorithms developed to reduce both memory and time for training and classification. These enhancements have enabled the evaluation and comparison of A3DE\u2019s effectiveness. The results provide further support for the\u00a0\u2026", "num_citations": "1\n", "authors": ["1067"]}
{"title": "Magnum Opus\n", "abstract": " Wednesdays and Fridays during the Fall and Spring semesters and once a week during Summer B. One copy per person. Additional copies are 25 cents. The Beacon is not responsible for the content of ads. Ad content is the sole responsibility of the company or vendor. The Beaconis an editorially independent newspaper partially funded by student and services fees that are appropriated by student government.", "num_citations": "1\n", "authors": ["1067"]}
{"title": "FaSS: Ensembles for Stable Learners\n", "abstract": " This paper introduces a new ensemble approach, Feature-Space Subdivision (FaSS), which builds local models instead of global models. FaSS is a generic ensemble approach that can use either stable or unstable models as its base models. In contrast, existing ensemble approaches which employ randomisation can only use unstable models. Our analysis shows that the new approach reduces the execution time to generate a model in an ensemble with an increased level of localisation in FaSS. Our empirical evaluation shows that FaSS performs significantly better than boosting in terms of predictive accuracy, when a stable learner SVM is used as the base learner. The speed up achieved by FaSS makes SVM ensembles a reality that would otherwise infeasible for large data sets, and FaSS SVM performs better than Boosting J48 and Random Forests when SVM is the preferred base learner.", "num_citations": "1\n", "authors": ["1067"]}
{"title": "Multi-strategy ensemble learning, ensembles of bayesian classifiers, and the problem of false discoveries\n", "abstract": " This talk covers an ensemble of my research contributions that I believe are likely to resonate with a current audience.", "num_citations": "1\n", "authors": ["1067"]}
{"title": "PRICAI 2006: Trends in Artificial Intelligence: 9th Pacific Rim International Conference on Artificial Intelligence, Guilin, China, August 7-11, 2006, Proceedings\n", "abstract": " This book constitutes the refereed proceedings of the 9th Pacific Rim International Conference on Artificial Intelligence, PRICAI 2006, held in Guilin, China in August 2006. The book presents 81 revised full papers and 87 revised short papers together with 3 keynote talks. The papers are organized in topical sections on intelligent agents, automated reasoning, machine learning and data mining, natural language processing and speech recognition, computer vision, perception and animation, and more.", "num_citations": "1\n", "authors": ["1067"]}
{"title": "Anytime learning and classification for online applications.\n", "abstract": " Many online applications of machine learning require fast classification and hence utilize efficient classifiers such as na\u00efve Bayes. However, outside periods of peak computational load, additional computational resources will often be available. Anytime classification can use whatever computational resources may be available at classification time to improve the accuracy of the classifications made.", "num_citations": "1\n", "authors": ["1067"]}
{"title": "K-optimal pattern discovery: An efficient and effective approach to exploratory data mining\n", "abstract": " Most data-mining techniques seek a single model that optimizes an objective function with respect to the data. In many real-world applications several models will equally optimize this function. However, they may not all equally satisfy a user\u2019s preferences, which will be affected by background knowledge and pragmatic considerations that are infeasible to quantify into an objective function.               Thus, the program may make arbitrary and potentially suboptimal decisions. In contrast, methods for exploratory pattern discovery seek all models that satisfy user-defined criteria. This allows the user select between these models, rather than relinquishing control to the program. Association rule discovery [1] is the best known example of this approach. However, it is based on the minimum-support technique, by which patterns are only discovered that occur in the data more than a user-specified number of times\u00a0\u2026", "num_citations": "1\n", "authors": ["1067"]}
{"title": "Not so naive Bayesian classification\n", "abstract": " Of numerous proposals to improve the accuracy of naive Bayes by weakening its attribute independence assumption, both LBR and TAN have demonstrated remarkable error performance. However, both techniques obtain this outcome at a considerable computational cost. We present a new approach to weakening the attribute independence assumption by averaging all of a constrained class of classifiers. In extensive experiments this technique delivers comparable prediction accuracy to LBR and TAN with substantially improved computational efficiency.", "num_citations": "1\n", "authors": ["1067"]}
{"title": "Improving the Prediction of the Roll Separating Force in a Hot Steel Finishing Mill\n", "abstract": " This paper focuses on the development of a hybrid phenomenological/inductive model to improve the current physical setup force model on a five stand industrial hot strip finishing mill. We approached the problem from two directions. In the first approach, the starting point was the output of the current setup force model. A feedforward multilayer perceptron (MLP) model was then used to estimate the true roll separating force using some other available variables as additional inputs to the model.It was found that it is possible to significantly improve the estimation of a roll separating force from 5.3% error on average with the current setup model to 2.5% error on average with the hybrid model. The corresponding improvements for the first coils are from 7.5% with the current model to 3.8% with the hybrid model. This was achieved by inclusion, in addition to each stand's force from the current model, the contributions from setup forces from the other stands, as well as the contributions from a limited set of additional variables such as: a) aim width; b) setup thickness; c) setup temperature; and d) measured force from the previous coil.", "num_citations": "1\n", "authors": ["1067"]}
{"title": "Convex hulls in concept induction\n", "abstract": " This paper investigates modelling concepts as a few, large convex hulls rather than as many, small, axis-orthogonal divisions as is done by systems which currently dominate classification learning. It is argued that this approach produces classifiers which have less strong hypothesis language bias and which, because of the fewness of the concepts induced, are more understandable. The design of such a system is described and its performance is investigated.Convex hulls are shown to be a useful inductive generalisation technique offering rather different biases than well-known systems such as C4.5 and CN2. The types of domains where convex hulls can be usefully employed are described.", "num_citations": "1\n", "authors": ["1067"]}
{"title": "Polygonal inductive generalisation system\n", "abstract": " Classification learning has been dominated by the induction of axisorthogonal decision surfaces. While induction of alternate forms of decision surface has received some attention in the context of decision trees, this issue has received little attention in the context of decision rules. An inductive learning algorithm has been developed which creates arbitrarily shaped concepts. Results from a prototype implementation demonstrate that the approach performs well on target concepts that are not readily represented by long, flat decision surfaces.", "num_citations": "1\n", "authors": ["1067"]}