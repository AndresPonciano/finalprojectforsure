{"title": "The goal question metric approach\n", "abstract": " As with any engineering discipline, software development requires a measurement mechanism for feedback and evaluation. Measurement is a mechanism for creating a corporate memory and an aid in answering a variety of questions associated with the enactment of any software process. It helps support project planning (eg, How much will a new project cost?); it allows us to determine the strengths and weaknesses of the current processes and products (eg, What is the frequency of certain types of errors?); it provides a rationale for adopting/refining techniques (eg, What is the impact of the technique XX on the productivity of the projects?); it allows us to evaluate the quality of specific processes and products (eg, What is the defect density in a specific system after deployment?). Measurement also helps, during the course of a project, to assess its progress, to take corrective action based on this assessment, and to evaluate the impact of such action.", "num_citations": "5604\n", "authors": ["140"]}
{"title": "A validation of object-oriented design metrics as quality indicators\n", "abstract": " This paper presents the results of a study in which we empirically investigated the suite of object-oriented (OO) design metrics introduced in (Chidamber and Kemerer, 1994). More specifically, our goal is to assess these metrics as predictors of fault-prone classes and, therefore, determine whether they can be used as early quality indicators. This study is complementary to the work described in (Li and Henry, 1993) where the same suite of metrics had been used to assess frequencies of maintenance changes to classes. To perform our validation accurately, we collected data on the development of eight medium-sized information management systems based on identical requirements. All eight projects were developed using a sequential life cycle model, a well-known OO analysis/design method and the C++ programming language. Based on empirical and quantitative analysis, the advantages and drawbacks of\u00a0\u2026", "num_citations": "2412\n", "authors": ["140"]}
{"title": "The TAME project: Towards improvement-oriented software environments\n", "abstract": " Experience from a dozen years of analyzing software engineering processes and products is summarized as a set of software engineering and measurement principles that argue for software engineering process models that integrate sound planning and analysis into the construction process. In the TAME (Tailoring A Measurement Environment) project at the University of Maryland, such an improvement-oriented software engineering process model was developed that uses the goal/question/metric paradigm to integrate the constructive and analytic aspects of software development. The model provides a mechanism for formalizing the characterization and planning tasks, controlling and improving projects based on quantitative analysis, learning in a deeper and more systematic way about the software process and product, and feeding the appropriate experience back into the current and future projects. The\u00a0\u2026", "num_citations": "1981\n", "authors": ["140"]}
{"title": "Iterative and incremental developments. a brief history\n", "abstract": " Although many view iterative and incremental development as a modern practice, its application dates as far back as the mid-1950s. Prominent software-engineering thought leaders from each succeeding decade supported IID practices, and many large projects used them successfully. These practices may have differed in their details, but all had a common theme-to avoid a single-pass sequential, document-driven, gated-step approach.", "num_citations": "1932\n", "authors": ["140"]}
{"title": "A methodology for collecting valid software engineering data\n", "abstract": " An effective data collection method for evaluating software development methodologies and for studying the software development process is described. The method uses goal-directed data collection to evaluate methodologies with respect to the claims made for them. Such claims are used as a basis for defining the goals of the data collection, establishing a list of questions of interest to be answered by data analysis, defining a set of data categorization schemes, and designing a data collection form. The data to be collected are based on the changes made to the software during development, and are obtained when the changes are made. To ensure accuracy of the data, validation is performed concurrently with software development and data collection. Validation is based on interviews with those people supplying the data. Results from using the methodology show that data validation is a necessary part of\u00a0\u2026", "num_citations": "1606\n", "authors": ["140"]}
{"title": "The road to academic excellence: The making of world-class research universities\n", "abstract": " The positive contribution of tertiary education is increasingly recognized as not limited to middle-income and advanced countries, since it applies equally to low-income economies. Tertiary education can help countries become more globally competitive by developing a skilled, productive, and flexible labor force and by creating, applying, and spreading new ideas and technologies. Research universities are reckoned among the central institutions of the 21st century knowledge economies. This book extends the analysis of the framework presented in The Challenge of Establishing World-Class Universities (Salmi 2009) and by examining the recent experience of 11 universities in nine countries that have grappled with the challenges of building successful research institutions in difficult circumstances and learning from the lessons of these experiences.", "num_citations": "1249\n", "authors": ["140"]}
{"title": "Property-based software engineering measurement\n", "abstract": " Little theory exists in the field of software system measurement. Concepts such as complexity, coupling, cohesion or even size are very often subject to interpretation and appear to have inconsistent definitions in the literature. As a consequence, there is little guidance provided to the analyst attempting to define proper measures for specific problems. Many controversies in the literature are simply misunderstandings and stem from the fact that some people talk about different measurement concepts under the same label (complexity is the most common case). There is a need to define unambiguously the most important measurement concepts used in the measurement of software products. One way of doing so is to define precisely what mathematical properties characterize these concepts, regardless of the specific software artifacts to which these concepts are applied. Such a mathematical framework could\u00a0\u2026", "num_citations": "1050\n", "authors": ["140"]}
{"title": "Experimentation in software engineering\n", "abstract": " A framework is presented for analyzing most of the experimental work performed in software engineering over the past several years. The framework of experimentation consists of four categories corresponding to phases of the experimentation process: definition, planning, operation, and interpretation. A variety of experiments are described within the framework and their contribution to the software engineering discipline is discussed. Some recommendations for the application of the experimental process in software engineering are included.", "num_citations": "966\n", "authors": ["140"]}
{"title": "Software errors and complexity: an empirical investigation0\n", "abstract": " An analysis of the distributions and relationships derived from the change data collected during development of a medium-scale software project produces some surprising insights into the factors influencing software development. Among these are the tradeoffs between modifying an existing module as opposed to creating a new one, and the relationship between module size and error proneness.", "num_citations": "962\n", "authors": ["140"]}
{"title": "Comparing the effectiveness of software testing strategies\n", "abstract": " This study applies an experimentation methodology to compare three state-of-the-practice software testing techniques: a) code reading by stepwise abstraction, b) functional testing using equivalence partitioning and boundary value analysis, and c) structural testing using 100 percent statement coverage criteria. The study compares the strategies in three aspects of software testing: fault detection effectiveness, fault detection cost, and classes of faults detected. Thirty-two professional programmers and 42 advanced students applied the three techniques to four unit-sized programs in a fractional factorial experimental design. The major results of this study are the following. 1) With the professional programmers, code reading detected more software faults and had a higher fault detection rate than did functional or structural testing, while functional testing detected more faults than did structural testing, but functional\u00a0\u2026", "num_citations": "754\n", "authors": ["140"]}
{"title": "Comparing detection methods for software requirements inspections: A replicated experiment\n", "abstract": " Software requirements specifications (SRS) are often validated manually. One such process is inspection, in which several reviewers independently analyze all or part of the specification and search for faults. These faults are then collected at a meeting of the reviewers and author(s). Usually, reviewers use Ad Hoc or Checklist methods to uncover faults. These methods force all reviewers to rely on nonsystematic techniques to search for a wide variety of faults. We hypothesize that a Scenario-based method, in which each reviewer uses different, systematic techniques to search for different, specific classes of faults, will have a significantly higher success rate. We evaluated this hypothesis using a 3/spl times/2/sup 4/ partial factorial, randomized experimental design. Forty eight graduate students in computer science participated in the experiment. They were assembled into sixteen, three-person teams. Each team\u00a0\u2026", "num_citations": "606\n", "authors": ["140"]}
{"title": "Iterative enhancement: A practical technique for software development\n", "abstract": " This paper recommends the iterative enhancement' technique as a practical means of using a top-down, stepwise refinement approach to software development. This technique begins with a simple initial implementation of a property chosen (skeletal) subproject which is followed by the gradual enhancement of successive implementations in order to build the full implementation. The development and quantitative analysis of a production compiler for the language SIMPL-T is used to demonstrate that the application of iterative enhancement to software development is practical and efficient, encourages the generation of an easily modifiable product, and facilities reliability.", "num_citations": "545\n", "authors": ["140"]}
{"title": "Identifying and qualifying reusable software components\n", "abstract": " Identification and qualification of reusable software based on software models and metrics is explored. Software metrics provide a way to automate the extraction of reusable software components from existing systems, reducing the amount of code that experts must analyze. Also, models and metrics permit feedback and improvement to make the extraction process fit a variety of environments. Some case studies are described to validate the experimental approach. They deal with only the identification phase and use a very simple model of a reusable code component, but the results show that automated techniques can reduce the amount of code that a domain expert needs to evaluate to identify reusable parts.< >", "num_citations": "505\n", "authors": ["140"]}
{"title": "The role of experimentation in software engineering: past, current, and future\n", "abstract": " Software engineering needs to follow the model of other physical sciences and develop an experimental paradigm for the field. This paper proposes the approach towards developing an experimental component of such a paradigm. The approach is based upon a quality improvement paradigm that addresses the role of experimentation and process improvement in the content of industrial development. The paper outlines a classification scheme for characterizing such experiments.", "num_citations": "457\n", "authors": ["140"]}
{"title": "How reuse influences productivity in object-oriented systems\n", "abstract": " THIS article presents the results of a study conduct-ed at the University of Maryland in which we assessed the impact of reuse on quality and pro-ductivity in object-oriented (OO) systems. Reuse is assumed to be an effective strategy for building high-quality software. However, there is currently little empirical information about what to expect from reuse in terms of productivity and quality gains. The study is one step toward a better understanding of the benefits of reuse in an OO framework in light of currently available technology. Data was collected for four months\u2014September through December 1994\u2014on the development of eight small (less than 15,000 source lines of code [KSLOC]) systems with equivalent functional requirements. All eight projects were developed using the Waterfall-style Software Engineering Life Cycle Model, an OO design method, and the C++ programming language. The study found\u00a0\u2026", "num_citations": "426\n", "authors": ["140"]}
{"title": "A meta-model for software development resource expenditures\n", "abstract": " One of the basic goals of software engineering is the establishment of useful models and equations to predict the cost of any given programming project. Many models have been proposed over the last several years, but, because of differences in the data collected, types of projects and environmental factors among software development sites, these models are not transportable and are only valid within the organization where they were developed. This result seems reasonable when one considers that a model developed at a certain environment will only be able to capture the impact of the factors which have a variable effect within that environment. Those factors which are constant at that environment, and therefore do not cause variations in the productivity among projects produced there, may have different or variable effects at another environment.", "num_citations": "412\n", "authors": ["140"]}
{"title": "Defining and validating measures for object-based high-level design\n", "abstract": " The availability of significant measures in the early phases of the software development life-cycle allows for better management of the later phases, and more effective quality assessment when quality can be more easily affected by preventive or corrective actions. We introduce and compare various high-level design measures for object-based software systems. The measures are derived based on an experimental goal, identifying fault-prone software parts, and several experimental hypotheses arising from the development of Ada systems for Flight Dynamics Software at the NASA Goddard Space Flight Center (NASA/GSFC). Specifically, we define a set of measures for cohesion and coupling, which satisfy a previously published set of mathematical properties that are necessary for any such measures to be valid. We then investigate the measures' relationship to fault-proneness on three large scale projects, to\u00a0\u2026", "num_citations": "400\n", "authors": ["140"]}
{"title": "System structure analysis: Clustering with data bindings\n", "abstract": " This paper examines the use of cluster analysis as a tool for system modularization. Several clustering techniques are discussed and used on two medium-size systems and a group of small projects. The small projects are presented because they provide examples (that will fit into a paper) of certain types of phenomena. Data bindings between the routines of the system provide the basis for the bindings. It appears that the clustering of data bindings provides a meaningful view of system modularization.", "num_citations": "378\n", "authors": ["140"]}
{"title": "Goal question metric (gqm) approach\n", "abstract": " As with any engineering discipline, software development requires a measurement mechanism for feedback and evaluation. Measurement supports creating a corporate memory and is an aid in answering a variety of questions associated with the enactment of any software process. Measurement also helps, during the course of a project, to assess its progress, to take corrective action based on this assessment, and to evaluate the impact of such action. According to many studies made on the application of metrics and models in industrial environments, measurement in order to be effective must be.  Focused on specific goals Applied to all life\u2010cycle products, processes, and resources Interpreted on the basis of characterization and understanding of the organizational context, environment, and goals   This means that measurement must be defined in a top\u2010down fashion. It must be focused, based on goals and\u00a0\u2026", "num_citations": "330\n", "authors": ["140"]}
{"title": "Support for comprehensive reuse\n", "abstract": " Reuse of products, processes and other knowledge will be the key to enable the software industry to achieve the dramatic improvement in productivity and quality required to satisfy anticipated growing demands. Although experience shows that certain kinds of reuse can be successful, general success has been elusive. A software life-cycle technology that allows comprehensive reuse of all kinds of software-related experience could provide the means of achieving the desired order-of-magnitude improvements. In this paper, we introduce a comprehensive framework of models, model-based characterisation schemes, and support mechanisms for better understanding, evaluating, planning and supporting all aspects of reuse.", "num_citations": "327\n", "authors": ["140"]}
{"title": "Viewing maintenance as reuse-oriented software development\n", "abstract": " The author presents a high-level organizational paradigm for development and maintenance in which an organization can learn from development and maintenance tasks and then apply that paradigm to several maintenance process models. Associated with the paradigm is a mechanism for setting measurable goals, making it possible to evaluate the process and the product and learn from experience. He discusses three maintenance models: the quick-fix, the iterative-enhancement, and the full-reuse model. He establishes a framework for classifying reusable objects and selecting a model. He offers a scheme that categorizes three aspects of reuse: the reusable object, the reusable object's context, and the process of transforming that object. The author then discusses what he terms reuse enablers: an improvement paradigm that helps organizations evaluate, learn, and enhance their software processes and\u00a0\u2026", "num_citations": "321\n", "authors": ["140"]}
{"title": "COTS-based systems top 10 list\n", "abstract": " Presents a COTS-based system (CBS) software defect-reduction list as hypotheses, rather than results, that also serve as software challenges for enhancing our empirical understanding of CBSs. The hypotheses are: (1) more than 99% of all executing computer instructions come from COTS products (each instruction passed a market test for value); (2) more than half the features in large COTS software products go unused; (3) the average COTS software product undergoes a new release every 8-9 months, with active vendor support for only its latest three releases; (4) CBS development and post-deployment efforts can scale as high as the square of the number of independently developed COTS products targeted for integration; (5) CBS post-deployment costs exceed CBS development costs; (6) although glue-code development usually accounts for less than half the total CBS software development effort, the effort\u00a0\u2026", "num_citations": "314\n", "authors": ["140"]}
{"title": "The experimental paradigm in software engineering\n", "abstract": " Software can be viewed as a part of a system solution that can be encoded to execute on a computer as a set of instructions; it includes all the associated documentation necessary to understand, transform and use that solution. Software engineering can be defined as the disciplined development and evolution of software systems based upon a set of principles, technologies, and processes.We will concentrate on three primary characteristics of software and software engineering; its inherent complexity, the lack of well defined primitives or components of the discipline, and the fact that software is developed, not produced. This combination makes software something quite different than anything we have dealt with before.", "num_citations": "314\n", "authors": ["140"]}
{"title": "Tailoring the software process to project goals and environments\n", "abstract": " ABSTRACT\", This paper presents a methodology for improving the software process by tailoring it to the specific project goals and environment. This improvement process is aimed at the global software process model as well as at the methods and tools supporting that model. The basic idea is to use defect profiles to help characterize the environment and evalu-ate the project goals and the effectiveness of methods and tools in a quantitative way. The improvement process is implemented iteratively by setting project improvement goals, characterizing those goals and the environment, in part, via defect profiles in a quantitative way, choosing methods and tools fitting those characteristics, evaluating the actual behavior of the chosen set of methods and tools, and refining the project goals based on the evaluation results. All these activities require analysis of large amounts of data and, therefore, support by an\u00a0\u2026", "num_citations": "304\n", "authors": ["140"]}
{"title": "A pattern recognition approach for software engineering data analysis\n", "abstract": " In order to understand, evaluate, predict, and control the software development process with regard to such perspectives as productivity, quality, and reusability, one needs to collect meaningful data and analyze them in an effective way. However, software engineering data have several inherent problems associated with them and the classical statistical analysis techniques do not address these problems very well. In this paper, we define a specific pattern recognition approach for analyzing software engineering data, called Optimized Set Reduction (OSR), that overcomes many of the problems associated with statistical techniques. OSR provides mechanisms for building models for prediction that provide accuracy estimates, risk management evaluation and quality assesssment. The construction of the models can be automated and evolve with new data over time to provide an evolutionary learning approach (the\u00a0\u2026", "num_citations": "302\n", "authors": ["140"]}
{"title": "Quantitative evaluation of software methodology\n", "abstract": " This paper presented a paradigm for evaluating software de Velopment methods and tools. The basic idea is to generate a set of goals which are refined into quantifiable questions which specify metrics to be collected on the software development and maintenance process and product. These metrics can be used to characterize, evaluate, predict and motivate. They can be used in an active as well as passive way by learning from analyzing the data and improving the methods and tools based upon what is learned from that analysis. Several examples were given representing each of the different approaches to evaluation.", "num_citations": "294\n", "authors": ["140"]}
{"title": "Developing interpretable models with optimized set reduction for identifying high-risk software components\n", "abstract": " Applying equal testing and verification effort to all parts of a software system is not very efficient, especially when resources are tight. Therefore, one needs to low/high fault frequency components so that testing/verification effort can be concentrated where needed. Such a strategy is expected to detect more faults and thus improve the resulting reliability of the overall system. The authors present the optimized set reduction approach for constructing such models, which is intended to fulfill specific software engineering needs. The approach to classification is to measure the software system and build multivariate stochastic models for predicting high-risk system components. Experimental results obtained by classifying Ada components into two classes (is, or is not likely to generate faults during system and acceptance rest) are presented. The accuracy of the model and the insights it provides into the error-making\u00a0\u2026", "num_citations": "254\n", "authors": ["140"]}
{"title": "The evolution and impact of code smells: A case study of two open source systems\n", "abstract": " Code smells are design flaws in object-oriented designs that may lead to maintainability issues in the further evolution of the software system. This study focuses on the evolution of code smells within a system and their impact on the change behavior (change frequency and size). The study investigates two code smells, God Class and Shotgun Surgery, by analyzing the historical data over several years of development of two large scale open source systems. The detection of code smells in the evolution of those systems was performed by the application of an automated approach using detection strategies. The results show that we can identify different phases in the evolution of code smells during the system development and that code smell infected components exhibit a different change behavior. This information is useful for the identification of risk areas within a software system that need refactoring to assure a\u00a0\u2026", "num_citations": "252\n", "authors": ["140"]}
{"title": "The software engineering laboratory: an operational software experience factory\n", "abstract": " For 15 years, the Software Engineering Laboratory(SEL) has been carrying out studies and experiments for the purpose of understanding, assessing, and improving software and softwrne processes within a production software development environment at the National Aeronautics and Space Administration/Goddard Space Flight Center (NASA/GSFC). The SEL comprises three major organizations: q NASA/GSFC, Flight Dynamics Division q University of Maryland, Department of Computer Science q Computer Sciences Co~ oration, Flight Dynamics Technology GroupThese organizations have jointly carried out several hundred software studies, producing hundreds of reports, papers, and documents, all of which describe some aspect of the software engineering technology that has been analyzed in the flight dynamics environment at NASA. The studies range from small, controlled experiments(such as analyzing\u00a0\u2026", "num_citations": "249\n", "authors": ["140"]}
{"title": "Improve soft-ware quality by reusing knowledge and experience\n", "abstract": " THE APPROACHES FOR IMPROVING QUALITY IN MANUFACTURING PROCESSES DON'T WORK ESPECIALLY WELL FOR SOFTWARE DEVELOPMENT. THE Authors provide a quality improvement paradigm for the software industry that builds on manufacturing models but focuses on reused learning and experience by establishing\" experience factories.\" Their iterative process enables an organization to acquire core competencies to support its strategic capabilities.? f", "num_citations": "232\n", "authors": ["140"]}
{"title": "Analyzing error-prone system structure\n", "abstract": " One central feature of the structure of a software system is the nature of the interconnections among its components (eg, subsystems, modules). The concepts of coupling and strength have been used in the past to refer to the degree of interconnection among and within components. The purpose of this study is to quantify ratios of coupling and strength and use them to identify error-prone system structure. We use measures of data interaction, called data bindings, as the basis for calculating software coupling and strength and analyzing system structure. We selected a 148 000 source line system from a production environment for empirical analysis. We collected software error data from high-level system design through system test and from some field operation of the system. We describe the methods used for gathering data during the ongoing project and characterize the software error data collected. We apply a\u00a0\u2026", "num_citations": "229\n", "authors": ["140"]}
{"title": "Software development: A paradigm for the future\n", "abstract": " A new paradigm for software development that treats software development as an experimental activity is presented. Built-in mechanisms are provided for learning how to develop software better and reusing previous experience in the forms of knowledge, processes, and products. Models and measures are used to aid in the tasks of characterization, evaluation, and motivation. An organization scheme is proposed for separating the project-specific focus from the organization's learning and reuse focuses of software development. The implications of this approach for corporations, research, and education are discussed, and some research activities currently underway at the University of Maryland that support this approach are presented.< >", "num_citations": "227\n", "authors": ["140"]}
{"title": "Software process evolution at the SEL\n", "abstract": " The Software Engineering Laboratory of the National Aeronautics and Space Administration's Goddard Space Flight Center has been adapting, analyzing, and evolving software processes for the last 18 years (1976-94). Their approach is based on the Quality Improvement Paradigm, which is used to evaluate process effects on both product and people. The authors explain this approach as it was applied to reduce defects in code. In examining and adapting reading techniques, we go through a systematic process of evaluating the candidate process and refining its implementation through lessons learned from previous experiments and studies. As a result of this continuous, evolutionary process, we determined that we could successfully apply key elements of the cleanroom development method in the SEL environment, especially for projects involving fewer than 50000 lines of code (all references to lines of code\u00a0\u2026", "num_citations": "221\n", "authors": ["140"]}
{"title": "Cleanroom software development: An empirical evaluation\n", "abstract": " The Cleanroom software development approach is intended to produce highly reliable software by integrating formal methods for specification and design, nonexecution-based program development, and statistically based independent testing. In an empirical study, 15 three-person teams developed versions of the same software system (800-2300 source lines); ten teams applied Cleanroom, while five applied a more traditional approach. This analysis characterizes the effect of Cleanroom on the delivered product, the software development process, and the developers.", "num_citations": "211\n", "authors": ["140"]}
{"title": "The future generation of software: a management perspective\n", "abstract": " The engineering process that underlies software development is examined. A brief summary of how information technology has affected both institutions and individuals in the past few decades is given. Engineering with models and metrics is then discussed. Improving software quality, making software engineering technology more transferable, and transferring software technology into an organization are addressed.< >", "num_citations": "206\n", "authors": ["140"]}
{"title": "Understanding and predicting the process of software maintenance releases\n", "abstract": " One of the major concerns of any maintenance organization is to understand and estimate the cost of maintenance releases of software systems. Planning the next release so as to maximize the increase in functionality and the improvement in quality are vital to successful maintenance management. The objective of the paper is to present the results of a case study in which an incremental approach was used to better understand the effort distribution of releases and build a predictive effort model for software maintenance releases. The study was conducted in the Flight Dynamics Division (FDD) of NASA Goddard Space Flight Center (GSFC). The paper presents three main results: (1) a predictive effort model developed for the FDD's software maintenance release process, (2) measurement-based lessons learned about the maintenance process in the FDD, (3) a set of lessons learned about the establishment of a\u00a0\u2026", "num_citations": "203\n", "authors": ["140"]}
{"title": "Investigating and improving a COTS-based software development\n", "abstract": " The work described in this paper is an investigation of COTS-based software development within a particular NASA environment, with an emphasis on the processes used. Fifteen projects using a COTS-based approach were studied and their actual process was documented. This process is evaluated to identify essential differences in comparison to traditional software development. The main differences, and the activities for which projects require more guidance, are requirements definition and COTS selection, high level design, integration and testing.", "num_citations": "196\n", "authors": ["140"]}
{"title": "An empirical study of a syntactic complexity family\n", "abstract": " A family of syntactic complexity metrics is defined that generates several metrics commonly occurring in the literature. The paper uses the family to answer some questions about the relationship of these metrics to error-proneness and to each other. Two derived metrics are applied; slope which measures the relative skills of programmers at handling a given level of complexity and r square which is indirectly related to the consistency of performance of the programmer or team. The study suggests that individual differences have a large effect on the significance of results where many individuals are used. When an individual is isolated, better results are obtainable. The metrics can also be used to differentiate between projects on which a methodology was used and those on which it was not.", "num_citations": "193\n", "authors": ["140"]}
{"title": "Lessons learned from 25 years of process improvement: the rise and fall of the NASA software engineering laboratory\n", "abstract": " For 25 years the NASA/GSFC Software Engineering Laboratory (SEL) has been a major resource in software process improvement activities. But due to a changing climate at NASA, agency reorganization, and budget cuts, the SEL has lost much of its impact. In this paper we describe the history of the SEL and give some lessons learned on what we did right, what we did wrong, and what others can learn from our experiences. We briefly describe the research that was conducted by the SEL, describe how we evolved our understanding of software process improvement, and provide a set of lessons learned and hypotheses that should enable future groups to learn from and improve on our quarter century of experiences.", "num_citations": "190\n", "authors": ["140"]}
{"title": "A reference architecture for the component factory\n", "abstract": " Software reuse can be achieved through an organization that focuses on utilization of life cycle products from previous developments. The component factory is both an example of the more general concepts of experience and domain factory and an organizational unit worth being considered independently. The critical features of such an organization are flexibility and continuous improvement. In order to achieve these features we can represent the architecture of the factory at different levels of abstraction and define a reference architecture from which specific architectures can be derived by instantiation. A reference architecture is an implementation and organization independent representation of the component factory and its environment. The paper outlines this reference  architecture, discusses the instantiation process, and presents some examples of specific architectures by comparing them in the framework\u00a0\u2026", "num_citations": "190\n", "authors": ["140"]}
{"title": "Models and metrics for software management and engineering\n", "abstract": " This paper attempts to characterize and present a state of the art view of several quantitative models and metrics of the software life cycle. These models and metrics can be used to aid in managing and engineering software projects. They deal with various aspects of the software process and product, including resource allocation and estimation, changes and errors, size, complexity and reliability. Some indication is given of the extent to which the various models have been used and the success they have achieved.", "num_citations": "190\n", "authors": ["140"]}
{"title": "Evolving and packaging reading technologies\n", "abstract": " Reading is a fundamental technology for achieving quality software. This paper provides a motivation for reading as a quality improvement technology, based upon experiences in the Software Engineering Laboratory at NASA Goddard Space Flight Center, and shows the evolution of our study of reading via a series of experiments. The experiments range from early reading vs. testing experiments to various Cleanroom experiments that employed reading to the development of new reading technologies currently under study.", "num_citations": "187\n", "authors": ["140"]}
{"title": "The experience factory and its relationship to other improvement paradigms\n", "abstract": " This paper describes the Quality Improvement Paradigm and the Experience Factory Organization as mechanisms for improving software development. It compares the approach with other improvement paradigms.", "num_citations": "187\n", "authors": ["140"]}
{"title": "Metric analysis and data validation across Fortran projects\n", "abstract": " The desire to predict the effort in developing or explain the quality of software has led to the proposal of several metrics in the literature. As a step toward validating these metrics, the Software Engineering Laboratory has analyzed the Software Science metrics, cyclomatic complexity, and various standard program measures for their relation to 1) effort (including design through acceptance testing), 2) development errors (both discrete and weighted according to the amount of time to locate and frix), and 3) one another. The data investigated are collected from a production Fortran environment and examined across several projects at once, within individual projects and by individual programmers across projects, with three effort reporting accuracy checks demonstrating the need to validate a database. When the data come from individual programmers or certain validated projects, the metrics' correlations with actual\u00a0\u2026", "num_citations": "169\n", "authors": ["140"]}
{"title": "COTS-based software development: Processes and open issues\n", "abstract": " The work described in this paper is an investigation of the COTS-based software development within a particular NASA environment, with an emphasis on the processes used. Fifteen projects using a COTS-based approach were studied and their actual process was documented. This process is evaluated to identify essential differences in comparison to traditional software development. The main differences, and the activities for which projects require more guidance, are requirements definition and COTS selection, high level design, integration and testing. Starting from these empirical observations, a new process and set of guidelines for COTS-based development are developed and briefly presented.", "num_citations": "168\n", "authors": ["140"]}
{"title": "Understanding and documenting programs\n", "abstract": " This paper reports on an experiment in trying to understand an unfamiliar program of some complexity and to record the authors' understanding of it. The goal was to simulate a practicing programmer in a program maintenance environment using the techniques of program design adapted to program understanding and documentation; that is, given a program, a specification and correctness proof were developed for the program. The approach points out the value of correctness proof ideas in guiding the discovery process. Toward this end, a variety of techniques were used: direct cognition for smaller parts, discovering and verifying loop invariants for larger program parts, and functions determined by additional analysis for larger program parts. An indeterminate bounded variable was introduced into the program documentation to summarize the effect of several program variables and simplify the proof of correctness.", "num_citations": "163\n", "authors": ["140"]}
{"title": "Applying the Goal/Question/Metric paradigm in the experience factory\n", "abstract": " This paper discusses the use of the Goal/Question/Metric (GQM) paradigm as a mechanism for supporting the setting of operational goals for software projects. These goals can be used to focus a particular software project from the perspective of the software manager or the organization itself so that the organization can learn more about its business and package its knowledge in an experience base in the form of models in the Experience Factory. This Experience Factory repiesents an organization within the corporation that actively acquires and packages knowledge to support current and future projects. GQM is used for defining and interpreting software measurement that must take place within and across projects. Templates are provided for defining goals and generating questions. Different types of metrics are discussed. Examples of both process and product goals are defined.", "num_citations": "148\n", "authors": ["140"]}
{"title": "Implementing the Experience Factory concepts as a set of Experience Bases.\n", "abstract": " This talk takes the Experience Factory concept, which was originally developed as organizational support for software development and generalizes it to organizational support for any aspect of a business, eg, business practices. The Experience Factory supports the evolution of processes and other forms of knowledge, based upon experiences within the organization, and related knowledge gathered from outside the organization. It then discusses how you might design an appropriate experience base for the particular set of organizational needs determined to be of importance. Specific examples are given in developing experience bases for specific organizations and it discusses the Experience Management System (EMS) currently being evolved and how it has been applied.", "num_citations": "143\n", "authors": ["140"]}
{"title": "Software quality: An overview from the perspective of total quality management\n", "abstract": " This essay presents a tutorial that discusses software quality in the context of total quality management (TQM). Beginning with a historical perspective of software engineering, the tutorial examines the definition of software quality and discusses TQM as a management philosophy along with its key elements: customer focus, process improvement, the human side of quality, and data, measurement, and analysis. It then focuses on the software-development specifics and the advancements made on many fronts that are related to each of the TQM elements. In conclusion, key directions for software quality improvements are summarized.", "num_citations": "135\n", "authors": ["140"]}
{"title": "Validation of an approach for improving existing measurement frameworks\n", "abstract": " Software organizations are in need of methods to understand, structure, and improve the data their are collecting. We have developed an approach for use when a large number of diverse metrics are already being collected by a software organization (M.G. Mendonca et al., 1998; M.G. Mendonca, 1997). The approach combines two methods. One looks at an organization's measurement framework in a top-down goal-oriented fashion and the other looks at it in a bottom-up data-driven fashion. The top-down method is based on a measurement paradigm called Goal-Question-Metric (GQM). The bottom-up method is based on a data mining technique called Attribute Focusing (AF). A case study was executed to validate this approach and to assess its usefulness in an industrial environment. The top-down and bottom-up methods were applied in the customer satisfaction measurement framework at the IBM Toronto\u00a0\u2026", "num_citations": "124\n", "authors": ["140"]}
{"title": "Perspective-based usability inspection: An empirical validation of efficacy\n", "abstract": " Inspection is a fundamental means of achieving software usability. Past research showed that the current usability inspection techniques were rather ineffective. We developed perspective-based usability inspection, which divides the large variety of usability issues along different perspectives and focuses each inspection session on one perspective. We conducted a controlled experiment to study its effectiveness, using a post-test only control group experimental design, with 24 professionals as subjects. The control group used heuristic evaluation, which is the most popular technique for usability inspection. The experimental design and the results are presented, which show that inspectors applying perspective-based inspection not only found more usability problems related to their assigned perspectives, but also found more overall problems. Perspective-based inspection was shown to be more effective\u00a0\u2026", "num_citations": "124\n", "authors": ["140"]}
{"title": "Experimental Software Engineering Issues:: Critical Assessment and Future Directions. International Workshop, Dagstuhl Castle, Germany, September 14-18, 1992. Proceedings\n", "abstract": " We have only begun to understand the experimental nature of software engineering, the role of empirical studies and measurement within software engineering, and the mechanisms needed to apply them successfully. This volume presents the proceedings of a workshop whose purpose was to gather those members of the software engineering community who support an engineering approach based upon empirical studies to provide an interchange of ideas and paradigms for research. The papers in the volume are grouped into six parts corresponding to the workshop sessions: The experimental paradigm in software engineering; Objectives and context of measurement/experimentation; Procedures and mechanisms for measurement/experimentation; Measurement-based modeling; packaging for reuse/reuse of models; and technology transfer, teaching and training. Each part opens with a keynote paper and ends with a discussion summary. The workshop served as an important event in continuing to strengthen empirical software engineering as a major subdiscipline ofsoftware engineering. The deep interactions and important accomplishments from the meeting documented in these proceedings have helped identify key issues in moving software engineering as a whole towards a true engineering discipline.", "num_citations": "121\n", "authors": ["140"]}
{"title": "Communication and organization: An empirical study of discussion in inspection meetings\n", "abstract": " This paper describes an empirical study that addresses the issue of communication among members of a software development organization. In particular, data was collected concerning code inspections in one software development project. The question of interest is whether or not organizational structure (the network of relationships between developers) has an effect on the amount of effort expended on communication between developers. The independent variables in this study are various attributes of the organizational structure in which the inspection participants work. The dependent variables are measures of the communication effort expended in various parts of the code inspection process, focusing on the inspection meeting. Both quantitative and qualitative methods were used, including participant observation, structured interviews, generation of hypotheses from field notes, statistical tests of\u00a0\u2026", "num_citations": "119\n", "authors": ["140"]}
{"title": "Foundations of empirical software engineering: the legacy of Victor R. Basili\n", "abstract": " Although software engineering can trace its beginnings to a NATO conf-ence in 1968, it cannot be said to have become an empirical science until the 1970s with the advent of the work of Prof. Victor Robert Basili of the University of Maryland. In addition to the need to engineer software was the need to understand software. Much like other sciences, such as physics, chemistry, and biology, software engineering needed a discipline of obs-vation, theory formation, experimentation, and feedback. By applying the scientific method to the software engineering domain, Basili developed concepts like the Goal-Question-Metric method, the Quality-Improvement-Paradigm, and the Experience Factory to help bring a sense of order to the ad hoc developments so prevalent in the software engineering field. On the occasion of Basili\u2019s 65th birthday, we present this book c-taining reprints of 20 papers that defined much of his work. We divided the 20 papers into 6 sections, each describing a different facet of his work, and asked several individuals to write an introduction to each section. Instead of describing the scope of this book in this preface, we decided to let one of his papers, the keynote paper he gave at the International C-ference on Software Engineering in 1996 in Berlin, Germany to lead off this book. He, better than we, can best describe his views on what is-perimental software engineering.", "num_citations": "118\n", "authors": ["140"]}
{"title": "SEL's software process improvement program\n", "abstract": " We select candidates for process change on the basis of quantified Software Engineering Laboratory (SEL) experiences and clearly defined goals for the software. After we select the changes, we provide training and formulate experiment plans. We then apply the new process to one or more production projects and take detailed measurements. We assess process success by comparing these measures with the continually evolving baseline. Based upon the results of the analysis, we adopt, discard, or revise the process.< >", "num_citations": "116\n", "authors": ["140"]}
{"title": "A controlled experiment quantitatively comparing software development approaches\n", "abstract": " A software engineering research study has been undertaken to empirically analyze and compare various software development approaches; its fundamental features and initial findings are presented in this paper. An experiment was designed and conducted to confirm certain suppositions concerning the beneficial effects of a particular disciplined methodology for software development. The disciplined methodology consisted of programming teams employing certain techniques and organizations commonly defined under the umbrella term structured programming. Other programming teams and individual programmers both served as control groups for comparison. The experimentally tested hypotheses involved a number of quantitative, objective, unobtrusive, and automatable measures of programming aspects dealing with the software development process and the developed software product. The experiment's\u00a0\u2026", "num_citations": "115\n", "authors": ["140"]}
{"title": "Representing software engineering models: the TAME goal-oriented approach\n", "abstract": " This paper describes a methodology as well as a knowledge representation and reasoning framework for top down goal oriented characterization, modeling and execution of software engineering activities. A prototype system (ES-TAME) is described which demonstrates the underlying knowledge representation and reasoning principles. ES-TAME provides an object-oriented meta-model concept in order to provide effective support for tailorable and reusable software engineering models. It provides the basic mechanisms, functions and attributes for all the other models. It is based on inter-object relationships, dynamic viewpoints and selective inheritance in addition to traditional object-oriented mechanisms. Descriptive software engineering models (SEMs) include representations for basic software engineering activities like life cycle models, project models, resource models, design methods, quality models etc\u00a0\u2026", "num_citations": "113\n", "authors": ["140"]}
{"title": "Analogy-based practical classification rules for software quality estimation\n", "abstract": " Software metrics-based quality estimation models can be effective tools for identifying which modules are likely to be fault-prone or not fault-prone. The use of such models prior to system deployment can considerably reduce the likelihood of faults discovered during operations, hence improving system reliability. A software quality classification model is calibrated using metrics from a past release or similar project, and is then applied to modules currently under development. Subsequently, a timely prediction of which modules are likely to have faults can be obtained. However, software quality classification models used in practice may not provide a useful balance between the two misclassification rates, especially when there are very few faulty modules in the system being modeled.               This paper presents, in the context of case-based reasoning, two practical classification rules that allow appropriate\u00a0\u2026", "num_citations": "107\n", "authors": ["140"]}
{"title": "An Investigation of Human Factors in Software Development.\n", "abstract": " Recently, considerable attention has been devoted to the notion that factors directly related to the psychological nature of human beings play a major role in the development of computer software. 110 If human factors do significantly affect software development, then varying the size of the programming team and the degree of methodological discipline\u2014two supposedly potent human factorsshould induce measurable differences in both the development process and the developed product. Controlled experimentation involving easily measured\" low-level\" programming aspects can serve not only to verify this hypothesis but also to accumulate a detailed set of empirically supported conclusions. Interpreting these conclusions in view of subjective reasoning about software can yield further understanding about the effect of human factors on certain\" high-level\" software properties that are difficult to measure directly.To\u00a0\u2026", "num_citations": "107\n", "authors": ["140"]}
{"title": "A characterisation schema for software testing techniques\n", "abstract": " One of the major problems within the software testing area is how to get a suitable set of cases to test a software system. This set should assure maximum effectiveness with the least possible number of test cases. There are now numerous testing techniques available for generating test cases. However, many are never used, and just a few are used over and over again. Testers have little (if any) information about the available techniques, their usefulness and, generally, how suited they are to the project at hand upon, which to base their decision on which testing techniques to use. This paper presents the results of developing and evaluating an artefact (specifically, a characterisation schema) to assist with testing technique selection. When instantiated for a variety of techniques, the schema provides developers with a catalogue containing enough information for them to select the best suited techniques for a\u00a0\u2026", "num_citations": "106\n", "authors": ["140"]}
{"title": "Software engineering practices in the US and Japan.\n", "abstract": " The term software engineering first appeared in the late 1960's to describe ways to develop, manage, and maintain software so that resulting products are reliable, correct, efficient, and flexible. I The 15 years of software engineering study by the computer science community has created a need to assess the impact that numerous advances have had on actual software production. To address this need, IBM asked the University of Maryland to conduct a survey of different program development environments in industry to determine the state of the art in software development and to ascertain which software engineering techniques are most effective. Unlike other surveys, such as the recent one on Japanesetechnology, 2 we were less interested in recent research topics. Journals, such as the IEEE Transactions on Software Engineering adequately report such developments; we were more interested in discovering which methods andtools are actually being used by industry today. 3 This report contains the results of that survey.The goal of this project, which began in spring 1981 and continued through summer 1983, was to sample about 20 organizations, including IBM, and study their develop-ment practices. We contacted major hardware vendors in the US, and most agreed to participate. Several other software companies and other\" high-technology\" companies were contacted and agreed to participate. While we ac-knowledge that this survey was not all inclusive, we did study each company in depth, and based on discussions with others in the field, we believe that what we found was typical.", "num_citations": "101\n", "authors": ["140"]}
{"title": "Practical benefits of goal-oriented measurement\n", "abstract": " Software measurement is an essential component of mature software technology. It supports qual-ity as well as project management. As far as quality management is concerned, measurement can help investigate software related phenomena and thus contribute to building better software pro-duct, process and quality models. As far as project management is concerned, measurement can help state software requirements unambiguously, assess their proper implementation throughout the software project, and achieve convincing product certification. The measurement goal of interest determines which metrics are appropriate. Over the years, several'top-down'measure-ment approaches for deriving metrics from goals have been proposed. Examples include the QFD approach by Akao, the SQM approach by Murine based on prior work by Boehm and McCall, and the GQM approach by Basili. In this paper, the practical benefits of reliability measurement based on the GQM approach are discussed.", "num_citations": "98\n", "authors": ["140"]}
{"title": "Analyzing medium-scale software development\n", "abstract": " The collection and analysis of data from programming projects is necessary for the appropriate evaluation of software engineering methodologies. Towards this end, the Software Engineering Laboratory was organized between the University of Maryland and NASA Goddard Space Flight Center. This paper describes the structure of the Laboratory and provides some data on project evaluation from some of the early projects that have been monitored. The analysis relates to resource forecasting using a model of the project life cycle based upon the Rayleigh equation and to error rates applying ideas developed by Belady and Lehman.", "num_citations": "97\n", "authors": ["140"]}
{"title": "Experience in implementing a learning software organization\n", "abstract": " In an effort to improve software development and acquisition processes and explicitly reuse knowledge from previous software projects, DaimlerChrysler created a software experience center (SEC). The authors report on challenges the company faced in creating the SEC.", "num_citations": "87\n", "authors": ["140"]}
{"title": "Defining factors, goals and criteria for reusable component evaluation.\n", "abstract": " This paper presents an approach for defining evaluation criteria for reusable software components. We introduce a taxonomy of factors that influence selection, describe each of them, and present a hierarchical decomposition method for deriving reuse goals from factors and formulating the goals into an evaluation criteria hierarchy. We present some highlights from two case studies in which the approach was applied. The approach presented in this paper is a part of the OTSO'method that has been developed for reusable component selection process.", "num_citations": "87\n", "authors": ["140"]}
{"title": "Aligning Organizations Through Measurement The GQM+ Strategies Approach\n", "abstract": " Building the right products and services as well as building products and services right is the key to the success of most organizations. This requires that an organization is able to establish well-suited goals and strategies, connect and communicate them to assure that all parts of the organization are working in the same direction, recognize when goals or strategies need to be changed, and understand the effects of those changes. Aligning and integrating goals and strategies in an organization helps direct all resources, competencies, and activities towards value creation. Aligning an organization\u2019s goals and strategies requires specifying the connections between them so that the links are explicit and allow for analytic reasoning about what is successful and where change is necessary. Applying measurement principles can support this analytic reasoning. In particular, goaloriented measurement helps\u00a0\u2026", "num_citations": "85\n", "authors": ["140"]}
{"title": "The role of controlled experiments in software engineering research\n", "abstract": " Empirical studies play an important role in the evolution of the software engineering discipline. They allow us to build a body of knowledge in software engineering that has been supported by observation and empirical evidence. These studies allow us to test out theories, identify important variables and to build models that can be supported by empirical evidence.", "num_citations": "85\n", "authors": ["140"]}
{"title": "Qualitative software complexity models: A summary\n", "abstract": " The panel accepted as a working definition of software complexity the following: Complexity is the measure of the resources expended by another system in interacting with a piece of software. Categories of systems that may interact with software are machines, other software, people, and even the external environment. If the interacting system is a machine, the measures deal with execution time and memory space. If the interacting system is software, the measures might focus on the number of interfaces. If the interacting system is people, the measures are concerned with human efforts to comprehend, to maintain, to change, to test, etc., that software. The external en-vironment acts more like a set of constraints; that is, if a software development project requires travel to another site, there are certain physical limitations and expenditures in travel time that must be considered.", "num_citations": "84\n", "authors": ["140"]}
{"title": "Gaining intellectual control of software development\n", "abstract": " Recent disruptions caused by several events have shown how thor-oughly the world has come to depend on software. The rapid pro-liferation of the Melissa virus hinted at a dark side to the ubiquitous connectivity that supports the information-rich Internet and lets e-commerce thrive. Although the oft-predicted Y2K apocalypse failed to materialize, many software experts insist that disaster was averted only because countries around the globe spent billions to ensure their critical software would be Y2K-compliant. When denial-of-service attacks shut down some of the largest sites on the Web last February, the concerns caused by the disruptions spread far beyond the complaints of frustrated customers, affecting even the stock prices of the targeted sites. Indeed, as software plays an ever-greater role in managing the daily functions of modern life, its economic importance becomes proportionately greater. It\u2019s no coincidence that technology stocks have led the upsurge of stock market indices, that the US government\u2019s antitrust case against Microsoft has become headline news around the world, or that some companies\u2019 aggressive pursuit of software patents has caused widespread controversy. Yet despite its critical importance, software remains surprisingly fragile. Prone to unpredictable performance, dangerously open to malicious attack, and vulnerable to failure at implementation despite the most rigorous development processes, in many cases software has been assigned tasks beyond its maturity and reliability.", "num_citations": "82\n", "authors": ["140"]}
{"title": "Using the GQM paradigm to investigate influential factors for software process improvement\n", "abstract": " In planning software process improvement activities, it is essential to determine the factors that most influence the success of a software project. In this article, we present an investigative and analytical framework for evaluating software process factors based on the Goal/Question/Metric (GQM) paradigm. We built descriptive models of the software process, defects, and cost. These models were used as a common basis of quantitative analysis in the study. We also developed evaluative models that clarify the relationship between the basic metrics, the analysis method, and the goals of the analysis. We confirmed the usefulness of our analytical framework, by applying it in an actual development environment at Matsushita Communication Industrial Company in Japan, where we studied four communications-software projects. This article reports the patterns we noted in the data and suggests process improvement\u00a0\u2026", "num_citations": "82\n", "authors": ["140"]}
{"title": "Software Process Improvement in the NASA Software Engineering Laboratory.\n", "abstract": " The Software Engineering Laboratory SEL was established in 1976 for the purpose of studying and measuring software processes with the intent of identifying improvements that could be applied to the production of ground support software within the Flight Dynamics Division FDD at the National Aeronautics and Space Administration NASAGoddard Space Flight Center GSFC. The SEL has three member organizations NASAGSFC, the University of Maryland, and Computer Sciences Corporation CSC. The concept of process improvement within the SEL focuses on the continual understanding of both process and product as well as goal-driven experimentation and analysis of process change within a production environment.Descriptors:", "num_citations": "76\n", "authors": ["140"]}
{"title": "A pilot study to compare programming effort for two parallel programming models\n", "abstract": " ContextWriting software for the current generation of parallel systems requires significant programmer effort, and the community is seeking alternatives that reduce effort while still achieving good performance.ObjectiveMeasure the effect of parallel programming models (message-passing vs. PRAM-like) on programmer effort.Design, setting, and subjectsOne group of subjects implemented sparse-matrix dense-vector multiplication using message-passing (MPI), and a second group solved the same problem using a PRAM-like model (XMTC). The subjects were students in two graduate-level classes: one class was taught MPI and the other was taught XMTC.Main outcome measuresDevelopment time, program correctness.ResultsMean XMTC development time was 4.8\u00a0h less than mean MPI development time (95% confidence interval, 2.0\u20137.7), a 46% reduction. XMTC programs were more likely to be correct, but the\u00a0\u2026", "num_citations": "75\n", "authors": ["140"]}
{"title": "A change analysis process to characterize software maintenance projects\n", "abstract": " In order to improve software maintenance processes, we need to be able to first characterize and assess them. This task needs to be performed in depth and with objectivity since the problems are complex. One approach is to set up a measurement program specifically aimed at maintenance. However, establishing a measurement program requires that one understands the issues and is able to characterize the maintenance environment and processes in order to collect suitable and cost-effective data. Also, enacting such a program and getting usable data sets takes time. A short term substitute is needed. We propose a characterization process aimed specifically at maintenance and based on a general qualitative analysis methodology. This process is rigorously defined in order to be repeatable and usable by people who are not acquainted with such analysis procedures. A basic feature of our approach is that\u00a0\u2026", "num_citations": "75\n", "authors": ["140"]}
{"title": "Programming measurement and estimation in the Software Engineering Laboratory\n", "abstract": " This paper presents an attempt to examine a set of basic relationships among various software development variables, such as size, effort, project duration, staff size, and productivity. These variables are plotted against each other for 15 Software Engineering Laboratory projects that were developed for NASA/Goddard Space Flight Center by Computer Sciences Corporation. Certain relationships are derived in the form of equations, and these equations are compared with a set derived by Walston and Felix for IBM Federal Systems Division project data. Although the equations do not have the same coefficients, they are seen to have similar exponents. In fact, the Software Engineering Laboratory equations tend to be within one standard error of estimate of the IBM equations.", "num_citations": "73\n", "authors": ["140"]}
{"title": "Communication and organization in software development: An empirical study\n", "abstract": " The empirical study described in this paper addresses the issue of communication among members of a software development organization. In particular, we have studied interactions between participants in a review process. The question of interest is whether or not organizational relationships among the review participants have an effect on the amount of communication effort expended. The study uses both quantitative and qualitative methods for data collection and analysis. These methods include participant observation, structured interviews, graphical data presentation, and nonparametric statistics. The results of this study indicate that several organizational factors do affect communication effort, but not always in a simple, straightforward way. Not surprisingly, people take less time to communicate when they are familiar with one another and when they work in close physical proximity. However, certain\u00a0\u2026", "num_citations": "70\n", "authors": ["140"]}
{"title": "Characterizing and modeling the cost of rework in a library of reusable software components\n", "abstract": " In this paper we characterize and model the cost of rework in a Component Factory (CF) organization. A CF is responsible for developing and packaging reusable software components. Data was collected on corrective maintenance activities for the Generalized Support Software reuse asset library located at the Flight Dynamics Division of NASA's GSFC. We then constructed a predictive model of the cost of rework using the C4. 5 system for generating a logical classification model. The predictor variables for the model are measures of internal software product attributes. The model demonstrates good prediction accuracy, and can be used by managers to allocate resources for corrective maintenance activities. Furthermore, we used the model to generate proscriptive coding guidelines to improve programming practices so that the cost of rework can be reduced in the future. The general approach we have used is\u00a0\u2026", "num_citations": "66\n", "authors": ["140"]}
{"title": "A unified model of dependability: Capturing dependability in context\n", "abstract": " In contemporary societies, individuals and organizations increasingly depend on services delivered by sophisticated software-intensive systems to achieve personal and business goals. So, a system must have engineered and guaranteed dependability, regardless of continuous, rapid, and unpredictable technological and context changes. The International Federation for Information Processing Working Group defines dependability as \"the trustworthiness\" of a computing system, which allows reliance to be justifiably placed on the services it deliver.", "num_citations": "65\n", "authors": ["140"]}
{"title": "A Prototype Experience Management System for a Software Consulting Organization.\n", "abstract": " The Experience Management System (EMS) is aimed at supporting the capture and reuse of software-related experience, based on the Experience Factory concept. It is being developed for use in a multinational software engineering consultancy, Q-Labs. Currently, a prototype EMS exists and has been evaluated. This paper focuses on the EMS architecture, underlying data model, implementation, and user interface.", "num_citations": "65\n", "authors": ["140"]}
{"title": "Evaluating automatable measures of software development\n", "abstract": " There is a need for distinguishing a set of useful automatable measures of the software aevelopment process and product. Measures ara considered useful if they are sensitive to externally observaole differences in development environments and their relative values correspond to some intuition regarding tnese characteristic differences. Such measures could provide an objective quantitative foundation for constructing quality assurance standards and for calibrating mathematical models of software reliability and resource estimation. This paper presents a set of automatable measures that were implemented, evaluated in a controlled experiment, and found to satisfy these usefulness criteria. The measures include computer job steps, program changes, program size, and cyclomatic complexity.", "num_citations": "65\n", "authors": ["140"]}
{"title": "A practical framework for eliciting and modeling system dependability requirements: Experience from the NASA high dependability computing project\n", "abstract": " The dependability of a system is contextually subjective and reflects the particular stakeholder\u2019s needs. In different circumstances, the focus will be on different system properties, e.g., availability, real-time response, ability to avoid catastrophic failures, and prevention of deliberate intrusions, as well as different levels of adherence to such properties. Close involvement from stakeholders is thus crucial during the elicitation and definition of dependability requirements. In this paper, we suggest a practical framework for eliciting and modeling dependability requirements devised to support and improve stakeholders\u2019 participation. The framework is designed around a basic modeling language that analysts and stakeholders can adopt as a common tool for discussing dependability, and adapt for precise (possibly measurable) requirements. An air traffic control system, adopted as testbed within the NASA High\u00a0\u2026", "num_citations": "64\n", "authors": ["140"]}
{"title": "An empirical study of communication in code inspections\n", "abstract": " This paper describes an empirical study which addresses the issue of communication among members of a software development organization. In particular, data was collected concerning code inspections in one software development project. The question of interest is whether or not organizational structure (the network of relationships between developers) has an effect on the amount of effort expended on communication between developers. Both quantitative and qualitative methods were used, including participant observation, structured interviews, generation of hypotheses from field notes, some simple statistical tests of relationships, and interpretation of results with qualitative anecdotes. The study results show that past and present working relationships between inspection participants affect the amount of meeting time spent in different types of discussion, thus affecting the overall meeting length. Reporting\u00a0\u2026", "num_citations": "64\n", "authors": ["140"]}
{"title": "An experience management system for a software engineering research organization\n", "abstract": " Most businesses rely on the fact that their employees possess relevant knowledge and that they can apply it to the task at hand. The problem is that this knowledge is not owned by the organization. It is owned and controlled by its employees. Maintaining an appropriate level of knowledge in the organization is a very important issue. It is, however, not an easy task for most organizations, and it is particularly problematic for software organizations, which are human- and knowledge-intensive. Knowledge management is a relatively new area that has attempted to address these problems. This paper introduces an approach called the \"knowledge dust-to-pearls\" approach. This approach addresses some of the issues with knowledge management by providing low-barrier mechanisms to \"jump start\" the experience base. This approach allows the experience base to become more useful more quickly than traditional\u00a0\u2026", "num_citations": "61\n", "authors": ["140"]}
{"title": "Combining self-reported and automatic data to improve programming effort measurement\n", "abstract": " Measuring effort accurately and consistently across subjects in a programming experiment can be a surprisingly difficult task. In particular, measures based on self-reported data may differ significantly from measures based on data which is recorded automatically from a subject's computing environment. Since self-reports can be unreliable, and not all activities can be captured automatically, a complete measure of programming effort should incorporate both classes of data. In this paper, we show how self-reported and automatic effort can be combined to perform validation and to measure total programming effort.", "num_citations": "59\n", "authors": ["140"]}
{"title": "Towards a comprehensive framework for reuse: A reuse-enabling software evolution environment\n", "abstract": " Reuse of products, processes and knowledge will be the key to enable the software industry to achieve the dramatic improvement in productivity and quality required to satisfy the anticipated growing demands. Although experience shows that certain kinds of reuse can be successful, general success has been elusive. A software life-cycle technology which allows broad and extensive reuse could provide the means to achieving the desired order-of-magnitude improvements. This paper motivates and outlines the scope of a comprehensive framework for understanding, planning, evaluating and motivating reuse practices and the necessary research activities. As a first step towards such a framework, a reuse-enabling software evolution environment model is introduced which provides a basis for the effective recording of experience, the generalization and tailoring of experience, the formalization of experience, and\u00a0\u2026", "num_citations": "59\n", "authors": ["140"]}
{"title": "Empirical studies to build a science of computer science\n", "abstract": " We learn to develop software by building, testing, and evolving models.", "num_citations": "55\n", "authors": ["140"]}
{"title": "Characterizing and assessing a large-scale software maintenance organization\n", "abstract": " One important component of a software process is the organizational context in which the process is enacted. This component is ofien missing or incomplete in current process modeling approaches. One technique for modeling this perspective is the Actor-Dependency (AD) Model. This paper reports on a case study which used this approach to analyze and assess a large software maintenance organization. Our goal was to identify the approach's strengths and weaknesses while providing practical recommendations for improvement and research directions. The AD model was found to be very useful in capturing the important properties of the organizational context of the maintenance process, and aided in the understanding of the flaws found in this process. However, a number of opportunities for extending and improving the AD model were identified. Among others, there is a need to incorporate quantitative\u00a0\u2026", "num_citations": "55\n", "authors": ["140"]}
{"title": "An approach to improving existing measurement frameworks\n", "abstract": " Software organizations are in need of methods for understanding, structuring, and improving the data they are collecting. This paper discusses an approach for use when a large number of diverse metrics are already being collected by a software organization. The approach combines two methods. One looks at an organization's measurement framework in a top-down fashion and the other looks at it in a bottom-up fashion. The top-down method, based on the goal-question-metric (GQM) paradigm, is used to identify the measurement goals of data users. These goals are then mapped to the metrics being used by the organization, allowing us to: (1) identify which metrics are and are not useful to the organization, and (2) determine whether the goals of data user groups can be satisfied by the data that are being collected by the organization. The bottom-up method is based on a data mining technique called attribute\u00a0\u2026", "num_citations": "54\n", "authors": ["140"]}
{"title": "The experience factory and its relationship to other quality approaches\n", "abstract": " This chapter describes the principles behind a specific set of integrated software quality improvement approaches which include the Quality Improvement Paradigm, an evolutionary and experimental improvement framework based on the scientific method and tailored for the software business, the Goal/Question/Metric Paradigm, a paradigm for establishing project and corporate goals and a mechanism for measuring against those goals, and the Experience Factory Organization, an organizational approach for building software competencies and supplying them to projects on demand. It then compares these approaches to a set of approaches used in other businesses, such as the Plan-Do-Check-Act, Total Quality Management, Lean Enterprise Systems, and the Capability Maturity Model.", "num_citations": "54\n", "authors": ["140"]}
{"title": "Metrics for Ada packages: an initial study\n", "abstract": " Many novel features of Ada present programmers with a formidable learning task. The study of four first-time Ada programmers suggests that a background in the software engineering practices supported by Ada is necessary to learn to use the features of the language.", "num_citations": "53\n", "authors": ["140"]}
{"title": "The ASC-Alliance projects: A case study of large-scale parallel scientific code development\n", "abstract": " Computational scientists face many challenges when developing software that runs on large-scale parallel machines. However, software-engineering researchers haven't studied their software development processes in much detail. To better understand the nature of software development in this context, the authors examined five large-scale computational science software projects operated at the five ASC-Alliance centers.", "num_citations": "51\n", "authors": ["140"]}
{"title": "A knowledge-based approach to the analysis of loops\n", "abstract": " The paper presents a knowledge-based analysis approach that generates first order predicate logic annotations of loops. A classification of loops according to their complexity levels is presented. Based on this taxonomy, variations on the basic analysis approach that best fit each of the different classes are described. In general, mechanical annotation of loops is performed by first decomposing them using data flow analysis. This decomposition encapsulates closely related statements in events, that can be analyzed individually. Specifications of the resulting loop events are then obtained by utilizing patterns, called plans, stored in a knowledge base. Finally, a consistent and rigorous functional abstraction of the whole loop is synthesized from the specifications of its individual events. To test the analysis techniques and to assess their effectiveness, a case study was performed on an existing program of reasonable\u00a0\u2026", "num_citations": "50\n", "authors": ["140"]}
{"title": "Towards automated support for extraction of reusable components\n", "abstract": " A cost effective introduction of software reuse techniques requires the reuse of existing software developed in many cases without aiming at reusability. This paper discusses the problems related to the analysis and reengineering of existing software in order to reuse it. We introduce a process model for component extraction and focus on the problem of analyzing and qualifying software components which are candidates for reuse. A prototype tool for supporting the extraction of reusable components is presented. One of the components of this tool aids in understanding programs and is based on the functional model of correctness. It can assist software engineers in the process of finding correct formal specifications for programs. A detailed description of this component and an example to demonstrate a possible operational scenario are given.", "num_citations": "50\n", "authors": ["140"]}
{"title": "Paradigms for experimentation and empirical studies in software engineering\n", "abstract": " The software engineering field requires major advances in order to attain the high standards of quality and productivity that are needed by the complex systems of the future. The immaturity of the field is reflected by the fact that most of its technologies have not yet been analyzed to determine their effects on quality and productivity. Moreover, when these analyses have occured the resulting guidance is not quantitative but only ethereal. One fundamental area of software engineering that is just beginning to blossom is the use of measurement techniques and empirical methods. These techniques need to be adopted by software researchers and practitioners in order to help the field respond to the demands being placed upon it. This paper outlines four paradigms for experimentation and empirical study in software engineering and describes their interrelationships: (1) Improvement paradigm (2) Goal-question-metric\u00a0\u2026", "num_citations": "49\n", "authors": ["140"]}
{"title": "Data collection, validation and analysis\n", "abstract": " One of the major problems with doing measurement of the software development process and the product is the ability to collect reliable data that can be used to understand and evaluate the development process and product and the various models and metrics. The data collection process consists of several phases\u2014establishing the environment in which the project is being developed, the actual data collection process itself, the validation of the collection process and the data, and, finally, the careful analysis and interpretation of that data with respect to specific models and metrics. We will discuss each of these phases.", "num_citations": "48\n", "authors": ["140"]}
{"title": "Using defect tracking and analysis to improve software quality\n", "abstract": " Defect tracking is a critical component to a successful software quality effort. In fact, Robert Grady of Hewlett-Packard stated in 1996 that \u201csoftware defect data is [the] most important available management information source for software process improvement decisions,\u201d and that \u201cignoring defect data can lead to serious consequences for an organization\u2019s business\u201d[Grady96]. However, classifying defects can be a difficult task. As Ostrand and Weyuker paraphrased a 1978 report by Thibodeau, defect classification schemes of that time period often had serious problems, including \u201cambiguous, overlapping, and incomplete categories, too many categories, and confusion of error causes, fault symptoms, and actual faults.\u201d[OstrandWeyuker84]. Yet the classification of defects is very important, and by examining the lessons learned by other organizations, one hopes to be in better position to implement or improve one\u2019s own defect classification and analysis efforts.To this end, this report discusses five defect categorization and analysis efforts from four different organizations. This list of organizations should be taken as a sample of the range of schemes covered in research and in industry over the previous twenty-five years. The analysis efforts at these organizations generally focus on one of three goals: finding the nature of defects, finding the location of defects, and finding when the defects are inserted, with the intent of using this information to characterize or analyze the environment or a specific development process. At the conclusion of this discussion, the results of two surveys covering the defect classification and analysis practices of approximately\u00a0\u2026", "num_citations": "46\n", "authors": ["140"]}
{"title": "An empirical study of perspective-based usability inspection\n", "abstract": " Inspection is a fundamental means of achieving software usability. Past research showed that during usability inspection the success rate (percentage of problems detected) of each individual inspector was rather low. We developed perspective-based usability inspection, which divides the large variety of usability issues along different perspectives and focuses each inspection session on one perspective. We conducted a controlled experiment to study its effectiveness, using a post-test only control group experimental design, with 24 professionals as subjects. The control group used heuristic evaluation, which is the most popular technique for usability inspection. The experimental results are that 1) for usability problems covered by each perspective, the inspectors using that perspective had higher success rate than others; 2) for all usability problems, perspective inspectors had higher average success rate than\u00a0\u2026", "num_citations": "45\n", "authors": ["140"]}
{"title": "An evaluation of expert systems for software engineering management\n", "abstract": " The development of four separate, prototype expert systems to aid in software engineering management is described. Given the values for certain metrics, these systems provide interpretations which explain any abnormal patterns of these values during the development of a software project. The four expert systems which solve the same problem, were built using two different approaches to knowledge acquisition, a bottom-up approach and a top-down approach and two different expert system methods, rule-based deduction and frame-based abduction. In a comparison to see which methods might better suit the needs of this field, it was found that the bottom-up approach led to better results that did the top-down approach, and the rule-based deduction systems using simple rules provided more complete and correct solutions than did the frame-based abduction systems.< >", "num_citations": "45\n", "authors": ["140"]}
{"title": "Quantitative assessment of maintenance: an industrial case study\n", "abstract": " In this paper we discuss a study aiming at the improvement of measurement and evaluation procedures used in an industrial maintenance environment. We used a general evaluation and improvement methodology for deriving a set of metrics tailored to the maintenance problems in this particular environment. Some of the required maintenance data were already collected in this environment, others were suggested to be collected in the future. We discuss the general measurement, evaluation and improvement methodology used, the specific maintenance improvement goals important to this environment, the set of metrics derived for quantifying those goals, the suggested changes to the current data collection procedures,\" and preliminary analysis results based on a limited set of already available data. It is encouraging that based on this limited set of data we are already able to demonstrate benefits of the\u00a0\u2026", "num_citations": "45\n", "authors": ["140"]}
{"title": "A classification procedure for the effective management of changes during the maintenance process.\n", "abstract": " During software operation, maintainers are often faced with numerous change requests. Given available resources such as effort and calendar time, changes, if approved, have to be planned to fit within budget and schedule constraints. In this paper, we address the issue of assessing the difficulty of a change based on known or predictable data. This paper should be considered as a first step towards the construction of customized economic models for maintainers. In it, we propose a modeling approach, based on regular statistical techniques, that can be used in a variety of software maintenance environments. This approach can be easily automated, and is simple for people with limited statistical experience to use. Moreover, it deals effectively with the uncertainty usually associated with both model inputs and outputs. The modeling approach is validated on a data set provided by the NASA Goddard Space Flight\u00a0\u2026", "num_citations": "44\n", "authors": ["140"]}
{"title": "Metrics of software architecture changes based on structural distance\n", "abstract": " Software architecture is an important form of abstraction, representing the overall system structure and the relationship among components. When software is modified from one version to another, its architecture may change. Software modification involving architectural change is often difficult when the change goes beyond the original architectural design, involving changes to the connectivity of multiple components. Existing research has looked at architectural change at the level of architecture metrics such as size, complexity, coupling and cohesion, which abstract a particular version of the software in isolation. In this paper, we argue that this level of abstraction is often too high to characterize some interesting aspects of the architectural change process, and propose an approach that takes into account the change in connectivity from version to version of individual components. In this approach, two endpoints of\u00a0\u2026", "num_citations": "43\n", "authors": ["140"]}
{"title": "Calculation and use of an environment's characteristic software metric set\n", "abstract": " Since both cost/quality and production environments differ, this study presents an approach for customizing a characteristic set of software metrics to an environment. The approach is applied in the Software Engineering Laboratory (SEL), a NASA Goddard production environment, to 49 candidate process and product metrics of 652 modules from six (51,000 to 112,000 lines) projects. For this particular environment, the method yielded the characteristic metric set (source lines, fault correction effort per executable statement, design effort, code effort, number of I/O parameters, number of versions). The uses examined for a characteristic metric set include forecasting the effort for development, modification, and fault correction of modules based on historical data.", "num_citations": "42\n", "authors": ["140"]}
{"title": "The Experience Factory: How to build and run one\n", "abstract": " Near-Video-on-Demand (NVOD) provides different service than true Video-on-Demand (VOD). Since the NVOD service needs to be much cheaper than the VOD service, it is important to minimize the server's cost. In this paper, we present a novel video layout strategy for NVOD servers that enforces sequential disk access. Thus, the disk bandwidth is optimally utilized. We define a model that analyzes the storage subsystem behavior, buffer requirements, and usage. Strategies that match the actual disk bandwidth to the application bandwidth requirements are developed. Using this layout strategy, each disk can deliver 50% more streams than can a VOD system, and memory buffers are reduced by almost half. Since disks and memory account for a significant portion of the total system cost in a video server, using these strategies significantly reduces server costs. From this layout strategy we further derive two other\u00a0\u2026", "num_citations": "40\n", "authors": ["140"]}
{"title": "Empirical evaluation of a risk management method\n", "abstract": " This paper argues that three main obstacles for wider use of risk management technology are low awareness of the technology, limitations of existing risk management approaches, and lack of empirical evidence of the usefulness of risk management methods. This paper addresses the last two of these issues. First, we present a risk management method that attempts to avoid the limitations we have recognized in many current risk management approaches. The method, called Riskit, allows a thorough documentation of risk scenarios, uses a sound approach for ranking risks, and supports multiple goals and stakeholders. Second, we will discuss the inherent difficulties in evaluating risk management methods empirically and present an example of an empirical study that was carried out to evaluate the feasibility of the method.", "num_citations": "40\n", "authors": ["140"]}
{"title": "An analysis of errors in a reuse-oriented development environment\n", "abstract": " Component reuse is widely considered vital for obtaining significant improvement in development productivity. However, as an organization adopts a reuse-oriented development process, the nature of the problems in development is likely to change. In this article, we use a measurement-based approach to better understand and evaluate an evolving reuse process. More specifically, we study the effects of reuse across seven projects in narrow domain from a single development organization. An analysis of the errors that occur in new and reused components across all phases of system development provides insight into the factors influencing the reuse process. We found significant differences between errors associated with new and various types of reused components in terms of the types of errors committed. In addition, we identified differences when errors are introduced and the effect that the errors have on the\u00a0\u2026", "num_citations": "39\n", "authors": ["140"]}
{"title": "Error localization during software maintenance: generating hierarchical system descriptions from the source code alone.\n", "abstract": " One central feature of the structure of a software system is the coupling among its components (eg, subsystems, modules) and the cohesion within them. The purpose of this study is to quantify ratios of coupling and cohesion and use them in the generation of hierarchical system descriptions. The ability of the hierarchical descriptions to localize errors by identifying error-prone system structure is evaluated using actual error data. Measures of data interaction, called data bindings, are used as the basis for calculating software coupling and cohesion. A 135,000 source line system from a production environment has been selected for empirical analysis. Software error data was collected from high-level system design through system test and from some field operation of the system. A set of five tools is applied to calculate the data bindings automatically, and cluster analysis is used to determine a hierarchical description\u00a0\u2026", "num_citations": "38\n", "authors": ["140"]}
{"title": "User interface evaluation and empirically-based evolution of a prototype experience management tool\n", "abstract": " Experience management refers to the capture, structuring, analysis, synthesis, and reuse of an organization's experience in the form of documents, plans, templates, processes, data, etc. The problem of managing experience effectively is not unique to software development, but the field of software engineering has had a high-level approach to this problem for some time. The Experience Factory is an organizational infrastructure whose goal is to produce, store, and reuse experiences gained in a software development organization. This paper describes The Q-Labs Experience Management System (Q-Labs EMS), which is based on the Experience Factory concept and was developed for use in a multinational software engineering consultancy. A critical aspect of the Q-Labs EMS project is its emphasis on empirical evaluation as a major driver of its development and evolution. The initial prototype requirements were\u00a0\u2026", "num_citations": "37\n", "authors": ["140"]}
{"title": "Evaluating and comparing software metrics in the software engineering laboratory\n", "abstract": " There has appeared in the literature a great number of metrics that attempt to measure the effort or complexity in developing and understanding software(1). There have also been several attempts to independently validate these measures on data from different organizations gathered by different people(2). These metrics have many purposes. They can be used to evaluate the software development process or the software product. They can be used to estimate the cost and quality of the product. They can also be used during development and evolution of the software to monitor the stability and quality of the product. Among the most popular metrics have been the software science metrics of Halstead, and the cyclomatic complexity metric of McCabe. One question is whether these metrics actually measure such things as effort and complexity. One measure of effort may be the time required to produce a product. One\u00a0\u2026", "num_citations": "36\n", "authors": ["140"]}
{"title": "Empirical Software Engineering Issues. Critical Assessment and Future Directions: International Workshop, Dagstuhl Castle, Germany, June 26-30, 2006, Revised Papers\n", "abstract": " Victor R. Basili, Dieter Rombach, and Kurt Schneider Introduction In 1992, a Dagstuhl seminar was held on \u201cExperimental Software Engineering Issues\u201d(seminar no. 9238). Its goal was to discuss the state of the art of empirical software engineering (ESE) by assessing past accomplishments, raising open questions, and proposing a future research agenda. Since 1992, the topic of ESE has been adopted more widely by academia as an interesting and promising research topic, and in industrial practice as a necessary infrastructure technology for goal-oriented, sustained process improvement. At the same time, the spectrum of methods applied in ESE has broadened. For example, in 1992, the empirical methods applied in software engineering were basically restricted to quantitative studies (mostly controlled experiments), whereas since then, a range of qualitative methods have been introduced, from observational to ethnographical studies. Thus, the field can be said to have moved from experimental to empirical software engineering. We believe that it is now time to again bring together practitioners and researchers to identify both the progress made since 1992 and the most important challenges for the next five to ten years.", "num_citations": "35\n", "authors": ["140"]}
{"title": "Quantitative Software-Qualit\u00e4tssicherung: Eine Methode zur Definition und Nutzung geeigneter Ma\u00dfe\n", "abstract": " Quantitative Software-Qualit\u00e4tssicherung: Eine Methode zur Definition und Nutzung geeigneter Ma\u00dfe - Digitale Bibliothek - Gesellschaft f\u00fcr Informatik eV GI Logo GI Logo Anmelden Digitale Bibliothek Gesamter Bestand Bereiche & Sammlungen Titel Autor Erscheinungsdatum Schlagwort Diese Sammlung Titel Autor Erscheinungsdatum Schlagwort Toggle navigation Digital Bibliothek der Gesellschaft f\u00fcr Informatik eV GI-DL English Deutsch Deutsch English Deutsch Dokumentanzeige Startseite Informatik Spektrum Band 10 (1987) Band 10 - Heft 3 (Juni 1987) Dokumentanzeige Startseite Informatik Spektrum Band 10 (1987) Band 10 - Heft 3 (Juni 1987) Dokumentanzeige Quantitative Software-Qualit\u00e4tssicherung: Eine Methode zur Definition und Nutzung geeigneter Ma\u00dfe Autor(en): Rombach, H. Dieter [DBLP] ; Basili, Victor R. [DBLP] Vollst\u00e4ndige Referenz BibTeX Rombach, HD & Basili, VR, (1987). Quantitative -: . -\u2026", "num_citations": "34\n", "authors": ["140"]}
{"title": "What\u2019s working in HPC: Investigating HPC user behavior and productivity\n", "abstract": " Productivity in High Performance Computing (HPC) systems can be difficult to define, complicated by the sometimes competing motivations of the people involved. For example, scheduling policies at many centers are geared toward maximizing system utilization, while users are motivated only by the desire to produce scientific results. Neither of these motivating forces directly relates to the common metric widely put forward as a measure of merit in HPC: high code performance as measured in floating-point operations per second (FLOPS). This paper evaluates some factors contributing to the net gain or loss of productivity for users on today\u2019s HPC systems, and explores whether or not those factors are accurately being accounted for in the way systems are evaluated and scheduled. Usage patterns are identified through job logs and ticket analysis, and further explained with user surveys and interviews. This paper reveals insight into productivity on current HPC systems, where user\u2019s time is spent, what bottlenecks are experienced, and the resulting implications for HPC system design, use and administration.", "num_citations": "33\n", "authors": ["140"]}
{"title": "Q\u2010MOPP: qualitative evaluation of maintenance organizations, processes and products\n", "abstract": " In this paper, we propose a qualitative, inductive method for characterizing and evaluating software maintenance processes, thereby identifying their specific problems and needs. This method encompasses a set of procedures which attempt to determine causal links between maintenance problems and flaws in the maintenance organization and process. This allows for a set of concrete steps to be taken for maintenance quality and productivity improvement, based on a tangible understanding of the relevant maintenance issues in a particular maintenance environment. Moreover, this understanding provides a solid basis on which to define relevant software maintenance models and measures. A case study of the application of this method, called Q\u2010MOPP, is presented to further illustrate its feasibility and benefits. \u00a9 1998 John Wiley & Sons, Ltd.", "num_citations": "32\n", "authors": ["140"]}
{"title": "A comparative analysis of functional correctness\n", "abstract": " The functional correctness technique is presented and discussed. It is also explained that the underlying theory has an implicatmn for the derivation of loop invariants. The functional verification conditions concerning program loops are then shown to be a specialization of the commonly used inductive assertion verification conditions. Next, the functional technique is compared and contrasted with subgoal induction. Finally, the difficulty of proving initialized loop programs is examined in light of both the inductive assertion and functional correctness theories.", "num_citations": "32\n", "authors": ["140"]}
{"title": "Application of a development time productivity metric to parallel software development\n", "abstract": " Evaluation of High Performance Computing (HPC) systems should take into account software development time productivity in addition to hardware performance, cost, and other factors. We propose a new metric for HPC software development time productivity, defined as the ratio of relative runtime performance to relative programmer effort. This formula has been used to analyze several HPC benchmark codes and classroom programming assignments. The results of this analysis show consistent trends for various programming models. This method enables a high-level evaluation of development time productivity for a given code implementation, which is essential to the task of estimating cost associated with HPC software development.", "num_citations": "31\n", "authors": ["140"]}
{"title": "ARROWSMITH-P: A prototype expert system for software engineering management\n", "abstract": " Although the field of software engineering is relatively new, it can benefit from the use of expert systems. Two prototype expert systems were developed to aid in software engineering management. Given the values for certain metrics, these systems will provide interpretations which explain any abnormal patterns of these values during the development of a software project. The two systems, which solve the same problem, were built using different methods, rule-based deduction and frame-based abduction. A comparison was done to see which method was better suited to the needs of this field. It was found that both systems performed moderately well, but the rule-based deduction system using simple rules provided more complete solutions than did the frame-based abduction system.", "num_citations": "31\n", "authors": ["140"]}
{"title": "Cots-based systems\u2013twelve lessons learned about maintenance\n", "abstract": " This paper presents the twelve most significant lessons the CeBASE community has learned across a wide variety of projects, domains, and organizations about COTS-Based Systems (CBS) maintenance. Because many of the lessons identified are not intuitive, the source and implications of the lesson are discussed as well within the context of maintenance model for CBS.", "num_citations": "30\n", "authors": ["140"]}
{"title": "The experience factory organization\n", "abstract": " can conduct this task during the project and shortly after they complete it. It addresses both acquiring knowledge that was not documented as part of the core activities and analyzing documents to create new knowledge. Included in this task are all forms of lessons learned and postmortem analyses that identify what went right or wrong regarding both the software product and process.These activities also include project data analyses, such as comparisons of estimated and actual costs and effort, planned and actual calendar time, or analysis of change history to reflect project events. These tasks collect and create knowledge about a particular project; any organization can perform them. Although these activities\u2019 results are useful by themselves, they can also be the basis for further knowledge creation and learning. They can be", "num_citations": "30\n", "authors": ["140"]}
{"title": "Risk knowledge capture in the Riskit Method\n", "abstract": " This paper describes how measurement data and experience can be captured for risk management purposes. The approach presented is a synthesis of the Riskit risk management method and the Experience Factory. In this paper we describe the main goals for risk knowledge capture and derive a classification of information based on those goals. We will describe the Riskit method and its integration with the Experience Factory. We will also outline the initial experiences we have gained from applying the proposed approach in practice.", "num_citations": "30\n", "authors": ["140"]}
{"title": "Monitoring software development through dynamic variables\n", "abstract": " This paper describes research conducted by the Software Engineering Laboratory (SEL) on the use of dynamic variables as a tool to monitor software development. The intent of the project is to identify project independent measures which may be used in a management tool for monitoring software development. This study examines several Fortran projects with similar profiles. The staff was experienced in developing these types of projects. The projects developed serve similar functions. Because these projects are similar we believe some underlying relationships exist that are invariant between the projects. These relationships, once well defined, may be used to compare the development of different projects to determine whether they are evolving the same way previous projects in this environment evolved.", "num_citations": "29\n", "authors": ["140"]}
{"title": "Overview of the Software Engineering Laboratory\n", "abstract": " The Software Engineering Laboratory (SEL) is an organization which is' functioning for the purpose of studying and evaluating software development techniques in an environment where scientific application software systems are routinely generated to support efforts at the National Aeronautics and Space Administration (NASA). This laboratory has been a joint effort between NASA/Goddard Space Flight Center (GSFC), Computer Sciences Corporation (CSC), Computer Sciences Technicolor Associates (CSTA), and the University of Maryland.", "num_citations": "29\n", "authors": ["140"]}
{"title": "Open source software development process modeling\n", "abstract": " This chapter draws attention to software process modeling for open source software development. It proposes a three-layered open source software development process model. Its \u2018definitional\u2019 and \u2018generic\u2019 levels specify the common features of all fully-fledged open source projects. Its\u2019 specific\u2019 level allows to describe fine-grained process model fragments characteristics of different open source projects. In this chapter, the specific level is exemplified with the release management process of NetBeans IDE and Apache HTTP Server projects. The underlying modeling approach is SPEM (Software Process Engineering Meta-model) from the OMG. The paper closes with a discussion of the interest of explicit software process models for (1) process understanding and communication, (2) process comparison, reuse, and improvement, (3) process enactment support.", "num_citations": "28\n", "authors": ["140"]}
{"title": "Software engineering research and industry: a symbiotic relationship to foster impact\n", "abstract": " Software engineering is not only an increasingly challenging endeavor that goes beyond the intellectual capabilities of any single individual engineer but also an intensely human one. Tools and methods to develop software are employed by engineers of varied backgrounds within a large variety of organizations and application domains. As a result, the variation in challenges and practices in system requirements, architecture, and quality assurance is staggering. Human, domain, and organizational factors define the context within which software engineering methodologies and technologies are to be applied and therefore the context that research needs to account for, if it is to be impactful. This article provides an assessment of the current challenges faced by software engineering research in achieving its potential, a description of the root causes of such challenges, and a proposal for the field to move forward\u00a0\u2026", "num_citations": "27\n", "authors": ["140"]}
{"title": "An experience management system for a software consulting organization\n", "abstract": " Software is a major expense for most organizations and is on the critical path to almost all organizational activities. Individual software development organizations in general strive to develop higher quality systems at a lower cost for both their internal and external customers. Yet the processes used to develop such software are still very primitive in the way that experience is incorporated. Learning is often from scratch, and each new development team has to relearn the mistakes of its predecessors. Reuse of an organization\u2019s own products, processes, and experience is becoming more accepted as a feasible solution to this problem. But implementation of the idea, in most cases, has not gone beyond reuse of small-scale code components in very specific, well-defined, situations. True learning within a software development organization requires that organizational experiences, both technological and social, be analyzed and synthesized so that members of the organization can learn from them and apply them to new problems.Suppose, for example, that a member of a software development group is considering the use of a particular software engineering technology on a forthcoming project. This member has heard that this technology has been used successfully in other projects in some other part of the organization, but cannot easily find out where or by whom. He or she would like very much to learn from the experiences of those previous projects, first to help make the decision to use the technology or not, then to help implement the technology in the current project. It would be helpful, obviously, to avoid the inevitable mistakes that are made\u00a0\u2026", "num_citations": "27\n", "authors": ["140"]}
{"title": "A heuristic for deriving loop functions\n", "abstract": " The problem of analyzing an initialized loop and verifying that the program computes some particular function of its inputs is addressed. A heuristic technique for solving these problems is proposed that appears to work well in many commonly occurring cases. The use of the technique is illustrated with a number of applications. An attribute of initialized loops is identified that corresponds to the ``effort'' required to apply this method in a deterministic (i.e., guaranteed to succeed) manner. It is explained that in any case, the success of the proposed heuristic relies on the loop exhibiting a ``reasonable'' form of behavior.", "num_citations": "27\n", "authors": ["140"]}
{"title": "Evaluating software development characteristics: Assessment of software measures in the Software Engineering Laboratory.[reliability engineering]\n", "abstract": " The purpose of this presentation is to discuss some of the work done on metrics in the Software Engineering Laboratory. To put things in perspective, there are many factors that affect software quality and each of these factors has several criteria which define it. Metrics represent some sort of measurement as to whether or not we have achieved a particular criteria. For example, one factor that we would like the software to possess is reliability. One of the many criteria that goes to make up this generalized factor of reliability might be fault tolerance. One of the metrics that can be used to evaluate fault tolerance might be the number of crashes of the system.There are many views of metrics.. We can think of metrics as being subjective or objective. Subjective metrics normally do not involve any exact measurement;.. they. tend to.... be.. an estimate-of extent to a^ degree in the\u2014application of some technique or a classification or qualification of a problem or experience. Subjective metrics are usually done on a relative scale; eg, they may be binary (yes or no), or discrete numbers (zero, 1, 2, 3). Examples of subjective metrics would be a qualitative judgment on the use of Process Design Language or an evaluation of the experience of programmers in a particular application. Objective metrics, on the other hand, tend to be absolute measures taken on the product or process. For example, the time of development,", "num_citations": "27\n", "authors": ["140"]}
{"title": "A transportable extendable compiler\n", "abstract": " This report describes the development of a transportable extendable self\u2010compiler for the language SIMPL\u2010T. SIMPL\u2010T is designed as the base language for a family of languages. The structure of the SIMPL\u2010T compiler and its transportable bootstrap are described. In addition, the procedures for generating a compiler for a new machine and for boot\u2010strapping the new compiler on to the new machine are demonstrated.", "num_citations": "27\n", "authors": ["140"]}
{"title": "Analyzing the test process using structural coverage\n", "abstract": " A large, commercially developed FORTRAN program was modiiled to produce structural coverage metrics. The modifled program was executed on a set of functionally generated acceptance tests and a large sample of operational usage cases. The resulting structural coverage metrics are combined with fauit and error data to evaiuate structurai coverage in the SEL environmen t.We can show that in this environment the functionally generated tests seem to be a good approximation of operational use. The relative proportions of the exercised statement subclasses (executable, assignment, CALL, DO, IF, READ, WRITE) changes as the structural coverage of the program increases. We also propose a method for evaluating if two sets of input data exercise a program in a similar manner.", "num_citations": "26\n", "authors": ["140"]}
{"title": "Identifying domain-specific defect classes using inspections and change history\n", "abstract": " We present an iterative, reading-based methodology for analyzing defects in source code when change history is available. Our bottom-up approach can be applied to build knowledge of recurring defects in a specific domain, even if other sources of defect data such as defect reports and change requests are unavailable, incomplete or at the wrong level of abstraction for the purposes of the defect analysis. After defining the methodology, we present the results of an empirical study where our method was applied to analyze defects in parallel programs which use the MPI (Message Passing Interface) library to express parallelism. This library is often used in the domain of high performance computing, where there is much discussion but little empirical data about the frequency and severity of defect types. Preliminary results indicate the methodology is feasible and can provide insights into the nature of real defects\u00a0\u2026", "num_citations": "25\n", "authors": ["140"]}
{"title": "SIMPL-T: A structured programming language\n", "abstract": " SIMPL-T is a member of a family of languages that are designed to be relatively machine independent and whose compilers are relatively transportable onto a variety of machines. It is a procedure oriented, nonblock structured programming language that was designed to conform to the standards of structured programming and modular design. There are three data types in SIMPL-T: integer, string and character.The first member of the SIMPL family, the typeless language SIMPL-X, was bootstrapped onto the 1108 in the Fall of 1972. The implementation of SIMPL-T was completed in January, 1974. This Computer Note is primarily intended as the reference manual for SIMPL-T. However since it is anticipated that it will be used in teaching SIMPL-T, the material has been organized so that the manual can be used in the classroom.", "num_citations": "25\n", "authors": ["140"]}
{"title": "Measuring productivity on high performance computers\n", "abstract": " In the high performance computing domain, the speed of execution of a program has typically been the primary performance metric. But productivity is also of concern to high performance computing developers. In this paper we will discuss the problems of defining and measuring productivity for these machines and we develop a model of productivity that includes both a performance component and a component that measures the development time of the program. We ran several experiments using students in high performance courses at several universities, and we report on those results with respect to our model of productivity.", "num_citations": "24\n", "authors": ["140"]}
{"title": "Investigation of audience perceptions of Transport Accident Commission road safety advertising\n", "abstract": " The perceptions of the Transport Accident Commission (TAC) road safety public education program held by 90 drivers were investigated. Participants provided ratings of the general approach used by the TAC, their recollection of how they had responded to selected TAC advertisements in the past, and their responses to TAC advertisements viewed during the assessment session. The results indicated that the participants were generally positive towards the TAC program, that their perceptions or attitudes were largely driven by the style of advertising (emotive vs. enforcement), and that advertisements judged to be effective tended to be rated as strongly emotive and highly informative or original. The results are discussed in terms of their implications for future development of new advertisements and for future research in this area. Key Words: Public education, driver behaviour, highway safety Reproduction of this page is authorised", "num_citations": "24\n", "authors": ["140"]}
{"title": "Documenting programs using a library of tree structured plans\n", "abstract": " An overview of a knowledge-based approach which helps in the mechanical documentation and understanding of computer programs is given. This approach performs mechanical annotation of loops by first decomposing them into fragments, called events. It then recognizes the high-level concepts, represented by the events, based on patterns, called plans, stored in a knowledge-base. The design and utilization of the plans are described in detail, and how to generalize their structure is discussed. The generalized tree structure can facilitate plan recognition and reduce the size of the knowledge-base. A case study on a real program of some practical importance, containing a set of 77 loops, has been performed. Results concerning the plans designed for this case study are given.< >", "num_citations": "24\n", "authors": ["140"]}
{"title": "Reusing existing software\n", "abstract": " The paper examines the problems related to the analysis of existing software in order to reuse it. The source programs are analyzed in two steps: the first step is dedicated to the identification of the reusable components, the second one to their computer assisted classification. The paper deals in more detail with the identification phase on the basis of the research work in progress on a system called CARE (Computer Aided Reuse Engineering). t This work was supported by the ITALSIEL Sp. A. with a Grant given to the Industrial Associated Program of the Department of Computer Science of the University of Maryland. Computer support provided in part through the facilities of the Computer Sci-ence Center at the University of Maryland.", "num_citations": "23\n", "authors": ["140"]}
{"title": "Characterization of an Ada software development\n", "abstract": " This article examines the use of Ada in a software project developed by the General Electric Company. The project was monitored by the University of Maryland and GE to identify areas of success and difficulty in learning and using Ada as both a design and a coding language. Since production-quality Ada translators were not readily available, the study focused on training and early software development. Our study also presents the major factors to consider before using Ada in software development, particularly when training in Ada is necessary. Our study attempts to meet several goals. The first focuses on characterization of the effort, the changes, and the errors of the project. The second considers how Ada was used on the project. The third concerns evaluation of the data collection and validation process, while the fourth concentrates on the development of measures for the Ada Programming Support Environment.Descriptors:", "num_citations": "22\n", "authors": ["140"]}
{"title": "Aligning corporate and IT goals and strategies in the oil and gas industry\n", "abstract": " Companies increasingly recognize that IT plays a significant role in their current and future business strategies, and IT departments increasingly need to justify their role in terms of contributions to business goals. Currently, little experience exists on how to effectively create this missing business-IT link. In 2010, ECOPETROL, a global player in the oil and gas industry, launched an initiative to align their IT activities with their overall business goals. IT is becoming a key information provider for making business-oriented decisions and achieving business success. Consequently, the view of IT is from that of a support organization to that of a value-creating information provider. This article describes how ECOPETROL is utilizing IT services to improve their competitiveness in the marketplace. They are applying the GQM\u2009+\u2009Strategies\u00ae approach for measurement-based IT-business alignment.", "num_citations": "21\n", "authors": ["140"]}
{"title": "Analysis of parallel software development using the relative development time productivity metric\n", "abstract": " We have previously introduced a common metric for measuring relative development time productivity of HPC software development. The RDTP metric has been applied to data from benchmark codes and classroom experiments, with consistent trends for various programming models.In general the results support the theory that traditional HPC programming models such as MPI yield good speedup but require more relative effort than other implementations (Figure 5). OpenMP generally provides speedup comparable to MPI, but requires less effort. This leads to higher values of the RDTP metric. There are questions of scalability with regard to OpenMP that are not addressed by this study.", "num_citations": "21\n", "authors": ["140"]}
{"title": "Open source and empirical software engineering\n", "abstract": " Editorial: Open Source and Empirical Software Engineering: Empirical Software Engineering: Vol 6, No 3 ACM Digital Library home ACM home Google, Inc. (search) Advanced Search Browse About Sign in Register Advanced Search Journals Magazines Proceedings Books SIGs Conferences People More Search ACM Digital Library SearchSearch Advanced Search Empirical Software Engineering Periodical Home Latest Issue Archive Authors Affiliations Award Winners More HomeBrowse by TitlePeriodicalsEmpirical Software EngineeringVol. , No. Editorial: Open Source and Empirical Software Engineering article Editorial: Open Source and Empirical Software Engineering Share on Author: Victor Robert Basili profile image Victor R. Basili View Profile Authors Info & Affiliations Publication: Empirical Software EngineeringSeptember 2001 8citation 0 Downloads Metrics Total Citations8 Total Downloads0 Last 12 0 6 \u2026", "num_citations": "21\n", "authors": ["140"]}
{"title": "Metrics of Interest in an ADA Development\n", "abstract": " The emergence of Ada provides the opportunity and necessity for measurement, analysis, and experimentation in software development. Over the past several months, we have been studying a software project developed in Ada. One of the goals of the study is to identify metrics which are useful for evaluating and predicting the complexity, quality, and cost of Ada programs. This reprint defines a set of metrics for use with software development in Ada. The metrics are gathered into six categories effort, changes, dimension, langauge use, data use, and execution. They are described further using formula generators, distributions, and formulas. Examples of each metric, as well as specific uses, are also included. Finally, our continuing research in this area is described. AuthorDescriptors:", "num_citations": "21\n", "authors": ["140"]}
{"title": "The past, present, and future of experimental software engineering\n", "abstract": " This paper gives a 40 year overview of the evolution of experimental software engineering, from the past to the future, from a personal perspective. My hypothesis is that my work followed the evolution of the field. I use my own experiences and thoughts as a barometer of how the field has changed and present some opinions about where we need to go.", "num_citations": "20\n", "authors": ["140"]}
{"title": "Customizing gqm models for software project monitoring\n", "abstract": " This paper customizes Goal/Question/Metric (GQM) project monitoring models for various projects and organizations to take advantage of the data from the software tool EPM and to allow the tailoring of the interpretation models based upon the context and success criteria for each project and organization. The basic idea is to build less concrete models that do not include explicit baseline values to interpret metrics values. Instead, we add hypothesis and interpretation layers to the models to help people of different projects make decisions in their own context. We applied the models to two industrial projects, and found that our less concrete models could successfully identify typical problems in software projects.", "num_citations": "19\n", "authors": ["140"]}
{"title": "A metric space for productivity measurement in software development\n", "abstract": " We define a metric space to measure the contributions of individual programmers to a software development project. It allows us to measure the distance between the contributions of two different programmers as well as the absolute contribution of each individual programmer. Our metric is based on an action function that provides a picture of how one programmer's approach differs from another at each instance of time during the project. We apply our metric to data we collected from students taking a course in parallel programming. We display the pictures for two students who showed approximately equal contributions but who followed very different paths through the course.", "num_citations": "19\n", "authors": ["140"]}
{"title": "Validating the tame resource data model\n", "abstract": " The authors present a conceptual model of software development resource data and validates the model by references to the published literatures on necessary resource data for development support environments. The conceptual model was developed using a top-down strategy. A resource data model is a prerequisite to the development of integrated project support environments which aim to assist in the processes of resource estimation, evaluation, and control. The model proposed is a four-dimensional view of resources which can be used for resource estimation, utilization, and review. The model is validated by reference to three publications on resource databases, and the implications of the model arising out of these comparisons is discussed.<>", "num_citations": "19\n", "authors": ["140"]}
{"title": "Implementing quantitative SQA: A practical model.\n", "abstract": " ecause software affects more and more aspects of our life, the costeffective development and maintenance of high-quality software is increasingly important. Software quality assurance (SQA) has become an indispensable dimension of software development, designed to guarantee that quality and productivity requirements are fulfilled. We put this special issue together to help people understand the increasing importance of SQA as an essential part of software projects, outline some new ideas and approaches to SQA, and report on some practical experiences.", "num_citations": "19\n", "authors": ["140"]}
{"title": "Towards reusable measurement patterns\n", "abstract": " Software measurement programs can help organizations make better decisions regarding their software projects. However, creating and establishing software measurement programs can be both costly and difficult. This paper addresses the problem by focusing on reusability of metrics for software measurement programs through the identification of measurement patterns. We illustrate our work with identifying measurement patterns by providing an extensive and detailed measurement example that is broken down into interdependent building blocks and activities.", "num_citations": "18\n", "authors": ["140"]}
{"title": "Four applications of a software data collection and analysis methodology\n", "abstract": " The evaluation of software technologies suffers because of the lack of quantitative assessment of their effect on software development and modification. A seven-step data collection and analysis methodology couples software technology evaluation with software measurement. Four in-depth applications of the methodology are presented. The four studies represent each of the general categories of analyses on the software product and development process: 1) blocked subject-project studies, 2) replicated project studies, 3) multi-project variation studies, and 4) single project studies. The four applications are in the areas of, respectively, 1) software testing strategies, 2) Cleanroom software development, 3) characteristic software metric sets, and 4) software error analysis.", "num_citations": "18\n", "authors": ["140"]}
{"title": "Can the Parr curve help with manpower distribution and resource estimation problems?\n", "abstract": " This paper analyzes the resource utilization curve devel oped by Parr. The curve is compared with several other curves, including the Rayleigh curve, a parabola, and a trapezoid, with respect to how well they fit manpower uti lization. The evaluation is performed for several projects developed in the Software Engineering Laboratory of the 6\u201312 man-year variety. The conclusion drawn is that the Parr curve can be made to fit the data better than the other curves. However, because of the noise in the data, it is difficult to confirm the shape of the manpower distri bution from the data alone and therefore difficult to vali date any particular model. Also, since the parameters used in the curve are not easily calculable or estimable from known data, the curve is not effective for resource estimation.", "num_citations": "18\n", "authors": ["140"]}
{"title": "Empirically driven SE research: state of the art and required maturity\n", "abstract": " Software engineering researchers are increasingly relying on the empirical approach to advance the state of the art. The level of empirical rigor and evidence required to guide software engineering research, however, can vary drastically depending on many factors. In this session we identify some of these factors through a discussion of the state of the art in performing empirical studies in software engineering, and we show how we can utilize the notion of empirical maturity to set and adjust the empirical expectations for software engineering research efforts. Regarding the state of the art in performing empirical studies, we will offer perspectives on two classes of study: those concerned with humans utilizing a technology, eg, a person applying a methodology, a technique, or a tool, where human skills and the ability to interact with the technology are some of the primes issues, and those concerned with the\u00a0\u2026", "num_citations": "17\n", "authors": ["140"]}
{"title": "Evaluating COTS component dependability in context\n", "abstract": " The software industry has increasingly expanded its adoption of COTS components for complex, mission-critical applications. Using COTS products can shorten development and deployment time because they let system developers focus on creating domain-specific services. Selecting the right COTS component, however, is no easy task. We present a practical process that developers can use to empirically evaluate component dependability in their context. Our approach uses the unified model of dependability, a requirements engineering approach specially devised to capture dependability in context. This model clearly specifies the measurable characteristics the component must have to be dependable in a specific context. The model then serves as a reference, providing guidance on effectively designing experiments to compare similar components and interpret collected data. The process can be applied to\u00a0\u2026", "num_citations": "17\n", "authors": ["140"]}
{"title": "An empirical study to compare two parallel programming models\n", "abstract": " While there are strong beliefs within the community about whether one particular parallel programming model is easier to use than another, there has been little research to analyze these claims empirically. Currently, the most popular paradigm is message-passing, as implemented by the MPI library [1]. However, MPI is considered to be difficult for developing programs, because it forces the programmer to work at a very low level of abstraction. One alternative parallel programming model is the PRAM model, which supports fine-grained parallelism and has a substantial history of algorithmic theory [2]. It is not possible to program current parallel machines using the PRAM model because modern architectures are not designed to support such a model efficiently. However, current trends towards multicore chips suggest that large-scale, fine-grained uniform-memory access parallel machines may soon be feasible\u00a0\u2026", "num_citations": "16\n", "authors": ["140"]}
{"title": "Opt: Organization and process together\n", "abstract": " The role of humans in the software development process must be studied in the context of their organizational configuration. The organizational structure within which the process executes has a profound effect on its outcome. The communication and interaction problems associated with human involvement in development cannot be addressed by process improvement alone. The solution must include organizational improvement as well. Because organizational factors are complex, their analysis is a non-trivial task. Methods are needed for improving both organizations and processes, as well as the relationships between them. The aim of the OPT approach is the improvement of software development through improvement of organizational structures and processes. The approach is an iterative improvement method. The steps include modeling the relationship between the organization and the process\u00a0\u2026", "num_citations": "16\n", "authors": ["140"]}
{"title": "SIMPL-X A Language for Writing Structured Programs\n", "abstract": " This rep or tc on tains a des cription of the pr o gramming", "num_citations": "16\n", "authors": ["140"]}
{"title": "A personal perspective on the evolution of empirical software engineering\n", "abstract": " This paper offers a four-decade overview of the evolution of empirical software engineering from a personal perspective. It represents what I saw as major milestones in terms of the kind of thinking that affected the nature of the work. I use examples from my own work as I feel that work followed the evolution of the field and is representative of the thinking at various points in time. I try to say where we fell short and where we need to go, in the end discussing the barriers we still need to address.", "num_citations": "15\n", "authors": ["140"]}
{"title": "A statistical neural network framework for risk management process-from the proposal to its preliminary validation for efficiency\n", "abstract": " This paper enhances the currently available formal risk management models and related frameworks by providing an independent mechanism for checking out their results. It provides a way to compare the historical data on the risks identified by similar projects to the risk found by each framework Based on direct queries to stakeholders, existing approaches provide a mechanism for estimating the probability of achieving software project objectives before the project starts (Prior probability). However, they do not estimate the probability that objectives have actually been achieved, when risk events have occurred during project development. This involves calculating the posterior probability that a project missed its objectives, or, on the contrary, the probability that the project has succeeded. This paper provides existing frameworks with a way to calculate both prior and posterior probability. The overall risk evaluation\u00a0\u2026", "num_citations": "15\n", "authors": ["140"]}
{"title": "Identifying implicit process variables to support future empirical work\n", "abstract": " The most basic questions that researchers must address when introducing a new process or technique are what is the intended effect of that process and can that effect be demonstrated empirically. As the understanding of a process progresses, researchers become interested in more sophisticated questions about a process or technique, such as studying the relationship between a particular type of variable and the outcome of the process. Quite often, researchers will find few, if any, studies in the literaturethat explicitly identify and analyze the effects of potential variables on the process. This paper proposes a methodology to aid in performing a literature search to be used as a basis for new research into these types of variables. The methodology provides guidance on making use of a large range of studies from which to extract potential variables. Throughout the paper, the methodology is illustrated with a specific example. The example focuses on searching for variables that deal with the individual variations among software inspectors that affect their performance during an inspection. At the end of the example, after following the steps of the methodology, a list of potential variables among software inspectors is identified. The paper concludes with the next steps to be taken concerning the identified variables and hypotheses.", "num_citations": "15\n", "authors": ["140"]}
{"title": "Ada reusability and measurement\n", "abstract": " The demand for software has exceeded the industry's capacity to supply it. Although advances in software development technology have increased the efficiency of developers, none have provided the dramatic improvements in quality and productivity which will be necessary to meet the current and future demands. Software reuse provides an answer to this dilemma. This paper describes two reuse studies performed at the University of Maryland Department of Computer Science. The first study defines a means of measuring data bindings to characterize and identify reusable (sets of) components of existing software. The second study defines ideally reusable components and a way of measuring the distance from that ideal for any given component. One important result of both studies has been the identification of a set of guidelines which can be used to assist developers to create more reusable software, to select\u00a0\u2026", "num_citations": "15\n", "authors": ["140"]}
{"title": "Finding relationships between effort and other variables in the SEL\n", "abstract": " Estimating the amount of effort required for a software development project is one of the major aspects of resource estimation for that project. In this study, we examined the relationship between effort and other variables for 23 Software Engineering Laboratory projects that were developed for NASA/Goddard Space Flight Center. These variables fell into two categories: those which can be determined in the early stages of project development and may therefore be useful in a baseline equation for predicting effort in future projects, and those which can be used mainly to characterize or evaluate effort requirements and thus enhance our understanding of the software development process in this environment. The results of our analyses are presented in this paper.", "num_citations": "15\n", "authors": ["140"]}
{"title": "Measures and risk indicators for early insight into software safety\n", "abstract": " The purpose of the system safety process is to identify and mitigate hazards associated with the operation and maintenance of a system under development. System safety is often implemented through an approach that identifies hazards and defines actions that will mitigate the hazard and verify that the mitigations have been implemented. The residual risk is the risk remaining when a hazard cannot be completely mitigated. The goal of the system safety process is to reduce this residual risk to an acceptable level, as defined by the safety certifier. Cost is a consideration in determining the level of", "num_citations": "14\n", "authors": ["140"]}
{"title": "Lessons learned in use of Ada-oriented design methods\n", "abstract": " Abstract \u2018As Ada is introduced into new environments, both managers and developers need to understand the ways in which the decision to use Ada as the target language will affect the software development lifecycle. The Flight Dynamics division at NASA Goddard Space Flight Center is involved in a study analyzing the effects of Ada on the development of their software. This project is one of the \ufb01rst to use Ada in this environment. In the study, two teams are each developing satellite simulators from the same speci\ufb01cations, one in Ads and one in FORTRAN, the standard language in this environment. This paper will address the lessons learned during the design phase including the e\ufb02ect of speci\ufb01cations on Ada-oriented design, the importance of the design method chosen, the importance of the documentation style for the chosen design method, and the e\ufb01ects of Ada-oriented design on the software develop\u00a0\u2026", "num_citations": "14\n", "authors": ["140"]}
{"title": "Towards a comprehensive framework for reuse: model-based reuse characterization schemes\n", "abstract": " Reuse of products, processes and related knowledge will be the key to enable the software industry to achieve the dramatic improvement in productivity and quality required to satisfy the anticipated growing demands. We need a comprehensive framework of models and model-based characterization schemes for better understanding, evaluating, and planning all aspects of reuse. In this paper we define requirements for comprehensive reuse models and related characterization schemes, assess state-of-the-art reuse characterization schemes relative to these requirements and motivate the need for more comprehensive reuse characterization schemes. We introduce a characterization scheme based upon a general reuse model, apply it and discuss its benefits, and suggest a model for integrating reuse into software development.Descriptors:", "num_citations": "13\n", "authors": ["140"]}
{"title": "Recent advances in software measurement\n", "abstract": " Summary form only given. There has been continued progress in the area of software measurement. The main inroads have been made in the scope of measurement, a deeper understanding of the perspectives from which measurement can be applied, the development of frameworks for the definition and interpretation of measurement, the refinement of the measures and the models on which the metrics are based, the automation of the models and measures, and the increased application of measurement in many organizations.< >", "num_citations": "13\n", "authors": ["140"]}
{"title": "Analyzing error-prone system coupling and cohesion\n", "abstract": " Several researchers have proposed methods for relating the structure of a software system to its quality (eg,[BE82][HK81][Eme84). One pivotal step in assess-ing the structure of a software system is characterizing its coupling and cohesion. Intuitively, the cohesion in a software system is the amount of interaction within pieces (eg, subsystems, modules) of a system. Correspondingly, coupling in a software system is the amount of interaction across pieces of a system. Cohesion may sometimes be referred to as \u201cstrength.\u201d Various interpretations for coupling and cohesion have been proposed [SMC74]. In this paper, we present an empirical study that evaluates the effectiveness of cohesion and coupling in identifying errorprone system structure. Our measurement of cohesion and coupling is based on intra-system interaction in terms of software data bindings [BT75][HB85]. Our measurement of error-proneness is\u00a0\u2026", "num_citations": "13\n", "authors": ["140"]}
{"title": "TAME: Tailoring an Ada measurement environment\n", "abstract": " TAME | Proceedings of the Joint Ada conference fifth national conference on Ada technology and fourth Washington Ada Symposium ACM Digital Library home ACM home Google, Inc. (search) Advanced Search Browse About Sign in Register Advanced Search Journals Magazines Proceedings Books SIGs Conferences People More Search ACM Digital Library SearchSearch Advanced Search ada Conference Proceedings Upcoming Events Authors Affiliations Award Winners More HomeConferencesADAProceedingsWADAS '87TAME: tailoring an Ada Measurement Environment ARTICLE TAME: tailoring an Ada Measurement Environment Share on Authors: Victor Robert Basili profile image Victor R. Basili View Profile , Hans Dieter Rombach profile image H. Dieter Rombach View Profile Authors Info & Affiliations Publication: WADAS '87: Proceedings of the Joint Ada conference fifth national conference on Ada 318\u2026", "num_citations": "13\n", "authors": ["140"]}
{"title": "TAME: Integrating measurement into software environments\n", "abstract": " Based upon a dozen years of analyzing software engineering processes and products, we propose a set of software engineering process and measurement principles. These principles lead to the view that an Integrated Software Engineering Environment (lSEE) should support multiple process models across the full software life cycle, the technical and management aspects of soft, ware engineering, and the planning, construction, and feedback and learning activities. These activities need to be tailored to the speci\ufb01c project under development and they must be tractable for management control. The tailorability and tractability attributes require the support of a measurement process. The measurement process needs to be top\u2014down, based upon operation-ally defined goals.The TAME project uses the goal/question/metric paradigm to support this type of meas-urement paradigm. It provides for the establishment of\u00a0\u2026", "num_citations": "13\n", "authors": ["140"]}
{"title": "SIMPL-T: A Structured Programming Language.\n", "abstract": " SIMPL-T is a member of a family of languages that are designed to be relatively machine independent and whose compilers are relatively tranportable onto a variety of machines. It is a procedure oriented, nonblock structured programming language that was designed to conform to the standards of structured programming and modular design. There are three data types in SIMPL-T integer, string and character. This manual describes the implementation of the language SIMPL-T for the Univac 11061108 computers using the Exec 8 operating system.Descriptors:", "num_citations": "13\n", "authors": ["140"]}
{"title": "A statistical neural network framework for risk management process\n", "abstract": " This paper enhances the currently available formal risk management models and related frameworks by providing an independent mechanism for checking out their results. It provides a way to compare the historical data on the risks identified by similar projects to the risk found by each framework Based on direct queries to stakeholders, existing approaches provide a mechanism for estimating the probability of achieving software project objectives before the project starts (Prior probability). However, they do not estimate the probability that objectives have actually been achieved, when risk events have occurred during project development. This involves calculating the posterior probability that a project missed its objectives, or, on the contrary, the probability that the project has succeeded. This paper provides existing frameworks with a way to calculate both prior and posterior probability. The overall risk evaluation, calculated by those two probabilities, could be compared to the evaluations that each framework has found within its own process. Therefore, the comparison is performed between what those frameworks assumed and what the historical data suggested both before and during the project. This is a control mechanism because, if those comparisons do not agree, further investigations could be carried out. A case study is presented that provides an efficient way to deal with those issues by using Artificial Neural Networks (ANN) as a statistical tool (eg, regression and probability estimator). That is, we show that ANN can automatically derive from historical data both prior and posterior probability estimates. This paper shows the verification\u00a0\u2026", "num_citations": "12\n", "authors": ["140"]}
{"title": "Software defect reduction top 10 list\n", "abstract": " Recently, a National Science Foundation grant enabled us to establish the Center for Empirically Based Software Engineering (CeBASE), which seeks to transform software engineering as much as possible from a fad-based practice to an engineering-based practice through derivation, organization, and dissemination of empirical data on software development and evolution phenomenology. The phrase \u201cas much as possible\u201d reflects the fact that software development must remain a people-intensive and continually changing field. We have found, however, that researchers have established objective and quantitative data, relationships, and predictive models that help software developers avoid predictable pitfalls and improve their ability to predict and control efficient software projects.Here we describe developments in this area that have taken place since the publication of \u201cIndustrial Metrics Top 10 List\u201d in 1987 (B. Boehm, IEEE Software, Sept. 1987, pp. 84\u201385). Given that CeBASE places a high priority on software defect reduction, we think it is fitting to update that earlier article by providing the following Software Defect Reduction Top 10 List.", "num_citations": "12\n", "authors": ["140"]}
{"title": "A method for documenting code components\n", "abstract": " We propose a set of criteria for facilitating the rigorous understanding of code components via documentation and evaluate existing notations and approaches with respect to these criteria. We present an overview of an analysis approach designed to generate program documentation that satisfies these criteria. Because of the inherent difficulty and importance of reasoning about loops, we focus on understanding and documenting loops. We decompose loops into their component parts and obtain formal specifications of the resulting loop fragments by use of a knowledge base. We build this knowledge base for a specific application domain by designing plans that allow us to recognize stereotyped code patterns and associate them with their formal specifications. Finally, we synthesize a consistent and accurate specification of the whole loop construct from the specifications of its fragments. To evaluate our loop\u00a0\u2026", "num_citations": "12\n", "authors": ["140"]}
{"title": "Simulation Modeling of Software Development Processes\n", "abstract": " INTRODUCTION Reducing the cost of large scale software projects and shortening cycle time, or time to market, is a major goal of most software development organizations. To pursue such a goal, organizations can set productivity goals for each project, and put in place statistical productivity controls to enable developers and management to take corrective actions when there are deviations from the goal, and to distinguish a random deviation from meaningful deviations. Simulation is one of the methods for performing such control. It can be used at various points in the software life cycle to perform risk analysis, in terms of time to product, and cost, to verify conformance to expectations, and to perform continuous process improvement and optimization. This requires that organizations use metrics and models to evaluate and predict effort and", "num_citations": "12\n", "authors": ["140"]}
{"title": "Facts and myths affecting software reuse\n", "abstract": " Discusses the three most important facts or myths affecting reuse. There is a great deal of misunderstanding about reuse in the software domain and it is difficult to pick out only three: there has been to much emphasis on the reuse of code; software reuse implies some form of modification of the artifact being reused; and software development processes do not explicitly support reuse, in fact they implicitly inhibit reuse.< >", "num_citations": "12\n", "authors": ["140"]}
{"title": "An evaluation of Ada source code reuse\n", "abstract": " This paper presents the results of a metric-based investigation into the nature and benefits of reuse in an Ada development environment. Four medium scale Ada projects developed in one organization over a three year period were analyzed. The study indicates benefits of reuse in terms of reduced error density and increased productivity. The Ada generic features are observed as an enabler of reuse at higher levels of abstraction. Finally, using several metrics, we identify trends indicating an improving reuse process.", "num_citations": "12\n", "authors": ["140"]}
{"title": "A language design for vector machines\n", "abstract": " This paper deals with a programming language under development at NASA's Langley Research Center for the CDC STAR-100. The design goals for the language are that it be basic in design and able to be extended as deemed necessary to serve the user community, capable of the expression of efficient algorithms by forcing the user to make the maximum use of the specialized hardware design, and easy to implement so that the language and compiler could be developed with a minimum of effort. The key to the language was in choosing the basic data types and data structures. Scalars, vectors, and strings are available data types in the language. Each basic data type has an associated set of operators which consist primarily of the operations provided by the hardware. The only data structure in the language is a restricted form of the array. Only vector and string data types may be stored in arrays, forcing the\u00a0\u2026", "num_citations": "12\n", "authors": ["140"]}
{"title": "Realizing the benefits of the CMMISM with the CeBASE method\n", "abstract": " Future systems will be increasingly software\u2010intensive, but the type of software development they will need is not well covered by current development and maturity models such as the waterfall model and Software Capability Maturity Model\u00ae (CMM\u00ae). Future development of software\u2010intensive systems will need situation\u2010specific balancing of discipline and flexibility to address such issues as COTS, open source, distribution, mobility rapid change, agents, collaboration support, and simultaneous achievement of rapid development and high dependability. This article shows how the CMMISM's integration of modern systems engineering, software engineering, and integrated process and product development concepts provides a framework for redressing the shortfalls of the Software CMM\u00ae, and for enabling projects and organizations to achieve the right balance of discipline and flexibility for their particular situations\u00a0\u2026", "num_citations": "11\n", "authors": ["140"]}
{"title": "A Validation of Object-Oriented Design Metrics\n", "abstract": " This paper presents the results of a study conducted at the University o-fMaryland in which we experimentally investigated the suite of Object-Oriented (00) design metrics introduced by [Chidamber&Kemerer, 1994]. In order to do this, we assessed these metrics as predictors of fault-prone classes. This study is complementary to [Lie&Henry, 1993] where the same suite of metrics had been used to assess frequencies of maintenance changes to classes. To perform our validation accurately, we collected data on the development of eight medium-sized information management systems based on identical requirements. All eight projects were developed using a sequential life cycle model, a well-known O0 analysis/design method and the C++ programming language. Based on experimental results, the advantages and drawbacks of these 00 metrics are discussed and suggestions for improvement are provided\u00a0\u2026", "num_citations": "11\n", "authors": ["140"]}
{"title": "Packaging reusable components: the specification of programs\n", "abstract": " Packaging reusable components | Guide books ACM Digital Library home ACM home Google, Inc. (search) Advanced Search Browse About Sign in Register Advanced Search Journals Magazines Proceedings Books SIGs Conferences People More Search ACM Digital Library SearchSearch Advanced Search Browse Browse Digital Library Collections More HomeBrowse by TitleReportsPackaging reusable components: the specification of programs ABSTRACT No abstract available. Index Terms 1.Packaging reusable components 1.Computing methodologies 1.Artificial intelligence 1.Knowledge representation and reasoning 2.Machine learning 1.Machine learning approaches 1.Rule learning 2.Software and its engineering 1.Software creation and management 1.Designing software 1.Requirements analysis 2.Software development techniques 1.Reusability 3.Theory of computation 1.Semantics and reasoning 1.1.-\u2026", "num_citations": "11\n", "authors": ["140"]}
{"title": "Ada reusability analysis and measurement\n", "abstract": " The demand for software has exceeded the industry\u2019s capacity to supply it. Projects are frequently scaled down, delayed or even cancelled because of the time and effort required to develop the software for them. Further, the demand for software will continue to increase in the foreseeable future. Software reuse provides an answer to this dilemma. Although process and tool reuse is common practice, lifecycle product reuse is still in its infancy. Ultimately, reuse of early lifecycle products might provide the largest payoff, however for the near term, gains can be realized and further work can be guided by understanding how software can be developed with a minimum of newly-generated source lines of code.             This paper describes several parallel studies being conducted at the University of Maryland Department of Computer Science which address various related software reuse topics.             One\u00a0\u2026", "num_citations": "11\n", "authors": ["140"]}
{"title": "Generalizing specifications for uniformly implemented loops\n", "abstract": " The problem of generalizing functional specifications for while loops is considered. This problem occurs frequently when trying to verify that an initialized loop satisfies some functional specification, i.e., produces outputs which are some function of the program inputs. The notion of a valid generalization of a loop specification is defined. A particularly simple valid generalization, a base generalization, is discussed. A property of many commonly occurring while loops, that of being uniformly implemented, is defined. A technique is presented which exploits this property in order to systematically achieve a valid generalization of the loop specification. Two classes of uniformly implemented loops that are particularly susceptible to this form of analysis are defined and discussed. The use of the proposed technique is illustrated with a number of applications. Finally, an implication of the concept of uniform loop\u00a0\u2026", "num_citations": "11\n", "authors": ["140"]}
{"title": "A comparison of the axiomatic and functional models of structured programming\n", "abstract": " This paper discusses axiomatic and functional models of the semantics of structured programming. The models are presented together with their respective methodologies for proving program correctness and for deriving correct programs. Examples using these methodologies are given. Finally, the models are compared and contrasted.", "num_citations": "11\n", "authors": ["140"]}
{"title": "Experiences from an exploratory case study with a software risk management method\n", "abstract": " This paper presents the results of an exploratory case study in which a risk management method was used and compared with a method currently used by the organization. The goal of the case study was to obtain feedback on an early version of a risk management method, called Riskit, that has been developed at the University of Maryland. This paper also presents an overview of Riskit method version 0.10 and describes the comparison method currently used by the case study organization, as well as the case study design.", "num_citations": "10\n", "authors": ["140"]}
{"title": "Reengineering existing software for reusability\n", "abstract": " A cost effective introduction of software reuse techniques requires the reuse of existing software, developed in many cases without ainning to reusability. The paper discusses the problems related to the analysis and reengineering of existing software in order to reuse it. A process model is presented that analyzes the existing programs in two steps: the first one uses measurement to identify the candidate reusable components, the second one rcognizes the reusable software components and packages them into reusable units. An important characteristic of this process model is that the first step can be fully automated in order to deal with a smaller amount of code in the second phase. The current research, ongoin at the Department of Computer Science of the University of Maryland, on this process model and on the CARE (Computer Aided Reuse Engineering) System for support of this process is presented and\u00a0\u2026", "num_citations": "10\n", "authors": ["140"]}
{"title": "The SIMPL family of programming languages and compilers\n", "abstract": " The SIMPL family is a set of structured programming languages and compilers under development at the University of Maryland. The term\" family\" implies that the languages contain some basic common features, such as a common subset of data types and control structures. Each of the languages in the SIMPL family is built as an extension to a base language. The compiler for the base language is written in the base language itself, and the compiler for each new language is an extension to the base compiler.", "num_citations": "10\n", "authors": ["140"]}
{"title": "OPT: An approach to organizational and process improvement\n", "abstract": " Software development and maintenance enterprises constitute an extremely complex, varied, and poorly understood class of organizations. This is due in part to the newness of the technology and the dynamic nature of the field, but it also stems from the complexity of human-machine interactions. A major driver of the effectiveness of such an organization is the relationship between the software development process and the organizational structure. Little attention has been paid to this relationship, Scacchi\u2019s work [6] being one exception. Our approach addresses this issue in more detail. This paper describes the OPT method for improving both the organizational structures and processes that constitute software development environments. This method is meant to be part of a continuous improvement program, and is modeled after the Quality Improvement Paradigm [1]. The approach includes mechanisms for modeling the relationship between an organizational structure and a development process, for measuring this relationship quantitatively, and for using this information to plan specific improvements to the environment.", "num_citations": "9\n", "authors": ["140"]}
{"title": "The software-cycle models for re-engineering and reuse\n", "abstract": " This paper reports on the progress of a study which will contribute to our ability to perform high-level, component-based programming by describing means to obtain useful components, methods for the contlguration and integration of those components, and an underlying economic model of the costs and benefits associated with this approach to reuse. One goal of the study is to develop and demonstrate methods to recover reusable components from domain-specific software through a mmbination of tools, to perform the identification, extraction, and re-engineering of components, and domain experts, to direct the application of those tools. A second goal of the study is to enable the reuse of those components by identifying techniques for conf@ ring and recombining the re-engineered software. This component-nxovery or software-cycle model addressesnot only the selection and re-engineering of components, but\u00a0\u2026", "num_citations": "9\n", "authors": ["140"]}
{"title": "Software reclamation: Improving post-development reusability\n", "abstract": " This paper describes part oi a multl-year study of software reuse being perlorn'ted. at the University oi Maryland. The part or the study which is reported here explores techniques for the transiorntation oi Ada programs which preserve function but which result In program components that are more independent. and presumably thereiore. more reusable. Goals for the larger study include a precise specification oi the transtormatlon technique and its application in a large development organization. Expected results oi the larger study. which are partially covered here. are the identification oi reuse promoters and inhbitors both in the problem space and in the solution space. the development oi a set of metrit: which out be applied to both developing and completed soitware to reveal the degree oi reusability which can be expected oi that software. and the development oi guidelines lor both developers and reviewers oi\u00a0\u2026", "num_citations": "9\n", "authors": ["140"]}
{"title": "The Cleanroom case study in the software engineering laboratory: project description and early analysis\n", "abstract": " This case study analyzes the application of the cleanroom software development methodology to the development of production software at the NASA/Goddard Space Flight Center. The cleanroom methodology emphasizes human discipline in program verification to produce reliable software products that are right the first time. Preliminary analysis of the cleanroom case study shows that the method can be applied successfully in the FDD environment and may increase staff productivity and product quality. Compared to typical Software Engineering Laboratory (SEL) activities, there is evidence of lower failure rates, a more complete and consistent set of inline code documentation, a different distribution of phase effort activity, and a different growth profile in terms of lines of code developed. The major goals of the study were to: (1) assess the process used in the SEL cleanroom model with respect to team structure, team activities, and effort distribution; (2) analyze the products of the SEL cleanroom model and determine the impact on measures of interest, including reliability, productivity, overall life-cycle cost, and software quality; and (3) analyze the residual products in the application of the SEL cleanroom model, such as fault distribution, error characteristics, system growth, and computer usage.", "num_citations": "9\n", "authors": ["140"]}
{"title": "Use of Ada for FAA's advanced automation system (AAS)\n", "abstract": " Use of Ada for FAA's advanced automation system (AAS) | Software Risk Management ACM Digital Library home ACM home Google, Inc. (search) Advanced Search Browse About Sign in Register Advanced Search Journals Magazines Proceedings Books SIGs Conferences People More Search ACM Digital Library SearchSearch Advanced Search Browse Browse Digital Library Collections More HomeBrowse by TitleBooksSoftware Risk ManagementUse of Ada for FAA's advanced automation system (AAS) chapter Use of Ada for FAA's advanced automation system (AAS) Share on Authors: Victor Robert Basili profile image VR Basili View Profile , Barry William Boehm profile image BW Boehm View Profile , Judith A Clapp profile image JA Clapp View Profile , Dale J Gaumer profile image D. Gaumer View Profile , M Holden profile image M. Holden View Profile , JK Summers profile image JK Summers View Profile & \u2026", "num_citations": "9\n", "authors": ["140"]}
{"title": "Measuring the software process and product: lessons learned in the SEL\n", "abstract": " The software development process and product can and should be measured. The software measurement process at the Software Engineering Laboratory (SEL) has taught a major lesson: develop a goal-driven paradigm (also characterized as a goal/question/metric paradigm) for data collection. Project analysis under this paradigm leads to a design for evaluating and improving the methodology of software development and maintenance.", "num_citations": "9\n", "authors": ["140"]}
{"title": "Evaluating software testing strategies\n", "abstract": " This study compares the strategies of code reading, functional testing, and structural testing in three aspects of software testing: fault detection effectiveness, fault detection cost, and classes of faults detected. Thirty two professional programmers applied the three techniques to three unlt-sized programs in a fractional factorial experi-mental design. The major results of this study so far are the following. 1) Code readers detected more faults than did those using the other techniques, while functional testers detected more faults than did structural testers. 2) Code readers had a higher fault detection rate than did those using the other methods, while there was no difference between functional testers and structural testers. 3) Subjects testing the abstract data type detected the most faults and had the highest fault detection rate, while individuals testing the database maintainer found the fewest faults and spent the most effort test-Ing. 4) Subjects of Intermediate and Junior expertise were not different in number or percentage of faults found, fault detection rate, or fault detection effort; subjects of advanced expertise found a greater number of faults than did the others, found a greater percentage of faults than did just those of Junior expertise, and were not different from the others in either fault detection rate or effort. 5) Code readers and functional testers both detected more omission faults and more control faults than did structural testers, while code readers detected more interface faults than did those using the other methods.", "num_citations": "9\n", "authors": ["140"]}
{"title": "Measuring software development characteristics in the local environment\n", "abstract": " This paper discusses the characterization and analysis facilities being performed by the Software Engineering Laboratory which can be done with minimal effort on many projects. Some examples are given of the kinds of analyses that can be done to aid in managing, understanding and characterizing the development of software in a production environment.", "num_citations": "9\n", "authors": ["140"]}
{"title": "A knowledge-based approach to program understanding\n", "abstract": " Program understanding plays an important role in nearly all software related tasks. It is vital to the development, maintenance and reuse activities. Program understanding is indispensable for improving the quality of software development. Several development activities such as code reviews, debugging and some testing approaches require programmers to read and understand programs. Maintenance activities cannot be performed without a deep and correct understanding of the component to be maintained. Program understanding is vital to the reuse of code components because they cannot be utilized without a clear understanding of what they do. If a candidate reusable component needs to be modified, an understanding how it is designed is also required. of This monograph presents a\u00b7 knowledge-based approach to the automation of program understanding. This approach generates rigorous program documentation mechanically by combining and building on strengths of a practical program decomposition method, the axiomatic correctness notation, and the knowledge based analysis approaches. More specifically, this approach documents programs by generating first order predicate logic annotations of their loops. In this approach, loops are classified according to their complexity levels. Based on this taxonomy, variations on the basic analysis approach that best fit each of the different classes are described. In general, mechanical annotation of loops is performed by first decomposing them using data flow analysis. This decomposition encapsulates interdependent statements in events, which can be analyzed individually.", "num_citations": "8\n", "authors": ["140"]}
{"title": "Implementing an Internet-Enabled Software Experience Factory: Work in Progress\n", "abstract": " Many companies already have informal or semi-formal methods of recording and sharing experiences about software development within their organizations. They may hold meetings and conferences on a regular basis to discuss and share experiences informally. They may record project data in project binders or in computerized databases. They may even write \u201clessons learned\u201d documents to record subjective experiences with projects. However, typically much of the information about experiences is not systematically stored or packaged in a form for effective retrieval and reuse, and consequently is never used. Moreover, the realisation of personnel that much of the data will never be accessed creates a negative feedback loop where the valuable experiences are not recorded in the first place.The experience factory approach aims to establish an organizational infrastructure to facilitate systematic and continuous organizational learning through the sharing and reuse of experiences in software engineering [Basili et al. 1994]. It involves setting up a group separate to the development teams, called the experience factory, which is responsible for collecting experiences from development projects, packaging the experiences by building empirical models and structuring informal knowledge, and validating and spreading experience packages into development projects. It aims to install processes and techniques into organizations to encourage data collection, facilitate packaging of experiences, and promote the dissemination (push) and retrieval (pull) of experiences.", "num_citations": "8\n", "authors": ["140"]}
{"title": "The maturing of the Quality Improvement Paradigm in the SEL\n", "abstract": " The Software Engineering Laboratory uses a paradigm for improving the software process and product, called the quality improvement paradigm. This paradigm has evolved over the past 18 years, along with our software development processes and product. Since 1976, when we first began the SEL, we have learned a great deal about improving the software process and product, making a great many mistakes along the way. Quality improvement paradigm, as it is currently defined, can be broken up into six steps: characterize the current project and its environment with respect to the appropriate models and metrics; set the quantifiable goals for successful project performance and improvement; choose the appropriate process model and supporting methods and tools for this project; execute the processes, construct the products, and collect, validate, and analyze the data to provide real-time feedback for corrective action; analyze the data to evaluate the current practices, determine problems, record findings, and make recommendations for future project improvements; and package the experience gained in the form of updated and refined models and other forms of structured knowledge gained from this and prior projects and save it in an experience base to be reused on future projects.", "num_citations": "8\n", "authors": ["140"]}
{"title": "A tool for assisting the understanding and formal development of software\n", "abstract": " This paper presents a program understanding tool which documents programs by generating predicate logic annotations of their loops. The tool is based on an analysis by decomposition approach which utilizes a knowledge-base of plans in recognizing the high-level concepts in programs. Using data flow analysis, the decomposition encapsulates closely related statements in separate parts, called events, which can be analyzed individually. The first order predicate logic annotations of loops are synthesized from these individual analysis results. A summary of the results of a case study, performed on a pre-existing program of reasonable size, is given. The loops in this study, which are used as input data to the tool, serve to validate our analysis approach. Finally, different applications of the tool, including the application of assisting the formal development of software, are discussed. This discussion focuses on how\u00a0\u2026", "num_citations": "8\n", "authors": ["140"]}
{"title": "Methodological and architectural issues in the experience factory\n", "abstract": " The concept of the experience factory has been introduced in order to institutionalize the collective learning of the organization that is at the root of continuous improvement and competitive advantage. The real advantage will come from the ability of the software organization to deliver solutions that anticipate the needs of the system users. The experience factory can be a logical and/or physical organization, but it is important that activities are separated and made independent from the ones of the project organization. (H.A.)", "num_citations": "8\n", "authors": ["140"]}
{"title": "Mathematical principles for a first course in software engineering\n", "abstract": " An introductory computer science course is developed, much as calculus is a basic course for mathematics and the physical sciences, concerned primarily with theoretical foundations and methodology rather than apprenticeship through applications. In this work, the principles taught in the course are described and an example illustrating them is given.< >", "num_citations": "8\n", "authors": ["140"]}
{"title": "The software engineering laboratory: Objectives\n", "abstract": " A great deal of time and money has been and will continue to be spent in developing software. Much effort has gone into the generation of various software development methodologies that are meant to improve both the process and the product ([MYER, 75],[BAKE, 74],[WOLV, 72]). Unfortunately, it has not always been clear what the underlying principles involved in the software development process are and what effect the methodologies have; it is not always clear what constitutes a better product. Thus progress in finding techniques that produce better, cheaper software depends on developing new deeper understandings of good software and the software development process through studying the underlying principles involved in software and the development process. At the same time we must continue to produce software.", "num_citations": "8\n", "authors": ["140"]}
{"title": "Using visualization to understand dependability: a tool support for requirements analysis\n", "abstract": " Dealing with dependability requirements is a complex task for stakeholders and analysts as many different aspects of a system must be taken into account at the same time: services characteristics and quality properties, failure modes and tolerable failure rates, reactions and recovery time in case of failure, and so on. Visualization helps cope with this complexity. In this paper, we build upon a practical framework for eliciting and modeling dependability requirements to show how graphical data representation can facilitate requirements analysis during the requirements elicitation and definition process. An air traffic control system, adopted as a testbed within the NASA high dependability computing project, is used as a case study", "num_citations": "7\n", "authors": ["140"]}
{"title": "Packaging and disseminating lessons learned from COTS-Based software development\n", "abstract": " The appropriate management of experience and knowledge has become a crucially important capability for organizations of all types and software organizations are no exception. We describe an initiative aimed at helping the software engineering community share experience, in the form of lessons learned. The Center for Empirically Based Software Engineering (CeBASE) COTS lessons learned repository (CLLR) is described, including its motivation, its current status and capabilities, and the plans for its evolution. The contribution of this work lies not only in the approach itself and its validation, but also in the creation of a community of interest, which is fundamental in order to ensure the success of such an initiative. The knowledge and experience that are captured, carefully processed, and made available to the software engineering community also form part of this contribution. The community is supported by\u00a0\u2026", "num_citations": "7\n", "authors": ["140"]}
{"title": "Issues in COTS-based software development\n", "abstract": " This paper summarizes the results of a study on fifteen projects that used a COTS-based approach. The process they followed is evaluated to identify essential differences in comparison to traditional software development. The main differences, and the activities for which projects require more guidance, are requirements definition and COTS selection, high level design, integration and testing.", "num_citations": "7\n", "authors": ["140"]}
{"title": "Building an experience factory for maintenance\n", "abstract": " This paper reports the preliminary results of a study of the software maintenance process in the Flight Dynamics Division (FDD) of the National Aeronautics and Space Administration/Goddard Space Flight Center (NASA/GSFC). This study is being conducted by the Software Engineering Laboratory (SEL), a research organization sponsored by the Software Engineering Branch of the FDD, which investigates the effectiveness of software engineering technologies when applied to the development of applications software.This software maintenance study began in October 1993 and is being conducted using the Quality Improvement Paradigm (QIP), a process improvement strategy based on three iterative steps: understanding, assessing, and packaging. The preliminary results presented in this paper represent the outcome of the understanding phase, during which SEL researchers characterized the maintenance\u00a0\u2026", "num_citations": "7\n", "authors": ["140"]}
{"title": "Data binding tool: A tool for measurement based ada source reusability and design assessment\n", "abstract": " This paper reports on the data binding tool (dbt). It assists in Ada source code reusability and system design assessment. Software system components are defined in the context of the language using a flexible scheme. Data binding metrics are utilized to measure the intercomponent interactions and cluster analysis is used to present structural configurations of software systems. The clustering technique and the tool design problems are discussed. Reusability potential and design assessment of examined systems are examined.", "num_citations": "7\n", "authors": ["140"]}
{"title": "Monitoring an Ada software development project\n", "abstract": " As any science matures, the role of measurement, analysis and experimentation grows. The software engineering community has seen the continue d development of new software development method s and tools and their use in various environments. The evaluation of methods and tools began with subjective criteria and has been developing towar d more objective data collection, measurement and controlled experiments. While this trend is encouraging, these evaluation studies have largel y been on a\" one-shot\" basis. What has been missin g is a systematic approach which defines a long-range program for the study, analysis and evaluation of a specific method or tool.The emergence of Ada provides a focal point for developing such a systematic study. As a first step, research teams from the University of Maryland and General Electric have embarked upon an eighteen-month collaborative effort. The purpose\u00a0\u2026", "num_citations": "7\n", "authors": ["140"]}
{"title": "Analyzing a syntactic family of complexity metrics\n", "abstract": " A family of syntactic complexity metrics which contains a number of current metrics is defined. The family is used as a basis for experimental analysis of metrics. Once the family has been implemented, several metrics may be readily formed and computed. This paper uses the family to compare a few simple syntactic metrics to each other. The study also indicates that individual differences have a large impact on the significance of results where many individuals are used. A metric for determining the relative skills of programmers at handling a given level of complexity is also suggested. The study uses the metrics to demonstrate differences between projects on which a methodology was used vs. those on which it was not.Descriptors:", "num_citations": "7\n", "authors": ["140"]}
{"title": "Auto-Associative Neural Networks to improve the accuracy of estimation models\n", "abstract": " Prediction of software engineering variables with high accuracy is still an open problem. The primary reason for the lack of high accuracy in prediction might be because most models are linear in the parameters and so are not sufficiently flexible and suffer from redundancy. In this chapter, we focus on improving regression models by decreasing their redundancy and increasing their parsimony, ie, we turn the model into a model with fewer variables than the former. We present an empirical auto-associative neural network-based strategy for model improvement, which implements a reduction technique called Curvilinear component analysis. The contribution of this chapter is to show how multi-layer feedforward neural networks can be a useful and practical mechanism for improving software engineering estimation models.", "num_citations": "6\n", "authors": ["140"]}
{"title": "Adopting curvilinear component analysis to improve software cost estimation accuracy model, application strategy, and an experimental verification\n", "abstract": " Cost estimation is a critical issue for software organizations. Good estimates can help us make more informed decisions (controlling and planning software risks), if they are reliable (correct) and valid (stable). In this study, we apply a variable reduction technique (based on auto-associative feed--forward neural networks \u2013 called Curvilinear component analysis) to log-linear regression functions calibrated with ordinary least squares. Based on a COCOMO 81 data set, we show that Curvilinear component analysis can improve the estimation model accuracy by turning the initial input variables into an equivalent and more compact representation. We show that, the models obtained by applying Curvilinear component analysis are more parsimonious, correct, and reliable.", "num_citations": "6\n", "authors": ["140"]}
{"title": "Get Your Experience Factory Ready for the Next Decade--Ten Years after\" How to Build and Run One\"--\n", "abstract": " This one-day tutorial aims at industry practitioners, managers and developers alike, who want to learn more about how to successfully design, implement and run an Experience Factory, to systematically build up and manage the experience of an organization. State-of- the art methods and techniques on how to initially set-up or to further develop and improve an organization's Experience Factory are discussed. Participants should come from organizations (not only from the software domain) that are interested in implementing an Experience Factory to help effectively support improvement activities (such as TQM, ISO 9000, CMMI, SPICE, or TSP) to gain competitive advantages.", "num_citations": "6\n", "authors": ["140"]}
{"title": "A relative development time productivity metric for HPC systems\n", "abstract": " Software is often the dominant cost associated with developing DoD High Performance Embedded Computing (HPEC) systems. Historically there has been no quantifiable methodology for comparing the difficulty of developing code on different HPEC systems and trading off ease of development vs. execution performance. The DARPA High Productivity Computing Systems (HPCS) program is developing methodologies for the High Performance Computing (HPC) community, which may also be applicable to the HPEC community. This paper presents early results of one approach for measuring the relative development time productivity of different parallel programming environments. This metric, defined as the ratio of relative execution performance to relative programmer effort, has been used to analyze several HPC benchmark codes and classroom programming assignments. The results of this analysis show consistent trends for various programming models. This approach enables a high-level evaluation of relative development time productivity for a given programming model, which is essential to the task of estimating software development cost for HPC and HPEC systems.", "num_citations": "6\n", "authors": ["140"]}
{"title": "The study of software maintenance organizations and processes\n", "abstract": " In order to improve the quality of software products, it is necessary to enhance the quality of the software processes used to develop them. This recognition has led to a proliferation of work, both empirical and non-empirical, on software development processes, some of which has dealt specifically with maintenance. One important but often overlooked component of both development and maintenance processes is the organizational context in which they are enacted. Our position is that this is an even more pertinent issue in maintenance than in development because maintainers are not generally the original developers of a system. The software has a history in which a number of people have been involved, and the experience of those people becomes relevant during maintenance. Thus an efficient flow of information is crucial, and this flow can be hindered or facilitated by the organizational relationships between\u00a0\u2026", "num_citations": "6\n", "authors": ["140"]}
{"title": "Evolving the Reuse Process at the Flight Dynamics Division (FDD) Goddard Space Flight Center\n", "abstract": " This paper presents the interim results from the Software Engineering Laboratory's (SEL) Reuse Study. The team conducting this study has, over the past few months, been studying the Generalized Support Software (GSS) domain asset library and architecture, and the various processes associated with it. In particular, we have characterized the process used to configure GSS-based attitude ground support systems (AGSS) to support satellite missions at NASA's Goddard Space Flight Center. To do this, we built detailed models of the tasks involved, the people who perform these tasks, and the interdependencies and information flows among these people. These models were based on information gleaned from numerous interviews with people involved in this process at various levels. We also analyzed effort data in order to determine the cost savings in moving from actual development of AGSSs to support each mission (which was necessary before GSS was available) to configuring AGSS software from the domain asset library.While characterizing the GSS process, we became aware of several interesting factors which affect the successful continued use of GSS. Many of these issues fall under the subject of evolving technologies, which were not available at the inception of GSS, but are now. Some of these technologies could be incorporated into the GSS process, thus making the whole asset library more usable. Other technologies are being considered as an alternative to the GSS process altogether. In this paper, we outline some of issues we will be considering in our continued study of GSS and the impact of evolving technologies.", "num_citations": "6\n", "authors": ["140"]}
{"title": "The experience factory strategy and practice\n", "abstract": " The quality movement, that has had in recent years a dramatic impact on all industrial sectors, has recently reached the systems and software industry. Although some concepts of quality management, originally developed for other product types, can be applied to software, its specificity as a product which is developed and not produced requires a special approach. This paper introduces a quality paradigm specifically tailored on the problems of the systems and software industry.Reuse of products, processes and experience originating from the system life cycle is seen today as a feasible solution to the problem of developing higher quality systems at a lower cost. In fact, quality improvement is very often achieved by defining and developing an appropriate set of strategic capabilities and core competencies to support them. A strategic capability is, in this context, a corporate goal defined by the business position of\u00a0\u2026", "num_citations": "6\n", "authors": ["140"]}
{"title": "Recognizing patterns for software development prediction and evaluation\n", "abstract": " Managing a large scale software development requires the use of quantitative models to provide insight and support control based upon historical data from similar projects. Basili introduces a paradigm of measurement based, improvement-oriented software development, called the Improvement Paradigm [1]. This paradigm provides an experimental view of the software activities with a focus on learning and improvement. This implies the need for quantitative approaches for the following uses:                                     to build models of the software process, product, and other forms of experience (e.g., effort, schedule, and reliability) for the purpose of prediction.                                                     to recognize and quantify the influential factors (e.g. personnel capability, storage constraints) on various issues of interest (e.g. productivity and quality) for the purpose of understanding and monitoring the development\u00a0\u2026", "num_citations": "6\n", "authors": ["140"]}
{"title": "Special Feature: The Flex Software Design System: Designers Need Languages, too\n", "abstract": " An experimental design language gives evidence that software design may benefit from the same formal techniques programmers are using.", "num_citations": "6\n", "authors": ["140"]}
{"title": "A panel session\u2014User experience with new software methods\n", "abstract": " This paper brie\ufb02y describes the experience we have had in using the University of Michigan developed Problem Statement Language (PSL)/Problem Statement Analyzer (PSA). 1 We are using these computer assisted requirements tools to document and analyze operational \ufb02ight software requirements developed for the Titan 34D segment of the Space Transportation System. The Titan 34D segment is providing real-time guidance, checkout and control requirements for implementation on the Interim Upper Stage. These boost phase requirements are highly time-critical and computationbound. In addition, they must be documented in accordance to the Software Part 1 Speci\ufb01cation format of MlL-STD-483.2", "num_citations": "6\n", "authors": ["140"]}
{"title": "A structured approach to language design\n", "abstract": " This report is an attempt at systematizing a set of ground rules for high-level language design. It recommends the use of a hierarchical semantic model schema. HGL, in a step by step, top-down approach imposing more and more structure on the language components as the design becomes solidified. The approach is demonstrated by showing the stepwise design of the high-level language, GRAAL. The method recommended is divided into three major phases. The first is an informal one. The second is encoding the language components into a very high-level model. This high-level design allows a redesign of language components before they have been specified at too detailed a level. The third phase is to design the compiler in HGL using the final language design.", "num_citations": "6\n", "authors": ["140"]}
{"title": "A hierarchical machine model for the semantics of programming languages\n", "abstract": " I. Introduction A formal definitional facility for specifying the semantics of a programming language should provide a tool that aids in the design, definition, implementation and comparison of programming languages. This paper deals with the development of such a facility. The approach taken is that the metalanguage for defining the semantics of a language should try to represent the semantic structure of that language in a manner similar to the way that phrase structure grammars are used to represent the syntactic structure of languages. Just as we think of the complexity of the syntactic structure of a language as being classified by the complexity of the algorithm required to translate it, we would like to be able to think of the complexity of the semantic structure of a language as being categorized by the complexity of the algorithm required to interpret it. The purpose of this paper is to demonstrate this formal\u00a0\u2026", "num_citations": "6\n", "authors": ["140"]}
{"title": "Is there a future for Empirical Software Engineering?\n", "abstract": " Is there a future for Empirical Software Engineering? Page 1 Is there a future for Empirical Software Engineering? Victor R. Basili Department of Computer Science University of Maryland College Park, Maryland ISESE 2006 Page 2 \u00a9 University of Maryland 2006 2 Approach \u2022 Claes asked me to take a 40 year perspective (-20, now, +20) \u2022 This talk provides a personal perspective on the evolution of empirical software engineering \u2022 I will try to map change across several variables \u2013 Kinds of studies \u2013 Set of methods \u2013 Publication/review issues \u2013 Community of researchers \u2013 Replication / Meta analysis \u2013 Attention to context variables Page 3 \u00a9 University of Maryland 2006 3 Outline \u2022 Phase I \u2013 Early days: Running studies \u2022 Phase II \u2013 Tying studies together within an environment and domain \u2022 Phase III \u2013 Expanding out across environments, limiting techniques \u2022 Phase IV \u2013 Focusing on a domain Page 4 Phase I: 1974 \u2013 : for \u2026", "num_citations": "5\n", "authors": ["140"]}
{"title": "The Role of Empirical Study in Software Engineering.\n", "abstract": " The Role of Empirical Study in Software Engineering Page 1 The Role of Empirical Study in Software Engineering Victor R. Basili Professor, University of Maryland and Director, Fraunhofer Center - Maryland \u00a9 2004 Experimental Software Engineering Group, University of Maryland Page 2 2 Outline \u2022 Empirical Studies \u2013 Motivation \u2013 Specific Methods \u2013 Example: SEL \u2022 Applications \u2013 CeBASE \u2013 NASA High Dependability Computing Project \u2013 The Future Combat Systems Project \u2013 DoE High Productivity Computing System Page 3 3 Motivation for Empirical Software Engineering Understanding a discipline involves building models, eg, application domain, problem solving processes And checking our understanding is correct, eg, testing our models, experimenting in the real world Analyzing the results involves learning, the encapsulation of knowledge and the ability to change or refine our models over time The of a over \u2026", "num_citations": "5\n", "authors": ["140"]}
{"title": "Empirical evaluation of techniques and methods used for achieving and assessing software high dependability\n", "abstract": " For achieving high dependability of software intensive systems, not only product dependability benchmarking is needed but also benchmarking of technologies and processes for achieving and assessing software dependability. Dependability engineering, and more specifically technology management and assessment of effectiveness and efficiency of different technology interventions, is the objective of the work we introduce here. This work is performed as part of the High Dependability Computing Project (HDCP) 1 that is an incremental, five-year, cooperative agreement, part of a broad strategy for dependable computing, that links NASA, corporate partners and universities and research centers such as Carnegie Mellon, University of Maryland, Fraunhofer Center Maryland, University of Southern California, Massachusetts Institute of Technology, University of Washington and University of Wisconsin. For now the focus is on NASA projects, but the results will be captured and organized in an experience base, so that they could be disseminated and applied to other organizations. For example, the first step would be to extend the results to organizations that are members of the High Dependability Computing Consortium (HCC) 2 and the Sustainable Computing Consortium (SCC) 3.As part of our activities we are looking at a series of steps to evaluate such interventions. Developing high dependability software requires specifying the dependability requirements, using development techniques and methods (that we will call \u201ctechnologies\u201d) to build-in high-dependability as the product is developed, and also technologies to verify that the required\u00a0\u2026", "num_citations": "5\n", "authors": ["140"]}
{"title": "A Web Repository of Lessons Learned from COTS-Based Software Development1\n", "abstract": " A Web Repository of Lessons Learned from COTS-Based Software Development1 Page 1 Software Engineering Technology September 2002 www.stsc.hill.af.mil 25 The development of commercial offthe-shelf (COTS)-based software is different in many respects from in-house software development. Since COTS requires different activities and skills, we need to build a body of knowledge about COTSbased software development. Thus, the authors have built a Web-based repository of lessons learned, seeded with about 70 lessons extracted from literature, including journal articles [1], workshop presentations [2], and government reports [3, 4]. The authors also organized online eWorkshops [5] and are using these discussions to synthesize new lessons and refine existing ones2. They are also consolidating the repository with an unpublished set of lessons learned from the Software Engineering Institute. The lessons a \u2026", "num_citations": "5\n", "authors": ["140"]}
{"title": "The experimental software engineering group: A perspective\n", "abstract": " Results\u2022 Developed five families of reading techniques\u2013parameterized for use in different contexts and\u2013evaluated experimentally in those contexts", "num_citations": "5\n", "authors": ["140"]}
{"title": "Software Process Improvement in the NASA Software Engineering Laboratory\n", "abstract": " : The Software Engineering Laboratory (SEL) was established in 1976 for the purpose of studying and measuring software processes with the intent of identifying improvements that could be applied to the production of ground support software within the Flight Dynamics Division (FDD) at the National Aeronautics and Space Administration (NASA)/Goddard Space Flight Center (GSFC). The SEL has three member organizations: NASA/GSFC, the University of Maryland, and Computer Sciences Corporation (CSC). The concept of process improvement within the SEL focuses on the continual understanding of both process and product as well as goal-driven experimentation and analysis of process change within a production environment. 1 Background 1.1 SEL History The Software Engineering Laboratory (SEL) was created in 1976 at NASA/Goddard Space Flight Center (GSFC) for the purpose of understanding and improving the overall software process and products that were being created within the Flight Dynamics Divisi...", "num_citations": "5\n", "authors": ["140"]}
{"title": "Experimental software engineering issues\n", "abstract": " Since its inception in 1968, software engineering has struggled to find its identity. Today, we can identify three different approaches to study of the discipline of software engineering in the research corn munit y: the mathematical or formal methods approach, the system building approach, and the empirical studies group. Within the mathematical or formai methods group, the emphasis is on finding better formal methods and languages and software development is viewed as a mathematical transformation process. Within the system building group, the emphasis is on finding better methods for structuring large systems and software development is viewed as a creative task which cannot be controlled other than through rigid constraints on the resuIting producl Within the empirical studies group, the emphasis is on understanding the strengths and weaknesses of methods and tools in order to tailor them to the specific goals of a particuiar software project.", "num_citations": "5\n", "authors": ["140"]}
{"title": "The Software Industry: A State of the Art Survey\n", "abstract": " T his project be gan during the spring| o \u00ba 19Bi. The gaal uas t\u0119 o samp J e 15 to 20 ar gan i za ti or s kk aa L HHLL LCCC 0CHHHHHHH of th ispro je ct-IBM, and stud\u0173 their SLLL0 tttttt praet ic es Th is tas a cc amp lished via a two-step\u00bb res ess. K detai 1 ad surve\u0173 Form uas sent; to each of the sartie is atins eannanies. In r esg ar se to the return o \u00ba tLLL tttt a ttS00tlSll vi sit taus s ma d e. This vi sit; clar i f\u00bai ed the an sujer s given en the Form. kle be 1 i eve that this process, a 1 though 1 ini ting the numb er af p 1 a ces surv ega d. re su 1 t ed in nore accurate ttttttLLL tt b ein gpr es er te dt har", "num_citations": "5\n", "authors": ["140"]}
{"title": "Challenges in measuring hpcs learner productivity in an age of ubiquitous computing: The hpcs program\n", "abstract": " Collecting development data automatically is difficult in this era of ubiquitous home computing. This paper describes our efforts in the High Productivity Computing Systems project to better calculate effort data among a set of student programming exercises. 1.", "num_citations": "4\n", "authors": ["140"]}
{"title": "Study of Large Scale Software Testing.\n", "abstract": " CiNii \u8ad6\u6587 - Study of Large Scale Software Testing. CiNii \u56fd\u7acb\u60c5\u5831\u5b66\u7814\u7a76\u6240 \u5b66\u8853\u60c5\u5831\u30ca\u30d3\u30b2\u30fc\u30bf [\u30b5\u30a4\u30cb\u30a3] \u65e5\u672c\u306e\u8ad6\u6587\u3092\u3055\u304c\u3059 \u5927\u5b66\u56f3\u66f8\u9928\u306e\u672c\u3092\u3055\u304c\u3059 \u65e5\u672c\u306e\u535a\u58eb\u8ad6\u6587\u3092\u3055\u304c\u3059 \u65b0\u898f\u767b\u9332 \u30ed\u30b0\u30a4\u30f3 English \u691c\u7d22 \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u9589\u3058\u308b \u30bf\u30a4\u30c8\u30eb \u8457\u8005\u540d \u8457\u8005ID \u8457\u8005\u6240\u5c5e \u520a\u884c\u7269\u540d ISSN \u5dfb\u53f7\u30da\u30fc\u30b8 \u51fa\u7248\u8005 \u53c2\u8003\u6587\u732e \u51fa\u7248\u5e74 \u5e74\u304b\u3089 \u5e74\u307e\u3067 \u691c\u7d22 \u691c\u7d22 \u691c\u7d22 CiNii\u7a93\u53e3\u696d\u52d9\u306e\u518d\u958b \u306b\u3064\u3044\u3066 Study of Large Scale Software Testing. OHBA M. \u88ab\u5f15\u7528\u6587\u732e: 1\u4ef6 \u8457\u8005 OHBA M. \u53ce\u9332 \u520a\u884c\u7269 \u88ab\u5f15\u7528\u6587\u732e: 1\u4ef6\u4e2d 1-1\u4ef6\u3092 \u8868\u793a 1 \u5b66\u7fd2\u3059\u308b\u7d44\u7e54-\u30bd\u30d5\u30c8\u30a6\u30a7\u30a2\u30fb\u30d7\u30ed\u30bb\u30b9\u6539\u5584\u306b\u57fa\u3065\u304f\u7d44\u7e54\u306e \u7d99\u7d9a\u5b66\u7fd2- \u5927\u5834 \u5145 \u60c5\u5831\u51e6\u7406 00038(00005), 421-427, 1997-05-15 \u53c2\u8003\u6587\u732e9\u4ef6 \u88ab\u5f15\u7528\u6587\u732e1\u4ef6 Tweet \u5404\u7a2e\u30b3\u30fc\u30c9 NII\u8ad6\u6587ID(NAID) 10000037780 \u8cc7\u6599\u7a2e\u5225 \u56f3\u66f8\u5168\u4f53 \u30c7\u30fc\u30bf\u63d0\u4f9b\u5143 CJP\u5f15\u7528 \u66f8\u304d\u51fa\u3057 RefWorks\u306b\u66f8\u304d\u51fa\u3057 EndNote\u306b\u66f8\u304d\u51fa\u3057 Mendeley\u306b\u66f8\u304d\u51fa\u3057 Refer/BiblX\u3067\u8868\u793a RIS\u3067 \u8868\u793a BibTeX\u3067\u8868\u793a TSV\u3067\u8868\u793a \u554f\u984c\u306e\u6307\u6458 \u30da\u30fc\u30b8\u30c8\u30c3\u30d7\u3078 \u30b9\u30de\u30fc\u30c8\u30d5\u30a9\u30f3\u7248 | PC\u7248 CiNii\u306b\u3064\u3044\u3066 \u2026", "num_citations": "4\n", "authors": ["140"]}
{"title": "Maintenance= reuse-oriented software development\n", "abstract": " In this paper, we view maintenance as a reuse process. In this context, we discuss a set of models that can be used to support the maintenance process. We present a high level reuse framework that characterizes the object of reuse, the process for adapting that object for its target application, and the reused object within its target application. Based upon this framework, we offer a qualitative comparison of the three maintenance process models with regard to their strengths and weaknesses and the circumstances in which they are appropriate. To provide a more systematic, quantitative approach for evaluating the appropriateness of the particular maintenance model, we provide a measurement scheme, based upon the reuse framework, in the form of an organized set of questions that need to be answered. To support the reuse perspective, a set of reuse enablers are discussed.", "num_citations": "4\n", "authors": ["140"]}
{"title": "A structure coverage tool for Ada software systems\n", "abstract": " The last few years have seen an increased emphasis upon the development of techniques for assessing and controlling the quality of software products. Research in the U. SA 3''\u2022 13'4, and practice in the Japanese computer manufacturers' softivare factories17, M'12-\u0438 have recognized the importance of measures of program structure on one hand, and techniques for quality assurance on the other as fundamental aspects of software quality assurance.", "num_citations": "4\n", "authors": ["140"]}
{"title": "A plan for empirical studies of programmers\n", "abstract": " A plan for empirical studies of programmers | Papers presented at the first workshop on empirical studies of programmers on Empirical studies of programmers ACM Digital Library home ACM home Google, Inc. (search) Advanced Search Browse About Sign in Register Advanced Search Journals Magazines Proceedings Books SIGs Conferences People More Search ACM Digital Library SearchSearch Advanced Search Browse Browse Digital Library Collections More HomeBrowse by TitleProceedingsPapers presented at the first workshop on empirical studies of programmers on Empirical studies of programmersA plan for empirical studies of programmers Article A plan for empirical studies of programmers Share on Author: Victor Robert Basili profile image Victor R. Basili Univ. of Maryland, College Park Univ. of Maryland, College Park View Profile Authors Info & Affiliations Publication: Papers presented at the first \u2013.\u2026", "num_citations": "4\n", "authors": ["140"]}
{"title": "Monitoring an ada software development\n", "abstract": " Ada evolved from a desire within the Department of Defense to have a standard language for the development of real-time and large scale systems. In addition to providing features needed by those types of systems, Ada supports structured programming, data abstraction, modularity, and information hiding. Research with these techniques indicates that their use should improve the quality of the software developmen t process and its product. While, programmers who are most familiar with various assembly languages and FORTRAN may use structured programming, generally they are no t familiar with the other concepts. The problems with training programmers in Ada an d its associated design and programming methods and then redeveloping current systems in Ada is unknown.In order to understand the effect of using Ada, the University of Maryland and th e General Electric Company began a joint project. The\u00a0\u2026", "num_citations": "4\n", "authors": ["140"]}
{"title": "A Quantitative Analysis of a Software Development in Ada\n", "abstract": " The Department of Defense has spent a considerable amount of money and resources on the development of the new programming language Ada. To develop a better understanding of the nature of this language, it is necessary to pull it out of the research arena and use it in an industrial environment where one must deal with issues such as training, budgets and support facilities. To gain insight into the use of this new language, the University of Maryland and General Electric have jointly undertaken a study of the development of a software project written in Ada. A set of goals and questions was established at the beginning of the project. These include generic goals for any software development project, goals relating to Ada as a design and implementation language, and goals relating to metrics for the Ada Programming Support Environment APSE. Data collected from the project were analyzed. This paper describes the observations which provide answers to a subset of the goals and questions. While some of the answers are relevant only to our particular environment, others apply to any group wishing to use Ada for the first time, and still others apply to any Ada environment. More specifically, this paper addresses those goals that are related to effort, changes, errors and programmer characteristics.Descriptors:", "num_citations": "4\n", "authors": ["140"]}
{"title": "Monitoring software development through dynamic variables'\n", "abstract": " 4, 5 size, lines of code, and documentation. These studies, for the most part, used data collected at the end of past projects to predict the behavior of similar projects in the future. In 1981 the SEL concluded that many of these factors were too dependent on the environment to be useful 6 for the models that had been developed. Any model which attempts to trace these relationships should therefore be cali-brated to the environment being examined. The meta-model proposed by the SEL is 6 designed for such flexibility.", "num_citations": "4\n", "authors": ["140"]}
{"title": "Tutorial on Structured Programming, Integrated Practices\n", "abstract": " This tutorial is a major rewrite of the earlier tutorial entitled \u201cStruc-tured Programming,\u201d by the same authors. Its pupose is to provide an integrated set of techniques that can be used throughout the software development process. This integrated set of techniques makes up what has been commonly labeled Structured Programming Methodology, which covers the design, development, and validation of software, as well as the organizational structure required to execute the methodol-ogy in a product environment.Section I presents an overview. Section II deals with the basic definitions of the concepts with an emphasis on readability and correctness. Section III deals with the actual design and development process. Sec-tion IV discusses segmentation/modularization and techniques for creating a well-segmented program. Section V talks about the top-down development of the software product and an incremental development procedure called\" iterative enhancement.\" Section VI describes soft-ware development libraries as well as their application in a particular programming environment. Section VII deals with the concepts of structured walk-throughs and design inspections. Section VIII discusses the organization of people by presenting the chief programmer team concept and how it is applied to a particular product and environment.", "num_citations": "4\n", "authors": ["140"]}
{"title": "Investigating Software Development Approaches.\n", "abstract": " This paper reports on research comparing various approaches, or methodologies, for software development. The study focuses on the quantitative analysis of the application of certain methodologies in an experimental environment, in order to further understand their effects and better demonstrate their advantages in a controlled environment. A series of statistical experiments were conducted comparing programming teams that used a disciplined methodology consisting of top-down design, process design language usage, structured programming, code reading, and chief programmer team organization with programming teams and individual programmers that employed ad hoc approaches. Specific details of the experimental setting, the investigative approach used to plan, execute, and analyze the experiments, and some of the results of the experiments are discussed. authorDescriptors:", "num_citations": "4\n", "authors": ["140"]}
{"title": "Structured Programming: Tutorial: Compcon, Fall 75, Eleventh IEEE Computer Society Conference, Mayflower Hotel, Washington, DC\n", "abstract": " This tutorial on structured programming is divided into four lectures. The first lecture intro-duces several aspects of structured programming as a software development methodology. Some of the theoretical and practical foundations of structured programming are given, viewing programs as functions and relating these concepts to program correctness.", "num_citations": "4\n", "authors": ["140"]}
{"title": "Sets and Graphs in GRAAL\n", "abstract": " This paper is an attempt at presenting a high level model of the set and graph aspects of the graph algorithmic language GRAAL [5]. The problem area for which the language GRAAL was designed was the solution of graph problems of the type primarily arising in applications. It was designed with two objectives in mind. The first was to develop a language which permitted the writing of graph algorithms in a highly readable form with as natural a set of primitives as possible for describing the algorithm. The second was to allow for a wide variety of graphs of different types and complexity with as little degradation as possible in the efficient implementation and execution of an algorithm designed for a specific type of problem.", "num_citations": "4\n", "authors": ["140"]}
{"title": "FGRAAL: FORTRAN extended graph algorithmic language\n", "abstract": " ALGOKITHMC LANGUAGE GRAAL< TECHN, REPORT TR-158) AS IT HAS BEEN IMPLEMENTED FOR THE UNIVAC 1108. FGRAAL IS AN EXTENSION OF FORTRA^ v. AND is INTENDED FOR DESCRIBING AND IMPLEMENTING GRAPH ALGORITHMS OF THE TyPE PRIMARILY ARISING IN APPLICATIONS. THE FORMAL DESCRIPTION CONTAINED IN THIS REPORT REPRESENTS A SUPPLEMENT TO THE FORTRAN V. MANUAL FOR THE UNIVAC U08 (Up-4060)\u00bb THAT IS, ONLY THE NEW FEATyRES OF THE LANGUAGE ARE DESCRIBED. SEVERAL TYPICAL GRAPH ALGORITHMS* BRITTEN IN FGRAAL* ARE INCLUDED TO ILLUSTRATE VARIOUS FEATURES OF THE LANGUAGE AND TO SHOW ITS APPLICABILITY.", "num_citations": "4\n", "authors": ["140"]}
{"title": "Graal\u2014a Graph Algorithmic Language\n", "abstract": " The authors recently developed a new algorithmic language, GRAAL, for describing and implementing graph algorithms as they arise in applications (see Rheinboldt, Basili, Mesztenyi (1972A)). These algorithms involve a wide variety of graphs of different types and complexity, such as highly structured graphs with multiple arcs and self loops and with functions defined over the nodes and arcs, or very large, sparse graphs in which only the adjacency relations between the nodes are of interest. One of the design objectives of GRAAL was to allow for this wide range of possibilities without degrading overly the efficient implementation and execution of any specific class of algorithms. A second objective relates to the general need for a language which facilitates the design and communication of graph algorithms independent of a computer. In line with this, the aim was to ensure a concise and clear description\u00a0\u2026", "num_citations": "4\n", "authors": ["140"]}
{"title": "An experience based evaluation process for ERP bids\n", "abstract": " Enterprise Resource Planning ERP systems integrate information across an entire organization that automate core activities such as finance accounting, human resources, manufacturing, production and supply chain management etc. to facilitate an integrated centralized system and rapid decision making resulting in cost reduction, greater planning, and increased control. Many organizations are updating their current management information systems with ERP systems. This is not a trivial task. They have to identify the organizations objectives and satisfy a myriad of stakeholders. They have to understand what business processes they have, how they can be improved, and what particular systems would best suit their needs. They have to understand how an ERP system is built, it involves the modification of an existing system with its own set of business rules. Deciding what to ask for and how to select the best option is a very complex operation and there is limited experience with this type of contracting in organizations. In this paper we discuss a particular experience with contracting out an ERP system, provide some lessons learned, and offer suggestions in how the RFP and bid selection processes could have been improved.", "num_citations": "3\n", "authors": ["140"]}
{"title": "Learning through application: The maturing of the qip in the sel\n", "abstract": " Empirical studies\u2014formal research that uses respected, validated methods for establishing the truth of an assertion\u2014have started to make headway within software engineering. The good news is that these studies have finally become recognized as an important component of the discipline. One sees more and more empirical studies and experiments in the literature to confirm or reject the effectiveness of some method, technique, or tool.The bad news is that these studies are not yet used for discovery. The experiment is an addon, tried after the concept is considered complete. The scientific method, however, is classically based on applying a method, technique, or tool and learning from the results how to evolve the concept. This is how theories are tested and evolved over time. In the software engineering discipline, where the theories and models are still in the formative stages and processes are applied by humans as part of a creative process, observing the application or performing exploratory studies should be an important step in the evolution of the discipline.", "num_citations": "3\n", "authors": ["140"]}
{"title": "The Use of Empirical Studies in the Development of High End Computing Applications\n", "abstract": " This report provides a description of the research and development activities towards learning much about the development and measurement of productivity in high performance computing environments. Many objectives were accomplished including the development of a methodology for measuring productivity in the parallel programming domain. This methodology was tested over 25 times at 8 universities across the United States and can be used to aid other researchers studying similar environments. The productivity measurement methodology incorporates both development time and performance into a single productivity number. An Experiment Manager tool for collecting data on the development of parallel programs, as well as a suite of tools to aid in the capture and analysis of such data was also developed. Lastly, several large scale development environments were studied in order to better understand the environment used to build large parallel programming applications. That work also included several surveys and interviews with many professional programmers in these environments.Descriptors:", "num_citations": "3\n", "authors": ["140"]}
{"title": "Using uncertainty as a model selection and comparison criterion\n", "abstract": " Over the last 25+ years, software estimation research has been searching for the best model for estimating variables of interest (eg, cost, defects, and fault proneness). This research effort has not lead to a common agreement. One problem is that, they have been using accuracy as the basis for selection and comparison. But accuracy is not invariant; it depends on the test sample, the error measure, and the chosen error statistics (eg, MMRE, PRED, Mean and Standard Deviation of error samples). Ideally, we would like an invariant criterion. In this paper, we show that uncertainty can be used as an invariant criterion to figure out which estimation model should be preferred over others. The majority of this work is empirically based, applying Bayesian prediction intervals to some COCOMO model variations with respect to a publicly available cost estimation data set coming from the PROMISE repository.", "num_citations": "3\n", "authors": ["140"]}
{"title": "Iterative enhancement: A practical technique for software development\n", "abstract": " This paper recommends the\" iterative enhancement\" technique as a practical means of using a top-down, stepwise refinement approach to software development. This technique begins with a simple initial implementation of a properly chosen (skeletal) subproject which is followed by the gradual enhancement of successive implementations in order to build the full implementation. The development and quantitative analysis of a production compiler for the language SIMPL-T is used to demonstrate that the application of iterative enhancement to software development is practical and efficient, encourages the generation of an easily modifiable product, and facilitates reliability.", "num_citations": "3\n", "authors": ["140"]}
{"title": "Matching software measurements to business goals\n", "abstract": " Development strategy\u2022 Analyze, compare, and integrate methods\u2013Methods like GQM, BSC, PSM, SPC, Mikko\u2013Frameworks like QIP, EF, PROFESS\u2022 Build prototype process for customer engagement\u2022 Document measurement processes and scenarios\u2022 Collect a representative sample of industrial case-studies to\u2013Generate sample sets of corporate goals, contexts, scenarios\u2013Seed the experience base", "num_citations": "3\n", "authors": ["140"]}
{"title": "The SEL adapts to meet changing times\n", "abstract": " Since 1976, the Software Engineering Laboratory (SEL) has been dedicated to understanding and improving the way in which one NASA organization, the Flight Dynamics Division (FDD) at Goddard Space Flight Center, develops, maintains, and manages complex flight dynamics systems. It has done this by developing and refining a continual process improvement approach that allows an organization such as the FDD to fine-tune its process for its particular domain. Experimental software engineering and measurement play a significant role in this approach. The SEL is a partnership of NASA Goddard, its major software contractor, Computer Sciences Corporation (CSC), and the University of Maryland's (LTM) Department of Computer Science. The FDD primarily builds software systems that provide ground-based flight dynamics support for scientific satellites. They fall into two sets: ground systems and simulators. Ground systems are midsize systems that average around 250 thousand source lines of code (KSLOC). Ground system development projects typically last 1-2 years. Recent systems have been rehosted to workstations from IBM mainframes, and also contain significant new subsystems written in C and C++. The simulators are smaller systems averaging around 60 KSLOC that provide the test data for the ground systems. Simulator development lasts up to 1 year. Most of the simulators have been built in Ada on workstations. The SEL is responsible for the management and continual improvement of the software engineering processes used on these FDD projects.", "num_citations": "3\n", "authors": ["140"]}
{"title": "Software Engineering Laboratory (SEL) cleanroom process model\n", "abstract": " The Software Engineering Laboratory (SEL) cleanroom process model is described. The term 'cleanroom' originates in the integrated circuit (IC) production process, where IC's are assembled in dust free 'clean rooms' to prevent the destructive effects of dust. When applying the clean room methodology to the development of software systems, the primary focus is on software defect prevention rather than defect removal. The model is based on data and analysis from previous cleanroom efforts within the SEL and is tailored to serve as a guideline in applying the methodology to future production software efforts. The phases that are part of the process model life cycle from the delivery of requirements to the start of acceptance testing are described. For each defined phase, a set of specific activities is discussed, and the appropriate data flow is described. Pertinent managerial issues, key similarities and differences between the SEL's cleanroom process model and the standard development approach used on SEL projects, and significant lessons learned from prior cleanroom projects are presented. It is intended that the process model described here will be further tailored as additional SEL cleanroom projects are analyzed.", "num_citations": "3\n", "authors": ["140"]}
{"title": "Characterizing Resource Data: A Model for Logical Association of Software Data\n", "abstract": " _-This paper presents a conceptual model of software deveIOpment resource data. A conceptual model, such as this, is a pre-requisite to the development of integrated project support environments which aim to assist in the processes of resource \u201cestimation evaluation and control. The model proposed is a four dimensional view of resources which can be used for resource estimation, utilization, and review. A process model is presented showing the use of the data model, and instances of the goal, question, metric paradigm are presented to show the applicability of the models to the __ measurement task. The model is validated by reference to published literature on resource databases and the implications of the model in these database environments is discussed.. _", "num_citations": "3\n", "authors": ["140"]}
{"title": "Software development in Ada\n", "abstract": " Ada will soon become a part of systems developed for the US Department of Defense. NASA must determine whether it will become part of its environment and particularly whether it will become a part of the Space Station development. How-ever, there are several issues about Ada which should be considered before this decl-slon is made. What Information is needed to make that decision? What are the training needs for Ada? How should the life cycle be modified to use Ada most effectively? What other Issues should management consider before making a decl-slon? These are but a few of the issues that should be considered.", "num_citations": "3\n", "authors": ["140"]}
{"title": "Changes and Errors as Measures of Software Development\n", "abstract": " In addition to resource expenditures, there are other These laws can be demonstrated by using the following aspects of software development that can give us informa-metrics: tion about managing and engineering the process and the RSN, the release number; product. One such aspect is the changes and errors generated D,, the age of system at release R; during development and maintenance. Information ob- I,, the time between releases R-1 and R; tained by monitoring the changes in the software helps us M,, the number of modules in the system; ascertain the level of effort necessary to arrive at a satisfac- MH,, the number of modules handled during release in-tory product. If we can classify the changes that occur or terval I,(estimator of activity undertaken in each retheir origins, we can categorize the environment and learn lease); how to manage or minimize the detrimental effects of par- HR,, MH,/1,, the handle rate; ticular types of changes. For example, suppose the user is Cr, MH,/M,, the complexity, which is the fraction of regenerating a series of major changes at a continual rate. leased system modules that were handled during theThese changes may provide management with the informa-course of the release R. tion it needs to reclassify the environment as a more complex type, permitting modification of the cost parameters in the C, has been observed to be monotonically increasing and resource estimation model and in the reestimation of cost approaching unity over time (for OS 360~ 20 releases over 10", "num_citations": "3\n", "authors": ["140"]}
{"title": "FLEX: a flexible, automated process design system.\n", "abstract": " The FLEX Design System is a software design language and its processor that together form a tool for use in computer software development activities. The system combines features originating in earlier Process Design Languages PDLs with many features found in modern programming languages. The system is quite flexible and can be adapted to different programming environments in effect the language can be configured to produce a family of less flexible PDLs. Among the features offered by the FLEX language are a modular design structure, type abstraction, definable operators, generic routines, strong type checking, consistency checking of all functional interfaces, and protection of selected data from alteration in certain environments. AuthorDescriptors:", "num_citations": "3\n", "authors": ["140"]}
{"title": "Language as a Tool for Scientific Programming\n", "abstract": " Programming languages act as software development tools for problems from a specific application area. The needs of the various scientific programming applications vary greatly with the size and style of the problems which include everything from small numerical algorithms to large-scale systems. This latter requires language primitives for a multitude of subproblems that include the management of data, the interfaces with the system at various levels, etc. One way of making available all the necessary primitives is to develop one very large language to cover all the needs. An alternative is to use a hierarchical family of languages, each covering a different aspect of the larger problem, e.g., a mathematical language, a data base management language, a graphics language, etc. The concept of a family of languages built from a small common base offers a modular, well-interfaced set of small specialized languages\u00a0\u2026", "num_citations": "3\n", "authors": ["140"]}
{"title": "SIMPL-R and its application to large, sparse matrix problems\n", "abstract": " A description of the computer programming language SIMPL-R is given.\u201d SIMPL\u2014R is the member of the SIMPL family of structured programming languages intended for use with numerical computa\u2014tions such as those which arise in connection with the solution of scientific and engineering problems. _An example is given showing an implementation in SIMPL-R of an algorithm for the solution of sparse matrix problems using an arc-graph data structure. Comparisons with the same algorithm coded in FORTRAN show that the nonoptimizing SIMPL~ R compiler produces code which is ten to twenty percent faster than that produced by the opti mizing FORTRAN compiler.", "num_citations": "3\n", "authors": ["140"]}
{"title": "An Analysis of the Contracting Process for an ERP System\n", "abstract": " Enterprise Resource Planning (ERP) systems integrate information across an entire organization that automate core activities such as finance/accounting, human resources, manufacturing, production and supply chain management\u2026 etc. to facilitate an integrated centralized system and rapid decision making\u2013resulting in cost reduction, greater planning, and increased control. Many organizations are updating their current management information systems with ERP systems. This is not a trivial task. They have to identify the organization\u2019s objectives and satisfy a myriad of stakeholders. They have to understand what business processes they have, how they can be improved, and what particular systems would best suit their needs. They have to understand how an ERP system is built; it involves the modification of an existing system with its own set of business rules. Deciding what to ask for and how to select the best option is a very complex operation and there is limited experience with this type of contracting in organizations. In this paper we discuss a particular experience with contracting out an ERP system, provide some lessons learned, and offer suggestions in how the RFP and bid selection processes could have been improved.", "num_citations": "2\n", "authors": ["140"]}
{"title": "A preliminary empirical study to compare MPI and OpenMP\n", "abstract": " Context: The rise of multicore is bringing shared-memory parallelism to the masses. The community is struggling to identify which parallel models are most productive.Objective: Measure the effect of MPI and OpenMP models on programmer productivity.Design: One group of programmers solved the sharks and fishes problem using MPI and a second group solved the same problem using OpenMP, then each programmer switched models and solved the same problem again. The participants were graduate students in an HPC course.Measures: Development effort (hours), program correctness (grades), program performance (speedup versus serial implementation).Results: Mean OpenMP development time was 9.6 hours less than MPI (95% CI, 0.37-19 hours), a 43% reduction. No statistically significant difference was observed in assignment grades. MPI performance was better than OpenMP performance for 4 out\u00a0\u2026", "num_citations": "2\n", "authors": ["140"]}
{"title": "What's so hard about replication of Software Engineering experiments?\n", "abstract": " \u2022 Why replicate experiments:\u2013to verify the results from the first experiment\u2013to expand our knowledge of the discipline\u2013to build models that can be used to predict and be challenged", "num_citations": "2\n", "authors": ["140"]}
{"title": "An approach to improving parametric estimation models in the case of violation of assumptions based upon risk analysis\n", "abstract": " Title of Dissertation: An Approach to Improving Parametric Estimation Models in the Case of Violation of Assumptions Based upon Risk Analysis", "num_citations": "2\n", "authors": ["140"]}
{"title": "Gaining Early Insight into Software Safety: Measures of Potential Problems and Risks\n", "abstract": " \u25aa Systems of systems provide more complex problems for safety engineers\u2014not stand-alone and manually controlled\u25aa A large network centric system of systems will have a large number of safety hazards (with multiple causes and controls) that safety engineers must track and verify before the system is deployed\u2014a predominately manual process won\u2019t work\u25aa Software is an ever-increasing part of the system", "num_citations": "2\n", "authors": ["140"]}
{"title": "High Dependability Computing Program Modeling Dependability The Unified Model of Dependability\n", "abstract": " Individuals and organizations increasingly use sophisticated software systems from which they demand great reliance.\u201cReliance\u201d is contextually subjective and depends on the particular stakeholder\u2019s needs; therefore, in different circumstances, the stakeholders will focus on different properties of such systems, eg, continuity, availability, performance, real-time response, ability to avoid catastrophic failures, capability of resisting adverse conditions, and prevention of deliberate privacy intrusions. The concept of dependability enables these various concerns to be subsumed within a single conceptual framework. Achieving dependability is a major challenge, which has spawned many efforts both at national and international levels. This work is part of the High Dependability Computing Program (HDCP), a five-year cooperative research agreement between NASA and various universities and research centers to increase NASA\u2019s ability to engineer highly dependable software systems. HDCP brings together, under the common goal of improving systems dependability, a", "num_citations": "2\n", "authors": ["140"]}
{"title": "Science and engineering for software development: a recognition of Harlan D. Mills' legacy\n", "abstract": " To create a fitting memorial to Harlan, we turn our attention (as he would wish) to some of the themes that occupied him during his life. Even though it is a commemorative event, the colloquium is not just reminiscence on one man\u2019s work. Rather, it is focused on the extension of that work to today\u2019s state of the art. It aims to underscore the relevance of his ideas to modern challenges in research and practice.2 COLLOQUIUM PROGRAM The program will include invited presentations, refereed papers, a panel session, and an award lecture. The invited speakers are Professor Fred P. Brooks and Professor David L. Parnas. The panel session is chaired by Professor Victor Basili and includes Mr Terry Baker, Dr Susan L. Gerhart, Professor Al Hevner and Professor Jesse Poore as members. The award lecture will be presented by the 1999 recipient of the annual \u201cHarlan D. Mills Practical Visionary Prize.\u201d This award is\u00a0\u2026", "num_citations": "2\n", "authors": ["140"]}
{"title": "Defining and validating measures for object-based high-level design\n", "abstract": " The availability of significant measures in the early phases of the software development life-cycle allows for better management of the later phases, and more effective quality assessment when quality can be more easily affected by preventive or corrective actions. In this paper, we introduce and compare various high-level design measures for object-based software systems. The measures are derived based on an experimental goal, identifying fault-prone software parts, and several experimental hypotheses arising from the development of Ada systems for Flight Dynamics Software at the NASA Goddard Space Flight Center (NASA/GSFC). Specifically, we define a set of measures for cohesion and coupling, and theoretically analyze them by checking their compliance with a previously published set of mathematical properties that we deem important. We then investigate their relationship to fault-proneness on three\u00a0\u2026", "num_citations": "2\n", "authors": ["140"]}
{"title": "Support for comprehensive\n", "abstract": " Reuse of products, processes and other knowledge will be the key to enable the software industry to achieve the dramatic improvement in productivity and quality required to satisfy anticipated growing demands. Although experience shows that certain kinds of reuse can be successful, general success has been elusive. A software life-cycle technology that allows comprehensive reuse of all kinds of software-related experience could provide the means of achieving the desired order-of-magnitude improvements. In this paper, we introduce a comprehensive framework of models, model-based characterisation schemes, and support mechanisms for better understanding, evaluating, planning and supporting all aspects of reuse.", "num_citations": "2\n", "authors": ["140"]}
{"title": "Towards a mature measurement environment: Creating a software engineering research environment\n", "abstract": " Software engineering researchers are building tools, defining methods, and models; however, there are problems with the nature and style of the research. The research is typically bottom-up, done in isolation so the pieces cannot be easily logically or physically integrated. A great deal of the research is essentially the packaging of a particular piece of technology with little indication of how the work would be integrated with other prices of research. The research is not aimed at solving the real problems of software engineering, i.e., the development and maintenance of quality systems in a productive manner. The research results are not evaluated or analyzed via experimentation or refined and tailored to the application environment. Thus, it cannot be easily transferred into practice. Because of these limitations we have not been able to understand the components of the discipline as a coherent whole and the relationships between various models of the process and product. What is needed is a top down experimental, evolutionary framework in which research can be focused, logically and physically integrated to produce quality software productively, and evaluated and tailored to the application environment. This implies the need for experimentation, which in turn implies the need for a laboratory that is associated with the artifact we are studying. This laboratory can only exist in an environment where software is being built, i.e., as part of a real software development and maintenance organization. Thus, we propose that Software Engineering Laboratory (SEL) type activities exist in all organizations to support software engineering research. We\u00a0\u2026", "num_citations": "2\n", "authors": ["140"]}
{"title": "Examining the modularity of Ada programs\n", "abstract": " Examining the modularity of Ada programs | Proceedings of the Joint Ada conference fifth national conference on Ada technology and fourth Washington Ada Symposium ACM Digital Library home ACM home Google, Inc. (search) Advanced Search Browse About Sign in Register Advanced Search Journals Magazines Proceedings Books SIGs Conferences People More Search ACM Digital Library SearchSearch Advanced Search ada Conference Proceedings Upcoming Events Authors Affiliations Award Winners More HomeConferencesADAProceedingsWADAS '87Examining the modularity of Ada programs ARTICLE Examining the modularity of Ada programs Share on Authors: Elizabeth E Katz profile image Elizabeth E. Katz View Profile , Victor Robert Basili profile image Victor R. Basili View Profile Authors Info & Affiliations Publication: WADAS '87: Proceedings of the Joint Ada conference fifth national on Ada \u20130'.\u2026", "num_citations": "2\n", "authors": ["140"]}
{"title": "Teaching principles of computer programming\n", "abstract": " We describe a first course of two semesters in computer science that is based on mathematical principles of computer program analysis and design, rather than on apprenticeship programming. The course is modeled on elementary mathematics courses in both content and pedagogy. The content is based on a calculus of programs in which the functional semantics of programs and program parts are derived from the character valued functions that define the texts of programs. The first semester uses a minimal, but practical, subset of Pascal that has only character and character file data types, called CF Pascal, and is treated as a ruler and compass language for pedagogical advantage. This simple language, with no numbers, no arrays, no gotos, permits a complete and exact treatment of its operational semantics before the functional semantics is developed. The second semester begins with a treatment of data\u00a0\u2026", "num_citations": "2\n", "authors": ["140"]}
{"title": "Proposals for tool and methodology evaluation experiments: ACM SIGSOFT sponsored software engineering symposium on tool and methodology evaluation\n", "abstract": " The First ACM SIGSOFT Software Engineering Symposium was held in Pingree Park, Colorado, on June 9-11, 1981. The theme fo r the symposium was tool and methodology evaluation. Bill Riddle was general chairperson and I was program chairman. The Program Committee consisted of Les Belady, Carl Davis, Susan Gerhart, Bill Howden, John Musa, and Leon Stucki.", "num_citations": "2\n", "authors": ["140"]}
{"title": "Tutorial on Models and Metrics for Software Management and Engineering: Initially Presented at COMPSAC80, the IEEE Computer Society's Fourth International Computer Software\u00a0\u2026\n", "abstract": " \u201cThis tutorial presents a new, quantitative approach to software management and software engineering that has taken shape over the past few years.\u201d", "num_citations": "2\n", "authors": ["140"]}
{"title": "Transporting up: a case study\n", "abstract": " This paper discusses the various aspects of transporting one language, simpl-t, from the UNIVAC 1108 computer to the CDC 6700 computer, a machine of larger word size. Special attention is given to the operational mechanism, the testing plan, the problems encountered in transporting, and the various statistics kept during the development.", "num_citations": "2\n", "authors": ["140"]}
{"title": "Experiences with a simple structured programming language\n", "abstract": " A great deal of interest has developed in structured programming [Dahl, Dijkstra, and Hoare, 1972] during the past few years. This paper is concerned with some experiences obtained in the use of a structured programming language in the computer science curriculum at the University of Maryland. The language used was SIMPL-X [Basili, 1973], a language designed and implemented at the University of Maryland.", "num_citations": "2\n", "authors": ["140"]}
{"title": "SIMPL-X\n", "abstract": " This report contains a description of the programming language, SIMPL-X, which is the base language for a family of programming languages that will be extensions to SIMPL-X and whose compilers will be written in SIMPL-X and its extensions. It is a transportable compiler-writing, systems language which was developed to provide a basis for the redefinition of the graph algorithmic language GRAAL. SIMPL-X is a procedure-oriented, non-block structured language with an extensive set of operators, including arithmetic, relational, logical, bit manipulation, shift, indirect reference, address reference, and partword operators. It is designed for writing GRAAL programs that conform to the standards of structured programming and modular design and for efficiently expressing and implementing algorithms written in it. In addition, it appears to be a good language for modeling and certifying the correctness and\u00a0\u2026", "num_citations": "2\n", "authors": ["140"]}
{"title": "On a programming language for graph algorithms\n", "abstract": " Science Foundation and Grant NGL-21-002-008 from the National Aeronautics and Space Administration.", "num_citations": "2\n", "authors": ["140"]}
{"title": "Aligning an Organization\u2019s Goals and Strategies through Measurement: GQM+Strategies\u00ae\n", "abstract": " Organizations need to be able to develop operational business goals define strategies for implementing them communicate the goals throughout the organization translate the goals into lower levels for projects assess the effectiveness of their strategies recognize the achievement of their business goals measure for visibility, control, and improvement", "num_citations": "1\n", "authors": ["140"]}
{"title": "Keynote\n", "abstract": " Why do we replicate experiments?-to verify the results from the first experiment, to expand our knowledge of the discipline, to build models that can be used to predict and to be challenged. So what does it mean to replicate an experiment? What are the criteria for a replication? Physicists and sociologists experiment and replicate experiments, but their expectations are different with respect to verifiability, expansion of knowledge, precision in prediction. The difference has to do with the nature of the domain they are studying. The domain affects their ability to generate relevant and testable hypotheses, identify, control, and manipulate the context variables, supply the appropriate documentation. In all cases it requires the support of a sufficiently large community of researchers who think empirically. Maybe we should have started with the question: What's so different/hard about experimentation in software engineering\u00a0\u2026", "num_citations": "1\n", "authors": ["140"]}
{"title": "Measurement and Model Building Discussion and Summary\n", "abstract": " This chapter summarizes the discussions that took place during the Measurement and Model Building session of the Dagstuhl seminar on Empirical Software Engineering (ESE). The goal of this session was to address questions concerning two topics: data sharing and effective data interpretation.", "num_citations": "1\n", "authors": ["140"]}
{"title": "Measurement and model building introduction\n", "abstract": " The goal of empirical study is to build, test, and evolve models of a discipline. This requires studying the variables of interest in multiple contexts and building a set of models that can be evolved, discredited, or used with confidence. This implies we need to perform multiple studies, both replicating as closely as possible and varying some of the variables to test the robustness of the current model. It involves running many studies in different environments, addressing as many context variables as possible and either building well parameterized models or families of models that are valid under different conditions. This is beyond the scope of an individual research group. Thus it involves two obvious problems: how do we share data and artifacts across multiple research groups and what are good methods for effectively interpreting data, especially across multiple studies.", "num_citations": "1\n", "authors": ["140"]}
{"title": "Cedar\u2013cyberinfrastructure for empirical data analysis and reuse\n", "abstract": " The NSF Next Generation Cybertools program has the ambitious goal of producing technologies that \u201cnot only change ways in which social and behavioral scientists research the behavior of organizations and individuals, but also serve sciences more broadly.\u201d This goal is particularly salient because the increased automation and \u201cdigitization\u201d of work creates a sea of information about organizations and their processes. The availability of data creates the potential to revolutionize the way we understand, design, and manage organizations. To gain insight from this sea of data (rather than being drowned by it), we need ways to find patterns, interpret them and generalize appropriately.In commercial organizations, opportunities to exploit improved mechanisms for qualitative and quantitative data exist in every core business process, such as new product development, customer support, supply chain management, and basic accounting. In addition to competitive pressures for process control and improvement, which date back to the early days of scientific management [57], commercial organizations are facing increased demands for compliance monitoring and internal controls [38]. Technologies, such as Enterprise Resource Planning systems, and continuous assurance auditing systems [73] create a virtual tidal wave of quantitative accounting data, but organizations lack effective ways to integrate the qualitative data needed to interpret it [38].", "num_citations": "1\n", "authors": ["140"]}
{"title": "Investigating the interaction between inspection process specificity and software development experience\n", "abstract": " This paper describes a study conducted to compare the interaction of experience and specificity in a requirements inspection technique. Two versions of a requirements inspection technique, PBR, were generated. One version had a high level of specificity and the other had a low level of specify. These techniques  were used by subjects of varying experience levels to determine if experience and specificity were related. The results of the study indicated very little difference among the treatment groups. As a result, we examined any assumptions that we made about the environment. In doing so, we uncovered some issues that must be addressed in future studies that focus on people. This paper provides a complete description of the results obtained and describes the assumptions that we made and their impact on the reliability of the results. (UMIACS-TR-2003-100)", "num_citations": "1\n", "authors": ["140"]}
{"title": "Software Improvement Feedback Loops: The SEL Experience\n", "abstract": " Used the SEL as a laboratory to build models, test hypotheses, Used the University to test high risk ideas Developed technologies, methods and theories when necessary Learned what worked and didn\u2019t work, applied ideas when applicable Kept the business going with an aim at improvement, learning", "num_citations": "1\n", "authors": ["140"]}
{"title": "Keynote on \u201cExperimental software engineering\u201d\n", "abstract": " This presentation offers a view of software development based upon building knowledge through model building, experimentation, and learning. It treats the study of software engineering as a laboratory science. To this end, various abstraction techniques, experimental designs, and learning methods must evolve to support this paradigm.To support experimental software engineering research, we need to provide a framework for building relevant practical Software Engineering knowledge that will increase the effectiveness of individual experiments. To support the practitioner, we need to provide a better basis for making judgements about selecting and tailoring processes and organizations integrate their experiences with processes.", "num_citations": "1\n", "authors": ["140"]}
{"title": "OSMA Software Program: Domain Analysis Guidebook\n", "abstract": " Domain analysis is the process of identifying and organizing knowledge about a class of problems. This guidebook presents a method of performing experience domain analysis in software development organizations. The purpose of the guidebook is to facilitate the reader in characterizing two given development environments, applying domain analysis to model each, and then applying an evaluation process, based upon the Goal/Metric/Paradigm, to transfer a given development technology from one of the environments to the other. This guidebook describes this process and gives an example of its use within NASA. Also cross-referenced as UMIACS-TR-99-16", "num_citations": "1\n", "authors": ["140"]}
{"title": "Riskit: Increasing Confidence in Risk Management\n", "abstract": " Software development is often plagued with unanticipated problems which cause projects to miss deadlines, exceed budgets, or deliver less than satisfactory products. While these problems cannot be eliminated totally, some of them can be controlled better by taking appropriate preventive action. Risk management is an area of project management that deals with these threats before they occur. Organizations may be able to avoid a large number of problems if they use systematic risk management procedures and techniques early in projects.Several risk management approaches have been introduced during the past decade [1-5] and while some organizations, especially in the US defense sector [1, 6], have defined their own risk management approaches, most organizations do not manage their risks explicitly and systematically [7]. Risk management based on intuition and individual initiative alone is seldom effective and rarely consistent.", "num_citations": "1\n", "authors": ["140"]}
{"title": "A Framework for Collecting and Analyzing Usability Data\n", "abstract": " Usability has been recognized as an important factor in software quality. In this paper, we give a framework for collecting and analysis data for software usability evaluation. We use the Goal/Question/Metric method, build a model for the human-computer interaction process, derive operational scenarios from the model, and define the questions and metrics for usability evaluation for each of these scenarios. We discuss the different mechanisms needed for collecting different usability data, and give suggestions about data analysis as well as organizational strategies for data interpretation. The framework is intended to be used to form the basic structure of an organization's usability engineering activities.", "num_citations": "1\n", "authors": ["140"]}
{"title": "CMSC 735: A Quantitative Approach to Software Management and Engineering\n", "abstract": " The application of a successful engineering discipline requires: A combination of technical and managerial solutions A well defined set of product needs to satisfy the customer to assist the developer in accomplishing those needs to create competencies for future business A well defined set of processes to accomplish what needs to be accomplished to control development to improve the business A closed loop process that supports learning and feedback Key technologies for supporting these needs include: modeling, measurement, reuse of processes, products, and other forms of knowledge relevant to the discipline", "num_citations": "1\n", "authors": ["140"]}
{"title": "Evolving and packaging reading technologies\n", "abstract": " Reading is a fundamental technology for achieving quality software. This paper provides a motivation for reading as a quality improvement technology, based upon experiences in the Software Engineering Laboratory at NASA Goddard Space Flight Center and shows the evolution of our study of reading via a series of experiments. The experiments range from the early reading vs. testing experiments to various Cleanroom experiments that employed reading to the development of new reading technologies currently under study.", "num_citations": "1\n", "authors": ["140"]}
{"title": "Establishing measurement for software quality improvement\n", "abstract": " Establishing measurement for software quality improvement | Proceedings of the IFIP TC8 Open Conference on Business Process Re-engineering: Information Systems Opportunities and Challenges ACM Digital Library home ACM home Google, Inc. (search) Advanced Search Browse About Sign in Register Advanced Search Journals Magazines Proceedings Books SIGs Conferences People More Search ACM Digital Library SearchSearch Advanced Search Browse Browse Digital Library Collections More HomeBrowse by TitleProceedingsProceedings of the IFIP TC8 Open Conference on Business Process Re-engineering: Information Systems Opportunities and ChallengesEstablishing measurement for software quality improvement Article Establishing measurement for software quality improvement Share on Authors: Ross Jeffery profile image Ross Jeffery View Profile , Victor Robert Basili profile image Victor R. -\u2026", "num_citations": "1\n", "authors": ["140"]}
{"title": "Software Errors and Complexity: An Empirical Investigation\n", "abstract": " Panel Editor An analysis of the distributions and relationships derived from the change data collected uring development of a medium-scale software project produces ome surprising insights into the factors influencing software development. Among these are the tradeoffs between modifying an existing module as opposed to creating a new one, and the relationship between module size and error proneness.", "num_citations": "1\n", "authors": ["140"]}
{"title": "Integrating automated support for a software management cycle into the TAME system\n", "abstract": " Software managers are interested in the quantitative management of software quality, cost and progress. There have been many of models and tools developed, but they are of limited scope. An integrated software management methodology, which can be applied throughout the software life cycle for any number purposes, is required.", "num_citations": "1\n", "authors": ["140"]}
{"title": "The Experimental Aspects of a Professional Degree in Software Engineering\n", "abstract": " Software engineering needs a support mechanism to aid in the transition of research results into practice. Such a mechanism for providing education, training and practical experience in software engineering could be provided by a special degree program: a Master of Software Engineering (MSE). The key to such a curriculum is the establishment of the equivalent of a teaching hospital through various software development organizations: a teaching software engineering laboratory. Combining classroom education with skill development, the professional software development laboratory will use the latest techniques and tools, and the practitioner will have the opportunity to gain experience in using them under the supervision of experts.", "num_citations": "1\n", "authors": ["140"]}
{"title": "A study of fault prediction and reliability assessment in the SEL environment\n", "abstract": " This paper presents an empirical study on estimation and prediction of faults, prediction of fault detection and correction effort, and reliability assessment in the Software Engineering Laboratory environment (SEL).", "num_citations": "1\n", "authors": ["140"]}
{"title": "Analyzing A Syntactic Family of Complexity Metrics\n", "abstract": " A family of syntactic complexity metrics is defined that generates several metrics commonly occurring in the literature. The paper uses the family to answer some questions about the relationship of these metrics to error-proneness and to each other. Two derived metrics are applied; slope which measures thfe\u00e9 relative skills of programmers at handling a given level of complexity and r square which is indirectly related to the consistency of performance of the programmer or team. The study suggests that individual differences have a large effect on the significance of results where many individuals are used. When an individual is isolated, better results are obtainable. The metrics can also be used to differentiate between projects on which a methodology was used and those on which it was not.", "num_citations": "1\n", "authors": ["140"]}
{"title": "Generalizing Specifications for Uniformly Implemented Loops.\n", "abstract": " The problem of generalizing functional specifications for WHILE loops is considered. This problem occurs frequently when trying to verify that an initialized loop satisfies some functional specification, ie, produces outputs which are some function of the program inputs. The notion of a valid generalization of a loop specification is defined. A particularly simple valid generalization, a base generalization, is discussed. A property of many commonly occurring WHILE loops, that of being uniformly implemented, is defined. A technique is presented which exploits this property in order to systemically achieve a valid generalization of the loop specification. Two classes of uniformly implemented loops which are particularly susceptible to this form of analysis are defined and discussed. The use of the proposed technique is illustrated with a number of applications. Finally, an implication of the concept of uniform loop implementation for the validation of the obtained generalization is explained. AuthorDescriptors:", "num_citations": "1\n", "authors": ["140"]}
{"title": "The Package-Based Deveiopment Process in the Flight Dynamics Division\n", "abstract": " The SEL has been operating for more than two decades in the FDD and has adapted to the constant movement of the software development environment. The SEL\u2019s Improvement Paradigm shows that process improvement is an iterative process. Understanding, Assessing and Packaging are the three steps that are followed in this cyclical paradigm. As the improvement process cycles back to the \ufb01rst step, after having packaged some experience, the level of understanding will be greater. In the past, products resulting from the packaging step have been large process documents, guidebooks, and training programs. As the technical world moves toward more modularized software, we have made a move toward more modularized software development process documentation, as such the products of the packaging step are becoming smaller and more frequent. In this manner, the QIP takes on a more spiral\u00a0\u2026", "num_citations": "1\n", "authors": ["140"]}