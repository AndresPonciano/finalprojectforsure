{"title": "Report on the programming language Haskell: a non-strict, purely functional language version 1.2\n", "abstract": " \"Some half dozen persons have written technically on combinatory logic, and most of these, including ourselves, have published something erroneous. Since some of our fellow sinners are among the most careful and competent logicians on the contemporary scene, we regard this as evidence that the subject is refractory. Thus fullness of exposition is necessory for accurary; and excessive condensation would be false economy here, even more than it is ordinarily.\"", "num_citations": "1996\n", "authors": ["307"]}
{"title": "Why functional programming matters\n", "abstract": " As software becomes more and more complex, it is more and more important to structure it well. Well-structured software is easy to write, easy to debug, and provides a collection of modules that can be re-used to reduce future programming costs. Conventional languages place conceptual limits on the way problems can be modularised. Functional languages push those limits back. In this paper we show that two features of functional languages in particular, higher-order functions and lazy evaluation, can contribute greatly to modularity. As examples, we manipulate lists and trees, program several numerical algorithms, and implement the alpha-beta heuristics (an Artificial Intelligence algorithm used in game-playing programs). Since modularity is the key to successful programming, functional languages are vitally important to the real world.", "num_citations": "1399\n", "authors": ["307"]}
{"title": "Generalising monads to arrows\n", "abstract": " Monads have become very popular for structuring functional programs since Wadler introduced their use in 1990. In particular, libraries of combinators are often based on a monadic type. Such libraries share (in part) a common interface, from which numerous benefits flow, such as the possibility to write generic code which works together with any library. But, several interesting and useful libraries are fundamentally incompatible with the monadic interface. In this paper I propose a generalisation of monads, which I call arrows, with significantly wider applicability. The paper shows how many of the techniques of monadic programming generalise to the new setting, and gives examples to show that the greater generality is useful. In particular, three non-monadic libraries for efficient parsing, building graphical user interfaces, and programming active web pages fit naturally into the new framework.", "num_citations": "634\n", "authors": ["307"]}
{"title": "Programming language concepts and paradigms\n", "abstract": " Reserve List Page 1 Com S 541 | Programming Languages 1 August 22, 1997 Reserve List See the \\Introduction to the Literature\" for books not on reserve. 1 General Author: Watt, David A. (David Anthony) Title: Programming language concepts and paradigms Publisher: New York : Prentice Hall, 1990. LOCATION: CALL NUMBER STATUS: PARKS LIBRARY Reserve QA76.7 .W39 1990 Not checked out Room Author: Friedman, Daniel P. Title: Essentials of programming languages Publisher: Cambridge, Mass. : MIT Press ; New York : McGraw-Hill, c1992. LOCATION: CALL NUMBER STATUS: PARKS LIBRARY Reserve QA76.7 .F73 1992 Not checked out Room 2 Object-Oriented Programming Languages Author: Budd, Timothy. Title: A Little Smalltalk Publisher: Reading, Mass. : Addison-Wesley, c1987. LOCATION: CALL NUMBER STATUS: PARKS LIBRARY Reserve QA76.6 .B835 1987 Not checked out Room : , -\u2026", "num_citations": "494\n", "authors": ["307"]}
{"title": "A history of Haskell: being lazy with class\n", "abstract": " This paper describes the history of Haskell, including its genesis and principles, technical contributions, implementations and tools, and applications and impact.", "num_citations": "446\n", "authors": ["307"]}
{"title": "Proving the correctness of reactive systems using sized types\n", "abstract": " We have designed and implemented a type-based analysis for proving some basic properties of reactive systems. The analysis manipulates rich type expressions that contain information about the sizes of recursively defined data structures. Sized types are useful for detecting deadlocks, nontermination, and other errors in embedded programs. To establish the soundness of the analysis we have developed an appropriate semantic model of sized types.", "num_citations": "417\n", "authors": ["307"]}
{"title": "Super-combinators a new implementation method for applicative languages\n", "abstract": " There is a growing interest nowadays in functional programming languages and systems, and in special hardware for executing them. Many of these implementations are based on a system called graph reduction (GR), in which a program is represented as a graph which is transformed, or reduced, by the machine until it represents the desired answer. The various graph reduction implementations differ in the structure of the \u201cmachine code\u201d(the program graph) and the compilation algorithms necessary to produce it from a source language. This paper describes a new implementation method using super-combinators which is apparently more efficient than its predecessors. Consideration of the new method also helps clarify the relationships between several other graph-reduction schemes. This paper is necessarily brief, but a fuller account can be found in [Hughes].", "num_citations": "300\n", "authors": ["307"]}
{"title": "Projections for strictness analysis\n", "abstract": " Contexts have been proposed as a means of performing strictness analysis on non-flat domains. Roughly speaking, a context describes how much a sub-expression will be evaluated by the surrounding program. This paper shows how contexts can be represented using the notion of projection from domain theory. This is clearer than the previous explanation of contexts in terms of continuations. In addition, this paper describes finite domains of contexts over the non-flat list domain. This means that recursive context equations can be solved using standard fixpoint techniques, instead of the algebraic manipulation previously used.", "num_citations": "261\n", "authors": ["307"]}
{"title": "Testing telecoms software with Quviq QuickCheck\n", "abstract": " We present a case study in which a novel testing tool, Quviq QuickCheck, is used to test an industrial implementation of the Megaco protocol. We considered positive and negative testing and we used our developed specification to test an old version in order to estimate how useful QuickCheck could potentially be when used early in development. The results of the case study indicate that, by using Quviq QuickCheck, we would have been able to detect faults early in the development. We detected faults that had not been detected by other testing techniques. We found unclarities in the specifications and potential faults when the software is used in a different setting. The results are considered promising enough to Ericsson that they are investing in an even larger case study, this time from the beginning of the development of a new product.", "num_citations": "234\n", "authors": ["307"]}
{"title": "QuickCheck: a lightweight tool for random testing of Haskell programs\n", "abstract": " QuickCheck is a tool which aids the Haskell programmer in formulating and testing properties of programs. Properties are discribed as Haskell functions, and can be automatically tested on random input, but it is also possible to define custom test data generators. We present a number of case studies, in which the tool was successfully used, and also point out some pitfalls to avoid. Random testing is especially suitable for functional programs because properties can be stated at a fine grain. When a function is built from separately tested components, then random testing suffuces to obtain good coverage of the definition under test.", "num_citations": "223\n", "authors": ["307"]}
{"title": "A distributed garbage collection algorithm\n", "abstract": " In the future high performance will be achieved by using many processors in parallel. Such parallel machines will often be programmed in applicative or logic programming languages, both of which require a garbage collector. Even if imperative languages are used, sophisticated artificial intelligence applications, which rely on list processing, will require a garbage collector. The problem of garbage collection on a parallel computer is thus an important one.With present day technology, the cost of communication between parallel processors is much higher than the cost of communication within a processor. Even the Inmos transputer (not quite present-day technology at the time of writing) communicates with other transputers almost 25 times more slowly than it communicates with its own memory [13]. In the case of a single board computer connected to a fast local area network this ratio is even worse, This may\u00a0\u2026", "num_citations": "184\n", "authors": ["307"]}
{"title": "Lazy memo-functions\n", "abstract": " In this paper we introduce a slight variation on the old idea of a memo-function. We call our variant\" lazy memo-functions\" since they are suitable for use in systems with lazy evaluation. Lazy memo-functions are much more useful than the conventional kind. They can be used to define functions on cyclic structures in a natural way, and to allow modular versions of algorithms to be used when the only efficient alternative is to destroy the algorithm's structure. They can be implemented more efficiently than ordinary memo-functions, and can be used to define ordinary memofunctions if these are really required.Memo-functions were originally invented by Michie [14]. The idea behind them is very simple: a memo-function is like an ordinary function, but it remembers all the arguments it is applied to, together with the results computed from them. If it is ever re-applied to an argument the memo-function does not recompute\u00a0\u2026", "num_citations": "182\n", "authors": ["307"]}
{"title": "Recursion and dynamic data-structures in bounded space: Towards embedded ML programming\n", "abstract": " We present a functional language with a type system such that well typed programs run within stated space-bounds. The language is a strict, first-order variant of ML with constructs for explicit storage management. The type system is a variant of Tofte and Talpin's region inference system to which the notion of sized types, of Hughes, Pareto and Sabry, has been added.", "num_citations": "179\n", "authors": ["307"]}
{"title": "The design and implementation of programming languages\n", "abstract": " CiNii \u8ad6\u6587 - The Design and Implementation of Programming Languages CiNii \u56fd\u7acb\u60c5\u5831\u5b66 \u7814\u7a76\u6240 \u5b66\u8853\u60c5\u5831\u30ca\u30d3\u30b2\u30fc\u30bf[\u30b5\u30a4\u30cb\u30a3] \u65e5\u672c\u306e\u8ad6\u6587\u3092\u3055\u304c\u3059 \u5927\u5b66\u56f3\u66f8\u9928\u306e\u672c\u3092\u3055\u304c\u3059 \u65e5\u672c\u306e\u535a\u58eb\u8ad6\u6587\u3092 \u3055\u304c\u3059 \u65b0\u898f\u767b\u9332 \u30ed\u30b0\u30a4\u30f3 English \u691c\u7d22 \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u9589\u3058\u308b \u30bf\u30a4\u30c8\u30eb \u8457\u8005\u540d \u8457\u8005 ID \u8457\u8005\u6240\u5c5e \u520a\u884c\u7269\u540d ISSN \u5dfb\u53f7\u30da\u30fc\u30b8 \u51fa\u7248\u8005 \u53c2\u8003\u6587\u732e \u51fa\u7248\u5e74 \u5e74\u304b\u3089 \u5e74\u307e\u3067 \u691c\u7d22 \u691c\u7d22 \u691c\u7d22 CiNii\u306e\u30b5\u30fc\u30d3\u30b9\u306b\u95a2\u3059\u308b\u30a2\u30f3\u30b1\u30fc\u30c8\u3092\u5b9f\u65bd\u4e2d\u3067\u3059\uff0811/11(\u6c34)-12/23(\u6c34)\uff09 CiNii Research\u30d7\u30ec\u7248\u306e\u516c\u958b \u306b\u3064\u3044\u3066 The Design and Implementation of Programming Languages HUGHES RJM \u88ab\u5f15\u7528 \u6587\u732e: 1\u4ef6 \u8457\u8005 HUGHES RJM \u53ce\u9332\u520a\u884c\u7269 Ph. D. Thesis, Oxford University Ph. D. Thesis, Oxford University 130, 1983 \u88ab\u5f15\u7528\u6587\u732e: 1\u4ef6\u4e2d 1-1\u4ef6\u3092 \u8868\u793a 1 \u5b8c\u5168\u9045\u5ef6\u8a55\u4fa1\u306b\u9069\u3057\u305f\u95a2\u6570\u30d7\u30ed\u30b0\u30e9\u30e0\u306e \u5171\u6709\u89e3\u6790 \u91d1\u5b50 \u656c\u4e00 , \u5c3e\u4e0a \u80fd\u4e4b , \u6b66\u5e02 \u6b63\u4eba \u60c5\u5831\u51e6\u7406\u5b66\u4f1a\u8ad6\u6587\u8a8c 35(3), 391-403, 1994-03-15 \u53c2\u8003\u6587\u732e10\u4ef6 CiNii\u5229\u7528\u8005\u30a2\u30f3\u30b1\u30fc\u30c8 Tweet \u5404\u7a2e\u30b3\u30fc\u30c9 NII\u8ad6\u6587ID(NAID) 10024062964 \u306b/\u2026", "num_citations": "175\n", "authors": ["307"]}
{"title": "Monads and effects\n", "abstract": " A tension in language design has been between simple semantics on the one hand, and rich possibilities for side-effects, exception handling and so on on the other. The introduction of monads has made a large step towards reconciling these alternatives. First proposed by Moggi as a way of structuring semantic descriptions, they were adopted by Wadler to structure Haskell programs. Monads have been used to solve long-standing problems such as adding pointers and assignment, inter-language working, and exception handling to Haskell, without compromising its purely functional semantics. The course introduces monads, effects, and exemplifies their applications in programming (Haskell) and in compilation (MLj). The course presents typed metalanguages for monads and related categorical notions, and then describes how they can be further refined by introducing effects.", "num_citations": "153\n", "authors": ["307"]}
{"title": "QuickCheck testing for fun and profit\n", "abstract": " One of the nice things about purely functional languages is that functions often satisfy simple properties, and enjoy simple algebraic relationships. Indeed, if the functions of an API satisfy elegant laws, that in itself is a sign of a good design\u2014the laws not only indicate conceptual simplicity, but are useful in practice for simplifying programs that use the API, by equational reasoning or otherwise.", "num_citations": "133\n", "authors": ["307"]}
{"title": "Backwards analysis of functional programs\n", "abstract": " We begin with a summary of the main ideas behind abstract interpretation. Many of the same ideas emerge in backward analysts.Consider a first-order functional language, manipulating values in the domain D. An abstract interpretation is based on a domain of abstract values A, and a function abs from D to A. An abstract function f# is associated with every function f in the language. f# is intended to approximate f, in the sense that", "num_citations": "125\n", "authors": ["307"]}
{"title": "A novel representation of lists and its application to the function \u201creverse\u201d\n", "abstract": " A representation of lists as first-class functions is proposed. Lists represented in this way can be appended together in constant time, and can be converted back into ordinary lists in time proportional to their length. Programs which construct lists using append can often be improved by using this representation. For example, naive reverse can be made to run in linear time, and the conventional \u2018fast reverse\u2019 can then be derived easily. Examples are given in KRC (Turner, 1982), the notation being explained as it is introduced. The method can be compared to Sleep and Holmstr\u00f6m's proposal (1982) to achieve a similar effect by a change to the interpreter.", "num_citations": "125\n", "authors": ["307"]}
{"title": "Fast and loose reasoning is morally correct\n", "abstract": " Functional programmers often reason about programs as if they were written in a total language, expecting the results to carry over to non-total (partial) languages. We justify such reasoning.Two languages are defined, one total and one partial, with identical syntax. The semantics of the partial language includes partial and infinite values, and all types are lifted, including the function spaces. A partial equivalence relation (PER) is then defined, the domain of which is the total subset of the partial language. For types not containing function spaces the PER relates equal values, and functions are related if they map related values to related values.It is proved that if two closed terms have the same semantics in the total language, then they have related semantics in the partial language. It is also shown that the PER gives rise to a bicartesian closed category which can be used to reason about values in the domain of the\u00a0\u2026", "num_citations": "104\n", "authors": ["307"]}
{"title": "Programming with arrows\n", "abstract": " Consider this simple Haskell definition, of a function which counts the number of occurrences of a given word w in a string:                            count w = length . filter (==w) . words                          This is an example of \u201cpoint-free\u201d programming style, where we build a function by composing others, and make heavy use of higher-order functions such as filter. Point-free programming is rightly popular: used appropriately, it makes for concise and readable definitions, which are well suited to equational reasoning in the style of Bird and Meertens [2]. It\u2019s also a natural way to assemble programs from components, and closely related to connecting programs via pipes in the UNIX shell.", "num_citations": "79\n", "authors": ["307"]}
{"title": "Expressing and reasoning about non-deterministic functional programs\n", "abstract": " For some time functional language researchers have experimented with non-deterministic constructs for these otherwise deterministic languages. One strong motivation to do so is the desire to express necessarily non-deterministic programs, such as operating systems and interactive systems that service requests from several terminals in the order in which they are made. Another motivation is that some parallel algorithms are internally non-deterministic even though their result is not. Since functional languages are claimed to be well suited to parallel programming we certainly wish to be able to express such algorithms. Non-determinism can also be related to program refinement: a non-deterministic function may be considered to be a specification which is refined to a deterministic implementation. We will not address this third aspect here.", "num_citations": "72\n", "authors": ["307"]}
{"title": "Strictness detection in non-flat domains\n", "abstract": " A function f is said to be strict if f\u00b1=\u00b1, that is, if whenever the evaluation of its argument fails to terminate then so does its result. Knowledge about strictness is of great practical importance in the implementation of functional languages, because of the freedom it gives to change the evaluation order. For example, the argument of~ strict function can safely be evaluated before the call, avoiding the overhead normally associated with lazy evaluation. This is done in the Ponder compiler, where it appears to improve performance by a factor of between three and five 3. Alternatively, the arguments of strict functions can be evaluated in parallel with the call, leading to better exploitation of parallel hardware z.In order to enjoy these benefits it is necessary to be able to identify strict functions automatically. The classic paper on this subject is Mycroft's 5. Mycroft gives a method which can identify very many strict functions, but\u00a0\u2026", "num_citations": "70\n", "authors": ["307"]}
{"title": "Testing noninterference, quickly\n", "abstract": " Information-flow control mechanisms are difficult to design and labor intensive to prove correct. To reduce the time wasted on proof attempts doomed to fail due to broken definitions, we advocate modern random testing techniques for finding counterexamples during the design process. We show how to use QuickCheck, a property-based random-testing tool, to guide the design of a simple information-flow abstract machine. We find that both sophisticated strategies for generating well-distributed random programs and readily falsifiable formulations of noninterference properties are critically important. We propose several approaches and evaluate their effectiveness on a collection of injected bugs of varying subtlety. We also present an effective technique for shrinking large counterexamples to minimal, easily comprehensible ones. Taken together, our best methods enable us to quickly and automatically generate\u00a0\u2026", "num_citations": "64\n", "authors": ["307"]}
{"title": "Type specialisation for the \u03bb-calculus; or, a new paradigm for partial evaluation based on type inference\n", "abstract": " Partial evaluation is a powerful automated strategy for transforming programs, some of whose inputs are known. The classic simple example is the power function, powern x= if n= l then x else x\u2022 power (n-1) x which, given that n is known to be 3, can be transformed into the specialised version power 3 x= xx (x\u2022 x)", "num_citations": "62\n", "authors": ["307"]}
{"title": "Testing AUTOSAR software with QuickCheck\n", "abstract": " AUTOSAR (AUTomotive Open System ARchitecture) is an evolving standard for embedded software in vehicles, defined by the automotive industry, and implemented by many different vendors. On behalf of Volvo Cars, we have developed model-based acceptance tests for some critical AUTOSAR components, to guarantee that implementations from different vendors are compatible. We translated over 3000 pages of textual specifications into QuickCheck models, and tested many different implementations using large volumes of generated tests. This exposed over 200 issues, which we raised with Volvo and the software vendors. Compared to an earlier manual approach, ours is more efficient, more effective, and more correct.", "num_citations": "59\n", "authors": ["307"]}
{"title": "Restricted data types in Haskell\n", "abstract": " The implementations of abstract type constructors must often restrict the type parameters: for example, one implementation of sets may require equality on the element type, while another implementation requires an ordering. Haskell has no mechanism to abstract over such restrictions, which can hinder us from replacing one implementation by another, or making several implementations instances of the same class. This paper proposes a language extension called restricted data types to address this problem. A restricted data type de nition speci es a condition which argument types must satisfy for the data type to be well-formed. Every type in a program must be well-formed, and we add an explicit notation to express such requirements. Thus programmers can simply state that a type must be well-formed, rather than repeat its restriction explicitly.We explain our extension via a simulation using multi-parameter classes, which serves to specify its semantics. We show its application to the design of a collection class and to the class of monads, and we discuss extensions to compile-time context reduction needed to implement it.", "num_citations": "59\n", "authors": ["307"]}
{"title": "Testing erlang data types with quviq quickcheck\n", "abstract": " When creating software, data types are the basic bricks. Most of the time a programmer will use data types defined in library modules, therefore being tested by many users over many years. But sometimes, the appropriate data type is unavailable in the libraries and has to be constructed from scratch. In this way, new basic bricks are created, and potentially used in many products in the future. It pays off to test such data types thoroughly.", "num_citations": "52\n", "authors": ["307"]}
{"title": "A library for secure multi-threaded information flow in Haskell\n", "abstract": " Li and Zdancewic have recently proposed an approach to provide information-flow security via a library rather than producing a new language from the scratch. They have shown how to implement such a library in Haskell by using arrow combinators. However, their approach only works with computations that have no side-effects. In fact, they leave as an open question how their library, and the mechanisms in it, need to be modified to consider these kind of effects. Another absent feature in the library is support for multithreaded programs. Information-flow in multi-threaded programs still remains as a challenge, and no support for that has been implemented yet. It is not surprising, then, that the two main stream compilers that provide information-flow security, Jif and FlowCaml, lack support for multithreading. Following ideas taken from literature, this paper presents an extension to Li and Zdancewic's library that\u00a0\u2026", "num_citations": "52\n", "authors": ["307"]}
{"title": "Tag elimination and Jones-optimality\n", "abstract": " Tag elimination is a program transformation for removing unnecessary tagging and untagging operations from automatically generated programs. Tag elimination was recently proposed as having immediate applications in implementations of domain specific languages (where it can give a two-fold speedup), and may provide a solution to the long standing problem of Jones-optimal specialization in the typed setting. This paper explains in more detail the role of tag elimination in the implementation of domain-specific languages, presents a number of significant simplifications and a high-level, higher-order, typed self-applicable interpreter. We show how tag elimination achieves Jones-optimality.", "num_citations": "49\n", "authors": ["307"]}
{"title": "Compile-time analysis of functional programs\n", "abstract": " Compile-time analysis of functional programs | Research topics in functional programming ACM Digital Library home ACM home Google, Inc. (search) Advanced Search Browse About Sign in Register Advanced Search Journals Magazines Proceedings Books SIGs Conferences People More Search ACM Digital Library SearchSearch Advanced Search Browse Browse Digital Library Collections More HomeBrowse by TitleBooksResearch topics in functional programmingCompile-time analysis of functional programs chapter Compile-time analysis of functional programs Share on Author: John M Hughes profile image John Hughes View Profile Authors Info & Affiliations Publication: Research topics in functional programmingJune 1990 Pages 117\u2013153 4citation 0 Downloads Metrics Total Citations4 Total Downloads0 Last 12 Months0 Last 6 weeks0 Get Citation Alerts New Citation Alert added! This alert has been and : \u2026", "num_citations": "49\n", "authors": ["307"]}
{"title": "Verifying Haskell programs using constructive type theory\n", "abstract": " Proof assistants based on dependent type theory are closely related to functional programming languages, and so it is tempting to use them to prove the correctness of functional programs. In this paper, we show how Agda, such a proof assistant, can be used to prove theorems about Haskell programs. Haskell programs are translated into an Agda model of their semantics, by translating via GHC's Core language into a monadic form specially adapted to represent Haskell's polymorphism in Agda's predicative type system. The translation can support reasoning about either total values only, or total and partial values, by instantiating the monad appropriately. We claim that, although these Agda models are generated by a relatively complex translation process, proofs about them are simple and natural, and we offer a number of examples to support this claim.", "num_citations": "47\n", "authors": ["307"]}
{"title": "Avoiding unnecessary updates\n", "abstract": " Graph reduction underlies most implementations of lazy functional languages, allowing separate computations to share results when subterms are evaluated. Once a term is evaluated, the node of the graph representing the computation is updated with the value of the term. However, in many cases, no other computation requires this value, so the update is unnecessary. In this paper we take some steps towards an analysis for determining when these updates may be omitted.", "num_citations": "42\n", "authors": ["307"]}
{"title": "Functional Programming Languages and Computer Architecture: 5th ACM Conference. Cambridge, MA, USA, August 26-30, 1991 Proceedings\n", "abstract": " This book offers a comprehensive view of the best and the latest work in functional programming. It is the proceedings of a major international conference and contains 30 papers selected from 126 submitted. A number of themes emerge. One is a growing interest in types: powerful type systems or type checkers supporting overloading, coercion, dynamic types, and incremental inference; linear types to optimize storage, and polymorphic types to optimize semantic analysis. The hot topic of partial evaluation is well represented: techniques for higher-order binding-time analysis, assuring termination of partial evaluation, and improving the residual programs a partial evaluator generates. The thorny problem of manipulating state in functional languages is addressed: one paper even argues that parallel programs with side-effects can be\" more declarative\" than purely functional ones. Theoretical work covers a new model of types based on projections, parametricity, a connection between strictness analysis and logic, and a discussion of efficient implementations of the lambda-calculus. The connection with computer architecture and a variety of other topics are also addressed.", "num_citations": "41\n", "authors": ["307"]}
{"title": "Beginner's luck: a language for property-based generators\n", "abstract": " Property-based random testing \u00e0 la QuickCheck requires building efficient generators for well-distributed random data satisfying complex logical predicates, but writing these generators can be difficult and error prone. We propose a domain-specific language in which generators are conveniently expressed by decorating predicates with lightweight annotations to control both the distribution of generated values and the amount of constraint solving that happens before each variable is instantiated. This language, called Luck, makes generators easier to write, read, and maintain.", "num_citations": "37\n", "authors": ["307"]}
{"title": "How functional programming mattered\n", "abstract": " In 1989 when functional programming was still considered a niche topic, Hughes wrote a visionary paper arguing convincingly \u2018why functional programming matters\u2019. More than two decades have passed. Has functional programming really mattered? Our answer is a resounding \u2018Yes!\u2019. Functional programming is now at the forefront of a new generation of programming technologies, and enjoying increasing popularity and influence. In this paper, we review the impact of functional programming, focusing on how it has changed the way we may construct programs, the way we may verify programs, and fundamentally the way we may think about programs.", "num_citations": "34\n", "authors": ["307"]}
{"title": "Mysteries of Dropbox:  Property-Based Testing of a Distributed Synchronization Service\n", "abstract": " File synchronization services such as Dropbox are used by hundreds ofmillions of people to replicate vital data. Yet rigorous models of theirbehavior are lacking. We present the first formal -- and testable -- model ofthe core behavior of a modern file synchronizer, and we use it to discoversurprising behavior in two widely deployed synchronizers. Our model isbased on a technique for testing nondeterministic systems that avoidsrequiring that the system's internal choices be made visible to the testing framework.", "num_citations": "32\n", "authors": ["307"]}
{"title": "Making choices lazily\n", "abstract": " We present a natural semantics that models the untyped, normal order A-calculus plus McCarthy\u2019s amb in the context of call-by-need parameter passing. This results in a singular semantics for amb. Previous work on singular choice has concentrated on erratic choice, a less interesting nondeterministic choice operator, and only in relation to callby-value parameter passing, or call-by-name restricted to deterministic terms.The natural semantics contains rules for both convergent and divergent behaviour, allowing it to distinguish programs that differ only in their divergent behaviour. As aresult, it is more discriminating than current domain-theoretic models. This, and the fact that it models singular amb, makes the natural semantics suitable for reasoning about lazy, functional languages containing McCarthy\u2019s amb.", "num_citations": "32\n", "authors": ["307"]}
{"title": "Experiences with QuickCheck: testing the hard stuff and staying sane\n", "abstract": " This is not a typical scientific paper. It does not present a new method, with careful experiments to evaluate it, and detailed references to related work. Rather, it recounts some of my experiences over the last 15 years, working with QuickCheck, and its purpose is as much to entertain as to inform.               QuickCheck is a random testing tool that Koen Claessen and I invented, which has since become the testing tool of choice in the Haskell community. In 2006 I co-founded Quviq, to develop and market an Erlang version, which we have since applied for a wide variety of customers, encountering many fascinating testing problems as a result.               This paper introduces Quviq QuickCheck, and in particular the extensions made for testing stateful code, via a toy example in C. It goes on to describe the largest QuickCheck project to date, which developed acceptance tests for AUTOSAR C code on behalf of\u00a0\u2026", "num_citations": "29\n", "authors": ["307"]}
{"title": "How to give a good research talk\n", "abstract": " Giving a good research talk is not easy. We try to identify some things which we have found helpful, in the hope that they may be useful to you.", "num_citations": "29\n", "authors": ["307"]}
{"title": "Software testing with quickcheck\n", "abstract": " This paper presents a tutorial, with extensive exercises, in the use of Quviq QuickCheck\u2014a property-based testing tool for Erlang, which enables developers to formulate formal specifications of their code and to use them for testing. We cover the basic concepts of properties and test-data generators, properties for testing abstract data types, and a state-machine modelling approach to testing stateful systems. Finally we discuss applications of QuickCheck in industry.", "num_citations": "28\n", "authors": ["307"]}
{"title": "Type specialisation for imperative languages\n", "abstract": " We extend type specialisation to a computational lambda calculus with first-class references. The resulting specialiser has been used to specialise a self-interpreter for this typed computational lambda calculus optimally. Furthermore, this specialiser can perform operations on references at specialisation time, when possible.", "num_citations": "28\n", "authors": ["307"]}
{"title": "Polish parsers, step by step\n", "abstract": " We present the derivation of a space efficient parser combinator library: the constructed parsers do not keep unnecessary references to the input, produce online results and efficiently handle ambiguous grammars. The underlying techniques can be applied in many contexts where traditionally backtracking is used. We present two data types, one for keeping track of the progress of the search process, and one for representing the final result in a linear way. Once these data types are combined into a single type, we can perform a breadth-first search, while returning parts of the result as early as possible.", "num_citations": "27\n", "authors": ["307"]}
{"title": "Haskell++: An object-oriented extension of Haskell\n", "abstract": " Lazy functional languages such as Haskell Hud92] provide excellent support for writing re-useable code. Polymorphism, higher-order functions, and lazy evaluation are all key contributing features: instead of writing many sort functions at di erent types, we re-use one; instead of writing many functions which recurse over lists, we capture common recursion patterns as higher-order functions map, foldr and so on, and just re-use them; instead of writing many loops that iterate n times, we write loops producing in nite lists and re-use take n to count the iterations.This re-useability is re ected, for example, in the very heavy use that Haskell programs make of functions from the standard prelude. Indeed, we have argued elsewhere Hug89] that these features are largely responsible for the improved productivity that functional programming o ers. Haskell's overloading system WB89] also contributes to code re-useability. For example, most numeric functions in Haskell programs can be re-used with any implementation of numbers. Although in this case overloading can be regarded as syntactic sugar for parameterising numeric functions on the implementations of the arithmetic operations, the sugar is important because it makes re-useable code easy to write. Most programmers would probably regard passing the operations explicitly as unacceptably clumsy, and therefore wouldn't do it. The result: less re-useable code.", "num_citations": "26\n", "authors": ["307"]}
{"title": "Reversing abstract interpretations\n", "abstract": " Program analyses are often presented as one of two brands: forwards or backwards. In this paper we explore the significance of the direction of analysis, and show how arbitrary abstract interpretations may be reversed.", "num_citations": "25\n", "authors": ["307"]}
{"title": "FlexSoC: Combining flexibility and efficiency in SoC designs\n", "abstract": " The FlexSoC project aims at developing a design framework that makes it possible to combine the computational speed and energy-efficiency of specialized hardware accelerators with the flexibility of programmable processors. FlexSoC approaches this problem by defining a uniform programming interface across the heterogeneous structure of processing resources. This paper justifies our approach and also discusses the central research issues we will focus on in the areas of VLSI design, computer architecture, and programming and verification.", "num_citations": "24\n", "authors": ["307"]}
{"title": "Module-sensitive program specialisation\n", "abstract": " We present an approach for specialising large programs, such as programs consisting of several modules, or libraries. This approach is based on the idea of using a compiler generator (cogen) for creating generating extensions. Generating extensions are specialisers specialised with respect to some input program. When run on some input data the generating extension produces a specialised version of the input program. Here we use the cogen to tailor modules for specialisation. This happens once and for all, independently of all other modules. The resulting module can then be used as a building block for generating extensions for complete programs, in much the same way as the original modules can be put together into complete programs. The result of running the final generating extension is a collection of residual modules, with a module structure derived from the original program.", "num_citations": "23\n", "authors": ["307"]}
{"title": "Reversing abstract interpretations\n", "abstract": " Many semantic analyses of functional languages have been developed using the Cousots' abstract interpretation fralnework [CC77]. Some, such as Mycroft's pioneering strictness analysis [MycS1] and Burn, Hankin and Abramsky's extension of it to higher= m~ ler [BHA86], operate on abstract values representing the past history of the computation, and are therefore called forwards analyses. Others, such as W~ Uer and Hughes' projection-based strictness analysis [WH87], or Hall's analysis of strictness patterns [Hal87] propagate abstract contexts representing the future of the computation, and are called backwards analyses. However, although the type of abstract information may suggest a\" natural\" direction, it is in fact possible to perform any analysis in either direction. The goal of this paper is to show how to reverse any given analysis.Why might one prefer one direction of analysis over another? We shall draw an\u00a0\u2026", "num_citations": "23\n", "authors": ["307"]}
{"title": "Towards binding-time improvement for free\n", "abstract": " We show how application of commutative-like laws can improve binding time properties. The quality of residual programs obtained by partial evaluation depends crucially on the binding time properties of the source program. Likewise does the sharing obtained by full laziness depend on the binding time properties of the program. This paper suggest that the commutative-like laws, derivable from the types of polymorphic functions using the \u201cfree theorems\u201d approach, gives rise to a large set of non-trivial binding time improving transformations. The paper runs through a number of examples and shows how these laws can be applied in a rather systematic way to achieve binding time improvements. Ultimately it is our hope to automate the derivation and application of these binding time improving transformations.", "num_citations": "23\n", "authors": ["307"]}
{"title": "Projections for polymorphic strictness analysis\n", "abstract": " We apply the categorical properties of polymorphic functions to compile-time analysis, specifically projection-based strictness analysis. First we interpret parameterised types as functors in a suitable category, and show that they preserve monics and epics. Then we define \u201cstrong\u201d and \u201cweak\u201d polymorphism \u2014 the latter admitting certain projections that are not polymorphic in the usual sense. We prove that, under the right conditions, a weakly polymorphic function is characterised by a single instance. It follows that the strictness analysis of one simple instance of a polymorphic function yields results that apply to all. We show how this theory may be applied.             In comparison to earlier polymorphic strictness analysis methods, ours can apply polymorphic information to a particular instance very simply. The categorical approach simplifies our proofs, enabling them to be carried out at a higher level, and making\u00a0\u2026", "num_citations": "23\n", "authors": ["307"]}
{"title": "Specification based testing with QuickCheck\n", "abstract": " Specification Based Testing with QuickCheck Page 1 Specification Based Testing with QuickCheck John Hughes Chalmers University/Quviq AB Page 2 Page 3 What is QuickCheck? \u2022 A library for writing and testing properties of program code \u2022 Some code: \u2022 A property: Page 4 Properties as Code A quantifier! A set! A predicate! A booleanvalued expression! A macro! An ordinary function definition! A test data generator! Page 5 DEMO Page 6 QuickCheck in a Nutshell Properties Test case Test case Test case Test case Test case Minimal Test case Page 7 QuickCheck Properties: things with a counterexample <bool-exp> ?FORALL(<var>,<generator>,<property>) ?IMPLIES(<bool-exp>,<property>) conjunction, disjunction ?EXISTS(<var>,<generator>,<property>) Page 8 QuickCheck Generators int(), bool(), real()\u2026 choose(<int>,<int>) {<generator>,<generator>\u2026} oneof(<list-of-generators>) ?LET(<var>,<generator>,<>) \u2026", "num_citations": "22\n", "authors": ["307"]}
{"title": "Implementing functional databases\n", "abstract": " Implementing functional databases | Advances in database programming languages ACM Digital Library home ACM home Google, Inc. (search) Advanced Search Browse About Sign in Register Advanced Search Journals Magazines Proceedings Books SIGs Conferences People More Search ACM Digital Library SearchSearch Advanced Search Browse Browse Digital Library Collections More HomeBrowse by TitleBooksAdvances in database programming languagesImplementing functional databases chapter Implementing functional databases Share on Authors: Guy Argo View Profile , John Hughes View Profile , Philip Trinder View Profile , Jon Fairbairn View Profile , John Launchbury View Profile Authors Info & Affiliations Advances in database programming languagesNovember 1990 Pages 165\u2013176https://doi.org/10.1145/101620.101630 Published:01 November 1990 0citation 0 Downloads Metrics Total 0 0 \u2026", "num_citations": "22\n", "authors": ["307"]}
{"title": "The alpha-beta algorithm: An exercise in program transformation\n", "abstract": " In this paper we use program transformation [1, 3] to develop the'alpha-beta'pruning algorithm from a specification of minimaxing [5]. The pruning algorithm is nontrivial, and yet the transformation turns out to be relatively straightforward. We regard the exercise as providing yet more evidence of the importance of transformational techniques, both for producing efficient programs and explaining them.", "num_citations": "21\n", "authors": ["307"]}
{"title": "Find more bugs with QuickCheck!\n", "abstract": " Random testing is increasingly popular and successful, but tends to spend most time rediscovering the \"most probable bugs\" again and again, reducing the value of long test runs on buggy software. We present a new automated method to adapt random test case generation so that already-discovered bugs are avoided, and further test effort can be devoted to searching for new bugs instead. We evaluate our method primarily against RANDOOP-style testing, in three different settings - our method avoids rediscovering bugs more successfully than RANDOOP and in some cases finds bugs that RANDOOP did not find at all.", "num_citations": "20\n", "authors": ["307"]}
{"title": "Fast abstract interpretation using sequential algorithms\n", "abstract": " Abstract interpretation in the framework introduced by Cousot and Cousot requires finding fixpoints of continuous functions between abstract lattices [CC77]. Very often these abstract functions are expressed as typed,~-expressions, even when the language being analysed isn't functional--for example an abstract interpretation derived from a denotational semantics in the style of Nielson [Nie82] will naturally be in this form. So implementing an abstract interpreter often requires an evaluator for, k-expressions over lattices. Of course, evaluation must always terminate--even when the result is the bottom element of the lattice concerned. In practice this evaluation is intractable. The problem is caused by function types: lattices of functions grow more than exponentially in the size of their type. A-expressions of quite simple types therefore denote functions that are too enormous to manipulate efficiently. Attempts to find\u00a0\u2026", "num_citations": "20\n", "authors": ["307"]}
{"title": "Global variables in Haskell\n", "abstract": " Haskell today provides good support not only for a functional programming style, but also for an imperative one. Elements of imperative programming are needed in applications such as web servers, or to provide efficient implementations of well-known algorithms, such as many graph algorithms. However, one element of imperative programming, the global variable, is surprisingly hard to emulate in Haskell. We discuss several existing methods, none of which is really satisfactory, and finally propose a new approach based on implicit parameters. This approach is simple, safe, and efficient, although it does reveal weaknesses in Haskell's present type system.", "num_citations": "18\n", "authors": ["307"]}
{"title": "Type specialization\n", "abstract": " Type specialisation is an approach to offline program specialisation in which static information is derived by a kind of type inference rather than by evaluation or reduction. Type specialisers have been implemented for an extended-calculus and for a subset of Haskell; their advantage is that they can achieve a better flow of static information than a partial evaluator can. For example, consider the following term:(lift (@ 3))@(\u043c \u043c+ 1)Source terms are two-level typed-expressions; we write application as'@'and as usual we underline dynamic operations. In this example all functions and applications are dynamic, while the arithmetic is static, and the result is converted to a dynamic integer by lift. With these annotations, this two-level term cannot be specialised by a partial evaluator, because the \u040c-redexes are marked dynamic and so must not be reduced. In particular@ 3 cannot be reduced to a constant, and so cannot be\u00a0\u2026", "num_citations": "17\n", "authors": ["307"]}
{"title": "An introduction to program specialisation by type inference\n", "abstract": " In this paper we present a new paradigm for partial evaluation, based not on transforming terms into terms, but on transforming terms and types into terms and types. An immediate advantage is that residual programs need not involve the same types as source programs, so type specialisation can be accomplished naturally. Furthermore, while a conventional partial evaluator handles a static expression by reducing it to a constant residual term, we carry the static information in the residual type, which leads to improved static information flow and thereby better specialisations. Thanks to this, optimal specialisation of a typed interpreter is possible. Our partial evaluator is specified in a very modular fashion: each feature is associated with a type in the source language, and its specialisation is specified via corresponding introduction and elimination rules. As a result, specialisation features can be freely combined: for example, we have for the first time combined constructor specialisation with higher-order functions, deriving a firstification transformation and a closure analyses as a consequence. This paper is an introduction to the material in [Hug96], which appeared in the Proceedings of the Dagstuhl Workshop on Partial Evaluation, 1996 (Springer LNCS).", "num_citations": "17\n", "authors": ["307"]}
{"title": "Towards relating forwards and backwards analyses\n", "abstract": " In this paper we take steps towards a unified framework in which both forwards and backwards analyses may be discussed. We present natural deduction style rules, instances of which may be used to define analyses in either direction. Some insights resulting from the approach are drawn out, together with a conjecture that non-relational forwards analysis is unable to discover head-strictness.", "num_citations": "16\n", "authors": ["307"]}
{"title": "Automatic grading of programming exercises using property-based testing\n", "abstract": " We present a framework for automatic grading of programming exercises using property-based testing, a form of model-based black-box testing. Models are developed to assess both the functional behaviour of programs and their algorithmic complexity. From the functional correctness model a large number of test cases are derived automatically. Executing them on the body of exercises gives rise to a (partial) ranking of programs, so that a program A is ranked higher than program B if it fails a strict subset of the test cases failed by B. The model for algorithmic complexity is used to compute worst-case complexity bounds. The framework moreover considers code structural metrics, such as McCabe's cyclomatic complexity, giving rise to a composite program grade that includes both functional, non-functional, and code structural aspects. The framework is evaluated in a course teaching algorithms and data structures\u00a0\u2026", "num_citations": "15\n", "authors": ["307"]}
{"title": "A type specialisation tutorial\n", "abstract": " The essence of partial evaluation is beautifully simple: we just take a program, together with values of some of its inputs; we perform the operations that depend only on known inputs, build a new program from the other operations, and finally obtain a residual program which solves the same problem as the original for a subclass of the cases. Work by Neil Jones and his group over the past decade and a half has demonstrated just how powerful this simple idea really is.", "num_citations": "15\n", "authors": ["307"]}
{"title": "Using temporal relations to specify and test an instant messaging server\n", "abstract": " Asynchronous events are awkward to handle in specification-based testing. State machine specifications become very complex when variable event order, timing constraints, and timing uncertainties must all be captured. We propose an alternative formalism for specifying asynchronous behaviour based on temporal relations, designed to support more declarative and modular specifications. Temporal relations are in a sense a combination of bulk data types and temporal logic. We illustrate the formalism by specifying parts of a simplified instant messaging server, and show that it can handle timing uncertainty very simply. We have implemented the formalism as part of Quviq QuickCheck, a commercial specification-based testing tool, and we describe its application to testing ejabberd, the leading instant messaging server based on the open XMPP protocol.", "num_citations": "14\n", "authors": ["307"]}
{"title": "Branching processes for quickcheck generators\n", "abstract": " In QuickCheck (or, more generally, random testing), it is challenging to control random data generators' distributions---specially when it comes to user-defined algebraic data types (ADT). In this paper, we adapt results from an area of mathematics known as branching processes, and show how they help to analytically predict (at compile-time) the expected number of generated constructors, even in the presence of mutually recursive or composite ADTs. Using our probabilistic formulas, we design heuristics capable of automatically adjusting probabilities in order to synthesize generators which distributions are aligned with users' demands. We provide a Haskell implementation of our mechanism in a tool called DRaGeN and perform case studies with real-world applications. When generating random values, our synthesized QuickCheck generators show improvements in code coverage when compared with those\u00a0\u2026", "num_citations": "13\n", "authors": ["307"]}
{"title": "An expressive semantics of mocking\n", "abstract": " We present a semantics of mocking, based on a process calculus-like formalism, and an associated mocking framework. We can build expressive mocking specifications from a small, orthogonal set of operators. Our framework detects and rejects ambiguous specifications as a validation measure. We report our experience testing software components for the car industry, which needed the full power of our framework.", "num_citations": "12\n", "authors": ["307"]}
{"title": "The correctness of type specialisation\n", "abstract": " Type specialisation, like partial evaluation, is an approach to specialising programs. But type specialisation works in a very different way, using a form of type inference. Previous articles have described the method and demonstrated its power as a program transformation, but its correctness has not previously been addressed. Indeed, it is not even clear what correctness should mean: type specialisation transforms programs to others with different types, so clearly cannot preserve semantics in the usual sense.               In this paper we explain why finding a correctness proof was difficult, we motivate a correctness condition, and we prove that type specialisation satisfies it. Perhaps unsurprisingly, type-theoretic methods turned out to crack the nut", "num_citations": "12\n", "authors": ["307"]}
{"title": "Implementing projection-based strictness analysis\n", "abstract": " Projection-based backwards strictness analysis has been understood for some years. Surprisingly, even though the method is fairly simple and quite general, no reports of its implementation have appeared. This paper describes ideas underlying our prototype implementation of the analysis for a simple programming language. The implementation serves as a case study before applying the method in the Glasgow Haskell compiler.", "num_citations": "12\n", "authors": ["307"]}
{"title": "Nondeterministic functional programming with sets\n", "abstract": " Nondeterminism can be introduced into a functional language, along with a set of laws for reasoning about the behaviour of programs, without disturbing referential transparency. We show how to do this by adding a new type constructor for sets and a carefully selected family of operations on sets. Instead of specifying a nondeterministic choice explicitly with choose or amb, a programmer specifies the set of values which the program might compute. Operations on sets are restricted in order to maintain laws for reasoning about programs; in particular, no function can choose an element from a set. The implementation is specified via rewrite rules that transform a program in the nondeterministic language into an ordinary functional program augmented with amb (which is not directly accessible to the programmer). The denotational semantics for this language is based on the Hoare powerdomain, so it includes\u00a0\u2026", "num_citations": "12\n", "authors": ["307"]}
{"title": "Partial evaluation and separate compilation\n", "abstract": " Hitherto all partial evaluators have processed a complete program to produce a complete residual program. We are interested in treating programs as collections of modules which can be processed independently:'separate partial evaluation', so to speak. In this paper we still assume that the original program is processed in its entirety, but we show how to specialise it to the static data bit-by-bit, generating a different module for each bit. When the program to be specialised is an interpreter, this corresponds to specialising it to one module of its object language at a time: each module of the object language gives rise to one module of the residual program.", "num_citations": "11\n", "authors": ["307"]}
{"title": "Binding-time analysis for polymorphic types\n", "abstract": " Offline partial evaluators specialise programs with annotations which distinguish specialisation-time (or static) computations from run-time ones. These annotations are generated by a binding-time analyser, via type inference in a suitable type-system. Henglein and Mossin developed a type system which allows polymorphism in binding-times, so that the same code can be specialised with different computations being static at different uses. We extend their work to permit polymorphism in types as well. This is particularly important for separately compiled libraries.               Following Henglein and Mossin, binding-times are passed as parameters during specialisation, but types are not. Instead, we pass coercion functions when necessary, which makes specialisation less \u201cinterpretive\u201d than it would otherwise be. We keep track of the coercions needed by assigning qualified types to polymorphic functions\u00a0\u2026", "num_citations": "10\n", "authors": ["307"]}
{"title": "Projections for polymorphic first-order strictness analysis\n", "abstract": " We apply the categorical properties of polymorphic functions to compile-time analysis, specifically projection-based strictness analysis. First we interpret parameterised types as functors in a suitable category, and show that they preserve monics and epics. Then we define \u201cstrong\u201d and \u201cweak\u201d polymorphism, the latter admitting certain projections that are not polymorphic in the usual sense. We prove that, under the right conditions, a weakly polymorphic function is characterised by a single instance. It follows that the strictness analysis of one simple instance of a polymorphic function yields results that apply to all. We show how this theory may be applied. In comparison with earlier polymorphic strictness analysis methods, ours can apply polymorphic information to a particular instance very simply. The categorical approach simplifies our proofs, enabling them to be carried out at a higher level, and making them\u00a0\u2026", "num_citations": "10\n", "authors": ["307"]}
{"title": "Haskell 98\n", "abstract": " \\Some half dozen persons have written technically on combinatory logic, and most of these, including ourselves, have published something erroneous. Since some of our fellow sinners are among the most careful and competent logicians on the contemporary scene, we regard this as evidence that the subject is refractory. Thus fullness of exposition is necessary for accuracy; and excessive condensation would be false economy here, even more than it is ordinarily.\"", "num_citations": "8\n", "authors": ["307"]}
{"title": "An iterative powerdomain construction\n", "abstract": " A finite domain for the abstract interpretation of lazy lists is known, but is not easily generalisable to other lazy data structures. A construction for finite abstract domains is presented which is quite general, based on the notion of a \u2018set of elements\u2019. The abstraction for elements is given, and a new powerdomain is developed. Finally, a means of iterative calculation of the sub-domain which contains all the \u2018useful\u2019 points is arrived at.", "num_citations": "8\n", "authors": ["307"]}
{"title": "Linking unit tests and properties\n", "abstract": " QuickCheck allows us to verify software against particular properties. A property can be regarded as an abstraction over many unit tests. QuickCheck uses generated random input data to test such properties. If a counterexample is found, it becomes immediately clear what we have tested. This is not the case when all tests pass, since we do not (and shall not) see the actual generated test cases. How can we be sure about what is tested? QuickCheck has the ability to gather statistics about the test cases, which is insightful. But still it does not tell us whether the particular unit test scenarios we have in mind are included. For this reason, we have developed a tool that can answer this question. It checks if a given unit test can be generated by a property, making it easier to judge the property's quality. We have applied our tool to an industrial use case of testing the AUTOSAR basic software modules and shows that it can\u00a0\u2026", "num_citations": "7\n", "authors": ["307"]}
{"title": "Principal type specialisation\n", "abstract": " Type specialisation is an approach to program specialisation that works with both a program and its type to produce specialised versions of each. As it combines many powerful features, it appears to be a good framework for automatic program production,--despite the fact that it was designed originally to express optimal specialisation for interpreters written in typed languages. The original specification of type specialisation used a system of rules expressing it as a generalised type system, rather than the usual view of specialisation as generalised evaluation. That system, while powerful, has some weaknesses not widely recognized--the most important being the inability to express principal type specialisations (a principal specialisation is one that is\" more general\" than any other for a given specialisable term, and from which those can be obtained by a suitable notion of instantiation). This inability is a problem\u00a0\u2026", "num_citations": "7\n", "authors": ["307"]}
{"title": "Combining verification methods in software development\n", "abstract": " The goal of this programme is to develop methods for improving software quality. The approach is to integrate a variety of veri cation methods into a framework which permits a smooth progression from hacking code to fully formal proofs of correctness. By a pragmatic integration of di erent techniques in a next-generation program development tool, we hope to handle systems on a much larger scale than hitherto.This proposal builds on and combines our extensive and internationally well-known research in interactive theorem provers, formal methods, program analysis and transformation, and automatic testing. Our long experience with functional languages, which we use both as implementation tools and a test-bed, improves our chances of success in tackling these di cult problems.", "num_citations": "7\n", "authors": ["307"]}
{"title": "Extending a partial evaluator which supports separate compilation\n", "abstract": " Hitherto all partial evaluators have processed a complete program to produce a complete residual program. We are interested in treating programs as collections of modules which can be processed independently: \u2018separate partial evaluation\u2019, so to speak. In this paper we still assume that the original program is processed in its entirety, but we show how to specialise it to the static data bit-by-bit, generating a different module for each bit. When the program to be specialised is an interpreter, this corresponds to specialising it to one module of its object language at a time: each module of the object language gives rise to one module of the residual program.", "num_citations": "7\n", "authors": ["307"]}
{"title": "A loop-detecting interpreter for lazy, higher-order programs\n", "abstract": " Interpreters that detect some forms of non-termination have a variety of applications, from abstract interpretation to partial evaluation. A simple and often used strategy is to test for a repeated state, but this cannot handle infinite values (such as first-class functions) or unevaluated states such as arise in lazy programs. In this paper we propose using Berry and Curien\u2019s theory of sequential algorithms as a semantic foundation for loop detection: we obtain straightforwardly a loop detector for lazy higher-order functional programs, which is more effective than the simple strategy even for strict first order cases.", "num_citations": "7\n", "authors": ["307"]}
{"title": "A semantics for locally bottom-avoiding choice\n", "abstract": " We present a small step reduction semantics for the untyped lazy lambda calculus plus a non-deterministic choice. The addition of a fair scheduler enables the choice operator to avoid divergence, and so the semantics implements locally bottom-avoiding choice and not erratic choice. A big step semantics that is sound and complete with respect to the small step semantics is also presented. It includes inference rules for both convergent and divergent behaviour. The operational equivalence that is derived from the big step semantics is much more discriminating than that arising in current denotational models. The semantics is suitable as a basis for reasoning about non-deterministic functional programs.", "num_citations": "6\n", "authors": ["307"]}
{"title": "Modelling of Autosar libraries for large scale testing\n", "abstract": " We demonstrate a specific method and technology for model-based testing of large software projects with the QuickCheck tool using property-based specifications. Our specifications are very precise, state-full models of the software under test (SUT). In our approach we define (a) formal descriptions of valid function call sequences (public API), (b) postconditions that check the validity of each call, and (c) call-out specifications that define and validate external system interactions (SUT calling external API). The QuickCheck tool automatically generates and executes tests from these specifications. Commercially, this method and tool have been used to test large parts of the industrially developed automotive libraries based on the Autosar standard. In this paper, we exemplify our approach with a circular buffer specified by Autosar, to demonstrate the capabilities of the model-based testing method of QuickCheck. Our example is small compared to the commercial QuickCheck models, but faithfully addresses many of the same challenges.", "num_citations": "5\n", "authors": ["307"]}
{"title": "Experiences from teaching functional programming at Chalmers\n", "abstract": " This note recounts my experiences of teaching functional programming at Chalmers University in Gothenburg, the successes and the problems I encountered. A functional language has been used for the eight week introductory programming course at Chalmers since the early 1990s, initially using ML. I took over the course in the late 1990s, in connection with a switch to Haskell, and taught it every subsequent year until recently.In the late eighties and early nineties, many functional programming enthusiasts pushed to teach the subject in the first programming course, believing it to be a good vehicle for conveying basic programming concepts\u2014and perhaps also that students would be\u201d spoiled\u201d if they learned imperative programming first. However, there are problems associated with doing this: it does not meet student expectations; many students have nothing to compare it with, and so cannot appreciate the\u00a0\u2026", "num_citations": "5\n", "authors": ["307"]}
{"title": "Relational reversal of abstract interpretation\n", "abstract": " Many semantic analyses of functional languages have been developed using the Cousots\u2019 abstract interpretation framework. Some operate on abstract values representing the past history of the computation, and are therefore called forwards analyses. Others propagate abstract contexts representing the future of the computation, and are called backwards analyses. Each form of analysis brings its own insights, and has the potential to influence the other form. For example, it may be very easy to see how to analyse a particular programming language construct in one direction, but not in the direction needed for a particular analysis. Potentially, one might be able to draw on the given analysis to aid in the Resign of a corresponding reversal. This is the topic of this paper. We show how to reverse any given analysis (forwards or backwards), obtaining a relational reversal which is equivalent to the original. This\u00a0\u2026", "num_citations": "4\n", "authors": ["307"]}
{"title": "How to Specify it!\n", "abstract": " Property-based testing tools test software against a specification, rather than a set of examples. This tutorial paper presents five generic approaches to writing such specifications (for purely functional code). We discuss the costs, benefits, and bug-finding power of each approach, with reference to a simple example with eight buggy variants. The lessons learned should help the reader to develop effective property-based tests in the future.", "num_citations": "3\n", "authors": ["307"]}
{"title": "Typing the wild in Erlang\n", "abstract": " Developing a static type system suitable for Erlang has been of ongoing interest for almost two decades now. The challenge with retrofitting a static type system onto a dynamically typed language, such as Erlang, is the loss of flexibility in programming offered by the language. In light of this, many attempts to type Erlang trade sound type checking for the ability to retain flexibility. Hence, simple type errors which would be caught by the type checker of a statically typed language are easily missed in these developments. This has us wishing for a way to avoid such errors in Erlang programs.", "num_citations": "3\n", "authors": ["307"]}
{"title": "How well are your requirements tested?\n", "abstract": " We address the question: to what extent does covering requirements ensure that a test suite is effective at revealing faults? To answer it, we generate minimal test suites that coverall requirements, and assess the tests they contain. They turn out to be very poor -- ultimately because the notion of covering a requirement is more subtle than it appears to be at first. We propose several improvements to requirements tracking during testing, which enable us to generate minimal test suites close to what a human developer would write. However, there remains a class of plausible bugs which such suites are very poor at finding, but which random testing finds rather easily.", "num_citations": "3\n", "authors": ["307"]}
{"title": "Accelerating race condition detection through procrastination\n", "abstract": " Race conditions are notoriously frustrating to find, and good tools can help. The main difficulty is reliably provoking the race condition. In previous work we presented a randomising scheduler for Erlang that helps with this task.", "num_citations": "3\n", "authors": ["307"]}
{"title": "Abstract Interpretation of Higher Order Functions Using Concrete Data Structures (Summary)\n", "abstract": " An implementation of abstract interpretation is outlined, which uses techniques taken from work on semantics of sequential languages. For a familiar example, analysis is much faster than with the frontiers method.", "num_citations": "3\n", "authors": ["307"]}
{"title": "A loop-detecting interpreter for lazy programs\n", "abstract": " This paper develops a loop detecting interpreter for a lazy language. The loop detection scheme is a further development of what takes place in memoising interpreters and in pending analysis. It is our hope that the work described in this paper, in the end, will help to improve both of these, and be a stepping stone towards a non-trivial partial evaluator for a lazy language.", "num_citations": "3\n", "authors": ["307"]}
{"title": "Abstract interpretation of polymorphic functions\n", "abstract": " Abstract interpretation is one of the popular techniques used in doing program analysis. In this paper, we define an abstract interpretation of polymorphic functions which may be used to perform strictness analysis. The abstract functions of polymorphic functions are again polymorphic. Finally it is proved that the abstract interpretation is safe.", "num_citations": "3\n", "authors": ["307"]}
{"title": "Understanding formal specifications through good examples\n", "abstract": " Formal specifications of software applications are hard to understand, even for domain experts. Because a formal specification is abstract, reading it does not immediately convey the expected behaviour of the software. Carefully chosen examples of the software\u2019s behaviour, on the other hand, are concrete and easy to understand\u2014but poorly-chosen examples are more confusing than helpful. In order to understand formal specifications, software developers need good examples.", "num_citations": "2\n", "authors": ["307"]}
{"title": "Pretty-printing: an exercise in functional programming\n", "abstract": " We designed a very small library (around one page) defining five combinators for pretty-printing. Despite, or perhaps because of their simplicity, these combinators have proved sufficiently efficient and expressive for quite extensive practical use. The combinators are simple to specify and enjoy many algebraic properties. The algebra is useful for reasoning about pretty-printers that use the library, and played an important role in deriving its implementations.", "num_citations": "2\n", "authors": ["307"]}
{"title": "Do Judge a Test by its Cover: Combining Combinatorial and Property-Based Testing\n", "abstract": " Property-based testing uses randomly generated inputs to validate high-level program specifications. It can be shockingly effective at finding bugs, but it often requires generating a very large number of inputs to do so. In this paper, we apply ideas from combinatorial testing, a powerful and widely studied testing methodology, to modify the distributions of our random generators so as to find bugs with fewer tests. The key concept is combinatorial coverage, which measures the degree to which a given set of tests exercises every possible choice of values for every small combination of input features.In its \u201cclassical\u201d form, combinatorial coverage only applies to programs whose inputs have a very particular shape\u2014essentially, a Cartesian product of finite sets. We generalize combinatorial coverage to the richer world of algebraic data types by formalizing a class of sparse test descriptions based on regular tree expressions. This new definition of coverage inspires a novel combinatorial thinning algorithm for improving the coverage of random test generators, requiring many fewer tests to catch bugs. We evaluate this algorithm on two case studies, a typed evaluator for System F terms and a Haskell compiler, showing significant improvements in both.", "num_citations": "1\n", "authors": ["307"]}
{"title": "Testing the hard stuff and staying sane\n", "abstract": " \" We know there is a lurking bug somewhere in the dets code. We have got'bad object'and'premature eof'every other month the last year. We have not been able to track the bug down since the dets files is repaired automatically next time it is opened.\u201c", "num_citations": "1\n", "authors": ["307"]}
{"title": "Functional Programming: Proceedings of the 1989 Glasgow Workshop 21\u201323 August 1989, Fraserburgh, Scotland\n", "abstract": " Functional Programming is a relatively new area of computer science. These proceedings contain 25 papers representing an excellent snapshot of the current state of functional programming and are written by the leading computer scientists in this aera. In some universities, a functional programming language is used as the introductory teaching language and computer architectures are being designed and investigated to support functional languages.", "num_citations": "1\n", "authors": ["307"]}