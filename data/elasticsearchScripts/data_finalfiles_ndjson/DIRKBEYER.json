{"title": "CPAchecker: A Tool for Configurable Software Verification\n", "abstract": " Configurable software verification is a recent concept for expressing different program analysis and model checking approaches in one single formalism. This paper presents CPAchecker, a tool and framework that aims at easy integration of new verification components. Every abstract domain, together with the corresponding operations, implements the interface of configurable program analysis (CPA). The main algorithm is configurable to perform a reachability analysis on arbitrary combinations of existing CPAs. In software verification, it takes a considerable amount of effort to convert a verification idea into actual experimental results \u2014 we aim at accelerating this process. We hope that researchers find it convenient and productive to implement new verification ideas and algorithms using this flexible and easy-to-extend platform, and that it advances the field by making it easier to perform practical\u00a0\u2026", "num_citations": "605\n", "authors": ["278"]}
{"title": "Configurable Software Verification: Concretizing the Convergence of Model Checking and Program Analysis\n", "abstract": " In automatic software verification, we have observed a theoretical convergence of model checking and program analysis. In practice, however, model checkers are still mostly concerned with precision, e.g., the removal of spurious counterexamples; for this purpose they build and refine reachability trees. Lattice-based program analyzers, on the other hand, are primarily concerned with efficiency. We designed an algorithm and built a tool that can be configured to perform not only a purely tree-based or a purely lattice-based analysis, but offers many intermediate settings that have not been evaluated before. The algorithm and tool take one or more abstract interpreters, such as a predicate abstraction and a shape analysis, and configure their execution and interaction using several parameters. Our experiments show that such customization may lead to dramatic improvements in the precision-efficiency\u00a0\u2026", "num_citations": "239\n", "authors": ["278"]}
{"title": "Web Service Interfaces\n", "abstract": " We present a language for specifying web service interfaces. A web service interface puts three kinds of constraints on the users of the service. First, the interface specifies the methods that can be called by a client, together with types of input and output parameters; these are called signature constraints. Second, the interface may specify propositional constraints on method calls and output values that may occur in a web service conversation; these are called consistency constraints. Third, the interface may specify temporal constraints on the ordering of method calls; these are called protocol constraints. The interfaces can be used to check, first, if two or more web services are compatible, and second, if a web service A can be safely substituted for a web service B. The algorithm for compatibility checking verifies that two or more interfaces fulfill each others' constraints. The algorithm for substitutivity checking verifies\u00a0\u2026", "num_citations": "203\n", "authors": ["278"]}
{"title": "Symbolic Invariant Verification for Systems with Dynamic Structural Adaptation\n", "abstract": " The next generation of networked mechatronic systems will be characterized by complex coordination and structural adaptation at run-time. Crucial safety properties have to be guaranteed for all potential structural configurations. Testing cannot provide safety guarantees, while current model checking and theorem proving techniques do not scale for such systems. We present a verification technique for arbitrarily large multi-agent systems from the mechatronic domain, featuring complex coordination and structural adaptation. We overcome the limitations of existing techniques by exploiting the local character of structural safety properties. The system state is modeled as a graph, system transitions are modeled as rule applications in a graph transformation system, and safety properties of the system are encoded as inductive invariants (permitting the verification of infinite state systems). We developed a symbolic\u00a0\u2026", "num_citations": "171\n", "authors": ["278"]}
{"title": "Clustering Software Artifacts Based on Frequent Common Changes\n", "abstract": " Changes of software systems are less expensive and less error-prone if they affect only one subsystem. Thus, clusters of artifacts that are frequently changed together are subsystem candidates. We introduce a two-step method for identifying such clusters. First, a model of common changes of software artifacts, called co-change graph, is extracted from the version control repository of the software system. Second, a layout of the co-change graph is computed that reveals clusters of frequently co-changed artifacts. We derive requirements for such layouts, and introduce an energy model for producing layouts that fulfill these requirements. We evaluate the method by applying it to three example systems, and comparing the resulting layouts to authoritative decompositions.", "num_citations": "167\n", "authors": ["278"]}
{"title": "Software verification and verifiable witnesses\n", "abstract": " SV-COMP 2015 marks the start of a new epoch of software verification: In the 4th Competition on Software Verification, software verifiers produced for each reported property violation a machine-readable error witness in a common exchange format (so far restricted to reachability properties of sequential programs without recursion). Error paths were reported previously, but always in different, incompatible formats, often insufficient to reproduce the identified bug, and thus, useless to the user. The common exchange format and the support by a large set of verification tools that use the format will make a big difference: One verifier can re-verify the witnesses produced by another verifier, visual error-path navigation tools can be developed, and here in the competition, we use witness checking to make sure that a verifier that claimed a found bug, had really found a valid error path. The other two changes to SV\u00a0\u2026", "num_citations": "160\n", "authors": ["278"]}
{"title": "Reliable and reproducible competition results with benchexec and witnesses (report on SV-COMP 2016)\n", "abstract": " The 5 Competition on Software Verification (SV-COMP 2016) continues the tradition of a thorough comparative evaluation of fully-automatic software verifiers. This report presents the results of the competition and includes a special section that describes how SV-COMP ensures that the experiments are reliably executed, precisely measured, and organized such that the results can be reproduced later. SV-COMP uses BenchExec for controlling and measuring the verification runs, and requires violation witnesses in an exchangeable format, whenever a verifier reports that a property is violated. Each witness was validated by two independent and publicly-available witness validators. The tables report the state of the art in software verification in terms of effectiveness and efficiency. The competition used 6\u00a0661 verification tasks that each consisted of a C program and a property (reachability, memory safety\u00a0\u2026", "num_citations": "144\n", "authors": ["278"]}
{"title": "Predicate Abstraction with Adjustable-Block Encoding\n", "abstract": " Several successful software model checkers are based on a technique called single-block encoding (SBE), which computes costly predicate abstractions after every single program operation. Large-block encoding (LBE) computes abstractions only after a large number of operations, and it was shown that this significantly improves the verification performance. In this work, we present adjustable-block encoding (ABE), a unifying framework that allows to express both previous approaches. In addition, it provides the flexibility to specify any block size between SBE and LBE, and also beyond LBE, through the adjustment of one single parameter. Such a unification of different concepts makes it easier to understand the fundamental properties of the analysis, and makes the differences of the variants more explicit. We evaluate different configurations on example C programs, and identify one that is currently the best.", "num_citations": "144\n", "authors": ["278"]}
{"title": "Competition on Software Verification\n", "abstract": " This report describes the definitions, rules, setup, procedure, and results of the 1st International Competition on Software Verification. The verification community has performed competitions in various areas in the past, and SV-COMP\u201912 is the first competition of verification tools that take software programs as input and run a fully automatic verification of a given safety property. This year\u2019s competition is organized as a satellite event of the International Conference on Tools and Algorithms for the Construction and Analysis of Systems (TACAS).", "num_citations": "136\n", "authors": ["278"]}
{"title": "Efficient Relational Calculation for Software Analysis\n", "abstract": " Calculating with graphs and relations has many applications in the analysis of software systems, for example, the detection of design patterns or patterns of problematic design and the computation of design metrics. These applications require an expressive query language, in particular, for the detection of graph patterns, and an efficient evaluation of the queries even for large graphs. In this paper, we introduce RML, a simple language for querying and manipulating relations based on predicate calculus, and CrocoPat, an interpreter for RML programs. RML is general because it enables the manipulation not only of graphs (i.e., binary relations), but of relations of arbitrary arity. CrocoPat executes RML programs efficiently because it internally represents relations as binary decision diagrams, a data structure that is well-known as a compact representation of large relations in computer-aided verification. We evaluate\u00a0\u2026", "num_citations": "136\n", "authors": ["278"]}
{"title": "Status Report on Software Verification\n", "abstract": " This report describes the 3rd International Competition on Software Verification (SV-COMP 2014), which is the third edition of a thorough comparative evaluation of fully automatic software verifiers. The reported results represent the state of the art in automatic software verification, in terms of effectiveness and efficiency. The verification tasks of the competition consist of nine categories containing a total of 2 868\u00a0C\u00a0programs, covering bit-vector operations, concurrent execution, control-flow and integer data-flow, device-drivers, heap data structures, memory manipulation via pointers, recursive functions, and sequentialized concurrency. The specifications include reachability of program labels and memory safety. The competition is organized as a satellite event at TACAS 2014 in Grenoble, France.", "num_citations": "123\n", "authors": ["278"]}
{"title": "Explicit-State Software Model Checking Based on CEGAR and Interpolation\n", "abstract": " Abstraction, counterexample-guided refinement, and interpolation are techniques that are essential to the success of predicate-based program analysis. These techniques have not yet been applied together to explicit-value program analysis. We present an approach that integrates abstraction and interpolationbased refinement into an explicit-value analysis, i.e., a program analysis that tracks explicit values for a specified set of variables (the precision). The algorithm uses an abstract reachability graph as central data structure and a path-sensitive dynamic approach for precision adjustment. We evaluate our algorithm on the benchmark set of the Competition on Software Verification 2012 (SV-COMP\u201912) to show that our new approach is highly competitive. We also show that combining our new approach with an auxiliary predicate analysis scores significantly higher than the SV-COMP\u201912 winner.", "num_citations": "123\n", "authors": ["278"]}
{"title": "Software verification with validation of results\n", "abstract": " This report describes the 2017 Competition on Software Verification (SV-COMP), the 6 edition of the annual thorough comparative evaluation of fully-automatic software verifiers. The goal is to reflect the current state of the art in software verification in terms of effectiveness and efficiency. The major achievement of the 6 edition of SV-COMP is that the verification results were validated in most categories. The verifiers have to produce verification witnesses, which contain hints that a validator can later use to reproduce the verification result. The answer of a verifier counts only if the validator confirms the verification result. SV-COMP uses two independent, publicly available witness validators. For 2017, a new category structure was introduced that now orders the verification tasks according to the property to verify on the top level, and by the type of programs (e.g., which kind of data types are used) on a\u00a0\u2026", "num_citations": "121\n", "authors": ["278"]}
{"title": "Program Analysis with Dynamic Precision Adjustment\n", "abstract": " We present and evaluate a framework and tool for combining multiple program analyses which allows the dynamic (on-line) adjustment of the precision of each analysis depending on the accumulated results. For example, the explicit tracking of the values of a variable may be switched off in favor of a predicate abstraction when and where the number of different variable values that have been encountered has exceeded a specified threshold. The method is evaluated on verifying the SSH client/server software and shows significant gains compared with predicate abstraction-based model checking.", "num_citations": "120\n", "authors": ["278"]}
{"title": "Second Competition on Software Verification\n", "abstract": " This report describes the 2nd International Competition on Software Verification (SV-COMP 2013), which is the second edition of this thorough evaluation of fully automatic verifiers for software programs. The reported results represent the 2012 state-of-the-art in automatic software verification, in terms of effectiveness and efficiency, and as available and participated. The benchmark set of verification tasks consists of 2 315 programs, written in C, and exposing features of integers, heap-data structures, bit-vector operations, and concurrency; the properties include reachability and memory safety. The competition is again organized as a satellite event of TACAS.", "num_citations": "109\n", "authors": ["278"]}
{"title": "Conditional Model Checking: A Technique to Pass Information between Verifiers\n", "abstract": " Software model checking, as an undecidable problem, has three possible outcomes:(1) the program satisfies the specification,(2) the program does not satisfy the specification, and (3) the model checker fails. The third outcome usually manifests itself in a space-out, time-out, or one component of the verification tool giving up; in all of these failing cases, significant computation is performed by the verification tool before the failure, but no result is reported. We propose to reformulate the model-checking problem as follows, in order to have the verification tool report a summary of the performed work even in case of failure: given a program and a specification, the model checker returns a condition \u03a8---usually a state predicate---such that the program satisfies the specification under the condition \u03a8---that is, as long as the program does not leave the states in which \u03a8 is satisfied. In our experiments, we investigated as one\u00a0\u2026", "num_citations": "103\n", "authors": ["278"]}
{"title": "Rabbit: A Tool for BDD-Based Verification of Real-Time Systems\n", "abstract": " This paper gives a short overview of a model checking tool for real-time systems. The modeling language are timed automata extended with concepts for modular modeling. The tool provides reachability analysis and refinement checking, both implemented using the data structure BDD. Good variable orderings for the BDDs are computed from the modular structure of the model and an estimate of the BDD size. This leads to a significant performance improvement compared to the tool RED and the BDD-based version of Kronos.", "num_citations": "100\n", "authors": ["278"]}
{"title": "Lazy Shape Analysis\n", "abstract": " Many software model checkers are based on predicate abstraction. If the verification goal depends on pointer structures, the approach does not work well, because it is difficult to find adequate predicate abstractions for the heap. In contrast, shape analysis, which uses graph-based heap abstractions, can provide a compact representation of recursive data structures. We integrate shape analysis into the software model checker Blast. Because shape analysis is expensive, we do not apply it globally. Instead, we ensure that, like predicates, shape graphs are computed and stored locally, only where necessary for proving the verification goal. To achieve this, we extend lazy abstraction refinement, which so far has been used only for predicate abstractions, to three-valued logical structures. This approach does not only increase the precision of model checking, but it also increases the efficiency of shape\u00a0\u2026", "num_citations": "92\n", "authors": ["278"]}
{"title": "Reliable benchmarking: Requirements and solutions\n", "abstract": " Benchmarking is a widely used method in experimental computer science, in particular, for the comparative evaluation of tools and algorithms. As a consequence, a number of questions need to be answered in order to ensure proper benchmarking, resource measurement, and presentation of results, all of which is essential for researchers, tool developers, and users, as well as for tool competitions. We identify a set of requirements that are indispensable for reliable benchmarking and resource measurement of time and memory usage of automatic solvers, verifiers, and similar tools, and discuss limitations of existing methods and benchmarking tools. Fulfilling these requirements in a benchmarking framework can (on Linux systems) currently only be done by using the cgroup and namespace features of the kernel. We developed BenchExec, a ready-to-use, tool-independent, and open-source\u00a0\u2026", "num_citations": "89\n", "authors": ["278"]}
{"title": "Witness validation and stepwise testification across software verifiers\n", "abstract": " It is commonly understood that a verification tool should provide a counterexample to witness a specification violation. Until recently, software verifiers dumped error witnesses in proprietary formats, which are often neither human-nor machine-readable, and an exchange of witnesses between different verifiers was impossible. To close this gap in software-verification technology, we have defined an exchange format for error witnesses that is easy to write and read by verification tools (for further processing, eg, witness validation) and that is easy to convert into visualizations that conveniently let developers inspect an error path. To eliminate manual inspection of false alarms, we develop the notion of stepwise testification: in a first step, a verifier finds a problematic program path and, in addition to the verification result FALSE, constructs a witness for this path; in the next step, another verifier re-verifies that the witness\u00a0\u2026", "num_citations": "78\n", "authors": ["278"]}
{"title": "Automatic verification of C and Java programs: SV-COMP 2019\n", "abstract": " This report describes the 2019 Competition on Software Verification (SV-COMP), the 8 edition of a series of comparative evaluations of fully automatic software verifiers for C programs, and now also for Java programs. The competition provides a snapshot of the current state of the art in the area, and has a strong focus on replicability of its results. The repository of benchmark verification tasks now supports a new, more flexible format for task definitions (based on YAML), which was a precondition for conveniently benchmarking Java programs in the same controlled competition setting that was successfully applied in the previous years. The competition was based on 10\u00a0522 verification tasks for C\u00a0programs and 368 verification tasks for Java programs. Each verification task consisted of a program and a property (reachability, memory safety, overflows, termination). SV-COMP 2019 had 31 participating\u00a0\u2026", "num_citations": "76\n", "authors": ["278"]}
{"title": "Relational Programming with CrocoPat\n", "abstract": " Many structural analyses of software systems are naturally formalized as relational queries, for example, the detection of design patterns, patterns of problematic design, code clones, dead code, and differences between the as-built and the as-designed architecture. This paper describes CrocoPat, an application-independent tool for relational programming. Through its efficiency and its expressive language, CrocoPat enables practically important analyses of real-world software systems that are not possible with other graph analysis tools, in particular analyses that involve transitive closures and the detection of patterns in graphs. The language is easy to use, because it is based on the well-known first-order predicate logic. The tool is easy to integrate into other software systems, because it is a small command-line tool that uses a simple text format for input and output of relations.", "num_citations": "76\n", "authors": ["278"]}
{"title": "Boosting k-induction with continuously-refined invariants\n", "abstract": " -induction is a promising technique to extend bounded model checking from falsification to verification. In software verification, -induction works only if auxiliary invariants are used to strengthen the induction hypothesis. The problem that we address is to generate such invariants (1)\u00a0automatically without user-interaction, (2)\u00a0efficiently such that little verification time is spent on the invariant generation, and (3)\u00a0that are sufficiently strong for a -induction proof. We boost the -induction approach to significantly increase effectiveness and efficiency in the following way: We start in parallel to -induction a data-flow-based invariant generator that supports dynamic precision adjustment and refine the precision of the invariant generator continuously during the analysis, such that the invariants become increasingly stronger. The -induction engine is extended such that the invariants from the invariant generator are\u00a0\u2026", "num_citations": "74\n", "authors": ["278"]}
{"title": "Benchmarking and resource measurement\n", "abstract": " Proper benchmarking and resource measurement is an important topic, because benchmarking is a widely-used method for the comparative evaluation of tools and algorithms in many research areas. It is essential for researchers, tool developers, and users, as well as for competitions. We formulate a set of requirements that are indispensable for reproducible benchmarking and reliable resource measurement of automatic solvers, verifiers, and similar tools, and discuss limitations of existing methods and benchmarking tools. Fulfilling these requirements in a benchmarking framework is complex and can (on Linux) currently only be done by using the cgroups feature of the kernel. We provide                                                                                     , a ready-to-use, tool-independent, and free implementation of a benchmarking framework that fulfills all presented requirements, making reproducible benchmarking and\u00a0\u2026", "num_citations": "72\n", "authors": ["278"]}
{"title": "Improvements in BDD-Based Reachability Analysis of Timed Automata\n", "abstract": " To develop efficient algorithms for the reachability analysis of timed automata, a promising approach is to use binary decision diagrams (BDDs) as data structure for the representation of the explored state space. The size of a BDD is very sensitive to the ordering of the variables. We use the communication structure to deduce an estimation for the BDD size. In our experiments, this guides the choice of good variable orderings, which leads to an efficient reachability analysis. We develop a discrete semantics for closed timed automata to get a finite state space required by the BDD-based representation and we prove the equivalence to the continuous semantics regarding the set of reachable locations. An upper bound for the size of the BDD representing the transition relation and an estimation for the set of reachable configurations based on the communication structure is given. We implemented these\u00a0\u2026", "num_citations": "67\n", "authors": ["278"]}
{"title": "Simple and Efficient Relational Querying of Software Structures\n", "abstract": " Many analyses of software systems can be formalized as relational queries, for example the detection of design patterns, of patterns of problematic design, of code clones, of dead code, and of differences between the as-built and the as-designed architecture. This paper describes the concepts of CrocoPat, a tool for querying and manipulating relations. CrocoPat is easy to use, because of its simple query and manipulation language based on predicate calculus, and its simple file format for relations. CrocoPat is efficient, because it internally represents relations as binary decision diagrams, a data structure that is well-known as a compact representation of large relations in computer-aided verification. CrocoPat is general, because it manipulates not only graphs (ie binary relations), but n-ary relations.", "num_citations": "64\n", "authors": ["278"]}
{"title": "CrocoPat: Efficient Pattern Analysis in Object-Oriented Programs\n", "abstract": " CrocoPat is a new tool for efficient pattern-based analysis of large object-oriented programs. Patterns can be flexibly specified by expressions based on standard mathematics provided by the tool language. It is easy to specify patterns in different variants in a compact form, adapted to specific situations.", "num_citations": "63\n", "authors": ["278"]}
{"title": "Correctness witnesses: Exchanging verification results between verifiers\n", "abstract": " Standard verification tools provide a counterexample to witness a specification violation, and, since a few years, such a witness can be validated by an independent validator using an exchangeable witness format. This way, information about the violation can be shared across verification tools and the user can use standard tools to visualize and explore witnesses. This technique is not yet established for the correctness case, where a program fulfills a specification. Even for simple programs, it is often difficult for users to comprehend why a given program is correct, and there is no way to independently check the verification result. We close this gap by complementing our earlier work on violation witnesses with correctness witnesses. While we use an extension of the established common exchange format for violation witnesses to represent correctness witnesses, the techniques for producing and validating\u00a0\u2026", "num_citations": "61\n", "authors": ["278"]}
{"title": "Advances in automatic software verification: SV-COMP 2020\n", "abstract": " This report describes the 2020 Competition on Software Verification (SV-COMP), the 9th edition of a series of comparative evaluations of fully automatic software verifiers for C and Java programs. The competition provides a snapshot of the current state of the art in the area, and has a strong focus on replicability of its results. The competition was based on 11 052 verification tasks for C programs and 416 verification tasks for Java programs. Each verification task consisted of a program and a property (reachability, memory safety, overflows, termination). SV-COMP 2020 had 28 participating verification systems from 11 countries.", "num_citations": "52\n", "authors": ["278"]}
{"title": "Precision reuse for efficient regression verification\n", "abstract": " Continuous testing during development is a well-established technique for software-quality assurance. Continuous model checking from revision to revision is not yet established as a standard practice, because the enormous resource consumption makes its application impractical. Model checkers compute a large number of verification facts that are necessary for verifying if a given specification holds. We have identified a category of such intermediate results that are easy to store and efficient to reuse: abstraction precisions. The precision of an abstract domain specifies the level of abstraction that the analysis works on. Precisions are thus a precious result of the verification effort and it is a waste of resources to throw them away after each verification run. In particular, precisions are reasonably small and thus easy to store; they are easy to process and have a large impact on resource consumption. We\u00a0\u2026", "num_citations": "52\n", "authors": ["278"]}
{"title": "Software verification: Testing vs. model checking\n", "abstract": " In practice, software testing has been the established method for finding bugs in programs for a long time. But in the last 15\u00a0years, software model checking has received a lot of attention, and many successful tools for software model checking exist today. We believe it is time for a careful comparative evaluation of automatic software testing against automatic software model checking. We chose six existing tools for automatic test-case generation, namely AFL-fuzz, CPATiger, Crest-ppc, FShell, Klee, and PRtest, and four tools for software model checking, namely Cbmc, CPA-Seq, Esbmc-incr, and Esbmc-kInd, for the task of finding specification violations in a large benchmark suite consisting of 5\u00a0693\u00a0C\u00a0programs. In order to perform such an evaluation, we have implemented a framework for test-based falsification (tbf) that executes and validates test cases produced by test-case generation tools in order to find\u00a0\u2026", "num_citations": "51\n", "authors": ["278"]}
{"title": "A unifying view on SMT-based software verification\n", "abstract": " After many years of successful development of new approaches for software verification, there is a need to consolidate the knowledge about the different abstract domains and algorithms. The goal of this paper is to provide a compact and accessible presentation of four SMT-based verification approaches in order to study them in theory and in practice. We present and compare the following different \u201cschools of thought\u201d of software verification: bounded model checking, k-induction, predicate abstraction, and lazy abstraction with interpolants. Those approaches are well-known and successful in software verification and have in common that they are based on SMT solving as the back-end technology. We reformulate all four approaches in the unifying theoretical framework of configurable program analysis and implement them in the verification framework\u00a0CPAchecker. Based on this, we can present an\u00a0\u2026", "num_citations": "44\n", "authors": ["278"]}
{"title": "Reuse of verification results\n", "abstract": " Verification is a complex algorithmic task, requiring large amounts of computing resources. One approach to reduce the resource consumption is to reuse information from previous verification runs. This paper gives an overview of three techniques for such information reuse. Conditional model checking outputs a condition that describes the state space that was successfully verified, and accepts as input a condition that instructs the model checker which parts of the system should be verified; thus, later verification runs can use the output condition of previous runs in order to not verify again parts of the state space that were already verified. Precision reuse is a technique to use intermediate results from previous verification runs to accelerate further verification runs of the system; information about the level of abstraction in the abstract model can be reused in later verification runs. Typical model checkers\u00a0\u2026", "num_citations": "42\n", "authors": ["278"]}
{"title": "Impact of Inheritance on Metrics for Size, Coupling, and Cohesion in Object-Oriented Systems\n", "abstract": " In today\u2019s engineering of object oriented systems many different metrics are used to get feedback about design quality and to automatically identify design weaknesses. While the concept of inheritance is covered by special inheritance metrics its impact on other classical metrics (like size, coupling or cohesion metrics) is not considered; this can yield misleading measurement values and false interpretations. In this paper we present an approach to work the concept of inheritance into classical metrics (and with it the related concepts of overriding, overloading and polymorphism). This is done by some language dependent flattening functions that modify the data on which the measurement will be done. These functions are implemented within our metrics tool Crocodile and are applied for a case study: the comparison of the measurement values of the original data with the measurement values of the flattened\u00a0\u2026", "num_citations": "41\n", "authors": ["278"]}
{"title": "Linux Driver Verification\n", "abstract": " Linux driver verification is a large application area for software verification methods, in particular, for functional, safety, and security verification. Linux driver software is industrial production code \u2014 IT infrastructures rely on its stability, and thus, there are strong requirements for correctness and reliability. This implies that if a verification engineer has identified a bug in a driver, the engineer can expect quick response from the development community in terms of bug confirmation and correction. Linux driver software is complex, low-level systems code, and its characteristics make it necessary to bring to bear techniques from program analysis, SMT solvers, model checking, and other areas of software verification. These areas have recently made a significant progress in terms of precision and performance, and the complex task of verifying Linux driver software can be successful if the conceptual state-of-the-art\u00a0\u2026", "num_citations": "37\n", "authors": ["278"]}
{"title": "Refinement selection\n", "abstract": " Counterexample-guided abstraction refinement (CEGAR) is a property-directed approach for the automatic construction of an abstract model for a given system. The approach learns information from infeasible error paths in order to refine the abstract model. We address the problem of selecting which information to learn from a given infeasible error path. In previous work, we presented a method that enables refinement selection by extracting a set of sliced prefixes from a given infeasible error path, each of which represents a different reason for infeasibility of the error path and thus, a possible way to refine the abstract model. In this work, we (1) define and investigate several promising heuristics for selecting an appropriate precision for refinement, and (2) propose a new combination of a value analysis and a predicate analysis that does not only find out which information to learn from an infeasible error\u00a0\u2026", "num_citations": "36\n", "authors": ["278"]}
{"title": "Co-change Visualization\n", "abstract": " Clustering layouts of software systems combine two important aspects: they reveal groups of related artifacts of the software system, and they produce a visualization of the results that is easy to understand. Co-change visualization is a lightweight method for computing clustering layouts of software systems for which the change history is available. This paper describes CCVisu, a tool that implements cochange visualization. It extracts the co-change graph from a version repository, and computes a layout, which positions the artifacts of the software system in a two-or threedimensional space. Two artifacts are positioned closed together in the layout if they were often changed together. The tool is designed as a framework, easy to use, and easy to integrate into reengineering environments; several formats for data interchange are already implemented. The graph layout is currently provided in VRML and SVG format, in a standard text format, or directly drawn on the screen.", "num_citations": "32\n", "authors": ["278"]}
{"title": "Algorithms for Software Model Checking: Predicate Abstraction vs. IMPACT\n", "abstract": " CEGAR, SMT solving, and Craig interpolation are successful approaches for software model checking. We compare two of the most important algorithms that are based on these techniques: lazy predicate abstraction (as in Blast) and lazy abstraction with interpolants (as in Impact). We unify the algorithms formally (by expressing both in the CPA framework) as well as in practice (by implementing them in the same tool). This allows us to flexibly experiment with new configurations and gain new insights, both about their most important differences and commonalities, as well as about their performance characteristics. We show that the essential contribution of the Impact algorithm is the reduction of the number of refinements, and compare this to another approach for reducing refinement effort: adjustable-block encoding (ABE).", "num_citations": "30\n", "authors": ["278"]}
{"title": "BDD-based software verification\n", "abstract": " In software model checking, most successful symbolic approaches use predicates as representation of the state space, and SMT solvers for computations on the state space; BDDs are often used as auxiliary data structure. Although BDDs are applied with great success in hardware verification, BDD representations of software state spaces were not yet thoroughly investigated, mainly because not all operations that are needed in software verification are efficiently supported by BDDs. We evaluate the use of a pure BDD representation of integer values, and focus on a particular class of programs: event-condition-action (ECA) programs with limited operations. A symbolic representation using BDDs seems appropriate for ECA programs under certain conditions. We configure a program analysis based on BDDs and experimentally compare it to four approaches to verify reachability properties of ECA programs\u00a0\u2026", "num_citations": "27\n", "authors": ["278"]}
{"title": "An Application of Web-Service Interfaces\n", "abstract": " We present a case study to illustrate our formalism for the specification and verification of the method-invocation behavior of web-service applications constructed from asynchronously interacting multi-threaded distributed components. Our model is expressive enough to allow the representation of recursion and dynamic thread creation, and yet permits the algorithmic analysis of the following two questions: (1) Does a given service satisfy a safety specification? (2) Can a given service be substituted by a another service in an arbitrary context? Our case study is based on the Amazon.com E-Commerce Services (ECS) platform.", "num_citations": "27\n", "authors": ["278"]}
{"title": "JavaSMT: A unified interface for SMT solvers in Java\n", "abstract": " Satisfiability Modulo Theory (SMT) solvers received a lot of attention in the research community in the last decade, and consequently their expressiveness and performance have significantly improved. In the areas of program analysis and model checking, many of the newly developed tools rely on SMT solving. The SMT-LIB initiative defines a common format for communication with an SMT solver. However, tool developers often prefer to use the solver API instead, because many features offered by SMT solvers such as interpolation, optimization, and formula introspection are not supported by SMT-LIB directly. Additionally, using SMT-LIB for communication incurs a performance overhead, because all the queries to the solver have to be serialized to strings. Yet using the API directly creates the problem of a solver lock-in, which makes evaluating a tool with different solvers very difficult. We present JavaSMT\u00a0\u2026", "num_citations": "26\n", "authors": ["278"]}
{"title": "Reducer-based construction of conditional verifiers\n", "abstract": " Despite recent advances, software verification remains challenging. To solve hard verification tasks, we need to leverage not just one but several different verifiers employing different technologies. To this end, we need to exchange information between verifiers. Conditional model checking was proposed as a solution to exactly this problem: The idea is to let the first verifier output a condition which describes the state space that it successfully verified and to instruct the second verifier to verify the yet unverified state space using this condition. However, most verifiers do not understand conditions as input.", "num_citations": "24\n", "authors": ["278"]}
{"title": "A Simple and Effective Measure for Complex Low-Level Dependencies\n", "abstract": " The measure dep-degree is a simple indicator for structural problems and complex dependencies on code-level. We model low-level dependencies between program operations as use-def graph, which is generated from reaching definitions of variables. The more dependencies a program operation has, the more different program states have to be considered and the more difficult it is to understand the operation. Dep-degree is simple to compute and interpret, flexible and scalable in its application, and independently complementing other indicators. Preliminary experiments suggest that the measure dep-degree, which simply counts the number of dependency edges in the use-def graph, is a good indicator for readability and understandablity.", "num_citations": "23\n", "authors": ["278"]}
{"title": "CCVisu: Automatic visual software decomposition\n", "abstract": " Understanding the structure of large existing (and evolving) software systems is a major challenge for software engineers. In reverse engineering, we aim to compute, for a given software system, a decomposition of the system into its subsystems. CCVisu is a lightweight tool that takes as input a software graph model and computes a visual representation of the system's structure, ie, it structures the system into separated groups of artifacts that are strongly related, and places them in a 2-or 3-dimensional space. Besides the decomposition into subsystems, it reveals the relatedness between the subsystems via interpretable distances. The tool reads a software graph from a simple text file in RSF format, eg, call, inheritance, containment, or co-change graphs. The resulting system structure is currently either directly presented on the screen, or written to an output file in SVG, VRML, or plain text format. The tool is\u00a0\u2026", "num_citations": "23\n", "authors": ["278"]}
{"title": "BDD-Based Software Model Checking with CPAchecker\n", "abstract": " In symbolic software model checking, most approaches use predicates as symbolic representation of the state space, and SMT solvers for computations on the state space; BDDs are sometimes used as auxiliary data structure. The representation of software state spaces by BDDs was not yet thoroughly investigated, although BDDs are successful in hardware verification. The reason for this is that BDDs do not efficiently support all operations that are needed in software verification. In this work, we evaluate the use of a pure BDD representation of integer variable values, and focus on a particular class of programs: event-conditionaction systems with limited operations. A symbolic representation using BDDs seems appropriate for this particular class of programs. We implement a program analysis based on BDDs and experimentally compare three symbolic techniques to verify reachability properties of ECA\u00a0\u2026", "num_citations": "22\n", "authors": ["278"]}
{"title": "Software verification: 10th comparative evaluation (SV-COMP 2021)\n", "abstract": " SV-COMP 2021 is the 10th edition of the Competition on Software Verification (SV-COMP), which is an annual comparative evaluation of fully automatic software verifiers for C and Java programs. The competition provides a snapshot of the current state of the art in the area, and has a strong focus on reproducibility of its results. The competition was based on 15 201 verification tasks for C programs and 473 verification tasks for Java programs. Each verification task consisted of a program and a property (reachability, memory safety, overflows, termination). SV-COMP 2021 had 30 participating verification systems from 27 teams from 11 countries.", "num_citations": "21\n", "authors": ["278"]}
{"title": "Symbolic execution with CEGAR\n", "abstract": " Symbolic execution, a standard technique in program analysis, is a particularly successful and popular component in systems for test-case generation. One of the open research problems is that the approach suffers from the path-explosion problem. We apply abstraction to symbolic execution, and refine the abstract model using counterexample-guided abstraction refinement (Cegar), a standard technique from model checking. We also use refinement selection with existing and new heuristics to influence the behavior and further improve the performance of our refinement procedure. We implemented our new technique in the open-source software-verification framework CPAchecker. Our experimental results show that the implementation is highly competitive.", "num_citations": "20\n", "authors": ["278"]}
{"title": "Can Decision Diagrams Overcome State Space Explosion in Real-Time Verification?\n", "abstract": " In this paper we analyze the efficiency of binary decision diagrams (BDDs) and clock difference diagrams (CDDs) in the verification of timed automata. Therefore we present analytical and empirical complexity results for three communication protocols. The contributions of the analyses are: Firstly, they show that BDDs and CDDs of polynomial size exist for the reachability sets of the three protocols. This is the first evidence that CDDs can grow only polynomially for models with non-trivial state space explosion. Secondly, they show that CDD-based tools, which currently use at least exponential space for two of the protocols, will only find polynomial-size CDDs if they use better variable orders, as the BDD-based tool Rabbit does. Finally, they give insight into the dependency of the BDD and CDD size on properties of the model, in particular the number of automata and the magnitude of the clock values.", "num_citations": "20\n", "authors": ["278"]}
{"title": "Rabbit: Verification of Real-Time Systems\n", "abstract": " This paper gives a short overview of a model checking tool for Cottbus Timed Automata, which is a modular modeling language based on timed and hybrid automata. For timed automata, the current version of the tool provides BDD-based verification using an integer semantics. Reachability analysis as well as refinement checking is possible. To find good variable orderings it uses the component structure of the model and an upper bound for the BDD size. For hybrid automata, reachability analysis based on the double description method is implemented.", "num_citations": "20\n", "authors": ["278"]}
{"title": "CoVeriTest: Cooperative Verifier-Based Testing.\n", "abstract": " Testing is a widely used method to assess software quality. Coverage criteria and coverage measurements are used to ensure that the constructed test suites adequately test the given software. Since manually developing such test suites is too expensive in practice, various automatic test-generation approaches were proposed. Since all approaches come with different strengths, combinations are necessary in order to achieve stronger tools. We study cooperative combinations of verification approaches for test generation, with high-level information exchange. We present CoVeriTest, a hybrid approach for test-case generation, which iteratively applies different conditional model checkers. Thereby, it allows to adjust the level of cooperation and to assign individual time budgets per verifier. In our experiments, we combine explicit-state model checking and predicate abstraction (from CPAchecker) to systematically study differentCoVeriTest configurations. Moreover, CoVeriTest achieves higher coverage than state-of-the-art test-generation tools for some programs.", "num_citations": "19\n", "authors": ["278"]}
{"title": "Software verification in the Google App-Engine cloud\n", "abstract": " Software verification often requires a large amount of computing resources. In the last years, cloud services emerged as an inexpensive, flexible, and energy-efficient source of computing power. We have investigated if such cloud resources can be used effectively for verification. We chose the platform-as-a-service offer Google App Engine and ported the open-source verification framework CPAchecker to it. We provide our new verification service as a web front-end to users who wish to solve single verification tasks (tutorial usage), and an API for integrating the service into existing verification infrastructures (massively parallel bulk usage). We experimentally evaluate the effectiveness of this service and show that it can be successfully used to offload verification work to the cloud, considerably sparing local verification resources.", "num_citations": "19\n", "authors": ["278"]}
{"title": "Modeling a Production Cell as a Distributed Real-Time System with Cottbus Timed Automata\n", "abstract": " We build on work in designing modeling languages for hybrid systems in the development of CTA, the Cottbus Timed Automata. Our design features a facility to specify a hybrid system modulary and hierarchically, communication through CSP-like synchronizations but with special support to specify explicitly different roles which the interface signals and variables of a module play, and to instanziate recurring elements serveral times from a template. Continuous system components are modeled with analogue variables having piecewise constant derivatives. Discrete system aspects like control modes are modeled with the discrete variables and the states of a finite automaton. Our approach to specifying distributed hybrid systems is illustrated with the specification of a component of a production cell, a transport belt.", "num_citations": "19\n", "authors": ["278"]}
{"title": "Verification-aided debugging: An interactive web-service for exploring error witnesses\n", "abstract": " Traditionally, a verification task is considered solved as soon as a property violation or a correctness proof is found. In practice, this is where the actual work starts: Is it just a false alarm? Is the error reproducible? Can the error report later be re-used for bug fixing or regression testing? The advent of exchangeable witnesses is a paradigm shift in verification, from simple answers true and false towards qualitatively more valuable information about the reason for the property violation. This paper explains a convenient web-based toolchain that can be used to answer the above questions. We consider as example application the verification of C programs. Our first component collects witnesses and stores them for later re-use; for example, if the bug is fixed, the witness can be tried once again and should now be rejected, or, if the bug was not scheduled for fixing, the database can later provide the witnesses in\u00a0\u2026", "num_citations": "18\n", "authors": ["278"]}
{"title": "Efficient Reachability Analysis and Refinement Checking of Timed Automata using BDDs\n", "abstract": " For the formal specification and verification of real-time systems we use the modular formalism Cottbus Timed Automata (CTA), which is an extension of timed automata [AD94]. Matrix-based algorithms for the reachability analysis of timed automata are implemented in tools like Kronos, Uppaal, HyTech and Rabbit. A new BDD-based version of Rabbit, which supports also refinement checking, is now available.", "num_citations": "18\n", "authors": ["278"]}
{"title": "Sliced path prefixes: An effective method to enable refinement selection\n", "abstract": " Automatic software verification relies on constructing, for a given program, an abstract model that is (1)\u00a0abstract enough to avoid state-space explosion and (2)\u00a0precise enough to reason about the specification. Counterexample-guided abstraction refinement is a standard technique that suggests to extract information from infeasible error paths, in order to refine the abstract model if it is too imprecise. Existing approaches \u2014including our previous work\u2014 do not choose the refinement for a given path systematically. We present a method that generates alternative refinements and allows to systematically choose a suited one. The method takes as input one given infeasible error path and applies a slicing technique to obtain a set of new error paths that are more abstract than the original error path but still infeasible, each for a different reason. The (more abstract) constraints of the new paths can be passed to a\u00a0\u2026", "num_citations": "17\n", "authors": ["278"]}
{"title": "An Interface Formalism for Web Services\n", "abstract": " Web application development using distributed components and web services presents new software integration challenges, because solutions often cross vendor, administrative, and other boundaries across which neither binary nor source code can be shared. We present a methodology that addresses this problem through a formalism for specifying and manipulating behavioral interfaces of multithreaded open software components that communicate with each other through method calls. An interface constrains both the implementation and the user of a web service to fulfill certain assumptions that are specified by the interface. Our methodology consists of three increasingly expressive classes of interfaces. Signature interfaces specify the methods that can be invoked by the user, together with parameters. Consistency interfaces add propositional constraints, enhancing signature interfaces with the ability to specify choice and causality. Protocol interfaces specify, in addition, temporal ordering constraints on method invocations. We provide approaches to check if two or more interfaces are compatible; if a web service can be safely substituted for another one; and if a web service satisfies a specification that represents a desired behavioral property.", "num_citations": "17\n", "authors": ["278"]}
{"title": "Efficient Verification of Timed Automata Using BDDs\n", "abstract": " This paper investigates the efficient reachability analysis of timed automata. It describes a discretization of time which preserves the reachability properties. The discretization allows to represent sets of configurations of timed automata as binary decision diagrams (BDDs). Further techniques, like computing good variable orderings, are applied to use the full potential of BDDs as compact and canonical representation of large sets. We implemented these concepts within the tool Rabbit. The highly improved performance is shown for some example models. For additional speedup we used an on-the-fly algorithm and refinement checking for large models.", "num_citations": "16\n", "authors": ["278"]}
{"title": "Shape refinement through explicit heap analysis\n", "abstract": " Shape analysis is a promising technique to prove program properties about recursive data structures. The challenge is to automatically determine the data-structure type, and to supply the shape analysis with the necessary information about the data structure. We present a stepwise approach to the selection of instrumentation predicates for a TVLA-based shape analysis, which takes us a step closer towards the fully automatic verification of data structures. The approach uses two techniques to guide the refinement of shape abstractions: (1)\u00a0during program exploration, an explicit heap analysis collects sample instances of the heap structures, which are used to identify the data structures that are manipulated by the program; and (2)\u00a0during abstraction refinement along an infeasible error path, we consider different possible heap abstractions and choose the coarsest one that eliminates the infeasible path. We\u00a0\u2026", "num_citations": "15\n", "authors": ["278"]}
{"title": "Combining k-induction with continuously-refined invariants\n", "abstract": " Bounded model checking (BMC) is a well-known and successful technique for finding bugs in software. k-induction is an approach to extend BMC-based approaches from falsification to verification. Automatically generated auxiliary invariants can be used to strengthen the induction hypothesis. We improve this approach and further increase effectiveness and efficiency in the following way: we start with light-weight invariants and refine these invariants continuously during the analysis. We present and evaluate an implementation of our approach in the open-source verification-framework CPAchecker. Our experiments show that combining k-induction with continuously-refined invariants significantly increases effectiveness and efficiency, and outperforms all existing implementations of k-induction-based software verification in terms of successful verification results.", "num_citations": "14\n", "authors": ["278"]}
{"title": "Mining co-change clusters from version repositories\n", "abstract": " Clusters of software artifacts that are frequently changed together are subsystem candidates, because one of the main goals of software design is to make changes local. The contribution of this paper is a visualization-based method that supports the identification of such clusters. First, we define the co-change graph as a simple but powerful model of common changes of software artifacts, and describe how to extract the graph from version control repositories. Second, we introduce an energy model for computing force-directed layouts of co-change graphs. The resulting layouts have a well-defined interpretation in terms of the structure of the visualized graph, and clearly reveal groups of frequently co-changed artifacts. We evaluate our method by comparing the layouts for three example projects with authoritative subsystem decompositions.Classification: D. 2.7 Distribution, Maintenance, and Enhancement\u2013Restructuring, reverse engineering, and reengineering, D. 2.7 Distribution, Maintenance, and Enhancement\u2013Version control, G. 2.2 Graph Theory, I. 5.3 Clustering", "num_citations": "14\n", "authors": ["278"]}
{"title": "International competition on software testing (Test-Comp)\n", "abstract": " Tool competitions are a special form of comparative evaluation, where each tool has a team of developers or supporters associated that makes sure the tool is properly configured to show its best possible performance. Tool competitions have been a driving force for the development of mature tools that represent the state of the art in several research areas. This paper describes the International Competition on Software Testing (Test-Comp), a comparative evaluation of automatic tools for software test generation. Test-Comp 2019 is presented as part of TOOLympics 2019, a satellite event of the conference TACAS.", "num_citations": "13\n", "authors": ["278"]}
{"title": "A tool for verified design using alloy for specification and CrocoPat for verification\n", "abstract": " The context of our work is a project that focuses on methods and tools for modeling enterprise architectures. An enterprise architecture model represents the structure of an enterprise across multiple levels, from the markets in which it operates down to the implementation of the technical systems that support its operation. These models are based on an ontology that defines the model elements and their relations. In this paper, we describe an efficient method to fully automatically verify the design that our modeling tool manages. We specify the ontology in Alloy, and use the efficient interpreter for relational programs CrocoPat to verify that the design fulfills all constraints specified in the ontology. Technically, we transform all constraints from Alloy into a relational program in CrocoPat\u2019s programming language. Then, we execute the relational program and feed it with a relational representation of the design as input, in order to check that the design element instances fulfill all constraints of the Alloy representation of the ontology. We also present the current limitations of our approach and how\u2013by overcoming these limitations\u2013we can develop an Alloy-based parameterized modeling tool.", "num_citations": "13\n", "authors": ["278"]}
{"title": "Crocopat 2.1 Introduction and Reference Manual\n", "abstract": " CrocoPat is an efficient, powerful and easy-to-use tool for manipulating relations of arbitrary arity, including directed graphs. This manual provides an introduction to and a reference for CrocoPat and its programming language RML. It includes several application examples, in particular from the analysis of structural models of software systems.", "num_citations": "13\n", "authors": ["278"]}
{"title": "Formale Verifikation von Realzeit-Systemen mittels Cottbus Timed Automata\n", "abstract": " Da ich wahrend meiner Tatigkeit in Cottbus nicht\u2019kontextfrei\u2019gearbeitet habe und eine solche Arbeit schwerlich ohne die Hilfe und Unterstutzung anderer entsteht, mochte ich mich bei all jenen bedanken, die mich gepragt haben und von denen ich lernen konnte. Bei vier Personen mochte ich mich speziell bedanken: Mein Betreuer Claus Lewerentz sorgte fur eine angenehme Atmosphare in der Arbeitsgruppe. Seine ganzheitliche Interpretation einer Promotion als Weiterentwicklung der Personlichkeit setzte er mit vielfaltiger Unterstutzung in den verschiedensten Bereichen um, wofur ich ihm sehr danke. Durch seinen Anleitungsstil und seine\u2019Softskills\u2019 motivierte er mich auch in aussichtslosen Situationen. In vielen Gesprachen uber Forschung, Lehre und Privatleben konnte ich von ihm lernen. Heinrich Rustgab mirden Ansto\u00df zumThema dieserArbeit; in derDiskussionmit ihm entstanden die Cottbus Timed Automata. Er nahm sich immer Zeit fur meine Probleme undgab mirFeedback zu meinerArbeit. Ichdankeihm auchfurdie zahlreichen Gesprache uber die\u2019unscharfen\u2019Dinge des (wissenschaftlichen) Lebens. Der intensiven Zusammenarbeit mit Andreas Noack, meinem ehemaligen Diplomanden und jetzigen Kollegen, verdanke ich wesentliche Resultate des 3. Kapitels. Meine Begeisterung fur BDDs wurde durch ihn vertieft, und er gab mir viele seiner Erfahrungen mit dieser Datenstruktur weiter. Seine akribische Korrekturarbeit machte ihn zu einem wertvollen Partner.Ganz besonders bedanke ich mich bei meiner Frau Simone: Taglich unterstutzte sie mich, indem sie mir den Rucken von den Verp\ufb02ichtungen des Alltags freihielt und mich in der\u00a0\u2026", "num_citations": "13\n", "authors": ["278"]}
{"title": "Cottbus timed automata: Formal definition and semantics\n", "abstract": " We present a formalism for modular modelling of hybrid systems, the Cottbus Timed Automata. For the theoretical basis, we build on work about timed and hybrid automata. We use concepts from concurrency theory to model communication of separately defined modules, but we extend these concepts to be able to express explicitly read-and write-access to signals and variables.", "num_citations": "13\n", "authors": ["278"]}
{"title": "Second Competition on Software Testing: Test-Comp 2020.\n", "abstract": " This report describes the 2020 Competition on Software Testing (Test-Comp), the 2nd edition of a series of comparative evaluations of fully automatic software test-case generators for C programs. The competition provides a snapshot of the current state of the art in the area, and has a strong focus on replicability of its results. The competition was based on 3 230 test tasks for C programs. Each test task consisted of a program and a test specification (error coverage, branch coverage). Test-Comp 2020 had 10 participating test-generation systems.", "num_citations": "11\n", "authors": ["278"]}
{"title": "SMT-based software model checking: an experimental comparison of four algorithms\n", "abstract": " After many years of successful development of new algorithms for software model checking, there is a need to consolidate the knowledge about the different algorithms and approaches. This paper gives a coarse overview in terms of effectiveness and efficiency of four algorithms. We compare the following different \u201cschools of thought\u201d of algorithms: bounded model checking, k-induction, predicate abstraction, and lazy abstraction with interpolants. Those algorithms are well-known and successful in software verification. They have in common that they are based on SMT solving as the back-end technology, using the theories of uninterpreted functions, bit vectors, and floats as underlying theory. All four algorithms are implemented in the verification framework CPAchecker. Thus, we can present an evaluation that really compares only the core algorithms, and keeps the design variables such as parser front end\u00a0\u2026", "num_citations": "11\n", "authors": ["278"]}
{"title": "Conditional model checking\n", "abstract": " Software model checking, as an undecidable problem, has three possible outcomes: (1) the program satisfies the specification, (2) the program does not satisfy the specification, and (3) the model checker fails. The third outcome usually manifests itself in a space-out, time-out, or one component of the verification tool giving up; in all of these failing cases, significant computation is performed by the verification tool before the failure, but no result is reported. We propose to reformulate the model-checking problem as follows, in order to have the verification tool report a summary of the performed work even in case of failure: given a program and a specification, the model checker returns a condition P ---usually a state predicate--- such that the program satisfies the specification under the condition P ---that is, as long as the program does not leave states in which P is satisfied. We are of course interested in model checkers that return conditions P that are as weak as possible. Instead of outcome (1), the model checker will return P = true; instead of (2), the condition P will return the part of the state space that satisfies the specification; and in case (3), the condition P can summarize the work that has been performed by the model checker before space-out, time-out, or giving up. If complete verification is necessary, then a different verification method or tool may be used to focus on the states that violate the condition. We give such conditions as input to a conditional model checker, such that the verification problem is restricted to the part of the state space that satisfies the condition. Our experiments show that repeated application of conditional model\u00a0\u2026", "num_citations": "11\n", "authors": ["278"]}
{"title": "Strategy Selection for Software Verification Based on Boolean Features\n", "abstract": " Software verification is the concept of determining, given an input program and a specification, whether the input program satisfies the specification or not. There are different strategies that can be used to approach the problem of software verification, but, according to comparative evaluations, none of the known strategies is superior over the others. Therefore, many tools for software verification leave the choice of which strategy to use up\u00a0to the user, which is problematic because the user might not be an expert on strategy selection. In the past, several learning-based approaches were proposed in order to perform the strategy selection automatically. This automatic choice can be formalized by a strategy selector, which is a function that takes as input a model of the given program, and assigns a verification strategy. The goal of this paper is to identify a small set of program features that (1)\u00a0can be statically\u00a0\u2026", "num_citations": "10\n", "authors": ["278"]}
{"title": "Co-change visualization applied to PostgreSQL and ArgoUML: (MSR challenge report)\n", "abstract": " Co-change visualization is a method to recover the subsystem structure of a software system from the version history, based on common changes and visual clustering. This paper presents the results of applying the tool CCVisu which implements co-change visualization, to the two open-source software systems PostgreSQL and ArgoUML The input of the method is the co-change graph, which can be easily extracted by CCVisu from a Cvs version repository. The output is a graph layout that places software artifacts that were often commonly changed at close positions, and artifacts that were rarely co-changed at distant positions. This property of the layout is due to the clustering property of the underlying energy model, which evaluates the quality of a produced layout. The layout can be displayed on the screen, or saved to a file in SVG or VRML format.", "num_citations": "10\n", "authors": ["278"]}
{"title": "Evaluating Software Verification Systems: Benchmarks and Competitions\n", "abstract": " This report documents the program and the outcomes of Dagstuhl Seminar 14171 \u201cEvaluating Software Verification Systems: Benchmarks and Competitions\u201d. The seminar brought together a large group of current and future competition organizers and participants, benchmark maintain- ers, as well as practitioners and researchers interested in the topic. The seminar was conducted as a highly interactive event, with a wide spectrum of contributions from participants, including talks, tutorials, posters, tool demstrations, hands-on sessions, and a live competition.", "num_citations": "9\n", "authors": ["278"]}
{"title": "DepDigger: A tool for detecting complex low-level dependencies\n", "abstract": " We present a tool that identifies complex data-flow dependencies on code-level, based on the measure dep-degree. Low-level dependencies between program operations are modeled by the use-def graph, which is generated from reaching definitions of variables. The tool annotates program operations with their dep-degree values, such that 'difficult' program operations are easy to locate. We hope that this tool helps detecting and preventing code degeneration, which is often a challenge in today's software projects, due to the high refactoring and restructuring frequency.", "num_citations": "9\n", "authors": ["278"]}
{"title": "CrocoPat: A Tool for Efficient Pattern Recognistion in Large Object Oriented Programs\n", "abstract": " Nowadays, software systems are too large to be understandable by reading the source code. For reengineering activities, methods and tools for automated design recovery are needed. CrocoPat is a tool for efficient pattern-based design analysis of object-oriented programs. Patterns can be flexibly specified by expressions based on standard mathematics. The software meta model is interpreted in terms of relations, and the patterns are described by relational expressions over these relations. The tool represents the abstract model of the program using a data structure based on binary decision diagrams for performance improvement. The representation is proved to allow for an efficient recognition also for large systems up to 10\u2019000 classes comprising several MLOC source code.", "num_citations": "9\n", "authors": ["278"]}
{"title": "A tool for modular modelling and verification of hybrid systems\n", "abstract": " A new modelling notation and a verification tool for hybrid systems is introduced: The Cottbus Timed Automaton (CTA). In contrast to existing modelling concepts, the new formalism has the advantage to be capable of modelling hybrid systems as a modular structure of components which communicate through the elements of an explicitly defined interface. The interface consists of signals and variables declared with different access modes. This paper describes how to describe a system and how to verify it. The current version of the tool uses the double description method to represent the regions.", "num_citations": "9\n", "authors": ["278"]}
{"title": "CPU Energy Meter: A tool for energy-aware algorithms engineering\n", "abstract": " Verification algorithms are among the most resource-intensive computation tasks. Saving energy is important for our living environment and to save cost in data centers. Yet, researchers compare the efficiency of algorithms still in terms of consumption of CPU time (or even wall time). Perhaps one reason for this is that measuring energy consumption of computational processes is not as convenient as measuring the consumed time and there is no sufficient tool support. To close this gap, we contribute CPU Energy Meter, a small tool that takes care of reading the energy values that Intel CPUs track inside the chip. In order to make energy measurements as easy as possible, we integrated CPU Energy Meter into BenchExec, a benchmarking tool that is already used by many researchers and competitions in the domain of formal methods. As evidence for usefulness, we explored the energy consumption of some state-of-the-art verifiers and report some interesting insights, for example, that energy consumption is not necessarily correlated with CPU time.", "num_citations": "8\n", "authors": ["278"]}
{"title": "TestCov: Robust test-suite execution and coverage measurement\n", "abstract": " We present TestCov, a tool for robust test-suite execution and test-coverage measurement on C programs. TestCov executes program tests in isolated containers to ensure system integrity and reliable resource control. The tool provides coverage statistics per test and for the whole test suite. TestCov uses the simple, XML -based exchange format for test-suite specifications that was established as standard by Test-Comp. TestCov has been successfully used in Test-Comp '19 to execute almost 9 million tests on 1720 different programs. The source code of TestCov is released under the open-source license Apache 2.0 and available at https://gitlab.com/sosy-lab/software/test-suite-validator. A full artifact, including a demonstration video, is available at https://doi.org/10.5281/zenodo.3418726.", "num_citations": "8\n", "authors": ["278"]}
{"title": "A light-weight approach for verifying multi-threaded programs with CPAchecker\n", "abstract": " Verifying multi-threaded programs is becoming more and more important, because of the strong trend to increase the number of processing units per CPU socket. We introduce a new configurable program analysis for verifying multi-threaded programs with a bounded number of threads. We present a simple and yet efficient implementation as component of the existing program-verification framework CPAchecker. While CPAchecker is already competitive on a large benchmark set of sequential verification tasks, our extension enhances the overall applicability of the framework. Our implementation of handling multiple threads is orthogonal to the abstract domain of the data-flow analysis, and thus, can be combined with several existing analyses in CPAchecker, like value analysis, interval analysis, and BDD analysis. The new analysis is modular and can be used, for example, to verify reachability properties as well as to detect deadlocks in the program. This paper includes an evaluation of the benefit of some optimization steps (e.g., changing the iteration order of the reachability algorithm or applying partial-order reduction) as well as the comparison with other state-of-the-art tools for verifying multi-threaded programs.", "num_citations": "7\n", "authors": ["278"]}
{"title": "MetaVal: Witness validation via verification\n", "abstract": " Witness validation is an important technique to increase trust in verification results, by making descriptions of error paths (violation witnesses) and important parts of the correctness proof (correctness witnesses) available in an exchangeable format. This way, the verification result can be validated independently from the verification in a second step. The problem is that there are unfortunately not many tools available for witness-based validation of verification results. We contribute to closing this gap with the approach of validation via verification, which is a way to automatically construct a set of validators from a set of existing verification engines. The idea is to take as input a specification, a program, and a verification witness, and produce a new specification and a transformed version of the original program such that the transformed program satisfies the new specification if the witness is useful to confirm the result\u00a0\u2026", "num_citations": "6\n", "authors": ["278"]}
{"title": "Explicit-value analysis based on CEGAR and interpolation\n", "abstract": " Abstraction, counterexample-guided refinement, and interpolation are techniques that are essential to the success of predicate-based program analysis. These techniques have not yet been applied together to explicit-value program analysis. We present an approach that integrates abstraction and interpolation-based refinement into an explicit-value analysis, i.e., a program analysis that tracks explicit values for a specified set of variables (the precision). The algorithm uses an abstract reachability graph as central data structure and a path-sensitive dynamic approach for precision adjustment. We evaluate our algorithm on the benchmark set of the Competition on Software Verification 2012 (SV-COMP'12) to show that our new approach is highly competitive. In addition, we show that combining our new approach with an auxiliary predicate analysis scores significantly higher than the SV-COMP'12 winner.", "num_citations": "6\n", "authors": ["278"]}
{"title": "First international competition on software testing\n", "abstract": " Tool competitions are a special form of comparative evaluation, where each tool has a team of developers or supporters associated that makes sure the tool is properly configured to show its best possible performance. In several research areas, tool competitions have been a driving force for the development of mature tools that represent the state of the art in their field. This paper describes and reports the results of the 1 International Competition on Software Testing (Test-Comp 2019), a comparative evaluation of automatic tools for software test generation. Test-Comp 2019 was presented as part of TOOLympics 2019, a satellite event of the conference TACAS. Nine test generators were evaluated on 2 356 test-generation tasks. There were two test specifications, one for generating a test that covers a particular function call and one for generating a test suite that tries to cover the branches of the program.", "num_citations": "5\n", "authors": ["278"]}
{"title": "Status report on software testing: Test-Comp 2021\n", "abstract": " This report describes Test-Comp 2021, the 3rd edition of the Competition on Software Testing. The competition is a series of annual comparative evaluations of fully automatic software test generators for C programs. The competition has a strong focus on reproducibility of its results and its main goal is to provide an overview of the current state of the art in the area of automatic test-generation. The competition was based on 3 173 test-generation tasks for C programs. Each test-generation task consisted of a program and a test specification (error coverage, branch coverage). Test-Comp 2021 had 11 participating test generators from 6 countries.", "num_citations": "5\n", "authors": ["278"]}
{"title": "Verification artifacts in cooperative verification: survey and unifying component framework\n", "abstract": " The goal of cooperative verification is to combine verification approaches in such a way that they work together to verify a system model. In particular, cooperative verifiers provide exchangeable information (verification artifacts) to other verifiers or consume such information from other verifiers with the goal of increasing the overall effectiveness and efficiency of the verification process. This paper first gives an overview over approaches for leveraging strengths of different techniques, algorithms, and tools in order to increase the power and abilities of the state of the art in software verification. To limit the scope, we restrict our overview to tools and approaches for automatic program analysis. Second, we specifically outline cooperative verification approaches and discuss their employed verification artifacts. Third, we formalize all artifacts in a uniform way, thereby fixing their semantics and providing verifiers with a\u00a0\u2026", "num_citations": "5\n", "authors": ["278"]}
{"title": "Software Verification with PDR: An Implementation of the State of the Art\n", "abstract": " Property-directed reachability (PDR) is a SAT/SMT-based reachability algorithm that incrementally constructs inductive invariants. After it was successfully applied to hardware model checking, several adaptations to software model checking have been proposed. We contribute a replicable and thorough comparative evaluation of the state of the art: We (1) implemented a standalone PDR algorithm and, as improvement, a PDR-based auxiliary-invariant generator for k-induction, and (2) performed an experimental study on the largest publicly available benchmark set of C verification tasks, in which we explore the effectiveness and efficiency of software verification with PDR. The main contribution of our work is to establish a reproducible baseline for ongoing research in the area by providing a well-engineered reference implementation and an experimental evaluation of the existing techniques.", "num_citations": "5\n", "authors": ["278"]}
{"title": "Domain-independent multi-threaded software model checking\n", "abstract": " Recent development of software aims at massively parallel execution, because of the trend to increase the number of processing units per CPU socket. But many approaches for program analysis are not designed to benefit from a multi-threaded execution and lack support to utilize multi-core computers. Rewriting existing algorithms is difficult and error-prone, and the design of new parallel algorithms also has limitations. An orthogonal problem is the granularity: computing each successor state in parallel seems too fine-grained, so the open question is to find the right structural level for parallel execution. We propose an elegant solution to these problems: Block summaries should be computed in parallel. Many successful approaches to software verification are based on summaries of control-flow blocks, large blocks, or function bodies. Block-abstraction memoization is a successful domain-independent approach\u00a0\u2026", "num_citations": "5\n", "authors": ["278"]}
{"title": "Interpolation for value analysis\n", "abstract": " Abstraction, counterexample-guided refinement, and interpolation are techniques that are essential to the success of predicate-based program analysis. These techniques have not yet been applied together to value analysis. We present an approach that integrates abstraction and interpolation-based refinement into a value analysis, i.e., a program analysis that tracks values for a specified set of variables (the precision).", "num_citations": "5\n", "authors": ["278"]}
{"title": "A formal evaluation of DepDegree based on weyuker's properties\n", "abstract": " Complexity of source code is an important characteristic that software engineers aim to quantify using static software measurement. Several measures used in practice as indicators for software complexity have theoretical flaws. In order to assess the quality of a software measure, Weyuker established a set of properties that an indicator for program-code complexity should satisfy. It is known that several well-established complexity indicators do not fulfill Weyuker's properties. As an``early achievement''in a larger project on evaluating software measures, we show that DepDegree, a measure for data-flow dependencies, satisfies all of Weyuker's properties.", "num_citations": "5\n", "authors": ["278"]}
{"title": "A formalism for modular modelling of hybrid systems\n", "abstract": " We present a formalism for modular modelling of hybrid systems, the Cottbus Timed Automata. For the theoretical basis, we build on work about timed and hybrid automata. We use concepts from concurrency theory to model communication of separately defined modules, but we extend these concepts to be able to express explicitly read-and write-access to signals and variables.", "num_citations": "5\n", "authors": ["278"]}
{"title": "Partial verification and intermediate results as a solution to combine automatic and interactive verification techniques\n", "abstract": " Many of the current verification approaches can be classified into automatic and interactive techniques, each having different strengths and weaknesses. Thus, one of the current open problems is to design solutions to combine the two approaches and accelerate technology transfer. We outline four existing techniques that might be able to contribute to combination solutions: (1)\u00a0Conditional model checking is a technique that gives detailed information (in form of a condition) about the verified state space, i.e., informs the user (or tools later in a tool chain) of the outcome. Also, it accepts as input detailed information (again as condition) about what the conditional model checker has to do. (2)\u00a0Correctness witnesses, stored in a machine-readable exchange format, contain (partial) invariants that can be used to prove the correctness of a system. For example, tools that usually expect invariants from the user can\u00a0\u2026", "num_citations": "4\n", "authors": ["278"]}
{"title": "Augmenting predicate analysis with auxiliary invariants\n", "abstract": " Predicate analysis is a common approach to software model checking. Abstractions of programs are computed out of predicates found with craig interpolation. The found interpolants are, however, in some cases not ideal, and lead to long-running verification runs. To reduce the reliance on interpolation this thesis evaluates the effects of using separately computed, auxiliary, invariants instead.Our work is based on the CPA concept, CPACHECKER and the Predicate CPA. It is split into two major parts, on the one hand we introduce a new algorithm for concurrent execution of several analysis in CPACHECKER, as well as communication between such analysis, and on the other hand we show how the Predicate CPA can be augmented with additional formulas in several ways. We chose to evaluate: appending invariants to the precision of the analysis and conjoining invariants either to the path formula or to the abstraction formula. The invariants we want to use are generated by some new approaches directly in CPACHECKER. They can be separated in two classes, on the one hand, the on-the-fly and lightweight invariant generation heuristics which try to find invariants for a certain given program location only, and on the other hand complete analyses, which results are then used for generating invariants for the whole program.", "num_citations": "4\n", "authors": ["278"]}
{"title": "Domain-type-guided refinement selection based on sliced path prefixes\n", "abstract": " Abstraction is a successful technique in software verification, and interpolation on infeasible error paths is a successful approach to automatically detect the right level of abstraction in counterexample-guided abstraction refinement. Because the interpolants have a significant influence on the quality of the abstraction, and thus, the effectiveness of the verification, an algorithm for deriving the best possible interpolants is desirable. We present an analysis-independent technique that makes it possible to extract several alternative sequences of interpolants from one given infeasible error path, if there are several reasons for infeasibility in the error path. We take as input the given infeasible error path and apply a slicing technique to obtain a set of error paths that are more abstract than the original error path but still infeasible, each for a different reason. The (more abstract) constraints of the new paths can be passed to a standard interpolation engine, in order to obtain a set of interpolant sequences, one for each new path. The analysis can then choose from this set of interpolant sequences and select the most appropriate, instead of being bound to the single interpolant sequence that the interpolation engine would normally return. For example, we can select based on domain types of variables in the interpolants, prefer to avoid loop counters, or compare with templates for potential loop invariants, and thus control what kind of information occurs in the abstraction of the program. We implemented the new algorithm in the open-source verification framework CPAchecker and show that our proof-technique-independent approach yields a significant\u00a0\u2026", "num_citations": "4\n", "authors": ["278"]}
{"title": "CheckDep: a tool for tracking software dependencies\n", "abstract": " Many software developers use a syntactical `diff' in order to performa quick review before committing changes to the repository. Others are notified of the change by e-mail (containing diffs or change logs), and they review the received information to determine if their work is affected. We lift this simple process from the code level to the more abstract level of dependencies: a software developer can use CheckDep to inspect introduced and removed dependencies before committing new versions, and other developers receive summaries of the changed dependencies via e-mail. We find the tool useful in our software-development activities and now make the tool publicly available.", "num_citations": "4\n", "authors": ["278"]}
{"title": "Violation witnesses and result validation for multi-threaded programs\n", "abstract": " Invariants and error traces are important results of a program analysis, and therefore, a standardized exchange format for verification witnesses is used by many program analyzers to store and share those results. This way, information about program traces and variable assignments can be shared across tools, e.g., to validate verification results, or provided to users, e.g., to visualize and explore the results in order to fix bugs or understand the reason for a program\u2019s correctness. The standard format for correctness and violation witnesses that was used by SV-COMP for several years was only applicable to sequential (single-threaded) programs. To enable the validation of results for multi-threaded programs, we extend the existing standard exchange format by adding information about thread management and thread interleaving. We contribute a reference implementation of a validator for violation witnesses in the\u00a0\u2026", "num_citations": "3\n", "authors": ["278"]}
{"title": "Conditional Testing\n", "abstract": " There are several powerful automatic testers available, each with different strengths and weaknesses. To immediately benefit from different strengths of different tools, we need to investigate ways for quick and easy combination of techniques. Until now, research has mostly investigated integrated combinations, which require extra implementation effort. We propose the concept of conditional testing and a set of combination techniques that do not require implementation effort: Different testers can be taken \u2018off the shelf\u2019 and combined in a way that they cooperatively solve the problem of test-case generation for a given input program and coverage criterion. This way, the latest advances in test-case generation can be combined without delay. Conditional testing passes the test goals that a first tester has covered to the next tester, so that the next tester does not need to repeat work (as in combinations without\u00a0\u2026", "num_citations": "3\n", "authors": ["278"]}
{"title": "Tools and Algorithms for the Construction and Analysis of Systems: 24th International Conference, TACAS 2018, Held as Part of the European Joint Conferences on Theory and\u00a0\u2026\n", "abstract": " computer architecture;  computer software selection and evaluation;  formal logic;  formal methods;  model checker;  model checking;  multi core processors;  program compilers;  programming languages;  semantics;  software engineering;  specifications;  state space;  verification", "num_citations": "3\n", "authors": ["278"]}
{"title": "JavaSMT 3: Interacting with SMT Solvers in Java\n", "abstract": " Satisfiability Modulo Theories (SMT) is an enabling technology with many applications, especially in computer-aided verification. Due to advances in research and strong demand for solvers, there are many SMT solvers available. Since different implementations have different strengths, it is often desirable to be able to substitute one solver by another. Unfortunately, the solvers have vastly different APIs and it is not easy to switch to a different solver (lock-in effect). To tackle this problem, we developed JavaSMT, which is a solver-independent framework that unifies the API for using a set of SMT solvers. This paper describes version\u00a03 of JavaSMT, which now supports eight SMT solvers and offers a simpler build and update process. Our feature comparisons and experiments show that different SMT solvers significantly differ in terms of feature support and performance characteristics. A unifying Java API for\u00a0\u2026", "num_citations": "2\n", "authors": ["278"]}
{"title": "Evaluating tools for software verification (track introduction)\n", "abstract": " Over the last several years, tools for program analysis and verification have became much more mature. There are now a number of competitions that evaluate and compare the implemented analyses for a given set of benchmarks. The comparison of the analyses either focuses on the analysis results themselves (verification of specified properties) or on the impact on a client analysis. This track is concerned with methods of evaluation for comparing analysis and verification techniques and how verified program properties can be represented such that they remain reproducible and reusable as intermediate results in the overall verification process (i.e., for other verification tools or verification steps).", "num_citations": "2\n", "authors": ["278"]}
{"title": "In-place vs. copy-on-write CEGAR refinement for block summarization with caching\n", "abstract": " Block summarization is an efficient technique in software verification to decompose a verification problem into separate tasks and to avoid repeated exploration of reusable parts of a program. In order to benefit from abstraction at the same time, block summarization can be combined with counterexample-guided abstraction refinement (CEGAR). This causes the following problem: whenever CEGAR instructs the model checker to refine the abstraction along a path, several block summaries are affected and need to be updated. There exist two different refinement strategies: a destructive in-place approach that modifies the existing block abstractions and a constructive copy-on-write approach that does not change existing data. While the in-place approach is used in the field for several years, our new approach of copy-on-write refinement has the following important advantage: A\u00a0complete exportable proof of\u00a0\u2026", "num_citations": "2\n", "authors": ["278"]}
{"title": "CPA-SymExec: efficient symbolic execution in CPAchecker\n", "abstract": " We present CPA-SymExec, a tool for symbolic execution that is implemented in the open-source, configurable verification framework CPAchecker. Our implementation automatically detects which symbolic facts to track, in order to obtain a small set of constraints that are necessary to decide reachability of a program area of interest. CPA-SymExec is based on abstraction and counterexample-guided abstraction refinement (CEGAR), and uses a constraint-interpolation approach to detect symbolic facts. We show that our implementation can better mitigate the path-explosion problem than symbolic execution without abstraction, by comparing the performance to the state-of-the-art Klee-based symbolic-execution engine Symbiotic and to Klee itself. For the experiments we use two kinds of analysis tasks: one for finding an executable path to a specific location of interest (eg, if a test vector is desired to show that a certain\u00a0\u2026", "num_citations": "2\n", "authors": ["278"]}
{"title": "Reusing precisions for efficient regression verification\n", "abstract": " Continuous testing during development is a well-established technique for software-quality assurance. Continuous model checking from revision to revision is not yet established as a standard practice, because the enormous resource consumption makes its application impractical. Model checkers compute a large number of verification facts that are necessary for verifying if a given specification holds. We have identified a category of such intermediate results that are easy to store and efficient to reuse: abstraction precisions. The precision of an abstract domain specifies the level of abstraction that the analysis works on. Precisions are thus a precious result of the verification effort and it is a waste of resources to throw them away after each verification run. In particular, precisions are small and thus easy to store; they are easy to process and have a large impact on resource consumption. We experimentally show the impact of precision reuse on industrial verification problems, namely, 59 device drivers with 1119 revisions from the Linux kernel.", "num_citations": "2\n", "authors": ["278"]}
{"title": "A comparative study of decision diagrams for real-time verification\n", "abstract": " In this paper we analyze the efficiency of binary decision diagrams (BDDs) and clock difference diagrams (CDDs) in the verification of timed automata. Therefore we present empirical and analytical complexity results for three communication protocols. The contributions of the analyses are: Firstly, they explain the empirical results. Secondly, they show that BDDs and CDDs of polynomial size exist for the reachability sets of the three protocols. Thirdly, they show that CDD-based tools, which currently use at least exponential space for two of the protocols, will only find polynomial-sized CDDs if they use better variable orders, as the BDD-based tool Rabbit does. Finally, they give insight into the dependency of the BDD and CDD size on properties of the model, in particular the number of automata and the magnitude of the clock values.", "num_citations": "2\n", "authors": ["278"]}
{"title": "Concepts of Cottbus Timed Automata.\n", "abstract": " Today, many industrial production cells are controlled by software. Many such systems have to deal with requirements which the developer has to guarantee. Because of the complexity of the implementation one of the main problems for developing the software for reactive systems is to be sure that such properties are fulfilled. One way to handle the problems is to use formal methods: This means to develop a formal model which is used to prove the properties of the specification with tool support.There are many different methods to model such reactive systems. Some of these abstract from real-time aspects of the system. We chose a problem area where we have real-time re-quirements, for example the throughput of the modelled production cell. So we have to use formal methods which support models of real-time systems. In the past we looked for automata-based approaches. We used the concepts of timed and", "num_citations": "2\n", "authors": ["278"]}
{"title": "Strategy Selection for Software Verification Based on Boolean Features\n", "abstract": " Software verification is the concept of determining, given an input program and a specification, whether the input program satisfies the specification or not. There are different strategies that can be used to approach the problem of software verification, but, according to comparative evaluations, none of the known strategies is superior over the others. Therefore, many tools for software verification leave the choice of which strategy to use up to the user, which is problematic because the user might not be an expert on strategy selection. In the past, several learning-based approaches were proposed in order to perform the strategy selection automatically. This automatic choice can be formalized by a strategy selector, which is a function that takes as input a model of the given program, and assigns a verification strategy. The goal of this paper is to identify a small set of program features that (1) can be statically determined for each input program in an efficient way and (2) sufficiently distinguishes the input programs such that a strategy selector for picking a particular verification strategy can be defined that outperforms every constant strategy selector. Our results can be used as a baseline for future comparisons, because our strategy selector is simple and easy to understand, while still powerful enough to outperform the individual strategies. We evaluate our feature set and strategy selector on a large set of 5 687 verification tasks and provide a replication package for comparative evaluation.", "num_citations": "2\n", "authors": ["278"]}
{"title": "Reliable and reproducible competition results with BenchExec and witnesses\n", "abstract": " The 5th Competition on Software Verification (SV-COMP 2016) continues the tradition of a thorough comparative evaluation of fully-automatic software verifiers. This report presents the results of the competition and includes a special section that describes how SV-COMP ensures that the experiments are reliably executed, precisely measured, and organized such that the results can be reproduced later. SV-COMP uses BenchExec for controlling and measuring the verification runs, and requires violation witnesses in an exchangeable format, whenever a verifier reports that a property is violated. Each witness was validated by two independent and publicly-available witness validators. The tables report the state of the art in software verification in terms of effectiveness and efficiency. The competition used 6661 verification tasks that each consisted of a C program and a property (reachability, memory safety, termination). SV-COMP 2016 had 35 participating verification systems (22 in 2015) from 16 countries.", "num_citations": "2\n", "authors": ["278"]}
{"title": "Cooperative verifier-based testing with CoVeriTest\n", "abstract": " Testing is a widely applied technique to evaluate software quality, and coverage criteria are often used to assess the adequacy of a generated test suite. However, manually constructing an adequate test suite is typically too expensive, and numerous techniques for automatic test-suite generation were proposed. All of them come with different strengths. To build stronger test-generation tools, different techniques should be combined. In this paper, we study cooperative combinations of verification approaches for test generation, which exchange high-level information. We present CoVeriTest, a hybrid technique for test-suite generation. CoVeriTest iteratively applies different conditional model checkers and allows users to adjust the level of cooperation and to configure individual time limits for each conditional model checker. In our experiments, we systematically study different CoVeriTest cooperation setups\u00a0\u2026", "num_citations": "1\n", "authors": ["278"]}
{"title": "An Interface Theory for Program Verification\n", "abstract": " Program verification is the problem, for a given program P and a specification \u03d5, of constructing a proof of correctness for the statement \u201cprogram P satisfies specification \u03d5\u201d(P \u22a8 \u03d5) or a proof of violation (). Usually, a correctness proof is based on inductive invariants, and a violation proof on a violating program trace. Verification engineers typically expect that a verification tool exports these proof artifacts. We propose to view the task of program verification as constructing a behavioral interface (represented eg by an automaton). We start with the interface I_ P of the program itself, which represents all traces of program executions. To prove correctness, we try to construct a more abstract interface I_ C of the program (overapproximation) that satisfies the specification. This interface, if found, represents more traces than I_ P that are all correct (satisfying the specification). Ultimately, we want a compact representation of\u00a0\u2026", "num_citations": "1\n", "authors": ["278"]}
{"title": "Software Verification with PDR: Implementation and Empirical Evaluation of the State of the Art\n", "abstract": " Property-directed reachability (PDR) is a SAT/SMT-based reachability algorithm that incrementally constructs inductive invariants. After it was successfully applied to hardware model checking, several adaptations to software model checking have been proposed. We contribute a replicable and thorough comparative evaluation of the state of the art: We (1) implemented a standalone PDR algorithm and, as improvement, a PDR-based auxiliary-invariant generator for k-induction, and (2) performed an experimental study on the largest publicly available benchmark set of C verification tasks, in which we explore the effectiveness and efficiency of software verification with PDR. The main contribution of our work is to establish a reproducible baseline for ongoing research in the area by providing a well-engineered reference implementation and an experimental evaluation of the existing techniques.", "num_citations": "1\n", "authors": ["278"]}
{"title": "A data set of program invariants and error paths\n", "abstract": " The analysis of correctness proofs and counterexamples of program source code is an important way to gain insights into methods that could make it easier in the future to find invariants to prove a program correct or to find bugs. The availability of high-quality data is often a limiting factor for researchers who want to study real program invariants and real bugs. The described data set provides a large collection of concrete verification results, which can be used in research projects as data source or for evaluation purposes. Each result is made available as verification witness, which represents either program invariants that were used to prove the program correct (correctness witness) or an error path to replay the actual bug (violation witness). The verification results are taken from actual verification runs on 10522 verification problems, using the 31 verification tools that participated in the 8th edition of the International\u00a0\u2026", "num_citations": "1\n", "authors": ["278"]}
{"title": "Interactive Visualization of Verification Results from CPAchecker with D3\n", "abstract": " CPAchecker is a tool for configurable software verification and is available for free under the Apache 2.0 License. It allows the verification of software that has been preprocessed with the C preprocessor. CPAchecker generates HTML report files depending on the verification outcome. The report includes graphical representations of the program flow and the reached abstract states, the source code of the program, the generated log entries and statistics as well as the used configuration options. In addition if an error is found by the analysis the generated counterexample report will include information about the program path leading to the error-the error path. This allows the user, by interacting with the generated report, to quickly and easily analyze the program and determine where exactly the error occurs and what its cause is. This document describes the latest implementation of the generated reports by CPAchecker which, as opposed to the previous solution, does not require the execution of an external script or the installation of an additional software. Additionally providing more interactive capabilities, such as pan and zoom functionalities for better graph readability and the option to display an abstract reachability graph containing only the error path, for the user. The solution uses the Dagre-D3 and D3 JavaScript libraries for graph creation and web workers for multithreading in JavaScript in order to be able to handle costly computational tasks on a background-running threads and thereby ensure performance and provide better user experience.", "num_citations": "1\n", "authors": ["278"]}
{"title": "Exchanging verification witnesses between verifiers\n", "abstract": " Standard verification tools provide a counterexample to witness a specifica- tion violation. Since a few years, such a witness can be validated by an independent validator using an exchangeable witness format. This way, information about the violation can be shared across verifiers and the user can use standard tools to visualize and explore witnesses. This technique is not yet established for the correctness case, where a program fulfills a specification. Even for simple programs, users often struggle to comprehend why a program is correct, and there is no way to independently check the verification result. We recently closed this gap by complementing our earlier work on violation witnesses with correctness witnesses. The overall goal to make proofs avail- able to engineers is probably as old as programming itself, and proof-carrying code was proposed two decades ago \u2014 our goal is to make it practical: We consider witnesses as first-class exchangeable objects, stored independently from the source code and checked independently from the verifier that produced them, respecting the principle of separation of concerns. At any time, the correctness witness can be used to recon- struct a correctness proof to establish trust. We extended two state-of-the-art verifiers, CPACHECKER and ULTIMATEAUTOMIZER, to produce and validate witnesses.", "num_citations": "1\n", "authors": ["278"]}
{"title": "SV-COMP 2019\n", "abstract": " This report describes the 2019 Competition on Software Verification (SV-COMP), the 8th edition of a series of comparative evaluations of fully automatic software verifiers for C programs, and now also for Java programs. The competition provides a snapshot of the current state of the art in the area, and has a strong focus on replicability of its results. The repository of benchmark verification tasks now supports a new, more flexible format for task definitions (based on YAML), which was a precondition for conveniently benchmarking Java programs in the same controlled competition setting that was successfully applied in the previous years. The competition was based on 10 522 verification tasks for C programs and 368 verification tasks for Java programs. Each verification task consisted of a program and a property (reachability, memory safety, overflows, termination). SV-COMP 2019 had 31 participating verification systems from 14 countries.", "num_citations": "1\n", "authors": ["278"]}
{"title": "Formal Techniques for Distributed Systems\n", "abstract": " This volume contains the proceedings of the 2013 IFIP Joint International Conference on Formal Techniques for Distributed Systems (33rd FORTE/15th FMOODS). The joint conference is the result of merging the two international conferences Formal Techniques for Networked and Distributed Systems (FORTE) and Formal Methods for Open Object-Based Distributed Systems (FMOODS). The city of Florence, Italy, was selected as the conference venue, taking place during June 3\u20135, 2013. This edition of the conference was organized as part of the 8th International Federated Conference on Distributed Computing Techniques (DisCoTec).The FORTE/FMOODS conference series represents a forum for fundamental research on theory, models, tools, and applications for distributed systems. The conference encourages contributions that combine theory and practice, and that exploit formal methods and theoretical\u00a0\u2026", "num_citations": "1\n", "authors": ["278"]}
{"title": "Different Strategies for BDD-based Reachability Analysis of Timed Automata\n", "abstract": " OPUS 4 | Different Strategies for BDD-based Reachability Analysis of Timed Automata Deutsch Open Access Home Search Browse Publish FAQ Different Strategies for BDD-based Reachability Analysis of Timed Automata Beyer, Dirk, Heinig, Andy Export metadata BibTeX RIS XML Additional Services Share in Twitter Search Google Scholar Metadaten Author: Beyer, Dirk, Heinig, Andy Title of the source (English): Proceedings of the 2nd IEEE/IFIP Joint Workshop on Formal Specifications of Computer-Based Systems (FSCBS 2001, Washington, DC, April 2001) Place of publication: Stirling Editor: Rozenblit, Jerzy, Sveda, Miroslav, Rattray, Charles Document Type: Conference Proceeding Language: English Year of publication: 2001 First Page: 89 Last Page: 98 Faculty/Chair: Fakult\u00e4t 1 MINT - Mathematik, Informatik, Physik, Elektro- und Informationstechnik / FG Praktische Informatik / Softwaresystemtechnik Institution \u2026", "num_citations": "1\n", "authors": ["278"]}
{"title": "Reachability analysis and refinement checking for BDD-based model checking of timed automata\n", "abstract": " OPUS 4 | Reachability Analysis and Refinement Checking for BDD-based Model Checking of Timed Automata Deutsch Open Access Home Search Browse Publish FAQ Reachability Analysis and Refinement Checking for BDD-based Model Checking of Timed Automata Beyer, Dirk Export metadata BibTeX RIS XML Additional Services Share in Twitter Search Google Scholar Metadaten Author: Beyer, Dirk Publisher: Inst. of Computer Science Place of publication: Cottbus Document Type: Report Language: English Year of publication: 2001 Number of pages: 12 Series ; volume number: Computer science reports ; 2001,04 Faculty/Chair: Fakult\u00e4t 1 MINT - Mathematik, Informatik, Physik, Elektro- und Informationstechnik / FG Praktische Informatik / Softwaresystemtechnik Institution name at the time of publication: Fakult\u00e4t f\u00fcr Mathematik, Naturwissenschaften und Informatik (eBTU) / LS Praktische Informatik / Software-\u2026", "num_citations": "1\n", "authors": ["278"]}
{"title": "BDD-basierte Verifikation von Realzeit-Systemen\n", "abstract": " Diese Arbeit behandelt die effiziente Erreichbarkeitsanalyse von Timed Automata. Wir beschreiben eine Erreichbarkeitseigenschaften erhaltende Diskretisierung der Zeit. Diese erm\u00f6glicht es, Konfigurationsmengen von Timed Automata als Binary Decision Diagrams (BDDs) darzustellen. Die kompakte BDD-Repr\u00e4sentation gro\u00dfer Mengen erfordert geeignete Variablenordnungen. Zur deren Bestimmung nutzen wir Strukturinformationen aus der Modellierungsnotation Cottbus Timed Automaton. Wir belegen die erzielten Effizienzverbesserungen durch Me\u00dfwerte.", "num_citations": "1\n", "authors": ["278"]}
{"title": "Ein Analysewerkzeug f\u00fcr zeitbehaftete Automaten\n", "abstract": " Bei der Modellierung von Systemen besteht ein wesentliches Problem in der Abstraktion von der realen Welt. Zur Modellierung von reaktiven Systemen existieren leistungsf\u00e4hige und einsatztaugliche Werkzeuge. Da\u00df bei vielen Werkzeugen jedoch von der Zeit abstrahiert werden mu\u00df, stellt einen wesentlichen Nachteil dieser Methoden dar, da zus\u00e4tzlich zur Korrektheit des Systems auch die Korrektheit der Abstraktion von der Zeit bewiesen werden mu\u00df.Am Lehrstuhl Software-Systemtechnik der BTU Cottbus wurden Methoden untersucht, die es erm\u00f6glichen, Realzeitsysteme zu modellieren, ohne von der Zeit abstrahieren zu m\u00fcssen. Dazu werden die Konzepte ausgehend von einigen bereits existierenden Methoden \u00fcberarbeitet und erweitert. Dem steigenden Bedarf an Steuerungssoftware f\u00fcr eingebettete Systeme entsprechend wird Wert auf die Modellierung von hybriden Systemen gelegt. Ziel ist die konzeptionelle Erarbeitung einer gut handhabbaren und die M\u00f6glichkeiten der automatischen Verifikation unterst\u00fctzenden Methode.", "num_citations": "1\n", "authors": ["278"]}
{"title": "Configurable software verification\n", "abstract": " In automatic software verification, we have observed a theoretical convergence of model checking and program analysis. In practice, however, model checkers are still mostly concerned with precision, eg, the removal of spurious counterexamples; for this purpose they build and refine reachability trees. Lattice-based program analyzers, on the other hand, are primarily concerned with efficiency. We designed an algorithm and built a tool that can be configured to perform not only a purely tree-based or a purely lattice-based analysis, but offers many intermediate settings that have not been evaluated before. The algorithm and tool take one or more abstract interpreters, such as a predicate abstraction and a shape analysis, and configure their execution and interaction using several parameters. Our experiments show that such customization may lead to dramatic improvements in the precision-efficiency spectrum.", "num_citations": "1\n", "authors": ["278"]}