{"title": "Classifying relations via long short term memory networks along shortest dependency paths\n", "abstract": " Relation classification is an important research arena in the field of natural language processing (NLP). In this paper, we present SDP-LSTM, a novel neural network to classify the relation of two entities in a sentence. Our neural architecture leverages the shortest dependency path (SDP) between two entities; multichannel recurrent neural networks, with long short term memory (LSTM) units, pick up heterogeneous information along the SDP. Our proposed model has several distinct features:(1) The shortest dependency paths retain most relevant information (to relation classification), while eliminating irrelevant words in the sentence.(2) The multichannel LSTM networks allow effective information integration from heterogeneous sources over the dependency paths.(3) A customized dropout strategy regularizes the neural network to alleviate overfitting. We test our model on the SemEval 2010 relation classification task, and achieve an F1-score of 83.7%, higher than competing methods in the literature.", "num_citations": "571\n", "authors": ["1463"]}
{"title": "Missing value estimation for mixed-attribute data sets\n", "abstract": " Missing data imputation is a key issue in learning from incomplete data. Various techniques have been developed with great successes on dealing with missing values in data sets with homogeneous attributes (their independent attributes are all either continuous or discrete). This paper studies a new setting of missing data imputation, i.e., imputing missing data in data sets with heterogeneous attributes (their independent attributes are of different types), referred to as imputing mixed-attribute data sets. Although many real applications are in this setting, there is no estimator designed for imputing mixed-attribute data sets. This paper first proposes two consistent estimators for discrete and continuous missing target values, respectively. And then, a mixture-kernel-based iterative estimator is advocated to impute mixed-attribute data sets. The proposed method is evaluated with extensive experiments compared with\u00a0\u2026", "num_citations": "303\n", "authors": ["1463"]}
{"title": "Improved relation classification by deep recurrent neural networks with data augmentation\n", "abstract": " Nowadays, neural networks play an important role in the task of relation classification. By designing different neural architectures, researchers have improved the performance to a large extent in comparison with traditional methods. However, existing neural networks for relation classification are usually of shallow architectures (e.g., one-layer convolutional neural networks or recurrent networks). They may fail to explore the potential representation space in different abstraction levels. In this paper, we propose deep recurrent neural networks (DRNNs) for relation classification to tackle this challenge. Further, we propose a data augmentation method by leveraging the directionality of relations. We evaluated our DRNNs on the SemEval-2010 Task~8, and achieve an F1-score of 86.1%, outperforming previous state-of-the-art recorded results.", "num_citations": "197\n", "authors": ["1463"]}
{"title": "Some discussion on the development of software technology\n", "abstract": " This paper presents a survey of the past development and current status of software technology, along the development of fundamental computing models for software. It also analyses the main characteristics of Internet and the requirements and challenges that Internet brings to software technology. Based on this analysis, a new concept of Internetware is presented. Internetware is a new class of software that is running on Internet and has distinct features from conventional software, Furthermore, the main research directions on Internetware are discussed.", "num_citations": "165\n", "authors": ["1463"]}
{"title": "Building program vector representations for deep learning\n", "abstract": " Deep learning has made significant breakthroughs in various fields of artificial intelligence. Advantages of deep learning include the ability to capture highly complicated features, weak involvement of human engineering, etc. However, it is still virtually impossible to use deep learning to analyze programs since deep architectures cannot be trained effectively with pure back propagation. In this pioneering paper, we propose the \"coding criterion\" to build program vector representations, which are the premise of deep learning for program analysis. Our representation learning approach directly makes deep learning a reality in this new field. We evaluate the learned vector representations both qualitatively and quantitatively. We conclude, based on the experiments, the coding criterion is successful in building program representations. To evaluate whether deep learning is beneficial for program analysis, we feed the representations to deep neural networks, and achieve higher accuracy in the program classification task than \"shallow\" methods, such as logistic regression and the support vector machine. This result confirms the feasibility of deep learning to analyze programs. It also gives primary evidence of its success in this new field. We believe deep learning will become an outstanding technique for program analysis in the near future.", "num_citations": "142\n", "authors": ["1463"]}
{"title": "Missing data imputation by utilizing information within incomplete instances\n", "abstract": " This paper proposes to utilize information within incomplete instances (instances with missing values) when estimating missing values. Accordingly, a simple and efficient nonparametric iterative imputation algorithm, called the NIIA method, is designed for iteratively imputing missing target values. The NIIA method imputes each missing value several times until the algorithm converges. In the first iteration, all the complete instances are used to estimate missing values. The information within incomplete instances is utilized since the second imputation iteration. We conduct some experiments for evaluating the efficiency, and demonstrate: (1) the utilization of information within incomplete instances is of benefit to easily capture the distribution of a dataset; and (2) the NIIA method outperforms the existing methods in accuracy, and this advantage is clearly highlighted when datasets have a high missing ratio.", "num_citations": "112\n", "authors": ["1463"]}
{"title": "Modeling and analyzing the reliability and cost of service composition in the IoT: A probabilistic approach\n", "abstract": " Recently, many efforts have been devoted to explore the integration of Internet of Things (IoT) and Service-Oriented Computing (SOC). These works allow the real-world devices to provide their functionality as web services. However, two important issues, unreliable service providing and resource constraints, make the modeling and analysis of service composition in IoT a big challenge. In this paper, we propose a probabilistic approach to formally describe and analyze the reliability and cost-related properties of the service composition in IoT. First, a service composition in IoT is modeled as a finite state machine (FSM) which focuses on the functional part. Then, we extend this FSM model to a Markov Decision Process (MDP), which can specify the reliability of service operations. Furthermore, we extend MDP with cost structure, which can represent the different service quality attributes for each operation, such as\u00a0\u2026", "num_citations": "88\n", "authors": ["1463"]}
{"title": "Ontology-Based Requirements Elicitation\n", "abstract": " This paper presents a new approach to support the elicitation process. Based on the enterprise information systems, this approach is using the enterprise ontology and the domain ontology as the requirements meta models to steer domain users describing their real systems systematically. By reusing the domain software models, the requirements model of the target system can be constructed automatically. Compared with the available approaches for requirements elicitation, the approach presented in the paper shows the following significant features. First, it interacts with domain users using domain terminology, not software terminology, so allows the involvement of domain users. Secondly, it is possible to guarantee a complete, consistent and unambiguous requirements specification under the guidance of these meta models. Finally, it supports the automated construction of application requirements models with the help of its rich knowledge.", "num_citations": "84\n", "authors": ["1463"]}
{"title": "Tree-based convolutional neural networks: principles and applications\n", "abstract": " In recent years, neural networks have become one of the most popular models in various applications of artificial intelligence, including image recognition, speech processing, and natural language processing. The convolutional neural network and the recurrent neural network are among the most popular neural architectures. The former uses a sliding window to capture translation invariant features; it typically works with signals in a certain dimensional space (eg, 1D speech or 2D image). The latter is suitable to process time-series data as it iteratively aggregates information.However, such models cannot explicitly incorporate more complicated structures, eg, the parse tree of a sentence. Socher et al. propose a recursive neural network that propagates information recursively bottom up along a tree structure. Although the recursive network can encode the tree structure to some extent, it has a long propagation path\u00a0\u2026", "num_citations": "77\n", "authors": ["1463"]}
{"title": "Constructing virtual domain ontologies based on domain knowledge reuse\n", "abstract": " A methodology is presented to construct virtual domain ontologies reusing other domain knowledge already available. This methodology uses the semantic relevance of existing domain models and domain ontologies, and proposes the possibility of building ontologies following the view of semantic match. Firstly, a structural definition of domain ontology is given. Secondly, the semantic relevance between domain model and ontologies is discussed and a definition of relevance degree is given. Based on the evolution of population, the domain ontologies constructing technologies in the terminology from genetics such as selection, clone, mutation, crossover, synthesis and transgenic are discussed. Finally, a VDO (virtual domain ontologies) constructing system is introduced, and a case study in the field of some hotels and travel agencies is demonstrated.", "num_citations": "74\n", "authors": ["1463"]}
{"title": "A systematic literature review of requirements modeling and analysis for self-adaptive systems\n", "abstract": " [Context and motivation] Over the last decade, researchers and engineers have developed a vast body of methodologies and technologies in requirements engineering for self-adaptive systems. Although existing studies have explored various aspects of this topic, few of them have categorized and evaluated these areas of research in requirements modeling and analysis. [Question/Problem] This review aims to investigate what modeling methods, RE activities, requirements quality attributes, application domains and research topics have been studied and how well these studies have been conveyed. [Principal ideas/results] We conduct a systematic literature review to answer the research questions by searching relevant studies, appraising the quality of these studies and extracting available data. The results are derived by synthesizing the extracted data with statistical methods. [Contributions] This\u00a0\u2026", "num_citations": "70\n", "authors": ["1463"]}
{"title": "Building toward capability specifications of web services based on an environment ontology\n", "abstract": " Automated Web service discovery requires Web service capability specifications of a high precision. Semantic-based approaches are inherently more precise than conventional keyword-based approaches. This paper proposes to build capability specifications of Web services based on an environment ontology, the main concepts of which are the environment entities in a particular application domain and their interactions. For each environment entity, there is a tree-like hierarchical state machine modeling the effects that are to be achieved by the Web services on this environment entity. The proposed approach is based on the assumption that the Web service capability specifications, built on the effects of the Web services on the environment entities, are more accessible and observable. Algorithms for constructing the domain environment ontology and the matchmaking between the Web service capability\u00a0\u2026", "num_citations": "69\n", "authors": ["1463"]}
{"title": "Requirement engineering in service-oriented system engineering\n", "abstract": " Service-oriented computing has received significant attentions recently, and many applications are being developed using this approach. Thus there is a need to analyze system requirements. This paper examines issues related to service-oriented requirement engineering (SORE). SORE focuses on modeling, specifying, and analyzing application requirements for software that will be developed in a service-oriented manner running in an SOA infrastructure. This paper also presents key features of SORE and some technical challenges.", "num_citations": "67\n", "authors": ["1463"]}
{"title": "A bdd-based approach to verifying clone-enabled feature models\u2019 constraints and customization\n", "abstract": " In this paper, we present a kind of semantics for constraints in clone-enabled feature models, which resolves the problem of what kinds of constraint should be added to a feature model after some features are cloned. The semantics is composed of two patterns: the generating pattern and the adapting pattern, to address the two problems of what kind of constraints should be imposed on a clonable feature and its clones, and how an existing constraint should be transformed in the context that features involved in the constraint are cloned, respectively. After that, we propose a BDD-based approach to verifying clone-enabled feature models, an approach that makes efficient use of the BDD (binary decision diagram) data structures, by considering the specific characteristics of feature models\u2019 verification. Experiments show that this BDD-based approach is more efficient and can verify more complex feature\u00a0\u2026", "num_citations": "64\n", "authors": ["1463"]}
{"title": "Service discovery for internet of things: a context-awareness perspective\n", "abstract": " In Internet of Things (IoT), functionalities of devices are encapsulated as real-world services to interact with other things or traditional web services to realize the seamless integration physical world with information world, where service discovery plays an important role. However, the resource-constrained and mobility natures of devices and intermittent disconnection of wireless network result in services in IoT having highly dynamic environment, which asks for different requirements for context-aware service discovery approach than traditional web services and brings new challenges for service discovery in IoT. In this paper, we analyze the role of context and relations with entities in IoT firstly; then combing the characteristics of data in IoT, a common ontology based context model with the ability to handle uncertainty and temporal aspects of context is proposed and Dynamic Bayesian networks (DBN) is adopted to\u00a0\u2026", "num_citations": "60\n", "authors": ["1463"]}
{"title": "Review on the study of entanglement in quantum computation speedup\n", "abstract": " The role the quantum entanglement plays in quantum computation speedup has been widely disputed. Some believe that quantum computation\u2019s speedup over classical computation is impossible if entanglement is absent, while others claim that the presence of entanglement is not a necessary condition for some quantum algorithms. This paper discusses this problem systematically. Simulating quantum computation with classical resources is analyzed and entanglement in known algorithms is reviewed. It is concluded that the presence of entanglement is a necessary but not sufficient condition in the pure state or pseudo-pure state quantum computation speedup. The case with the mixed state remains open. Further work on quantum computation will benefit from the presented results.", "num_citations": "60\n", "authors": ["1463"]}
{"title": "A syntax-based approach to measuring the degree of inconsistency for belief bases\n", "abstract": " Measuring the degree of inconsistency of a belief base is an important issue in many real-world applications. It has been increasingly recognized that deriving syntax sensitive inconsistency measures for a belief base from its minimal inconsistent subsets is a natural way forward. Most of the current proposals along this line do not take the impact of the size of each minimal inconsistent subset into account. However, as illustrated by the well-known Lottery Paradox, as the size of a minimal inconsistent subset increases, the degree of its inconsistency decreases. Another lack in current studies in this area is about the role of free formulas of a belief base in measuring the degree of inconsistency. This has not yet been characterized well. Adding free formulas to a belief base can enlarge the set of consistent subsets of that base. However, consistent subsets of a belief base also have an impact on the syntax sensitive\u00a0\u2026", "num_citations": "57\n", "authors": ["1463"]}
{"title": "A general framework for measuring inconsistency through minimal inconsistent sets\n", "abstract": " Hunter and Konieczny explored the relationships between measures of inconsistency for a belief base and the minimal inconsistent subsets of that belief base in several of their papers. In particular, an inconsistency value termed MIV                                            C                 , defined from minimal inconsistent subsets, can be considered as a Shapley Inconsistency Value. Moreover, it can be axiomatized completely in terms of five simple axioms. MinInc, one of the five axioms, states that each minimal inconsistent set has the same amount of conflict. However, it conflicts with the intuition illustrated by the lottery paradox, which states that as the size of a minimal inconsistent belief base increases, the degree of inconsistency of that belief base becomes smaller. To address this, we present two kinds of revised inconsistency measures for a belief base from its minimal inconsistent subsets. Each of these measures\u00a0\u2026", "num_citations": "55\n", "authors": ["1463"]}
{"title": "Coupling distributed and symbolic execution for natural language queries\n", "abstract": " Building neural networks to query a knowledge base (a table) with natural language is an emerging research topic in deep learning. An executor for table querying typically requires multiple steps of execution because queries may have complicated structures. In previous studies, researchers have developed either fully distributed executors or symbolic executors for table querying. A distributed executor can be trained in an end-to-end fashion, but is weak in terms of execution efficiency and explicit interpretability. A symbolic executor is efficient in execution, but is very difficult to train especially at initial stages. In this paper, we propose to couple distributed and symbolic execution for natural language queries, where the symbolic executor is pretrained with the distributed executor\u2019s intermediate execution results in a step-by-step fashion. Experiments show that our approach significantly outperforms both distributed and symbolic executors, exhibiting high accuracy, high learning efficiency, high execution efficiency, and high interpretability.", "num_citations": "46\n", "authors": ["1463"]}
{"title": "Domain modeling-based software engineering: a formal approach\n", "abstract": " Many approaches have been proposed to enhance software productivity and reliability. These approaches typically fall into three categories: the engineering approach, the formal approach, and the knowledge-based approach. The optimal gain in software productivity cannot be obtained if one relies on only one of these approaches. Thus, the integration of different approaches has also become a major area of research. No approach can be said to be perfect if it fails to satisfy the following two criteria. Firstly, a good approach should support the full life cycle of software development. Secondly, a good approach should support the development of large-scale software for real use in many application domains. Such an approach can be referred to as a five-in-one approach. The authors of this book have, for the past eight years, conducted research in knowledge-based software engineering, of which the final goal is to develop a paradigm for software engineering which not only integrates the three approaches mentioned above, but also fulfils the two criteria on which the five-in-one approach is based. Domain Modeling-Based Software Engineering: A Formal Approach explores the results of this research. Domain Modeling-Based Software Engineering: A Formal Approach will be useful to researchers of knowledge-based software engineering, students and instructors of computer science, and software engineers who are working on large-scale projects of software development and want to use knowledge-based development methods in their work.", "num_citations": "46\n", "authors": ["1463"]}
{"title": "Internetware computing: issues and perspective\n", "abstract": " The Internetware is a new initiative to develop software on the web for web applications. The open and dynamic nature of Internet applications suggest new ways of thinking will be needed for this initiative. This paper discusses several important issues in Internetware and put forward to some relevant research directions. The relevant issues include lifecycle models, ontology and context systems, modeling and simulation, social networking, and adaptive control.", "num_citations": "45\n", "authors": ["1463"]}
{"title": "Measuring the blame of each formula for inconsistent prioritized knowledge bases\n", "abstract": " It is increasingly recognized that identifying the degree of blame or responsibility of each formula for inconsistency of a knowledge base (i.e. a set of formulas) is useful for making rational decisions to resolve inconsistency in that knowledge base. Most current techniques for measuring the blame of each formula with regard to an inconsistent knowledge base focus on classical knowledge bases only. Proposals for measuring the blames of formulas with regard to an inconsistent prioritized knowledge base have not yet been given much consideration. However, the notion of priority is important in inconsistency-tolerant reasoning. This article investigates this issue and presents a family of measurements for the degree of blame of each formula in an inconsistent prioritized knowledge base by using the minimal inconsistent subsets of that knowledge base. First of all, we present a set of intuitive postulates as general\u00a0\u2026", "num_citations": "44\n", "authors": ["1463"]}
{"title": "Handling over-fitting in test cost-sensitive decision tree learning by feature selection, smoothing and pruning\n", "abstract": " Cost-sensitive learning algorithms are typically designed for minimizing the total cost when multiple costs are taken into account. Like other learning algorithms, cost-sensitive learning algorithms must face a significant challenge, over-fitting, in an applied context of cost-sensitive learning. Specifically speaking, they can generate good results on training data but normally do not produce an optimal model when applied to unseen data in real world applications. It is called data over-fitting. This paper deals with the issue of data over-fitting by designing three simple and efficient strategies, feature selection, smoothing and threshold pruning, against the TCSDT (test cost-sensitive decision tree) method. The feature selection approach is used to pre-process the data set before applying the TCSDT algorithm. The smoothing and threshold pruning are used in a TCSDT algorithm before calculating the class probability\u00a0\u2026", "num_citations": "42\n", "authors": ["1463"]}
{"title": "A reinforcement learning-based framework for the generation and evolution of adaptation rules\n", "abstract": " One of the challenges in self-adaptive systems concerns how to make adaptation to themselves at runtime in response to possible and even unexpected changes from the environment and/or user goals. A feasible solution to this challenge is rule-based adaptation, in which, adaptation decisions are made according to predefined rules that specify what particular actions should be performed to react to different changing events from the environment. Although it has the characteristic of highly- efficient decision making for adaptation, rule-based adaptation has two limitations: 1. no guarantee that those predefined rules will lead to optimal or nearly-optimal adaptation results; 2. weak support to evolve these rules to cope with non-stationary environment and changeable user goals at runtime. In this paper, we propose a reinforcement learning-based framework to the generation and evolution of software adaptation rules\u00a0\u2026", "num_citations": "38\n", "authors": ["1463"]}
{"title": "A use case based approach to feature models' construction\n", "abstract": " In the research of software reuse, feature models have been widely adopted to organize the requirements of a set of applications in a software domain. However, there still lacks an effective approach to minimizing analysts' participation in feature models' construction. In this paper, we propose a use case based semi-automatic approach to the construction of feature models. The basic idea of this approach is to first construct a set of feature models for individual applications(called application feature models, AFMs) in a software domain, then adjust, and merge the set of AFMs to form a feature model for this domain (called a domain feature model, DFM). The main characteristic of this approach is that it provides a set of rules and algorithms to make the construction of AFMs (from use cases) and the construction of DFMs (by merging a set of AFMs) be carried out automatically. A running example is used to illustrate the\u00a0\u2026", "num_citations": "37\n", "authors": ["1463"]}
{"title": "Modeling and verifying web services driven by requirements: An ontology-based approach\n", "abstract": " Automatic discovery and composition of Web services is an important research area in Web service technology, in which the specification of Web services is a key issue. This paper presents a Web service capability description framework based on the environment ontology. This framework depicts Web services capability in two aspects: the operable environment and the environment changes resulting from behaviors of the Web service. On the basis of the framework, a requirement-driven Web service composition model has been constructed. This paper brings forward the formalization of Web service interactions with \u03c0 calculus. And an automatic mechanism converting conceptual capability description to the formal process expression has been built. This kind of formal specification assists in verifying whether the composite Web service model matches the requirement.", "num_citations": "36\n", "authors": ["1463"]}
{"title": "Inconsistency measurement of software requirements specifications: An ontology-based approach\n", "abstract": " Management of requirements inconsistency is key to the development of complex trustworthy software system, and precise measurement is precondition for the management of requirements inconsistency properly. But at present, although there are a lot of work on the detection of requirements inconsistency, most of them are limited in treating requirements inconsistency according to heuristic rules, we still lacks of promising method for handling requirements inconsistency properly. Based on an abstract requirements refinement process model, this paper takes domain ontology as infrastructure for the refinement of software requirements, the aim of which is to get requirements descriptions that are comparable. Thus we can measure requirements inconsistency based on tangent plane of requirements refinement tree, after we have detected inconsistent relations of leaf nodes at semantic level.", "num_citations": "36\n", "authors": ["1463"]}
{"title": "Ontology-oriented requirements analysis\n", "abstract": " A new approach to requirements analysis, called OORA (ontology-oriented requirements analysis) is proposed. The most important feature of this approach is that various ontology have been introduced to represent relationships between objects and/or classes. These ontology may significantly enhance the expressiveness of the current OOA (object-oriented analysis) approaches. First, the OORA requirements model is designed. After that, two key types of ontology are illustrated by two typical use cases. Finally, the main phases in OORA are given.", "num_citations": "36\n", "authors": ["1463"]}
{"title": "Formal ontology: Foundation of domain knowledge sharing and reusing\n", "abstract": " Domain analysis is the activity of identifying and representing the relevant information in a domain, so that the information can be shared and reused in similar systems. But until now, no efficient approaches are available for capturing and representing the results of domain analysis and then for sharing and reusing the domain knowledge. This paper proposes an ontology-oriented approach for formalizing the domain models. The architecture for the multiple-layer structure of the domain knowledge base is also discussed. And finally, some genetic algorithm-based methods have been given for supporting the knowledge sharing and reusing.", "num_citations": "35\n", "authors": ["1463"]}
{"title": "Measuring inconsistency in requirements specifications\n", "abstract": " In the field of requirements engineering, measuring inconsistency is crucial to effective inconsistency management. A practical measure must consider both the degree and significance of inconsistency in specification. The main contribution of this paper is providing an approach for measuring inconsistent specification in terms of the priority-based scoring vector, which integrates the measure of the degree of inconsistency with the measure of the significance of inconsistency. In detail, for each specification \u0394 that consists of a set of requirements statements, if L is a m-level priority set, we define a m-dimensional priority-based significance vector  to measure the significance of the inconsistency in \u0394. Furthermore, a priority-based scoring vector :  (\u0394)\u2192 N                            m\u2009+\u20091 has been defined to provide an ordering relation over specifications that describes which specification is \u201cmore\u00a0\u2026", "num_citations": "34\n", "authors": ["1463"]}
{"title": "Revisiting the meaning of requirements\n", "abstract": " Understanding the meaning of requirements can help elicit the real world requirements and refine their specifications. But what do the requirements of a desired software mean is not a well-explained question yet though there are many software development methods available. This paper suggests that the meaning of requirements could be depicted by the will-to-be environments of the desired software, and the optative interactions of the software with its environments as well as the causal relationships among these interactions. This paper also emphasizes the necessity of distinguishing the external manifestation from the internal structure of each system component during the process of requirements decomposition and refinement. Several decomposition strategies have been given to support the continuous decomposition. The external manifestation and the internal structure of the system component\u00a0\u2026", "num_citations": "32\n", "authors": ["1463"]}
{"title": "An ontology of problem frames for guiding problem frame specification\n", "abstract": " Problem Frames approach is a new and prospective tool for classifying, analyzing and structuring software development problems. However, it has not yet been widely used mainly because lacking of CASE tools for guiding the problem frame specification development. This paper proposes an ontology based solution for this kind of CASE tools. An ontology of Problem Frames approach has been developed for this purpose. It specifies the basic terms elicited from Problem Frames approach and gives a concept model of this approach. This ontology can serve as the guidance of specifying the application problems. A case study has been given for illustration.", "num_citations": "31\n", "authors": ["1463"]}
{"title": "Mining globally interesting patterns from multiple databases using kernel estimation\n", "abstract": " When extracting knowledge (or patterns) from multiple databases, the data from different databases might be too large in volume to be merged into one database for centralized mining on one computer, the local information sources might be hidden from a global decision maker due to privacy concerns, and different local databases may have different contribution to the global pattern. Dealing with multiple databases is essentially different from mining from a single database. In multi-database mining, the global patterns must be obtained by carefully analyzing the local patterns from individual databases. In this paper, we propose a nonlinear method, named KEMGP (kernel estimation for mining global patterns), to tackle this problem, which adopts kernel estimation to synthesizing local patterns for global patterns. We also adopt a method to divide all the data in different databases according to attribute dimensionality\u00a0\u2026", "num_citations": "30\n", "authors": ["1463"]}
{"title": "Environment Modeling based Requirements Engineering for Software Intensive Systems\n", "abstract": " Environment Modeling-Based Requirements Engineering for Software Intensive Systems provides a new and promising approach for engineering the requirements of software-intensive systems, presenting a systematic, promising approach to identifying, clarifying, modeling, deriving, and validating the requirements of software-intensive systems from well-modeled environment simulations. In addition, the book presents a new view of software capability, ie the effect-based software capability in terms of environment modeling. Provides novel and systematic methodologies for engineering the requirements of software-intensive systems Describes ontologies and easily-understandable notations for modeling software-intensive systems Analyzes the functional and non-functional requirements based on the properties of the software surroundings Provides an essential, practical guide and formalization tools for the task of identifying the requirements of software-intensive systems Gives system analysts and requirements engineers insight into how to recognize and structure the problems of developing software-intensive systems", "num_citations": "29\n", "authors": ["1463"]}
{"title": "Recognizing entailment and contradiction by tree-based convolution\n", "abstract": " Recognizing entailment and contradiction between two sentences has wide applications in NLP. Traditional methods include featurerich classifiers or formal reasoning. However, they are usually limited in terms of accuracy and scope. Recently, the renewed prosperity of neural networks has made many improvements in a variety of NLP tasks. In our previous work, the tree-based convolutional neural network (TBCNN) has achieved high performance in several sentence-level classification tasks. But whether TBCNN is applicable to the recognition of entailment and contradiction between two sentences remains unknown. In this paper, we propose TBCNN-pair model to recognize entailment/contradiction. Experimental results on a large dataset verify the rationale of using TBCNN as the sentencelevel model; leveraging additional heuristics like element-wise product/difference further improves the accuracy. Our model outperforms existing sentence encoding-based approaches by a large margin.", "num_citations": "28\n", "authors": ["1463"]}
{"title": "Requirement specification in pseudo-natural language in PROMIS\n", "abstract": " PROMIS, a knowledge-based tool for automatically prototyping MIS provided by us, is intended to deliver over the task of the requirement specification to the end users. It provides a pseudo-natural language for the end users to describe their own needs. PROMIS can then convert their requirements into an internal schema and then a MIS prototype automatically with the help of an internal domain knowledge base. After presenting an overview of PROMIS, this paper mainly focuses on the design and implementation of the requirement specification environment for the pseudo-natural language, including the syntax and the semantics of the language, the understanding and the analysing of it and the resulting internal conceptual model of MIS.", "num_citations": "28\n", "authors": ["1463"]}
{"title": "Modeling and verifying services of Internet of Things based on timed automata\n", "abstract": " The modeling and verifying of Internet of Things(IOT) services is now an important aspect of IOT software design. First, we introduce the concept of environment entities that are used to describe both the attributes and behaviors of things in the physical world. Then the behaviors of an IOT service are specified by its interaction with the corresponding environment entities, these interactions lead to the expected changes on the entities, and show the effectiveness of the IOT services. Based on timed automata, an IOT services modeling approach is proposed, in which different kinds of environment entities and IOT services are all modeled as individual timed automata. All these timed automata come into a network that represents the communication and concurrency of the whole IOT system, in which, the running of the IOT services will be represented as some computation path in the network. Based on the proposed\u00a0\u2026", "num_citations": "27\n", "authors": ["1463"]}
{"title": "Automatically acquiring requirements of business information systems by reusing business ontology\n", "abstract": " Requirement elicitation is the most important parts in the development process of the Business Information Systems (BIS). Errors made at this stage are extremely expensive to correct when they are discovered during testing or during actual working. No ideal methods have been presented up to now. This paper proposes an approach for automatically acquiring the customers\u2019 requirement by reusing the domain knowledge. Our knowledge base contains a business ontology and a set of generic domain models. By using this approach, the customers can describe their business information in a restricted natural language which do not contain any software terminology. And then, the construction of the application requirement specification can be divided into: selection of the domain, creation of the instance concepts, inheritance of the attributes and production of the specification. The construction algorithm has been given in this paper.", "num_citations": "27\n", "authors": ["1463"]}
{"title": "Hierarchical RNN with static sentence-level attention for text-based speaker change detection\n", "abstract": " Speaker change detection (SCD) is an important task in dialog modeling. Our paper addresses the problem of text-based SCD, which differs from existing audio-based studies and is useful in various scenarios, for example, processing dialog transcripts where speaker identities are missing (eg, OpenSubtitle), and enhancing audio SCD with textual information. We formulate text-based SCD as a matching problem of utterances before and after a certain decision point; we propose a hierarchical recurrent neural network (RNN) with static sentence-level attention. Experimental results show that neural networks consistently achieve better performance than feature-based approaches, and that our attention-based model significantly outperforms non-attention neural networks.", "num_citations": "26\n", "authors": ["1463"]}
{"title": "A comparative study on regularization strategies for embedding-based neural networks\n", "abstract": " This paper aims to compare different regularization strategies to address a common phenomenon, severe overfitting, in embedding-based neural networks for NLP. We chose two widely studied neural models and tasks as our testbed. We tried several frequently applied or newly proposed regularization strategies, including penalizing weights (embeddings excluded), penalizing embeddings, re-embedding words, and dropout. We also emphasized on incremental hyperparameter tuning, and combining different regularizations. The results provide a picture on tuning hyperparameters for neural NLP models.", "num_citations": "26\n", "authors": ["1463"]}
{"title": "Compressing neural language models by sparse word representations\n", "abstract": " Neural networks are among the state-of-the-art techniques for language modeling. Existing neural language models typically map discrete words to distributed, dense vector representations. After information processing of the preceding context words by hidden layers, an output layer estimates the probability of the next word. Such approaches are time- and memory-intensive because of the large numbers of parameters for word embeddings and the output layer. In this paper, we propose to compress neural language models by sparse word representations. In the experiments, the number of parameters in our model increases very slowly with the growth of the vocabulary size, which is almost imperceptible. Moreover, our approach not only reduces the parameter space to a large extent, but also improves the performance in terms of the perplexity measure.", "num_citations": "25\n", "authors": ["1463"]}
{"title": "Handling inconsistency in distributed software requirements specifications based on prioritized merging\n", "abstract": " Developing a desirable framework for handling inconsistencies in software requirements specifications is a challenging problem. It has been widely recognized that the relative priority of", "num_citations": "23\n", "authors": ["1463"]}
{"title": "Generating adversarial examples for holding robustness of source code processing models\n", "abstract": " Automated processing, analysis, and generation of source code are among the key activities in software and system lifecycle. To this end, while deep learning (DL) exhibits a certain level of capability in handling these tasks, the current state-of-the-art DL models still suffer from non-robust issues and can be easily fooled by adversarial attacks.", "num_citations": "22\n", "authors": ["1463"]}
{"title": "Toward better summarizing bug reports with crowdsourcing elicited attributes\n", "abstract": " Recent years have witnessed the growing demands for resolving numerous bug reports in software maintenance. Aiming to reduce the time testers/developers take in perusing bug reports, the task of bug report summarization has attracted a lot of research efforts in the literature. However, no systematic analysis has been conducted on attribute construction, which heavily impacts the performance of supervised algorithms for bug report summarization. In this study, we first conduct a survey to reveal the existing methods for attribute construction in mining software repositories. Then, we propose a new method named Crowd-Attribute to infer new effective attributes from the crowd-generated data in crowdsourcing and develop a new tool named Crowdsourcing Software Engineering Platform to facilitate this method. With Crowd-Attribute, we successfully construct 11 new attributes and propose a new supervised\u00a0\u2026", "num_citations": "22\n", "authors": ["1463"]}
{"title": "Smart contract-based negotiation for adaptive QoS-aware service composition\n", "abstract": " Smart contracts (SCs) run on the distributed ledger technology (DLT) platform and can implement agreements between participants without a trusted third party. This paper uses the DLT and SC techniques to build distributed applications composed of existing services. In practice, there are many functionally-equivalent services on the Internet. To beat their competitors, the service providers usually offer flexible QoS and use dynamic pricing strategies. Moreover, the service providers can change at runtime, e.g., they may encounter problems so that their QoS drops suddenly. This makes achieving the optimization goal at runtime (e.g., the maximization of the utility) more difficult. To address this problem, first, this paper proposes an SC-based negotiation framework. The SCs can ensure that the transactions are automatically and reliably performed as agreed upon between the service requesters and providers. The\u00a0\u2026", "num_citations": "21\n", "authors": ["1463"]}
{"title": "Mining binary constraints in the construction of feature models\n", "abstract": " Feature models provide an effective way to organize and reuse requirements in a specific domain. A feature model consists of a feature tree and cross-tree constraints. Identifying features and then building a feature tree takes a lot of effort, and many semi-automated approaches have been proposed to help the situation. However, finding cross-tree constraints is often more challenging which still lacks the help of automation. In this paper, we propose an approach to mining cross-tree binary constraints in the construction of feature models. Binary constraints are the most basic kind of cross-tree constraints that involve exactly two features and can be further classified into two sub-types, i.e. requires and excludes. Given these two sub-types, a pair of any two features in a feature model falls into one of the following classes: no constraints between them, a requires between them, or an excludes between them. Therefore\u00a0\u2026", "num_citations": "21\n", "authors": ["1463"]}
{"title": "Evolving quantum circuits at the gate level with a hybrid quantum-inspired evolutionary algorithm\n", "abstract": " This paper proposes an approach to evolve quantum circuits at the gate level, based on a hybrid quantum-inspired evolutionary algorithm. This approach encodes quantum gates as integers and combines the cost and correctness of quantum circuits into the fitness function. A fast algorithm of matrix multiplication with Kronecker product has been proposed to speed up the calculation of matrix multiplication in individuals evaluation. This algorithm is shown to be better than the known best algorithm for matrix multiplication when a certain condition holds. The approach of evolving quantum circuits is validated by some experiments and the effects of some parameters are investigated. And finally, some features of the approach are also discussed.", "num_citations": "21\n", "authors": ["1463"]}
{"title": "Towards automatic problem decomposition: an ontology-based approach\n", "abstract": " In this paper, we propose a conceptual description schema based on the Problem Frames (PF) approach, which treats environment as a first-class concept. Requirements are defined as problem descriptions in terms of the environment model. Thus, knowledge about the environment can be used to facilitate the derivation of software specifications from requirements. Heuristic rules that help structuring software problems are given. Major idea of the proposed approach is illustrated with a simple real world example.", "num_citations": "21\n", "authors": ["1463"]}
{"title": "Automatically multi-paradigm requirements modeling and analyzing: An ontology-based approach\n", "abstract": " There are several purposes for modeling and analyzing the problem domain before starting the software requirements analysis. First, it focuses on the problem domain, so that the domain users could be involved easily. Secondly, a comprehensive description on the problem domain will advantage getting a comprehensive software requirements model. This paper proposes an ontology-based approach for modeling the problem domain. It interacts with the domain users by using terminology that they can understand and guides them to provide the relevant information. A multiple paradigm analysis approach, with the basis of the description on the problem domain, has also been presented. Three criteria, i.e. the rationality of organization structure, the achievability of organization goals, and the feasibility of organization process, have been proposed. The results of the analysis could be used as feedbacks for\u00a0\u2026", "num_citations": "21\n", "authors": ["1463"]}
{"title": "Extending the problem frames approach for capturing non-functional requirements\n", "abstract": " The Problem Frames (PF) approach is a prospective tool for classifying, analyzing and structuring software development problems. However, it at current stage lacks a practical way to capture NFRs. This paper aims to extend the PF Approach for helping system analysts to capture non-functional requirements within the PF approach systematically. A new problem model and a model-guided process have been presented. NFRs specifications can be obtained according to that process. A case study shows the feasibility of this approach.", "num_citations": "19\n", "authors": ["1463"]}
{"title": "An Approach for Specifying Capability ofWeb Services based on Environment Ontology\n", "abstract": " Capability specification is key problem for Web service discovery. Conventional one-step process based capability specification has its limitations. This paper proposes an approach for semantic behavior-based capability specification of Web service to stride over the limitations. Meta-level environment ontology is proposed to provide formal and sharable specifications of environment resources in a particular domain. For each environment resource, there is a corresponding hierarchical state machine specifying its dynamic characteristics. Then, effects on the environment resources are modelled with the hierarchical state machines. On the basis of the environment ontology, forest-structured communicating hierarchical state machines (FCHM) are defined and expected to be semantics of capability specification of Web services, which can be derived from the effects that Web services impose on their environments", "num_citations": "19\n", "authors": ["1463"]}
{"title": "QoS-aware service composition using blockchain-based smart contracts\n", "abstract": " Smart contracts that run on blockchains can ensure the transactions are automatically, reliably performed as agreed upon between the participants without a trusted third party. In this work, we propose a smart-contract based algorithm for constructing service-based systems through the composition of existing services.", "num_citations": "18\n", "authors": ["1463"]}
{"title": "Integrating goals and problem frames in requirements analysis\n", "abstract": " We propose to integrate goal-oriented requirements language i* with concepts from problem frames (PF). The integration of goal and problem makes it possible to address designer's subjective intentions and the physical constraints from the environment at the same time. We illustrate the proposed approach with the meeting scheduler example", "num_citations": "18\n", "authors": ["1463"]}
{"title": "Automated requirements elicitation: Combining a model-driven approach with concept reuse\n", "abstract": " Extracting pertinent and useful information from customers has long plagued the process of requirements elicitation. This paper presents a new approach to support the elicitation process. This approach combines various techniques for requirements elicitation which include model-based concept acquisition, goal-driven structured interview and concept reuse. Compared with the available approaches for requirements elicitation, the most significant feature of our approach is that it supports both the automation of interaction with customers by using domain terminology, not software terminology and the automated construction of application requirements models using model-based concept elicitation and concept reuse. The capacity of this approach comes from its rich knowledge which is clustered into several abstract levels.", "num_citations": "18\n", "authors": ["1463"]}
{"title": "A formalism for extending the NFR Framework to support the composition of the goal trees\n", "abstract": " Non-functional requirements are considered as vital factors for producing software of better quality. As a widely adopted non-functional requirements analysis framework, the NFR Framework provides a graphical treatment for goal refinement and evaluation. This paper propose a logical representation for the NFR Framework. In addition, an NFR extension operator and an accompanying extension function for composing multiple viewpoints of NFR are proposed. Compared with the graphical NFR Framework, the logical approach further enhances the reasoning capabilities of the NFR framework. A real-world example has been used to illustrate the formal approach.", "num_citations": "17\n", "authors": ["1463"]}
{"title": "Why do neural dialog systems generate short and meaningless replies? a comparison between dialog and translation\n", "abstract": " This paper addresses the question: In neural dialog systems, why do sequence-to-sequence (Seq2Seq) neural networks generate short and meaningless replies for open-domain response generation? We conjecture that in a dialog system, due to the randomness of spoken language, there may be multiple equally plausible replies for one utterance, causing the deficiency of a Seq2Seq model. To evaluate our conjecture, we propose a systematic way to mimic the dialog scenario in machine translation systems with both real datasets and toy datasets generated elaborately. Experimental results show that we manage to reproduce the phenomenon of generating short and meaningless sentences in the translation setting.", "num_citations": "16\n", "authors": ["1463"]}
{"title": "Poet: Privacy on the edge with bidirectional data transformations\n", "abstract": " Comprehensive privacy mechanisms are essential in the pervasive internet-of-things systems of today, which are comprised of multiple distributed devices and diverse software stacks, while located in different legal or administrative domains. In such systems, often consisting of resource-constrained devices, guarantees of correctness and conformance to privacy policies is required, while data need to be synchronized among different software components. Motivated by the \"data protection by design and by default\" principle, we propose a technical framework to support data synchronization among edge components tailored for pervasive IoT applications. Our privacy-driven synchronization approach is based on a generically applicable privacy model and able to capture roles and permissions, actions on data, conditions and obligations that arise in privacy requirements. For automated and correct reflection of\u00a0\u2026", "num_citations": "16\n", "authors": ["1463"]}
{"title": "TDL: a transformation description language from feature model to use case for automated use case derivation\n", "abstract": " Software product line engineering (SPLE) is a widely adopted approach to systematic software reuse. One basic research issue in SPLE is the product derivation problem, which focuses on how to derive software products from reusable software assets efficiently. In this paper, we focus on a sub-problem of product derivation: the problem of automated use case derivation, ie deriving the use cases of a software product in an automated way. We take a feature-oriented approach to this problem, an approach involving two components: a feature model, and the transformation information from the feature model to a set of use cases. In particular, we propose a transformation description language (TDL) to specify the transformation information from a feature model to a set of related use cases, and to support automated derivation of use cases corresponding to a valid feature model configuration. In addition, we also\u00a0\u2026", "num_citations": "16\n", "authors": ["1463"]}
{"title": "Preliminary study of service discovery in Internet of things: Feasibility and limitation of SOA\n", "abstract": " In IoT(Internet of things) based on SOA(service-oriented architecture), functionalities of devices are encapsulated as services with a unified and common interface to be provided to the outside world. However, due to the massive, mobility and highly resource-constrained natures of devices, the unreliability of wireless network in IoT, services provided by devices have different characteristics with traditional Web services, and existing Web service discovery approaches can't satisfy the requirements of service discovery in IoT. This paper introduces some typical Web service discovery approaches firstly from two aspects: Web service discovery architecture and Web service matchmaking strategy. Then combining characteristics of service providing in IoT, it analyzes the problems of Web service discovery in IoT and possible solutions from four aspects: scalability, limited resource, heterogeneity and dynamic environment\u00a0\u2026", "num_citations": "16\n", "authors": ["1463"]}
{"title": "Managing software requirements changes based on negotiation-style revision\n", "abstract": " For any proposed software project, when the software requirements specification has been established, requirements changes may result in not only a modification of the requirements specification but also a series of modifications of all existing artifacts during the development. Then it is necessary to provide effective and flexible requirements changes management. In this paper, we present an approach to managing requirements changes based on Booth's negotiation-style framework for belief revision. Informally, we consider the current requirements specification as a belief set about the system-to-be. The request of requirements change is viewed as new information about the same system-to-be. Then the process of executing the requirements change is a process of revising beliefs about the system-to-be. We design a family of belief negotiation models appropriate for different processes of requirements\u00a0\u2026", "num_citations": "16\n", "authors": ["1463"]}
{"title": "From knowledge based software engineering to knowware based software engineering\n", "abstract": " The first part of this paper reviews our efforts on knowledge-based software engineering, namely PROMIS, started from 1990s. The key point of PROMIS is to generate applications automatically based on domain knowledge as well as software knowledge. That is featured by separating the development of domain knowledge from the development of software. But in PROMIS, we did not find an appropriate representation for the domain knowledge. Fortunately, in our recent work, we found such a carrier for knowledge modules, i.e. knowware. Knowware is a commercialized form of domain knowledge. This paper briefly introduces the basic definitions of knowware, knowledge middleware and knowware engineering. Three life circle models of knowware engineering and the design of corresponding knowware implementations are given. Finally we discuss application system automatic generation and domain\u00a0\u2026", "num_citations": "16\n", "authors": ["1463"]}
{"title": "Ontology-based inconsistency management of software requirements specifications\n", "abstract": " Management of requirements inconsistency is key to the development of trustworthy software systems. But at present, although there are a lot of work on this topic, most of them are limited in treating inconsistency at the syntactic level. We still lack a systematical method for managing requirements inconsistency at the semantic level.               This paper first proposes a requirements refinement model, which suggests that interactions between software agents and their ambiences are essential to capture the semantics of requirements. We suppose that the real effect of these interactions is to make the states of entities in the ambiences changed. So, we explicitly represent requirements of a software agent as a set of state transition diagrams, each of which is for one entity in the ambiences. We argue that, based on this model, the mechanism to deal with the inconsistency at the semantic level. A domain ontology\u00a0\u2026", "num_citations": "16\n", "authors": ["1463"]}
{"title": "Codesum: Translate program language to natural language\n", "abstract": " During software maintenance, programmers spend a lot of time on code comprehension. Reading comments is an effective way for programmers to reduce the reading and navigating time when comprehending source code. Therefore, as a critical task in software engineering, code summarization aims to generate brief natural language descriptions for source code. In this paper, we propose a new code summarization model named CodeSum. CodeSum exploits the attention-based sequence-to-sequence (Seq2Seq) neural network with Structure-based Traversal (SBT) of Abstract Syntax Trees (AST). The AST sequences generated by SBT can better present the structure of ASTs and keep unambiguous. We conduct experiments on three large-scale corpora in different program languages, i.e., Java, C#, and SQL, in which Java corpus is our new proposed industry code extracted from Github. Experimental results show that our method CodeSum outperforms the state-of-the-art significantly.", "num_citations": "15\n", "authors": ["1463"]}
{"title": "Modeling and specifying parametric adaptation mechanism for self-adaptive systems\n", "abstract": " Self-adaptive system (SAS) is capable of adjusting its behavior to cope with changes in the deployed environment. Parametric adaptation is an important fashion for achieving adaptation. Context can be defined as the reification of the environment. It may influence the decisions on how to adjust the system behavior. Thus, how to incorporate context into the parametric adaptation mechanism becomes a challenging issue. This paper provides solutions to this issue from a requirements engineering perspective. We develop the goal-oriented requirements model for SASs and build the context model for the environment, and then integrate these two models via defined relations. Adaptation goal model is derived by refining the adaptation goal with adaptation tasks which are underpinned by MAPE loop. Finally, we show how to utilize the specifications to design parametric adaptation algorithms. Our approach is\u00a0\u2026", "num_citations": "15\n", "authors": ["1463"]}
{"title": "Dptool: A tool for supporting the problem description and projection\n", "abstract": " Problem Frames (PF) approach is prospective for describing and analyzing software problems. Problem decomposition is fundamental for managing problem size and complexity in RE and `projection' has been argued to be an effective technique for analyzing and decomposing complex problems in PF. However, problem analysis in PF approach is still an empirical, tedious, and subjective process, and it heavily depends on the analysts' experiences. A suitable tool is highly demanded for providing guidance of the problem description and projection. This paper gives a supporting tool (named DPTool) for describing the problem and performing the problem projection.", "num_citations": "15\n", "authors": ["1463"]}
{"title": "Beyond knowledge engineering\n", "abstract": " Knowledge engineering stems from E. A. Figenbaum\u2019s proposal in 1977, but it will enter a new decade with the new challenges. This paper first summarizes three knowledge engineering experiments we have undertaken to show possibility of separating knowledge development from intelligent software development. We call it the ICAX mode of intelligent application software generation. The key of this mode is to generate knowledge base, which is the source of intelligence of ICAX software, independently and parallel to intelligent software development. That gives birth to a new and more general concept \u201cknowware\u201d. Knowware is a commercialized knowledge module with documentation and intellectual property, which is computer operable, but free of any built-in control mechanism, meeting some industrial standards and embeddable in software/hardware. The process of development, application and\u00a0\u2026", "num_citations": "15\n", "authors": ["1463"]}
{"title": "Modeling timing requirements in problem frames using CCSL\n", "abstract": " As the embedded systems are becoming more and more complex, requirements engineering approaches are needed for modeling requirements, especially the timing requirements. Among various requirements engineering approaches, the Problem Frames(PF) approach is particularly useful in requirements modeling for the embedded systems due to the characteristic that the PF pays special attention to the environment entities that will interact with the to-be software. However, no concern is given on timing requirements of the PF at present. This paper studies how to add timing constraints on problem domains in the PF. Our approach is to integrate the problem representation frame in the PF with the timing representation mechanism of MARTE(Modeling and Analysis of Real Time and Embedded systems). A unified problem frame modeling process integrated with timing constraints is provided, and problem frame\u00a0\u2026", "num_citations": "14\n", "authors": ["1463"]}
{"title": "Trust analysis of web services based on a trust ontology\n", "abstract": " This paper proposes a formalism for the Trust requirements modeling framework, which can be used as a means of studying the trustworthiness of service-oriented environments. We argue that a modeling framework, representing the underlying concepts and formal reasoning rules, can describe the Trust domain as well as support the dynamic analysis of trustworthiness. This model offers better understanding to the Trust relationships and will assist individuals in making rational decisions by computing trust level of potential interactors.", "num_citations": "14\n", "authors": ["1463"]}
{"title": "IntelliMerge: a refactoring-aware software merging technique\n", "abstract": " In modern software development, developers rely on version control systems like Git to collaborate in the branch-based development workflow. One downside of this workflow is the conflicts occurred when merging contributions from different developers: these conflicts are tedious and error-prone to be correctly resolved, reducing the efficiency of collaboration and introducing potential bugs. The situation becomes even worse, with the popularity of refactorings in software development and evolution, because current merging tools (usually based on the text or tree structures of source code) are unaware of refactorings. In this paper, we present IntelliMerge, a graph-based refactoring-aware merging algorithm for Java programs. We explicitly enhance this algorithm's ability in detecting and resolving refactoring-related conflicts. Through the evaluation on 1,070 merge scenarios from 10 popular open-source Java\u00a0\u2026", "num_citations": "13\n", "authors": ["1463"]}
{"title": "Environment based modeling approach for services in the Internet of Things\n", "abstract": " Services in Internet of Things (IoT services) are extensions of traditional Web services. Through sensor networks, IoT services can communicate directly with the entities in the physical environment, and even impact them directly. So, because of the influence of the physical environment (such as the time limitation, the resource limitation and the equipment fault probability), features of IoT services are more easily affected by the physical environment than the traditional Web service. In this case, the response time, the service consumption and the fault-tolerant ability become important factors in the performance of the whole IoT system. Therefore, modeling the IoT services detailedly and formally is very important for the analysis and prediction of the performance of the whole IoT system. In this paper, we propose an environment based IoT service modeling framework as well as an ontology of the IoT service and a triple-element model, and advance the corresponding modeling principle. Based on this framework, we proposed a detailed IoT model method, in which the IoT services and their corresponding environment can be described in probabilistic timed automata. The users desired IoT services features can be described in the temporal logic formula. With our method, the validity of the whole system and the satisfiability of its non-functional constraints can be analyzed detailedly and formally.", "num_citations": "13\n", "authors": ["1463"]}
{"title": "Feature-oriented stigmergy-based collaborative requirements modeling: an exploratory approach for requirements elicitation and evolution based on web-enabled collective\u00a0\u2026\n", "abstract": " Compared with traditional software applications, the requirements problem of Internetware applications exhibits a set of new characteristics that cannot be resolved by traditional engineer-centered face-to-face requirements methods. In this paper, we present an exploratory approach to the requirements elicitation and evolution problem of Internetware applications, based on the concept of web-enable collective intelligence, to accommodate the situation of large-scale user communities of Internetware applications with diverse and constantly evolving requirements. In particular, we propose a feature-oriented stigmergy-based collaborative requirements modeling method that combines feature-oriented requirements modeling with web-enabled stigmergy-based collaboration to support large-scale collaborative requirements eliciting and evolving activities for Internetware applications. Two experiments are\u00a0\u2026", "num_citations": "13\n", "authors": ["1463"]}
{"title": "Mining class-bridge rules based on rough sets\n", "abstract": " A class-bridge rule is the rule whose antecedent and consequent belong to different conceptual classes. This kind of rules stands for the correlation between conceptual classes. The study on class-bridge rules can benefit many domains such as criminal detection, chemical synthesis, biological grafting, etc. Class-bridge rules are different from other association rules as follows: (1) class-bridge rules can be generated from infrequent itemsets; (2) measurements of class-bridge rules include the distance between conceptual classes and the relation between the antecedents/consequents and their affiliated conceptual classes. This paper proposes an algorithm based on rough sets to mine and evaluate class-bridge rules. The experiment demonstrates its effectiveness.", "num_citations": "13\n", "authors": ["1463"]}
{"title": "A modeling approach for service-oriented application based on extensive reuse\n", "abstract": " This paper proposes an extensive reuse approach for SOM, which utilizes a multi-facets ontology system supporting reuse of various service-oriented assets, such as business processes, collaboration templates and services. The paper also presents an iterative service-oriented modeling process based on the ontology and assets repository, which reuses assets from the first to last modeling phase by matching between assets descriptions, and results in a service model that represents the specification of the required service-oriented application.", "num_citations": "13\n", "authors": ["1463"]}
{"title": "Multi-task learning based pre-trained language model for code completion\n", "abstract": " Code completion is one of the most useful features in the Integrated Development Environments (IDEs), which can accelerate software development by suggesting the next probable token based on the contextual code in real-time. Recent studies have shown that statistical language modeling techniques can improve the performance of code completion tools through learning from large-scale software repositories. However, these models suffer from two major drawbacks: a) Existing research uses static embeddings, which map a word to the same vector regardless of its context. The differences in the meaning of a token in varying contexts are lost when each token is associated with a single representation; b) Existing language model based code completion models perform poor on completing identifiers, and the type information of the identifiers is ignored in most of these models. To address these challenges, in this\u00a0\u2026", "num_citations": "12\n", "authors": ["1463"]}
{"title": "On early statistical requirements validation of cyber-physical space systems\n", "abstract": " Cyber-physical space systems are becoming increasingly important. Such systems have to satisfy requirements that are heavily affected by the physical space they operate in and by the active entities inhabiting the space, whose dynamic behaviors generate continuous topological changes. Reasoning about requirements in the early design phases is extremely challenging. High-level design can be facilitated by systematic application of separation of concerns throughout modeling, analysis, and early requirements validation. We outline an approach that identifies key recurrent concerns arising in cyber-physical space systems, supports systematic and semi-automatic modeling of separate concerns, and a formally defined composition of the separately developed models. Early requirements validation is then supported by leveraging statistical model checking techniques. We illustrate our approach through an\u00a0\u2026", "num_citations": "12\n", "authors": ["1463"]}
{"title": "Capturing requirements from expected interactions between software and its interactive environment: an ontology based approach\n", "abstract": " Requirements elicitation is one of the most important and challenging issues in requirements engineering. This paper proposes a systematic approach for capturing the software requirements from the expected interactions between the software-to-be and its interactive environment. Firstly, the software environment ontology and the interaction ontology are developed for serving as the meta-models for the descriptions of the interactive environment and the interactions. Then a process has been proposed for guiding the analysts to capture the software requirements step by step from descriptions of the interactive environment and the expected interactions. Finally, a case study has been presented for illustrating the usage of our approach.", "num_citations": "12\n", "authors": ["1463"]}
{"title": "User preference based autonomic generation of self-adaptive rules\n", "abstract": " The internetware system is a complex and distributed self-adaptive system, which challenges the method for making adaptation plans. Rule based approaches are very efficient to make plans in adaptive systems. To enable effective rule-based adaptation, we need to write a set of well behaved self-adaptive rules which could always lead to desirable states. This adaptive rules-set needs to be correct, com-plete, conflicts-free and well satisfy user goals, and it should updates according to user preferences. However, it is a difficult task for sys-tem users to define such a set of rules. To resolve this problem, we provide an rule generation engine, which could automatically generate well behaved self-adaptive rules according to user pref-erences. The rule generation engine is realized by a three-stage algorithm: stage 1 integrates user goals and user preferences, stage 2 establishes 1-1 tracing relationship between a\u00a0\u2026", "num_citations": "12\n", "authors": ["1463"]}
{"title": "Requirements analyses integrating goals and problem analysis techniques\n", "abstract": " One of the difficulties that goal-oriented requirements analyses encounters is that the efficiency of the goal refinement is based on the analysts' subjective knowledge and experience. To improve the efficiency of the requirements elicitation process, engineers need approaches with more systemized analysis techniques. This paper integrates the goal-oriented requirements language i \u2217  with concepts from a structured problem analysis notation, problem frames (PF). The PF approach analyzes software design as a contextualized problem which has to respond to constraints imposed by the environment. The proposed approach is illustrated using the meeting scheduler exemplar. Results show that integration of the goal and the problem analysis enables simultaneous consideration of the designer's subjective intentions and the physical environmental constraints.", "num_citations": "12\n", "authors": ["1463"]}
{"title": "A merging-based approach to handling inconsistency in locally prioritized software requirements\n", "abstract": " It has been widely recognized that the relative priority of requirements can help developers to resolve inconsistencies and make some necessary trade-off decisions. However, for most distributed development such as Viewpoints-based approaches, different stakeholders may assign different levels of priority to the same shared requirements statement from their own perspectives. The disagreement in the local priorities assigned to the same shared requirements statement often puts developers into a dilemma during inconsistency handling process. As a solution to this problem, we present a merging-based approach to handling inconsistency in the Viewpoints framework in this paper. In the Viewpoints framework, each viewpoint is a requirements collection with local prioritization. Informally, we transform such a requirements collection with local prioritization into a stratified knowledge base. Moreover, the\u00a0\u2026", "num_citations": "12\n", "authors": ["1463"]}
{"title": "Handling non-canonical software requirements based on annotated predicate calculus\n", "abstract": " Eliciting requirements for a proposed system inevitably involves the problem of handling undesirable information about customer's needs, including inconsistency, vagueness, redundancy, or incompleteness. We term the requirements statements involved in the undesirable information non-canonical software requirements. In this paper, we propose an approach to handling non-canonical software requirements based on Annotated Predicate Calculus (APC). Informally, by defining a special belief lattice appropriate for representing the stakeholder's belief in requirements statements, we construct a new form of APC to formalize requirements specifications. We then show how the APC can be employed to characterize non-canonical requirements. Finally, we show how the approach can be used to handle non-canonical requirements through a case study.", "num_citations": "12\n", "authors": ["1463"]}
{"title": "Towards an ontological approach to information system security and safety requirement modeling and reuse\n", "abstract": " Misuse cases are currently used to identify safety and security threats and subsequently capture safety and security requirements. There is limited consensus to the precise meaning of the basic terminology used for use/misuse case concepts. This paper delves into the use of ontology for the formal representation of the use-misuse case domain knowledge for eliciting safety and security requirements. We classify misuse cases into different category to reflect different type of misusers. This will allow participants during the requirement engineering stage to have a common understanding of the problem domain. We enhanced the misuse case domain to include abusive misuse case and vulnerable use case in order to boost the elicitation of safety requirements. The proposed ontological approach will allow developer to share and reuse the knowledge represented in the ontology thereby avoiding ambiguity and\u00a0\u2026", "num_citations": "11\n", "authors": ["1463"]}
{"title": "An engineerable ontology based approach for requirements elicitation in process centered problem domain\n", "abstract": " Requirements elicitation is one of the hardest and most critical parts of software development. Ontology technology provides a good way to support this work. In the literature, there many methods proposed about how to make use of ontology in requirement elicitation. However most of such methods assume that there already exists some domain ontology for reuse. Different from this works, our approach breaks this assumption and claims that we should merge the ontology acquiring process and the requirement elicitation process together. In order to do that, this paper proposes an upper level ontology for process centered problem domain to help the analysts to represent the requirement, and defines an engineerable process to guide the ontology acquiring integrated requirement elicitation process. In the end, we make a case study in the taxation domain to illustrate the effectiveness of our approach.", "num_citations": "11\n", "authors": ["1463"]}
{"title": "Approach for evaluating the trustworthiness of service agent\n", "abstract": " This paper addresses the service selection issue via a service agent framework coupled with a computational model based on trust ontology. The trust ontology proposed from the socio-cognitive view can offer a more insightful understanding in the trust relationships among service agents. A set of trust computational rules have been given to support the trust analysis and reasoning. Also the structure of the service agents has been given. Finally. a case study is used to illustrate how the service agents infer the trustworthiness of their partners by using the set of trust reasoning and computational rules.", "num_citations": "11\n", "authors": ["1463"]}
{"title": "Requirement driven agent collaboration based on functional ontology and AMD\n", "abstract": " With the increasing of Web services in Internet, composing existing services for satisfying new requirements has gained daily expanding attentions and interests. Many efforts have been pursued for supporting the essential activities in service composition. However, the existing techniques only focus on passive services which are waiting there for being discovered and invoked, we argue that it might be more attractive when Web services become active entities (service agent) distributed in Internet which can recognize the newly emergent requirements and compete with others for realize (part of) the requirements. Sometimes, more than one service agents have to collaborate to satisfy the requirements. That could be called as the requirement driven agent collaboration. This paper proposes a preliminary model for the requirement driven agent collaboration based on a functional ontology and the automated\u00a0\u2026", "num_citations": "11\n", "authors": ["1463"]}
{"title": "Improved knowledge base completion by the path-augmented TransR model\n", "abstract": " Knowledge base completion aims to infer new relations from existing information. In this paper, we propose path-augmented TransR (PTransR) model to improve the accuracy of link prediction. In our approach, we build PTransR based on TransR, which is the best one-hop model at present. Then we regularize TransR with information of relation paths. In our experiment, we evaluate PTransR on the task of entity prediction. Experimental results show that PTransR outperforms previous models.", "num_citations": "10\n", "authors": ["1463"]}
{"title": "Lightweight semantic service modelling for IoT: an environment-based approach\n", "abstract": " Combining service-oriented architecture (SOA) with internet of things (IoT) has been paid more and more attentions for encapsulating the functionalities of heterogeneous devices as IoT services and enabling things to interact and communicate with each other. However, the intermittent disconnection of wireless network, resource-constrained and mobility natures of devices, etc., bring challenges to service modelling. The dynamics availability and context of IoT service have become important concerns among others. This paper proposes an environment-based approach for service modelling in IoT. The WSMO-Lite has been adopted as the lightweight semantic service model. Then, the WSMO-Lite is extended for allowing modelling the dynamic context and availability of service. Firstly, an environmental entity model has been supplied for capturing the context. Secondly, the availability of service is represented in\u00a0\u2026", "num_citations": "10\n", "authors": ["1463"]}
{"title": "Stigmergy-based construction of internetware artifacts\n", "abstract": " A proposed approach supports the continual construction and evolution of model-based Internetware artifacts by a collective of Internet-connected stakeholders. The key mechanism is incremental graph superimposition (IGS), a refinement of stigmergy, the process that produces collective intelligence in social insects. Employing IGS, a collective of individuals collaboratively and continually construct a collective-level graph by incrementally aggregating the individuals' working results. Each individual can work independently without direct interaction with others, facilitating mass collaboration among a large number of individuals. A conceptual model illustrates the effects of IGS. A Web environment supports IGS-based conceptual-model construction and ongoing case studies.", "num_citations": "10\n", "authors": ["1463"]}
{"title": "Finding optimal solution for satisficing non-functional requirements via 0-1 programming\n", "abstract": " On-Functional Requirements (NFRs) are vital for the success of software systems. Generally speaking, NFRs are some implicit expectations about how well the software will work, often known as software quality. For building better software, the NFRs should be considered as criteria for design decision. However, different NFRs may produce different criteria on the implementation strategies of the software functions. A trade-off analysis is needed for getting an optimal plan during design decision to satisfice NFRs as well as possible. By focusing on the NFRs that can be quantitatively specified, this paper proposes an approach to finding such an optimal solution for helping to make better decision. This approach regards the NFRs as the constraints on the implementation strategies of the software functions and models the selection of implementation strategies as a 0-1 programming problem. Then, a 0-1 programming\u00a0\u2026", "num_citations": "10\n", "authors": ["1463"]}
{"title": "iMashup: assisting end-user programming for the service-oriented web\n", "abstract": " The Web is currently moving towards a platform with rich services. A notable trend is that end-users create mashups composing services with short, iterative development life cycles as well as updating with evolving needs. However, the large number of services and the high complexity of composition constraints make manual composition extremely difficult. Addressing this issue, we have developed an approach to assisting the end-users to build mashups in a simple and fast fashion. A tag-based model provides end-users a quick and intuitive insight of services. End-users simply describe their desired goals with tags. Interacting with a service repository, our approach employs a planning approach to suggest services that end-users might want to involve in the final outputs, including some additional interesting or relevant ones to induce more potential composition opportunities. End-users are allowed to iteratively\u00a0\u2026", "num_citations": "10\n", "authors": ["1463"]}
{"title": "On constructing environment ontology for semantic web services\n", "abstract": " This paper proposes constructing an environment ontology to represent domain knowledge about Web services. The capability of a Web service is considered in terms of the effects it imposes on the environment during execution. Thus, more fundamental and precise semantic specification for service capability than conventional interface-based description language can be obtained. Basic concepts of the ontology include resources residing in the environment. For each environment resource, there is a corresponding hierarchical state machine specifying its dynamic characteristics. Thus, the influence of a machine on its environments can be modelled with the state machines of the environment resources. Rules and algorithms to construct an environment ontology on the basis of generic domain ontology are introduced. And then guidelines for specifying Web service capability semantically based on the\u00a0\u2026", "num_citations": "10\n", "authors": ["1463"]}
{"title": "FECT: A modelling framework for automatically composing Web services\n", "abstract": " In this paper, we propose FECT, a new modelling framework for describing and composing heterogenous Web services to satisfy emergent requirements. In FECT, a three-dimension description pattern, i.e. the Environment, the Capability and the behavior Traces, has been proposed to describe Web services. FECT has the following features, which may distinguish it from other work. Firstly, the environment, with which Web services may interact, has been introduced for characterizing the context of Web services. Secondly, the capability of a Web service is captured as the effects on its environment. Thirdly, the composition process is accomplished by combining the traces of Web services.", "num_citations": "10\n", "authors": ["1463"]}
{"title": "Simplifying the formal verification of safety requirements in zone controllers through problem frames and constraint-based projection\n", "abstract": " Formal methods have been applied widely to verifying the safety requirements of communication-based train control (CBTC) systems, while the problem situations could be much simplified. In industrial practices of CBTC systems, however, huge complexity arises, which renders those methods nearly impossible to apply. In this paper, we aim to reduce the state space of formal verification problems in zone controller, a sub-system of a typical CBTC. We achieve the simplification goal by reducing the total number of device variables. To do this, two projection methods are proposed based on problem frames and constraints, respectively. The problem frame-based method decomposes the system according to sub-properties through functional decomposition, while the constraint-based projection method removes redundant variables. Our industrial case study demonstrates the feasibility through an evaluation\u00a0\u2026", "num_citations": "9\n", "authors": ["1463"]}
{"title": "Towards neural speaker modeling in multi-party conversation: The task, dataset, and models\n", "abstract": " In this paper, we address the problem of speaker classification in multi-party conversation, and collect massive data to facilitate research in this direction. We further investigate temporal-based and content-based models of speakers, and propose several hybrids of them. Experiments show that speaker classification is feasible, and that hybrid models outperform each single component.", "num_citations": "9\n", "authors": ["1463"]}
{"title": "\u7269\u8054\u7f51\u670d\u52a1\u53d1\u73b0\u521d\u63a2: \u4f20\u7edf SOA \u7684\u53ef\u884c\u6027\u548c\u5c40\u9650\u6027\n", "abstract": " \u57fa\u4e8e SOA (service-oriented architecture) \u7684\u7269\u8054\u7f51 (Internet of things, IoT) \u628a\u8bbe\u5907\u7684\u529f\u80fd\u670d\u52a1\u5316, \u4ee5\u4e00\u79cd\u7edf\u4e00\u548c\u901a\u7528\u7684\u63a5\u53e3\u5411\u5916\u754c\u63d0\u4f9b\u670d\u52a1. \u7531\u4e8e\u7269\u8054\u7f51\u4e2d\u8bbe\u5907\u7684\u6d77\u91cf\u6027, \u79fb\u52a8\u6027\u548c\u8d44\u6e90\u9ad8\u5ea6\u53d7\u9650\u6027, \u4ee5\u53ca\u65e0\u7ebf\u7f51\u7edc\u81ea\u8eab\u7684\u4e0d\u53ef\u9760\u6027, \u8bbe\u5907\u670d\u52a1\u4e0e\u4f20\u7edf\u7684 Web \u670d\u52a1\u76f8\u6bd4\u5177\u6709\u4e0d\u540c\u7684\u7279\u70b9, \u73b0\u6709\u7684 Web \u670d\u52a1\u53d1\u73b0\u65b9\u6cd5\u4e0d\u80fd\u6709\u6548\u5730\u6ee1\u8db3\u7269\u8054\u7f51\u4e2d\u670d\u52a1\u53d1\u73b0\u7684\u9700\u6c42. \u4ece Web \u670d\u52a1\u53d1\u73b0\u4f53\u7cfb\u7ed3\u6784\u548c\u5339\u914d\u7b56\u7565\u4e24\u4e2a\u65b9\u9762\u5bf9\u5178\u578b\u7684 Web \u670d\u52a1\u53d1\u73b0\u65b9\u6cd5\u8fdb\u884c\u4e86\u5206\u6790; \u7ed3\u5408\u7269\u8054\u7f51\u4e2d\u670d\u52a1\u63d0\u4f9b\u7684\u7279\u70b9, \u4ece\u53ef\u6269\u5c55\u6027, \u8d44\u6e90\u6709\u9650\u6027, \u5f02\u6784\u6027\u548c\u73af\u5883\u7684\u52a8\u6001\u53d8\u5316\u6027\u56db\u4e2a\u65b9\u9762, \u5206\u6790\u4e86\u5c06 Web \u670d\u52a1\u53d1\u73b0\u65b9\u6cd5\u5e94\u7528\u4e8e\u7269\u8054\u7f51\u670d\u52a1\u63d0\u4f9b\u4e2d\u6240\u9762\u4e34\u7684\u95ee\u9898, \u5e76\u8ba8\u8bba\u4e86\u53ef\u80fd\u7684\u89e3\u51b3\u601d\u8def; \u63a2\u8ba8\u4e86\u7269\u8054\u7f51\u4e2d\u670d\u52a1\u53d1\u73b0\u9700\u8981\u89e3\u51b3\u7684\u95ee\u9898.", "num_citations": "9\n", "authors": ["1463"]}
{"title": "From inconsistency handling to non-canonical requirements management: A logical perspective\n", "abstract": " As a class of defects in software requirements specification, inconsistency has been widely studied in both requirements engineering and software engineering. It has been increasingly recognized that maintaining consistency alone often results in some other types of non-canonical requirements, including incompleteness of a requirements specification, vague requirements statements, and redundant requirements statements. It is therefore desirable for inconsistency handling to take into account the related non-canonical requirements in requirements engineering. To address this issue, we propose an intuitive generalization of logical techniques for handling inconsistency to those that are suitable for managing non-canonical requirements, which deals with incompleteness and redundancy, in addition to inconsistency. We first argue that measuring non-canonical requirements plays a crucial role in handling them\u00a0\u2026", "num_citations": "9\n", "authors": ["1463"]}
{"title": "Avt vector: A quantitative security requirements evaluation approach based on assets, vulnerabilities and trustworthiness of environment\n", "abstract": " Security requirements analysis is gaining growing attention as new vulnerabilities and threats are emerging on daily basis, the systematic evaluation of security requirements is of utmost importance among the various decisions to be made related to security. This short contribution proposes using a 3-dimensional vector for quantitative evaluation of security requirements, which takes into account the importance of assets to be protected, the vulnerability of the system and the trustworthiness of environment.", "num_citations": "9\n", "authors": ["1463"]}
{"title": "Missing data analysis: a kernel-based multi-imputation approach\n", "abstract": " Many missing data analysis techniques are of single-imputation. However, single-imputation cannot provide valid standard errors and confidence intervals, since it ignores the uncertainty implicit in the fact that the imputed values are not the actual values. Filling in each missing value with a set of plausible values is called multi-imputation. In this paper we propose a kernel-based stochastic non-parametric multi-imputation method under MAR (Missing at Random) and MCAR (Missing Completely at Random) missing mechanisms in nonparametric regression settings. Furthermore, we present a kernel-based stochastic semi-parametric multi-imputation method while we have some priori knowledge about the dataset with missing. Our algorithms are designed specifically with the aim of optimizing the confidence-interval and the relative efficiency. The proposed technique is evaluated by experimentations, using\u00a0\u2026", "num_citations": "9\n", "authors": ["1463"]}
{"title": "An ontology-guided process for developing problem frame specification: an example\n", "abstract": " Problem Frames approach is a new and prospective tool for classifying, analyzing and structuring software development problems. However, there is not yet efficient CASE tool for guiding the problem frame specification development. An ontology based solution for this kind of CASE tool has been proposed, and an ontology of Problem Frames approach has been developed for this purpose. This paper presents the process of the ontology-guided problem frame specification development.", "num_citations": "9\n", "authors": ["1463"]}
{"title": "Identifying acceptable common proposals for handling inconsistent software requirements\n", "abstract": " The requirements specifications of complex systems are increasingly developed in a distributed fashion. It makes inconsistency management necessary during the requirements stage. However, identifying appropriate inconsistency handling proposals is still an important challenge. In particular, for inconsistencies involving many different stakeholders with different concerns, it is difficult to reach an agreement on inconsistency handling. To address this, this paper presents a vote-based approach to choosing acceptable common proposals for handling inconsistency. This approach focuses on the inconsistency in requirements that results from conflicting intentions of stakeholders. Informally speaking, we consider each distinct stakeholder (or a distributed artifact) involved in the inconsistency as a voter. Then we transform identification of an acceptable common proposal into a problem of combinatorial vote\u00a0\u2026", "num_citations": "9\n", "authors": ["1463"]}
{"title": "Detecting of requirements inconsistency: an ontology-based approach\n", "abstract": " Detecting of requirements inconsistency is key to the development of heterogeneous software system. But at present, although there is a lot of work on the detecting of requirements inconsistency, most of them is limited in detecting of requirements inconsistency at syntax level, we still lack of a promising method for the detection of requirements inconsistency at semantic level. Based on an abstract requirement refinement process model, this paper takes domain ontology as infrastructure for the refinement of software requirements, software requirements can then be seen as interactions between software agents and the ambiences they exist. We assume that the real effect of these interactions is to make the state of ambience changed, so we explicitly represent requirements of software agents as a set of state transition diagrams, each is for one entity in the ambience. And we realized semantic level inconsistency\u00a0\u2026", "num_citations": "9\n", "authors": ["1463"]}
{"title": "Companies' participation in oss development-an empirical study of openstack\n", "abstract": " Commercial participation continues to grow in open source software (OSS) projects and novel arrangements appear to emerge in company-dominated projects and ecosystems. What is the nature of these novel arrangements' Does volunteers' participation remain critical for these ecosystems' Despite extensive research on commercial participation in OSS, the exact nature and extent of company contributions to OSS development, and the impact of this engagement may have on the volunteer community have not been clarified. To bridge the gap, we perform an exploratory study of OpenStack: a large OSS ecosystem with intense commercial participation. We quantify companies' contributions via the developers that they provide and the commits made by those developers. We find that companies made far more contributions than volunteers and the distribution of the contributions made by different companies is also\u00a0\u2026", "num_citations": "8\n", "authors": ["1463"]}
{"title": "Environment-centric safety requirements for autonomous unmanned systems\n", "abstract": " Autonomous unmanned systems (AUS) emerge to take place of human operators in harsh or dangerous environments. However, such environments are typically dynamic and uncertain, causing unanticipated accidents when autonomous behaviours are no longer safe. Even though safe autonomy has been considered in the literature, little has been done to address the environmental safety requirements of AUS systematically. In this paper, we conduct a systematical literature review and set up a taxonomy of environment-centric safety requirements for AUS. We then analyse the neglected issues to suggest several new research directions towards the vision of environmental-centric safe autonomy.", "num_citations": "8\n", "authors": ["1463"]}
{"title": "Petri net based test case generation for evolved specification\n", "abstract": " Model-based testing can use a model to test a concrete program\u2019s implementation. When the model is changed due to the evolution of the specification, it is important to maintain the test suites up to date, such that it can be used for regression testing. A complete regeneration of the whole test suite from the new model, although inefficient, is still frequently used in practice. To address this problem effectively, we propose a test case reusability analysis technique to identify reusable test cases of the original test suite based on graph analysis, such that we can generate new test cases to cover only the change-related parts of the new model. The Market Information System (MIS) is employed to demonstrate the feasibility and effectiveness of the proposed method. Our experimental results show that the use of our method saves about 31.5% test case generation cost.", "num_citations": "8\n", "authors": ["1463"]}
{"title": "Transforming code with compositional mappings for API-library switching\n", "abstract": " API-library switching is to change an underlying API library to another one for a given program. API-library switching is frequent in software-development process, with typical examples as migrating programs across platforms or across incompatible versions of API libraries. Manual API-library switching is time-consuming and error-prone. To address this problem, previous research has proposed a series of rule based code-transformation approaches. However, these existing approaches are ineffective in conducting code transformation for those original code snippets containing a sequence of method invocations correlated with each other and in need of being considered together when transformed. We call the pair of an original code snippet and its transformed target code snippet as a compositional-mapping instance. Through our empirical investigation, we confirm that compositional-mapping instances widely\u00a0\u2026", "num_citations": "8\n", "authors": ["1463"]}
{"title": "Web service retrieval: An approach based on context ontology\n", "abstract": " This paper proposes an approach for supporting the automatic retrieval of Web services. Different from the available approaches, this approach uses a context ontology as the basis of the capability descriptions of Web services. When the capability descriptions of both Web services and the Web requests are grounded onto the same ontology, a matchmaking method has been given for judging the equivalency between them. A case study has been used for illustrating the main ideas", "num_citations": "8\n", "authors": ["1463"]}
{"title": "Managing the inconsistency of software requirements.\n", "abstract": " Analyzing and handling the inconsistency in requirements is critical to the development of complex software systems. How to solve this problem directly influences the quality of requirements specification, as well as the final software production. Based on a common-recognized management framework of requirements inconsistency, this paper summarizes representative researches on this topic. The main aim is to get a comprehensive understanding of the current techniques and methods of inconsistency management. Finally, the paper also identifies some issues which are still open to further research.", "num_citations": "8\n", "authors": ["1463"]}
{"title": "Ontology-based semantic cache in AOKB\n", "abstract": " When querying on a large-scale knowledge base, a major technique of improving performance is to preload knowledge to minimize the number of roundtrips to the knowledge base. In this paper, an ontology-based semantic cache is proposed for an agent and ontology-oriented knowledge base (AOKB). In AOKB, an ontology is the collection of relationships between a group of knowledge units (agents and/or other sub-ontologies). When loading some agentA, its relationships with other knowledge units are examined, and those who have a tight semantic tie withA will be preloaded at the same time, including agents and ub-ontologies in the same ontology whereA is. The preloaded agents and ontologies are saved at a semantic cache located in the memory. Test results show that up to 50% reduction in running time is achieved.", "num_citations": "8\n", "authors": ["1463"]}
{"title": "Companies' domination in FLOSS development: an empirical study of OpenStack\n", "abstract": " Because of the increasing acceptance and possibly expanding market of free/libre open source software (FLOSS), the spectrum and scale of companies that participate in FLOSS development have substantially expanded in recent years. Companies get involved in FLOSS projects to acquire user innovations [3, 12], to reduce costs [8, 11], to make money on complementary services [13], etc. Such intense involvement may change the nature of FLOSS development and pose critical challenges for the sustainability of the projects. For example, it has been found that a company's full control and intense involvement is associated with a decrease of volunteer inflow [13]. Sometimes a project may fail after one company pulls resources from the project [13]. This raises concerns about the domination of one company in a project. In large projects like OpenStack, there are often hundreds of companies involved in contributing\u00a0\u2026", "num_citations": "7\n", "authors": ["1463"]}
{"title": "Learning to generate comments for api-based code snippets\n", "abstract": " Comments play an important role in software developments. They can not only improve the readability and maintainability of source code, but also provide significant resource for software reuse. However, it is common that lots of code in software projects lacks of comments. Automatic comment generation is proposed to address this issue. In this paper, we present an end-to-end approach to generate comments for API-based code snippets automatically. It takes API sequences as the core semantic representations of method-level API-based code snippets and generates comments from API sequences with sequence-to-sequence neural models. In our evaluation, we extract 217K pairs of code snippets and comments from Java projects to construct the dataset. Finally, our approach gains 36.48% BLEU-4 score and 9.90% accuracy on the test set. We also do case studies on generated comments, which\u00a0\u2026", "num_citations": "7\n", "authors": ["1463"]}
{"title": "A blame-based approach to generating proposals for handling inconsistency in software requirements\n", "abstract": " Inconsistency has been considered one of the main classes of defects in software requirements specification. Various logic-based techniques have been proposed to manage inconsistencies in requirements engineering. However, identifying an appropriate proposal for resolving inconsistencies in software requirements is still a challenging problem. This paper proposes a logic-based approach to generating appropriate proposals for handling inconsistency in software requirements. Informally speaking, given an inconsistent requirements specification, the authors identify which requirements should be given priority to be changed for resolving the inconsistency in that specification, by balancing the blame of each requirement for the inconsistency against its value for that requirements specification. The authors follow the viewpoint that minimal inconsistent subsets of a set of formulas are the purest forms of\u00a0\u2026", "num_citations": "7\n", "authors": ["1463"]}
{"title": "Characterizing the implementation of software non-functional requirements from probabilistic perspective\n", "abstract": " Non-functional requirements are quality concerns of a software envisioned. As an effective treatment, goal-oriented method can capture NFR-related knowledge so that an evaluation for a specific implementation strategy can be provided. This paper makes a meaningful attempt to observe the implementation strategies of non-functional requirements in a probabilistic way, and obtain the probabilistic result for each satisficing status. The contribution of our work is to give a clear justification about whether there exists a proper implementation strategy for multiple non-functional requirements so that they can be guaranteed of the specific satisficing statuses, and if so how big the possibility is.", "num_citations": "7\n", "authors": ["1463"]}
{"title": "Web services composing by multiagent negotiation\n", "abstract": " Composing web services is gained daily attention in Service Oriented Computing. It includes the dynamic discovery, interaction and coordination of agent-based semantic web services. The authors first follow Function Ontology and Automated Mechanism Design for service agents aggregating. Then the problem is formulated but it is ineffective to solve it from the traditional global view. Because the complexity is NP-complete and it is difficult or even impossible to get some personal information. This paper provides a multi-agent negotiation idea in which each participant negotiates under the condition of its reservation payoff being satisfied. Numerical experiment is given and well evaluates the negotiation.", "num_citations": "7\n", "authors": ["1463"]}
{"title": "Towards controllable requirements engineering processes based on cybernetics\n", "abstract": " In this position paper, we set out to explore the possibilities of formulating problems in requirements engineering with concepts and frameworks from cybernetics. The objective is to seek practical synergies and to understand notable differences between the two areas. We try to understand to what extent known research results from cybernetics can be applied to address problems encountered at the requirements stage in software life cycle. In particular, we try to formulate the goal-oriented requirements analysis process as a feedback control system, in which a classical \"divide and conquer\" design philosophy is turned into a continuous augmentation process to existing design towards an optimal one. Similarly, control frameworks for the requirements elicitation process and requirements inconsistency management process are briefly explored. Potential control variables such as information entropy value for\u00a0\u2026", "num_citations": "7\n", "authors": ["1463"]}
{"title": "Evolving quantum oracles with hybrid quantum-inspired evolutionary algorithm\n", "abstract": " Quantum oracles play key roles in the studies of quantum computation and quantum information. But implementing quantum oracles efficiently with universal quantum gates is a hard work. Motivated by genetic programming, this paper proposes a novel approach to evolve quantum oracles with a hybrid quantum-inspired evolutionary algorithm. The approach codes quantum circuits with numerical values and combines the cost and correctness of quantum circuits into the fitness function. To speed up the calculation of matrix multiplication in the evaluation of individuals, a fast algorithm of matrix multiplication with Kronecker product is also presented. The experiments show the validity and the effects of some parameters of the presented approach. And some characteristics of the novel approach are discussed too.", "num_citations": "7\n", "authors": ["1463"]}
{"title": "Knowledge based hierarchical software reuse\n", "abstract": " Software reuse has been considered as a main impulse for enhancing software productivity. In this paper, a new strategy of software reuse, called KISSME, is considered. This approach emphasizes the combination of multiple reuse techniques at different stages of software development and the automation of this process based on its rich knowledge. The knowledge is clustered in terms of the different stages of software development and is represented by a new representation frame. Also described in this paper is the reuse environment PROMIS with a case study.", "num_citations": "7\n", "authors": ["1463"]}
{"title": "Early validation of cyber\u2013physical space systems via multi-concerns integration\n", "abstract": " Cyber\u2013physical space systems are engineered systems operating within physical space with design requirements that depend on space, e.g., regarding location or movement behavior. They are built from and depend upon the seamless integration of computation and physical components. Typical examples include systems where software-driven agents such as mobile robots explore space and perform actions to complete particular missions. Design of such a system often depends on multiple concerns expressed by different stakeholders, capturing different aspects of the system. We propose a model-driven approach supporting (a) separation of concerns during design, (b) systematic and semi-automatic integration of separately modeled concerns, and finally (c) early validation via statistical model checking. We evaluate our approach over two different case studies of cyber\u2013physical space systems.", "num_citations": "6\n", "authors": ["1463"]}
{"title": "Scalable multiple-view analysis of reactive systems via bidirectional model transformations\n", "abstract": " Systematic model-driven design and early validation enable engineers to verify that a reactive system does not violate its requirements before actually implementing it. Requirements may come from multiple stakeholders, who are often concerned with different facets - design typically involves different experts having different concerns and views of the system. Engineers start from a specification which may be sourced from some domain model, while validation is often done on state-transition structures that support model checking. Two computationally expensive steps may work against scalability: transformation from specification to state-transition structures, and model checking. We propose a technique that makes the former efficient and also makes the resulting transition systems small enough to be efficiently verified. The technique automatically projects the specification into submodels depending on a property\u00a0\u2026", "num_citations": "6\n", "authors": ["1463"]}
{"title": "Transforming timing requirements into CCSL constraints to verify cyber-physical systems\n", "abstract": " The timing requirements of embedded cyber-physical systems (CPS) constrain CPS behaviors made by scheduling analysis. Lack of physical entity properties modeling and the need of scheduling analysis require a systematic approach to specify timing requirements of CPS at the early phase of requirements engineering. In this work, we extend the Problem Frames notations to capture timing properties of both cyber and physical domain entities into Clock Constraint Specification Language (CCSL) constraints which is more explicit that LTL for scheduling analysis. Interpreting them using operational semantics as finite state machines, we are able to transform these timing requirements into CCSL scheduling constraints, and verify their consistency on NuSMV. Our TimePF tool-supported approach is illustrated through the verification of timing requirements for a representative problem in embedded CPS.", "num_citations": "6\n", "authors": ["1463"]}
{"title": "Delaydroid: reducing tail-time energy by refactoring android apps\n", "abstract": " Mobile devices with 3G/4G networking often waste energy in the so-called\" tail time\" during which the radio is kept on even though no communication is occurring. Prior work has proposed policies to reduce this energy waste by batching network requests. However, this work is challenging to apply in practice due to a lack of mechanisms. In response, we have developed DelayDroid, a framework that allows a developer to add the needed policy to existing, unmodified Android applications (apps) with no human effort. This allows such prior work (as well as our own policies) to be readily deployed and evaluated. The DelayDroid compile-time uses static analysis and bytecode refactoring to identify method calls that send network requests and modify such calls to detour them to the DelayDroid run-time. The run-time then applies a policy to batch them, avoiding the tail time energy waste. DelayDroid also includes a\u00a0\u2026", "num_citations": "6\n", "authors": ["1463"]}
{"title": "Reliability concerns in the problem frames approach and system reliability enhancement patterns\n", "abstract": " Software system reliability is a hot research topic in the field of software engineering. From the three factors of system reliability, this paper identifies five classes of reliability concerns for the Problem Frames (PF) Approach. This paper also presents five reliability enhancement patterns for each reliability concern. Reliability enhancement patterns are used to incorporate reliability requirements into the process of problem analysis explicitly, extend the boundary of problems and refine machine specifications in order to improve the reliability of the software systems. A supporting tool is also built. It helps system analysts to identify reliability requirements and use the reliability enhancement patterns correctly. Finally, a bank account management problem is used to show the feasibility of our work.", "num_citations": "6\n", "authors": ["1463"]}
{"title": "A problem oriented approach to modeling feedback loops for self-adaptive software systems\n", "abstract": " Self-adaptive software systems can adjust their behaviors at runtime to respond to the context changes. To operationalize the adaptive mechanism, feedback loops have been advocated in many works. However, most of existing works focus on the architecture design to realize the feedback loops. How to model the required feedback loops remains an issue. In this paper, we propose a problem oriented approach for this issue. This approach models the system composed by the self-adaptive software and its context as an adaptive control system which is equipped with two kinds of feedback loops: context-aware feedback loops and requirements-aware feedback loops. To model the feedback loops, we identify five classes of software problems to address the different concerns of the adaptive requirements behind the feedback loops. We illustrate our idea by applying it to a cruise control system.", "num_citations": "6\n", "authors": ["1463"]}
{"title": "CoFM: a web-based collaborative feature modeling system for internetware requirements' gathering and continual evolution\n", "abstract": " Internetware is a paradigm of open, decentralized and continually evolvable software systems running on the Internet. In the development of Internetware, the enormous amount of its stakeholders brings challenges to the gathering of common and essential requirements among these stakeholders and continual evolution of the requirements. In this paper, we present a web-based collaborative feature modeling system (CoFM) developed as a platform for gathering, organizing, evaluating, and negotiating Internetware requirements. The basic idea is to express and organize requirements in terms of user-perceivable features of desired Internetware application, and to allow stakeholders to propose, evaluate and negotiate these features collaboratively, in a shared feature model of the application. During the collaboration, the application provider can discover the common and important features that need to be\u00a0\u2026", "num_citations": "6\n", "authors": ["1463"]}
{"title": "Capability description and discovery of Internetware entity\n", "abstract": " Internetware can be formed through aggregation of Internetware entities bottom-up to satisfy the user\u2019s requirements. In the process, capability description of Internetware entity plays an important role. This paper proposes a capability description of Internetware entity based on environment ontology. The environment ontology includes environment entities and possible interactions with the environment entities in a particular domain. Tree-like hierarchical state machines are also included in the ontology to describe domain life-cycle of the environment entities. On the basis of the ideas, the capability of Internetware entity is embodied by interacting with environment entities and the changes of the environment entities during the interactions. Finally, this paper describes the matchmaking between capability descriptions of Internetware entities to support the discovery of Internetware entity. An example of online\u00a0\u2026", "num_citations": "6\n", "authors": ["1463"]}
{"title": "Summary queries for frequent itemsets mining\n", "abstract": " There are many advanced techniques that can efficiently mine frequent itemsets using a minimum-support. However, the question that remains unanswered is whether the minimum-support can really help decision makers to make decisions. In this paper, we study four summary queries for frequent itemsets mining, namely, (1) finding a support-average of itemsets, (2) finding a support-quantile of itemsets, (3) finding the number of itemsets that greater/less than the support-average, i.e., an approximated distribution of itemsets, and (4) finding the relative frequency of an itemset (compared its frequency with that of other itemsets in the same dataset). With these queries, a decision maker will know whether an itemset in question is greater/less than the support-quantile; the distribution of itemsets; and the frequentness of an itemset. Processing these summary queries is challenging, because the minimum-support\u00a0\u2026", "num_citations": "6\n", "authors": ["1463"]}
{"title": "Requirements driven agent collaboration\n", "abstract": " This paper proposes the requirements driven agent collaboration. This proposal assumes that there are plenty different service agents distributed in Internet. When a request for accomplishing a particular task occurs, these autonomous agents can recognize the newly emergent requirements and dynamically aggregate together to compete with others for fulfilling the requirements. This paper presents a preliminary framework for the requirement driven agent collaboration based on the automated mechanism design.", "num_citations": "6\n", "authors": ["1463"]}
{"title": "Web service composition: an approach using effect-based reasoning\n", "abstract": " This paper proposes an ontology-based approach to compose Web services using the effect-based reasoning. The environment ontology is to provide formal and sharable specifications of environment entities of Web services in a particular domain. For each environment entity, there is a corresponding hierarchical state machine for specifying its dynamic characteristics. Then, this approach proposes to use the effects of a Web service on its environment entities for specifying the Web service\u2019s capabilities and designates the effect as the traces of the state transitions the Web service can impose on its environment entities. So, the service composition can be conducted by the effect-based reasoning.", "num_citations": "6\n", "authors": ["1463"]}
{"title": "Deep-autocoder: Learning to complete code precisely with induced code tokens\n", "abstract": " Code completion is an essential part of modern IDEs. It assists the developers to speed up the process of coding and reducing typos. In this paper, we exploit the deep learning technique called LSTM to learn language models over large code corpus and make predictions of code elements. Unlike natural language, the innumerable identifiers lead to the vocabulary explosion and more difficult to predict. Therefore, we propose a new approach, the Induced Token based LSTM, to deal with the massive identifiers, thus decrease the vocabulary size. In order to induce the code tokens, we present two approaches, one is a constraint character-level LSTM and the other one is encoding identifiers with various preceding context before feeding them into a token-level LSTM. Based on the two approaches, a tool named Deep-AutoCoder is developed and evaluated in two classic completion scenarios, that is, method\u00a0\u2026", "num_citations": "5\n", "authors": ["1463"]}
{"title": "Learning to infer API mappings from API documents\n", "abstract": " To satisfy business requirements of various platforms and devices, developers often need to migrate software code from one platform to another. During this process, a key task is to figure out API mappings between API libraries of the source and target platforms. Since doing it manually is time-consuming and error-prone, several code-based approaches have been proposed. However, they often have the issues of availability on parallel code bases and time expense caused by static or dynamic code analysis.                 In this paper, we present a document-based approach to infer API mappings. We first learn to understand the semantics of API names and descriptions in API documents by a word embedding model. Then we combine the word embeddings with a text similarity algorithm to compute semantic similarities between APIs of the source and target API libraries. Finally, we infer API mappings from\u00a0\u2026", "num_citations": "5\n", "authors": ["1463"]}
{"title": "Goal model driven alternative selection: a quantitative approach\n", "abstract": " Model driven engineering (MDE) techniques can be used in requirement engineering to derive an implementation out of system requirements, which could be extended to derive models in the solution space out of models in the problem space. Goal models are useful to deal with problem space modeling and support requirements analysis activities including alternative selection, a procedure that is performed to evaluate the feasibility and desirability of alternative strategies with respect to quality goals. The results of alternative requirements selection can be referred to derive the configuration of solution space models and accordingly the implementation of software, since requirements elements can be traced to architecture elements or architecture design issues. Most of the existing goal-oriented requirement engineering (GORE) frameworks conduct alternative selection based on qualitative goal models, which are\u00a0\u2026", "num_citations": "5\n", "authors": ["1463"]}
{"title": "Ontology patterns for service\u2010oriented software development\n", "abstract": " Modern software often uses ontologies as its key component to store data and their relationships. This is different from using an ontology as a stand\u2010alone tool for knowledge sharing and representation. The ontology component needs to work with other software components and needs to evolve as the software evolves. Ontology design has been a research topic for years; however, most of these studies focus on using ontologies as stand\u2010alone applications. This paper studies ontology patterns that can be applied to design ontologies as an integral part of a service\u2010oriented application. The paper first briefly reviews various ontology design issues including a brief survey of existing ontology design patterns. The paper then outlines general principles for using ontologies in software applications, including the needs to incorporate ontology design process as a part of software development processes, design\u00a0\u2026", "num_citations": "5\n", "authors": ["1463"]}
{"title": "Research on the merging of feature models\n", "abstract": " Feature models provide an effective way to organize and reuse software requirements in a specific domain. Constructing a feature model needs a systematic analysis of as many applications as possible in a domain, to identify commonality, variability, and dependencies among requirements. With the increasing complexity of domains, the scale of feature models can be extremely large, and the construction of large feature models is an overwhelming task for human that computer-aided automation is needed. A feasible way is to merge existing feature models into a large one, and human developers only need to do some refactoring work. In this paper, we survey six methods of merging feature models. We propose a conceptual framework first, and then analyze and compare the six methods. Finally, we identify three problems in existing research, and propose possible ideas to handle these problems.", "num_citations": "5\n", "authors": ["1463"]}
{"title": "A unified use-misuse case model for capturing and analysing safety and security requirements\n", "abstract": " This paper proposes an enhanced use-misuse case model that allows both safety and security requirements to be captured during requirements elicitation. The proposed model extends the concept of misuse case by incorporating vulnerable use case and abuse case notations and relations that allows understanding and modeling different attackers and abusers behaviors during early stage of system development life cycle and finishes with a practical consistent combined model for engineering safety and security requirements. The model was successfully applied using health care information system gathered through the university of Kansas HISPC project. The authors were able to capture both security and safety requirements necessary for effective functioning of the system. In order to enhance the integration of the proposed model into risk analysis, the authors give both textual and detailed description of the\u00a0\u2026", "num_citations": "5\n", "authors": ["1463"]}
{"title": "An approach for capturing software requirements from interactive scenarios\n", "abstract": " BackgroundAbstract Requirements elicitation is a key phase in requirements engineering. This paper proposes an interactive scenario based requirements elicitation approach by combining scenarios with the problem frames approach. This approach features that:(1) it can capture deep requirements information through the classification of problem domains and interactions between system and problem domains; and (2) it can capture implicit requirements by introducing constraints of interactive scenarios which are defined as meaningful interactive sequences happed on the problem domains. This paper constructs a meta-model for requirements description, and gives a requirements elicitation process, which guides the requirements providers to input relevant information of requirements. A corresponding supporting tool is also built. It provides functions for editing, checking and validating requirements elements\u00a0\u2026", "num_citations": "5\n", "authors": ["1463"]}
{"title": "Knowledge guided software trustworthiness requirements elicitation\n", "abstract": " As software systems are used widely and deeply in society, trustworthiness, which refers to the ability to operate correctly under any situation, becomes one of the hot topics in practice and research. To develop trustworthy software system needs the elicitation of trustworthiness requirements. But the elicitation of the trustworthiness requirements needs the requirements engineer to possess lots of knowledge about undesired conditions a software system may face and the corresponding countermeasures the software system could take to deal with them. There still lacks a systematic method to guide the elicitation of trustworthiness requirements. This paper introduces a meta-model of trustworthiness, founds a knowledge base according to this meta-model, presents a trustworthiness requirements pattern and a method about how to generate patterns from the knowledge base to help eliciting trustworthiness\u00a0\u2026", "num_citations": "5\n", "authors": ["1463"]}
{"title": "Assignment problem in requirements driven agent collaboration and its implementation\n", "abstract": " Requirements Driven Agent Collaboration (RDAC) is a mechanism where the self-interested service agents actively and autonomously search for the required services submitted by the request agents and compete to offer the services. This mechanism would be more suitable for large number of autonomous service providers on internet compared with the current service-oriented computing framework. Collaboration is an important issue in this mechanism. This paper focuses on the collaboration issue in RDAC. First, we define the assignment problem in RDAC and show that it is NP-complete. Then, we model it as a Kripke structure with normative systems on it. This makes it possible to bridge the assignment problem and the existing efforts in normative systems, games, mechanisms, etc. Thirdly, a negotiation-based approach is given to solve the problem and the performance of the negotiation is evaluated by simulation. Finally, a tool for RDAC has been implemented to put it into practice and for further testing and evaluation.", "num_citations": "5\n", "authors": ["1463"]}
{"title": "An agent-based framework supporting trust evaluation for service selection\n", "abstract": " Trust is central to effective interactions in service-oriented computing in which service consumers select the right partners who can and will provide expected services. Available trust models lack of the analysis and reasoning on trustworthiness. This paper proposes an agent-based framework to support the trust evaluation for service selection. A trust ontology has been provided to analyze and reason the relations among trust concepts.", "num_citations": "5\n", "authors": ["1463"]}
{"title": "A Framework for Agent-based Service-Oriented Modelling\n", "abstract": " Service-oriented computing is becoming a direction of computing technology. For realising the mission of service-oriented computing, service-oriented architecture has been proposed to facilitate the application development via discoverable services distributed on Internet. That brings out service-oriented modelling as a new technical area to provide modelling and analysis techniques of service-oriented applications. The paper proposes a framework for agent-based service-oriented modelling. It treats both the service providers and the service requesters as service agents. Domain ontology has been used to provide the sharable domain knowledge as well as terminology for allowing the agents to understand each others. A modelling process has also been illustrated with the model development of an online auction service.", "num_citations": "5\n", "authors": ["1463"]}
{"title": "Service-oriented modeling: An extensive reuse method\n", "abstract": " Service-oriented modeling is defined as a comprehensive practice encompassing analysis, design and architecture of all organizational software entities. Currently, service-oriented modeling receives more attention in SOA project lifecycle, since SOA application is model-driven and can be fast developed after modeling. However, existing service-oriented modeling approaches reuse only service which is considered to be building blocks of cross-domain applications. This usually falls into the field of OO analysis, just using service to realize function at design and implementation phase, which disobeys the soul of SOA that fast developing roots in extensive reuse. This paper proposes a domain-specific hierarchical ontology system supporting reuse of various service-oriented assets, such as application template, collaboration pattern, workflow and service. The paper also presents a service-oriented modeling\u00a0\u2026", "num_citations": "5\n", "authors": ["1463"]}
{"title": "Environment ontology-based capability specification for web service discovery\n", "abstract": " During Web service discovery, capabilities of Web services are of major concern. This paper proposes an environment ontology based approach for specifying Web service capability semantically. First, a meta-level environment ontology is adopted in the proposed approach to provide formal and sharable specifications of environment resources in a particular domain. For each environment resource, we build a corresponding hierarchical state machine specifying its dynamic characteristics. Second, we propose to use the effect of a Web service on its environment resources for specifying the Web service capability and to designate the effect as the traces of the state transitions the Web service can impose on its environment resources. Finally, we give the mechanism to match service query with service capability description.", "num_citations": "5\n", "authors": ["1463"]}
{"title": "2nd international workshop on advances and applications of problem frames\n", "abstract": " Software problems originate from real world problems. A software solution must address its real world problem in a satisfactory way. A software engineer must therefore understand the real world problem that their software intends to address. To be able to do this, the software engineer must understand the problem context and how it is to be affected by the proposed software, expressed as the requirements. Without this knowledge the engineer can only hope to chance upon the right solution for the problem. Application of the Problem Frames approach may well be a way of meeting this need.", "num_citations": "5\n", "authors": ["1463"]}
{"title": "OSNET-a language for domain modeling\n", "abstract": " Domain analysis and domain modeling has been proved to be an important technique for accelerating application development. The major step in this process is to convert requirement specification into an appropriate conceptual model of the wanted application software. The former is usually given in some form of natural language or pseudo natural language. The latter can be represented in one of various conceptual modeling languages, either graphic or symbolic. One of the major problems one encounters in this process is the big gap between the format of requirement specification and that of conceptual modeling. In order to easy this transform process, we propose a new type of conceptual modeling language OSNET which combines the semantic network representation with the object oriented paradigm. Thus it has both advantages of representing natural language style specification and object oriented\u00a0\u2026", "num_citations": "5\n", "authors": ["1463"]}
{"title": "NEWCOM: An architecture description language in client/server style\n", "abstract": " This paper introduces an architecture description language, NEWCOM, which is a moderate abstract level between requirement specification and 4GL. NEWCOM programs describe the global architecture of MISs in client/server style. NEWCOM is platform-independent. It supports heterogeneous network structure and database platform. NEWCOM includes various components and connectors for them. Data warehousing and database evolvement technique is also tackled, which is helpful to form a global solution for enterprises. NEWCOM has openness, which is not only used as a designing document, but also used to generate MIS directly.", "num_citations": "5\n", "authors": ["1463"]}
{"title": "Re4cps: requirements engineering for cyber-physical systems\n", "abstract": " Cyber-Physical Systems (CPSs) connect the cyber world with the physical world through a network of interrelated elements, such as sensors and actuators, robots, and other computing devices. There are increasing number of beneficial applications in dependable sectors such as aviation, transportation, aerospace, healthcare, etc.. The inherent characteristics of CPSs pose a number of challenges to requirements engineering. Unlike normal information systems, CPSs need to continuously detect and adapt to the environment changes. The interactive environment becomes the first-class citizen because the features and the changing patterns in environment are must-to-be considered. Moreover, in such systems, many non-functional requirements are environment related, like timing, safety, security, and privacy requirements. This tutorial will introduce an environment modelling based approach to engineering the\u00a0\u2026", "num_citations": "4\n", "authors": ["1463"]}
{"title": "Learning embeddings of api tokens to facilitate deep learning based program processing\n", "abstract": " Deep learning has been applied for processing programs in recent years and gains extensive attention on the academic and industrial communities. In analogous to process natural language data based on word embeddings, embeddings of tokens (e.g. classes, variables, methods etc.) provide an important basis for processing programs with deep learning. Nowadays, lots of real-world programs rely on API libraries for implementation. They contain numbers of API tokens (e.g. API related classes, interfaces, methods etc.), which indicate notable semantics of programs. However, learning embeddings of API tokens is not exploited yet. In this paper, we propose a neural model to learn embeddings of API tokens. Our model combines a recurrent neural network with a convolutional neural network. And we use API documents as training corpus. Our model is trained on documents of five popular API libraries\u00a0\u2026", "num_citations": "4\n", "authors": ["1463"]}
{"title": "Domain hyponymy hierarchy discovery by iterative web searching and inferable semantics based concept selecting\n", "abstract": " The hyponymy hierarchy is an essential part of domain knowledge, which is widely used in many applications. With the development of the Internet, the World Wide Web is now an invaluable resource of hyponymy discovering. However, acquiring domain hyponymy hierarchy from the web is still a low efficient work, because the hyponymy acquiring process is often disturbed by numerous irrelevant terms. In this paper, we propose a new iterative domain hyponymy hierarchy discovering method, where irrelevant terms can be eliminated automatically by inferable semantic information. Our approach is evaluated by the experiments in two programming-related domains. The results show that our approach works well.", "num_citations": "4\n", "authors": ["1463"]}
{"title": "Discovering domain concepts and hyponymy relations by text relevance classifying based iterative web searching\n", "abstract": " Domain concepts and taxonomic relationships are an essential part of a domain ontology. They are used in a number of applications, including natural language processing, information retrieval, knowledge management and so on. Nowadays, with the continuous permeation of various kinds of Internet knowledge applications, numerous new concepts are emerged and released on to the Internet. So, the Internet has become an invaluable source of new concepts for almost every possible domain of knowledge. In order to ensure the domain ontologies keep pace with fast changing knowledge, we proposed an web searching based concepts and taxonomic relationships discovering approach. By our approach, the potential concepts on the Internet, which are taxonomically related with the give seeds concepts, can be discovered autonomously and iteratively. In this paper, the approach and a corresponding\u00a0\u2026", "num_citations": "4\n", "authors": ["1463"]}
{"title": "CoFM: an environment for collaborative feature modeling\n", "abstract": " Feature models provide an effective way to capture commonality and variability in a specific domain. Constructing a feature model needs a systematic review of existing software artifacts in a domain and is always a collaboration-intensive activity. However, existing feature modeling methods and tools lack explicit support of such collaborations. In this paper, we present an environment for feature modeling that promotes the collaboration between stakeholders as the basis of creating and evolving a feature model. We present concepts, methods, and a tool to show the feasibility of constructing feature models collaboratively, as well as how to integrate this environment with traditional feature modeling methods.", "num_citations": "4\n", "authors": ["1463"]}
{"title": "Inconsistency-based strategy for clarifying vague software requirements\n", "abstract": " It seems to be inevitable to confront vague information about customer\u2019s needs during the software requirements stage. It may be desirable to record and clarify the vague information to avoid missing real requirements. In this paper, we provide an inconsistency-based strategy to handle vague information in the framework of Annotated Predicate Calculus. This strategy permits the stakeholder to describe the different vague information using statements with different levels of belief, where each level of belief is determined by the degree of vagueness. By checking consistency of the union of vague requirements and clear requirements, we then heighten the level of belief in uncontroversial vague requirements. We also lower the levels of belief in requirements involved in undesirable inferences and leave them to be articulated in some following stage. To support this, Annotated Predicate Calculus is used to\u00a0\u2026", "num_citations": "4\n", "authors": ["1463"]}
{"title": "Automating application software generation\n", "abstract": " Many approaches have been proposed to enhance software productivity and reliability. These approaches typically fall into three categories: the engineering approach, the formal approach and the knowledge\u2010based approach. But the optimal gain in software productivity cannot be obtained if one relies on only one of these approaches. This paper describes the work in knowledge\u2010based software engineering conducted by the authors for the past 10 years. The final goal of the research is to develop a paradigm for software engineering which integrates the three approaches mentioned above. A knowledge\u2010based tool which can support the whole process of software development is provided in this paper.", "num_citations": "4\n", "authors": ["1463"]}
{"title": "Automating consistency verification of safety requirements for railway interlocking systems\n", "abstract": " Consistency verification of safety requirements is an important but still challenging task for safety-critical systems such as rail transit systems. That is mainly because requirements are typically written in natural language and with strong time constraints. Driven by the practical need from industry, in this paper we propose a systematic approach to specify safety requirements in a quasi-natural language and automatically verify their consistency using formal methods. Specifically, we define a domain specific language SafeNL to specify safety requirements, and then automatically transform them into formal constraints defined in the Clock Constraint Specification Language (CCSL). The transformed constraints can be automatically and efficiently verified by model checking. We conduct two practical case studies to analyze the safety requirements of an interlocking system in CASCO Signal Ltd. Results of the studies show\u00a0\u2026", "num_citations": "3\n", "authors": ["1463"]}
{"title": "Eliciting Activity Requirements from Crowd Using Genetic Algorithm\n", "abstract": " Web-based software systems face a wide range of users and situates in different context. Developing such systems needs to deal with the diversity and variability of requirements. Crowd-based requirements engineering performs requirements engineering activities, such as elicitation requirements from the crowd of stakeholders. That leads to the collected requirements being more diverse and wider coverage. However, the requirements elicited from crowd are not directly available and need to be merged into system requirements. It is a tedious and error-prone work without the help of automatic method. System requirements can be expressed in a variety of ways, of which activity diagram is widely used. This paper provides a method based on genetic algorithm. This approach targets to solve two key issues about the individual requirements representation and the requirements synthesis, one is using a\u00a0\u2026", "num_citations": "3\n", "authors": ["1463"]}
{"title": "Learning high-dimensional multimedia data\n", "abstract": " The internet revolution has made information acquisition easy and cheap and has been producing massive high-dimensional multimedia data, including text, audio, images, animation, video, etc. High-dimensional multimedia data bring new opportunities to modern society and challenges to researchers of the multimedia domain as well. The goal of this special issue is bridging the gap between machine learning methods and the real requirement of high-dimensional multimedia domain, aiming at gaining insight into the relationship between the current multimedia and the past ones, and also accurately predicting the future trends of multimedia data. Specifically, this special issue targeted the most recent technical progresses on learning techniques for high-dimensional multimedia data", "num_citations": "3\n", "authors": ["1463"]}
{"title": "Integrating Goal model into rule-based adaptation\n", "abstract": " Goal-oriented adaptation provides a powerful mechanism to develop self-adaptive systems, enabling systems to keep satisfying user goals in a dynamically changing environment. The goal-oriented approach normally reduces the adaptation planning as a global optimization process and leaves the system the task of determining the actions required to achieve the goals. However, the high computation cost of global optimization prevents a self-adaptive system from quickly adjusting itself to the dynamically changing environment at runtime, which is intolerable since efficiency of planning is of utmost importance in most self-adaptive systems. On the other hand, rule-based adaptation has the advantage of efficient planning process since it predefines the adaptation logic by rules instead of leaving the system the task of reasoning. To combine the advantages of both approaches, we propose a novel adaptation\u00a0\u2026", "num_citations": "3\n", "authors": ["1463"]}
{"title": "Towards ontological approach to eliciting risk-based security requirements\n", "abstract": " Security requirements managers aim at eliciting, reusing and keeping their sets of requirements. They desire well defined, consistent and up to date requirements throughout the system lifecycle. This paper presents security ontology (SO) which can be used as a basis for eliciting risk-based security requirements. The ontology is based on the security relationship model described in the national institute of standards and technology special publication 800-12 but use-misuse case concepts and some extensions were used. We extended use case with some elements (action and object) to facilitate information system (IS) security policy instantiation after the system has been deployed. We incorporated risk and privilege concepts in order to represent risk knowledge in an unambiguous way and to enable ontology control security issues respectively. This ontology enriches the modelling and management of risk-based\u00a0\u2026", "num_citations": "3\n", "authors": ["1463"]}
{"title": "Eliciting dependability requirements: a control cases based approach\n", "abstract": " At present, great demands are posed on software dependability. But how to elicit the dependability requirements is still a challenging task. This paper proposes a novel approach to address this issue. The essential idea is to model a dependable software system as a feedforward-feedback control system, and presents the use cases + control cases model to express the requirements of the dependable software systems. In this model, while the use cases are adopted to model the functional requirements, two kinds of control cases (namely the feedforward control cases and the feedback control cases) are designed to model the dependability requirements. The use cases + control cases model provides a unified framework to integrate the modeling of the functional requirements and the dependability requirements at a high abstract level. To guide the elicitation of the dependability requirements, a HAZOP\u00a0\u2026", "num_citations": "3\n", "authors": ["1463"]}
{"title": "Analyzing early requirements of cyber-physical systems through structure and goal modeling\n", "abstract": " Integrating the computing process and the physical process, cyber-physical systems (CPS) pose many challenges to the system analysis and modeling. While most of the existing work focuses on developing the precise and formal model of CPS, little attentions have been given to the early requirements analysis and modeling which focuses on what the users' requirements are and what the software and physical domains of CPS will do to meet the users' requirements. In this paper, we provide an approach for early requirements analysis and modeling of CPS. This approach proposes to build the structure model to capture the system architecture, and the goal model to capture the refinement relationships between the users' requirements and the assumptions and requirements on the domains in CPS. These models help to build a clear understanding about CPS between the users and the designers and pave the\u00a0\u2026", "num_citations": "3\n", "authors": ["1463"]}
{"title": "Requirements modeling and system reconfiguration for self-adaptation of internetware\n", "abstract": " Internetware is a new paradigm proposed in recent years, which is defined as a kind of software constructed by a set of autonomous software entities distributed over the Internet. The environment where Internetware operates, including the Internet and other physical condition that the domain should hold, changes unexpectedly and comes with uncertainty. So the self-adaptation issue becomes inevitable in terms of dynamically configuring the Internetware. Here come two basic questions: how can we derive the adaptation problem in Internetware and how can we solve this problem with some adaptation mechanisms? In this paper, we focus on the requirements level for solving this problem and propose to use i* framework for modeling requirements of Internetware and deriving the adaptation problem from the i* models. To solve the adaptation problem, we provide a synergy approach involving i* models and\u00a0\u2026", "num_citations": "3\n", "authors": ["1463"]}
{"title": "An Agent Based Framework for Internetware Computing.\n", "abstract": " Internetware intends to be a paradigm of Web-based software development. At present, researches on Internetware have gained daily expanding attentions and interests. This paper proposes an agent based framework for Internetware computing. Four principles are presented that are followed by this framework. They are the autonomy principle, the abstract principle, the explicitness principle and the competence principle. Three types of agents with different responsibilities are designed and specified. They are the capability providing agents, the capability planning agents and the capability consuming agents. In this sense, capability decomposition and satisfaction turns to be a key issue for this framework and becomes a communication protocol among these distributed and heterogenous agents. A capability conceptualization is proposed and based on the conceptualization, an agent coalition formation mechanism has been developed. This mechanism features that (1) all the participants make their one decisions on whether or not joining the coalition based on the capability realization pattern generated by the capability planning agents as well as the benefits they can obtain; and (2) the coalition selection is conducted by a negotiation process for satisfying the expectations of all the participants as the complexity of this problem has been proven to be NP-complete.", "num_citations": "3\n", "authors": ["1463"]}
{"title": "Semantic approach for service oriented requirements modeling\n", "abstract": " Services computing is an interdisciplinary subject that devotes to bridging the gap between business services and IT services. It is recognized that Requirements Engineering is fundamental in implementing the service oriented architecture. It takes traditional RE techniques great efforts to model business requirements and search for the appropriate services. In this paper, we propose an ontological approach to facilitate the service-oriented modeling framework. The general idea is to establishing a common semantic language to describe both the business requirements and services capabilities based on their effects on the environment. After that, we used a case study to illustrate this method and showed that substantial efforts can be spared to construct a service model from business requirements.", "num_citations": "3\n", "authors": ["1463"]}
{"title": "Elicitation of Dependability Requirements: A HAZOP-based Approach\n", "abstract": " Dependability is gradually being recognized as an important issue for critical software systems. So far, there still lacks an effective approach for dependability requirements elicitation. This paper presents our work-in progress on an HAZOP-based approach (called DRE-HAZOP) to eliciting the dependability requirements in a systematical way.", "num_citations": "3\n", "authors": ["1463"]}
{"title": "Elicit the Requirements on Software Dependability: A Knowledge-Based Approach\n", "abstract": " Dependability, which is regarded to be an integrative concept characterized by properties such as reliability, safety, security, and maintainability, has been paid more and more attentions recently. However, it has not been followed with enough interest by traditional requirements engineering approaches. This paper proposes a knowledge based approach for dependability requirements elicitations. This approach argues that the knowledge captured from the data about software failure can be utilized for the dependable system development. A classification scheme of the threats to system dependability has been given and a meta-model of the dependability-related knowledge has been built. On the basis of the unified model of dependability (UMD), this paper utilizes the knowledge to help the elicitation of stakeholders' requirements on the system dependability. An online banking system has been used for illustrating\u00a0\u2026", "num_citations": "3\n", "authors": ["1463"]}
{"title": "Aggregation of autonomous internetware entities\n", "abstract": " Researches on Internetware have gained daily expanding attentions and interests. Internetware intends to be a framework of Web-based software development. A key issue in this framework is how these Internetware entities aggregate to form a coalition to fulfill the newly occurred requirements. This paper assumes that the Internetware entities distributed in Internet are autonomous and builds a mechanism for the aggregation of autonomous Internetware entities driven by requirements. A function ontology has been constructed for allowing the requirements to be understandable by these Interenetware entities so that these entities can recognize the requirements and realize that they are able to contribute for the realization of the requirements. After that, the requester and the contributors will negotiate to form an effective coalition.", "num_citations": "3\n", "authors": ["1463"]}
{"title": "A trust measurement mechanism for service agents\n", "abstract": " Trust is central to interactions in Web systems in which service consumers select the trustworthy service agents to provide the expected services. Currently, the analysis of the essential meaning of trust has not been paid enough attention as well as the reasoning process on trustworthiness. This paper proposes an ontology-based trust model which can offer more insightful understanding to the trust relationships among service agents in Web systems. A trust ontology with a set of trust computational rules have been given for supporting trust analysis and reasoning. Also the structure of the service-oriented agents has been given. Finally a case study is used to illustrate how the service agents collaborate with each others to determine the trustworthiness of their partners.", "num_citations": "3\n", "authors": ["1463"]}
{"title": "NIIA: nonparametric iterative imputation algorithm\n", "abstract": " Many missing data imputation methods are based on only complete instances (instances without missing values in a dataset) when estimating plausible values for the missing values in the dataset. Actually, the information within incomplete instances (instances with missing values) can also play an important role in missing value imputation. For example, the information has been applied to identifying the neighbors of an instance with missing values in NN (nearest neighbor) imputation, and the class of the instance in clustering-based imputation, where NN and clustering-based imputations are well-known efficient algorithms. Therefore, in this paper we advocate to well utilize the information within incomplete instances when estimating missing values. As an attempt, a simple and efficient nonparametric iterative imputation algorithm, called NIIA method, is designed for imputing iteratively missing target\u00a0\u2026", "num_citations": "3\n", "authors": ["1463"]}
{"title": "Requirement driven service agent coalition formation and negotiation\n", "abstract": " With the increasing of Web services in Internet, composing existing services for satisfying new requirements has gained daily expanding attentions and interests. Many efforts have been pursued for supporting the essential activities in service composition. However, the existing techniques only focus on passive services which are waiting there for being discovered and invoked. We argue that it might be more attractive when Web services become active entities. This paper proposes a framework for the requirement driven agent coalition formation and negotiation. The aim of this research is trying to allow the Web services to discover the requirements and interact with each other autonomously for satisfying the requirements. That will reduce the need for human mediation. With a function ontology, service agents and requirements can have the same terminology for describing their capability. Based on this ontology\u00a0\u2026", "num_citations": "3\n", "authors": ["1463"]}
{"title": "An agent-based trust model for service-oriented systems\n", "abstract": " In this paper, we propose an agent-based trust modeling architecture for service-oriented computing. A Trust Ontology is built to offer better understanding to the trust relationships in service-oriented systems. Based on this ontology, algorithms have been given for supporting trust analysis, reasoning and computing. Finally a case study is presented to illustrate how this formalism of service trust framework can be used. We argue that our trust model can assist service agents to make rational trading decisions by measuring the trustworthiness of the potential partners.", "num_citations": "3\n", "authors": ["1463"]}
{"title": "A method for web service description by using problem frames approach\n", "abstract": " This paper proposes a method for web service description by using problem frame approach. Different from existing methods, this method uses problem diagram to describe the capability of available web service and use it to identify precise requirements. Domain ontology has been developed as a sharable background for the description of both the provided services and the service requests, so that the service discovery can be conducted automatically. A case study shows that this approach is highly promising.", "num_citations": "3\n", "authors": ["1463"]}
{"title": "Web Service Retrieval based on Environment Ontology.\n", "abstract": " Existed descriptions of Web Service are in syntax level, and still relies on people to understand and decide what a Web Service can do, which encumbers automatic and dynamic Web Service retrieval. In this paper, we propose environment ontology as the base of describing semantic of Web Services, whose resource and state transitions reflect Web Service\u2019s behavior. We construct the description format based on environment ontology and put forward a road map approach towards automatic and dynamic Web Service retrieval.", "num_citations": "3\n", "authors": ["1463"]}
{"title": "A composition model of internet components based on environment transitions.\n", "abstract": " InternetWare is a new model of software that is running on Internet and has distinct features from conventional software. In this paper, environment, in which software exist and with which software interact, has been introduced as a first class concept describing the InternetWare. The capability of software was captured as the effectson its environment, i. e. the transitionsof the environment. Then a composition model of the InternetWare was established on the basis of the environment transitions. And finallya case about a Web bookstore was given to illustrate this composition model.", "num_citations": "3\n", "authors": ["1463"]}
{"title": "Managing non-canonical software requirements.\n", "abstract": " It may be desirable to manage non-canonical software requirements to facilitate requirements developmentand improve the quality of requirements specifications during the requirements stage. To address this, the framework for managing frequent non-canonical requirements is provided. The technique for reasoning about requirements in the presence of inconsistency plays an important role in the framework. Moreover, it could beused to manage other non-canonical requirements associated with inconsistency, including vague", "num_citations": "3\n", "authors": ["1463"]}
{"title": "Context-Aware Tree-Based Convolutional Neural Networks for Natural Language Inference\n", "abstract": " Natural language inference (NLI) aims to judge the relation between a premise sentence and a hypothesis sentence. In this paper, we propose a context-aware tree-based convolutional neural network (TBCNN) to improve the performance of NLI. In our method, we utilize tree-based convolutional neural networks, which are proposed in our previous work, to capture the premise\u2019s and hypothesis\u2019s information. In this paper, to enhance our previous model, we summarize the premise\u2019s information in terms of both word level and convolution level by dynamic pooling and feed such information to the convolutional layer when we model the hypothesis. In this way, the tree-based convolutional sentence model is context-aware. Then we match the sentence vectors by heuristics including vector concatenation, element-wise difference/product so as to remain low computational complexity. Experiments show that\u00a0\u2026", "num_citations": "2\n", "authors": ["1463"]}
{"title": "An approach to analyzing and resolving problems in the problem-driven requirements elicitation\n", "abstract": " Software requirements are stakeholders' expectations for the envisioned software. Researchers propose a problem driven approach to help stakeholders identify the requirements, that is, stakeholders firstly identify the problems of the current software, then find the solutions for the problems, and then elicit requirements based on the solutions. However, stakeholders usually cannot identify objective and consistent solutions, and describe the solutions clearly. Our previous work focuses on the whole process of the problem-driven scenario-based requirements elicitation. In this paper, we enrich our previous work by proposing a collaborative problem analysis and resolution approach, with the purpose of helping stakeholders identify the objective and consistent solutions. The basic idea of the approach is that stakeholders first discuss the understandability, value, and reasons of the problems; then they identify objective solutions through associating reasons to the solutions. To this end, we provide the classifications of problems, a meta-model of problems and collaborative elements, and a collaborative problem analysis process. Moreover, our approach provides mechanisms to help stakeholders handle the interrelated problems, and mechanisms to assess the degree of the collaboration. We have implemented a Web-based tool (ie, CPART) and used the \u201cUniversity Class System\u201d to conduct case studies, which shows that our approach is useful in practice.", "num_citations": "2\n", "authors": ["1463"]}
{"title": "An approach for selecting implementation strategies of non-functional requirements\n", "abstract": " Internetware literally means\" the software paradigm for the Internet\". The Problem Frames (PF) approach can be used to model Internetware entities. Internetware entities are running on the open, dynamic and uncertainty environment of Internet, which makes the issue of non-functional requirements even more important. However, the PF approach doesn't pay enough attention to deal with non-functional requirements. This paper proposes an approach for selecting implementation strategies of non-functional requirements. Finally, an example shows the feasibility of our approach.", "num_citations": "2\n", "authors": ["1463"]}
{"title": "Unifying domain ontology with agent-oriented modeling of services\n", "abstract": " Modeling plays a crucial role in model-driven development of service-oriented systems. This paper proposes a framework for service-oriented modeling that combines an agent-oriented software development methodology with an ontology-based domain analysis technique. It aims at improving the dynamic composability of services at requirements and design stages through modeling. The framework consists of an architectural structure of service models and a process of modeling. The architecture combines agent-oriented models of software systems in which service providers and requesters are regarded as autonomous entities (and called agents), and domain ontology, which specifies the entities in the application domain and their dynamic behaviors. The domain ontology extends classic ontology by including causal and symbolic entities as well as autonomous entities. The approach is illustrated by an\u00a0\u2026", "num_citations": "2\n", "authors": ["1463"]}
{"title": "Enhancing use cases with subjective risk assessment\n", "abstract": " The aim of this article is to advance the discussion of use-misuse cases as a tool for information system security risk assessment during system development. We closely examined the limitations and came up with some basic pointers that needed to be addressed in order to overcome the limitations. We proposed some solutions to these lacks and present a framework and modeling process to achieve the solutions. We illustrate the use of the proposed model on popular e-shop system as a case study. The proposed model is able to allow managers and system developers to share a commonly understand view concerning the potential impact of various information system related threats that make sense to them within their limited resources.", "num_citations": "2\n", "authors": ["1463"]}
{"title": "Towards a more fundamental explanation of constraints in feature models: A requirement-oriented approach\n", "abstract": " One basic construct in feature models (FMs) is the constraints between features, which play the role of ensuring the consistency and completeness of any configuration of a FM. However, most of the existing research about FMs views constraints between features as a kind of black-box entities, and cares little about more fundamental problems relating to them, such as what are the origins of them, and whether there is an insight explanation for their existence. In this paper, we try to provide a more fundamental explanation of constraints between features. The basic idea is that constraints among features are not imposed by external, but rooted in the nature of features \u2013 that is, a feature is a kind of container for requirements, and the constraints between features naturally inherit from the constraints between requirements. Following this idea, we identify two general situations that usually relate different\u00a0\u2026", "num_citations": "2\n", "authors": ["1463"]}
{"title": "Requirement Model and Satisfiability Decision for Service Composition\n", "abstract": " Generally, the achievement of service composition depends on finding the matched services according to user requirements. Nevertheless the user's needs, which are expressed by natural language usually, are difficult to be used for aggregating services. This paper proposes a requirement model based on environment ontology for composition service. This model is based on the concepts- intention which is defined on one environment entity, and task which is a set of associated intentions. Furthermore, Petri net is applied to represent the relationship between tasks in one requirement, and a method to distinguish requirement is given in this model. Finally, a specific example, travel arrangement, demonstrates the effectiveness of the model.", "num_citations": "2\n", "authors": ["1463"]}
{"title": "A problem-driven collaborative approach to eliciting requirements of internetwares\n", "abstract": " In the software development, most stakeholders cannot clearly and objectively express their needs for the envisioned software systems. In this paper, we propose a problem-driven collaborative requirements elicitation approach, with the purpose of helping identify and extract the requirements of the Internetwares (a complex and new software paradigm). The basic idea of our approach is that the requirements of the software systems should be stated by stakeholders in an objective way (ie problem-identifying-solving way). That is, first identify the problems existed in the as-is problem domain, and then find the solutions to the problems. The solutions to the problems are the requirements of the envisioned software systems. To this end, we propose the structure of problems and a collaborative process for achieving the solutions.", "num_citations": "2\n", "authors": ["1463"]}
{"title": "Verifying software requirements based on answer set programming\n", "abstract": " It is widely recognized that most software project failures result from problems about software requirements. Early verification of requirements can facilitate many problems associated with the software developments. The requirements testing is useful to clarify problematical information during the requirements stage. However, for any complex and sizeable system, the development of requirements typically involves different stakeholders with different concerns. Then the requirements specifications are increasingly developed in a distributed fashion. This makes requirements testing rather difficult. The main contribution of this paper is to present an answer set programming-based logical approach to testing requirements specifications. Informally, for an individual requirements test case, we consider the computation of the output of the system-to-be in requirements testing as a problem of answer set\u00a0\u2026", "num_citations": "2\n", "authors": ["1463"]}
{"title": "A Scenario-Based Problem Decomposition\n", "abstract": " Problem analysis has long been recognized as key to requirements engineering, for which problem frames approach offers a structured way by means of providing a generic paradigm for analyzing problems of various types. Problem decomposition is an important concern in structuring problem which has also been recognized as the key to mastering problem size and complexity. However, at present it still lacks of feasible and effective ways for decomposing problems. In this paper, we propose a scenario-based approach for tackling the issues on problem decomposition. A problem decomposition process is suggested. And a small case has been used to show the feasibility of this approach.", "num_citations": "2\n", "authors": ["1463"]}
{"title": "3rd international workshop on advances and applications of problem frames\n", "abstract": " Michael Jackson's Problem Frames are a highly promising approach to early life-cycle software engineering. Their focus moves the engineer back to the problem to be solved rather than forward to the software and solving a poorly defined problem. By applying the Problem Frames approach, the software engineer can understand the problem context and how it is to be affected by the proposed software, and ultimately work towards the right solution for the problem. The influence of the Problem Frames approach and related work is spreading in the fields of domain modelling, business process modelling, requirements engineering, software architecture as well as software engineering in general.", "num_citations": "2\n", "authors": ["1463"]}
{"title": "Service-Oriented Modeling: An Ontology-based Approach\n", "abstract": " Service-oriented computing has been obtaining more and more attention recently. Current works mainly concentrate on the service products and the standards to realize service-oriented computing. An important unaddressed problem is service-oriented modeling that is one of the fundamental elements you need to support SOA. This paper proposes an ontology-based approach for service-oriented modeling. This approach introduces hierarchical ontology systems in specific domain, which not only facilitate service reuse, but also facilitate reuse of application templates, collaboration patterns and workflows. This paper also presents an iterative process for service-oriented modeling.", "num_citations": "2\n", "authors": ["1463"]}
{"title": "An experiment for showing some kind of artificial understanding\n", "abstract": " In this paper we design an experiment which can be depicted as a simple scenario, a very limited \u2018world\u2019. In this world, there are an actor that can pursue a project and an observer that is keeping its eyes on the actor. We try to show in the experiment that the observer can to some degree understand the actor based on its knowledge and some metaphors, i.e. understand what the actor is doing and why. As the conclusion of this experiment, we try to show some features of \u2018understanding\u2019. These are (1) that \u2018understanding\u2019 has to be based on some preliminary knowledge; (2) that \u2018understanding\u2019 is a process of incremental learning; (3) that, as for symbolic systems, some metaphors are necessary for mapping real entities into concepts in mind.", "num_citations": "2\n", "authors": ["1463"]}
{"title": "A Lightweight Fault Localization Approach based on XGBoost\n", "abstract": " Software fault localization is one of the key activities in software debugging. The program spectrum-based approach is widely used in fault localization. However, lots of program information, for example, the sequence of the execution statement and statement semantics, is missing when such an approach is utilized, which affects the performance. XGBoost is an effective learning algorithm, which can use the characteristics of the training data to build a classification tree during training. In addition, XGBoost can iteratively adjust the information value of the feature, so that the training process retains the importance information of the feature. This paper proposes applying XGBoost into fault localization utilizing information of program execution behaviors. A novel method called XGB-FL is developed, where the program spectrum information is converted into a coverage matrix to train the XGBoost model. We can get the\u00a0\u2026", "num_citations": "1\n", "authors": ["1463"]}
{"title": "Towards Full-line Code Completion with Neural Language Models\n", "abstract": " A code completion system suggests future code elements to developers given a partially-complete code snippet. Code completion is one of the most useful features in Integrated Development Environments (IDEs). Currently, most code completion techniques predict a single token at a time. In this paper, we take a further step and discuss the probability of directly completing a whole line of code instead of a single token. We believe suggesting longer code sequences can further improve the efficiency of developers. Recently neural language models have been adopted as a preferred approach for code completion, and we believe these models can still be applied to full-line code completion with a few improvements. We conduct our experiments on two real-world python corpora and evaluate existing neural models based on source code tokens or syntactical actions. The results show that neural language models can achieve acceptable results on our tasks, with significant room for improvements.", "num_citations": "1\n", "authors": ["1463"]}
{"title": "Privacy-Aware UAV Flights through Self-Configuring Motion Planning\n", "abstract": " During flights, an unmanned aerial vehicle (UAV) may not be allowed to move across certain areas due to soft constraints such as privacy restrictions. Current methods on self-adaption focus mostly on motion planning such that the trajectory does not trespass predetermined restricted areas. When the environment is cluttered with uncertain obstacles, however, these motion planning algorithms are not flexible enough to find a trajectory that satisfies additional privacy-preserving requirements within a tight time budget during the flights. In this paper, we propose a privacy risk aware motion planning method through the reconfiguration of privacy-sensitive sensors. It minimises environmental impact by re-configuring the sensor during flight, while still guaranteeing the safety and energy hard constraints such as collision avoidance and timeliness. First, we formulate a model for assessing privacy risks of dynamically\u00a0\u2026", "num_citations": "1\n", "authors": ["1463"]}
{"title": "Open Models: Beyond the Open Source Software Development\n", "abstract": " Free/libre and open source software (FLOSS) originated from \"hacker culture\" and struggled against software privatization. The idea is that anyone is freely licensed to use, copy, study, and change the software in any way, and the source code is openly shared so that people are encouraged to voluntarily improve the design of the software. This is in contrast to proprietary software, where the software is under restrictive copyright licensing and the source code is usually hidden from the users. To give the users such freedom and control in their use of software, Richard Stallman launched the GNU project (Stallman, 1983), together with a manifesto stating that everyone will be permitted to modify and redistribute GNU.", "num_citations": "1\n", "authors": ["1463"]}
{"title": "Solving pictorial jigsaw puzzle by stigmergy-inspired Internet-based human collective intelligence\n", "abstract": " The pictorial jigsaw (PJ) puzzle is a well-known leisure game for humans. Usually, a PJ puzzle game is played by one or several human players face-to-face in the physical space. In this paper, we focus on how to solve PJ puzzles in the cyberspace by a group of physically distributed human players. We propose an approach to solving PJ puzzle by stigmergy-inspired Internet-based human collective intelligence. The core of the approach is a continuously executing loop, named the EIF loop, which consists of three activities: exploration, integration, and feedback. In exploration, each player tries to solve the PJ puzzle alone, without direct interactions with other players. At any time, the result of a player's exploration is a partial solution to the PJ puzzle, and a set of rejected neighboring relation between pieces. The results of all players' exploration are integrated in real time through integration, with the output of a continuously updated collective opinion graph (COG). And through feedback, each player is provided with personalized feedback information based on the current COG and the player's exploration result, in order to accelerate his/her puzzle-solving process. Exploratory experiments show that: (1) supported by this approach, the time to solve PJ puzzle is nearly linear to the reciprocal of the number of players, and shows better scalability to puzzle size than that of face-to-face collaboration for 10-player groups; (2) for groups with 2 to 10 players, the puzzle-solving time decreases 31.36%-64.57% on average, compared with the best single players in the experiments.", "num_citations": "1\n", "authors": ["1463"]}
{"title": "Call Graph Based Android Malware Detection with CNN\n", "abstract": " With the increasing shipment of Android malware, malicious APK detection becomes more important. Based on static analysis, we propose a new perspective to detect malicious behaviors. In particular, we extract the patterns of suspicious APIs which are invoked together. We call these patterns local features. We propose a convolutional neural network(CNN) model based on APK\u2019s call graph to extract local features. With the comparison of detection experiments, we demonstrate that the local features indeed help to detect malicious APKs and our model is effective in extracting local features.", "num_citations": "1\n", "authors": ["1463"]}
{"title": "Learning sparse overcomplete word vectors without intermediate dense representations\n", "abstract": " Dense word representation models have attracted a lot of interest for their promising performances in various natural language processing (NLP) tasks. However, dense word vectors are uninterpretable, inseparable, and time and space consuming. We propose a model to learn sparse word representations directly from the plain text, rather than most existing methods that learn sparse vectors from intermediate dense word embeddings. Additionally, we design an efficient algorithm based on noise-contrastive estimation (NCE) to train the model. Moreover, a clustering-based adaptive updating scheme for noise distributions is introduced for effective learning when NCE is applied. Experimental results show that the resulting sparse word vectors are comparable to dense vectors on the word analogy tasks. Our models outperform dense word vectors on the word similarity tasks. The sparse word vectors are\u00a0\u2026", "num_citations": "1\n", "authors": ["1463"]}
{"title": "A knowware based infrastructure for rule based control systems in smart spaces\n", "abstract": " Many software systems are designed for realising some business logics which can be captured and represented as a set of logic rules, such as the logic rules for controlling devices in smart spaces. These logic rules in traditional software systems are usually coded in procedural style and interweaved with other elements of software such as the interface implementation etc. As we know, the logic rules are usually needed to be changed or updated frequently according to changing environment conditions and software requirements. However, mix of the logic rules and other elements makes it difficult to change and update the logic rules. Therefore, we propose a new infrastructure to separate the logic rules from the interweaved elements. In our infrastructure, the logic rules can be encapsulated as a knowware. The knowware is deployed in a reasoner as an independent component, which can interact with\u00a0\u2026", "num_citations": "1\n", "authors": ["1463"]}
{"title": "Measuring software requirements evolution caused by inconsistency.\n", "abstract": " It has been widely recognized that requirements evolution is unavoidable in any sizeable software project. Moreover, if the requirement evolution is not managed properly, it may result in many troublesome problems during the process of software development. For example, poor management of requirements evolution may lead to inconsistencies in requirements and incomparability between requirements and other work products. Repairing these problems can lead to extra consumption of development resources. However, inconsistency is considered as one of the concerns of requirements evolution. In this paper, we propose a family of logic-based measures to evaluating software requirements evolution caused by inconsistency handling. Each of these measurements provides a distinctive perspective of quantitative description for the requirements evolution. At first, we provide a syntax-based measure for the change in requirements statements during the requirements evolution. Then we provide a semantics-based approach to measuring the change in the expression ability of requirements specification during the process of evolution. Finally, we characterize three special kinds of requirements evolution based on these measurements, including the evolved requirements specification with minimal change, the evolved requirements specification with minimal significance change, and the evolved requirements specification with maximal plausibility.", "num_citations": "1\n", "authors": ["1463"]}
{"title": "A process algebra for environment-based specification of web services\n", "abstract": " It is now well-accepted that formal methods are helpful for many problems raised in Web Service area. In this paper, we propose a new process algebra to formalize the environment-based specification of Web Services. From our point of view, the capability of a Web Service is considered in terms of the effects it imposes on the environment during its execution. The environment is composed of numbers of environment entities. For each entity, we first use the algebraic specification language CASL to describe its data(static) aspect, and then use our new process algebra to formally specify its dynamic behavior aspect. Based on that, a Web Service is specified as a process, and we can easily characterize its interaction with the environment. Furthermore, a behavior equivalence between two web services is established by using the notion of strong bisimulation in the new process algebra, some important properties of\u00a0\u2026", "num_citations": "1\n", "authors": ["1463"]}
{"title": "An approach to generating proposals for handling inconsistent software requirements\n", "abstract": " Inconsistency has been considered as one of the main classes of defects in software requirements specification. Various logic-based techniques have been proposed to manage inconsistencies in requirements engineering. However, identifying an appropriate proposal for resolving inconsistencies in software requirements is still a challenging problem. In this paper, we propose a logic-based approach to generating appropriate proposals for handling inconsistency in software requirements. Informally speaking, given an inconsistent requirements specification, we identify which requirements should be given priority to be changed for resolving the inconsistency in that specification, by balancing the blame of each requirement for the inconsistency against its value for that requirements specification. We follow the viewpoint that minimal inconsistent subsets of a set of formulas are the purest forms of\u00a0\u2026", "num_citations": "1\n", "authors": ["1463"]}
{"title": "From use case model to service model: An environment ontology based approach\n", "abstract": " One fundamental problem in services computing is how to bridge the gap between business requirements and various heterogeneous IT services. This involves eliciting business requirements and building a solution accordingly by reusing available services. While the business requirements are commonly elicited through use cases and scenarios, it is not straightforward to transform the use case model into a service model, and the existing manual approach is cumbersome and error-prone. In this paper, the environment ontology, which is used to model the problem space, is utilized to facilitate the model transformation process. The environment ontology provides a common understanding between business analysts and software engineers. The required software functionalities as well as the available services' capabilities are described using this ontology. By semi-automatically matching the required capability of\u00a0\u2026", "num_citations": "1\n", "authors": ["1463"]}
{"title": "Principles of Practice in Multi-Agent Systems: 12th International Conference, PRIMA 2009, Nagoya, Japan, December 14-16, 2009, Proceedings\n", "abstract": " Agents are software processes that perceive and act in an environment, processing their perceptions to make intelligent decisions about actions to achieve their goals. Multi-agent systems have multiple agents that work in the same environment to achieve either joint or conflicting goals. Agent computing and technology is an exciting, emerging paradigm expected to play a key role in many society-changing practices from disaster response to manufacturing to agriculture. Agent and mul-agent researchers are focused on building working systems that bring together a broad range of technical areas from market theory to software engineering to user interfaces. Agent systems are expected to operate in real-world environments, with all the challenges complex environments present. After 11 successful PRIMA workshops/conferences (Pacific-Rim International Conference/Workshop on Multi-Agents), PRIMA became a new conference titled \u201cInternational Conference on Principles of Practice in Multi-Agent Systems\u201d in 2009. With over 100 submissions, an acceptance rate for full papers of 25% and 50% for posters, a demonstration session, an industry track, a RoboCup competition and workshops and tutorials, PRIMA has become an important venue for multi-agent research. Papers submitted are from all parts of the world, though with a higher representation of Pacific Rim countries than other major multi-agent research forums. This volume presents 34 high-quality and exciting technical papers on multimedia research and an additional 18 poster papers that give brief views on exciting research.", "num_citations": "1\n", "authors": ["1463"]}
{"title": "Requirement Driven Service Agent Collaboration and QoS Based Negotiation\n", "abstract": " Composing existing services for satisfying new requirements has gained daily expanding attentions and interests. Existing techniques assume that services are passively waiting for being discovered and invoked. However, it might be more attractive when services become active entities (service agents) distributed in Internet which can recognize the newly emergent requirements. Generally, some service agents collaborate to satisfy the requirements, and some service agents compete with each other for realizing (part of) the requirements. This is the idea of requirement driven service agent collaboration. Along with our previous work, this paper proposes an agent collaboration model and a QoS based negotiation framework. Collaboration is modeled as an optimization problem, and subsequently enhanced by a QoS based negotiation framework. Through negotiation, the chosen solution satisfies both the QoS\u00a0\u2026", "num_citations": "1\n", "authors": ["1463"]}
{"title": "Balancing academic and industrial needs in RE courses\n", "abstract": " As people start to realize how poorly documented and managed requirements can affect the success of a software project, requirements engineering education start to attract growing attention in software institutes and companies in China. This paper reports our considerations regarding course materials selection, course projects design and evaluation to better balance the needs of training future requirements engineering researchers and software engineering professionals at undergraduate and graduate-level, as well as software professionals.", "num_citations": "1\n", "authors": ["1463"]}
{"title": "Some cognitive aspects of a turing test for children\n", "abstract": " Knowledge, cognition and intelligence are three tightly connected concepts. The Turing test is widely accepted as a test stone for machine intelligence. This paper analyzes experiences obtained in a research project on a Turing test for children and discusses its meaning with respect to some knowledge and cognition issues.", "num_citations": "1\n", "authors": ["1463"]}