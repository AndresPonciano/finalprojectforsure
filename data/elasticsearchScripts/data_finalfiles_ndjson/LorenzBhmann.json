{"title": "Template-based question answering over RDF data\n", "abstract": " As an increasing amount of RDF data is published as Linked Data, intuitive ways of accessing this data become more and more important. Question answering approaches have been proposed as a good compromise between intuitiveness and expressivity. Most question answering systems translate questions into triples which are matched against the RDF data to retrieve an answer, typically relying on some similarity metric. However, in many cases, triples do not represent a faithful representation of the semantic structure of the natural language question, with the result that more expressive queries can not be answered. To circumvent this problem, we present a novel approach that relies on a parse of the question to produce a SPARQL template that directly mirrors the internal structure of the question. This template is then instantiated using statistical entity identification and predicate detection. We show that this\u00a0\u2026", "num_citations": "552\n", "authors": ["1826"]}
{"title": "Managing the life-cycle of linked data with the LOD2 stack\n", "abstract": " The LOD2 Stack is an integrated distribution of aligned tools which support the whole life cycle of Linked Data from extraction, authoring/creation via enrichment, interlinking, fusing to maintenance. The LOD2 Stack comprises new and substantially extended existing tools from the LOD2 project partners and third parties. The stack is designed to be versatile; for all functionality we define clear interfaces, which enable the plugging in of alternative third-party implementations. The architecture of the LOD2 Stack is based on three pillars: (1) Software integration and deployment using the Debian packaging system. (2) Use of a central SPARQL endpoint and standardized vocabularies for knowledge base access and integration between the different tools of the LOD2 Stack. (3) Integration of the LOD2 Stack user interfaces based on REST enabled Web Applications. These three pillars comprise the methodological\u00a0\u2026", "num_citations": "153\n", "authors": ["1826"]}
{"title": "User-driven quality evaluation of dbpedia\n", "abstract": " Linked Open Data (LOD) comprises of an unprecedented volume of structured datasets on the Web. However, these datasets are of varying quality ranging from extensively curated datasets to crowdsourced and even extracted data of relatively low quality. We present a methodology for assessing the quality of linked data resources, which comprises of a manual and a semi-automatic process. The first phase includes the detection of common quality problems and their representation in a quality problem taxonomy. In the manual process, the second phase comprises of the evaluation of a large number of individual resources, according to the quality problem taxonomy via crowdsourcing. This process is accompanied by a tool wherein a user assesses an individual resource and evaluates each fact for correctness. The semi-automatic process involves the generation and verification of schema axioms. We report the\u00a0\u2026", "num_citations": "151\n", "authors": ["1826"]}
{"title": "Class expression learning for ontology engineering\n", "abstract": " While the number of knowledge bases in the Semantic Web increases, the maintenance and creation of ontology schemata still remain a challenge. In particular creating class expressions constitutes one of the more demanding aspects of ontology engineering. In this article we describe how to adapt a semi-automatic method for learning OWL class expressions to the ontology engineering use case. Specifically, we describe how to extend an existing learning algorithm for the class learning problem. We perform rigorous performance optimization of the underlying algorithms for providing instant suggestions to the user. We also present two plugins, which use the algorithm, for the popular Prot\u00e9g\u00e9 and OntoWiki ontology editors and provide a preliminary evaluation on real ontologies.", "num_citations": "150\n", "authors": ["1826"]}
{"title": "Autosparql: Let users query your knowledge base\n", "abstract": " An advantage of Semantic Web standards like RDF and OWL is their flexibility in modifying the structure of a knowledge base. To turn this flexibility into a practical advantage, it is of high importance to have tools and methods, which offer similar flexibility in exploring information in a knowledge base. This is closely related to the ability to easily formulate queries over those knowledge bases. We explain benefits and drawbacks of existing techniques in achieving this goal and then present the QTL algorithm, which fills a gap in research and practice. It uses supervised machine learning and allows users to ask queries without knowing the schema of the underlying knowledge base beforehand and without expertise in the SPARQL query language. We then present the AutoSPARQL user interface, which implements an active learning approach on top of QTL. Finally, we evaluate the approach based on a\u00a0\u2026", "num_citations": "142\n", "authors": ["1826"]}
{"title": "Sorry, i don't speak SPARQL: translating SPARQL queries into natural language\n", "abstract": " Over the past years, Semantic Web and Linked Data technologies have reached the backend of a considerable number of applications. Consequently, large amounts of RDF data are constantly being made available across the planet. While experts can easily gather information from this wealth of data by using the W3C standard query language SPARQL, most lay users lack the expertise necessary to proficiently interact with these applications. Consequently, non-expert users usually have to rely on forms, query builders, question answering or keyword search tools to access RDF data. However, these tools have so far been unable to explicate the queries they generate to lay users, making it difficult for these users to i) assess the correctness of the query generated out of their input, and ii) to adapt their queries or iii) to choose in an informed manner between possible interpretations of their input. This paper\u00a0\u2026", "num_citations": "101\n", "authors": ["1826"]}
{"title": "ORE-a tool for repairing and enriching knowledge bases\n", "abstract": " While the number and size of Semantic Web knowledge bases increases, their maintenance and quality assurance are still difficult. In this article, we present ORE, a tool for repairing and enriching OWL ontologies. State-of-the-art methods in ontology debugging and supervised machine learning form the basis of ORE and are adapted or extended so as to work well in practice. ORE supports the detection of a variety of ontology modelling problems and guides the user through the process of resolving them. Furthermore, the tool allows to extend an ontology through (semi-)automatic supervised learning. A wizard-like process helps the user to resolve potential issues after axioms are added.", "num_citations": "94\n", "authors": ["1826"]}
{"title": "DL-Learner\u2014A Framework for Inductive Learning on the Semantic Web\n", "abstract": " In this system paper, we describe the DL-Learner framework, which supports supervised machine learning using OWL and RDF for background knowledge representation. It can be beneficial in various data and schema analysis tasks with applications in different standard machine learning scenarios, e.g.\u00a0in the life sciences, as well as Semantic Web specific applications such as ontology learning and enrichment. Since its creation in 2007, it has become the main OWL and RDF-based software framework for supervised structured machine learning and includes several algorithm implementations, usage examples and has applications building on top of the framework. The article gives an overview of the framework with a focus on algorithms and use cases.", "num_citations": "93\n", "authors": ["1826"]}
{"title": "Distributed semantic analytics using the SANSA stack\n", "abstract": " A major research challenge is to perform scalable analysis of large-scale knowledge graphs to facilitate applications like link prediction, knowledge base completion and reasoning. Analytics methods which exploit expressive structures usually do not scale well to very large knowledge bases, and most analytics approaches which do scale horizontally (i.e., can be executed in a distributed environment) work on simple feature-vector-based input. This software framework paper describes the ongoing Semantic Analytics Stack (SANSA) project, which supports expressive and scalable semantic analytics by providing functionality for distributed computing on RDF data.", "num_citations": "80\n", "authors": ["1826"]}
{"title": "Real-time RDF extraction from unstructured data streams\n", "abstract": " The vision behind the Web of Data is to extend the current document-oriented Web with machine-readable facts and structured data, thus creating a representation of general knowledge. However, most of the Web of Data is limited to being a large compendium of encyclopedic knowledge describing entities. A huge challenge, the timely and massive extraction of RDF facts from unstructured data, has remained open so far. The availability of such knowledge on the Web of Data would provide significant benefits to manifold applications including news retrieval, sentiment analysis and business intelligence. In this paper, we address the problem of the actuality of the Web of Data by presenting an approach that allows extracting RDF triples from unstructured data streams. We employ statistical methods in combination with deduplication, disambiguation and unsupervised as well as supervised machine learning\u00a0\u2026", "num_citations": "76\n", "authors": ["1826"]}
{"title": "Defacto\u2014temporal and multilingual deep fact validation\n", "abstract": " One of the main tasks when creating and maintaining knowledge bases is to validate facts and provide sources for them in order to ensure correctness and traceability of the provided knowledge. So far, this task is often addressed by human curators in a three-step process: issuing appropriate keyword queries for the statement to check using standard search engines, retrieving potentially relevant documents and screening those documents for relevant content. The drawbacks of this process are manifold. Most importantly, it is very time-consuming as the experts have to carry out several search processes and must often read several documents. In this article, we present DeFacto (Deep Fact Validation)\u2014an algorithm able to validate facts by finding trustworthy sources for them on the Web. DeFacto aims to provide an effective way of validating facts by supplying the user with relevant excerpts of web pages as well as\u00a0\u2026", "num_citations": "70\n", "authors": ["1826"]}
{"title": "Hawk\u2013hybrid question answering using linked data\n", "abstract": " The decentral architecture behind the Web has led to pieces of information being distributed across data sources with varying structure. Hence, answering complex questions often requires combining information from structured and unstructured data sources. We present HAWK, a novel entity search approach for Hybrid Question Answering based on combining Linked Data and textual data. The approach uses predicate-argument representations of questions to derive equivalent combinations of SPARQL query fragments and text queries. These are executed so as to integrate the results of the text queries into SPARQL and thus generate a formal interpretation of the query. We present a thorough evaluation of the framework, including an analysis of the influence of entity annotation tools on the generation process of the hybrid queries and a study of the overall accuracy of the system. Our results show that\u00a0\u2026", "num_citations": "70\n", "authors": ["1826"]}
{"title": "Universal OWL axiom enrichment for large knowledge bases\n", "abstract": " The Semantic Web has seen a rise in the availability and usage of knowledge bases over the past years, in particular in the Linked Open Data initiative. Despite this growth, there is still a lack of knowledge bases that consist of high quality schema information and instance data adhering to this schema. Several knowledge bases only consist of schema information, while others are, to a large extent, a mere collection of facts without a clear structure. The combination of rich schema and instance data would allow powerful reasoning, consistency checking, and improved querying possibilities as well as provide more generic ways to interact with the underlying data. In this article, we present a light-weight method to enrich knowledge bases accessible via SPARQL endpoints with almost all types of OWL 2 axioms. This allows to semi-automatically create schemata, which we evaluate and discuss using DBpedia.", "num_citations": "69\n", "authors": ["1826"]}
{"title": "Pattern based knowledge base enrichment\n", "abstract": " Although an increasing number of RDF knowledge bases are published, many of those consist primarily of instance data and lack sophisticated schemata. Having such schemata allows more powerful querying, consistency checking and debugging as well as improved inference. One of the reasons why schemata are still rare is the effort required to create them. In this article, we propose a semi-automatic schemata construction approach addressing this problem: First, the frequency of axiom patterns in existing knowledge bases is discovered. Afterwards, those patterns are converted to SPARQL based pattern detection algorithms, which allow to enrich knowledge base schemata. We argue that we present the first scalable knowledge base enrichment approach based on real schema usage patterns. The approach is evaluated on a large set of knowledge bases with a quantitative and qualitative result\u00a0\u2026", "num_citations": "57\n", "authors": ["1826"]}
{"title": "Hybrid acquisition of temporal scopes for RDF data\n", "abstract": " Information on the temporal interval of validity for facts described by RDF triples plays an important role in a large number of applications. Yet, most of the knowledge bases available on the Web of Data do not provide such information in an explicit manner. In this paper, we present a generic approach which addresses this drawback by inserting temporal information into knowledge bases. Our approach combines two types of information to associate RDF triples with time intervals. First, it relies on temporal information gathered from the document Web by an extension of the fact validation framework DeFacto. Second, it harnesses the time information contained in knowledge bases. This knowledge is combined within a three-step approach which comprises the steps matching, selection and merging. We evaluate our approach against a corpus of facts gathered from Yago2 by using DBpedia and Freebase as\u00a0\u2026", "num_citations": "33\n", "authors": ["1826"]}
{"title": "Concept learning\n", "abstract": " One of the bottlenecks of the ontology construction process is the amount of work required with various figures playing a role in it: domain experts contribute their knowledge that has to be formalized by knowledge engineers so that it can be mechanized. As the gap between these roles likely makes the process slow and burdensome, this problem may be tackled by resorting to machine learning techniques. By adopting algorithms from inductive logic programming, the effort of the domain expert can be reduced, ie he has to label individual resources as instances of the target concept. From those labels, axioms can be induced, which can then be confirmed by the knowledge engineer. In this chapter, we survey existing methods in this area and illustrate three different algorithms in more detail. Some basics like refinement operators, decision trees and information gain are described. Finally, we briefly present implementations of those algorithms.", "num_citations": "32\n", "authors": ["1826"]}
{"title": "SPARQL2NL: verbalizing SPARQL queries\n", "abstract": " Linked Data technologies are now being employed by a large number of applications. While experts can query the backend of these applications using the standard query language SPARQL, most lay users lack the expertise necessary to proficiently interact with these applications. Consequently, non-expert users usually have to rely on forms, query builders, question answering or keyword search tools to access RDF data. Yet, these tools are usually unable to make the meaning of the queries they generate plain to lay users, making it difficult for these users to i) assess the correctness of the query generated out of their input, and ii) to adapt their queries or iii) to choose in an informed manner between possible interpretations of their input.", "num_citations": "25\n", "authors": ["1826"]}
{"title": "Dl-learner structured machine learning on semantic web data\n", "abstract": " The following paper is an extended summary of the journal paper\" DL-Learner A framework for inductive learning on the Semantic Web\". In this system paper, we describe the DL-Learner framework. It is beneficial in various data and schema analytic tasks with applications in different standard machine learning scenarios, eg life sciences, as well as Semantic Web specific applications such as ontology learning and enrichment. Since its creation in 2007, it has become the main OWL and RDF-based software framework for supervised structured machine learning and includes several algorithm implementations, usage examples and has applications building on top of the framework.", "num_citations": "23\n", "authors": ["1826"]}
{"title": "Inductive lexical learning of class expressions\n", "abstract": " Despite an increase in the number of knowledge bases published according to Semantic Web W3C standards, many of those consist primarily of instance data and lack sophisticated schemata, although the availability of such schemata would allow more powerful querying, consistency checking and debugging as well as improved inference. One of the reasons why schemata are still rare is the effort required to create them. Consequently, numerous ontology learning approaches have been developed to simplify the creation of schemata. Those approaches usually either learn structures from text or existing RDF data. In this submission, we present the first approach combining both sources of evidence, in particular we combine an existing logical learning approach with statistical relevance measures applied on textual resources. We perform an experiment involving a manual evaluation on 100 classes of the\u00a0\u2026", "num_citations": "23\n", "authors": ["1826"]}
{"title": "ASSESS\u2014automatic self-assessment using linked data\n", "abstract": " The Linked Open Data Cloud is a goldmine for creating open and low-cost educational applications: First, it contains open knowledge of encyclopedic nature on a large number of real-world entities. Moreover, the data being structured ensures that the data is both human- and machine-readable. Finally, the openness of the data and the use of RDF as standard format facilitate the development of applications that can be ported across different domains with ease. However, RDF is still unknown to most members of the target audience of educational applications. Thus, Linked Data has commonly been used for the description or annotation of educational data. Yet, Linked Data has (to the best of our knowledge) never been used as direct source of educational material. With ASSESS, we demonstrate that Linked Data can be used as a source for the automatic generation of educational material. By using\u00a0\u2026", "num_citations": "15\n", "authors": ["1826"]}
{"title": "SML-Bench\u2013A benchmarking framework for structured machine learning\n", "abstract": " The availability of structured data has increased significantly over the past decade and several approaches to learn from structured data have been proposed. These logic-based, inductive learning methods are often conceptually similar, which would allow a comparison among them even if they stem from different research communities. However, so far no efforts were made to define an environment for running learning tasks on a variety of tools, covering multiple knowledge representation languages. With SML-Bench, we propose a benchmarking framework to run inductive learning tools from the ILP and semantic web communities on a selection of learning problems. In this paper, we present the foundations of SML-Bench, discuss the systematic selection of benchmarking datasets and learning problems, and showcase an actual benchmark run on the currently supported tools.", "num_citations": "10\n", "authors": ["1826"]}
{"title": "A holistic natural language generation framework for the semantic web\n", "abstract": " With the ever-growing generation of data for the Semantic Web comes an increasing demand for this data to be made available to non-semantic Web experts. One way of achieving this goal is to translate the languages of the Semantic Web into natural language. We present LD2NL, a framework for verbalizing the three key languages of the Semantic Web, i.e., RDF, OWL, and SPARQL. Our framework is based on a bottom-up approach to verbalization. We evaluated LD2NL in an open survey with 86 persons. Our results suggest that our framework can generate verbalizations that are close to natural languages and that can be easily understood by non-experts. Therewith, it enables non-domain experts to interpret Semantic Web data with more than 91\\% of the accuracy of domain experts.", "num_citations": "8\n", "authors": ["1826"]}
{"title": "The Tale of Sansa Spark.\n", "abstract": " We demonstrate the open-source Semantic Analytics Stack (SANSA), which can perform scalable analysis of large-scale knowledge graphs to facilitate applications such as link prediction, knowledge base completion and reasoning. The motivation behind this work lies in the lack of scalable methods for analytics which exploit expressive structures underlying semantically structured knowledge bases. The demonstration is based on the BigDataEurope technical platform, which utilizes Docker technology. We present various examples of using SANSA in the form of interactive Spark notebooks, which are executed with Apache Zeppelin. The technical platform and the notebooks are available on SANSA Github and can be deployed on any Docker-enabled host, locally or in a Docker Swarm cluster.", "num_citations": "8\n", "authors": ["1826"]}
{"title": "Web-scale extension of RDF knowledge bases from templated websites\n", "abstract": " Only a small fraction of the information on the Web is represented as Linked Data. This lack of coverage is partly due to the paradigms followed so far to extract Linked Data. While converting structured data to RDF is well supported by tools, most approaches to extract RDF from semi-structured data rely on extraction methods based on ad-hoc solutions. In this paper, we present a holistic and open-source framework for the extraction of RDF from templated websites. We discuss the architecture of the framework and the initial implementation of each of its components. In particular, we present a novel wrapper induction technique that does not require any human supervision to detect wrappers for web sites. Our framework also includes a consistency layer with which the data extracted by the wrappers can be checked for logical consistency. We evaluate the initial version of REX on three different datasets. Our\u00a0\u2026", "num_citations": "8\n", "authors": ["1826"]}
{"title": "Knowledge base creation, enrichment and repair\n", "abstract": " This chapter focuses on data transformation to RDF and Linked Data and furthermore on the improvement of existing or extracted data especially with respect to schema enrichment and ontology repair. Tasks concerning the triplification of data are mainly grounded on existing and well-proven techniques and were refined during the lifetime of the LOD2 project and integrated into the LOD2 Stack. Triplification of legacy data, i.e. data not yet in RDF, represents the entry point for legacy systems to participate in the LOD cloud. While existing systems are often very useful and successful, there are notable differences between the ways knowledge bases and Wikis or databases are created and used. One of the key differences in content is in the importance and use of schematic information in knowledge bases. This information is usually absent in the source system and therefore also in many LOD knowledge\u00a0\u2026", "num_citations": "7\n", "authors": ["1826"]}
{"title": "Towards SPARQL-based induction for large-scale RDF data sets\n", "abstract": " We show how to convert OWL Class Expressions to SPARQL queries where the instances of that concept are with a specific ABox equal to the SPARQL query result. Furthermore, we implement and integrate our converter into the CELOE algorithm (Class Expression Learning for Ontology Engineering), where it replaces the position of a traditional OWL reasoner. This will foster the application of structured machine learning to the Semantic Web, since most data is readily available in triple stores. We provide experimental evidence for the usefulness of the bridge. In particular, we show that we can improve the run time of machine learning approaches by several orders of magnitude.", "num_citations": "6\n", "authors": ["1826"]}
{"title": "The geoknow handbook\n", "abstract": " Within the GeoKnow project, various tools are developed and integrated which aim to simplify managing geospatial Linked Data on the web. In this article, we summarise the state of the art and describe the status of open geospatial data on the web. We continue by presenting the Linked Data Stack as technical underpinning of GeoKnow and give a first presentation of the platform providing a light-weight integration of those tools.", "num_citations": "6\n", "authors": ["1826"]}
{"title": "Integrating new refinement operators in terminological decision trees learning\n", "abstract": " The problem of predicting the membership w.r.t. a target concept for individuals of Semantic Web knowledge bases can be cast as a concept learning problem, whose goal is to induce intensional definitions describing the available examples. However, the models obtained through the methods borrowed from Inductive Logic Programming e.g. Terminological Decision Trees, may be affected by two crucial aspects: the refinement operators for specializing the concept description to be learned and the heuristics employed for selecting the most promising solution (i.e. the concept description that describes better the examples). In this paper, we started to investigate the effectiveness of Terminological Decision Tree and its evidential version when a refinement operator available in DL-Learner and modified heuristics are employed. The evaluation showed an improvement in terms of the predictiveness.", "num_citations": "5\n", "authors": ["1826"]}
{"title": "User interface for a template based question answering system\n", "abstract": " As an increasing amount of RDF data is published as Linked Data, intuitive ways of accessing this data become more and more important. Natural language question answering approaches have been proposed as a good compromise between intuitiveness and expressiveness. We present a user interface for the template based question answering system which covers the full question answering pipeline and answers factual questions with a list of RDF resources. Users can ask full-sentence, English factual questions and get a list of resources which are then visualized using those properties which are expected to carry the most important information for the user. The available knowledge bases are (1) DBpedia for general domain question answering and (2) Oxford real estate for housing searches. However, the system is easily extensible to other knowledge bases.", "num_citations": "5\n", "authors": ["1826"]}
{"title": "Checking and repairing ontological naming patterns using ORE and PatOMat.\n", "abstract": " Analysis of the naming of entities across ontological structures can help reveal both naming issues and underlying conceptualization issues. Cross-entity naming analysis thus extends the standard logical satisfiability checking by an extra, less rigorous and reliable but often farther reaching layer. We show how such naming patterns can be applied within the transformation pattern paradigm used by the PatOMat transformation framework. We describe how the PatOMat tool has been integrated into the (logic-oriented) Ontology Repair and Enrichment tool (ORE), and present the results of application of a prominent naming pattern,\u2018non-matching child\u2019, on a collection of linked data vocabularies.", "num_citations": "5\n", "authors": ["1826"]}
{"title": "OWL class expression to SPARQL rewriting\n", "abstract": " Motivation: OWL/DLs wurden \u00fcber Jahrzehnte als Sprachen entwickelt um Konzepte einer Dom\u00e4ne abzubilden. W\u00e4hrend OWL durchaus verbreitet ist, gibt es trotzdem viele F\u00e4lle in denen Wissensbasen direkt eher per SPARQL zugreifbar sind. Mit diesem Artikel soll erm\u00f6glicht werden OWL-Ausdr\u00fccke in SPARQL umzuschreiben, so dass dabei OWL und SPARQL Semantik ber\u00fccksichtigt werden.", "num_citations": "4\n", "authors": ["1826"]}
{"title": "Facilitating data-flows at a global publisher using the LOD2 stack\n", "abstract": " The publishing industry is at the verge of an era, wherein particular professional customers of publishing products are not so much interested in comprehensive books and journals, ie traditional publishing products, anymore as they now are interested in possibly structured information pieces delivered just-in-time as a certain information need arises. This requires a transformation of the publishing workflows towards the production of much richer meta-data for fine-grained and highly interlinked pieces of content. Linked Data can play a crucial role in this transition. The LOD2 Stack is an integrated distribution of aligned tools which support the whole lifecycle of Linked Data from extraction, authoring/creation via enrichment, interlinking, fusing to maintenance. In this application paper, we describe a real-world usage scenario of the LOD2 stack at a global publishing company. We give an overview over the LOD2 Stack and the underlying life-cycle of Linked Data, describe data-flows and usage scenarios at a publisher and then show how the stack supports those scenarios.", "num_citations": "4\n", "authors": ["1826"]}
{"title": "Schema-agnostic SPARQL-driven faceted search benchmark generation\n", "abstract": " In this work, we present a schema-agnostic faceted browsing benchmark generation framework for RDF data and SPARQL engines. Faceted search is a technique that allows narrowing down sets of information items by applying constraints over their properties, whereas facets correspond to properties of these items. While our work can be used to realise real-world faceted search user interfaces, our focus lies on the construction and benchmarking of faceted search queries over knowledge graphs. The RDF model exhibits several traits that seemingly make it a natural foundation for faceted search: all information items are represented as RDF resources, property values typically already correspond to meaningful semantic classifications, and with SPARQL there is a standard language for uniformly querying instance and schema information.However, although faceted search is ubiquitous today, it is typically not\u00a0\u2026", "num_citations": "2\n", "authors": ["1826"]}
{"title": "A holistic natural language generation framework for the semantic web\n", "abstract": " With the ever-growing generation of data for the Semantic Web comes an increasing demand for this data to be made available to non-semantic Web experts. One way of achieving this goal is to translate the languages of the Semantic Web into natural language. We present LD2NL, a framework for verbalizing the three key languages of the Semantic Web, ie, RDF, OWL, and SPARQL. Our framework is based on a bottom-up approach to verbalization. We evaluated LD2NL in an open survey with 86 persons. Our results suggest that our framework can generate verbalizations that are close to natural languages and that can be easily understood by non-experts. Therewith, it enables non-domain experts to interpret Semantic Web data with more than 91\\% of the accuracy of domain experts.", "num_citations": "1\n", "authors": ["1826"]}
{"title": "Linked Data Reasoning\n", "abstract": " In diesem Kapitel beschreiben wir die Grundlagen des Reasonings in RDF/OWL-Wissensbasen. Wir gehen darauf ein, welche unterschiedlichen Arten des Reasonings es gibt, geben einen \u00dcberblick \u00fcber verwendete Verfahren und beschreiben deren Einsatz im Linked Data Web.", "num_citations": "1\n", "authors": ["1826"]}
{"title": "LOD2 Deliverable D3. 3.1: Release of Knowledge Base Enrichment Algorit hms\n", "abstract": " The prototype deliverable consists of a DL-Learner software release and an accompanying deliverable report. The software is open source and can be downloaded at http://dl-learner. org. It implement several knowledge base enrichment algorithms developed or extended within LOD2. The deliverable describes those algorithms in more detail. It first gives an overview of enrichment in context of Linked Data and OWL. Afterwards, it presents the algorithms and a brief evaluation on DBpedia. Finally, the software itself is described and important pointers are given.", "num_citations": "1\n", "authors": ["1826"]}