{"title": "Design Science Methodology for Information Systems and Software Engineering\n", "abstract": " This book provides guidelines for practicing design science in the fields of information systems and software engineering research. A design process usually iterates over two activities: first designing an artifact that improves something for stakeholders and subsequently empirically investigating the performance of that artifact in its context. This \u201cvalidation in context\u201d is a key feature of the book-since an artifact is designed for a context, it should also be validated in this context. The book is divided into five parts. Part I discusses the fundamental nature of design science and its artifacts, as well as related design research questions and goals. Part II deals with the design cycle, ie the creation, design and validation of artifacts based on requirements and stakeholder goals. To elaborate this further, Part III presents the role of conceptual frameworks and theories in design science. Part IV continues with the empirical cycle to investigate artifacts in context, and presents the different elements of research problem analysis, research setup and data analysis. Finally, Part V deals with the practical application of the empirical cycle by presenting in detail various research methods, including observational case studies, case-based and sample-based experiments and technical action research. These main sections are complemented by two generic checklists, one for the design cycle and one for the empirical cycle. The book is written for students as well as academic and industrial researchers in software engineering or information systems. It provides guidelines on how to effectively structure research goals, how to analyze research problems concerning design\u00a0\u2026", "num_citations": "1151\n", "authors": ["1450"]}
{"title": "Requirements engineering paper classification and evaluation criteria: a proposal and a discussion\n", "abstract": " In recent years, members of the steering committee of the IEEE Requirements Engineering (RE) Conference have discussed paper classification and evaluation criteria for RE papers. The immediate trigger for this discussion was our concern about differences in opinion that sometimes arise in program committees about the criteria to be used in evaluating papers. If program committee members do not all use the same criteria, or if they use criteria different from those used by authors, then papers might be rejected or accepted for the wrong reasons. Surely not all papers should be evaluated according to the same criteria. Some papers describe new techniques but do not report on empirical research; others describe new conceptual frameworks for investigating certain RE problems; others report on industrial experience with existing RE techniques. Other kinds of papers can also be easily recognized. All of these\u00a0\u2026", "num_citations": "933\n", "authors": ["1450"]}
{"title": "Design science as nested problem solving\n", "abstract": " Design science emphasizes the connection between knowledge and practice by showing that we can produce scientific knowledge by designing useful things. However, without further guidelines, aspiring design science researchers tend to identify practical problems with knowledge questions, which may lead to methodologically unsound research designs. To solve a practical problem, the real world is changed to suit human purposes, but to solve a knowledge problem, we acquire knowledge about the world without necessarily changing it. In design science, these two kinds of problems are mutually nested, but this nesting should not blind us for the fact that their problem-solving and solution justification methods are different. This paper analyzes the mutual nesting of practical problems and knowledge problems, derives some methodological guidelines from this for design science researchers, and gives an\u00a0\u2026", "num_citations": "431\n", "authors": ["1450"]}
{"title": "Requirements engineering: frameworks for understanding\n", "abstract": " Requirements engineering | Guide books ACM Digital Library home ACM home Google, Inc. (search) Advanced Search Browse About Sign in Register Advanced Search Journals Magazines Proceedings Books SIGs Conferences People More Search ACM Digital Library SearchSearch Advanced Search Browse Browse Digital Library Collections More HomeBrowse by TitleBooksRequirements engineering: frameworks for understanding Export Citation Select Citation format Download citation Copy citation Categories Journals Magazines Books Proceedings SIGs Conferences Collections People About About ACM Digital Library Subscription Information Author Guidelines Using ACM Digital Library All Holdings within the ACM Digital Library ACM Computing Classification System Join Join ACM Join SIGs Subscribe to Publications Institutions and Libraries Connect Contact Facebook Twitter Linkedin The ACM Digital \u2026", "num_citations": "383\n", "authors": ["1450"]}
{"title": "Deontic logic in computer science: normative system specification\n", "abstract": " Most of the papers in this collection are from the First International Workshop on Deontic Logic in Computer Science, DEON91, held in Amsterdam in December 1991. The artificial intelligence (especially AI and law, and knowledge representation) communities and formal system specification communities are the audiences that would seem to be most interested. In fact, as a researcher in AI, I was surprised to find common ground with a visiting researcher in distributed systems by discussing the contents of this book, he being in the same field as Wieringa, and I being in the same field as Meyer. The editors enumerate the applications as follows: automation of law, authorization, system specification, electronic contracting, integrity constraint, and database security. Three additional papers appear, from Carlos Alchourron, Risto Hilpinen, and Tom Maibaum. By far the most provocative paper in the collection is\u00a0\u2026", "num_citations": "359\n", "authors": ["1450"]}
{"title": "A survey of structured and object-oriented software specification methods and techniques\n", "abstract": " This article surveys techniques used in structured and object-oriented software specification methods. The techniques are classified as techniques for the specification of external interaction and internal decomposition.  The external specification techniques are further subdivided into techniques for the specification of functions, behavior, and communication. After surveying the techniques, we summarize the way they are used in structured and object-oriented methods and indicate ways in which they can be combined. This article ends with a plea for simplicity in diagram techniques and for the use of formal semantics to define these techniques. The appendices show how the reviewed techniques are used in 6 structured and 19 object-oriented specification methods.", "num_citations": "340\n", "authors": ["1450"]}
{"title": "Enterprise architecture: Management tool and blueprint for the organisation\n", "abstract": " In current business practice, an integrated approach to business and IT is indispensable. Take for example a company that needs to assess the impact of introducing a new product in its portfolio. This may require defining additional business processes, hiring extra personnel, changing the supporting applications, and augmenting the technological infrastructure to support the additional load of these applications. Perhaps this may even require a change of the organisational structure.However, in many companies such an integrated view of the entire enterprise is still far off. This is an important problem, because changes in a company\u2019s strategy and business goals have significant consequences within all domains of the enterprise, such as the organisation structure, business processes, software systems, data management and technical infrastructure. Companies have to adjust processes to their environment, open\u00a0\u2026", "num_citations": "336\n", "authors": ["1450"]}
{"title": "Design methods for reactive systems: Yourdon, Statemate, and the UML\n", "abstract": " Design Methods for Reactive Systems describes methods and techniques for the design of software systems\u2014particularly reactive software systems that engage in stimulus-response behavior. Such systems, which include information systems, workflow management systems, systems for e-commerce, production control systems, and embedded software, increasingly embody design aspects previously considered alone\u2014such as complex information processing, non-trivial behavior, and communication between different components\u2014aspects traditionally treated separately by classic software design methodologies. But, as this book illustrates, the software designer is better served by the ability to intelligently pick and choose from among a variety of techniques according to the particular demands and properties of the system under development. Design Methods for Reactive Systems helps the software designer meet today's increasingly complex challenges by bringing together specification techniques and guidelines proven useful in the design of a wide range of software systems, allowing the designer to evaluate and adapt different techniques for different projects. Written in an exceptionally clear and insightful style, Design Methods for Reactive Systems is a book that students, engineers, teachers, and researchers will undoubtedly find of great value. Shows how the techniques and design approaches of the three most popular design methods can be combined in a flexible, problem-driven manner. Pedagogical features include summaries, rehearsal questions, exercises, discussion questions, and numerous case studies.", "num_citations": "294\n", "authors": ["1450"]}
{"title": "Tool support for verifying UML activity diagrams\n", "abstract": " We describe a tool that supports verification of workflow models specified in UML activity diagrams. The tool translates an activity diagram into an input format for a model checker according to a mathematical semantics. With the model checker, arbitrary propositional requirements can be checked against the input model. If a requirement fails to hold, an error trace is returned by the model checker, which our tool presents by highlighting a corresponding path in the activity diagram. We summarize our formal semantics, discuss the techniques used to reduce an infinite state space to a finite one, and motivate the need for strong fairness constraints to obtain realistic results. We define requirement-preserving rules for state space reduction. Finally, we illustrate the whole approach with a few example verifications.", "num_citations": "195\n", "authors": ["1450"]}
{"title": "Technical action research as a validation method in information systems design science\n", "abstract": " Current proposals for combining action research and design science start with a concrete problem in an organization, then apply an artifact to improve the problem, and finally reflect on lessons learned. The aim of these combinations is to reduce the tension between relevance and rigor. This paper proposes another way of using action research in design science, which starts with an artifact, and then tests it under conditions of practice by solving concrete problems with them. The aim of this way of using action research in design science is to bridge the gap between the idealizations made when designing the artifact and the concrete conditions of practice that occur in real-world problems.               The paper analyzes the role of idealization in design science and compares it with the requirements of rigor and relevance. It then proposes a way of bridging the gap between idealization and practice by means of\u00a0\u2026", "num_citations": "188\n", "authors": ["1450"]}
{"title": "Applications of deontic logic in computer science: A concise overview\n", "abstract": " Deontic logic is the logic that deals with actual as well as ideal behavior of systems. In this paper, we survey a number of applications of deontic logic in computer science that have arisen in the eighties, and give a systematic framework in which these applications can be classified. Many applications move in the direction of programming a computer in deontic logic to make the computer prohibit, permit or obligate people to do something. We discuss conditions under which this possibility is realistic and conditions under which it would be admissible to do so.", "num_citations": "168\n", "authors": ["1450"]}
{"title": "Using Dynamic Classes and Role Classes to Model Object Migration\n", "abstract": " In this article, we argue that object\u2010oriented models must be able to represent three kinds of taxonomic structures: static classes, dynamic classes, and role classes, that behave differently with respect to object migration. If CAR is a static subclass of VEHICLE, then a vehicle that is not a car can never migrate to the CAR subclass. On the other hand, if EMPIoyee is a dynamic subclass of the PERSON object class, then a PERSON that is not an employee may migrate to EMP. In both cases, an instance of the subclass is identical to an instance of the superclass. By contrast, if EMP is modeled as a role class of PERSON, then every employee differs from every person, but a PERSON instance can acquire one or more EMP instances as roles. The distinctions between the three kinds of classes are orthogonal, so that we can have, for example, dynamic subclass of object or role classes, or role classes of dynamic or static\u00a0\u2026", "num_citations": "156\n", "authors": ["1450"]}
{"title": "Deontic logic: A concise overview\n", "abstract": " In this paper we shall give a short (and incomplete) historic overview of the branch of modal logic that is concerned with (reasoning about) norms and normative behaviour, viz. deontic logic. Typically, deontic logic has operators for deontic/normative modalities such as prohibition, permission and obligation. The paper is meant as an introduction to the more advanced papers that appear in the book.", "num_citations": "156\n", "authors": ["1450"]}
{"title": "Verification support for workflow design with UML activity graphs\n", "abstract": " We describe a tool that supports verification of workflow models specified in UML activity graphs. The tool translates an activity graph into an input format for a model checker according to a semantics we published earlier. With the model checker arbitrary propositional requirements can be checked against the input model. If a requirement fails to hold an error trace is returned by the model checker. The tool automatically translates such an error trace into an activity graph trace by high-lighting a corresponding path in the activity graph. One of the problems that is dealt with is that model checkers require a finite state space whereas workflow models in general have an infinite state space. Another problem is that strong fairness is necessary to obtain realistic results. Only model checkers that use a special model checking algorithm for strong fairness are suitable for verifying workflow models. We analyse the structure of\u00a0\u2026", "num_citations": "147\n", "authors": ["1450"]}
{"title": "Comparing Petri net and activity diagram variants for workflow modelling\u2013a quest for reactive Petri nets\n", "abstract": " Petri net variants are widely used as a workflow modelling technique. Recently, UML activity diagrams have been used for the same purpose, even though the syntax and semantics of activity diagrams has not been yet fully worked out. Nevertheless, activity diagrams seem very similar to Petri nets and on the surface, one may think that they are variants of each other. To substantiate or deny this claim, we need to formalise the intended semantics of activity diagrams and then compare this with various Petri net semantics. In previous papers we have defined two formal semantics for UML activity diagrams that are intended for workflow modelling. In this paper, we discuss the design choices that underlie these two semantics and investigate whether these design choices can be met in low-level and high-level Petri net semantics. We argue that the main difference between the Petri net semantics and our\u00a0\u2026", "num_citations": "138\n", "authors": ["1450"]}
{"title": "Specifying Dynamic and Deontic Integrity Constraints\n", "abstract": " In the dominant view of knowledge bases (KB's), a KB is a set of facts (atomic sentences) and integrity constraints (IC's). An IC is then a sentence which must at least be consistent with the other sentences in the KB, This view obliterates the distinction between, for example, the constraint that age is a natural number (which is true of the universe of discourse (UoD) but may be false in a particular implementation of a KB), and the constraint that a class must have precisely one teacher (which is false of the UoD if a class actually has two teachers). The second constraint is called deontic and constrains the UoD; the first constraint is a necessary truth of the UoD and does not constrain the UoD. Instead, it constrains the implementation of the KB. We argue that the distinction between necessary and deontic IC's is relevant for KB modeling and that it imposes a more complicated modeling discipline on the KB designer than\u00a0\u2026", "num_citations": "130\n", "authors": ["1450"]}
{"title": "Aligning application architecture to the business context\n", "abstract": " Alignment of application architecture to business architecture is a central problem in the design, acquisition and implementation of information systems in current large-scale information-processing organizations. Current research in architecture alignment is either too strategic or too software implementation-oriented to be of use to the practicing information systems architect. This paper presents a framework to analyze the alignment problem and operationalizes this as an approach to application architecture design given a business context. We summarize guidelines for application architecture design and illustrate our approach and guidelines with an example.", "num_citations": "127\n", "authors": ["1450"]}
{"title": "Roles and dynamic subclasses: a modal logic approach\n", "abstract": " In this paper, we argue that object-oriented models must be able to represent three kinds of taxonomic structures: static subclasses, dynamic subclasses and role classes. If CAR is a static subclass of VEHICLE, then a vehicle that is not a car can never migrate to the CAR subclass. If EM Ployee is a dynamic subclass of PERSON, then a PERSON that is not an employee may migrate to EMP. In both cases, an instance of the subclass is identical to an instance of the superclass. Finally, if EMP is modeled as a role class of PERSON every employee differs from every person, but a PERSON instance can acquire one or more EMP instances as roles. We outline an approach to formalizing these taxonomic structures in order-sorted dynamic logic with equality.", "num_citations": "113\n", "authors": ["1450"]}
{"title": "Design science methodology: principles and practice\n", "abstract": " Design scientists have to balance the demands of methodological rigor that they share with purely curiosity-driven scientists, with the demands of practical utility that they share with utility-driven engineers. Balancing these conflicting demands can be conceptually complex and may lead to methodological mistakes. For example, treating a design question as an empirical research question may lead to researcher to omit the identification of the practical problem to solve, to omit the identification of stakeholder-motivated evaluation criteria, or to omit trade-off and sensitivity analysis. This tutorial aims to clear up this methodological mist in the case of software engineering (SE) research.", "num_citations": "110\n", "authors": ["1450"]}
{"title": "The methodological soundness of requirements engineering papers: a conceptual framework and two case studies\n", "abstract": " This paper was triggered by concerns about the methodological soundness of many RE papers. We present a conceptual framework that distinguishes design papers from research papers, and show that in this framework, what is called a research paper in RE is often a design paper. We then present and motivate two lists of evaluation criteria, one for research papers and one for design papers. We apply both of these lists to two samples drawn from the set of all submissions to the RE\u201903 conference. Analysis of these two samples shows that most submissions of the RE\u201903 conference are design papers, not research papers, and that most design papers present a solution to a problem but neither validate this solution nor investigate the problems that can be solved by this solution. We conclude with a discussion of the soundness of our results and of the possible impact on RE research and practice.", "num_citations": "106\n", "authors": ["1450"]}
{"title": "The role of deontic logic in the specification of information systems\n", "abstract": " In this paper we discuss the role that deontic logic plays in the specification of information systems, either because constraints on the systems directly concern norms or, and even more importantly, system constraints are considered ideal but violable (so-called\u2019 soft\u2019 constraints). To overcome the traditional problems with deontic logic (the so-called paradoxes), we first state the importance of distinguishing between ought-to-be and ought-to-do constraints and next focus on the most severe paradox, the so-called Chisholm paradox, involving contrary-to-duty norms. We present a multi-modal extension of standard deontic logic (SDL) to represent the ought-to-be version of the Chisholm set properly. For the ought-to-do variant we employ a reduction to dynamic logic, and show how the Chisholm set can be treated adequately in this setting. Finally we discuss a way of integrating both ought-to-be and ought-to-do\u00a0\u2026", "num_citations": "104\n", "authors": ["1450"]}
{"title": "A Formal Semantics for UML Activity Diagrams: Formalising Workflow Models\n", "abstract": " In this report we define a formal execution semantics for UML activity diagrams that is appropriate for workflow modelling. Our workflow models express software requirements and therefore assume a perfect implementation. In our semantics, software state changes do not take time. It is based upon the Statemate semantics of statecharts, extended with some transactional properties to deal with data manipulation. Our semantics also deals with real time and with multiple state instances. We first give an informal description of our semantics and then formalise this in terms of labelled transition systems. We compare our semantics with other semantics for UML activity diagrams and workflow modelling by analysing the different choices made in those semantics.", "num_citations": "97\n", "authors": ["1450"]}
{"title": "The identification of objects and roles\n", "abstract": " In this paper we investigate the relation between object identifiers (oid\u2019s), object classification and class change. We give a precise definition of a concept of oid that solves the problems with DB merges and with the representation of historical information. We also extensively treat the differences between these oid\u2019s, keys, surrogates and pseudo-oid\u2019s, which are the object identifiers often used in practice. We distinguish object classes from role classes by defining roles (instances of role classes) to be part of the state of objects (instances of object classes). An advantage of distinguishing classes from roles is that we can avoid migration of objects between classes and thus the problems associated with it. Furthermore, we show that roles as well as objects must be represented by globally unique and unchangeable identifiers. As a consequence, there are two distinct taxonomic hierarchies, one for object classes and one for role classes. In both hierarchies, the inheritance mechanism at the instance level is based on identity. There is also inheritance between these two hierarchies, though, and a natural mechanism for this is delegation from roles to objects. Finally, we argue that pure oid\u2019s cannot be implemented in practice, but that they serve a useful purpose as ideal towards which practice should be oriented.", "num_citations": "87\n", "authors": ["1450"]}
{"title": "Object Identifiers, Keys, and Surrogates: Object Identifiers Revisited\n", "abstract": " Sound naming schemes for objects are crucial in many parts of computer science, such as database modeling, database implementation, distributed and federated databases, and networked and distributed operating systems. Over the past 20 years, physical pointers, keys, surrogates and object identifiers have been used as naming schemes in database systems and elsewhere. However, there are some persistent confusions about the nature, applicability and limits of these schemes. In this article we give a detailed comparison of three naming schemes, viz. object identifiers, internal identifiers (often called surrogates) and keys. We discuss several ways in which identification schemes can be implemented, and show what the theoretical and practical limits of applicability of identification schemes are, independently from how they are implemented. In particular, we discuss problems with the recognition and\u00a0\u2026", "num_citations": "86\n", "authors": ["1450"]}
{"title": "A Modal Approach to Intentions, Commitments and Obligations:Intention plus Commitment yields Obligation\n", "abstract": " In this paper we introduce some new operators into our framework that make it possible to reason about decisions and commitments to do actions. In our framework, a decision leads to an intention to do an action. The decision in itself does not change the state of the world, but only the relation to possible future worls. A commitment to actually perform the intended action changes the deontic state of the world such that the intended action becomes obligated. Of course, the obligated action may never actually occur. In our semantic structure, we use static (ought-to-be) and dynamic (ought-to-do) obligation operators. The static operator resembles the classical conception of obligation as truth in ideal worlds, except that it takes the current state as well as the past history of the world into account. This is necessary because it allows us to compare the way a state is actually reached with the way we committed\u00a0\u2026", "num_citations": "83\n", "authors": ["1450"]}
{"title": "An introduction to requirements traceability\n", "abstract": " This report surveys the requirements traceability literature and gives some recommendations for further research and for an approach to consultancy concerning traceability in the 2RARE project. The problem of maintaining traceability in a development project is viewed as the problem of maintaining an information system that maintains the relevant links between items developed during the process. These items may vary from requirements to design and implementation documents. To develop and implement such an information system, we must identify the needs for tracking information, made a model of this information, design usage procedures for the system, and implement the system. Accordingly, the literature on traceability is organized under the headings needs, models and usage. Furthermore, tracking tools are brie y reviewed.Acknowledgements: This report bene ted from input given by Eric Dubois, Anthony Finkelstein and Hanna Luden.", "num_citations": "83\n", "authors": ["1450"]}
{"title": "Project GRAAL: Towards operational architecture alignment\n", "abstract": " This paper presents a framework for architecture alignment that can be positioned between approaches for software architecture, which concern software artefacts only, and strategic alignment models, which have a business focus. The framework is currently applied in case study research to find alignment patterns used in practice. First results presented in this paper indicate that the framework might yield an operationalization of strategic architecture alignment models. We also present an alignment pattern which shows a difference between how architectures are designed at the application level and the infrastructure level. We think this difference is significant for practical alignment models.", "num_citations": "82\n", "authors": ["1450"]}
{"title": "Security implications of virtualization: A literature study\n", "abstract": " Server virtualization is a key technology for today's data centers, allowing dedicated hardware to be turned into resources that can be used on demand.However, in spite of its important role, the overall security impact of virtualization is not well understood.To remedy this situation, we have performed a systematic literature review on the security effects of virtualization. Our study shows that, given adequate management, the core virtualization technology has a clear positive effect on availability, but that the effect on confidentiality and integrity is less positive.Virtualized systems tend to lose the properties of location-boundedness, uniqueness and monotonicity.In order to ensure corporate and private data security, we propose to either remove or tightly manage non-essential features such as introspection, rollback and transfer.", "num_citations": "81\n", "authors": ["1450"]}
{"title": "Requirements-level semantics for UML statecharts\n", "abstract": " We propose a formal real-time semantics for UML statecharts aimed at the requirements level. A requirements-level model assumes perfect technology and has a considerably simpler semantics than an implementation level model. Our semantics is an adaptation of the Statemate statechart semantics, with local variables, real time, identifier addressing, point-to-point communication, synchronous communication and dynamic object creation and deletion. We start with an informal comparison of Statemate and UML statechart semantics and then give a formalisation of our semantics in terms of labelled transition systems.", "num_citations": "81\n", "authors": ["1450"]}
{"title": "Empirical research methods for technology validation: Scaling up to practice\n", "abstract": " Before technology is transferred to the market, it must be validated empirically by simulating future practical use of the technology. Technology prototypes are first investigated in simplified contexts, and these simulations are scaled up to conditions of practice step by step as more becomes known about the technology. This paper discusses empirical research methods for scaling up new requirements engineering (RE) technology.When scaling up to practice, researchers want to generalize from validation studies to future practice. An analysis of scaling up technology in drug research reveals two ways to generalize, namely inductive generalization using statistical inference from samples, and analogic generalization using similarity between cases. Both are supported by abductive inference using mechanistic explanations of phenomena observed in the simulations. Illustrations of these inferences both in drug\u00a0\u2026", "num_citations": "80\n", "authors": ["1450"]}
{"title": "Towards a service-oriented MDA-based approach to the alignment of business processes with IT systems: From the business model to a web service composition model\n", "abstract": " In recent years, the automation of business processes has become one of the most prominent and promising uses of Web service technology. Consequently several languages have been created for the execution of business processes, making it possible to define new and more complex services or business processes which are implemented for example by means of Web service composition. Nevertheless, these kinds of languages are not suitable for use in the early stages of the development process of information systems. Special methodologies or techniques are therefore necessary to allow systems analysts to understand services from a business point of view, while facilitating the design and development of Web service composition. In this paper, we present a service-oriented approach to information system development that starts by identifying, through business modeling, the services required by the\u00a0\u2026", "num_citations": "77\n", "authors": ["1450"]}
{"title": "Actors, actions, and initiative in normative system specification\n", "abstract": " The logic of norms, called deontic logic, has been used to specify normative constraints for information systems. For example, one can specify in deontic logic the constraints that a book borrowed from a library should be returned within three weeks, and that if it is not returned, the library should send a reminder. Thus, the notion of obligation to perform an action arises naturally in system specification. Intuitively, deontic logic presupposes the concept of anactor who undertakes actions and is responsible for fulfilling obligations. However, the concept of an actor has not been formalized until now in deontic logic. We present a formalization in dynamic logic, which allows us to express the actor who initiates actions or choices. This is then combined with a formalization, presented earlier, of deontic logic in dynamic logic, which allows us to specify obligations, permissions, and prohibitions to perform an action\u00a0\u2026", "num_citations": "73\n", "authors": ["1450"]}
{"title": "Algebraic Foundations for Dynamic Conceptual Models\n", "abstract": " 1.1 The problem of conceptual modeling 1 1.2 An architecture for conceptual models 3 1.3 Introduction to the main ideas of the thesis 6 1.4 Overview of the thesis 22", "num_citations": "70\n", "authors": ["1450"]}
{"title": "Relevance and problem choice in design science\n", "abstract": " The supposed opposition of rigor versus relevance is based on the mistaken idea that rigor consists of linear technology transfer combined with positivistic science, and ignores the context-dependence of relevance as well as the incorporation of conditions of practice necessary for applicability of knowledge. Historical insights from the history of science and technology show that technology is not transferred linearly from research to practice, and that technical science has more in common with social science than a superficial comparison would reveal. In both fields, (1) practical problems are often solved without input from research, and (2) researchers often investigate past innovations rather than prepare future ones. And in both fields, (3) relevance is context-dependent, because it depends on changeable goals of stakeholders. Applicability is a more important requirement than relevance to a goal, where\u00a0\u2026", "num_citations": "64\n", "authors": ["1450"]}
{"title": "Goal-oriented requirements engineering and enterprise architecture: Two case studies and some lessons learned\n", "abstract": " An enterprise-architecture (EA) is a high-level representation of the enterprise, used for managing the relation between business and IT. [Problem] Ideally, all elements of an enterprise architecture can be traced to business goals ad vice versa, but in practice, this is not the case. In this experience paper we explore the use of goal-oriented requirements engineering (GORE) techniques to improve this bidirectional traceability. [Principal ideas/results] We collected GORE techniques from KAOS, i*, Tropos, BMM and TOGAF and integrated them in a language called ARMOR. This was used by enterprise architects in case study. It turned out that the language was too complex for the architects to understand as intended. Based on this we redefined ARMOR to contain only a minimum number of goal-oriented concepts, and this was tested in a second case study. This second case study suggests that the minimal\u00a0\u2026", "num_citations": "63\n", "authors": ["1450"]}
{"title": "A formalization of objects using equational dynamic logic\n", "abstract": " Order-sorted equational logic is extended with dynamic logic to a specification language for dynamic objects. Special attention is paid to different concepts of encapsulation that play a role in object-orientation. It is argued that the resulting language, CMSL, meets those requirements of the object-oriented database system manifesto [6] that are applicable to object-oriented conceptual models (as opposed to OO databases).             Areas: Integrating logic and object paradigm, formalization of object-oriented concepts", "num_citations": "61\n", "authors": ["1450"]}
{"title": "An integrated conceptual model for information system security risk management supported by enterprise architecture management\n", "abstract": " Risk management is today a major steering tool for any organisation wanting to deal with information system (IS) security. However, IS security risk management (ISSRM) remains a difficult process to establish and maintain, mainly in a context of multi-regulations with complex and inter-connected IS. We claim that a connection with enterprise architecture management (EAM) contributes to deal with these issues. A first step towards a better integration of both domains is to define an integrated EAM-ISSRM conceptual model. This paper is about the elaboration and validation of this model. To do so, we improve an existing ISSRM domain model, i.e. a conceptual model depicting the domain of ISSRM, with the concepts of EAM. The validation of the EAM-ISSRM integrated model is then performed with the help of a validation group assessing the utility and usability of the model.", "num_citations": "59\n", "authors": ["1450"]}
{"title": "Model-based qualitative risk assessment for availability of IT infrastructures\n", "abstract": " For today\u2019s organisations, having a reliable information system is crucial to safeguard enterprise revenues (think of on-line banking, reservations for e-tickets etc.). Such a system must often offer high guarantees in terms of its availability; in other words, to guarantee business continuity, IT systems can afford very little downtime. Unfortunately, making an assessment of IT availability risks is difficult: incidents affecting the availability of a marginal component of the system may propagate in unexpected ways to other more essential components that functionally depend on them. General-purpose risk assessment (RA) methods do not provide technical solutions to deal with this problem. In this paper we present the qualitative time dependency (QualTD) model and technique, which is meant to be employed together with standard RA methods for the qualitative assessment of availability risks based on the\u00a0\u2026", "num_citations": "59\n", "authors": ["1450"]}
{"title": "A real-time execution semantics for UML activity diagrams\n", "abstract": " We define a formal execution semantics for UML activity diagrams that is appropriate for workflow modelling. Our semantics is aimed at the requirements level by assuming that software state changes do not take time. It is based upon the Statemate semantics of statecharts, extended with some transactional properties to deal with data manipulation. Our semantics also deals with real-time and multiple state instances.We first give an informal description of our semantics and then formalise this in terms of transition systems.", "num_citations": "59\n", "authors": ["1450"]}
{"title": "LCM 3.0: a language for describing conceptual models\n", "abstract": " The syntax of the conceptual model specification language LCM is defined. LCM uses equational logic to specify data types and order-sorted dynamic logic to specify objects with identity and mutable state. LCM specifies database transactions as finite sets of atomic object transitions.", "num_citations": "57\n", "authors": ["1450"]}
{"title": "Requirements-level semantics and model checking of object-oriented statecharts\n", "abstract": " In this paper we define a requirements-level execution semantics for object-oriented statecharts and show how properties of a system specified by these statecharts can be model checked using tool support for model checkers. Our execution semantics is requirements-level because it uses the perfect technology assumption, which abstracts from limitations imposed by an implementation. Statecharts describe object life cycles. Our semantics includes synchronous and asynchronous communication between objects and creation and deletion of objects. Our tool support presents a graphical front-end to model checkers, making these tools usable to people who are not specialists in model checking. The model-checking approach presented in this paper is embedded in an informal but precise method for software requirements and design. We discuss some of our experiences with model checking.", "num_citations": "53\n", "authors": ["1450"]}
{"title": "An integrated framework for ought-to-be and ought-to-do constraints\n", "abstract": " Deontic logic is the logic to reason about ideal and actual behaviour. Besides the traditional role as an underlying logic for law and ethics (for a survey see Meyer and Wieringa, 1993), deontic logic has been proposed as a logic for the specification of legal expert systems [Biagioli et al., 1987; Stamper, 1980], authorization mechanisms [Minsky and Lockman, 1985] decision support systems [Kimbrough, 1988; Lee, 1988a, 1988b], database security rules [Glasgow et al., 1988], fault-tolerant software [Khosla and Mailbaum, 1987; Coenen, 1993], and database integrity constraints [Wieringa et al., 1989, 1991]. A survey of applications can be found in [Wieringa and Meyer, 1993]. In all these areas, we must be able to reason about the difference between ideal and actual behaviour. In many cases, it is important to distinguish ought-to-do statements (which may be interpreted as expressing imperatives of the form\" an actor\u00a0\u2026", "num_citations": "53\n", "authors": ["1450"]}
{"title": "Requirements researchers: are we really doing research?\n", "abstract": " A few years ago, Davis and Hickey (2002) suggested that a reason why the results of requirements engineering research are not used in practice is that requirements engineering researchers do not practice what they preach: they do not analyze the problems of requirements engineering practice, and therefore their solutions do not address these problems. Although I agree with this diagnosis, I think it must be taken one step further in order to achieve a vision of a solution. The additional step that I propose here is to realize that currently, most requirements engineering researchers do not do research. Many of us create unvalidated designs, and move on from one unvalidated design to the other. The remedy that I propose is that we should learn to distinguish design from research, and start doing research. What we should do research about is the engineering process, so let me first explain what I mean by\u00a0\u2026", "num_citations": "50\n", "authors": ["1450"]}
{"title": "Thirty one Problems in the Semantics of UML 1.3 Dynamics\n", "abstract": " In this discussion paper we list a number of problems we found with the current dynamic semantics as presented in the defining document of UML: the UML specification version 1.3 [2](shortly UML 1.3 from now on). For some of these problem, we suggest solutions. The intention is to contribute to a further improvement of the dynamic semantics of the UML. All problems were encountered during the attempt to formalize UML 1.3 semantics or during the attempt to explain the semantics to computer science students.", "num_citations": "50\n", "authors": ["1450"]}
{"title": "Equational specification of dynamic objects\n", "abstract": " An equational language to specify object-oriented conceptual models is defined. Objects are considered to be characterized by a unique object identifier and have static and dynamic structure.", "num_citations": "50\n", "authors": ["1450"]}
{"title": "An execution algorithm for UML activity graphs\n", "abstract": " We present a real-time execution semantics for UML activity graphs that is intended for workflow modelling. The semantics is defined in terms of execution algorithms that define how components of a workflow system execute an activity graph. The semantics stays close to the semantics of UML state machines, but differs from it in some minor points. Our semantics deals with real time. The semantics provides a basis for verification of UML activity graphs, for example using model checking, and also for executing UML activity graphs using simulation tools. We illustrate an execution by means of a small example.", "num_citations": "49\n", "authors": ["1450"]}
{"title": "Axiomatization, declarative semantics and operational semantics of passive and active updates in logic databases\n", "abstract": " The use of logic in database theory is commonly restricted to the specification of database states. Reasoning about state changes (the database updates) must then be done outside the logic. In this report, we consider a logic that also takes the updates into account. Taking propostional dynamic logic as a starting point, we define PDDL: prepositional dynamic database logic. The main features of PDDL are:         \u2022There are two kinds of atomic updates in PDDL, passive and active updates. Passive updates just change the truth value of an atom to true/false and active updates set one atom to true/false and then compute derived updates using a logic program. Just like the atomic actions of dynamic logic, the atomic updates can be combined into update programs with the operators sequential composition, choice and iteration. We have one more update action (also present in dynamic logic): the test of a formula\u00a0\u2026", "num_citations": "48\n", "authors": ["1450"]}
{"title": "The inheritance of dynamic and deontic integrity constraints\n", "abstract": " In [18,23], we presented a language for the specification of static, dynamic and deontic integrity constraints (IC's) for conceptual models (CM's). An important problem not discussed in that paper is how IC's are inherited in a taxonomic network of types. For example, if students are permitted to perform certain actions under certain preconditions, must we repeat these preconditions when specializing this action for the subtype of graduate students, or are they inherited, and if so, how? For static constraints, this problem is relatively trivial, but for dynamic and deontic constraints, it will turn out that it contains numerous pitfalls, caused by the fact that common sense supplies presuppositions about the structure of IC inheritance that are not warranted by logic. In this paper, we unravel some of these presuppositions and show how to avoid the pitfalls. We first formulate a number of general theorems about the\u00a0\u2026", "num_citations": "46\n", "authors": ["1450"]}
{"title": "Design science, engineering science and requirements engineering\n", "abstract": " For several decades there has been a debate in the computing sciences about the relative roles of design and empirical research, and about the contribution of design and research methodology to the relevance of research results. In this minitutorial we review this debate and compare it with evidence about the relation between design and research in the history of science and technology. Our review shows that research and design are separate but concurrent activities, and that relevance of research results depends on problem setting rather than on rigorous methods. We argue that rigorous scientific methods separate design from research, and we give simple model for how to do this in a problem-driven way.", "num_citations": "45\n", "authors": ["1450"]}
{"title": "Current established risk assessment methodologies and tools\n", "abstract": " The technology behind information systems evolves at an exponential rate, while at the same time becoming more and more ubiquitous. This brings with it an implicit rise in the average complexity of systems as well as the number of external interactions. In order to allow a proper assessment of the security of such (sub)systems, a whole arsenal of methodologies, methods and tools have been developed in recent years. However, most security auditors commonly use a very small subset of this collection, that best suits their needs. This thesis aims at uncovering the differences and limitations of the most common Risk Assessment frameworks, the conceptual models that support them, as well as the tools that implement them. This is done in order to gain a better understanding of the applicability of each method and/or tool and suggest guidelines to picking the most suitable one.", "num_citations": "43\n", "authors": ["1450"]}
{"title": "Guest editors' introduction: RE'03: practical requirements engineering solutions\n", "abstract": " Over the course of the last decade, requirements engineering has evolved into a multidisciplinary field that blends software engi-neering, systems engineering, product management, and psychology. To put it in precise words, RE is the branch of systems engineering concerned with the desired properties and constraints of software-intensive systems, the goals to be achieved in the software\u2019s environment, and assumptions about the environment. It deals with these aspects guest editors\u2019 introduction", "num_citations": "40\n", "authors": ["1450"]}
{"title": "Integrating Semi-formal and Formal Software Specification Techniques\n", "abstract": " In this paper, we report on the integration of informal, semiformal and formal system specification techniques. We present a framework for system specification called TRADE, within which several well-known semiformal specification techniques are placed. TRADE is based on an analysis of structured and object-oriented requirements specification methods. In this paper, we combine TRADE with the logic-based specification language Albert II and show that this leads to a coherent formal and semiformal requirements specification. We illustrate our approach with examples taken from a large distributed telecommunication application case study, performed in the context of the Esprit project 2RARE.", "num_citations": "40\n", "authors": ["1450"]}
{"title": "Cyber-crime science= crime science+ information security\n", "abstract": " Cyber-crime Science is an emerging area of study aiming to prevent cyber-crime by combining security protection techniques from Information Security with empirical research methods used in Crime Science. Information Security research has developed techniques for protecting the confidentiality, integrity, and availability of information assets but is less strong on the empirical study of the effectiveness of these techniques. Crime Science studies the effect of crime prevention techniques empirically in the real world, and proposes improvements to these techniques based on this. Combining both approaches, Cyber-crime Science transfers and further develops Information Security techniques to prevent cyber-crime, and empirically studies the effectiveness of these techniques in the real world. In this paper we review the main contributions of Crime Science as of today, illustrate its application to typical Information Security problems, namely phishing and on-line auction fraud, explore the interdisciplinary structure of Cyber-crime Science, and present an agenda for research in Cyber-crime Science in the form of a set of suggested research questions.[C. 2.0] Computer-communication networks General [Security and protection]", "num_citations": "39\n", "authors": ["1450"]}
{"title": "A comparison of Petri net and activity diagram variants\n", "abstract": " Petri net variants are widely used as a workflow modelling technique. Recently, UML activity diagrams are used for the same purpose, even though the syntax and semantics of activity diagrams has not been yet fully worked out. Nevertheless, activity diagrams seem very similar to Petri nets and on the surface, one may think that they are variants of each other. To substantiate this claim, we need to formalise the intended semantics of activity diagrams and then compare this with Petri Net semantics. In previous papers we have defined two formal semantics for UML activity diagrams that are intended for workflow modelling. In this paper, we discuss the design choices that underlie these two semantics and investigate whether these design choices are met in low-level and highlevel Petri net semantics. We argue that the main difference between the Petri net semantics and our semantics of UML activity diagrams is that the Petri net semantics models closed, active systems that are non-reactive, whereas our semantics of UML activity diagrams models open, reactive systems. Since workflow systems are open, reactive systems, we conclude that Petri nets are not entirely suitable for workflow modelling. We end with a discussion of the issues involved in defining a reactive Petri net semantics.", "num_citations": "38\n", "authors": ["1450"]}
{"title": "Benefits of location-based access control: A literature study\n", "abstract": " Location-based access control (LBAC) has been suggested as a means to improve IT security. By `grounding' users and systems to a particular location, attackers supposedly have more difficulty in compromising a system. However, the motivation behind LBAC and its potential benefits have not been investigated thoroughly. To this end, we perform a structured literature review, and examine the goals that LBAC can potentially fulfill, the specific LBAC systems that realize these goals and the context on which LBAC depends. Our paper has four main contributions: first we propose a theoretical framework for LBAC evaluation, based on goals, systems and context. Second, we formulate and apply criteria for evaluating the usefulness of an LBAC system. Third, we identify four usage scenarios for LBAC: open areas and systems, hospitals, enterprises, and finally data centers and military facilities. Fourth, we propose\u00a0\u2026", "num_citations": "37\n", "authors": ["1450"]}
{"title": "Risk-Based Confidentiality Requirements Specification for Outsourced IT Systems (Extended Version)\n", "abstract": " Today, companies are required to be in control of their IT assets, and to provide proof of this in the form of independent IT audit reports. However, many companies have outsourced various parts of their IT systems to other companies, which potentially threatens the control they have of their IT assets. To provide proof of being in control of outsourced IT systems, the outsourcing client and outsourcing provider need a written service level agreement (SLA) that can be audited by an independent party. SLAs for availability and response time are common practice in business, but so far there is no practical method for specifying confidentiality requirements in an SLA. Specifying confidentiality requirements is hard because in contrast to availability and response time, confidentiality incidents cannot be monitored: attackers who breach confidentiality try to do this unobserved by both client and provider. In addition, providers usually do not want to reveal their own infrastructure to the client for monitoring or risk assessment. Elsewhere, we have presented an architecture-based method for confidentiality risk assessment in IT outsourcing. In this paper, we adapt this method to confidentiality requirements specification, and present a case study to evaluate this new method.", "num_citations": "37\n", "authors": ["1450"]}
{"title": "Designing requirements engineering research\n", "abstract": " Engineering sciences study different different topics than natural sciences, and utility is an essential factor in choosing engineering research problems. But despite these differences, research methods for the engineering sciences are no different than research methods for any other kind of science. At most there is a difference in emphasis. In the case of requirements engineering research-and more generally software engineering research-there is a confusion about the relative roles of research and about design and the methods appropriate for each of these activities. This paper analyzes these roles and provides a classification of research methods that can be used in any science-engineering or otherwise.", "num_citations": "35\n", "authors": ["1450"]}
{"title": "Semantic and Pragmatic Interoperability: A Model for Understanding.\n", "abstract": " In this paper we present a conceptual model for understanding of semantic and pragmatic interoperability. We use the model to identify and classify the possible semantic interoperability problems.", "num_citations": "33\n", "authors": ["1450"]}
{"title": "Requirements engineering: problem analysis and solution specification\n", "abstract": " Taken literally, the term \u201crequirements engineering\u201d (RE) is a misnomer. A requirement is something that is wanted; engineering, according to Webster\u2019s, is calculated manipulation. If our wants would arise by calculated manipulation, then something would be wrong. Our wants should not be engineered. What should be engineered, are solutions that meet our wants.", "num_citations": "33\n", "authors": ["1450"]}
{"title": "Object-Oriented Analysis, Structured Analysis, and Jackson System Development\n", "abstract": " Conceptual modeling is the activity of producing a conceptual model of an actual or desired version of a universe of discourse (UoD). In this paper, two methods of conceptual modeling are compared, structured analysis (SA) and object-oriented analysis (OOA). This is done by transforming a model produced by the one into a model produced by the other method, using heuristics from several sources, such as Jackson system development and formal specification. It is shown that SA and OOA diverge in three important respects. First, the ordering of tasks in SA is shown to be virtually opposite to the task ordering in OOA. Second, a model produced by SA mixes information about the communication between objects as well as about the life cycle local to an object, which is separated in a model produced by the OOA method we propose in this paper. Third, the heuristics in SA are shown to be data-oriented, which leads to quite different modularization decisions than the object-oriented heuristics proper to OOA. The different approach taken by OOA on all three points is shown to lead to simpler models that better reflect the structure of the UoD.", "num_citations": "33\n", "authors": ["1450"]}
{"title": "Dynamic database logic: The first-order case\n", "abstract": " We present Dynamic Database Logic (DDL), a language designed to reason about database updates. DDL is based on dynamic logic. We give a declarative semantics and a sound proof system for DDL (for a restricted case, we also have a completeness result). We show with a detailed example, how the proof system of DDL can be used to prove correctness of an update program given some specification for the update program. The update language part of DDL is shown to be update complete and specification complete (in the sense of Abiteboul & Vianu).", "num_citations": "32\n", "authors": ["1450"]}
{"title": "Three Roles of Conceptual Models in Information System Design and Use.\n", "abstract": " This paper attempts to draw together results from information systems research, linguistic theory, and methodology in order to present a unified framework in which to understand conceptual models. Three different roles of conceptual models (CM's) in the design and use of information systems (IS's) are investigated. The descriptive role of a CM is that it is an abstract representation of the universe of discourse (UoD) of the IS; the normative role of a CM is that it contains prescriptions for the behavior of entities in the UoD. A third role of CM's emerges when a computer is viewed as a symbol-manipulating machine capable of performing speech acts like commanding and promising. These acts are commands or promises only against a background of shared conventions, which is stored in a shared CM. A CM playing this role is called institutional. This paper is an abstract of Wieringa [1989].", "num_citations": "32\n", "authors": ["1450"]}
{"title": "Combining static and dynamic modelling methods: a comparison of four methods\n", "abstract": " A conceptual model of a system is an explicit description of the behaviour required of the system. Methods for conceptual modelling include entity-relationship (ER) modelling, data flow modelling, Jackson System Development (JSD) and several object-oriented analysis methods. Given the current diversity of modelling methods, it is important for teaching as well as using these methods to know what the relationships between them is and to be able to indicate what the (im)possibilities of integrating different methods are. This paper compares three classical modelling methods (ER, data flow, JSD) on their possibilities for integration and combination. It is shown that there is a common core of these methods, which centres around the concept of system transaction and that unifies the static view of a system taken by ER modelling, with the dynamic view taken by JSD and the functional view taken by data flow\u00a0\u2026", "num_citations": "31\n", "authors": ["1450"]}
{"title": "External insider threat: A real security challenge in enterprise value webs\n", "abstract": " Increasingly, organizations collaborate with other organizations in value webs with various arrangements, such as outsourcing, partnering, joint ventures, or subcontracting. As the Jericho Forum (an industry consortium of the Open Group) observed, in all these forms of collaboration, the boundaries between organizations become permeable and, as a consequence, insiders and outsiders can no longer be neatly separated using the notion of a perimeter. Such organizational arrangements have security implications because individuals from the value web are neither outsiders nor completely insiders. To address this phenomenon this paper proposes a third set of individuals, called External Insiders. External insiders add challenges to the already known insider threat problem because, unlike outsiders, external insiders have granted access and are trusted; and, unlike traditional insiders, external insiders are not\u00a0\u2026", "num_citations": "30\n", "authors": ["1450"]}
{"title": "Integrating semi-formal and formal requirements\n", "abstract": " In this paper, we report on the integration of informal, semiformal and formal requirements specification techniques. We present a framework for requirements specification called TRADE, within which several well-known semiformal specification techniques are placed. TRADE is based on an analysis of structured and object-oriented requirements specification methods. In this paper, we combine TRADE with the logic-based specification language Albert II and show that this leads to a coherent formal and semiformal requirements specification. We illustrate our approach with examples taken from a large distributed telecommunication application case study performed in the context of the Esprit project 2RARE.", "num_citations": "28\n", "authors": ["1450"]}
{"title": "Postmodern software design with NYAM: Not yet another method\n", "abstract": " This paper presents a conceptual toolbox for software specification and design that contains techniques from structured and object-oriented specification and design methods. The toolbox is called TRADE (Toolkit for Requirements and Design Engineering). The TRADE tools are used in teaching informatics students structured and object-oriented specification and design techniques, but the toolkit may be of use to practicing software engineers as well. The conceptual framework of TRADE distinguishes external system interactions from internal components. External interactions in turns are divided into external functions, behavior and communication. The paper shows that structured and OO analysis offer a small number of specification techniques for these aspects, most of which can be combined in a coherent software design specification. It is also shown that the essential difference between structured\u00a0\u2026", "num_citations": "27\n", "authors": ["1450"]}
{"title": "Steps towards a method for the formal modeling of dynamic objects\n", "abstract": " Fragments of a method to formally specify object-oriented models of a universe of discourse are presented. The task of finding such models is divided into three subtasks, object classification, event specification, and the specification of the life cycle of an object. Each of these subtasks is further subdivided, and for each of the subtasks heuristics are given that can aid the analyst in deciding how to represent a particular aspect of the real world. The main sources of inspiration are Jackson System Development, algebraic specification of data- and object types, and algebraic specification of processes.", "num_citations": "27\n", "authors": ["1450"]}
{"title": "Improving requirements engineering by artefact orientation\n", "abstract": " The importance of continuously improving requirements engineering (RE) has been recognised for many years. Similar to available software process improvement approaches, most RE improvement approaches focus on a normative and solution-driven assessment of companies rather than on a problem-driven RE improvement. The approaches dictate the implementation of a one-size-fits-all reference model without doing a proper problem investigation first, whereas the notion of quality factually depends on whether RE achieves company-specific goals. The approaches furthermore propagate process areas and methods, without proper awareness of the quality in the created artefacts on which the quality of many development phases rely. Little knowledge exists about how to conduct a problem-driven RE improvement that gives attention to the improvement of the artefacts. A promising solution is to start\u00a0\u2026", "num_citations": "26\n", "authors": ["1450"]}
{"title": "A fixed-point characterization of a deontic logic of regular action\n", "abstract": " We define a deontic logic of regular action as a characterization within a modal \u03bc-calculus of action. First a semantics of deontic notions for regular action is given in terms of conditions on modal action structures. Then modal \u03bc-calculus formulas characterizing these conditions are constructed by closely following the structure of deterministic finite automatons for regular action.", "num_citations": "26\n", "authors": ["1450"]}
{"title": "A conceptual model specification language (CMSL Version 2)\n", "abstract": " Version 2 of a language (CMSL) to specify conceptual models is de ned. Version 1 was de ned in may 1990 22]. CMSL consists of two parts, the value speci cation language VSL and the object speci cation language OSL. In chapter 1, an informal explanation of the di erence between objects and values is given, after which VSL and OSL are introduced by means of small examples in chapters 2 and 3. There is a formal semantics of CMSL and an inference system for CMSL 29, 23, 24, 22, 26], but research on this still continues. A method for developing CMSL models is also being developed 25, 27]. A start has been made with a workbench for CMSL 11, 17, 18, 19, 20]. This report is a working document to keep track of the current results and research problems of CMSL and is not intended for publication.", "num_citations": "26\n", "authors": ["1450"]}
{"title": "Accident analysis methods and models\u2014a systematic literature review\n", "abstract": " As part of our co-operation with the Telecommunication Agency of the Netherlands, we want to formulate an accident analysis method and model for use in incidents in telecommunications that cause service unavailability. In order to not re-invent the wheel, we wanted to first get an overview of all existing accident analysis methods and models to see if we could find an overarching method and commonalities between models. Furthermore, we wanted to find any methods that had been applied to incidents in telecommunication networks or even been designed specifically for these incidents. In this article, we present a systematic literature review of incident and accident analysis methods across domains. We find that accident analysis methods have experienced a rise in attention over the last 15 years, leading to a plethora of methods. We discuss the three classes in which they are often categorized. We find that each class has its own advantages and disadvantages: an analysis using a sequential method may be easier to understand and communicate and quicker to execute, but may miss vital underlying causes that can later trigger new, similar accidents. An analysis using an epidemiological method takes more time, but it also finds underlying causes the resolution of which may prevent accidents from happening in the future. Systemic methods are appropriate for complex, tightly coupled systems and executing such a method takes a lot of time and resources, rendering it very expensive. This will often not be justified by the costs of the accident (especially in telecommunications networks) and it will therefore be too expensive to be employed in\u00a0\u2026", "num_citations": "25\n", "authors": ["1450"]}
{"title": "RiskREP: risk-based security requirements elicitation and prioritization\n", "abstract": " Companies are under pressure to be in control of their assets but at the same time they must operate as efficiently as possible. This means that they aim to implement \u201cgood-enough security\u201d but need to be able to justify their security investment plans. In this paper, we present a Risk-Based Requirements Prioritization method (RiskREP) that extends misuse case-based methods with IT architecturebased risk assessment and countermeasure definition and prioritization. Countermeasure prioritization is linked to business goals to achieve and based on cost of countermeasures and their effectiveness in reducing risks. RiskREP offers the potential to elicit complete security countermeasures, but also supports the deliberate decision and documentation of why the security analysis is focused on certain aspects. We illustrate RiskREP by an application to an action case.", "num_citations": "25\n", "authors": ["1450"]}
{"title": "A dynamic logic for reasoning about sub-ideal states\n", "abstract": " In this paper, we will show how dynamic logic can be used to reason about so-called sub-ideal states, ie states in which some deontic constraint is violated. For this purpose we have to extend the de nition of the deontic operators. Instead of only giving a postcondition with respect to violations for each action we will consider the complete state transition. In order to distinguish between di erent states of violation we will also index the violation predicates. The introduction of these two extensions makes it possible to reason in a exible and natural way about sub-ideal states.", "num_citations": "25\n", "authors": ["1450"]}
{"title": "Role-based access control in retrospect\n", "abstract": " A review of the state of the art of role-based access control can help practitioners assess RBAC's applicability to their organization and indicates where more research is needed to improve the RBAC model.", "num_citations": "24\n", "authors": ["1450"]}
{"title": "A specification language for static, dynamic and deontic integrity constraints\n", "abstract": " In the proof-theoretic view of knowledge bases (KB's), a KB is a set of facts (atomic sentences) and integrity constraints (IC's). An IC is then a sentence which must at least be consistent with the other sentences in the KB. This view obliterates the distinction between, for example, the constraint that age is a non-negative integer (which is true of the universe of discourse (UoD) but may be false in a particular implementation of a KB), and the constraint that a class must have precisely one teacher (which is false of the UoD if a class actually has two teachers). The second constraint is called deontic and constrains the UoD; the first constraint is a necessary truth of the UoD and does not constraint the UoD. Instead, it constrains the implementation of the KB. We show that both types of constraints can be specified in the single framework provided by a deontic variant of dynamic logic, which has the added advantage\u00a0\u2026", "num_citations": "24\n", "authors": ["1450"]}
{"title": "Risk assessment as an argumentation game\n", "abstract": " This paper explores the idea that IT security risk assessment can be formalized as an argumentation game in which assessors argue about how the system can be attacked by a threat agent and defended by the assessors. A system architecture plus assumptions about the environment is specified as an ASPIC                      \u2009+\u2009 argumentation theory, and an argument game is defined for exchanging arguments between assessors and hypothetical threat agents about whether the specification satisfies a given security requirement. Satisfaction is always partial and involves a risk assessment of the assessors. The game is dynamic in that the players can both add elements to and delete elements from the architecture specification. The game is shown to respect the underlying argumentation logic in that for any logically completed game \u2018won\u2019 by the defender, the security requirement is a justified conclusion from\u00a0\u2026", "num_citations": "22\n", "authors": ["1450"]}
{"title": "LCM 3.0-A Language for Describing Conceptual Models Syntax Definition\n", "abstract": " Data Types (ADT's) in the framework of order sorted algebraic specifications, in a way similar to the specification languages OBJ [6] and ASF [2]. The syntactical construct for defining ADTs in LCM is the hValueBlocki, in which you bind a name to a set of values, and define the appearance of functions and constants related to that set. The meaning of the expressions using values from that set is expressed by giving equations. As a simple example, a value block for specifying a sort representing the colors of a traffic light, together with a function nextcolor might look like this: begin value type TRAFFICLIGHTCOLOR functions--Constants are viewed as functions without parameters. red: TRAFFICLIGHTCOLOR; orange: TRAFFICLIGHTCOLOR; green: TRAFFICLIGHTCOLOR;--A function to return the next color to be displayed,--given the current color of a traffic light. nextcolor (TRAFFICLIGHTCOLOR): TRAFFICLIGHTCOLOR; predicates continue (TRAFFICLIGHTCOLOR); axioms nextcolor (red)=...", "num_citations": "22\n", "authors": ["1450"]}
{"title": "Harmfulness of code duplication-a structured review of the evidence\n", "abstract": " Duplication of code has long been thought to decrease changeabili-ty of systems, but recently doubts have been expressed whether this is true in general. This is a problem for researchers because it makes the value of research aimed against clones uncertain, and for practitioners as they cannot be sure whether their effort in reducing duplication is well-spent. In this paper we try to shed light on this is-sue by collecting empirical evidence in favor and against the nega-tive effects of duplication on changeability. We go beyond the flat yes/no-question of harmfulness and present an explanatory model to show the mechanisms through which duplication is suspected to affect quality. We aggregate the evidence for each of the causal links in the model. This sheds light on the current state of duplication re-search and helps practitioners choose between the available mitiga-tion strategies.", "num_citations": "21\n", "authors": ["1450"]}
{"title": "Surveying the factors that influence maintainability: research design\n", "abstract": " We want to explore and analyse design decisions that influence maintainability of software. Software maintainability is important because the effort expended on changes and fixes in software is a major cost driver. We take an empirical, qualitative approach, by investigating cases where a change has cost more or less than comparable changes, and analysing the causes for those differences. We will use this analysis of causes as input to following research in which the individual contributions of a selection of those causes will be quantitatively analysed.", "num_citations": "21\n", "authors": ["1450"]}
{"title": "Ontological distinctions between means-end and contribution links in the i* framework\n", "abstract": " The i* framework is a renowned Requirements Engineering approach. This work is part of an ongoing effort to provide ontological interpretations for the i* core concepts. With this, we aim at proposing a more uniform use of the language, in a way that it can be more easily learned by newcomers and more efficiently transferred to industry. Our approach is based on the application of a foundational ontology named UFO, which is used as a semantically coherent reference model to which the language should be isomorphic. In this paper, we focus on the Means-end and the Contribution links. We aim at presenting the community with some possible ontological interpretations of these links, aiming at promoting constructive debate and receiving feedback about the validity of our assumptions.", "num_citations": "19\n", "authors": ["1450"]}
{"title": "Checking the alignment of value-based business models and it functionality\n", "abstract": " Business--IT alignment is an ongoing activity of high importance for the success of a business. This is a hard task, especially in the context of value webs in which multiple businesses collaborate with each other to reach a common goal. Value models, as the outcome of the value web exploration phase, represent solutions for business people, but not for software engineers. The latter ones have to come up with a blueprint for the implementation at application level. This can have two faces: Either there are no systems at all and everything needs to be designed from scratch, or it is desired to use as many existing systems as possible. In the latter case, on which we focus in this paper, it must be checked which functionality existing systems should provide and whether they are usable for the given value web context. This affords several design activities, as well as checking consistency between different models. Our\u00a0\u2026", "num_citations": "19\n", "authors": ["1450"]}
{"title": "A Formal Analysis of the Shlaer-Mellor method: towards a toolkit for formal and informal requirements specification techniques\n", "abstract": " In this paper, we define a number of tools that we think belong to the core of any toolkit for requirements engineers. The tools are conceptual and hence, they need precise definitions that lay down as exactly as possible what their meaning and possible use is. We argue that this definition can best be achieved by a formal specification of the tool. This means that for each semi-formal requirements engineering tool we should provide a formal specification that precisely specifies its meaning. We argue that this mutually enhances the formal and semi-formal technique: it makes formal techniques more usable and, as we will argue, at the same time simplifies the diagram-based notations.               At the same time, we believe that the tools of the requirements engineer should, where possible, resemble the familiar semi-formal specification techniques used in practice today. In order to achieve this, we should\u00a0\u2026", "num_citations": "19\n", "authors": ["1450"]}
{"title": "A method for building and evaluating formal specifications of object-oriented conceptual models of database systems\n", "abstract": " This report describes a method called MCM (Method for Conceptual Modeling) for building and evaluating formal specifications of object-oriented models of database system behavior. An important aim of MCM is to bridge the gap between formal specification and informal understanding. Building a MCM model is a process that moves from the informal to the formal, evaluating the model is a process that moves back from the formal to the informal.", "num_citations": "19\n", "authors": ["1450"]}
{"title": "A unified checklist for observational and experimental research in software engineering (version 1)\n", "abstract": " Current checklists for empirical software engineering cover either experimental research or case study research but ignore the many commonalities that exist across all kinds of empirical research. Identifying these commonalities, and explaining why they exist, would enhance our understanding of empirical research in general and of the differences between experimental and case study research in particular. In this report we design a unified checklist for empirical research, and identify commonalities and differences between experimental and case study research. We design the unified checklist as a specialization of the general engineering cycle, which itself is a special case of the rational choice cycle. We then compare the resulting empirical research cycle with two checklists for experimental research, and with one checklist for case study research. The resulting checklist identifies important questions to be answered in experimental and case study research design and reports. The checklist provides insights in two different types of empirical research design and their relationships. Its limitations are that it ignores other research methods such as meta-research or surveys. It has been tested so far only in our own research designs and in teaching empirical methods. Future work includes expanding the comparison with other methods and application in more cases, by others than ourselves.", "num_citations": "18\n", "authors": ["1450"]}
{"title": "Towards a unified checklist for empirical research in software engineering: first proposal\n", "abstract": " Background: Current checklists for empirical software engineering cover either experimental research or case study research but ignore the many commonalities that exist across all kinds of empirical research. Identifying these commonalities, and explaining why they exist, would enhance our understanding of empirical research in general and of the differences between experimental and case study research in particular. Aim: In this short paper we design a unified checklist for empirical research, that identify commonalities and differences between experimental and case study research. Method: We design the unified checklist as a specialization of the general engineering cycle, which itself is a special case of the rational choice cycle. The unified checklist is based on an analysis and integration of a number of existing checklists. Results: The current version of the checklist exhibits a shared structure of\u00a0\u2026", "num_citations": "18\n", "authors": ["1450"]}
{"title": "Writing a report about design research\n", "abstract": " This short note provides guidelines on how to report about design research. It is intended as a guide for students who need to write a paper about a design research assignment, eg in the ARE course. It can also be used to report about Master's projects or even about Ph. D. projects.The most difficult part of writing the report\u2014and indeed of doing the research\u2014is figuring out what problems you need to solve. Is it a practical problem or a knowledge problem? How many problems are there? How is your answer evaluated? Section 2 treats the problem of the problem, and it is the largest section. Once you understand the problem to be solved, the rest is relatively easy. Sections 3 and 4 describe how you report about your solution to a practical problem and to a knowledge problem, respectively. The subjects treated here are treated more elaborately in the course Problem Analysis and Solution Requirements (PASR).", "num_citations": "18\n", "authors": ["1450"]}
{"title": "A Survey of Requirements Engineering Methods for Pervasive Services\n", "abstract": " Designing and deploying ubiquitous computing systems, such as those delivering large-scale mobile services, still requires large-scale investments in both development effort as well as infrastructure costs. Therefore, in order to develop the right system, the design process merits a thorough investigation of the wishes of the foreseen user base. Such investigations are studied in the area of requirements engineering (RE). In this report, we describe and compare three requirements engineering methods that belong to one specific form of RE, namely Goal-Oriented Requirements Engineering. By mapping these methods to a common framework, we assess their applicability in the field of ubiquitous computing systems.", "num_citations": "18\n", "authors": ["1450"]}
{"title": "The declarative problem frame: Designing systems that create and use norms\n", "abstract": " This paper analyzes design guidelines for reactive systems that control a social environment. In contrast to a physical environment, a social environment cannot be controlled by physical calculation, but it can be controlled by symbolic interaction in combination with the appropriate norms. The paper shows that this involves three kinds of norms: the desired effects of the system, norms needed to achieve these effects, and norms for the subject domain that are independent from the system. The norms presuppose by the responses of the system include the ability of the system to act on behalf of legal persons as well as responsibilities allocated to legal persons. The analysis leads to design guidelines and a problem frame for social control by machine.", "num_citations": "18\n", "authors": ["1450"]}
{"title": "Traceability and Modularity in Software Design\n", "abstract": " A software design specification consists of a number of documents that describe various aspect of the design at different levels of detail, that are lined in many ways. This paper shows how different designs may use different modularization criteria, and how documents describing these designs may be linked in a coherent way, even if the designs use techniques borrowed from structured as well as object-oriented analysis and design. Illustrations are taken from the meeting scheduler case study.", "num_citations": "18\n", "authors": ["1450"]}
{"title": "Synchromodal transport: pre-requisites, activities and effects\n", "abstract": " Synchromodal Transport is one of the most innovative concepts in logistics. But, there is considerable ambiguity in existing literature over a common reference point for SmT. In this paper we consolidate existing research domain of SmT, by proposing a common reference point for pre-requisites, activities and effects of SmT. Our research will benefit future researchers and logistic companies willing to move towards Synchromodal Transport.", "num_citations": "17\n", "authors": ["1450"]}
{"title": "Contextual Permission: A Solution to the Free Choice Paradox\n", "abstract": " In this paper, we give a solution to the Free Choice Paradox. This is done in two stages. First, we have a close look at the logical interpretation of the natural language statements that lead to the paradox. This leads to making the important distinction of permitting an action in isolation or permitting it in combination with some or any other action, ie in a certain context. This distinction is made formal by the introduction of a new operator on actions, which forces them to be performed in isolation. With this distinction made clear it is possible to give a\" new\", stronger definition for the permission operator, which solves the Free Choice Paradox and which does not lead to any new inconsistencies or paradoxes.", "num_citations": "17\n", "authors": ["1450"]}
{"title": "Value-oriented coordination process modeling\n", "abstract": " Business webs are collections of enterprises designed to jointly satisfy a consumer need. Designing business webs calls for modeling the collaboration of enterprises from different perspectives, in particular the business value and coordination process perspectives, and for mutually aligning these perspectives. However, business value modeling and coordination process modeling have different goals and use different concepts. Nevertheless, the resulting models should be consistent with each other because they refer to the same system. In this paper we define consistency between value models and coordination models in multi-perspective e-business web design and give guidelines to produce consistent coordination process models from business value models in a simple and stepwise manner. We provide an initial validation of these guidelines with a real-world example of business web design.", "num_citations": "16\n", "authors": ["1450"]}
{"title": "Smart Logistics: An Enterprise Architecture Perspective.\n", "abstract": " Logistic enterprises are increasingly becoming smarter and more efficient by using real-time contextual data. A currently unsolved problem for small to medium sized logistic service providers (SMLSPs) is, how to use real time data in existing business processes & IT systems. Enterprise architecture can be used as a tool to solve this problem and aid in adapting existing processes & IT. This would lead to improved operational planning and disruption handling; thereby bringing them closer to becoming smart, context aware logistic enterprises.", "num_citations": "15\n", "authors": ["1450"]}
{"title": "Argumentation-based security requirements elicitation: The next round\n", "abstract": " Information Security Risk Assessment can be viewed as part of requirements engineering because it is used to translate security goals into security requirements, where security requirements are the desired system properties that mitigate threats to security goals. To improve the defensibility of these mitigations, several researchers have attempted to base risk assessment on argumentation structures. However, none of these approaches have so far been scalable or usable in real-world risk assessments. In this paper, we present the results from our search for a scalable argumentation-based information security RA method. We start from previous work on both formal argumentation frameworks and informal argument structuring and try to find a promising middle ground. An initial prototype using spreadsheets is validated and iteratively improved via several Case Studies. Challenges such as scalability, quantify\u00a0\u2026", "num_citations": "15\n", "authors": ["1450"]}
{"title": "Understandability of goal-oriented requirements engineering concepts for enterprise architects\n", "abstract": " ArchiMate is a graphical language for modelling business goals and enterprise architecture. In previous work we identified possible understandability issues with the goal-oriented notations in ArchiMate. [Problem] We investigated how understandable the goal-oriented concepts really were in two quasi-experiments with practitioners. [Principal ideas/results] Only three concepts were understood by most or all subjects; the stakeholder concept, the goal concept and the requirement concept. The other concepts were misunderstood by most of our subjects. We offer explanations for these (mis)understandings. [Contribution] This paper provides new insights into the understandability and hence usability of goal-oriented concepts by practicing enterprise architects.", "num_citations": "15\n", "authors": ["1450"]}
{"title": "Monitoring collaboration from a value perspective\n", "abstract": " Collaborations among businesses can be described from different viewpoints. Two of these viewpoints are the value viewpoint, representing estimated values exchanged in a collaboration, and the coordination viewpoint, representing messages exchanged between the actors to coordinate the execution of a collaboration. To observe and maintain the value viewpoint during the complete life cycle, the estimated values have to be validated during the execution of the collaboration. However, since the value model is not implemented, the necessary information for monitoring the value viewpoint needs to be derived from the coordination viewpoint. Relating coordination and value viewpoint is a difficult process because the coordination viewpoint lacks information present in the value viewpoint. In this paper we define the relation between both viewpoints for the complete collaboration life cycle. Furthermore, we\u00a0\u2026", "num_citations": "15\n", "authors": ["1450"]}
{"title": "Requirements engineering: Solutions and trends\n", "abstract": " This last chapter of the book describes solutions and trends in the discipline of RE. Starting from a wrap-up of what was presented throughout this book, it suggests a framework of requirements engineering and indicates what current solutions are available in this framework. Beyond providing a short overview of the state of the practice, this chapter also summarizes current trends in RE. Four trends are evaluated, namely the growing usage of commercial off-the-shelf components and systems and how RE activities need to be adjusted; the evolving focus on product lifecycle management and the need to collaborate amongst very heterogeneous communities; the wish to learn and to share experiences on effective ways to implement RE in an organization and the growing interest in requirements engineers\u2019 skill sets. We finally provide an outlook into where requirements engineering is heading for.", "num_citations": "15\n", "authors": ["1450"]}
{"title": "Algebraic Specification of Object Dynamics in Knowledge Base Domains\n", "abstract": " Algebraic Specification of Object Dynamics in Knowledge Base Domains (1990) | www.narcis.nl KNAW KNAW Narcis Back to search results University of Twente Publication Algebraic Specification of Object Dynamics in Knowledge Base Domains (1990) Pagina-navigatie: Main Save publication Save as MODS Export to Mendeley Save as EndNote Export to RefWorks Title Algebraic Specification of Object Dynamics in Knowledge Base Domains Published in Artificial Intelligence in Databases and Information Systems (DS-3), 411 - 436 Author Wieringa, Roelf J.; van de Riet, RP Editor Meersman, RA; Shi, Z.; Kung, C. Date issued 1990-7 Access Restricted Access Reference(s) SCS-Services, EWI-10678 Language und Type Conference Paper Publisher North Holland Publication https://research.utwente.nl/en/publications/algebraic-specif... OpenURL Search this publication in (your) library ISBN 0444886451 Persistent \u2026", "num_citations": "15\n", "authors": ["1450"]}
{"title": "Persuasive technologies: a systematic literature review and application to pisa\n", "abstract": " Persuasive Technologies is an expansive field that covers various research areas including engineering and social sciences. This document summarizes current and historical models of information processing, persuasion and persuasive systems design in order to place other studies in the field within context. The Persuasive Systems Design Model is then selected as the most recent and comprehensive model in the field, afer which a series of sample context analyses are performed using this model. The case used for these context analyses is the PISA tool. Finally, we consider the limitations and possible future work of this literature review.", "num_citations": "14\n", "authors": ["1450"]}
{"title": "Real-world semantics of conceptual models\n", "abstract": " Conceptual modelling is the addition of more real-world semantics to the computations performed by a computer. It is argued that in a proper engineering approach to computing, three kinds of conceptual modelling need to be distinguished, (1) modelling a software solution, (2) modelling the domain in which it operates, and (3) modelling the impact of the software solution on the domain. Nearly 40 years of research in conceptual modelling has yielded a wealth of concepts and notations for conceptual modelling of the second variety, which we call domain-oriented. A summary and framework of these results are presented. Conceptual modelling of the third variety, which we call impact-oriented, has been developed less. A framework for this is provided too, and promising directions for impact-oriented conceptual modelling are identified.", "num_citations": "14\n", "authors": ["1450"]}
{"title": "Evaluating the structure of research papers: a case study\n", "abstract": " This paper is triggered by a concern for the methodological soundness of research papers in RE. We propose a number of criteria for methodological soundness, and apply these to a random sample of 37 submissions to the RE\u201903 conference. From this application, we draw a number of conclusions that we claim are valid for a larger sample than just these 37 submissions. Our major observation is that most submissions in our sample are solution-oriented: they present a solution and illustrate it with a problem, rather than search for a solution to a given problem class; and most papers do not analyze why and when a solution works or does not work. We end with discussion of the need to improve the methodological soundness of research papers in RE.", "num_citations": "14\n", "authors": ["1450"]}
{"title": "Modelling mobility aspects of security policies\n", "abstract": " Security policies are rules that constrain the behaviour of a system. Different, largely unrelated sets of rules typically govern the physical and logical worlds. However, increased hardware and software mobility forces us to consider those rules in an integrated fashion. We present SPIN models of four case studies where mobility plays a role. At present our models are ad-hoc. In each case the model captures both the system of interest and its security policy. The model is then formally checked against a security principle. The model checking activity shows examples of policies that are too weak to cope with mobility.", "num_citations": "14\n", "authors": ["1450"]}
{"title": "Tangible modelling to elicit domain knowledge: an experiment and focus group\n", "abstract": " Conceptual models represent social and technical aspects of the world relevant to a variety of technical and non-technical stakeholders. To build these models, knowledge might have to be collected from domain experts who are rarely modelling experts and don\u2019t usually have the time or desire to learn a modelling language. We investigate an approach to overcome this challenge by using physical tokens to represent the conceptual model. We call the resulting models tangible models. We illustrate this idea by creating a tangible representation of a socio-technical modelling language and provide initial evidence of the relative usability and utility of tangible versus abstract modelling. We discuss psychological and social theories that could explain these observations and discuss generalizability and scalability of the approach.", "num_citations": "13\n", "authors": ["1450"]}
{"title": "E3value to BPMN Model Transformation\n", "abstract": " Business value and coordination process perspectives need to be taken into consideration while modeling business collaborations. The need for these two models stems from the importance of separating the how from the what concerns. A business value model shows what is offered by whom to whom while a coordination process model shows how these offerings are fulfilled operationally. This case study addresses the model transformation between e3value and BPMN, commonly used for modeling business collaborations from value and coordination perspectives respectively.", "num_citations": "13\n", "authors": ["1450"]}
{"title": "MaDe4IC: an abstract method for managing model dependencies in inter-organizational cooperations\n", "abstract": " Inter-organizational cooperations are complex in terms of coordination, agreements, and value creation for involved partners. When managing complex cooperations, it is vital to maintain models describing them. Changing one model to regain consistency with the running system might result in new inconsistencies. As a consequence, this maintenance phase grows in complexity with increasing number of models. In this context, challenges are to ensure consistency at design time and to monitor the system at runtime, i.e., at design time, consistency between different models describing the cooperation needs to be ensured. At runtime, behavior of the software system needs to be compared with its underlying models. In this paper, we propose a structured and model-independent method that supports ensuring and maintaining consistency between running system and underlying models for inter\u00a0\u2026", "num_citations": "13\n", "authors": ["1450"]}
{"title": "Web services as product experience augmenters and the implications for requirements engineering: A position paper\n", "abstract": " There is currently little insight into what requirement engineering for web services is and in which context it will be carried out. In this position paper, we investigate requirements engineering for a special kind of web services, namely web services that are used to augment the perceived value of a primary service or product that is itself not a web service. We relate requirements engineering to a common enterprise architecture pattern and derive from this a number of research questions for further study.", "num_citations": "12\n", "authors": ["1450"]}
{"title": "Some finite-graph models for process algebra\n", "abstract": " In this paper, we present a number of closely related models of process algebra [2, 3, 4], called finite-graph models. In a finite-graph model of process algebra, each process is a bisimulafion class of a particular kind of process graphs, called recursive process graphs. Just as in the standard graph model [I], each guarded recursive specification has exactly one solution in a finite-graph model, but in contrast to the standard graph model, this solution can be shown to contain a finite recursive process graph as element.The finite-graph models were defined in order to be able to build an editor that can manipulate process graphs. It is well-known that there are finite guarded specifications that have no finite solution in the standard graph model; the specification of a stack is an example [1, page 63]. Figure 1 shows an approximation of a graph of a (terminating) stack process and figure 2 shows a recursive process graph\u00a0\u2026", "num_citations": "12\n", "authors": ["1450"]}
{"title": "Improving the semantic interoperability of IoT Early Warning Systems: the Port of Valencia use case\n", "abstract": " An early warning system (EWS) is a distributed system that monitors the physical world and issues warnings if it detects abnormal situations. The Internet of Things (IoT) offers opportunities to improve monitoring capabilities of EWS and to realize (near) real-time warning and response. This paper presents the development of an interoperable IoT-based EWS to detect accident risks with trucks that deliver goods at the Valencia port area. Our solution addresses the semantic integration of a variety of data sources with processing in safety-critical applications for effective emergency response. The solution considers existing domain-specific ontologies and standards, along with their serialization formats. Accident risks are assessed by monitoring the drivers\u2019 vital signs with ECG medical wearables and the trucks\u2019 position with speed and accelerometer data. Use cases include the detection of health issues and\u00a0\u2026", "num_citations": "11\n", "authors": ["1450"]}
{"title": "Empirical research in business process management: introduction to the special issue\n", "abstract": " In this editorial letter, we provide the readers of Information Systems and e-Business Management with an introduction to Business Process Management and the challenges of empirical research in this field. We then briefly describe selected examples of current research efforts in this field and how the papers accepted for this special issue contribute to extending our body of knowledge.", "num_citations": "11\n", "authors": ["1450"]}
{"title": "Classifying assumptions made during requirements verification of embedded systems\n", "abstract": " We are investigating ways to improve the process of modelling of embedded systems for formal verification. In the modelling process, we make a mathematical model of the system software and its environment (the plant), and we prove that the requirement holds for the model. But we also want to have an argument that increases our confidence that the model represents the system correctly (with respect to the requirement). Therefore, we document some of the modelling decisions in form of a list of the system assumptions made while modelling. Identifying the assumptions and deciding which ones are relevant is a difficult task and it cannot be formalized. To support this process, we give a classification of assumptions. We show our approach on an example.", "num_citations": "11\n", "authors": ["1450"]}
{"title": "Operational business-IT alignment in value webs\n", "abstract": " Value webs are constellations of profit-and-loss responsible actors that have independent decision-making authority and that have decided to cooperate for a specific purpose. To the extent that the actors are independent, they each decide independently whether to participate in the network, and because they are profit-and-loss responsible, this decision will be based on economic sustainability of the participation. This sustainability depends on the balance between the costs and benefits of participating. The costs are generated by the coordination process and IT infrastructure required to participate; the benefits materialize in the form of commercial transactions enabled by the participation. In this paper I summarize results of recent research into conceptual modeling techniques to design economically sustainable IT-enabled value webs.1", "num_citations": "11\n", "authors": ["1450"]}
{"title": "Competences of IT Architects\n", "abstract": " In this book we report on research conducted in the past few years under the auspices of the NAF1 into competences of IT architects. It is not possible to give a name to the profession of IT architect without raising protests from more than a few of the professionals who, in this book, we refer to as \u201cIT architects\u201d. Some insist on the label \u201cbusiness-IT architect\u201d, but others think \u201centerprise architect\u201d is a much better term. Some even use the term \u201cdigital architect\u201d to stress the fact that these architects supposedly shape the \u201cdigital world\u201d. Many are more liberal in their acceptance of labels for the profession, but \u201cIT architect\u201d just happens to be the one term that they definitively disagree with. Even more, some people in the community would argue that there is nothing wrong in using \u201coldfashioned\u201d term \u201cinformation architect\u201d. Quarrels about names tend to be as emotional as they are pointless. In this book we aim at making the relevant distinctions visible, but we do not propose a definitive set of labels from the different disciplines within the architect profession. We will use a consistent set of terms, but do not pretend to tell others to use the same set of terms. We, more or less arbitrarily, use \u201cIT architect\u201d as the most general label for the profession, within which all the others fall. Having said this, the intended audience of the book consist of IT architects who reflect on their profession, and of those who hire and/or manage IT architects. For these people it is important to have a clear picture of the competences required by IT architects. Different companies all use their differently defined IT architect job roles and architecture disciplines, and there is no uniformity of\u00a0\u2026", "num_citations": "11\n", "authors": ["1450"]}
{"title": "Mission 2 solution: requirements engineering education as central theme in the BIT programme\n", "abstract": " Design of integrated business-IT solutions is the main theme in the Business Information Technology programme (BIT) at the University of Twente. Our mission is to teach students to design solutions that are needed instead of solutions that are asked for. This makes requirements engineering an essential part of our education in business-IT alignment. Integration of requirements engineering (RE) in several courses is combined with challenging the students by authentic cases, taken from business practice, in which they have to apply theory and train their competences. This combination results in reflection as well as in RE experience and insight in the importance of requirements analysis.", "num_citations": "11\n", "authors": ["1450"]}
{"title": "Extending CTL with Actions and Real-Time\n", "abstract": " In this paper, we present the logic ATCTL, which is intended to be used for model checking models that have been specified in a lightweight version of the Unified Modelling Language (UML). Elsewhere, we have defined a formal semantics for LUML to describe the models. This paper's goal is to give a specification language for properties that fits LUML; LUML includes states, actions and real time. ATCTL extends CTL with concurrent actions and real time. It is based on earlier extensions of CTL by De Nicola and Vaandrager (ACTL) and Alur et al. (TCTL). This makes it easier to adapt existing model checkers to ATCTL. To show that we can check properties specified in ATCTL in models specified in LUML, we give a small example using the Kronos model checker.", "num_citations": "11\n", "authors": ["1450"]}
{"title": "The university library document circulation system specified in LCM\n", "abstract": " The speci cation language LCM (Conceptual Modeling Language) is used to specify a part of the university library document circulation system of the Free University 7]. LCM is version 3 of a language previously called CMSL (Conceptual Model Speci cation Language). The method used to specify the document circulation system is MCM (Conceptual Modeling Method). We draw a number of conclusions about the types of axioms that are encountered in this case study, as well as about the kinds of extensions that should be added to LCM to facilitate easier and more expressive modeling of this case study.", "num_citations": "11\n", "authors": ["1450"]}
{"title": "Reference architecture for integration platforms\n", "abstract": " In addition to in-house applications, networked enterprises are increasingly using data and services from various external sources. Conversion of data to useful information and IT alignment with business goals are big challenges faced by these enterprises. Integration platforms (IPs) aid enterprises in solving such challenges. However, the large number of commercial and academic IPs currently available have created a new problem for enterprises, namely whether to build their own IP or buy/rent a existing IP. Also, how to choose from the plethora of different design/solution options that are available? This paper presents a study and analysis of 31 IPs to bring out best practices in IP design. Following a commonality analysis of IPs from different research domains, an IP reference architecture is proposed. The reference architecture will aid enterprises in making better IP design/solution choices. It can also contribute\u00a0\u2026", "num_citations": "10\n", "authors": ["1450"]}
{"title": "Using a foundational ontology to investigate the semantics behind the concepts of the i* language\n", "abstract": " In the past few years, the community that develops i* has become aware of the problem of having so many variants, since it makes it difficult for newcomers to learn how to use the language and even to experts to efficiently exchange knowledge and disseminate their proposals. Moreover, this problem also delays the transfer of the i* framework to industrial settings. Our work is one of the current attempts to promote interoperability among the existing variants, and it does that by investigating the semantics behind the i* core concepts. For that, we apply a foundational ontology named UFO, which is used as a semantically coherent reference model to which the language should be isomorphic. In this paper, we report on the steps we have pursued, what we have accomplished so far, also setting the context for the work ahead.", "num_citations": "10\n", "authors": ["1450"]}
{"title": "Designing technical action research and generalizing from real-world cases\n", "abstract": " This tutorial presents a sound methodology for technical action research, which consist of testing a new artifact by using it to solve a real problem. Such a test would be useless if we could not generalize from it, and the tutorial introduces architectural inference as a way of supporting generalizations by technical action research.", "num_citations": "10\n", "authors": ["1450"]}
{"title": "Integrated assessment and mitigation of physical and digital security threats: Case studies on virtualization\n", "abstract": " Virtualization is one of the enabling technologies of cloud computing. It turns once dedicated physical computing resources such as servers into digital resources that can be provisioned on demand. Cloud computing thus tends to replace physical with digital security controls, and cloud security must be understood in this context. In spite of extensive research on new hardware-enabled solutions such as trusted platforms, not enough is known about the actual physical-digital security trade-off in practice. In this paper, we review what is currently known about security aspects of the physical-digital trade-off, and then report on three case studies of private clouds that use virtualization technology, with the purpose of identifying generalizable guidelines for security trade-off analysis. We identify the important security properties of physical and digital resources, analyze how these have been traded off against each other in\u00a0\u2026", "num_citations": "10\n", "authors": ["1450"]}
{"title": "Competencies of the ICT architect\n", "abstract": " This report presents the results of our research about competencies of different types of ICT architects. The goal of this report is not to describe the competencies that successful architects have. Rather, our goal is to describe what competencies NAF members and a few other key organizations think architects should have. In particular, we tried to extract an underlying shared view on architect's roles and competencies and show how the role structures and competency profiles of different organizations map to this underlying model. A consequence of this way of working is that if we find such an underlying model, then either we have found a set of competency profiles that successful architects have, or all these organizations are wrong in thinking that this is the profile of successful architects.", "num_citations": "10\n", "authors": ["1450"]}
{"title": "Software requirements engineering: the need for systems engineering and literacy\n", "abstract": " This viewpoint continues the favourite comparison between house engineering and software engineering, recently brought forward again by Dan Berry [1]. In the past eight years, I had two houses renovated and a third one built from scratch, which, after delivery, I had extended immediately by employing a different builder. Each of these four (rebuilding processes involved one main contractor and up to 10 subcontracting builders, who dealt with electricity, plumbing, painting, tiling, plastering, the central heating system, concrete floors, parquet floors, stairs, the kitchen, curtains, sun screens, etc. Some of these processes also involved negotiations with independent suppliers in the Netherlands and Germany, each with different trade laws. Financially this is not a very attractive sequence of moves, not to speak of the stress that this caused in agendas that are already overburdened, but a great opportunity to learn first\u00a0\u2026", "num_citations": "10\n", "authors": ["1450"]}
{"title": "Regular database update logics\n", "abstract": " We study regular first-order update logic (FUL), which is a variant of regular dynamic logic in which updates to function symbols as well as to predicate symbols are possible. We first study FUL without making assumptions about atomic updates. Second, we look at relational algebra update logic (RAUL), which can be viewed as an extension of relational algebra with assignment. RAUL is an instantiation of FUL. Third, we study dynamic database logic (DDL), which is another version of FUL, in which the atomic updates can be \u201cbulk updates\u201d of predicates and updates of updateable functions. In all three cases, we define syntax, declarative semantics, axiomatizations, and operational semantics of the logic. All axiom systems are shown to be sound. Assuming the domain closure and unique naming assumptions, we also give a proof sketch of completeness of the axiomatization of DDL. The operational semantics\u00a0\u2026", "num_citations": "10\n", "authors": ["1450"]}
{"title": "Subsystem design guidelines for extensible general-purpose software\n", "abstract": " We discuss subsystem design for extensible general-purpose information systemswe extract guidelines from a case study of the redesign and extension of an advanced workflow management system and place them into the context of existing software engineering research. Key aspect is the distinction between essential and physical architectures, related to software clustering and distribution.", "num_citations": "10\n", "authors": ["1450"]}
{"title": "Toolkit for Conceptual Modeling (TCM): User's Guide and Reference\n", "abstract": " The Toolkit for Conceptual Modeling (TCM) is a suite of graphical editors for a number of graphical notation systems that are used in software specification methods. The notations can be used to represent the conceptual structure of the software-hence the name of the suite. This manual describes version 1.6 of TCM. TCM runs on Unix systems with X Windows. The TCM ftp site is ftp://ftp. cs. vu. nl/pub/tcm. The TCM home page is http:/fwww. cs. vu. nl;-tcm. The use of TCM is free for education, academic research and other non-commercial purposes. This version of TCM contains graphical editors for several kinds of documents, namely diagrams, tables and trees.", "num_citations": "10\n", "authors": ["1450"]}
{"title": "Measuring Computer Literacy without Questionnaires.\n", "abstract": " Behavior Change Support Systems (BCSS) benefit from understanding the user: a user profile can help select the right way to formulate an argument, selecting the right tone, format and content. Part of such a profile is an adequate representation of the computer literacy of a user. Unfortunately, computer literacy is commonly measured by asking the user to fill in a questionnaire. This an obstacle to the adoption of a BCSS, and as such is not a good way to build a model of a user's computer literacy. In this paper we describe the setup of a series of experiments intended to identify indicators that can be measured automatically and that correlate well with a relevant concept of computer literacy.", "num_citations": "9\n", "authors": ["1450"]}
{"title": "Reusing knowledge in embedded systems modelling\n", "abstract": " Model\u2010based design is a promising technique to improve the quality of software and the efficiency of the software development process. We are investigating how to efficiently model embedded software and its environment to verify the requirements for the system controlled by the software. The software environment consists of mechanical, electrical and other parts; modelling it involves learning how these parts work, deciding what is relevant to model and how to model it. It is not possible to fully automate these steps. There are general guidelines, but given that every modelling problem differs, much is left to the modeller's own preference, background and experience. Still, when the next generation of a system is designed, the new system will have common elements with its previous version. Therefore, lessons learned from the current model could inform future models. We propose a framework for identifying the\u00a0\u2026", "num_citations": "9\n", "authors": ["1450"]}
{"title": "Research and design methodology for software and information engineers\n", "abstract": " This report is aimed at software engineers, information engineers, and knowledge engineers, or software professionals for short. Software professionals design, implement, maintain and investigate software systems with a variety of functions, ranging from the support of administrative processes to the control of production processes, from decision support to cruise control, from on-line auctions to word document management. And they do this in a variety of domains, such as finance, manufacturing, and health care, or indeed the domain of software production and maintenance itself.Furthermore, wherever software is used, the processes and organization of the work of users are changed, so that software professionals may find themselves deeply involved in the design, implementation and investigation of the work of software users. And since one particular application domain for software is the software production\u00a0\u2026", "num_citations": "9\n", "authors": ["1450"]}
{"title": "Architecture alignment\n", "abstract": " As we have described in Chap.                  1                                , achieving alignment between business and IT is one of the most important drivers for architecture. Architecture alignment is the problem of designing architectures at the infrastructure, application, and business levels such that each fits optimally with the other architectures. By studying project documentation obtained in case studies in several large Dutch organisations, we have tried to find alignment patterns that are actually used in practice. These results provide the context in which architectures are designed. Insight into this context helps the reader in better applying the techniques presented in this book.", "num_citations": "8\n", "authors": ["1450"]}
{"title": "A mobile ambients-based approach for network attack modelling and simulation\n", "abstract": " Attack graphs are an important support for assessment and subsequent improvement of network security. They reveal possible paths an attacker can take to break through security perimeters and traverse a network to reach valuable assets deep inside the network. Although scalability is no longer the main issue, Attack Graphs still have some problems that make them less useful in practice. First, Attack Graphs remain difficult to relate to the network topology. Second, Attack Graphs traditionally only consider the exploitation of vulnerable hosts. Third, Attack Graphs do not rely on automatic identification of potential attack targets. We address these gaps in our MsAMS (Multi-step Attack Modelling and Simulation) tool, based on Mobile Ambients. The tool not only allows the modelling of more static aspects of the network, such as the network topology, but also the dynamics of network attacks. In addition to Mobile\u00a0\u2026", "num_citations": "8\n", "authors": ["1450"]}
{"title": "Formalizing the UML in a systems engineering approach\n", "abstract": " This discussion note argues for embedding any formalization of semiformal notations in a methodology. I present a methodological framework for software specification based on systems engineering and show how the UML fits into this framework. Next, an essential modeling approach to formalizing the UML within this framework is argued. Finally, a transition system semantics for the UML is discussed, that fits this semantics approach. No formal details are given, but references are given to places where these can be found.", "num_citations": "8\n", "authors": ["1450"]}
{"title": "Validating specifications of dynamic systems using automated reasoning techniques\n", "abstract": " In this paper, we propose a new approach to validating formal specifications of observable behavior of discrete dynamic systems. By observable behavior we mean system behavior as observed by users or other systems in the environment of the system. Validation of a formal specification of an informal domain tries to answer the question whether the specification actually describes the intended domain. This differs from the verification problem, which deals with the correspondence between formal objects, eg between a formal specification of a system and an implementation of it. We consider formal specifications of object-oriented dynamic systems that are subject to static and dynamic integrity constraints. To validate that such a specification expresses the intended behavior, we propose to use a tool that can answer reachability queries. In a reachability query we ask whether the system can evolve from one state into another without violating the integrity constraints. If the query is answered positively, the system should exhibit an example path between the states; if the answer is negative, the system should explain why this is so. An example path produced by the tool can be used to produce scenarios for presentations of system behavior, but can also be used as a basis for acceptance testing. In this paper, we discuss the use of planning and theoremproving techniques to answer such queries, and illustrate the use of reachability queries in the context of information system development.", "num_citations": "8\n", "authors": ["1450"]}
{"title": "Actor-Oriented System Specification with Dynamic Logic\n", "abstract": " In this paper, we extend dynamic logic with the concept of an actor in order to be able to specify who takes the initiative of an action, who makes a choice, or who controls a synchronization of actions. We give two examples of application of this idea. First, we show how to generalize an approach taken up by De Nicola and Hennessy, who eliminate \u03c4 from CCS in favor of internal and external choice. We show that this generalization allows a more accurate specification of system behavior than is possible without it. Second, deontic logic has been used by several researchers as a system specification language. In the course of this application, a number of paradoxes of classical deontic logic have been resolved, except the paradox of free choice permission. We show that actors can be used to resolve this paradox as well.", "num_citations": "8\n", "authors": ["1450"]}
{"title": "Understandability of goal concepts by requirements engineering experts\n", "abstract": " ARMOR is a graphical language for modeling business goals and enterprise architectures. In previous work we have identified problems with understandability of goal-oriented concepts for practicing enterprise architects. In this paper we replicate the earlier quasi-experiments with experts in requirements engineering, to see if similar problems arise. We found that fewer mistakes were made in this replication than were made in the previous experiment with practitioners, but that the types of mistakes made in all the concepts were similar to the mistakes made in our previous experiments with enterprise architects. The stakeholder concept was used perfectly by our sample, but the goal decomposition relation was not understood. The subjects provided explanations for understandability problems that are similar to our previous hypothesized explanations. By replicating some of our earlier results, this paper\u00a0\u2026", "num_citations": "7\n", "authors": ["1450"]}
{"title": "Validation of embedded system verification models\n", "abstract": " The result of a model-based requirements verification shows that the model of a system satisfies (or not) formalised system requirements. The verification result is correct only if the model represents the system adequately. No matter what modelling technique we use, what precedes the model construction are non-formal activities. During these activities the modeller has to learn how the system works, what the requirements are, and to decide what is relevant to model and how to do it. Due to a partly non-formal nature of modelling steps, we do not have a formal proof that the model represents the system adequately. The most we can do is to increase the confidence in the model. In this paper we explore non-formal model validation steps while designing a formal model. On the example of a Uppaal performance model we designed in a company that produces printers, we will show what validation steps were\u00a0\u2026", "num_citations": "7\n", "authors": ["1450"]}
{"title": "Trust and business webs\n", "abstract": " A business web is a collection of enterprises designed to jointly satisfy a consumer need. A model that shows the creation, distribution, and consumption of goods or services of economic value in a business web is called value model. The goal of a value model is to help the stakeholders build a shared understanding of the business case and assess the potential profitability of collaboration in the business web. The participating stakeholders in a business web are assumed to act trustfully in the collaboration and therefore trust is left entirely outside the picture. However the assumption that stakeholders act trustfully is often not useful in practice (since there are malicious actors). In this paper we consider business webs from a trust perspective and introduce an approach for measuring the trustworthiness of the stakeholders participating in a business web.", "num_citations": "7\n", "authors": ["1450"]}
{"title": "Risk and business goal based security requirement and countermeasure prioritization\n", "abstract": " Companies are under pressure to be in control of their assets but at the same time they must operate as efficiently as possible. This means that they aim to implement \u201cgood-enough security\u201d but need to be able to justify their security investment plans. Currently companies achieve this by means of checklist-based security assessments, but these methods are a way to achieve consensus without being able to provide justifications of countermeasures in terms of business goals. But such justifications are needed to operate securely and effectively in networked businesses. In this paper, we first compare a Risk-Based Requirements Prioritization method (RiskREP) with some requirements engineering and risk assessment methods based on their requirements elicitation and prioritization properties. RiskREP extends misuse case-based requirements engineering methods with IT architecture-based risk\u00a0\u2026", "num_citations": "7\n", "authors": ["1450"]}
{"title": "Reusable Rationale Blocks: Improving quality and efficiency of design choices\n", "abstract": " In the current practice of designing software for user organizations, as experienced by the authors, designers often produce design knowledge again and again for every decision: they reinvent the wheel. We want to improve the quality, predictability, and efficiency of the software design process by reusing design knowledge. Our proposed solution consists of Reusable Rationale Blocks (RRBs). An RRB is a schema and a notation to write down decision rationale. To manage RRBs, we introduce a generalized design space, that consists of a collection of RRBs. And to use RRBs, f we define a process that can be added to any design process, as well as a set of heuristics to be used in applying this process. We illustrate our solution by a few examples taken from our own experience.", "num_citations": "7\n", "authors": ["1450"]}
{"title": "Experimental validation of a risk assessment method\n", "abstract": " [Context and motivation] It is desirable that requirement engineering methods are reliable, that is, that methods can be repeated with the same results. Risk assessments methods, however, often have low reliability when they identify risk mitigations for a system based on expert judgement. [Question/problem] Our goal is to assess the reliability of an availability risk assessment method for telecominfrastructures, and to identify possibilities for improvement of its reliability. [Principal ideas/results] We propose an experimental validation of reliability, and report on its application. We give a detailed analysis of sources of variation, explain how we controlled them and validated their mitigations, and motivate the statistical procedure used to analyse the outcome. [Contribution] Our results can be used to improve the reliability of risk assessment methods. Our approach to validating reliability can be useful for\u00a0\u2026", "num_citations": "6\n", "authors": ["1450"]}
{"title": "Towards middle-range usable design theories for software engineering\n", "abstract": " In this position paper we argue argue to reduce the ambition for a general theory of software engineering to that of a patchwork of middle-range theories about how artifacts can be used by practitioners. This proposal is placed in the wider context of the role of theory in the engineering sciences. A few suggestions are given for how to proceed along this way.", "num_citations": "6\n", "authors": ["1450"]}
{"title": "A new method to assess telecom service availability risks.\n", "abstract": " Protection of society against natural and man-made disasters is high on the societal and political agenda. Effective crisis management is more important than ever. Nowadays, crisis organisations depend crucially on reliable telecom services, and unexpected failure of telecommunication may have serious consequences. In order not to be caught unprepared, crisis organisations should therefore perform a risk assessment on telecom availability. Unfortunately, assessment of availability risks of modern, multi-operator telecom services is difficult; information sources are unreliable, and the relevant information is uncertain and difficult to obtain. This paper describes some of these difficulties, as well as the requirements of availability risk assessment methods for crisis telecommunication services. The paper outlines a new method that can be applied without requiring full knowledge of the physical layout of the telecom infrastructure. This new method relies on telecom service diagrams as a tool for risk analysis and to facilitate dialogue among the analysts.", "num_citations": "6\n", "authors": ["1450"]}
{"title": "Realizing security requirements with physical properties: A case study on paper voting\n", "abstract": " Well-established security models exist for testing and proving the logical security of IT systems. For example, we can assert the strength of cryptographic protocols and hash functions that prevent attackers from unauthorized changes of data. By contrast, security models for physical security have received far less attention. This situation is problematic, especially because IT systems are converging with physical systems, as is the case when SCADA systems are controlling industrial processes, or digital door locks in apartment buildings are replacing physical keys. In such cases, it is necessary to understand the strengths, weaknesses and combinations of physical and digital security mechanisms. To realize this goal, we must first learn how security requirements are realized by the physical environment alone and this paper presents a method for analyzing this, based on the KAOS requirements engineering\u00a0\u2026", "num_citations": "6\n", "authors": ["1450"]}
{"title": "Towards management of complex service compositions-position paper\n", "abstract": " Many companies offer physical products combined with on-line services. For example, product configuration, ordering, order tracking, and payments can be done on-line. The service part of the total offering (the composition) is typically composed of services offered by providers where performance of both the composition (provided by the company) and the input services (obtained from providers) is governed by service level agreements (SLAs). The goal of our approach is to diagnose the performance of an on-line service composition in terms of the performance of on-line input services, with respect to the performance indicators mentioned in the SLAs, and to do this in real-time. Classical SLA monitoring techniques are batch-oriented and are not usable in the highly dynamic environment of Web service provision, where provider relations may change even during service delivery. Our techniques use real-time\u00a0\u2026", "num_citations": "6\n", "authors": ["1450"]}
{"title": "Research findings on empirical evaluation of requirements specifications approaches\n", "abstract": " Numerous software requirements specification (SRS) approaches have been proposed in software engineering. However, there has been little empirical evaluation of the use of these approaches in specific contexts. This paper describes the results of a mapping study, a key instrument of the evidence-based paradigm, in an effort to understand what aspects of SRS are evaluated, in which context, and by using which research method. On the basis of 46 identified and categorized primary studies, we found that understandability is the most commonly evaluated aspect of SRS, experiments are the most commonly used research method, and the academic environment is where most empirical evaluation takes place.", "num_citations": "6\n", "authors": ["1450"]}
{"title": "An approach for maintaining models of an e-commerce collaboration\n", "abstract": " To keep an overview on complex e-commerce collaborations several models are used to describe them. When models overlap in describing a collaboration, the overlapping information should not contradict. Models are of different nature and maintained by different people. Therefore, keeping model-overlap contradiction-free is challenging. In this paper we propose a novel approach for maintaining models representing an E-Commerce collaboration. Applying this approach supports avoiding contradictions in models during evolution of E-Commerce collaborations.", "num_citations": "6\n", "authors": ["1450"]}
{"title": "A conceptual framework for research in cross-organizational ERP cost estimation\n", "abstract": " A Conceptual Framework for Research in Cross-organizational ERP Cost Estimation \u2014 University of Twente Research Information Skip to main navigation Skip to search Skip to main content University of Twente Research Information Logo Home Profiles Research Units Research Output Datasets Activities Prizes Press / Media Search by expertise, name or affiliation A Conceptual Framework for Research in Cross-organizational ERP Cost Estimation Maia Daneva, Roelf J. Wieringa Research output: Chapter in Book/Report/Conference proceeding \u203a Conference contribution \u203a Academic \u203a peer-review Overview Original language Undefined Title of host publication Proceedings of Workshop on Requirements Engineering and Project Management in Software Projects (PROMan) Pages - Publication status Published - 30 Aug 2005 Keywords METIS-226677 Cite this APA Author BIBTEX Harvard Standard RIS Vancouver \u2026", "num_citations": "6\n", "authors": ["1450"]}
{"title": "Case Base for Requirements Engineering: Problem Categories and Solution Techniques\n", "abstract": " We introduce a notion of business problem frames, categorizing the type of IT requirements problems found in organizations, as opposed to Jackson\u2019s problem frames which describe a problem in terms of the solution to that problem. A survey of students\u2019 projects showed that this a viable notion. We intend to build a case base of business problem frames, and their solutions, as a basis for further research and guidance to practitioners.", "num_citations": "6\n", "authors": ["1450"]}
{"title": "The Yourdon Systems Method and the toolkit for conceptual modeling\n", "abstract": " . A description of the structural aspects of the models by the Yourdon \u00cbystems Method (Y\u00cbM93). This is done compactly but as exactly and completely as possible, in the original Y\u00cbM93 terminology.", "num_citations": "6\n", "authors": ["1450"]}
{"title": "Arguesecure: out-of-the-box security risk assessment\n", "abstract": " Most established security risk assessment methodologies aim to produce ranked lists of risks. But ranking requires quantification of risks, which in turn relies on data which may not be available or estimations which might not be accurate. As an alternative, we have previously proposed argumentation-based risk assessment. In this paper, based on practitioner feedback, we introduce the latest iteration of this method accompanied by two dedicated tools: an online, collaborative web-portal and an offline version. We focus on the lessons learned in iteratively developing and evaluating these tools and the underlying framework. This new framework - called ArgueSecure - focuses on graphically modelling the risk landscape as a collapsible tree. This tree structure intuitively encodes argument traces, therefore maintaining traceability of the results and providing insight into the decision process.", "num_citations": "5\n", "authors": ["1450"]}
{"title": "Engineering security agreements against external insider threat\n", "abstract": " Companies are increasingly engaging in complex inter-organisational networks of business and trading partners, service and managed security providers to run their operations. Therefore, it is now common to outsource critical business processes and to completely move IT resources to the custody of third parties. Such extended enterprises create individuals who are neither completely insiders nor outsiders of a company, requiring new solutions to mitigate the security threat they cause. This paper improves the method introduced in Franqueira et al.(2012) for the analysis of such threat to support negotiation of security agreements in B2B contracts. The method, illustrated via a manufacturer-retailer example, has three main ingredients: modelling to scope the analysis and to identify external insider roles, access matrix to obtain need-to-know requirements, and reverse-engineering of security best practices to\u00a0\u2026", "num_citations": "5\n", "authors": ["1450"]}
{"title": "Requirements engineering conferences: Wither industry tracks?\n", "abstract": " This position paper argues that industry tracks have no place in any research conference. Instead, a research conference should always have room for industrial case studies, evaluated according to criteria for empirical research. Such case studies would not be acceptable at a practitioners' industrial conference, just as papers presented at such conferences would not be acceptable at research conferences. It follows as corollary that if researchers want to become familiar with problems and solutions of RE practice, they should visit industrial conferences.", "num_citations": "5\n", "authors": ["1450"]}
{"title": "Rationality of Cross-System Data Duplication: A Case Study\n", "abstract": " Duplication of data across systems in an organization is a problem because it wastes effort and leads to inconsistencies. Researchers have proposed several technical solutions but duplication still occurs in practice. In this paper we report on a case study of how and why duplication occurs in a large organization, and discuss generalizable lessons learned from this. Our case study research questions are why data gets duplicated, what the size of the negative effects of duplication is, and why existing solutions are not used. We frame our findings in terms of design rationale and explain them by providing a causal model. Our findings suggest that next to technological factors, organizational and project factors have a large effect on duplication. We discuss the implications of our findings for technical solutions in general.", "num_citations": "5\n", "authors": ["1450"]}
{"title": "From business value model to coordination process model\n", "abstract": " The increased complexity of business webs calls for modeling the collaboration of enterprises from different perspectives, in particular the business and process perspectives, and for mutually aligning these perspectives. Business value modeling and coordination process modeling both are necessary for a good e-business design, but these activities have different goals and use different concepts. Nevertheless, the resulting models should be consistent with each other because they refer to the same system from different perspectives. Hence, checking the consistency between these models or producing one based on the other would be of high value. In this paper we discuss the issue of achieving consistency in multi-level e-business design and give guidelines to produce consistent coordination process models from business value models in a stepwise manner.", "num_citations": "5\n", "authors": ["1450"]}
{"title": "Capturing assumptions while designing a verification model for embedded systems\n", "abstract": " A formal proof of a system correctness typically holds under a number of assumptions. Leaving them implicit raises the chance of using the system in a context that violates some assumptions, which in return may invalidate the correctness proof. The goal of this paper is to show how combining informal and formal techniques in the process of modelling and formal verification helps capturing these assumptions. As we focus on embedded systems, the assumptions are about the control software, the system on which the software is running and the system\u2019s environment. We present them as a list written in natural language that supplements the formally verified embedded system model. These two together are a better argument for system correctness than each of these given separately.", "num_citations": "5\n", "authors": ["1450"]}
{"title": "Architecture is structure plus synergy\n", "abstract": " The concept of architecture has been obfuscated by architects and philosophers who refuse to define it, and call it the expression of an idea (Sullivan 1924), similar to poetry (Tzonis et al. 1989), that tries to express the quality without a name (Alexander 1979). This sounds as impressive as it is meaningless. Architecture, like other forms of design, indeed involves creativity. And achitects, like other creative people, express solutions to problems. But so do scientists and engineers, and although in a metaphysical mood we can muse on the relationship between poetry and science, no one attempts to clarify what scientists do by explaining that they do the same thing as poets.", "num_citations": "5\n", "authors": ["1450"]}
{"title": "Understanding the dynamics of requirements evolution: a comparative case study of groupware implementation\n", "abstract": " This paper presents a conceptual framework that seeks to explain the dynamics of requirements change and evolution. As an initial validation of the framework, it was used to analyze two contrasting cases of groupware implementation. The framework makes a distinction between, on the one hand, requirements as problem definition and as solution specification, and, on the other hand, business requirements and software requirements. This distinction yields four different requirement domains. Changes in requirements can be triggered by breakdowns in any of these domains or by technology-driven initiatives. Requirements evolution, then, is the resolution of these breakdowns and the enactment of initiatives. An increased understanding of the dynamics of requirements evolution can be beneficial for structuring and managing a groupware implementation project and, more importantly, software maintenance in the post-deployment phase of a system.", "num_citations": "5\n", "authors": ["1450"]}
{"title": "An Investigation into Agency Requirements in E-Business Information Systems\n", "abstract": " In digital marketplaces, companies are present in the form of their software, which engages in business ina teractions with other companies. Each organisation that is active in the marketplace is trying to reach its own business goals, which may be in conflict with the goals of other organisations. The software by which an organisation is present in a digital marketplace must act on behalf of this organisation to reach these goals. Thus, there is a relation of agency between the software and the organisation that the software reprea sents. This relation gives rise to a number of agency requirements on the software, which are identified and compared with functional requirements. Results in the area of MultiaAgent Systems may be applicable in the design of information systems for which agency requirements hold. A number of such results are briefly described, and further research issues are identified.", "num_citations": "5\n", "authors": ["1450"]}
{"title": "Minimal Semantics for action Specifications in PDL\n", "abstract": " In this paper we investigate minimal semantics for Propositional Dynamic Logic formulas. The goal is to be able to write action speci cations in a declarative pre/post-condition style. The declarative speci cation of actions comes with some well known problems: the frame problem, the quali cation problem and the rami cation problem. We incorporate the assumptions that are inherent to both the frame and quali cation problem into the semantics of Dynamic Logic by de ning preferences over Dynamic logic models. This gives us an intended semantics that, for each declarative action speci cation, selects a unique meaning for each action.", "num_citations": "5\n", "authors": ["1450"]}
{"title": "Web-based Collaborative Security Requirements Elicitation.\n", "abstract": " This empirical study aims at evaluating a structured but informal security requirements engineering method supported by a collaborative Web-based tool. The method allows stakeholders to contribute to the risk analysis and security requirements of elicitation of a software or system in a structured manner that allows traceability between vulnerabilities and mitigations. The tool\u2019s collaborative and distributed workflow promotes higher levels of participation for busy practitioners with a minimum investment of time. REFSQ participants will have the opportunity to test our new platform, and to provide feedback. The experiment revolves around a fictitious scenario. Interested individuals can connect to our server at any time and all results will be publicly available. The tool is available as Open Source software and will later be made available as virtual machine too.", "num_citations": "4\n", "authors": ["1450"]}
{"title": "Behavior Change Support Systems for Privacy and Security.\n", "abstract": " This article proposes to use Behavior Change Support Systems (BCSSs) to improve the security of IT applications and the privacy of its users. We discuss challenges specific to BCSSs applied to information security, list research questions to be answered in order to meet these challenges, and propose an architecture for the Personal Information Security Assistant (PISA), a software framework designed to improve the privacy-related behaviors of end-users.", "num_citations": "4\n", "authors": ["1450"]}
{"title": "A2thOS: availability analysis and optimisation in SLAs\n", "abstract": " Information technology (IT) service availability is at the core of customer satisfaction and business success for today's organisations. Many medium\u2010 to large\u2010size organisations outsource part of their IT services to external providers, with service\u2010level agreements describing the agreed availability of outsourced service components. Availability management of partially outsourced IT services is a non\u2010trivial task since classic approaches for calculating availability are not applicable, and IT managers can only rely on their expertise to fulfil it. This often leads to the adoption of non\u2010optimal solutions. In this paper we present A2thOS, a framework to calculate the availability of partially outsourced IT services in the presence of SLAs and to achieve a cost\u2010optimal choice of availability levels for outsourced IT components while guaranteeing a target availability level for the service. Copyright \u00a9 2011 John Wiley & Sons, Ltd.", "num_citations": "4\n", "authors": ["1450"]}
{"title": "Towards Validating Risk Indicators Based on Measurement Theory\n", "abstract": " Due to the lack of quantitative information and for cost-efficiency purpose, most risk assessment methods use partially ordered values (eg high, medium, low) as risk indicators. In practice it is common to validate risk scales by asking stakeholders whether they make sense. This way of validation is subjective, thus error prone. If the metrics are wrong (not meaningful), then they may lead system owners to distribute security investments inefficiently. Therefore, when validating risk assessment methods it is important to validate the meaningfulness of the risk scales that they use. In this paper we investigate how to validate the meaningfulness of risk indicators based on measurement theory. Furthermore, to analyze the applicability of measurement theory to risk indicators, we analyze the indicators used by a particular risk assessment method specially developed for assessing confidentiality risks in networks of organizations.", "num_citations": "4\n", "authors": ["1450"]}
{"title": "Design Science and Software Engineering.\n", "abstract": " Design Science and Software Engineering Page 1 26th July 2009 ICSOFT09 1 Design Science and Software Engineering Roel Wieringa University of Twente The Netherlands Page 2 26th July 2009 ICSOFT09 2 Outline \u2022 Practical problems versus knowledge problems \u2013 Problem choice \u2022 Design science and software engineering \u2013 Theories \u2013 Research methods Page 3 26th July 2009 ICSOFT09 3 Information systems research problems (Department of management science) \u2022 1980s \u2013 Complaints about lack of empirical rigour \u2013 Papers about empirical methods for IS research \u2022 1990s \u2013 Empirical papers \u2022 2000s \u2013 Complaint about lack of relevance \u2013 \u201cRelevance will improve if we include designing in our research\u201d. Example papers at Int\u2019l Conf. on Information Systems 1997: \u201cSuccessful IS innovation: the contingent contributions of innovation characteristics and implementation process\u201d \u201cThe effects of task interruption \u2026", "num_citations": "4\n", "authors": ["1450"]}
{"title": "CRAC: Confidentiality risk analysis and IT-architecture comparison of business networks\n", "abstract": " The leakage of confidential information (eg industrial secrets, patient records and user credentials) is one of the risks that have to be accounted for and mitigated by organizations dealing with confidential data. Unfortunately, assessing confidentiality risk is challenging, particularly in the presence of crossorganization cooperation, like in the case of outsourcing. This is due to the complexity of business networks. This paper presents an IT-architecture based method for assessing and comparing confidentiality risks of IT-based business networks from the perspective of one of the organizations in the network.", "num_citations": "4\n", "authors": ["1450"]}
{"title": "RE 05: engineering successful products\n", "abstract": " At the Requirements Engineering conference series, researchers and practitioners exchange experiences, discuss problems, and propose solutions. The theme of RE 05--Engineering Successful Products--reflects the understanding that high-quality requirements are at the heart of successful products. To be successful, developers must understand the goals and needs of users, customers, and other stakeholders and must build products that address these goals.", "num_citations": "4\n", "authors": ["1450"]}
{"title": "Value-exchange patterns in business models of intermediaries that offer negotiation services\n", "abstract": " As a result of the diffusion of Internet-technology in mid-90s, the business world met a new disruptive possibility [4][5] to exchange data by computer networks at low cost. As any disruptive technology, computer-based networking has changed business activities and the constellation of businesses significantly. Because many of the equations upon which business models have been built have changed, a rethinking of these models is required. This especially holds for intermediary business functions; existing intermediate parties like travel agencies disappear, while at the same time new types of intermediaries emerge.Utilization of Internet technology by businesses spawned a new field of research, namely e-business research. Two main streams of e-business research exist [9]. The first stream aims at conceptualizing the principles that form the foundation of a business. The resulting business models describe a\u00a0\u2026", "num_citations": "4\n", "authors": ["1450"]}
{"title": "Architecture Alignment in a Large Government Organization: A Case Study\n", "abstract": " In this paper we view IT architecture as the structures present in the entire information technology support used by an organization. Research into IT architecture either is of a strategic nature, yielding no operational guidelines for the practicing IT-architect, or it is part of software engineering, yielding no guidelines that related software architecture to the business environment. In this paper we report on a detailed case study of an operational IT architecture process, in which we investigated the relationship between IT architecture and business context. We analyze this process in terms of a conceptual framework for IT architecture presented earlier. The major findings are that in this case study there is a close relationship between IT architecture and the structure of the IT department, which makes IT architecture design also a problem of organizational design; that application architecture is designed by aligning\u00a0\u2026", "num_citations": "4\n", "authors": ["1450"]}
{"title": "Techniques for reactive system design: the tools in TRADE\n", "abstract": " Reactive systems are systems whose purpose is to maintain a certain desirable state of affairs in their environment, and include information systems, groupware, workflow systems, and control software. The current generation of information system design methods cannot cope with the high demands that originate from mission-critical application, geographic distribution, and a mix of data-intensive, behavior-intensive and communication-intensive properties of many modern reactive systems. We define an approach to designing reactive software systems that deals with these dimensions by incorporating elements from various information system and software design techniques and extending this with formal specification techniques, in particular with model checking. We illustrate our approach with a smart card application and show how informal techniques can be combined with model checking.", "num_citations": "4\n", "authors": ["1450"]}
{"title": "Information Systems-correctness And Reusability-Selected Papers Form The Is-core Workshop\n", "abstract": " This volume contains papers on formal system specification. The chapters treat algebraic specification, temporal logic specification, default specifications and deontic logic specification. Applications include information systems, distributed systems, and real-time systems. One of the major themes in the book is the motivation to bring formal specification techniques one step further towards realistic applications.", "num_citations": "4\n", "authors": ["1450"]}
{"title": "Validating database constraints and updates using automated reasoning techniques\n", "abstract": " In this paper, we propose a new approach to the validation of formal speci cations of integrity constraints. The validation problem of formal speci cations consists of assuring whether the formal speci cation corresponds with what the domain specialist intends. This is distinct from the veri cation problem, which is the problem whether an implementation (which is a formal object) corresponds with a speci cation (which is also a formal object). We consider formal speci cations of object-oriented database systems that are subject to static and dynamic integrity constraints. To validate that such a speci cation expresses what we intend, we propose a system that can answer reachability queries, in which it is asked whether the system can evolve from one state into another without violating the integrity constraints. If the query is answered positively, the system should exhibit an example path between the two states; if the answer is negative, the system should explain why this is so. We discuss the use of planning and theorem-proving techniques to answer such queries, illustrating their application to reachability queries relevant for database system validation.", "num_citations": "4\n", "authors": ["1450"]}
{"title": "Requirements Engineering Since the Year One Thousand\n", "abstract": " This paper argues that for each advance in RE that has been made in the past 1000 years, the older practices were not replaced but still exist, and need to be studied empirically.", "num_citations": "3\n", "authors": ["1450"]}
{"title": "Investigating the usability and utility of tangible modelling of socio-technical architectures\n", "abstract": " Socio-technical models are models that represent social as well as technical elements of the modeling subject, where the technical part consists of both physical and digital elements. Examples are enterprise models and models of the target of assessment used in risk assessment. Constructing and validating these models often implies a challenging task of extracting and integrating information from a multitude of stakeholders which are rarely modelling experts and don\u2019t usually have the time or desire to engage in modelling activities. We investigate a promising approach to overcome this challenge by using physical tokens to represent the model. We call the resulting models tangible models. In this paper we illustrate this idea by creating a tangible representations of a socio-technical modelling language used in Risk Assessment and provide an initial validation of the relative usability and utility of tangible versus abstract modelling by an experiment and a focus group, respectively. We discuss possible psychological and social mechanisms that could explain the enhanced usability and utility of tangible modelling approaches for domain experts. Finally, we discuss the generalizability of this approach to other languages and modelling purposes.", "num_citations": "3\n", "authors": ["1450"]}
{"title": "Modelling telecom fraud with e3value\n", "abstract": " Telecommunication services are complex product packages that rely on a large and complex technical infrastructure. However, fraudulent use of such telecommunication services rarely exploits hardware vulnerabilities. Instead, most common exploits operate at a business level, capitalizing on the unexpected interaction between various product packages from multiple providers. As such, an assumption was made that in order to fully describe the scenarios, a modelling language capable of describing value transactions between actors is required. In order to validate this assumption, a business value modelling language, e3value (cf. section 1.1. 2) was selected, generic (non-misuse) business models were created and four misuse scenarios were modelled. This report showcases the models, discusses strengths and limitations encountered during modelling and draws conclusions with regard to the applicability, usability and utility of e3value models in modelling (Telecom) fraud as well as more generally in Risk Assessment.", "num_citations": "3\n", "authors": ["1450"]}
{"title": "Design and initial validation of the Raster method for telecom service availability risk assessment\n", "abstract": " Crisis organisations depend on telecommunication services; unavailability of these services reduces the effectiveness of crisis response. Crisis organisations should therefore be aware of availability risks, and need a suitable risk assessment method. Such a method needs to be aware of the exceptional circumstances in which crisis organisations operate, and of the commercial structure of modern telecom services. We found that existing risk assessment methods are unsuitable for this problem domain. Hence, crisis organisations do not perform any risk assessment, trust their supplier, or rely on service level agreements, which are not meaningful during crisis situations. We have therefore developed a new risk assessment method, which we call RASTER. We have tested RASTER using a case study at the crisis organisation of a government agency, and improved the method based on the analysis of case results. Our initial validation suggests that the method can yield practical results.", "num_citations": "3\n", "authors": ["1450"]}
{"title": "A trust ontology for business collaborations\n", "abstract": " There are currently some ontologies of business collaboration that facilitate automated collaboration, such as e3value, REA, and BMO. However, these ontologies model the situation that all business actors can be trusted. This is not true in practice. To realize automated business collaboration, trust needs to be added to the business ontology. In this paper, we extend the e3value ontology with the concept of trust and show how this can be used to reason about trust on actors in a business network. We take a minimal approach, ie rather than adding all the nuances of the concept of trust, we provide the minimal extension that allows an actor to reason about trusting other actors in a useful way. We end the paper with a discussion of how this approach can be generalized to other approaches.", "num_citations": "3\n", "authors": ["1450"]}
{"title": "Endurability and profitability analysis of collaborative networks\n", "abstract": " A collaborative network is a network consisting of a variety of autonomous actors (eg enterprises, organizations and people) that collaborate to better achieve common or compatible goals. A collaborative network starts with a contract and then the collaboration partners conduct business as described in the contract. Before engaging in such a collaboration, partners need to reach an agreement regarding their responsibilities in the collaboration and develop a shared understanding regarding the endurability and the profitability of the collaboration. Here in this paper, we aim at analysing the endurability of collaborative networks based on the trust relations between collaboration partners and also introducing a new approach to do profitability analysis for collaborative networks. Therefore, we enrich the value models of business collaborations with information about trust, endurability and profitability.", "num_citations": "3\n", "authors": ["1450"]}
{"title": "CRAC: Confidentiality risk assessment and IT-infrastructure comparison\n", "abstract": " CRAC is an IT-infrastructure-based method for assessing and comparing confidentiality risks of distributed IT systems. The method determines confidentiality risks by taking into account the effects of the leakage of confidential information (e.g. industrial secrets), and the paths that may be followed by different attackers (e.g. insider and outsider). We evaluate its effectiveness by applying it to a real-world outsourcing case.", "num_citations": "3\n", "authors": ["1450"]}
{"title": "Requirements Engineering: Foundation for Software Quality: 16th International Working Conference, REFSQ 2010, Essen, Germany, June 30-July 2, 2010. Proceedings\n", "abstract": " This volume compiles the papers accepted for presentation at the 16thWorking C-ference on Requirements Engineering: Foundation for Software Quality (REFSQ 2010), held in Essen during June 30 and July 1-2, 2010. Since 1994, when the first REFSQ took place, requirements engineering (RE) has never ceased to be a dominant factor influencing the quality of software, systems and services. Initially started as a workshop, the REFSQ working conference series has now established itself as one of the leading international forums to discuss RE in its (many) relations to quality. It seeks reports of novel ideas and techniques that enhance the quality of RE products and processes, as well as reflections on current research and industrial RE practices. One of the most appreciated characteristics of REFSQ is that of being a highly interactive and structured event. REFSQ 2010 was no exception to this tradition. In all, we received a healthy 57 submissions. After all submissions had been ca-fully assessed by three independent reviewers and went through electronic disc-sions, the Program Committee met and finally selected 15 top-quality full papers (13 research papers and 2 experience reports) and 7 short papers, resulting in an acc-tance rate of 38%. The work presented at REFSQ 2009 continues to have a strong anchoring in pr-tice with empirical investigations spanning over a wide range of application domains.", "num_citations": "3\n", "authors": ["1450"]}
{"title": "Architecture-based qualitative risk analysis for availability of it infrastructures\n", "abstract": " An IT risk assessment must deliver the best possible quality of results in a time-effective way. Organisations are used to customise the general-purpose standard risk assessment methods in a way that can satisfy their requirements. In this paper we present the QualTD Model and method, which is meant to be employed together with standard risk assessment methods for the qualitative assessment of availability risks of IT architectures, or parts of them. The QualTD Model is based on our previous quantitative model, but geared to industrial practice since it does not require quantitative data which is often too costly to acquire. We validate the model and method in a real-world case by performing a risk assessment on the authentication and authorisation system of a large multinational company and by evaluating the results wrt the goals of the stakeholders of the system. We also perform a review of the most popular standard risk assessment methods and an analysis of which one can be actually integrated with our QualTD Model.", "num_citations": "3\n", "authors": ["1450"]}
{"title": "Review of code clone articles\n", "abstract": " This report presents the results of a structured review of code clone literature. The aim of the review is to assemble a conceptual model of clone-related concepts which helps us to reason about clones. This conceptual model unifies clone concepts from a wide range of literature, so that findings about clones can be compared with each other. The conceptual model is work in progress; more research is needed to refine the concepts.", "num_citations": "3\n", "authors": ["1450"]}
{"title": "Conceptual modeling in social and physical contexts\n", "abstract": " The history of the computing sciences shows a shift in attention from the syntactic properties of computation to the semantics of computing in the real world. A large part of this shift has been brought about by the introduction of conceptual modeling languages. In this paper I review this history from the early 1970s and identify the elements of real-world semantics that these notations have been used for. In the physical domains typical of control systems, conceptual modeling is always combined with causal modeling in order to register and control behavior in the domain. Because causal relationships are domain-specific, conceptual modeling languages in physical domains can be expected to evolve into domain-specific languages used by engineers. By contrast, in social domains causal modeling plays a minor role. In social domains conceptual models are shared by the people in the domain, and therefore constitute the domain. This creates a different mechanism for registration and control, in which events can be made to occur by means of social convention. Because conceptual models constitute the social world, we can expect conceptual modeling languages to evolve into domain specific languages here too, but in contrast to conceptual modeling languages in physical domains, they will be used as means of communication between engineers and members of the social domain. This paper ends with a plea for more specialization and less standardization in conceptual modeling.", "num_citations": "3\n", "authors": ["1450"]}
{"title": "Competenties van de ICT-architect\n", "abstract": " Digitale architectuur staat binnen en buiten Nederland volop in de aandacht. In Nederland proberen organisaties zoals het SCIA en ERIA digitale architecten (onder verschillende namen) te certificeren, en internationaal heeft het TOGAF afgelopen zomer een certificatie-initiatief gelanceerd. In al deze initiatieven zijn de gepubliceerde competentiedefinities nogal vaag en onvolledig gedefinieerd, en bij sommige certificeringsinstellingen zijn ze geheel afwezig. Bovendien is de terminologie niet gestandaardiseerd, zodat gebruikersbedrijven niet weten wat ze krijgen als een consultancybedrijf ze een enterprise-architect of informatie-architect aanbiedt. In de afgelopen twee jaar is door de Universiteit Twente in opdracht van het Nederlands Architectuurforum (NAF) onderzoek gedaan naar competenties van ICT-architecten. In dit artikel vat ik de resultaten kort samen. Ik begin met een definitie van het competentiebegrip.", "num_citations": "3\n", "authors": ["1450"]}
{"title": "The GRAAL Architecture Framework\n", "abstract": " To describe ICT architectures in a business, we need a conceptual framework that provides us with the words in which to describe the architecture. The framework used in the GRAAL project is a result of 10 years of research in the design of information systems. It was inspired by similar frameworks used in systems engineering and industrial product engineering, but was adapted to suit the needs of information systems architects. In the course of 10 years it has been simplified continuously. At the same time, it was validated in many different projhects, large and small, in different organizations.The word``system''in the following refers to any coherent collection of elements. Examples are an information system, the entire enterprise system layer of a business, the business itself, or a value network of businesses. All of these are examples of systems, and they all have an architecture.", "num_citations": "3\n", "authors": ["1450"]}
{"title": "Towards Semantic Service Specification and Discovery.\n", "abstract": " This document presents the research approach of the SEINE project. The goal of the research is to improve the existing methods for service specification and discovery by using ontologies. The product of the research will be an ontology-based method for semantically rich service specification and discovery and infrastructure that implements the proposed method.", "num_citations": "3\n", "authors": ["1450"]}
{"title": "A semantics for persistency in propositional dynamic logic\n", "abstract": " This paper defines a minimal change semantics for PDL, that is based on minimization over a change ordering of labeled Kripke models. The definition of the change ordering has some striking resemblances with the notion of bisimulation. The minimal change semantics for PDL is shown to behave correctly in case of the notorious Yale shooting and stolen car example scenarios.", "num_citations": "3\n", "authors": ["1450"]}
{"title": "Embedding object-oriented design in system engineering\n", "abstract": " The Unified Modeling Language (UML) is a collection of techniques intended to document design decisions about software. This contrasts with systems engineering approaches such as for exampleStatemate and the Yourdon Systems Method (YSM), in which the design of an entire system consisting of software and hardware can be documented. The difference between the system- and the software level is reflected in differences between execution semantics as well as in methodology. In this paper, I show how the UML can be used as a system-level design technique. I give a conceptual framework for engineering design that accommodates the system- as well as the software level and show how techniques from the UML and YSM can be classified within this framework, and how this allows a coherent use of these techniques in a system engineering approach. These ideas are illustrated by a case study in\u00a0\u2026", "num_citations": "3\n", "authors": ["1450"]}
{"title": "Towards a Method for Evolutionary Implementation of Groupware\n", "abstract": " Groupware is a typical example of an application domain in which requirements are hard to elicit and keep changing before, during, and after the introduction of the system. This calls for an evolutionary implementation approach. Several socio-technical models give an explanation of the interaction between the technical and the social system, ie, the software and its organisational environment, but these models have not yet led to a clear method for evolutionary implementation. In order to arrive at such a method, we need a theoretical framework for how this adaptation process takes place. Adaptive Structuration Theory (AST) is a good candidate for such a framework. The BITE research project at the University of Twente aims to operationalise the concepts of AST through application in several industrial pilot projects. Based on these pilots we will develop a method for evolutionary implementation of groupware.", "num_citations": "3\n", "authors": ["1450"]}
{"title": "Preferential semantics for action specifications in first-order modal action logic\n", "abstract": " In this paper we investigate preferential semantics for declarative speci cations in a First Order Modal Action Logic. We address some well known problems: the frame problem, the quali cation problem and the rami cation problem. We incorporate the assumptions that are inherent to both the frame and quali cation problem into the semantics of the modal Action Logic by de ning orderings over Dynamic Logic models. These orderings allow us to identify for each declarative Dynamic Logic action speci cation a unique intended model.", "num_citations": "3\n", "authors": ["1450"]}
{"title": "Advanced Object-Oriented Requirement Specification Methods\n", "abstract": " Peer code review locates common coding standard violations and simple logical errors in the early phases of software development, and thus, reduces overall cost. Unfortunately, at GitHub, identifying an appropriate code reviewer for a pull request is challenging given that reliable information for reviewer identification is often not readily available. In this paper, we propose a code reviewer recommendation tool-CORRECT-that considers not only the relevant cross-project work experience (eg, external library experience) of a developer but also her experience in certain specialized technologies (eg, Google App Engine) associated with a pull request for determining her expertise as a potential code reviewer. We design our tool using client-server architecture, and then package the solution as a Google Chrome plug-in. Once the developer initiates a new pull request at GitHub, our tool automatically analyzes the\u00a0\u2026", "num_citations": "3\n", "authors": ["1450"]}
{"title": "Using the tools in TRADE II: Specification and design of a meeting scheduler system\n", "abstract": " This report contains a solution of the meeting scheduler case set for the Ninth IEEE International Workshop on Software Speci cation and Design (IWSSD-9), Ise-Shima, Japan, April 16 {18, 1998. The case description can be found in http://salab-www. cs. titech. ac. jp/iwssd9. html. The solution presented here uses tools from TRADE (Toolkit for Requirements and Design Engineering), a toolkit developed at the Vrije Universiteit, Amsterdam. I refer to the Meeting Scheduler System as MSS. This report makes no claim about the utility of meeting scheduler software.The report is structured as follows. Sections 2 to 7 present a speci cation and design of a vanilla version of the MSS, that contains minimal functionality. The TRADE speci cation is contained in the gures; the body of the text gives some comments upon the speci cation. The reader can get an impression of the TRADE speci cation by skimming the gures whose caption starts with TRADE speci cation. Section 8 lists a number of additional features to the vanilla version and shows how they could be implemented. Section 9 winds up the report with a discussion and conclusions.", "num_citations": "3\n", "authors": ["1450"]}
{"title": "Jackson System Development, Entity-relationship Analysis and Data Flow Models: a comparative study\n", "abstract": " This paper compares JSD with ER modelling and with data ow modelling. It is shown that the JSD method can be combined with ER modelling and that the result is a richer method than either of the two. The resulting method can serve as a basis for a practical object-oriented modelling method and has some resemblance to parts of well-known methods, like OMT. It is also argued that JSD and data ow modelling rest on opposite philosophies and cannot be combined in one modelling e ort. This is illustrated by transforming a JSD model into a data ow model and listing the di erences between the models. The results of this analysis are extrapolated to object-oriented models.", "num_citations": "3\n", "authors": ["1450"]}
{"title": "Towards More Individualized Interfaces: Automating the Assessment of Computer Literacy.\n", "abstract": " Computer Literacy is an important predictor for how proficient a person is in its interaction with computers, which can determine whether a person is motivated and able to use specific software. Measuring Computer Literacy or its constituent elements (Skills, Attitude, Knowledge and Experience) has traditionally been done using questionnaires. This method has several limitations: it is effort-intensive for subjects, subject to cognitive biases, and constitutes only a snapshot of a person\u2019s actual Computer Literacy. This limits the usefulness of Computer Literacy as a factor in persuasive systems design. In this paper, we describe an experiment to test the design of a system that extracts elements of Computer Literacy based on observation of human-computer interaction. This new method has the potential to enable the use of Computer Literacy in software design by addressing some of the barriers to its use\u201d in the wild\u201d, opening up new possibilities for tailored and adaptive systems.", "num_citations": "2\n", "authors": ["1450"]}
{"title": "Learning from Accidents: A Systematic Review of Accident Analysis Methods and Models\n", "abstract": " After a risk has manifested itself and has led to an accident, valuable lessons can be learned to reduce the risk of a similar accident occurring again. This calls for accident analysis methods. In the past 20 years, a large number of accident analysis methods have been proposed and it is difficult to find the right method to apply in a specific circumstance. The authors conducted a review of the state of the art of accident analysis methods and models across domains. They classify the models using the well-known categorization into sequential, epidemiological, and systemic methods. The authors find that these classes have their own characteristics in terms of speed of application versus pay-off. For optimum risk reduction, methods that take organizational issues into account can add valuable information to the risk management process in an organization.", "num_citations": "2\n", "authors": ["1450"]}
{"title": "Design Science Research Methods\n", "abstract": " \u2013SIKS dissertations http://www. siks. nl/dissertations. php\u2013Master theses in business informatics http://essay. utwente. nl/view/programme/60025. html\u2013Master theses in computer science http://essay. utwente. nl/view/programme/60300. html\u2013Master theses in human\u2010media interaction http://essay. utwente. nl/view/programme/60030. html", "num_citations": "2\n", "authors": ["1450"]}
{"title": "Design science research in information systems and software systems engineering.\n", "abstract": " \u2013Improve< problem context>\u2013by< treating it with a (re) designed artifact>\u2013such that< artifact requirements>\u2013in order to< stakeholder goals>", "num_citations": "2\n", "authors": ["1450"]}
{"title": "Validating the Raster Risk Assessment Method in Practice.\n", "abstract": " Telecommunication services are essential to modern information systems, especially so for crisis management. Telecoms systems are complex and difficult to analyse. Current risk assessment methods are either not used because of their complexity, or lack rigorous argumentation to justify their results because they are oversimplified. Our challenge has been to develop a risk assessment method that is both usable in practice and delivers understandable arguments to explain and justify its risk evaluations. After experiments to validate the method in laboratory environments, we now present the first results from successful application with practitioners in a regional crisis organization that provides evidence about the practical usability of the method.", "num_citations": "2\n", "authors": ["1450"]}
{"title": "Using value models to improve the cost/benefit analysis of inter-organizational system implementations\n", "abstract": " Jointly developing a business case for inter-organizational information systems (IOS) is difficult as: (1) in a business network there are benefits that may not appear at the site where costs occur, and (2) the involved stakeholders often have different or even conflicting organizational goals. This paper analyzes the use of value modeling as a way to address these two challenges and support business case development in a network. We carried out a case study to explore the usefulness of the value modeling logic during an IOS implementation project and conclude that the integration of value modeling into business case development can help to improve the quality of the business case. The value model allows business partners to get insights into the way value is exchanged in the network and check the distribution of costs and benefits, yet doing so without having to reveal confidential details about internal\u00a0\u2026", "num_citations": "2\n", "authors": ["1450"]}
{"title": "Explaining embedded software modelling decisions\n", "abstract": " As today's devices, gadgets and machines become more intelligent, the complexity of embedded software controlling them grows enormously. To deal with this complexity, embedded software is designed using model-based paradigms. The process of modelling is a combination of formal and creative, design steps. Because of the partially non-formal character of modelling, the relation between a model and the system cannot be expressed mathematically. Therefore, the modeller's justification that the model represents the system adequately can only be non-formal. In this paper we discuss the nature of non-formal modelling steps and pin-point those that create a 'link' between the model and the system. We propose steps to structure the explanation and justification of non-formal modelling decisions. This in turn should enhance confidence that the non-formal, physical world surrounding the embedded system is\u00a0\u2026", "num_citations": "2\n", "authors": ["1450"]}
{"title": "Managing trust in business webs using game theory\n", "abstract": " Business webs are collections of enterprises designed to jointly satisfy a consumer need. We design business webs using value models that show the participating stakeholders and the value objects which they are going to exchange during a specific period. Each partner agrees to act according to the value model, however during the business, stakeholders need to be sure if their partners are acting according to the value model or not. In some cases, the best way to find out the answer of that question is to run an inspection because the value exchanges are not observable without cost. Now, the crucial question here is \"how often a stakeholder needs to run an inspection and how these inspections affect the expected payoffs of the stakeholders?\". Here in this paper, using game theory concepts and techniques, we aim at finding out answers for these questions.", "num_citations": "2\n", "authors": ["1450"]}
{"title": "How to assess telecom service availability risks for crisis organisations?\n", "abstract": " Crisis organisations, such as fire services, disaster relief and emergency medical care, nowa-days depend on telecommunications services in an unprecedented manner. Unavailability of these services during a crisis may cost lives. In order not to be caught unprepared, crisis organisations should perform a risk assessment on telecom services availability. This risk assessment must take many factors into account. Some of these factors can be quantified objectively, but many factors are of a qualitative or even subjec-tive nature. We call these risk factors social factors. Existing risk assessment methods either cannot handle social factors, or do not meet all of the requirements of crisis organisations. Many crisis organisations therefore do not assess their telecom service availability risks. In this paper we propose a structured way to include social risk factors into qualitative risk descriptions. This is an important step towards a risk assess-ment method for telecom services used by crisis organisations which we are currently developing.", "num_citations": "2\n", "authors": ["1450"]}
{"title": "Introduction to the first international workshop on empirical research in business process management (ER-BPM 2009)\n", "abstract": " Providing effective IT support for business processes has become crucial for enterprises to stay competitive. In response to this need numerous process support paradigms (e.g., workflow management, service flow management, case handling), process specification standards (e.g., WS-BPEL, BPML, BPMN), process tools (e.g., ARIS Toolset, Tibco Staffware, FLOWer), and supporting methods have emerged in recent years. Summarized under the term \u201cBusiness Process Management\u201d (BPM), these paradigms, standards, tools, and methods have become a success-critical instrument for improving process performance.", "num_citations": "2\n", "authors": ["1450"]}
{"title": "Non-monotonic modelling from initial requirements: a proposal and comparison with monotonic modelling methods\n", "abstract": " Researchers make a significant effort to develop new modelling languages and tools. However, they spend less effort developing methods for constructing models using these languages and tools. We are developing a method for building an embedded system model for formal verification. Our method provides guidelines to build a model and to construct a correctness argument. We start from a high-level formula stating that a plant (a device that performs a task) and its control should satisfy requirements. As our knowledge about the system grows, we refine this formula and the model gradually, in a stepwise non-monotonic process, until we have a description that can be formally verified. In this paper we explain our method on a simple example and compare it briefly with two other methods: requirements progression and the goal-oriented KAOS approach. The requirements progression is an extension of a problem\u00a0\u2026", "num_citations": "2\n", "authors": ["1450"]}
{"title": "Structured Review of Code Clone Literature\n", "abstract": " This report presents the results of a structured review of code clone literature. The aim of the review is to assemble a conceptual model of clone-related concepts which helps us to reason about clones. This conceptual model unifies clone concepts from a wide range of literature, so that findings about clones can be compared with each other.", "num_citations": "2\n", "authors": ["1450"]}
{"title": "Obtaining formal models through non-monotonic refinement\n", "abstract": " When designing a model for formal verification, we want to be certain that what we proved about the model also holds for the system we modelled. This raises the question of whether our model represents the system, and what makes us confident about this. By performing so called, non-monotonic refinement in the modelling process, we make the steps and decisions explicit. This helps us to (1) increase the confidence that the model represents the system,(2) structure and organize the communication with domain experts and the problem owner, and (3) identify rational steps made while modelling. We focus on embedded control systems.", "num_citations": "2\n", "authors": ["1450"]}
{"title": "Towards the Integration of Value and Coordination Models (Position Paper)\n", "abstract": " Cross-organizational collaborations have a high complexity. Modelling these collaborations can be done from different perspectives. For example, the value perspective represents expected value exchanges in a collaboration while the coordination perspective represents the order in which these exchanges occur. How to maintain consistency between different models during design time as well as runtime constitutes a challenging topic. Defining criteria and definitions reflecting the relation between these models during the entire life cycle is not straightforward. Different criteria are used for different models since each model captures a specific aspect of the collaboration. In this paper we investigate the challenges arising when addressing the problem of maintaining adequate and consistent models of a collaboration during the entire life cycle of a collaboration. We propose a framework in which we connect business layer, process layer and implementation layer, presenting the direction for solving this multifaceted problem. We will describe several challenges we anticipate to encounter while implementing our framework.", "num_citations": "2\n", "authors": ["1450"]}
{"title": "Information technology as coordination infrastructure\n", "abstract": " Business information technology is traditionally viewed as information provision technology. In this view, organizations use their IT to implement databases that provide people with information when they want it. This view is persistent even though information provision is never an end in itself but always has the further purpose to support the coordination of activities of people. The role of IT as coordination technology became more prominent in the 1980s with the advent of network technology, that allowed activities across different businesses to be coordinated. This trend has accellerated since the growth of Internet usage, and today IT is used to support an increasingly varied range of processes performed by a variety of partners that do not all have a hierarchical relation to each other. This makes it difficult to analyze requirements for IT support and specify IT solutions: Business processes may not be well-defined, and interests of different businesses may clash. This report argues that to deal with this in requirements engineering and IT solution specification, business information technology should not be viewed as IT support for business processes but as IT support for the coordination of activities in one or more businesses. We will identify three basic coordination mechanisms, namely coordination by price, by management, and by shared norms, and for each of these mechanisms, we will identify requirements for IT support. The advent of flexible and standardized networking technology has facilitated the creation of novel coordination mechanisms within these three general paradigms, and we will give an inventory of generalized coordination\u00a0\u2026", "num_citations": "2\n", "authors": ["1450"]}
{"title": "Requirements for secure logging of decentralized cross-organizational workflow executions\n", "abstract": " The control of actions performed by parties involved in a decentralized cross-organizational workflow is done by several independent workflow engines. Due to the lack of a centralized coordination control, an auditing is required which supports a reliable and secure detection of malicious actions performed by these parties. In this paper we identify several issues which have to be resolved for such a secure logging system. Further, security requirements for a decentralized data store are investigated and evaluated with regard to decentralized data stores.", "num_citations": "2\n", "authors": ["1450"]}
{"title": "A conceptual framework for architecture alignment guidelines\u2014Project GRAAL WP1 Whitepaper\n", "abstract": " Current research in software architecture concentrates on the design and evaluation of software architectures with respect to a set of desired quality attributes and properties of the development organization [1, 2, 4]. This research is mainly concerned with designing software systems embedded in a technical environment, such as embedded control software. Moreover, the development situation is assumed to be a greenfield situation and focus is on translating requirements into structures of custom-made components. In business environments, however, it is almost never possible to assume a greenfield development situation. In business environments, existing components are configured and deployed by embedding them into existing hardware and software infrastructures. These components are often large-grained standard components that are acquired from outside sources, or are legacy components. The resulting architecture must be aligned with business strategies and processes, including the development organization, and even with the external business environment.Alignment between information systems and business has been an issue since the 1970s. In the last ten years, most of the work in this area has taken the approach of Henderson and Venkatraman [3]. This approach provides clarity at the strategic level but does not tell us how to partition software into applications and how to allocate these to the available implementation platform. The close intertwinement of external and internal business processes and application software in modern service organizations, however, requires a seamless integration of these systems with the\u00a0\u2026", "num_citations": "2\n", "authors": ["1450"]}
{"title": "Documenting the ICT architecture of TSI\n", "abstract": " In this report, we discuss the business-and software architecture of Travel Service International (TSI). We use this as a case study to validate our approach to ICT architecture. The techniques discussed in this report are explained at length in a textbook [1].", "num_citations": "2\n", "authors": ["1450"]}
{"title": "Reducing the extensions of CTL with actions and real time\n", "abstract": " In this report, we present the logic ATCTL, which combines two known extensions of CTL, namely ACTL and TCTL. ACTL extends CTL with constructs to describe actions and TCTL extends it with constructs to specify real-time properties. ATCTL combines both extensions. We use ATCTL as a language for property specification in which we can express state properties, action properties, and real-time properties. We show that the result can be reduced to ACTL as well as to TCTL, and therefore also to CTL. This makes model-checking ATCTL possible, because CTL model checkers exist.", "num_citations": "2\n", "authors": ["1450"]}
{"title": "Mu-calculus-based deontic logic for regular actions\n", "abstract": " This paper introduces deontic logic of regular actions as a fragment of the modal mu calculus Semantic characterizations of deontic notions for regular actions are given in terms of conditions on mu calculus structures and mu calculus formulas capturing this semantics are constructed", "num_citations": "2\n", "authors": ["1450"]}
{"title": "A logic for the specification of multi-object systems\n", "abstract": " We present Multi-Object Dynamic Logic (MODL), a generalization of Dynamic Logic of which the intended use is the declarative specification of systems that are conceptually described by a multitude of objects. In an example specification of the controls of a railroad crossing we demonstrate how MODL can be used to give semantics and reasoning capacity to graphical languages for communicating multi-object systems. Finally we study to what extend temporal and mixed dynamic/temporal properties can be expressed in MODL.", "num_citations": "2\n", "authors": ["1450"]}
{"title": "Two Case Studies of Subsystem Design for Extensible General-Purpose Software\n", "abstract": " Two Case Studies of Subsystem Design for Extensible General-Purpose Software \u2014 University of Twente Research Information Skip to main navigation Skip to search Skip to main content University of Twente Research Information Logo Home Profiles Research Units Research Output Datasets Activities Prizes Press / Media Search by expertise, name or affiliation Two Case Studies of Subsystem Design for Extensible General-Purpose Software PWPJ Grefen, Nicolaas Sikkel, Roelf J. Wieringa Research output: Book/Report \u203a Report \u203a Academic Overview Original language Undefined Publisher Centre for Telematics and Information Technology (CTIT) Number of pages 16 Publication status Published - 1998 Publication series Name CTIT Technical Report Series No. 98-14 ISSN (Print) 1381-3625 Keywords METIS-118600 Cite this APA Author BIBTEX Harvard Standard RIS Vancouver Grefen, PWPJ, Sikkel, N., & \u2026", "num_citations": "2\n", "authors": ["1450"]}
{"title": "A Modal Temporal Dynamic Logic: Doing the Deadline\n", "abstract": " In this paper an investigation into the aspects of formal system speci cation that manifest themselves when considering both time and system dynamics is carried out. A logic to express temporal dynamic properties is developed. yThis work was partly supported by the eu under esprit-iv wg 22704 aspire. zThis research is partly supported by the Netherlands Organization for Scienti c Research (nwo), sion project 612-323-419.", "num_citations": "2\n", "authors": ["1450"]}
{"title": "Minimal Semantics for Action Specifications in First-order Dynamic Logic\n", "abstract": " In this paper we investigate minimal semantics for First Order Dynamic Logic formulas. The goal is to be able to write action speci cations in a declarative pre/post-condition style. The declarative speci cation of actions comes with some well known problems: the frame problem, the quali cation problem and the rami cation problem. We incorporate the assumptions that are inherent to both the frame and quali cation problem into the semantics of Dynamic Logic by de ning orderings over Dynamic Logic models. These orderings allow us to identify for each declarative Dynamic Logic action speci cation a unique intended model. This unique model represents the system that must be associated with the speci cation given the prefential semantics that is de ned by the orderings.", "num_citations": "2\n", "authors": ["1450"]}
{"title": "Minimal semantics for transaction specifications in a multi-modal logic\n", "abstract": " This paper presents an extension of propositional dynamic database logic in which arbitrary database transactions can be speci ed declaratively. Typically, declaratively speci ed transactions are under-speci ed. Speci cations are usually supplemented with a frame assumption about what does not change as the result of a transaction, and a quali cation assumption stating that transactions occur, provided they respect certain conditions called'guards' and do not contravene any constraints. In addition these (static) constraints may be, or may not be interpreted to force rami cations of transactions. In the former case, both frame and quali cation assumption should also apply to rami cations. We solve these problems by de ning preferences over Kripke structures and showing how these can be used to de ne an intended semantics that, for each declarative transaction speci cation, selects a unique meaning for each transaction. In particular, the intended semantics formalizes a frame assumption and selects quali cations and rami cations for each speci cation. The notion of preferential entailment based on the semantics provides a natural way of reasoning about declaratively speci ed transactions.", "num_citations": "2\n", "authors": ["1450"]}
{"title": "Integrated Specification of Values, Objects and Processes for Object-Oriented Models\n", "abstract": " The goal of the research summarized in this abstract is first, to formalize object-oriented models and second, to develop a method for producing these models. This goal can be stated in semi-formal form as the solution of the following equations for the unknown terms that appear on the right-hand side of an equation, but do not appear at a left-hand side.(1) method= specification language+ semantics+ pragmatics (2) pragmatics= tasks+ task ordering+ heuristics By \u2018\u2018method\u2019\u2019we mean a method to develop a conceptual model (CM) of a universe of discourse (UoD). The specification language should be formal, and each consistent specification has an intended denotational model which can serve as a CM of a UoD. The semantics should include a significant portion of the structures that are found in the\" semantic data models\" studied in the past and in object-oriented models currently being developed. The pragmatics give advice on how to go about finding the relevant structures in the UoD and specify a model of them in the specification language.", "num_citations": "2\n", "authors": ["1450"]}
{"title": "Axiomatic specification of database domain statics\n", "abstract": " 1.1 The universe of discourse and the domain 1.2 An extension of the four-level architecture", "num_citations": "2\n", "authors": ["1450"]}
{"title": "Final report on the farmer's aid in plant disease diagnoses\n", "abstract": " This report is the final report on the FAD project. The FAD project was initiated in september 1985 to test the expert system shell Babylon by developing a prototype crop disease diagnosis system in it. A short overview of the history of the project and the main problems encountered is given in chapter 1. Chapter 2 describes the result of an attempt to integrate JSD with modelling techniques like generalisation and aggregation and chapter 3 concentrates on the method we used to elicit phytopathological knowledge from specialists. Chapter 4 gives the result of knowledge acquisition for the 10 wheat diseases most commonly occurring in the Netherlands. The user interface is described briefly in chapter 5 and chapter 6 gives an overview of the additions to the implementation we made to the version of FAD reported in our second report. Chapter 7, finally, summarises the conclusions of the project and gives recommendations for follow-up projects.", "num_citations": "2\n", "authors": ["1450"]}
{"title": "Towards a sustainable blockchain use case\n", "abstract": " Many blockchain applications do not survive the proof-ofconcept phase. We argue that most of these application do not have a high potential business use case. As blockchain is an expensive technology to deploy and develop, it calls for disruptive use cases. These use cases should exploit the philosophy of the blockchain technology, namely (1) removal of the middleman,(2) immutable data and (3) creation of an equal network with entities who do not trust each other on beforehand.", "num_citations": "2\n", "authors": ["1450"]}
{"title": "The Banking Industry Underestimates Costs of Cloud Migrations\n", "abstract": " Cloud computing is a growing part of the IT industry and is seen collectively as a solution to manage data and systems effectively. In numerous areas, cloud computing is being used as the standard for deploying application areas include; banking, telecommunications, and logistics. However, migrating IT infrastructure and applications to the cloud can induce hidden and overlooked costs. This study examines the cloud migration process of ten international corporate banks, to explore and understand costs occurring during a cloud migration. We have identified that the banking industry underestimates cloud migration costs and that budgets are exceeded. The top five cost categories that are underestimated: (1) managing dependencies of applications, (2) legislation, (3) support required from other non IT departments, (4) re-architecting of applications, and (5) hiring of external contractors. Main reasons for\u00a0\u2026", "num_citations": "1\n", "authors": ["1450"]}
{"title": "Availability Incidents in the Telecommunication Domain: A Literature Review\n", "abstract": " Non-availability incidents in public telecom services may have a wide-spread impact, such as disruption of internet services, mobile services, and land-line communication. This, in turn, may disrupt the life of consumers and citizens, and the provision of services by commercial and public organizations. These incidents are always analyzed and solved by the provider. In Europe, there is a legal obligation to report the analysis and solution of the incident to the national telecom regulator. However, these reports are highly confidential, and beyond some elementary descriptive statistics, they are not analyzed. This means that a significant opportunity is missed to draw lessons from these incidents, which could be valuable to other providers and to standardization bodies. In the LINC project, we aim to develop a method to draw lessons learned from registered non-availability incidents without compromising the confidentiality of those registrations. As a preparation for that, we have conducted a systematic literature review of non-availability incidents in public telecom services reported in the scientific and professional literature, to see what we can learn from the reported incident model and analysis methods used. In this report, we present an incident analysis taxonomy to establish a common terminological ground among researchers and practitioners.", "num_citations": "1\n", "authors": ["1450"]}
{"title": "Applying Generic AcciMap to a DDOS Attack on a Western-European Telecom Operator.\n", "abstract": " After a large incident on a telecommunications network, the operator typically executes an incident analysis to prevent future incidents. Research suggests that these analyses are done ad hoc, without a structured approach. In this paper, we conduct an investigation of a large incident according to the AcciMap method. We find that this method can be applied to telecommunications networks with a few small changes; we find that such a structured approach yields many more actionable recommendations than a more focused approach and we find that both the onset of an incident and the resolution phase merit their own analysis. We also find that such an analysis costs a lot of effort and we propose a more efficient approach to using this method. An unexpected outcome was that AcciMap may also be very useful for analyzing crisis organizations.", "num_citations": "1\n", "authors": ["1450"]}
{"title": "SmTIP: A Big Data Integration Platform for Synchromodal Transport\n", "abstract": " This chapter reports on the design of an integration platform that supports the collection and analysis of third\u2010party, real\u2010time data for the dynamic planning of cargo transportation. Especially, flexible allocation of cargo to transportation modes and routes is targeted, also known as synchromodal transportation. The chapter presents a domain model of synchromodal transport resulting from the problem investigation. It also presents the design of the synchromodal transportation integration platform (SmTIP) to improve dynamic planning and information services for synchromodal transport. It provides the development of a prototype for the validation of the SmTIP design. During SmTIP validation, a prototype of the SmTIP using Mendix was developed. The prototype provides a user interface to planners who are responsible for transport planning by dividing orders into orderlines which are then assigned to\u00a0\u2026", "num_citations": "1\n", "authors": ["1450"]}
{"title": "Accident Analysis Methods and Models-a Systematic Review.\n", "abstract": " After a risk has manifested itself and has led to an accident, valuable lessons can be learned that can be taken into account to reduce the risk of a similar accident occurring again. This calls for accident analysis methods. In the past 20 years a large number of accident analysis methods have been proposed and it is difficult to find the right method to apply in a specific circumstance. We conducted a review of the state of the art of accident analysis methods and models across domains. We classify the models using the well-known categorization into sequential, epidemiological and systemic methods. We find that these classes have their own characteristics in terms of speed of application versus pay-off. For optimum risk reduction, methods that take organizational issues into account can add valuable information to the risk management process in an organization.", "num_citations": "1\n", "authors": ["1450"]}
{"title": "Improving the semantic interoperability of IoT Early Warning Systems: the Port of Valencia use case\n", "abstract": " An early warning system (EWS) is a distributed system that monitors the physical world and issues warnings if it detects abnormal situations. The In-ternet-of-Things (IoT) offers opportunities to improve monitoring capabilities of EWS and to realize (near) real-time warning and response. This paper presents the development of an interoperable IoT-based EWS to detect accident risks with trucks that deliver goods at the Valencia port area. Our solution addresses the semantic integration of a variety of data sources with processing in safety-critical applications for effective emergency response. The solution considers existing domain-specific ontologies and standards, along with their serialization formats. Accident risks are assessed by monitoring the drivers\u2019 vital signs with ECG med-ical wearables, and the trucks\u2019 position with speed and accelerometer data. Use cases include the detection of health issues and vehicle collision with dangerous goods. This EWS is developed with the SEMIoTICS framework, which encom-passes a model-driven architecture that guides the application of data representa-tions, transformations and distributed software components. This framework en-ables an EWS to act as a semantic broker for situation-aware decision support.", "num_citations": "1\n", "authors": ["1450"]}
{"title": "Graphical modeling of security arguments: current state and future directions\n", "abstract": " Identifying threats and risks to complex systems often requires some form of brainstorming. In addition, eliciting security requirements involves making traceable decisions about which risks to mitigate and how. The complexity and dynamics of modern socio-technical systems mean that their security cannot be formally proven. Instead, some researchers have turned to modeling the claims underpinning a risk assessment and the arguments which support security decisions. As a result, several argumentation-based risk analysis and security requirements elicitation frameworks have been proposed. These draw upon existing research in decision making and requirements engineering. Some provide tools to graphically model the underlying argumentation structures, with varying degrees of granularity and formalism. In this paper, we compare these approaches, discuss their applicability and suggest avenues\u00a0\u2026", "num_citations": "1\n", "authors": ["1450"]}
{"title": "Computer literacy systematic literature review method\n", "abstract": " Although there have been many attempts to define the concept \u2018computer literacy\u2019, no consensus has been reached: many variations of the concept exist within literature. The majority of papers does not explicitly define the concept at all, instead using an unjustified subset of elements related to computers to assess a subject\u2019s level of computer literacy. This can limit the generalizability of research and can lead to fallacious conclusions. This is an internal report listing the method by which the research was conducted.", "num_citations": "1\n", "authors": ["1450"]}
{"title": "A study on tangible participative enterprise modelling\n", "abstract": " Enterprise modelling (EM) is concerned with discovering, structuring and representing domain knowledge pertaining to different aspects of an organization. Participative EM, in particular, is a useful approach to eliciting this knowledge from domain experts with different backgrounds. In related work, tangible modelling \u2013 modelling with physical objects \u2013 has been identified as beneficial for group modelling.                 This study investigates effects of introducing tangible modelling as part of participative enterprise modelling sessions. Our findings suggest that tangible modelling facilitates participation. While this can make reaching an agreement more time-consuming, the resulting models tend to be of higher quality than those created using a computer. Also, tangible models are easier to use and promote learnability. We discuss possible explanations of and generalizations from these observations.", "num_citations": "1\n", "authors": ["1450"]}
{"title": "Design science research methods and writing research papers\n", "abstract": " \u2013Improve< problem context>\u2013by< treating it with a (re) designed artifact>\u2013such that< artifact requirements>\u2013in order to< stakeholder goals>", "num_citations": "1\n", "authors": ["1450"]}
{"title": "An Empirical Study to validate the Use of Ontological Guidelines in the Creation of i* Models\n", "abstract": " i* is a well known goal modeling framework, developed by a large and geographically dispersed research community. Currently, i* users tend to ascribe different and conflicting meanings to its constructs, leading to a non uniform use of the language, and consequently undermining its adoption. In previous works, we proposed ontological guidelines to support the creation of i* models, in an attempt to provide a solution to this problem. In this paper, we present an empirical study, to evaluate these ontological guidelines. Results show that for more experienced conceptual modelers, the ontological guidelines indeed support i* modeling. However, results are not as positive for nonexperienced conceptual modelers.", "num_citations": "1\n", "authors": ["1450"]}
{"title": "Assumption-based risk identification method (ARM) in dynamic service provisioning\n", "abstract": " In this paper we consider service-oriented applications composed of component services provided by different, economically independent service providers. As in all composite applications, the component services are composed and configured to meet requirements for the composite application. However, in a field experiment of composite service-oriented applications wef found that, although the services as actually delivered by the service providers meet their requirements, there is still a mismatch across service providers due to unstated assumptions, and that this mismatch causes an incorrect composite application to be delivered to end-users. Identifying and analyzing these initially unstated assumptions turns requirements engineering for service-oriented applications into risk analysis. In this paper, we describe a field experiment with an experimental service-oriented homecare system, in which unexpected\u00a0\u2026", "num_citations": "1\n", "authors": ["1450"]}
{"title": "Care-giver tailoring of IT-based healthcare services for elderly at home: A field test and its results\n", "abstract": " One of the biggest drivers behind IT-based homecare solutions is the increasing aging population. We are specifically interested in \"service tailoring\" for the homecare domain, where healthcare professionals (care-givers) do the tailoring of services to support elderly (care-receivers). Our goal is that, using our approach, care-givers can create or modify services with less IT skills, time and/or effort, and care-receivers get services that are better suited for their specific and personal needs. As a proof of concept, we developed a software prototype of our approach. The prototype was subsequently used in a real-world field test at a care institution in the Netherlands to validate the approach. The validation focused on the usability aspects of the approach in terms of effectiveness, efficiency, learnability and satisfaction. This paper describes the design of the field test and reflects on the outcome of the validation experiments.", "num_citations": "1\n", "authors": ["1450"]}
{"title": "Risk Identification of Tailorable Context-aware Systems: a Case Study and Lessons Learned.\n", "abstract": " In this paper, we discuss possible risks posed by the application of tailorable context-aware systems in real-life practices. We use a tailorable context-aware system in the homecare domain as a case study to identify and analyse such risks. Next, we discuss which of these risks can be generalized to the use of tailorable context-aware system in other contexts than homecare. This would help the users of such systems to prevent the risks and guide the design and implementation of them.", "num_citations": "1\n", "authors": ["1450"]}
{"title": "Designing technical action research, and generalizing from single cases\n", "abstract": " Designing technical action research, and generalizing from single cases Page 1 Designing technical action research, and generalizing from single cases Roel Wieringa University of Twente The Netherlands 1 28th June 2012 CAiSE 2012, Gdansk Page 2 1. What is TAR? 2. Logical structure of TAR 3. Generalizing from TAR 4. Summary 2 28th June 2012 CAiSE 2012, Gdansk Page 3 1. Wat is Technical Action Research? 3 28th June 2012 CAiSE 2012, Gdansk Page 4 What is Technical Action Research? \u2022 Example \u2013 Researcher develops a technique to assess confidentiality risks in an IT architecture \u2013 She applies it to a problem that a company has ... \u2013 producing an advice to the company ... \u2013 and drawing lessons learned about the method \u2022 She served two goals: \u2013 The company\u2019s goal is to assess confidentiality risks \u2013 The researcher\u2019s goal is to learn something about her method 4 28th June 2012 CAiSE 2012, \u2026", "num_citations": "1\n", "authors": ["1450"]}
{"title": "Securing the extended enterprise: A method for analyzing external insider threat\n", "abstract": " In extended enterprises, the traditional dichotomy between insiders and outsiders becomes blurred: consultants, freelance administrators, and employees of business partners are both inside and outside of the enterprise. As a consequence, traditional controls to mitigate insider and outsider threat do not completely apply to this group of individuals, and additional or improved solutions are required. The ISO 27002 security standard, recognizing this need, proposes third-party agreements to cover security requirements in B2B relationships as a solution, but leaves open how to realize them to counter security problems of inter-organizational collaboration. To reduce this gap, this chapter presents a method for identifying external insiders and analyzing them from two perspectives: as threats and as possible mitigation. The output of the method provides input for further engineering of third-party agreements related to\u00a0\u2026", "num_citations": "1\n", "authors": ["1450"]}
{"title": "Engineering secure software and systems\n", "abstract": " Engineering Secure Software and Systems - NASA/ADS Now on home page ads icon ads Enable full ADS view NASA/ADS Engineering Secure Software and Systems Erlingsson, \u00dalfar ; Wieringa, Roel ; Zannone, Nicola Abstract Publication: Lecture Notes in Computer Science Pub Date: 2011 DOI: 10.1007/978-3-642-19125-1 Bibcode: 2011LNCS......E Keywords: Computer Science; Computer Communication Networks; Data Encryption; Software Engineering/Programming and Operating Systems; Data Structures; Cryptology and Information Theory; Math Applications in Computer Science; Models and Principles full text sources Publisher | \u00a9 The SAO/NASA Astrophysics Data System adshelp[at]cfa.harvard.edu The ADS is operated by the Smithsonian Astrophysical Observatory under NASA Cooperative Agreement NNX16AC86A NASA logo Smithsonian logo Resources About ADS ADS Help What's New Careers@\u2026", "num_citations": "1\n", "authors": ["1450"]}
{"title": "Structuring problem analysis for embedded systems modelling\n", "abstract": " Our interest is embedded systems validation as part of the model-driven approach. To design a model, the modeller needs to obtain knowledge about the system and decide what is relevant to model and how. A part of the modelling activities is inherently informal-it cannot be formalised in such a way to constitute a basis for automated model design. This does not mean that modelling has to be chaotic. We therefore propose an informal method that structures modelling activities. In this paper we will focus on one of the method ingredients-modelling guidelines. In the industrial case study we performed, we captured modelling steps and elements in a form of a modelling handbook. The goal was to make modelling more efficient by preventing next modellers re-inventing things, but also to preserve a modelling style recognized within company\u2019s context. We show in detail what these re-usable modelling elements are, and how identifying them can be generalised for designing modelling guidelines in general. Finally, we compare our work with work of researchers that formalise problem analysis.", "num_citations": "1\n", "authors": ["1450"]}
{"title": "Value-driven security agreements in extended enterprises\n", "abstract": " Today organizations are highly interconnected in business networks called extended enterprises. This is mostly facilitated by outsourcing and by new economic models based on pay-as-you-go billing; all supported by IT-as-a-service. Although outsourcing has been around for some time, what is now new is the fact that organizations are increasingly outsourcing critical business processes, engaging on complex service bundles, and moving infrastructure and their management to the custody of third parties. Although this gives competitive advantage by reducing cost and increasing flexibility, it increases security risks by eroding security perimeters that used to separate insiders with security privileges from outsiders without security privileges. The classical security distinction between insiders and outsiders is supplemented with a third category of threat agents, namely external insiders, who are not subject to the internal control of an organization but yet have some access privileges to its resources that normal outsiders do not have. Protection against external insiders requires security agreements between organizations in an extended enterprise. Currently, there is no practical method that allows security officers to specify such requirements. In this paper we provide a method for modeling an extended enterprise architecture, identifying external insider roles, and for specifying security requirements that mitigate security threats posed by these roles. We illustrate our method with a realistic example.", "num_citations": "1\n", "authors": ["1450"]}
{"title": "A handbook supporting model-driven software development (a case study)\n", "abstract": " The systems in the scope of our research in MOCA project [4] are embedded control systems. To prove their correctness one can use models for formal verification. Researchers have been investing a lot of effort in formal languages and tools development. However, not much attention was directed to the process of design and construction of these models. This process is non-formal, creative and rational at the same time, and not easy or useful to formalize. Our goal is to extract repeatable, generalizable, rational elements of the model design process. Those elements constitute a method we propose, for systematic model construction and verification. We started our work with literature study and a small but non-trivial modelling example we performed in our lab [3]. The result is the first version of our modelling method. Its ingredients are as follows.", "num_citations": "1\n", "authors": ["1450"]}
{"title": "Structured Review of the Evidence for Effects of Code Duplication on Software Quality\n", "abstract": " This report presents the detailed steps and results of a structured review of code clone literature. The aim of the review is to investigate the evidence for the claim that code duplication has a negative effect on code changeability. This report contains only the details of the review for which there is not enough place to include them in the companion paper published at a conference (Hordijk, Ponisio et al. 2009-Harmfulness of Code Duplication-A Structured Review of the Evidence).", "num_citations": "1\n", "authors": ["1450"]}
{"title": "A Mobile Ambients-based Approach for Network Attack Modelling and Simulation\n", "abstract": " Attack Graphs are an important support for assessment and subsequent improvement of network security. They reveal possible paths an attacker can take to break through security perimeters and traverse a network to reach valuable assets deep inside the network. Although scalability is no longer the main issue, Attack Graphs still have some problems that make them less useful in practice. First, Attack Graphs remain difficult to relate to the network topology. Second, Attack Graphs traditionally only consider the exploitation of vulnerable hosts. Third, Attack Graphs do not rely on automatic identification of potential attack targets. We address these gaps in our MsAMS (Multi-step Attack Modelling and Simulation) tool, based on Mobile Ambients. The tool not only allows the modelling of more static aspects of the network, such as the network topology, but also the dynamics of network attacks. In addition to Mobile Ambients, we use the PageRank algorithm to determine targets and hub scores produced by the HITS (Hypertext Induced Topic Search) algorithm to guide the simulation of an attacker searching for targets.", "num_citations": "1\n", "authors": ["1450"]}
{"title": "De-perimeterisation as a cycle: tearing down and rebuilding security perimeters\n", "abstract": " If an organisation wants to secure its IT assets, where should the security mechanisms be placed? The traditional view is the hard-shell model, where an organisation secures all its assets using a fixed security border: What is inside the security perimeter is more or less trusted, what is outside is not. Due to changes in technologies, business processes and their legal environments this approach is not adequate anymore. This paper examines this process, which was coined de-perimeterisation by the Jericho Forum. In this paper we analyse and define the concepts of perimeter and de-perimeterisation, and show that there is a long term trend in which de-perimeterisation is iteratively accelerated and decelerated. In times of accelerated de-perimeterisation, technical and organisational changes take place by which connectivity between organisations and their environment scales up significantly. In times of deceleration, technical and organisational security measures are taken to decrease the security risks that come with de-perimeterisation, a movement that we call re-perimeterisation. We identify the technical and organisational mechanisms that facilitate de-perimeterisation and re-perimeterisation, and discuss the forces that cause organisations to alternate between these two movements.", "num_citations": "1\n", "authors": ["1450"]}
{"title": "On quality issues in networked value constellations\n", "abstract": " One of the main purposes of collaborative networks is to satisfy specific consumer needs, which one company cannot satisfy alone. With the opening of the internet in the 1990s the number of companies that collaborate by means of computer networks increased rapidly. As far as one of our main foci is the consideration of value object exchanges between the involved business actors, we refer to such collaborative networks as networked value constellations or value webs. The business requirements of networked value constellations need to be enabled and operationalized by means of functional and quality requirements at the IT level. Our paper aims to build a sound understanding of how to plan quality related issues by considering distinct perspectives, namely the business perspective and the information systems perspective. Each perspective requires multiple quality-related considerations. From a business\u00a0\u2026", "num_citations": "1\n", "authors": ["1450"]}
{"title": "Introduction to the workshop on technology transfer in software engineering\n", "abstract": " The goal of the Workshop on Technology Transfer in Software Engineering is to increase our understanding of technology transfer in software engineering, and to learn from successful case studies. We wanted to bring researchers and practitioners together to create an inventory of problems in software engineering technology transfer as seen from both the research and practice points of view, and identify best practices and solutions that have been shown to work.", "num_citations": "1\n", "authors": ["1450"]}
{"title": "The influence of conceptual user models on the creation and interpretation of diagrams representing reactive systems\n", "abstract": " In system design, many diagrams of many different types are used. Diagrams communicate design aspects between members of the development team, and between these experts and the non-expert customers and future users. Mastering the creation of diagrams is often a challenging task, judging by particular errors persistently found in diagrams created by undergraduate computer science students. We assume a possible misalignment between human perception and cognition on the one hand and the diagrams \u2018structure and syntax on the other. This article presents the results of an investigation of such a misalignment. We focus on the deployment of so-called'conceptual user models'(mental models, created by users in their mind) at the creation of diagrams. We propose a taxonomy for mental mappings, used for categorization of representations. We describe an experiment where naive and novice subjects created one or several diagrams of a familiar task. We use our taxonomy for analysing these diagrams, both for the represented task structure and the symbols used. The results indeed show a mismatch between mental models and currently used diagram techniques.", "num_citations": "1\n", "authors": ["1450"]}
{"title": "Wat ICT-architecten moeten weten\n", "abstract": " Wat ICT-architecten moeten weten \u2014 University of Twente Research Information Skip to main navigation Skip to search Skip to main content University of Twente Research Information Logo Home Profiles Research Units Research Output Datasets Activities Prizes Press / Media Search by expertise, name or affiliation Wat ICT-architecten moeten weten Roelf J. Wieringa Research output: Contribution to journal \u203a Article \u203a Professional 571 Downloads (Pure) Overview Original language Undefined Pages (from-to) 17-17 Number of pages 1 Journal Automatisering gids Volume 40 Issue number 1 Publication status Published - Jan 2006 Keywords IR-65535 EWI-1637 METIS-237987 IS-ARCHITECTURE SCS-Services Access to Document AG-Competenties open AG-Competenties open Cite this APA Author BIBTEX Harvard Standard RIS Vancouver Wieringa, RJ (2006). Wat ICT-architecten moeten weten. Automatisering \u2026", "num_citations": "1\n", "authors": ["1450"]}
{"title": "Eliciting user requirements for ambient intelligent systems: a case study\n", "abstract": " Ambient intelligent (AmI) systems are electronic environments that are responsive and sensitive to the presence of people (Weiser, 1991). Eliciting requirements for AmI systems, like for any novel technology, is hard because of high uncertainties, such as: 1) both the users and use context are unknown; 2) there is no identified problem that needs to be solved (people cannot state in advance what they want); 3) there is no product idea; 4) it is unclear what future technology can do. There are currently no requirements engineering method for novel AmI technologies. In this short note, we present the current state of our research, which aims at defining a method for identifying requirements for AmI systems.", "num_citations": "1\n", "authors": ["1450"]}
{"title": "A Modelling Method for Embedded Systems\n", "abstract": " We suggest a systematic modelling method for embedded systems. The goal is to derive models (1) that share the relevant properties with the original system,(2) that are suitable for computer aided analysis, and (3) where the modelling process itself is transparent and efficient, which is necessary to detect modelling errors early and to produce model versions (eg for product families). Our aim is to find techniques to enhance the quality of the model and of the informal argument that it accurately represents the system. Our approach is to use joint decomposition of the system model and the correctness property, guided by the structure of the physical environment, following, eg, engineering blueprints. In this short note we describe our approch to combine Jackson\u2019s problem frame approach [1, 2] with a stepwise refinement method to arrive at provably correct designs of embedded systems.", "num_citations": "1\n", "authors": ["1450"]}
{"title": "A Classification of RE Papers: Are We Researching or Designing RE Techniques\n", "abstract": " Discussion of a paper in RE program committees is often complicated by lack of agreement about evaluation criteria to be applied to the paper. For some years now, successive program chairs have attempted to increase clarity by including a paper classification in their CFP, and making the evaluation criteria per paper class explicit. This short note presents a paper classification based on this experience. It can be used as guide by program chairs. It can also be used by authors as well as reviewers to understand what kind of paper they are writing or reviewing, and what criteria should be applied in evaluating the paper.", "num_citations": "1\n", "authors": ["1450"]}
{"title": "The impact of architectural decisions on quality attributes of enterprise information systems: a survey of the design space\n", "abstract": " Design of enterprise information systems is a problem-solving activity. A system architect, designer and programmer make numerous decisions about the structure and behaviour of the system on various levels. These decisions define the quality of the system under design (SuD) in all its aspects. An example of an application-level decision is whether to structure the domain logic according to a domain model, a table module or a transaction script. We want to investigate the effects of such decisions on quality attributes of software. This will allow us to make better software and to predict the quality of software before it is built. In this research, we try to empirically validate or reject hypotheses like:\u201cIn the majority of systems above 500 function points, systems with a domain model have better changeability than systems with a table module.\u201d If the validity of such hypotheses depend on the context of the system, we want to\u00a0\u2026", "num_citations": "1\n", "authors": ["1450"]}
{"title": "The Alignment Problem\n", "abstract": " The physical world is the world of computers, cables, printers, wireless access points, and in general anything that can be described using the basic measuring units of physics, Meters, Kilograms, Seconds, and Amperes. Software engineers often forget that the world of engineering is physical. Bridges, roads, buildings, steam engines, chemical processes, electrical networks, water supply systems, automobiles, airplanes, rockets, wind turbines, and all other machines and materials that engineers deal with are physical.", "num_citations": "1\n", "authors": ["1450"]}
{"title": "Toolkit for Conceptual Modeling (TCM) User\u2019s Guide and Reference\n", "abstract": " The Toolkit for Conceptual Modeling (TCM) is a collection of software tools to present conceptual models of software systems in the form of diagrams, trees and tables. A conceptual model of a system is a structure used to represent the behavior or decomposition of the system. TCM is meant to be used for requirements engineering, ie the activity of specifying and maintaining requirements for desired systems, in which a number of techniques and heuristics for problem analysis, function refinement, behavior specification, and decomposition specification are used. TCM contains editors for two major sets of software specification techniques: Structured Analysis (SA) and the Unified Modeling Language (UML). The first set includes amongst others editors for ER-diagrams, data and event flow diagrams and state-transition diagrams. The set of UML editors includes amongst others a class-diagram editor, a use-case diagram editor and an activity diagram editor. Furthermore, TCM contains three generic editors for generic diagrams, generic tables and generic trees and also a number of special purpose editors such as two editors for JSD and a process graph editor.The current version of TCM supports constraint checking for single documents (eg name duplication and cycles in is-a relationships). TCM distinguishes built-in constraints (of which a violation cannot even be attempted) from immediate constraints (of which an attempted violation is immediately prevented) and soft constraints (against which the editor provides a warning when it checks the drawing). TCM is planned to support constraint checking across documents. All editors have a similar\u00a0\u2026", "num_citations": "1\n", "authors": ["1450"]}
{"title": "The mutual exclusion problem in reasoning about action and change\n", "abstract": " Description: J. Broersen, J. Meyer, R. Wieringa;(2002) Non-Monotonic Reasoning (NMR);http://dx. doi. org/", "num_citations": "1\n", "authors": ["1450"]}
{"title": "Computers kunnen niet rechtspreken\n", "abstract": " Computers kunnen niet rechtspreken (2001) | www.narcis.nl KNAW KNAW Narcis Back to search results University of Twente Publication Computers kunnen niet rechtspreken (2001) Pagina-navigatie: Main Save publication Save as MODS Export to Mendeley Save as EndNote Export to RefWorks Title Computers kunnen niet rechtspreken Published in Automatisering gids. SDU. ISSN 0165-4683. Author Wieringa, Roelf J. Date issued 2001-05-04 Access Closed Access Reference(s) SCS-Services, EWI-10550, IR-61812 Language English Type Article Publisher SDU Publication https://research.utwente.nl/en/publications/computers-kunnen... OpenURL Search this publication in (your) library Persistent Identifier urn:nbn:nl:ui:28-61812 Metadata XML Source University of Twente Go to Website Navigation: Home about narcis login Nederlands contact Anna van Saksenlaan 51 2593 HW Den Haag narcis@dans.knaw.nl \u2026", "num_citations": "1\n", "authors": ["1450"]}
{"title": "Architectuur is systeem\n", "abstract": " De vraag of architectuur hetzelfde is als systeemontwerp is voor buitenstaanders wat moeilijk te volgen vanwege de vreemde betekenis die wij software engineers aan het begrip``ontwerp''toekennen. In de software engineering is een systeemontwerp de decompositie van de software in elementen die gezamelijk de software-functies realiseren. In alle andere ingenieurswetenschappen is ontwerpen het nemen van beslissingen over de gewenste eigenschappen van een product. De gewenste eigenschappen kunnen eigenschappen zijn van de uiterlijke vorm of de innerlijke struktuur van een systeem of zelfs over de materialen waarvan het gemaakt is. Het resultaat van de ontwerpactiviteit is een document die die gewenste eigenschappen vastlegt. Dat kan een blauwdruk zijn, of een bouwinstructie, een specificatie van onderdelen, etc.Dit bredere begrip van ontwerp omvat wat wij in de software engineering``requirements''noemen zowel als wat wij tegenwoordig vaak``architectuur''noemen. Kern van het bredere begrip dat ik hier voorstel, is dat ontwerpen probleemoplossen is. Elke ontwerpbeslissing is een beslissing over de eigenschappen die een product moet hebben om een probleem in de uiteindelijke gebruiksomgeving op te lossen. In nog algemenere termen houdt ontwerpen in dat we de slogan``bezint eer ge begint'serieus nemen. Een ontwerper bedenkt wat er gebouwd moet worden; een bouwer bouwt wat er ontworpen is.", "num_citations": "1\n", "authors": ["1450"]}
{"title": "Two Case Studies of Subsystem Design for General-Purpose CSCW Software Architectures\n", "abstract": " This paper discusses subsystem design guidelines for the software architecture of general-purpose computer supported cooperative work systems, ie, systems that are designed to be applicable in various application areas requiring explicit collaboration support. In our opinion, guidelines for subsystem level design are rarely given\u2013most guidelines currently given apply to the programming language level. We extract guidelines from a case study of the redesign and extension of an advanced commercial workflow management system and place them into the context of existing software engineering research. The guidelines are then validated against the design decisions made in the construction of a widely used web-based groupware system. Our approach is based on the well-known distinction between essential (logical) and physical architectures. We show how essential architecture design can be based on a direct mapping of abstract functional concepts as found in general-purpose systems to modules in the essential architecture. The essential architecture is next mapped to a physical architecture by applying software clustering and replication to achieve the required distribution and performance characteristics.", "num_citations": "1\n", "authors": ["1450"]}
{"title": "Analysing Atomic Dynamic UML Notions by Surfing through the UML Metamodel\n", "abstract": " . This paper analyses atomic notions in UML which are fundamental for the understanding of dynamic aspects. The notions considered are: Action, Event, Exception, Message, Method, Signal, Stimulus, Operation, and Reception. We surf through the UML metamodel by combining the different metamodel class diagrams, where these notions are defined, into a single class diagram. Thereby we point out the intent, similarities and differences between these notions. Thus before doing a formalization, we try to make the concepts a bit clearer than they appear in the UML Semantics [OMG99]. 1 Motivation!! UML?? 2000 Workshop Dynamic Behaviour in UML Models, Technical Report, Technical University of Munich, G. Reggio, A. Knapp, B. Rumpe, B. Selic, R. Wieringa (Eds.), 2000. The class diagram in Fig. 1 combines aspects of six different UML metamodel class diagrams and concentrates on the following notions: Action, Event, Exception, Message, Method, Signal, Stimulus, Operation, and Reception. I...", "num_citations": "1\n", "authors": ["1450"]}
{"title": "Ontologies and diagrams for software and systems engineering\n", "abstract": " Ontologies and diagrams for software and systems engineering \u2014 University of Twente Research Information Skip to main navigation Skip to search Skip to main content University of Twente Research Information Logo Home Profiles Research Units Research Output Datasets Activities Prizes Press / Media Search by expertise, name or affiliation Ontologies and diagrams for software and systems engineering Roelf J. Wieringa Research output: Contribution to conference \u203a Paper \u203a Academic \u203a peer-review 27 Downloads (Pure) Overview Original language Undefined Publication status Published - Jun 2000 Keywords SCS-Services EWI-10568 IR-64215 Access to Document t open t open Cite this APA Author BIBTEX Harvard Standard RIS Vancouver Wieringa, RJ (2000). Ontologies and diagrams for software and systems engineering. Wieringa, Roelf J. / Ontologies and diagrams for software and systems engineering. \u2026", "num_citations": "1\n", "authors": ["1450"]}
{"title": "Overheid moet stabiliteit opleiding garanderen\n", "abstract": " Vooral de eerste generatie opleiders had geen duidelijk beeld, zodat de informatica-opleiding vooral ingevuld werd vanuit de eigen achtergrond en die ligt voor de meerderheid van hen in de wiskunde of logica. Gevoegd bij het feit dat de meeste informatica-opleidingen zijn ontstaan als afstudeerspecialisatie van de opleiding wiskunde, verklaart dit de formele aard die de meeste informatica-opleidingen tot voor kort hadden. Dit probleem is met het verstrijken van de tijd kleiner geworden, omdat een jongere generatie opleiders zelf een informatica-opleiding heeft gevolgd en onder meer via stages en contractonderzoek goed op de hoogte is van de beroepspraktijk van de informaticus. Ook bestaat er de laatste jaren vanuit de politiek (via onderwijsvisitatie) en het bedrijfsleven (\u2019Keep IT Simple: Welke instroom-profielen vraagt de IT branche?\u2019, Fenit, 1997) een druk om de opleidingen een maatschappelijk relevante inhoud te geven. Dit heeft reeds tot een meer praktijkgerichte ori\u00ebntatie in de informatica-opleidingen geleid. Zoals bij alle onderwijsverbeteringen is dit een continu proces. Er is op dit moment bij de Nederlandse universiteiten geen reden om met dit verbeteringsproces te stoppen en maar eens met iets nieuws te beginnen. Bovendien wordt het gesignaleerde probleem niet opgelost door het starten van een opleiding software engineering (SE). Als deze opleiding door huidige informatica-opleiders gegeven zal worden is het probleem van het wereldvreemde beeld dat sommige opleiders van de beroepspraktijk hebben, niet opgelost. Een betere oplossing voor dat probleem zou zijn de docenten voor enige tijd in het bedrijfsleven te\u00a0\u2026", "num_citations": "1\n", "authors": ["1450"]}
{"title": "De onzichtbare wereld van de informaticus: Conceptuele modellen en virtuele objecten\n", "abstract": " De onzichtbare wereld van de informaticus: Conceptuele modellen en virtuele objecten \u2014 University of Twente Research Information Skip to main navigation Skip to search Skip to main content University of Twente Research Information Logo Home Profiles Research Units Research Output Datasets Activities Prizes Press / Media Search by expertise, name or affiliation De onzichtbare wereld van de informaticus: Conceptuele modellen en virtuele objecten Roelf J. Wieringa Research output: Book/Report \u203a Inaugural speech \u203a Other research output 22 Downloads (Pure) Overview Original language Dutch Place of Publication Enschede Publisher Universiteit Twente Number of pages 43 Publication status Published - 1998 Keywords SCS-Services EWI-16181 IR-68168 Access to Document oratie open Final published version, 357 KB Cite this APA Author BIBTEX Harvard Standard RIS Vancouver Wieringa, RJ (1998). \u2026", "num_citations": "1\n", "authors": ["1450"]}
{"title": "Integration of formal and informal techniques for requirements engineering\n", "abstract": " Goal of the research reported here is to develop an integrated toolbox for writing and validating a speci cation of required system behavior. The toolbox is to consist of methods, techniques and heuristics in the rst place; eventually, some of this may be implemented in a software tool. The toolbox is intended to help the analyst bridge the gap between informal and formal requirements. The analyst must bridge this gap in two directions: from informal requirements to formal requirements (called requirements elicitation) and back again (called requirements validation).The kind of systems for which requirements are sought is, rst of all, the class of information systems. Recent developments in practice and in research have however led to the dissolution of boundaries between information systems, systems for electronic data interchange (EDI) among organizations, and control systems (eg embedded real-time systems). Another boundary that tends to be dissolved is that between knowledge-based systems and these other groups of systems. Whatever the exact nature of the boundaries between these systems, note that they all are systems, and that, therefore, all general principles of system development pronounced here should apply to all of them them.", "num_citations": "1\n", "authors": ["1450"]}
{"title": "Built-in value type specifications in LCM 3.1\n", "abstract": " This report is an attempt to establish an (algebraic) speci cation of a number of frequently used datatypes. The datatypes that are speci ed are LCM 3.1's built-in value types, and they are speci ed using LCM itself. The resulting speci cations are annotated with hopefully clarifying comments, with the purpose of adding educational value.", "num_citations": "1\n", "authors": ["1450"]}
{"title": "Comparison of the Notations Used in the Shlaer/Mellor Method and in TCM\n", "abstract": " This report compares two notation systems for requirements modeling, the notations used in the Shlaer-Mellor method for object-oriented analysis and the notations uses in TCM (Toolkit for Conceptual Modeling). The notations used in the Shlaer-Mellor method are semi-formal, ie they consist of diagrams annotated by natural language text. The notations used in TCM are semi-formal, as in the Shlaer-Mellor notation, but there is also a formal part. The formal part of a TCM speci cation is written down in LCM (Language for Conceptual Modeling), a language based on order-sorted dynamic logic. The formal and semi-formal parts of a TCM speci cation supplement each other and each can be used without using the other. Because the semi-formal and formal notations in TCM are precisely related, the semi-formal notations have unambiguous de nitions, and the formal notations have simple and clear diagram representations. The report analyzes the notations used for information model, state model, process model and communication model used in the Shlaer-Mellor method and shows how these relate to the notations used for the class model, life cycle model and communication model in TCM. It is shown that the notations of the Shlaer-Mellor method contain ambiguities and redundancies, that are resolved when we transform these notations into TCM notations. However, some of these redundancies provide useful information and TCM is extended with a simpli ed form of some of the Shlaer-Mellor notations. The report repeatedly refers to gures from 39]. For copyright reasons, these gures are not reproduced here. The reader is expected to have a\u00a0\u2026", "num_citations": "1\n", "authors": ["1450"]}
{"title": "Een analyse van methodes voor informatiesysteemontwikkeling\n", "abstract": " Er is in de afgelopen decenia een veelheid van methodes voor informatiesysteemontwikkeling voorgesteld en een aantal daarvan wordt in de praktijk gebruikt. Dit is op zichzelf geen probleem, ware het niet dat er nog weinig inzicht bestaat in de onderliggende struktuur van al deze methodes, zodat een analytisch raamwerk, waarbinnen methodes vergeleken en geevalueerd kunnen worden, ontbreekt. Dit is een probleem voor zowel het onderwijs in methodes, de theorievorming over methodes, als de praktijk van systeemontwikkeling. In dit artikel wordt een analytisch raamwerk voor methodes voor informatiesysteemontwikkeling gegeven, dat uit een klein aantal algemeen toepasbare principes afgeleidt wordt. Dit raamwerk wordt gebruikt om een drietal groepen methodes te identificeren, methodes voor ontwikkeling, modellering en projectmanagement. Scheiding van deze methodes is nuttig voor de theorievorming rond methodes, voor de praktijk waarin methodes toegepast worden en voor het onderwijs in methodes. Het artikel besluit met enige opmerkingen over het gebruik van het gepresenteerde analytische kader voor de evaluatie van methodes, voor method engineering en voor de identificatie van verschillende soorten functies in de automatisering.", "num_citations": "1\n", "authors": ["1450"]}
{"title": "Generalization and specialization of object dynamics\n", "abstract": " This report presents a quite detailed analysis of the modeling approaches of different object-oriented database systems, namely ABSURD, OBLOG, MOKUM, TAXIS and GAL/LEO, with emphasis on the spuialization of object dynamics, in a taxonomic structure. It is done unifonnly l7y applying each system to the specification of an UoD example, the University world. The potentialities of the systems are compared and ambiguities discovered. An auempt to resolve some of these ambiguities is made. The issues dealing with inheritance and taxonomy of the dynamics of an UoD specification are particularly treated. The formaliza tion of specialization issues for events and processes, which is specially cared in the systems ABSURD and OBLOG, is described in detail.", "num_citations": "1\n", "authors": ["1450"]}
{"title": "Role change in database domains\n", "abstract": " In data modelling the universe of discourse (UoD) is divided up into classes having a taxonomic structure which is intended to express some of the structure inherent in the UoD. Some of these classes, for example the class of persons or departments, may be called\" natural kinds,\" in that they are a fixed set of possible objects, existing in some possible state of the UoD, and all of which have a similar structure and behavior. Others have a more dynamic nature, such as the class of students. Whereas an object is created as as person and, when it ceases to be a person, ceases to exist, an object may come to be a student and cease to be one without coming into existence or passing away. A class like persons is a natural kind, and a class like students will be called roles in this report. This report studies the formal definition of roles and the resulting taxonomy of natural kinds and roles.", "num_citations": "1\n", "authors": ["1450"]}
{"title": "Machine Intelligence and Explication\n", "abstract": " This report is an MA (\" doctoraal\") thesis submitted to the department of philosophy, university of Amsterdam. It attempts to answer the question whether machines can think by conceptual analysis. Ideally, a conceptual analysis should give plausible explications of the concepts of\" machine\" and\" intelligence\" and then investigate the intersection of the sets of entities defined b; these explications. If the intersection is empty and the a priori argument is correct (or plausible), then empirical research into machine intelligence will (plausibly) not result in an intelligent machine. On the other hand, if conceptual analysis cannot show the intersection to be empty, it remains an empirical (or rather, technical) question whether such machines can actually be constructed.Such a neat argument cannot be produced, however, due to the vagueness of the concept of intelligence. It is quite possible to provide a rather uncontroversial explication of the concept of machine. Existing controversy about the possibility of machine intelligence is about the nature of intelligence, not about the nature of machines. Indeed, if intelligence could be unambiguously defined, we could (in principle; build a machine to implement it. Those who believe that intelligence cannot be realized in a machine, cannot base their arguments on an explicit and uncontroversial analysis of the concept of intelligence.", "num_citations": "1\n", "authors": ["1450"]}
{"title": "The place of expert systems in a typology of information systems\n", "abstract": " This article considers definitions and claims of Expert Systems (ES) and analyzes them in view of traditional Information systems (IS). It is argued that the valid specifications for ES do not differ fran those for IS. Consequently the theoretical study and the practical development of ES should not be a monodiscipline. Integration of ES development in classical mathematics and computer science opens the door to existing knowledge and experience. Aspects of existing ES are reviewed from this interdisciplinary point of view.", "num_citations": "1\n", "authors": ["1450"]}
{"title": "Fenomenologische opmerkingen over metingen van hersenmechanismes van visuele aandacht\n", "abstract": " Fenomenologische opmerkingen over metingen van hersenmechanismes van visuele aandacht Staff Publications Staff Publications external user (warning warning ) Log in as language uk About 'Staff publications' is the digital repository of Wageningen University & Research 'Staff publications' contains references to publications authored by Wageningen University staff from 1976 onward. Publications authored by the staff of the Research Institutes are available from 1995 onwards. Full text documents are added when available. The database is updated daily and currently holds about 240,000 items, of which 72,000 in open access. We have a manual that explains all the features Home Library My Library Staff Publications Wageningen Universit... Browse record nr. 78494 Record number 78494 Title Fenomenologische opmerkingen over metingen van hersenmechanismes van visuele aandacht Author(s) Wieringa, \u2026", "num_citations": "1\n", "authors": ["1450"]}