{"title": "Service-oriented architectures testing: A survey\n", "abstract": " Testing of Service Oriented Architectures (SOA) plays a critical role in ensuring a successful deployment in any enterprise. SOA testing must span several levels, from individual services to inter-enterprise federations of systems, and must cover functional and non-functional aspects.               SOA unique combination of features, such as run-time discovery of services, ultra-late binding, QoS aware composition, and SLA automated negotiation, challenge many existing testing techniques. As an example, run-time discovery and ultra-late binding entail that the actual configuration of a system is known only during the execution, and this makes many existing integration testing techniques inadequate. Similarly, QoS aware composition and SLA automated negotiation means that a service may deliver with different performances in different contexts, thus making most existing performance testing techniques to fail\u00a0\u2026", "num_citations": "299\n", "authors": ["103"]}
{"title": "Testing services and service-centric systems: Challenges and opportunities\n", "abstract": " This paper provides users and system integrators with an overview of service-oriented architecture (SOA) testing's fundamental technical issues and solutions, focusing on Web services as a practical implementation of the SOA model. The paper discusses SOA testing across two dimensions: testing perspectives, wherein various stakeholders have different needs and raise different testing requirements; and testing level, wherein each SOA testing level poses unique challenges", "num_citations": "297\n", "authors": ["103"]}
{"title": "New frontiers of reverse engineering\n", "abstract": " Comprehending and modifying software is at the heart of many software engineering tasks, and this explains the growing interest that software reverse engineering has gained in the last 20 years. Broadly speaking, reverse engineering is the process of analyzing a subject system to create representations of the system at a higher level of abstraction. This paper briefly presents an overview of the field of reverse engineering, reviews main achievements and areas of application, and highlights key open research issues for the future.", "num_citations": "229\n", "authors": ["103"]}
{"title": "Analyzing cloning evolution in the linux kernel\n", "abstract": " Identifying code duplication in large multi-platform software systems is a challenging problem. This is due to a variety of reasons including the presence of high-level programming languages and structures interleaved with hardware-dependent low-level resources and assembler code, the use of GUI-based configuration scripts generating commands to compile the system, and the extremely high number of possible different configurations.This paper studies the extent and the evolution of code duplications in the Linux kernel. Linux is a large, multi-platform software system; it is based on the Open Source concept, and so there are no obstacles in discussing its implementation. In addition, it is decidedly too large to be examined manually: the current Linux kernel release (2.4.18) is about three million LOCs.Nineteen releases, from 2.4.0 to 2.4.18, were processed and analyzed, identifying code duplication among\u00a0\u2026", "num_citations": "186\n", "authors": ["103"]}
{"title": "Search-based techniques applied to optimization of project planning for a massive maintenance project\n", "abstract": " This paper evaluates the use of three different search-based techniques, namely genetic algorithms, hill climbing and simulated annealing, and two problem representations, for planning resource allocation in large massive maintenance projects. In particular, the search-based approach aims to find an optimal or near optimal order in which to allocate work packages to programming teams, in order to minimize the project duration. The approach is validated by an empirical study of a large, commercial Y2K massive maintenance project, which compares these techniques with each other and with a random search (to provide base line comparison data). Results show that an ordering-based genome encoding (with tailored cross over operator) and the genetic algorithm appear to provide the most robust solution, though the hill climbing approach also performs well. The best search technique results reduce the project\u00a0\u2026", "num_citations": "141\n", "authors": ["103"]}
{"title": "An approach to support web service classification and annotation\n", "abstract": " The need for supporting the classification and semantic annotation of services constitutes an important challenge for service-centric software engineering. Late-binding and, in general, service matching approaches, require services to be semantically annotated. Such a semantic annotation may require, in turn, to be made in agreement to a specific ontology. Also, a service description needs to properly relate with other similar services. This paper proposes an approach to i) automatically classify services to specific domains and ii) identify key concepts inside service textual documentation, and builds a lattice of relationships between service annotations. Support vector machines and formal concept analysis have been used to perform the two tasks. Results obtained classifying a set of Web services show that the approach can provide useful insights in both service publication and service retrieval phases.", "num_citations": "127\n", "authors": ["103"]}
{"title": "Object-oriented design patterns recovery\n", "abstract": " Object-Oriented (OO) design patterns are an emergent technology: they are reusable micro-architectures, high-level building blocks. A system which has been designed using well-known, documented and accepted design patterns is also likely to exhibit good properties such as modularity, separation of concerns and maintainability. While for forward engineering the benefits of using design patterns are clear, using reverse engineering technologies to discover instances of patterns in a software artifact (e.g., design or code) may help in several key areas, among which are program understanding, design-to-code traceability and quality assessment. This paper describes a conservative approach and experimental results, based on a multi-stage reduction strategy using OO software metrics and structural properties to extract structural design patterns from OO design or C++ code. To assess the effectiveness of the\u00a0\u2026", "num_citations": "126\n", "authors": ["103"]}
{"title": "An automatic approach to identify class evolution discontinuities\n", "abstract": " When a software system evolves, features are added, removed and changed. Moreover, refactoring activities are periodically performed to improve the software internal structure. A class may be replaced by another, two classes can be merged, or a class may be split in two others. As a consequence, it may not be possible to trace software features between a release and another. When studying software evolution, we should be able to trace a class lifetime even when it disappears because it is replaced by a similar one, split or merged. Such a capability is also essential to perform impact analysis. This work proposes an automatic approach, inspired on vector space information retrieval, to identify class evolution discontinuities and, therefore, cases of possible refactoring. The approach has been applied to identify refactorings performed over 40 releases of a Java open source domain name server. Almost all the\u00a0\u2026", "num_citations": "122\n", "authors": ["103"]}
{"title": "TransientMeter: A distributed measurement system for power quality monitoring\n", "abstract": " This paper describes the design and the implementation of TransientMeter, a monitoring system for the detection, classification, and measurement of transient disturbances on electrical power systems. TransientMeter relies on a common object request broker architecture (CORBA) as a communication interface, wavelet-based methods for automatic signal classification and characterization, and on a smart trigger circuit for disturbance detection. The system has been successfully applied to detect, classify, and measure disturbances in an industrial environment.", "num_citations": "103\n", "authors": ["103"]}
{"title": "Mining source code descriptions from developer communications\n", "abstract": " Very often, source code lacks comments that adequately describe its behavior. In such situations developers need to infer knowledge from the source code itself or to search for source code descriptions in external artifacts. We argue that messages exchanged among contributors/developers, in the form of bug reports and emails, are a useful source of information to help understanding source code. However, such communications are unstructured and usually not explicitly meant to describe specific parts of the source code. Developers searching for code descriptions within communications face the challenge of filtering large amount of data to extract what pieces of information are important to them. We propose an approach to automatically extract method descriptions from communications in bug tracking systems and mailing lists. We have evaluated the approach on bug reports and mailing lists from two open\u00a0\u2026", "num_citations": "101\n", "authors": ["103"]}
{"title": "Negotiation of service level agreements: An architecture and a search-based approach\n", "abstract": " Software systems built by composing existing services are more and more capturing the interest of researchers and practitioners. The envisaged long term scenario is that services, offered by some competing providers, are chosen by some consumers and used for their own purpose, possibly, in conjunction with other services. In the case the consumer is not anymore satisfied by the performance of some service, he can try to replace it with some other service. This implies the creation of a global market of services and poses new requirements concerning validation of exploited services, security of transactions engaged with services, trustworthiness, creation and negotiation of Service Level Agreements with these services. In this paper we focus on the last aspect and present our approach for negotiation of Service Level Agreements. Our architecture supports the actuation of various negotiation processes\u00a0\u2026", "num_citations": "100\n", "authors": ["103"]}
{"title": "Modeling clones evolution through time series\n", "abstract": " The actual effort to evolve and maintain a software system is likely to vary depending on the amount of clones (i.e., duplicated or slightly different code fragments) present in the system. This paper presents a method for monitoring and predicting clones evolution across subsequent versions of a software system. Clones are firstly identified using a metric-based approach, then they are modeled in terms of time series identifying a predictive model. The proposed method has been validated with an experimental activity performed on 27 subsequent versions of mSQL, a medium-size software system written in C. The time span period of the analyzed mSQL releases covers four years, from May 1995 (mSQL 1.0.6) to May 1999 (mSQL 2. 0. 10). For any given software release, the identified models was able to predict the clone percentage of the subsequent release with an average error below 4 %. A higher prediction error\u00a0\u2026", "num_citations": "99\n", "authors": ["103"]}
{"title": "Search-based testing of service level agreements\n", "abstract": " The diffusion of service oriented architectures introduces the need for novel testing approaches. On the one side, testing must be able to identify failures in the functionality provided by service. On the other side, it needs to identify cases in which the Service Level Agreement (SLA) negotiated between the service provider and the service consumer is not met. This would allow the developer to improve service performances, where needed, and the provider to avoid promising Quality of Service (QoS) levels that cannot be guaranteed. This paper proposes the use of Genetic Algorithms to generate inputs and configurations for service-oriented systems that cause SLA violations. The approach has been implemented in a tool and applied to an audio processing workflow and to a service for chart generation. In both cases, the approach was able to produce test data able to violate some QoS constraints.", "num_citations": "97\n", "authors": ["103"]}
{"title": "Speaking a common language: A conceptual model for describing service-oriented systems\n", "abstract": " The diffusion of service-oriented computing is today heavily influencing many software development and research activities. Despite this, service-oriented computing is a relatively new field, where many aspects still suffer from a lack of standardization. Also, the service-oriented approach is bringing together researchers from different communities or from organizations having developed their own solutions. This introduces the need for letting all these people communicate with each other using a common language and a common understanding of the technologies they are using or building.               This paper proposes a conceptual model that describes actors, activities and entities involved in a service-oriented scenario and the relationships between them. While being created for a European project, the model is easily adaptable to address the needs of any other service-oriented initiative.", "num_citations": "86\n", "authors": ["103"]}
{"title": "An empirical comparison of methods to support QoS-aware service selection\n", "abstract": " Run-time binding is an important and useful feature of Service Oriented Architectures (SOA), which aims at selecting, among functionally equivalent services, the ones that optimize some QoS objective of the overall application. To this aim, it is particularly relevant to forecast the QoS a service will likely exhibit in future invocations.", "num_citations": "84\n", "authors": ["103"]}
{"title": "Using test cases as contract to ensure service compliance across releases\n", "abstract": " Web Services are entailing a major shift of perspective in software engineering: software is used and not owned, and operation happens on machines that are out of the user control. This means that the user cannot decide the strategy to migrate to a new version of a service, as it happens with COTS. Therefore, a key issue is to provide users with means to build confidence that a service i) delivers over the time the desired function and ii) at the same time it is able to meet Quality of Service requirements.               This paper proposes the use of test cases as a form of contract between the provider and the users of a service, and describes an approach and a tool to allow users running a test suite against a service, to discover if functional or non-functional expectations are maintained over the time. The approach has been evaluated by applying it to two case studies.", "num_citations": "81\n", "authors": ["103"]}
{"title": "A robust search-based approach to project management in the presence of abandonment, rework, error and uncertainty\n", "abstract": " Managing a large software project involves initial estimates that may turn out to be erroneous or that might be expressed with some degree of uncertainty. Furthermore, as the project progresses, it often becomes necessary to rework some of the work packages that make up the overall project. Other work packages might have to be abandoned for a variety of reasons. In the presence of these difficulties, optimal allocation of staff to project teams and teams to work packages is far from trivial. This paper shows how genetic algorithms can be combined with a queuing simulation model to address these problems in a robust manner. A tandem genetic algorithm is used to search for the best sequence in which to process work packages and the best allocation of staff to project teams. The simulation model, that computes the project estimated completion date, guides the search. The possible impact of rework, abandonment\u00a0\u2026", "num_citations": "79\n", "authors": ["103"]}
{"title": "The use of search\u2010based optimization techniques to schedule and staff software projects: an approach and an empirical study\n", "abstract": " Allocating resources to a software project and assigning tasks to teams constitute crucial activities that affect project cost and completion time. Finding a solution for such a problem is NP\u2010hard; this requires managers to be supported by proper tools for performing such an allocation. This paper shows how search\u2010based optimization techniques can be combined with a queuing simulation model to address these problems. The obtained staff and task allocations aim to minimize the completion time and reduce schedule fragmentation. The proposed approach allows project managers to run multiple simulations, compare results and consider trade\u2010offs between increasing the staffing level and anticipating the project completion date and between reducing the fragmentation and accepting project delays. The paper presents results from the application of the proposed search\u2010based project planning approach to data\u00a0\u2026", "num_citations": "73\n", "authors": ["103"]}
{"title": "Understanding web applications through dynamic analysis\n", "abstract": " The relevance and pervasiveness of Web applications as a vital part of modern enterprise systems has significantly increased in recent years. However, the lack of adequate documentation promotes the need for reverse engineering tools aiming at supporting Web application maintenance and evolution tasks. A nontrivial Web application is a complex artifact integrating technologies such as scripting languages, middleware, Web services, data warehouses and databases. The task to recover abstractions requires the adoption of dynamic analyses to complement the information gathered with static analyses. This paper presents an approach and a tool, named WANDA, that instruments Web applications and combines static and dynamic information to recover the as-is architecture and, in general, the UML documentation of the application itself. To this aim we propose an extension of the Conallen UML diagrams to\u00a0\u2026", "num_citations": "73\n", "authors": ["103"]}
{"title": "Improving network applications security: a new heuristic to generate stress testing data\n", "abstract": " Buffer overflows cause serious problems in different categories of software systems. For example, if present in network or security applications, they can be exploited to gain unauthorized grant or access to the system. In embedded systems, such as avionics or automotive systems, they can be the cause of serious accidents. This paper proposes to combine static analysis and program slicing with evolutionary testing, to detect buffer overflow threats. Static analysis identifies vulnerable statements, while slicing and data dependency analysis identify the relationship between these statements and program or function inputs, thus reducing the search space. To guide the search towards discovering buffer overflow in this work we define three multi-objective fitness functions and compare them on two open-source systems. These functions account for terms such as the statement coverage, the coverage of vulnerable\u00a0\u2026", "num_citations": "70\n", "authors": ["103"]}
{"title": "SOA: Testing and Self-checking\n", "abstract": " The dynamic nature of service-oriented architectures poses new challenges to system validation. Traditional testing is unable to cope with certain aspects of a service\u2013oriented system validation, essentially because of the impossibility to test all (often unforeseen) system\u2019s configurations. On the other hand, run\u2013time monitoring, while able to deal with the intrinsic dynamism and adaptiveness of a service\u2013oriented system, are unable to provide confidence that a system will behave correctly before it is actually deployed.", "num_citations": "68\n", "authors": ["103"]}
{"title": "Codes: Mining source code descriptions from developers discussions\n", "abstract": " Program comprehension is a crucial activity, preliminary to any software maintenance task. Such an activity can be difficult when the source code is not adequately documented, or the documentation is outdated. Differently from the many existing software re-documentation approaches, based on different kinds of code analysis, this paper describes CODES (mining sourCe cOde Descriptions from developErs diScussions), a tool which applies a\" social''approach to software re-documentation. Specifically, CODES extracts candidate method documentation from StackOverflow discussions, and creates Javadoc descriptions from it. We evaluated CODES to mine Lucene and Hibernate method descriptions. The results indicate that CODES is able to extract descriptions for 20% and 28% of the Lucene and Hibernate methods with a precision of 84% and 91% respectively.", "num_citations": "63\n", "authors": ["103"]}
{"title": "Designing your next empirical study on program comprehension\n", "abstract": " The field of program comprehension is characterized by both the continuing development of new tools and techniques and the adaptation of existing techniques to address program comprehension needs for new software development and maintenance scenarios. The adoption of these techniques and tools in industry requires proper experimentation to assess the advantages and disadvantages of each technique or tool and to let the practitioners choose the most suitable approach for a specific problem. The objective of this working session is to encourage researchers and practitioners working in the area of program comprehension to join forces to design and carry out studies related to program comprehension, including observational studies, controlled experiments, case studies, surveys, and contests, and to develop standards for describing and carrying out such studies in a way that facilitates replication of\u00a0\u2026", "num_citations": "62\n", "authors": ["103"]}
{"title": "Cooperative co-evolutionary optimization of software project staff assignments and job scheduling\n", "abstract": " This paper presents an approach to Search Based Software Project Management based on Cooperative Co-evolution. Our approach aims to optimize both developers\u2019 team staffing and work package scheduling through cooperative co-evolution to achieve early overall completion time. To evaluate our approach, we conducted an empirical study, using data from four real-world software projects. Results indicate that the Co-evolutionary approach significantly outperforms a single population evolutionary algorithm. Cooperative co-evolution has not previously been applied to any problem in Search Based Software Engineering (SBSE), so this paper reports the first application of cooperative co-evolution in the SBSE literature. We believe that co-evolutionary optimization may fit many applications in other SBSE problem domains, since software systems often have complex inter-related subsystems and are\u00a0\u2026", "num_citations": "61\n", "authors": ["103"]}
{"title": "Web services regression testing\n", "abstract": " Service-oriented Architectures (SOA) introduce a major shift of perspective in software engineering: in contrast to components, services are used instead of being physically integrated. This leaves the user with no control over changes that can happen in the service itself. When the service evolves, the user may not be aware of the changes, and this can entail unexpected system failures.             When a system integrator discovers a service and starts to use it, she/he may need to periodically re-test it to build confidence that (i) the service delivers over the time the desired functionality and (ii) at the same time it is able to meet Quality of Service requirements. Test cases can be used as a form of contract between a provider and the system integrators. This chapter describes an approach and a tool to allow users to run a test suite against a service to discover if functional and non-functional expectations are\u00a0\u2026", "num_citations": "55\n", "authors": ["103"]}
{"title": "The effect of communication overhead on software maintenance project staffing: a search-based approach\n", "abstract": " Brooks' milestone 'Mythical Man Month' established the observation that there is no simple conversion between people and time in large scale software projects. Communication and training overheads yield a subtle and variable relationship between the person-months required for a project and the number of people needed to complete the task within a given timeframe. This paper formalises several instantiations of Brooks' law and uses these to construct project schedule and staffing instances \u2014 using a search-based project staffing and scheduling approach \u2014 on data from two large real world maintenance projects. The results reveal the impact of different formulations of Brooks' law on project completion time and on staff distribution across teams, and the influence of other factors such as the presence of dependencies between work packages on the effect of communication overhead.", "num_citations": "50\n", "authors": ["103"]}
{"title": "Focus: A recommender system for mining api function calls and usage patterns\n", "abstract": " Software developers interact with APIs on a daily basis and, therefore, often face the need to learn how to use new APIs suitable for their purposes. Previous work has shown that recommending usage patterns to developers facilitates the learning process. Current approaches to usage pattern recommendation, however, still suffer from high redundancy and poor run-time performance. In this paper, we reformulate the problem of usage pattern recommendation in terms of a collaborative-filtering recommender system. We present a new tool, FOCUS, which mines open-source project repositories to recommend API method invocations and usage patterns by analyzing how APIs are used in projects similar to the current project. We evaluate FOCUS on a large number of Java projects extracted from GitHub and Maven Central and find that it outperforms the state-of-the-art approach PAM with regards to success rate\u00a0\u2026", "num_citations": "42\n", "authors": ["103"]}
{"title": "An approach for mining services in database oriented applications\n", "abstract": " The diffusion of service oriented architectures is slowed down by the lack of enough services available for satisfying service integrator needs. Nevertheless, many features desired by service integrators have already been implemented in existing software systems. To this aim, approaches able to identify potential services into a legacy system source code are highly desirable. This paper proposes an approach to identify, from database-oriented applications, pieces of functionality to be potentially exported as services. The identification is performed by clustering, through formal concept analysis, queries dynamically extracted by observing interactions between the application and the database. The approach has been assessed by identifying potential services in two Java software systems", "num_citations": "41\n", "authors": ["103"]}
{"title": "Using concept lattices to support service selection\n", "abstract": " When building a service-oriented system, a service integrator retrieves a set of potentially useful services from registries and then inspects their documentation to eventually decide which services to use. This task needs to be supported by proper tools that help service interface/documentation understanding, highlighting the relationships existing between the retrieved services. This paper proposes an approach, based on Formal Concept Analysis, to understand relationships between services, as well as between operations of a complex service, by analyzing service interfaces and documentation. The approach allows an analyst to cluster similar services, highlights hierarchical relationships and, in general, commonalities and differences between services. To support the proposed approach, we developed a tool that provides several service browsing capabilities. Finally, the approach has been evaluated with\u00a0\u2026", "num_citations": "40\n", "authors": ["103"]}
{"title": "Trend analysis and issue prediction in large-scale open source systems\n", "abstract": " Effort to evolve and maintain a software system is likely to vary depending on the amount and frequency of change requests. This paper proposes to model change requests as time series and to rely on time series mathematical framework to analyze and model them. In particular, this paper focuses on the number of new change requests per KLOC and per unit of time. Time series can have a two-fold application: they can be used to forecast future values and to identify trends. Increasing trends can indicate an increase in customer requests for new features or a decrease in the software system quality. A decreasing trend can indicate application stability and maturity, but also a reduced popularity and adoption. The paper reports case studies over about five years for three large open source applications: Eclipse, Mozilla and JBoss. The case studies show the capability of time series to model change request density\u00a0\u2026", "num_citations": "39\n", "authors": ["103"]}
{"title": "Search-based techniques for optimizing software project resource allocation\n", "abstract": " In software development, testing and maintenance, as in other large scale engineering activities, effective project planning is essential. Failure to plan and/or poor planning can cause delays and costs that, given timing and budget constraints, are often unacceptable, leading to business\u2013critical failures. Traditional tools such as the Project Evaluation and Review Technique (PERT), the Critical Path Method (CPM), Gantt diagrams and Earned Value Analysis help to plan and track project milestones. While these tools and techniques are important, they cannot assist with the identification of optimal scheduling assignment in the presence of configurable resource allocation. However, most large scale software projects involve several teams of programmers and many individual project work packages. As such, the optimal allocation of teams of programmers (the primary resource cost drivers) to Work Packages\u00a0\u2026", "num_citations": "38\n", "authors": ["103"]}
{"title": "Estimating the number of remaining links in traceability recovery\n", "abstract": " Although very important in software engineering, establishing traceability links between software artifacts is extremely tedious, error-prone, and it requires significant effort. Even when approaches for automated traceability recovery exist, these provide the requirements analyst with a, usually very long, ranked list of candidate links that needs to be manually inspected. In this paper we introduce an approach called Estimation of the Number of Remaining Links (ENRL) which aims at estimating, via Machine Learning (ML) classifiers, the number of remaining positive links in a ranked list of candidate traceability links produced by a Natural Language Processing techniques-based recovery approach. We have evaluated the accuracy of the ENRL approach by considering several ML classifiers and NLP techniques on three datasets from industry and academia, and concerning traceability links among different\u00a0\u2026", "num_citations": "33\n", "authors": ["103"]}
{"title": "An approach for search based testing of null pointer exceptions\n", "abstract": " Uncaught exceptions, and in particular null pointer exceptions (NPEs), constitute a major cause of crashes for software systems. Although tools for the static identification of potential NPEs exist, there is need for proper approaches able to identify system execution scenarios causing NPEs. This paper proposes a search-based test data generation approach aimed at automatically identify NPEs. The approach consists of two steps: (i) an inter-procedural data and control flow analysis - relying on existing technology - that identifies paths between input parameters and potential NPEs, and (ii) a genetic algorithm that evolves a population of test data with the aim of covering such paths. The algorithm is able to deal with complex inputs containing arbitrary data structures. The approach has been evaluated on to test class clusters from six Java open source systems, where NPE bugs have been artificially introduced\u00a0\u2026", "num_citations": "29\n", "authors": ["103"]}
{"title": "Visualizing the evolution of web services using formal concept analysis\n", "abstract": " The service-oriented paradigm constitutes a promising technology that allows many software systems to benefit of interesting mechanisms such as late binding and automatic discovery. From a service integrator's perspective, it is relevant to understand service evolution, to assess which could be its impact on his/her own system or, eventually, to change the bindings between the system and the services. Given the lack of source code availability, this task is, however, limited to understand how service interfaces evolve. We propose an approach, based on formal concept analysis, to understand how relationships between sets of services change across service evolution. The concept lattice is able to highlight hierarchy relationships and, in general, to identify commonalities and differences between services. Examples built upon real sets of services show the feasibility of the proposed approach.", "num_citations": "28\n", "authors": ["103"]}
{"title": "CrossRec: Supporting software developers by recommending third-party libraries\n", "abstract": " When creating a new software system, or when evolving an existing one, developers do not reinvent the wheel but, rather, seek available libraries that suit their purpose. In such a context, open source software repositories contain rich resources that can provide developers with helpful advice to support their tasks. However, the heterogeneity of resources and the dependencies among them are the main obstacles to the effective mining and exploitation of the available data. In this sense, advanced techniques and tools are needed to mine the metadata to bring in meaningful recommendations. In this paper, we present CrossRec, a recommender system to assist open source software developers in selecting suitable third-party libraries. CrossRec exploits a collaborative filtering technique to recommend libraries to developers by relying on the set of dependencies, which are currently included in the project being\u00a0\u2026", "num_citations": "27\n", "authors": ["103"]}
{"title": "XOgastan: XML-oriented gcc AST analysis and transformations\n", "abstract": " Software maintenance, program analysis and transformation tools almost always rely on static source code analysis as the first and fundamental step to gather information. In the past, two different strategies have been adopted to develop tool suites. There are tools encompassing or implementing the source parse step, where the parser is internal to the toolkit, developed and maintained with it. A different approach builds tools on the top of external, already available, components such as compilers that output the abstract syntax tree, or make it available via an API. We present an approach and a tool, XOgastan, developed exploiting the gcc/g++ ability to save a representation of the intermediate abstract syntax tree into a file. XOgastan translates the gcc/g++ format into a graph exchange language representation, thus taking advantage of the high number of currently available XML tools for the subsequent analysis\u00a0\u2026", "num_citations": "26\n", "authors": ["103"]}
{"title": "Modeling web maintenance centers through queue models\n", "abstract": " The Internet and the World Wide Web's pervasiveness are changing the landscape of several different areas, ranging from information gathering/managing and commerce to software development, maintenance and evolution. Traditional telephone-centric services, such as ordering of goods, maintenance/repair intervention requests and bug/defect reporting, are moving towards Web-centric solutions. This paper proposes the adoption of queuing theory to support the design, staffing, management and assessment of Web-centric service centers. Data from a mailing list archiving a mixture of corrective maintenance and information requests were used to mimic a service center. Queuing theory was adopted to model the relation between the number of servers and the performance level. Empirical evidence revealed that, by adding an express lane and a dispatcher service time, the variability is greatly reduced and\u00a0\u2026", "num_citations": "26\n", "authors": ["103"]}
{"title": "Using participant similarity for the classification of epidemiological data on hepatic steatosis\n", "abstract": " Clinical decision support relies on the findings of epidemiological (longitudinal and cross-sectional) studies on predictive features and risk factors for diseases. Such features flow into the diagnostic procedures. Personalized medicine, which aims to optimize clinical decision making by taking individual characteristics of the patients into account, relies on the findings of epidemiology on groups of cohort participants that have common risk factors and exhibit the outcome under study. The identification of such groups requires modeling and exploiting similarity among individuals described through medical tests. In this work, we study how similarity measures for complex objects contribute to class separation for a multifactorial disorder. We present a data preparation, partitioning and classification workflow on cohort participants for the disorder \"hepatic steatosis\", and report on our findings on classifier performance and\u00a0\u2026", "num_citations": "24\n", "authors": ["103"]}
{"title": "Library miniaturization using static and dynamic information\n", "abstract": " Moving to smaller libraries can be considered as a relevant task when porting software systems to limited-resource devices (e.g., hand-helds). Library miniaturization will be particularly effective if based on both dynamic (keeping into account dependencies exploited during application execution in a given user profile) and static (keeping into account all possible dependencies) information. This paper presents distributed software architecture, based on Web services, to collect dynamic information at run-time, and an approach for miniaturization of libraries, exploiting both dynamic and static information with the aim of reducing the memory requirements of executables. New, smaller libraries are identified via hierarchical clustering and genetic algorithms; clustering produces a first initial solution, then optimized by multi-objective genetic algorithms. The approach has been applied to medium size open source software\u00a0\u2026", "num_citations": "24\n", "authors": ["103"]}
{"title": "Smart formatter: Learning coding style from existing source code\n", "abstract": " The quality of identifiers, the coding style and formatting are important aspects that influence program understandings and maintenance. This is confirmed by the presence of several approaches and tools aimed at supporting and improving the source code quality. This paper describes a tool named Smart Formatter that allows programmers to learn coding style rules from existing source code, and apply these rules to the code under development.", "num_citations": "23\n", "authors": ["103"]}
{"title": "Yaab (yet another ast browser): Using ocl to navigate asts\n", "abstract": " In the last decades several tools and environments defined and introduced languages for querying, navigating and transforming abstract syntax trees. These environments were meant to support software maintenance, reengineering and program comprehension activities. Instead of introducing a new language, this paper proposes to adopt the Object Constraint Language (OCL) to express queries over an object model representing the abstract syntax tree of the code to be analyzed. OCL is part of the UML lingua franca and thus several advantages can be readily obtained. Central to the idea is to shift the analysis paradigm from a tree-based to an object-oriented paradigm, and to provide a meta-model decoupling the query language from the target language. This paper presents the current status in implementing an OCL interpreter with the ability of querying an object model representing the abstract syntax tree\u00a0\u2026", "num_citations": "23\n", "authors": ["103"]}
{"title": "Compiler hacking for source code analysis\n", "abstract": " Many activities related to software quality assessment and improvement, such as empirical model construction, data flow analysis, testing or reengineering, rely on static source code analysis as the first and fundamental step for gathering the necessary input information. In the past, two different strategies have been adopted to develop tool suites. There are tools encompassing or implementing the source parse step, where the parser is internal to the toolkit, and is developed and maintained with it. A different approach builds tools on the top of external already-available components such as compilers that output the program abstract syntax tree, or that make it available via an API.               This paper discusses techniques, issues and challenges linked to compiler patching or wrapping for analysis purposes. In particular, different approaches for accessing the compiler parsing information are compared, and\u00a0\u2026", "num_citations": "22\n", "authors": ["103"]}
{"title": "Guidelines on the use of Fit tables in software maintenance tasks: Lessons learned from 8 experiments\n", "abstract": " Executable acceptance test case-in particular Fit (Framework for Integrated Test) tables-originally intended for the development phase proved useful in maintenance activities too. Empirical evidence suggests that Fit tables are useful in improving the comprehension of change requirements and the correctness of the maintained code. Stemming from eight experiments formerly performed by the authors, this paper presents a set of lessons learned and guidelines useful for project managers on the use of Fit tables in maintenance tasks. Specifically, the paper discusses the use of Fit tables in maintenance tasks considering a set of dimensions, ranging from maintainerspsila experience to the nature of application being maintained and to the kind of benefits introduced by Fit tables. Benefits of Fit tables, such as improving the code correctness and comprehension, increase with developers experience and complex\u00a0\u2026", "num_citations": "19\n", "authors": ["103"]}
{"title": "Evolution doctor: a framework to control software system evolution\n", "abstract": " Real world software systems undergo, during their lifetime, to repeated maintenance activities. Due to the market pressure and to the need for having back the system operational in the shortest time possible, maintenance tends to introduce negative side effects. Some examples are the growth of the cloning percentage, the increase of library size, the presence of unused objects, or the lost of source file organization. This thesis proposes a framework, named evolution doctor, to diagnose and cure such phenomena. The framework permits the analysis and prediction of several indicators of software system evolution (size, complexity, cloning). Then, the framework defines a set of methods and tools to cure the problems: remove clones and unused objects, reorganize libraries, and restructure the source file directory organizations.", "num_citations": "19\n", "authors": ["103"]}
{"title": "Identifying and locating interference issues in php applications: The case of wordpress\n", "abstract": " The large success of Content management Systems (CMS) such as WordPress is largely due to the rich ecosystem of themes and plugins developed around the CMS that allows users to easily build and customize complex Web applications featuring photo galleries, contact forms, and blog pages. However, the design of the CMS, the plugin-based architecture, and the implicit characteristics of the programming language used to develop them (often PHP), can cause interference or unwanted side effects between the resources declared and used by different plugins. This paper describes the problem of interference between plugins in CMS, specifically those developed using PHP, and outlines an approach combining static and dynamic analysis to detect and locate such interference. Results of a case study conducted over 10 WordPress plugins shows that the analysis can help to identify and locate plugin\u00a0\u2026", "num_citations": "18\n", "authors": ["103"]}
{"title": "Using fuzzy logic to relax constraints in GA-based service composition\n", "abstract": " The rapid diffusion of web services is changing the software engineering landscape. One of the most interesting features offered by service\u2013oriented systems is the possibility to perform dynamic binding, ie choosing, among sets of semantically equivalent services, those which better contribute to meet some constraints (eg, related to the cost or to any other Quality of Service attributes) and optimize some other criteria (eg, the response time). Solving this problem is NP\u2013hard, and approaches to tackle it using Genetic Algorithms have been proposed.In some cases, especially when it is not possible to find any solution to the aforementioned problem, it would be useful to relax constraints, in order to find some alternative solutions that, while not meeting the initial constraints, at least offer a reasonable Quality of Service. This paper proposes the use of fuzzy logic to address the imprecision in specifying QoS constraints, estimating QoS values and expressing Service Level Agreements.", "num_citations": "17\n", "authors": ["103"]}
{"title": "\" Talking tests\": a Preliminary Experimental Study on Fit User Acceptance Tests\n", "abstract": " This short paper reports a pilot experiment conducted with master students, in which we investigated whether fit test cases were helpful to clarify change requirements in a maintenance task.", "num_citations": "14\n", "authors": ["103"]}
{"title": "Dynamic composition of web applications in human-centered processes\n", "abstract": " Dynamic binding has been recently applied with success to composite services. However, many business processes are human-centered, that is, tasks may be performed by humans other than software, and so these processes require a high interaction between users and Web applications. Inspired from previous work in service dynamic binding and information propagation in web application mashups, this paper proposes an approach to enable, through proxy portlets, dynamic binding between human-centered processes and web applications or web services, also enabling the propagation of information - e.g., form parameters and operation outputs - between different activities.", "num_citations": "13\n", "authors": ["103"]}
{"title": "A distributed architecture for dynamic analyses on user-profile data\n", "abstract": " Combining static and dynamic information is highly relevant in many reverse engineering, program comprehension and maintenance tasks. To allow dynamic information reflecting software system usage scenarios, it should be collected during a long period of time, in a real user environment. This, however, poses several challenges. First and foremost, it is necessary to model the extraction of any relevant dynamic information from execution traces, thus avoiding to collect a large amount of unmanageable data. Second, we need a distributed architecture that allows to collect and compress such an information from geographically distributed users. We propose a probabilistic model for representing dynamic information, as well as a Web-service based distributed architecture for its collection and compression. The new architecture has been instantiated to collect inter-procedural program execution traces up to a\u00a0\u2026", "num_citations": "13\n", "authors": ["103"]}
{"title": "A search-based approach for dynamically re-packaging downloadable applications\n", "abstract": " Mechanisms such as Java Web Start enable on-the-fly downloading and execution of applications installed on remote servers, without the need for having them installed on the local machine.", "num_citations": "12\n", "authors": ["103"]}
{"title": "Managing and assessing the risk of component upgrades\n", "abstract": " This paper describes the experience, carried out by Ericsson Telecomunicazioni S.p.A (Italy), in managing the migration of their legacy products towards a product line approach and, specifically, how the update of third-party software products is handled in such product lines. The paper describes the Ericsson application scenario in the development and evolution of network management products. Then, it provides an overview of how the company adopts (i) an internal toolkit to manage third party software products, with the aim of determining the impact of their updates upon variants of the network management system, and (ii) a risk management framework, which helps the developer to decide whether and when update third-party products.", "num_citations": "9\n", "authors": ["103"]}
{"title": "Mining developers' communication to assess software quality: Promises, challenges, perils\n", "abstract": " Summary form only given. In the recent decades, power consumption of System on Chip (SoC) is getting more dominant and Through-Silicon Via (TSV) technology has emerged as a promising solution to enhance system integration at lower cost and reduce footprint. Powerful microprocessor and immense memory capability integrated in standard 2D IC enabled to improve IC performance by shrinking IC dimensions. Our research evaluates the impact of Through-Silicon Via (TSV) on 3D chip performance as well as power consumption and investigates to understand the optimum TSV dimension (i.e., diameter, height, etc\u2026) for 3D IC fabrication. The key idea is using the physical and electrical modeling of TSV which considers the coupling effects as well as TSV-to-bulk silicon parameters in 3D circuitry. In addition, by combining the conventional metrics for planar IC technology with TSV modeling, several\u00a0\u2026", "num_citations": "9\n", "authors": ["103"]}
{"title": "Combining quantitative and qualitative methods (when mining software data)\n", "abstract": " In recent years, empirical research in software engineering has gained a lot of maturity. This is in part thanks to the availability of a large amount of data from different kinds of software repositories, as well as of techniques to mine them, eg, for tracing data from heterogeneous sources, summarizing changes, or mining unstructured and semistructured data sources. Noticeably, the analysis of data from software repositories has also gained a lot of attention in industry, where developers rely on software data to make important decisions. This chapter discusses how quantitative techniques for mining software archives can be complemented with different kinds of qualitative methods to get insights from the analyzed data and provide a rationale to quantitative results.", "num_citations": "8\n", "authors": ["103"]}
{"title": "What topics do Firefox and Chrome contributors discuss?\n", "abstract": " Firefox and Chrome are two very popular open source Web browsers, implemented in C/C++. This paper analyzes what topics were discussed in Firefox and Chrome bug reports over time. To this aim, we indexed the text contained in bug reports submitted each semester of the project history, and identified topics using Latent Dirichlet Allocation (LDA). Then, we investigated to what extent Firefox and Chrome developers/contributors discussed similar topics, either in different periods, or over the same period. Results indicate a non-negligible overlap of topics, mainly on issues related to page layouting, user interaction, and multimedia contents.", "num_citations": "8\n", "authors": ["103"]}
{"title": "Discovery of SOA patterns via model checking\n", "abstract": " Design pattern recovery has been proved to be an useful mechanism to assess the quality of object-oriented systems, to facilitate their comprehension, and to help identifying reusable assets. The diffusion of Service Oriented Architectures (SOA) is fostering the introduction of new patterns, realizing recurring mechanisms that service-oriented systems often realize. Examples are proxies used to enable dynamic binding or to ensure fault tolerance, service compositions, and loggers used for monitoring purposes. This paper describes how SOA patterns can be identified by analyzing SOAP messages collected by monitoring a service oriented system. Namely, the paper presents a model checking-based approach where patterns, described by parametric selective mu-calculus logic formulae, are verified on a synthetized model of the system.", "num_citations": "8\n", "authors": ["103"]}
{"title": "Snoring: A noise in defect prediction datasets\n", "abstract": " In order to develop and train defect prediction models, researchers rely on datasets in which a defect is often attributed to a release where the defect itself is discovered. However, in many circumstances, it can happen that a defect is only discovered several releases after its introduction. This might introduce a bias in the dataset, i.e., treating the intermediate releases as defect-free and the latter as defect-prone. We call this phenomenon as \"sleeping defects\". We call \"snoring\" the phenomenon where classes are affected by sleeping defects only, that would be treated as defect-free until the defect is discovered. In this paper we analyze, on data from 282 releases of six open source projects from the Apache ecosystem, the magnitude of the sleeping defects and of the snoring classes. Our results indicate that 1) on all projects, most of the defects in a project slept for more than 20% of the existing releases, and 2) in the\u00a0\u2026", "num_citations": "6\n", "authors": ["103"]}
{"title": "SBSE meets software maintenance: Achievements and open problems\n", "abstract": " Software maintenance is, together with testing, one of the most critical and effort-prone activities in software development. Surveys conducted in the past [4] have estimated that up to 80% of the overall software development cost is due to maintenance activities. For such a reason, automated techniques aimed at supporting developers in their daunting tasks are highly appealing. Problems developers often face off include understanding undocumented software, improving the software design and source code structure to ease future maintenance tasks, and producing patches to fix bugs.               Finding a solution for the aforementioned problems is intrinsically NP-Hard, and therefore such problems are not tractable with conventional algorithmic techniques; this is particularly true in presence of very complex and large software systems. For this reason, search-based optimization techniques have been\u2014and\u00a0\u2026", "num_citations": "6\n", "authors": ["103"]}
{"title": "Frontiers of reverse engineering: A conceptual model\n", "abstract": " Software reverse engineering is a crucial task to reconstruct high-level views of a software system - with the purpose of understanding and/or maintaining it - when the only reliable source of information is the source code, or even the system binaries. This paper discusses key reverse engineering concepts through a UML conceptual model. Specifically, the model is composed of a set of UML class diagrams describing relationships existing among reverse engineering processes, tools, artifacts, and stakeholders.", "num_citations": "6\n", "authors": ["103"]}
{"title": "Regression testing of web services\n", "abstract": " Web Services are entailing a major shift of perspective in software engineering: software is used and not owned and operation happens on machines that are out of the user control. This entails that the user cannot decide the strategy to migrate to a new version of a service, as it happens with COTS. Therefore, providing users with means to build confidence that a service delivers over time the desired function with the expected Quality of Service becomes a key issue. This paper proposes the use of test cases as a form of contract between the provider and the users of a service, and discusses an approach to allow users to repeatedly run a test suite against a service to discover if functional or non\u2013functional expectations are maintained over the time. The approach is validated by applying it in two case studies.", "num_citations": "5\n", "authors": ["103"]}
{"title": "Recommending api function calls and code snippets to support software development\n", "abstract": " Software development activity has reached a high degree of complexity, guided by the heterogeneity of the components, data sources, and tasks. The proliferation of open-source software (OSS) repositories has stressed the need to reuse available software artifacts efciently. To this aim, it is necessary to explore approaches to mine data from software repositories and leverage it to produce helpful recommendations. We designed and implemented FOCUS as a novel approach to provide developers with API calls and source code while they are programming. The system works on the basis of a context-aware collaborative ltering technique to extract API usages from OSS projects. In this work, we show the suitability of FOCUS for Android programming by evaluating it on a dataset of 2,600 mobile apps. The empirical evaluation results show that our approach outperforms two state-of-the-art API recommenders, UP\u00a0\u2026", "num_citations": "4\n", "authors": ["103"]}
{"title": "A novel process and its implementation for the multi-objective miniaturization of software\n", "abstract": " Smart phones, gaming consoles, wireless routers are ubiquitous; the increasing diffusion of such devices with limited resources, together with society\u2019s unsatiated appetite for new applications, pushes companies to miniaturize their programs. Miniaturizing a program for a hand-held device is a time-consuming task often requiring complex decisions. Companies must accommodate conflicting constraints: customers\u2019 satisfaction may be in conflict with a device\u2019s limited storage and memory. This paper proposes a process, MoMS, for the multi-objective miniaturization of software to help developers miniaturize programs while satisfying multiple conflicting constraints. The process directs: the elicitation of customer pre-requirements, their mapping to program features, and the selection of the features to port. We present two case studies based on Pooka, an email client, and SIP Communicator, an instant messenger, to demonstrate that MoMS supports miniaturization and helps reduce effort by 77%, on average, over a manual approach.", "num_citations": "4\n", "authors": ["103"]}
{"title": "Best practices for validating research software prototypes-MARKOS case study\n", "abstract": " A common approach in research projects is to plan the evaluation and in particular the validation phase In the last months of the project. When done late in the project, validation cannot provide valuable inputs to the development of the research idea, because there is no space to bring about changes, and very often ends up as confirmatory tests. Software validation can and should occur since creating the first lines of code until the completion of the system in test. Thus, we propose the approach where research project can adopt the agile paradigm used by startups \u201crelease early, release often\u201d, not only to development, but also to validation: \u201cvalidate early, validate often\u201d, to improve the quality of projects results on one hand and to be in line with potential users on the other.", "num_citations": "3\n", "authors": ["103"]}
{"title": "SOA testing technologies further reading web services-interoperability\n", "abstract": " March\u2758 April 2006 IT Pro 11 ability to properly handle exceptions. Although testing costs are limited in this case (the developer does not have to pay when testing his own service), nonfunctional testing isn\u2019t realistic because it doesn\u2019t account for the provider and consumer infrastructure, and the network configuration or load.", "num_citations": "3\n", "authors": ["103"]}
{"title": "Using SVM and concept analysis to support web service classification and annotation\n", "abstract": " The need for supporting the classification and semantic annotation of services constitutes an important challenge for service\u2013centric software engineering. Late\u2013binding and, in general, service matching approaches, require services to be semantically annotated. Such a semantic annotation may require, in turn, to be made in agreement to a specific ontology. Also, a service description needs to properly relate with other similar services.This paper proposes an approach to i) automatically classify services to specific domains and ii) identify key concepts inside service textual documentation, and build a lattice of relationships between service annotations. Support Vector Machines and Formal Concept Analysis have been used to perform the two tasks. Results obtained classifying a set of web services show that the approach can provide useful insights in both service publication and service retrieval phases.", "num_citations": "3\n", "authors": ["103"]}
{"title": "How Can I Use This Method?\n", "abstract": " How Can I Use This Method? IRIS nascondi/visualizza icone a destra nascondi/visualizza menu in alto Aiuto Sfoglia Scorri i prodotti per: Autore Titolo Riviste Serie Login IRIS Archivio della ricerca dell'Universit\u00e0 degli studi del Sannio Catalogo Ricerca 4 Contributo in Atti di Convegno (Proceeding) 4.1 Contributo in Atti di convegno How Can I Use This Method? Italiano Italiano English How Can I Use This Method? / Laura Moreno; Gabriele Bavota; Di Penta M; Rocco Oliveto; Andrian Marcus. - (2015), pp. 880-890. ((Intervento presentato al convegno 37th IEEE/ACM International Conference on Software Engineering, ICSE 2015 tenutosi a Florence, Italy nel May 16-24, 2015. Scheda breve Scheda completa Titolo: How Can I Use This Method? Autori interni: DI PENTA, Massimiliano mostra contributor esterni Data di pubblicazione: 2015 Handle: http://hdl.handle.net/20.500./13350 ISBN: 978-1-4799-1934-5 Appare : in \u2026", "num_citations": "2\n", "authors": ["103"]}
{"title": "Nothing else matters: what predictive model should i use?\n", "abstract": " In past and recent years---also thanks to the availability of data from software repositories---several kinds of models have been built to characterize and, in some cases, to predict software change-and fault-proneness.", "num_citations": "2\n", "authors": ["103"]}
{"title": "Introduction to the special issue on search based software engineering\n", "abstract": " This special issue of Empirical Software Engineering consists of revised and extended versions of five selected papers originally presented at the 1st International Symposium on Search Based Software Engineering (SSBSE 2009). The symposium was held at Cumberland Lodge, Windsor, UK, in May 2009, and brought together the academic researchers and industrial practitioners that make up the international Search Based Software Engineering (SBSE) community for the first conference dedicated to the subject.Search Based Software Engineering is the application of search-based optimization techniques\u2014for example, genetic algorithms, hill climbing, simulated annealing, taboo search, and ant colony optimization\u2014to the solution of software engineering problems. Many such problems are often difficult or impossible to solve manually, and moreover, as modern software becomes more complex and adopts\u00a0\u2026", "num_citations": "2\n", "authors": ["103"]}
{"title": "Empirical studies on software evolution: Should we (try to) claim causation?\n", "abstract": " In recent and past years, there have been hundreds of studies aimed at characterizing the evolution of a software system. Many of these studies analyze the behavior of a variable over a given period of observation. How does the size of a software system evolve? What about its complexity? Does the number of defects increase over time or does it remain stable?", "num_citations": "2\n", "authors": ["103"]}
{"title": "Special Issue on Search\u2010Based Software Maintenance\n", "abstract": " Search-based software engineering (SBSE) consists of the application of metaheuristic search techniques\u2014such as genetic algorithms, simulated annealing, hill climbing, and tabu search\u2014to software engineering problems. This idea is based on the observation that some software engineering activities can be thought of as optimization problems, and on the fact that techniques providing exact solutions are, very often, unable to adequately scale to large, real-world software engineering problems.", "num_citations": "2\n", "authors": ["103"]}
{"title": "A language-independent framework for software miniaturization\n", "abstract": " One of the undesired effects of software evolution is the proliferation of unused components, or components unlikely to be used by a given subset of the applications. As a consequence, the size of binaries and libraries tends to grow. One of the major trends of today\u2019s software market is the porting of applications on hand-held devices or, in general, on devices having a limited amount of resources available. Several forms of refactoring and, in particular, the miniaturization of libraries and applications, are therefore necessary.We propose a framework and a toolkit covering several aspects of software miniaturization, such as removing unused objects and code clones, as well as creating small, cohese libraries refactoring the existing ones. The last step has been implemented using a hybrid approach based on hierarchical clustering, genetic algorithms and hill climbing, also incorporating the developer\u2019s knowledge. Most of the framework activities are language independent (relying on object module analysis) thus they do not require any kind of source code parsing and are applicable to software systems developed with", "num_citations": "2\n", "authors": ["103"]}
{"title": "The impact of api change-and fault-proneness on the user ratings of android apps\n", "abstract": " The Impact of API Change- and Fault-Proneness on the User Ratings of Android Apps IRIS nascondi/visualizza icone a destra nascondi/visualizza menu in alto Aiuto Sfoglia Scorri i prodotti per: Autore Titolo Riviste Serie Login IRIS Archivio della ricerca dell'Universit\u00e0 degli studi del Sannio Catalogo Ricerca 1 Contributo su Rivista 1.1 Articolo in rivista The Impact of API Change- and Fault-Proneness on the User Ratings of Android Apps Italiano Italiano English The Impact of API Change- and Fault-Proneness on the User Ratings of Android Apps / Gabriele Bavota; Mario Linares V\u00e1squez; Carlos Eduardo Bernal-C\u00e1rdenas; Di Penta M; Rocco Oliveto; Denys Poshyvanyk. - In: IEEE TRANSACTIONS ON SOFTWARE ENGINEERING. - ISSN 0098-5589. - 41:4(2015), pp. 384-407. Scheda breve Scheda completa Titolo: The Impact of API Change- and Fault-Proneness on the User Ratings of Android Apps Autori interni: DI :\u2026", "num_citations": "1\n", "authors": ["103"]}
{"title": "The Evolution of Project Inter-dependencies in a Software Ecosystem: The Case of Apache\n", "abstract": " The Evolution of Project Inter-dependencies in a Software Ecosystem: The Case of Apache IRIS nascondi/visualizza icone a destra nascondi/visualizza menu in alto Aiuto Sfoglia Scorri i prodotti per: Autore Titolo Riviste Serie Login IRIS Archivio della ricerca dell'Universit\u00e0 degli studi del Sannio Catalogo Ricerca 4 Contributo in Atti di Convegno (Proceeding) 4.1 Contributo in Atti di convegno The Evolution of Project Inter-dependencies in a Software Ecosystem: The Case of Apache Italiano Italiano English The Evolution of Project Inter-dependencies in a Software Ecosystem: The Case of Apache / Gabriele, Bavota; Canfora, G; DI PENTA, Massimiliano; Rocco, Oliveto; Sebastiano, Panichella. - (2013), pp. 280-289. ((Intervento presentato al convegno 29th IEEE International Conference on Software Maintenance (ICSM 2013) tenutosi a Eindhoven; Netherlands nel 22 September 2013-September 2013. Scheda breve : \u2026", "num_citations": "1\n", "authors": ["103"]}
{"title": "Empirical Studies in Reverse Engineering and Maintenance: Employing Developers to Evaluate Your Approach and Tool\n", "abstract": " This tutorial will aim at providing practical guidelines on how to conduct empirical evaluations of software maintenance and reengineering approaches and tools. The tutorial will describe how to conduct empirical evaluations carried out involving human subjects (i.e., developers), and then will provide a \"by-example'' introduction to the main statistical procedures to be used for analyzing empirical study results.", "num_citations": "1\n", "authors": ["103"]}
{"title": "A Heuristic-based Approach to Identify Concepts in Execution Traces\n", "abstract": " A Heuristic-Based Approach to Identify Concepts in Execution Traces IRIS nascondi/visualizza icone a destra nascondi/visualizza menu in alto Aiuto Sfoglia Scorri i prodotti per: Autore Titolo Riviste Serie Login IRIS Archivio della ricerca dell'Universit\u00e0 degli studi del Sannio Catalogo Ricerca 4 Contributo in Atti di Convegno (Proceeding) 4.1 Contributo in Atti di convegno A Heuristic-Based Approach to Identify Concepts in Execution Traces Italiano Italiano English A Heuristic-Based Approach to Identify Concepts in Execution Traces / Fatemeh Asadi; Di Penta M; Giuliano Antoniol; Yann-Ga\u00ebl Gu\u00e9h\u00e9neuc:. - (2010), pp. 31-40. ((Intervento presentato al convegno 14th European Conference on Software Maintenance and Reengineering, CSMR 2010 nel 2010. Scheda breve Scheda completa Titolo: A Heuristic-Based Approach to Identify Concepts in Execution Traces Autori interni: DI PENTA, Massimiliano mostra esterni \u2026", "num_citations": "1\n", "authors": ["103"]}
{"title": "Traceability improvement for software miniaturization\n", "abstract": " On the one hand, software companies try to reach the maximum number of customers, which often translate into integrating more features into their programs, leading to an increase in size, memory footprint, screen complexity, and so on. On the other hand, hand-held devices are now pervasive and their customers ask for programs similar to those they use everyday on their desktop computers. Companies are left with two options, either to develop new software for hand-held devices or perform manual refactoring to port it on hand-held devices, but both options are expensive and laborious. Software miniaturization can aid companies to port their software to hand-held devices. However, traceability is backbone of software miniaturization, without up-to-date traceability links it becomes diffi cult to recover desired artefacts for miniaturized software. Unfortunately, due to continuous changes, it is a tedious and time-consuming task to keep traceability links up-to-date. Often traceability links become outdated or completely vanish. Several traceability recovery approaches have been developed in the past. Each approach has some benefits and limitations. However, these approaches do not tell which factors can affect traceability recovery process. Our current research proposal is based on the premise that controlling potential quality factors and combining different traceability approaches can improve traceability quality for software miniaturization. In this research proposal, we introduce traceability improvement for software miniaturization (TISM) process. TISM has three sub processes, namely, traceability factor controller (TFC), hybrid traceability (HT\u00a0\u2026", "num_citations": "1\n", "authors": ["103"]}
{"title": "Architecting, Analyzing and Testing Service-Oriented Systems.\n", "abstract": " The today\u2019s diffusion of web services and service\u2013oriented architecture is posing the basis for radical changes in the way of developing, evolving and testing software systems. This tutorial outlines some main research challenges on this topic, and provides guidelines and practical solutions for i) realizing service\u2013oriented systems able to support QoS-aware dynamic binding ii) helping the comprehension of service\u2013oriented systems and iii) testing service\u2013oriented systems.", "num_citations": "1\n", "authors": ["103"]}
{"title": "Tools for software maintenance and reengineering\n", "abstract": " This book describes nine software maintenance tools, some of which presented during the tool demonstration session of the 8th European Conference of Software Maintenance and Reengineering (CSMR) held in Tampere, Finland, in March 2004. The book is organized in three sections, describing tools related to different maintenance and reengineering phases or tasks; namely fact extraction from source code, software visualization and web reengineering. Contributions came from some of the best worldwide recognized people in the software maintenance and reengineering community.We hope that this book will become not only a reference for maintainers and researchers in the field, but also a starting point for new research activities and for the development of new tools related to the new frontiers of software engineering.", "num_citations": "1\n", "authors": ["103"]}
{"title": "Complexity and Feasibility Issues in Object Oriented Clone Detection\n", "abstract": " Large multi-platform software systems are likely to encompass a variety of programming languages, coding styles, idioms and hardware-dependent code. Analyzing multi-platform source code, however, is a challenging task. Assembler code is often mixed with high-level Object Oriented (OO) or procedural programming languages. Furthermore, scripting languages, configuration files, and hardware specific resources are typically used. Often, systems were originally conceived as a single platform application, with a limited number of functionalities and of supported devices. Then, they evolved by adding new functionalities and were ported on new product families. In other words, new devices and target platforms were added. When writing a device driver or porting an existing application to a new processor, developers may decide to copy an entire working subsystem and then modify the code to cope with the new hardware. This approach is believed to increase the chances that the developers\u2019 work will have little unplanned effect on the original piece of code they have just copied. However, this evolving practice promotes the appearing of duplicated code snippets, also called clones. There have been many publications proposing various ways of identifying similar code fragments and components in a software system [14, 9, 6, 2, 10, 12, 3]. However, those publications essentially focused on procedural programming languages such as C and only few contribution addressed the problem of detecting duplicated code in OO systems [8, 1]. Moreover, algorithms presented and published to detect duplications in procedural systems exhibit linear or\u00a0\u2026", "num_citations": "1\n", "authors": ["103"]}