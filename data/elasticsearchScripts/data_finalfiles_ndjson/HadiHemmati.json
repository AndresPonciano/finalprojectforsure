{"title": "An industrial investigation of similarity measures for model-based test case selection\n", "abstract": " Applying model-based testing (MBT) in practice requires practical solutions for scaling up to large industrial systems. One challenge that we have faced while applying MBT was the generation of test suites that were too large to be practical, even for simple coverage criteria. The goal of test case selection techniques is to select a subset of the generated test suite that satisfies resource constraints while yielding a maximum fault detection rate. One interesting heuristic is to choose the most diverse test cases based on a pre-defined similarity measure. In this paper, we investigate and compare possible similarity functions to support similarity-based test selection in the context of state machine testing, which is the most common form of MBT. We apply the proposed similarity measures and a selection strategy based on genetic algorithms to an industrial software system. We compare their fault detection rate based on\u00a0\u2026", "num_citations": "77\n", "authors": ["1908"]}
{"title": "A similarity-based approach for test case prioritization using historical failure data\n", "abstract": " Test case prioritization is a crucial element in software quality assurance in practice, specially, in the context of regression testing. Typically, test cases are prioritized in a way that they detect the potential faults earlier. The effectiveness of test cases, in terms of fault detection, is estimated using quality metrics, such as code coverage, size, and historical fault detection. Prior studies have shown that previously failing test cases are highly likely to fail again in the next releases, therefore, they are highly ranked, while prioritizing. However, in practice, a failing test case may not be exactly the same as a previously failed test case, but quite similar, e.g., when the new failing test is a slightly modified version of an old failing one to catch an undetected fault. In this paper, we define a class of metrics that estimate the test cases quality using their similarity to the previously failing test cases. We have conducted several\u00a0\u2026", "num_citations": "75\n", "authors": ["1908"]}
{"title": "Exploring Test Suite Diversification and Code Coverage in Multi-Objective Test Case Selection\n", "abstract": " Test case selection is a classic testing technique to choose a subset of existing test cases for execution, due to the limited budget and tight deadlines. While 'code coverage' is the state of practice among test case selection heuristics, recent literature has shown that `test case diversity' is also a very promising approach. In this paper, we first compare these two heuristics for test case selection in several real-world case studies (Apache Ant, Derby, JBoss, NanoXML and Math). The results show that neither of the two techniques completely dominates the other, but they can potentially be complementary. Therefore, we next propose a novel approach that maximizes both code coverage and diversity among the selected test cases using NSGA-II multi- objective optimization, and the results show a significant improvement in fault detection rate. Specifically, sometimes this novel approach detects up to 16%(Ant), 10%(JBoss\u00a0\u2026", "num_citations": "67\n", "authors": ["1908"]}
{"title": "Prioritizing Manual Test Cases in Traditional and Rapid Release Environments\n", "abstract": " Test case prioritization is one of the most practically useful activities in testing, specially for large scale systems. The goal is ranking the existing test cases in a way that they detect faults as soon as possible, so that any partial execution of the test suite detects maximum number of defects for the given budget. Test prioritization becomes even more important when the test execution is time consuming, e.g., manual system tests vs. automated unit tests. Most existing test case prioritization techniques are based on code coverage, which requires access to source code. However, manual testing is mainly done in a black- box manner (manual testers do not have access to the source code). Therefore, in this paper, we first examine the existing test case prioritization techniques and modify them to be applicable on manual black-box system testing. We specifically study a coverage- based, a diversity-based, and a risk driven\u00a0\u2026", "num_citations": "49\n", "authors": ["1908"]}
{"title": "How effective are code coverage criteria?\n", "abstract": " Code coverage is one of the main metrics to measure the adequacy of a test case/suite. It has been studied a lot in academia and used even more in industry. However, a test case may cover a piece of code (no matter what coverage metric is being used) but miss its faults. In this paper, we studied several existing and standard control and data flow coverage criteria on a set of developer-written fault-revealing test cases from several releases of five open source projects. We found that a) basic criteria such as statement coverage is very weak (detecting only 10% of the faults), b) combining several control-flow coverage together is better than the strongest criterion alone (28% vs. 19%), c) a basic data-flow coverage can detect many undetected faults (79% of the undetected faults by control-flow coverage can be detected by a basic def/use pair coverage), and d) on average 15% of the faults may not be detected by any\u00a0\u2026", "num_citations": "44\n", "authors": ["1908"]}
{"title": "ChronoTwigger: A Visual Analytics Tool for Understanding Source and Test Co-Evolution\n", "abstract": " Applying visual analytics to large software systems can help users comprehend the wealth of information produced by source repository mining. One concept of interest is the co-evolution of test code with source code, or how source and test files develop together over time. For example, understanding how the testing pace compares to the development pace can help test managers gauge the effectiveness of their testing strategy. A useful concept that has yet to be effectively incorporated into a co-evolution visualization is co-change. Co-change is a quantity that identifies correlations between software artifacts, and we propose using this to organize our visualization in order to enrich the analysis of co-evolution. In this paper, we create, implement, and study an interactive visual analytics tool that displays source and test file changes over time (co-evolution) while grouping files that change together (co-change). Our\u00a0\u2026", "num_citations": "30\n", "authors": ["1908"]}
{"title": "Prioritizing manual test cases in rapid release environments\n", "abstract": " Test case prioritization is an important testing activity, in practice, specially for large scale systems. The goal is to rank the existing test cases in a way that they detect faults as soon as possible, so that any partial execution of the test suite detects the maximum number of defects for the given budget. Test prioritization becomes even more important when the test execution is time consuming, for example, manual system tests versus automated unit tests. Most existing test case prioritization techniques are based on code coverage, which requires access to source code. However, manual testing is mainly performed in a black\u2010box manner (manual testers do not have access to the source code). Therefore, in this paper, the existing test case prioritization techniques (e.g. diversity\u2010based and history\u2010based techniques) are examined and modified to be applicable on manual black\u2010box system testing. An empirical study on\u00a0\u2026", "num_citations": "24\n", "authors": ["1908"]}
{"title": "Studying test case failure prediction for test case prioritization\n", "abstract": " Background: Test case prioritization refers to the process of ranking test cases within a test suite for execution. The goal is ranking fault revealing test cases higher so that in case of limited budget one only executes the top ranked tests and still detects as many bugs as possible. Since the actual fault detection ability of test cases is unknown before execution, heuristics such as\" code coverage\" of the test cases are used for ranking test cases. Other test quality metrics such as\" coverage of the changed parts of the code\" and\" number of fails in the past\"'have also been studied in the literature. Aims: In this paper, we propose using a logistic regression model to predict the failing test cases in the current release based on a set of test quality metrics. Method: We have studied the effect of including our newly proposed quality metric (\" similarity-based\" metric) into this model for tests prioritization. Results: The results of our\u00a0\u2026", "num_citations": "19\n", "authors": ["1908"]}
{"title": "Test case analytics: Mining test case traces to improve risk-driven testing\n", "abstract": " In risk-driven testing, test cases are generated and/or prioritized based on different risk measures. For example, the most basic risk measure would analyze the history of the software and assigns higher risk to the test cases that used to detect bugs in the past. However, in practice, a test case may not be exactly the same as a previously failed test, but quite similar. In this study, we define a new risk measure that assigns a risk factor to a test case, if it is similar to a failing test case from history. The similarity is defined based on the execution traces of the test cases, where we define each test case as a sequence of method calls. We have evaluated our new risk measure by comparing it to a traditional risk measure (where the risk measure would be increased only if the very same test case, not a similar one, failed in the past). The results of our study, in the context of test case prioritization, on two open source projects\u00a0\u2026", "num_citations": "19\n", "authors": ["1908"]}
{"title": "Investigating nlp-based approaches for predicting manual test case failure\n", "abstract": " System-level manual acceptance testing is one of the most expensive testing activities. In manual testing, typically, a human tester is given an instruction to follow on the software. The results as \"passed\" or \"failed\" will be recorded by the tester, according to the instructions. Since this is a labourintensive task, any attempt in reducing the amount of this type of expensive testing is essential, in practice. Unfortunately, most of the existing heuristics for reducing test executions (e.g., test selection, prioritization, and reduction) are either based on source code or specification of the software under test, which are typically not being accessed during manual acceptance testing. In this paper, we propose a test case failure prediction approach for manual testing that can be used as a noncode/ specifcation-based heuristic for test selection, prioritization, and reduction. The approach uses basic Information Retrieval (IR) methods\u00a0\u2026", "num_citations": "15\n", "authors": ["1908"]}
{"title": "A model to support context-aware service migration in pervasive computing environments\n", "abstract": " Continuous improvement and extension of pervasive computing environments, in which objects have high mobility and limited computing power, implies service migration to be considered as one of the main issues. On the other hand, context-awareness which is the most distinguishable feature of pervasive computing helps better serving of users. Accordingly, migration in the presence of context-awareness is considered as the underlying of our motivation. This improves the performance of migration, which is not easily achievable through traditional migration techniques. In this paper we introduce a model to support context-aware migration besides its migration process phases and framework. Our model consists of, an improved service migration, state management and context-awareness all together. The results of our performance evaluation are presented through simulation.", "num_citations": "15\n", "authors": ["1908"]}
{"title": "A framework to support run-time assured dynamic reconfiguration for pervasive computing environments\n", "abstract": " With the increasing use of pervasive computing environments (PCEs), developing dynamic reconfigurable software in such environments becomes an important issue. The ability to change software components in running systems has advantages such as building adaptive, long-life, and self-reconfigurable software as well as increasing invisibility in PCEs. As dynamic reconfiguration is performed in error-prone wireless mobile systems frequently, it can threaten system safety. Therefore, a mechanism to support assured dynamic reconfiguration at run-time for PCEs is required. In this paper, we propose an assured dynamic reconfiguration framework (ADRF) with emphasis on assurance analysis. The framework is implemented and is available for further research. To evaluate the framework, an abstract case study including reconfigurations has been applied using our own developed simulator for PCEs. Our\u00a0\u2026", "num_citations": "12\n", "authors": ["1908"]}
{"title": "The risk-utility tradeoff for data privacy models\n", "abstract": " Nowadays with growth of information technologies, organizations are constantly collecting information about individuals. Public availability of these datasets can considerably benefit the society. To ensure data privacy of a released dataset, various privacy models have been introduced. While many privacy models and techniques have been proposed for data sanitization, the area of sanitized data evaluation has received less attention. This paper investigates the four most well-known data privacy models: k-anonymity, l-diversity, t-closeness, and , \u03f5-differential privacy. We evaluate the data utility (usefulness of sanitized data) and the disclosure risk (re-identification risk of an individual) of the sanitized data for each model. We use a combination of several data utility and risk metrics to measure the impact of a privacy parameter (e.g., k, \u03f5) on a particular privacy model. This enables us to compare the risk-utility\u00a0\u2026", "num_citations": "10\n", "authors": ["1908"]}
{"title": "Using fault history to improve mutation reduction\n", "abstract": " Mutation testing can be used to measure test suite quality in two ways: by treating the kill score as a quality metric, or by treating each surviving, non-equivalent mutant as an indicator of an inadequacy in the test suite. The first technique relies on the assumption that the mutation score is highly correlated with the suite's real fault detection rate, which is not well supported by the literature. The second technique relies only on the weaker assumption that the\" interesting\" mutants (ie, the ones that indicate an inadequacy in the suite) are in the set of surviving mutants. Using the second technique also makes improving the suite straightforward.", "num_citations": "10\n", "authors": ["1908"]}
{"title": "Advances in techniques for test prioritization\n", "abstract": " With the increasing size of software systems and the continuous changes that are committed to the software's codebase, regression testing has become very expensive for real-world software applications. Test case prioritization is a classic solution in this context. Test case prioritization is the process of ranking existing test cases for execution with the goal of finding defects sooner. It is useful when the testing budget is limited and one needs to limit their test execution cost, by only running top n test cases, according to the testing budget. There are many heuristics and algorithms to rank test cases. In this chapter, we will see some of the most common test case prioritization techniques from software testing literature as well as trends and advances in this domain.", "num_citations": "9\n", "authors": ["1908"]}
{"title": "Self-reconfiguration in highly available pervasive computing systems\n", "abstract": " High availability of software systems is an essential requirement for pervasive computing environments. In such systems self-adaptation, using dynamic reconfiguration is also a key feature. However, dynamic reconfiguration potentially decreases the system availability by making parts of the system temporary frozen, especially during incomplete or faulty execution of the reconfiguration process. In this paper, we propose Assured Dynamic Reconfiguration Framework (ADRF), consisting of run-time analysis phases, assuring the desired correctness and completeness of dynamic reconfiguration process. We also specify factors that affect availability of reconfigurable software in pervasive computing systems. Observing the effects of these factors, we present availability improvement of our method in comparison to the other reconfiguration mechanisms.", "num_citations": "7\n", "authors": ["1908"]}
{"title": "An Access Control Framework for Pervasive Computing Environments.\n", "abstract": " The explosive evolution of pervasive computing environments presents several new challenges such as smart spaces, invisibility, localized scalability, and masking uneven conditioning. One of the key challenges in such environments is how to manage security and access control. A suitable approach for managing access control in pervasive computing environments should consider different features of such environment which are not supported by traditional access control mechanisms. The features impose the access control mechanisms to satisfy especial requirements. In this paper, we propose an access control framework supporting different requirements of pervasive computing technology. The framework is capable of providing solutions to the requirements by some functionalities such as decision making based on user intent and in an invisible manner and reauthorization of granted services based on context changes.", "num_citations": "7\n", "authors": ["1908"]}
{"title": "Revisiting hyper-parameter tuning for search-based test data generation\n", "abstract": " Search-based software testing (SBST) has been studied a lot in the literature, lately. Since, in theory, the performance of meta-heuristic search methods are highly dependent on their parameters, there is a need to study SBST tuning. In this study, we partially replicate a previous paper on SBST tool tuning and revisit some of the claims of that paper. In particular, unlike the previous work, our results show that the tuning impact is very limited to only a small portion of the classes in a project. We also argue the choice of evaluation metric in the previous paper and show that even for the impacted classes by tuning, the practical difference between the best and an average configuration is minor. Finally, we will exhaustively explore the search space of hyper-parameters and show that half of the studied configurations perform the same or better than the baseline paper\u2019s default configuration.", "num_citations": "6\n", "authors": ["1908"]}
{"title": "Similarity-based test case selection: Toward scalable and practical model-based testing\n", "abstract": " The growing complexity and size of software systems, along with the increasing role of software in everyday life, makes verification and validation, and testing in particular, essential in software engineering. High fault revealing power, with minimum cost, is the ultimate goal in software testing. Model-based testing (MBT) targets this goal by automating test generation in a systematic way from abstract models of the software under test. Automation reduces the test generation cost dramatically, but the total testing cost includes the cost of test execution and evaluation as well. In practice, there are limitations in testing budget both in terms of time and testing resources. A testing approach cannot be scalable and practical in large industrial systems, unless it addresses all dimensions of the testing cost. But the systematic test generation nature of MBT potentially results in large test suites with execution costs that far exceed the testing budget. Therefore, a mechanism for adjusting the size of the output test suite in MBT is an absolute necessity to ensure success in industry.This thesis proposes techniques that minimize the test suite size while preserving (to the maximum extent) its fault detection rate. The proposed family of techniques, called similarity-based test case selection, hypothesizes that the more diverse the test cases, the higher the fault detection rate. The thesis initiates with a systematic review on search-based techniques for test case generation, which is a starting point for identifying the potential approaches for search-based test case selection being used in similarity-based test case selection. Finding the most effective ways of defining similarity\u00a0\u2026", "num_citations": "5\n", "authors": ["1908"]}
{"title": "An empirical study on practicality of specification mining algorithms on a real-world application\n", "abstract": " Dynamic model inference techniques have been the center of many research projects recently. There are now multiple open source implementations of state-of-the-art algorithms, which provide basic abstraction and merging capabilities. Most of these tools and algorithms have been developed with one particular application in mind, which is program comprehension. The output models can abstract away the details of the program and represent the software behaviour in a concise and easy to understand form. However, one application context that is less studied is using such inferred models for debugging, where the behaviour to abstract is a faulty behaviour (e.g., a set of execution traces including a failed test case). We tried to apply some of the existing model inference techniques in a real-world industrial context to support program comprehension for debugging. Our initial experiments have shown many\u00a0\u2026", "num_citations": "4\n", "authors": ["1908"]}
{"title": "A Search-Based Testing Framework for Deep Neural Networks of Source Code Embedding\n", "abstract": " Over the past few years, deep neural networks (DNNs) have been continuously expanding their real-world applications for source code processing tasks across the software engineering domain, e.g., clone detection, code search, comment generation. Although quite a few recent works have been performed on testing of DNNs in the context of image and speech processing, limited progress has been achieved so far on DNN testing in the context of source code processing, that exhibits rather unique characteristics and challenges.In this paper, we propose a search-based testing framework for DNNs of source code embedding and its downstream processing tasks like Code Search. To generate new test inputs, we adopt popular source code refactoring tools to generate the semantically equivalent variants. For more effective testing, we leverage the DNN mutation testing to guide the testing direction. To demonstrate\u00a0\u2026", "num_citations": "3\n", "authors": ["1908"]}
{"title": "Interactive semi-automated specification mining for debugging: An experience report\n", "abstract": " ContextSpecification mining techniques are typically used to extract the specification of a software in the absence of (up-to-date) specification documents. This is useful for program comprehension, testing, and anomaly detection. However, specification mining can also potentially be used for debugging, where a faulty behavior is abstracted to give developers a context about the bug and help them locating it.ObjectiveIn this project, we investigate this idea in an industrial setting. We propose a very basic semi-automated specification mining approach for debugging and apply that on real reported issues from an AutoPilot software system from our industry partner, MicroPilot Inc. The objective is to assess the feasibility and usefulness of the approach in a real-world setting.MethodThe approach is developed as a prototype tool, working on C code, which accept a set of relevant state fields and functions, per issue, and\u00a0\u2026", "num_citations": "3\n", "authors": ["1908"]}
{"title": "Evaluating specification-level MC/DC criterion in model-based testing of safety critical systems\n", "abstract": " Safety-critical software systems in the aviation domain, e.g., a UAV autopilot software, needs to go through a formal process of certification (e.g., DO-178C standard). One of the main requirements for this certification is having a set of explicit test cases for each software requirement. To achieve this, the DO-178C standard recommends using a model-driven approach. For instance, model-based testing (MBT) is recommended in its DO-331 supplement to automatically generate system-level test cases for the requirements provided as the specification models. In addition, the DO-178C standard also requires high level of source code coverage, which typically is achieved by a separate set of structural testing. However, the standard allows targeting high code coverage with MBT, only if the applicants justify their plan on how to achieve high code coverage through model-level testing. In this study, we propose using the\u00a0\u2026", "num_citations": "2\n", "authors": ["1908"]}
{"title": "Analytics-based safety monitoring and verification\n", "abstract": " Safety-critical systems in domains such as aviation, railway, and automotive are often subject to a formal process of safety certification. The goal of this process is to ensure that these systems will operate safely without posing risks to the user, the public, or the environment [1]. It is typically expensive and time consuming for companies to certify their software. Therefore, any attempt to automate any part of the required process is very appreciated. In this research project, we report on our on-going project with an industry partner in the avionics domain, where we propose a framework to combine specification mining, model-based testing, and analytics to help with monitoring and verification of safety critical systems.", "num_citations": "2\n", "authors": ["1908"]}
{"title": "Structural and Behavioral Run-Time Validation of Dynamic Reconfiguration in Pervasive Computing Environments\n", "abstract": " Increasing use of pervasive computing environments (PCEs) and the need for adaptive software frameworks in PCEs, makes dynamic reconfigurable software an important issue. Dynamic reconfigurable software, changes its architecture during the system execution time. Reconfigurable software in PCEs is at the risk of incorrect execution, because of the error-proneness nature of the environment, lots of device mobility, and complex reconfiguration. In this paper, we propose Assured Dynamic Reconfiguration Framework (ADRF) to support structural and behavioral run-time validation of dynamic reconfiguration in PCEs. The framework is implemented and is available for further research. To evaluate the framework, several case studies including reconfigurations have been applied using our own developed simulator for PCEs. Our experience shows that ADRF properly preserves reconfiguration assurance.", "num_citations": "1\n", "authors": ["1908"]}