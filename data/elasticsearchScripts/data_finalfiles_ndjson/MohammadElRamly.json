{"title": "Reverse engineering legacy interfaces: An interaction-driven approach\n", "abstract": " Legacy systems constitute valuable assets to the organizations that own them. However, due to the development of newer and faster hardware platforms and the invention of novel interface styles, there is a great demand for their migration to new platforms. We present a method for reverse engineering the system interface that consists of two tasks. Based on traces of the users interaction with the system, the \"interface mapping\" task constructs a \"map\" of the system interface, in terms of the individual system screens and the transitions between them. The subsequent \"task and domain modeling\" task uses the interface map and task-specific traces to construct an abstract model of a user's task as an information exchange plan. The task model specifies the screen transition diagram that the user has to traverse in order to accomplish the task in question, and the flow of information that the user exchanges with the system\u00a0\u2026", "num_citations": "73\n", "authors": ["1249"]}
{"title": "User interface reverse engineering in support of interface migration to the web\n", "abstract": " Legacy systems constitute valuable assets to the organizations that own them, and today, there is an increased demand to make them accessible through the World Wide Web to support e-commerce activities. As a result, the problem of legacy-interface migration is becoming very important. In the context of the CELLEST project, we have developed a new process for migrating legacy user interfaces to web-accessible platforms. Instead of analyzing the application code to extract a model of its structure, the CELLEST process analyzes traces of the system-user interaction to model the behavior of the application's user interface. The produced state-transition model specifies the unique legacy-interface screens (as states) and the possible commands leading from one screen to another (as transitions between the states). The interface screens are identified as clusters of similar-in-appearance snapshots in the\u00a0\u2026", "num_citations": "71\n", "authors": ["1249"]}
{"title": "From run-time behavior to usage scenarios: an interaction-pattern mining approach\n", "abstract": " A key challenge facing IT organizations today is their evolution towards adopting e-business practices that gives rise to the need for reengineering their underlying software systems. Any reengineering effort has to be aware of the functional requirements of the subject system, in order not to violate the integrity of its intended uses. However, as software systems get regularly maintained throughout their lifecycle, the documentation of their requirements often become obsolete or get lost. To address this problem of\" software requirements loss\", we have developed an interaction-pattern mining method for the recovery of functional requirements as usage scenarios. Our method analyzes traces of the run-time system-user interaction to discover frequently recurring patterns; these patterns correspond to the functionality currently exercised by the system users, represented as usage scenarios. The discovered scenarios\u00a0\u2026", "num_citations": "68\n", "authors": ["1249"]}
{"title": "Mining system-user interaction traces for use case models\n", "abstract": " While code understanding is the primary program comprehension activity, it is quite challenging to recognize the application requirements from code, since they have usually been occluded by a set of layers of later implementation decisions. An alternative source of evidence, especially valuable for understanding the purposes for which the application was built, can be the dynamic behavior of the system, and more specifically the system-user interaction. We have developed a method for modeling the application behavior from the user's perspective in the form of use case models, using recorded traces of system-user interaction. We use data mining and pattern matching methods to mine these traces for frequently occurring user tasks. When interesting patterns are discovered, they are augmented with semantic information and they are used to build use case models. We demonstrate a successful application of this\u00a0\u2026", "num_citations": "59\n", "authors": ["1249"]}
{"title": "Modeling the system-user dialog using interaction traces\n", "abstract": " It is generally the case that some user interface (UI) reverse engineering is needed for every non-trivial reengineering project. Typically, this is done through code analysis, which can be very difficult and/or expensive. When code analysis is not a must, as for wrapping purposes, system-user interaction can be an alternative input for the reverse engineering process. In the CelLEST project, we have developed a prototype, called LeNDI (Legacy Navigation Domain Identifier), to test this idea. LeNDI records traces of the legacy screen snapshots and user actions, while the user interacts with the legacy system. Then, it extracts a set of features for every snapshot and employs artificial intelligence methods to build a model of the legacy UI, called the state-transition graph. LeNDI uses two clustering methods to group similar snapshots together as one system screen modeled by one node on the graph. LeNDI uses the user\u00a0\u2026", "num_citations": "54\n", "authors": ["1249"]}
{"title": "Architectural transformations: From legacy to three-tier and services\n", "abstract": " With frequent advances in technology, the need to evolve software arises. Given that in most cases it is not desirable to develop everything from scratch, existing software systems end up being reengineered. New software architectures and paradigms are responsible for major changes in the way software is built. The importance of Service Oriented Architectures (SOAs) has been widely growing over the last years. These present difficult challenges to the reengineering of legacy applications. In this chapter, we present a new methodology to address these challenges. Additionally, we discuss issues of the implementation of the approach based on existing program and model transformation tools and report on an example, the migration of an application from two-tier to three-tier architecture.", "num_citations": "53\n", "authors": ["1249"]}
{"title": "Similarity in programs\n", "abstract": " An overview of the concept of program similarity is presented. It divides similarity into two types-syntactic and semantic-and provides a review of eight categories of methods that may be used to measure program similarity. A summary of some applications of these methods is included. The paper is intended to be a starting point for a more comprehensive analysis of the subject of similarity in programs, which is critical to understand if progress is to be made in fields such as clone detection.", "num_citations": "42\n", "authors": ["1249"]}
{"title": "Understanding web usage for dynamic web-site adaptation: A case study\n", "abstract": " Every day, new information, products and services are being offered by providers on the World Wide Web. At the same time, the number of consumers and the diversity of their interests is increasing. As a result, providers are seeking ways to infer customers' interests and to adapt their Web sites to make the content of interest more easily accessible. Pattern mining is a promising approach in support of this goal. Assuming that past navigation behavior is an indicator of users' interests, then records of this behavior, kept in the form of Web-server logs, can be mined to infer what users are interested in. On that basis, recommendations can be dynamically generated, to help new Web-site visitors find information of interest faster. In this paper, we discuss our experience with pattern mining for dynamic Web-site adaptation. Our particular approach is tailored to \"focused\" Web sites that offer information on a well-defined\u00a0\u2026", "num_citations": "37\n", "authors": ["1249"]}
{"title": "Analysis of Web\u2010usage behavior for focused Web sites: a case study\n", "abstract": " The number of Web users and the diversity of their interests increase continuously; Web\u2010content providers seek to infer these interests and to adapt their Web sites to improve accessibility of the offered content. Usage\u2010pattern mining is a promising approach in support of this goal. Assuming that past navigation behavior is an indicator of the users' interests, then, Web\u2010server logs can be mined to infer what the users are interested in. On that basis, the Web site may be reorganized to make the interesting content more easily accessible or recommendations can be dynamically generated to help new visitors find information of interest faster. In this paper, we discuss a case study examining the effectiveness of sequential\u2010pattern mining for understanding the users' navigation behavior in focused Web sites. This study examines the Web site of an undergraduate course, as an example of a focused Web site that offers\u00a0\u2026", "num_citations": "31\n", "authors": ["1249"]}
{"title": "An experiment in automatic conversion of legacy Java programs to C\n", "abstract": " Source-to-source transformation is an important tool for migrating key legacy programs to modern languages and platforms and giving them new life. Many organizations cannot do without their legacy systems on the one hand, but are stuck in an old technology on the other hand. Converting to a newer programming language can ease integration with modern technologies, give access to a wider developers population and/or lower maintenance costs. Serious language conversion efforts use automated tools, since manual conversion is out of question for nontrivial programs. We present our experiment in building a Java to C# transformer, Java2C#, that partially converts legacy Java code (version 1.1 or earlier) to C#. Java2C# is written in TXL, a language specially designed for program transformation, using tree re-writing. We explore and discuss the challenges and issues to consider when automatically transforming Java to C# and when building automated language transformers in general.", "num_citations": "30\n", "authors": ["1249"]}
{"title": "Forms2Net\u2013migrating oracle forms to microsoft. NET\n", "abstract": " Forms2Net is an ATX Software commercial reengineering tool that automatically converts Oracle Forms applications to the equivalent .NET (C#) ones, with approximately 75% rate of automatic conversion. From the reengineering and transformation theoretical viewpoint, Forms2Net falls in the general category of language-platform conversion tools. As theory and practice indicate, for such tools to be effective, there are two major issues that must be handled: (a) the resolution of the semantic gap between the pair of source-target languages and (b) the resolution of the dependencies (e.g., API dependencies) on functionalities provided by default by the source platform or on programming idiosyncrasies of the source platform (in this case Oracle Forms). This paper presents the important practical aspects of Forms2Net and the underlying technology. We discuss the semantic gap between Oracle Forms and\u00a0\u2026", "num_citations": "24\n", "authors": ["1249"]}
{"title": "Architecture migration driven by code categorization\n", "abstract": " In this paper, we report on the development of a methodology for the evolution of software towards new architectures. In our approach, we represent source code as graphs. This enables the use of graph transformation rules, allowing the automation of the transformation process. Prior to its model representation, the source code is subject to a preparatory step of semi-automatic code annotation according to the contribution of each of its parts in the target architecture. This paper first describes the overall methodology and then focuses on the code annotation and model transformation parts. We also discuss issues of the implementation of the approach based on existing tools.", "num_citations": "15\n", "authors": ["1249"]}
{"title": "Experience in teaching a software reengineering course\n", "abstract": " Software engineering curricula emphasize developing new software systems. Little attention is given to how to change and modernize existing systems, ie, the theory and practice of software maintenance and reengineering. This paper presents the author's experience in teaching software reengineering in a masters-level course at University of Leicester, UK. It presents the course objectives, outline and the lessons learned. The main lessons are: first, there is a big shortage of educational materials for teaching software reengineering. Second, selecting the suitable materials (that balance theory and practice) and the right tool (s) for the level of students and depth of coverage required is a difficult task. Third, teaching reengineering using toy exercises and assignments does not convey the practical aspects of the subject. While, teaching with real, even small size, exercises and assignments, is almost infeasible\u00a0\u2026", "num_citations": "15\n", "authors": ["1249"]}
{"title": "Legacy systems migration in CelLEST\n", "abstract": " Most research on legacy User Interface migration has adopted code understanding as the means for system modeling and reverse engineering. The methodological assumption underlying the CELLEST project is that \u201cthe purpose of system migration is to enable, and possibly optimize, its current uses on a new platform\u201d. This is why CELLEST uses traces of the system-user interaction to reverse engineer the legacy interface, extract its current uses and generate GUIs on new platforms as wrappers for it.", "num_citations": "13\n", "authors": ["1249"]}
{"title": "Software Reengineering at the Architectural Level: Transformation of Legacy Systems\n", "abstract": " In this paper, we put forward a methodology for reengineering the architecture of a legacy software system. The proposed approach is not restricted to any specific source and target architectures, or programming language. It consists in (1) achieving a representation of the source code through its categorization and structuring,(2) transforming it into the new intended architecture, and (3) generating the code for the target platform. First, the code is categorized according to its purpose by pre-defined rules and represented as a model that is an instance of a type graph. Then, this representation is transformed into the intended target architectural paradigm using graph transformation techniques. The generation of the target code is not covered in this report but will be studied in the near future. The approach attempts to address problems that are repeatedly encountered in legacy reengineering industry projects.", "num_citations": "12\n", "authors": ["1249"]}
{"title": "Runtime monitoring of soa applications: Importance, implementations and challenges\n", "abstract": " This paper presents the usages, current status and challenges that face monitoring real runtime SOA applications from both research and industry points of view. SOA application monitoring can be done for collecting statistics, guaranteeing quality of service, generating test cases or other purposes. The key challenges that face SOA monitoring are (1) monitoring overhead and performance degradation, (2) the diversity of supported formats and protocols, which is further complicated by the growth in the number of integrated applications that requires complex logic to be able to monitor individual paths across multiple services, and (3) the distribution of services which is further complicated by deployment in the cloud. We cover academic perspectives that are typically proposals for models or architectures for SOA middleware for monitoring. And we cover as well real monitoring techniques supported in SOA\u00a0\u2026", "num_citations": "10\n", "authors": ["1249"]}
{"title": "On using 3D animation for teaching computer programming in Cairo University\n", "abstract": " In Egypt and in many developing countries, there is a high level of unemployment among youth with high and middle diplomas but with limited IT and computer programming skills or with limited ability to readjust to market changes and new work requirements. This is sometimes attributed to a psychological barrier (e.g., fear or intimidation) to IT and computer programming. In this paper, we study the impact of a novel teaching method, namely the Alice 3D learning environment, proposed elsewhere for teaching computer programming concepts to novice users in an easy and attractive way. We conducted an experiment to measure the effectiveness of Alice by using it to teach programming concepts to first-level students in the Faculty of Computers and Information, Cairo University in the academic year 2008-2009. This was done before introducing them to a high-level language. We then conducted a survey to\u00a0\u2026", "num_citations": "9\n", "authors": ["1249"]}
{"title": "Automatic short answer scoring based on paragraph embeddings\n", "abstract": " Automatic scoring systems for students\u2019 short answers can eliminate from instructors the burden of grading large number of test questions and facilitate performing even more assessments during lectures especially when number of students is large. This paper presents a supervised learning approach for short answer automatic scoring based on paragraph embeddings. We review significant deep learning based models for generating paragraph embeddings and present a detailed empirical study of how the choice of paragraph embedding model influences accuracy in the task of automatic scoring.", "num_citations": "7\n", "authors": ["1249"]}
{"title": "Code clone detection using sequential pattern mining\n", "abstract": " This paper presents a new technique for clone detection using sequential pattern mining titled EgyCD. Over the last decade many techniques and tools for software clone detection have been proposed such as textual approaches, lexical approaches, syntactic approaches, semantic approaches\u2026, etc. In this paper, we explore the potential of data mining techniques in clone detection. In particular, we developed a clone detection technique based on sequential pattern mining (SPM). The source code is treated as a sequence of transactions processed by the SPM algorithm to find frequent itemsets. We run three experiments to discover code clones of Type I, Type II and Type III and for plagiarism detection. We compared the results with other established code clone detectors. Our technique discovers all code clones in the source code and hence it is slower than the compared code clone detectors since they discover few code clones compared with EgyCD.", "num_citations": "5\n", "authors": ["1249"]}
{"title": "Plagiarism detection using sequential pattern mining\n", "abstract": " This research presents a new technique for plagiarism detection using sequential pattern mining titled EgyCD. Over the last decade many techniques and tools for software clone detection have been proposed such as textual approaches, lexical approaches, syntactic approaches, semantic approaches\u2026, etc. In this paper, the research explores the potential of data mining techniques in plagiarism detection. In particular, the research proposed a plagiarism technique based on sequential pattern mining (SPM), words/statements are treated as a sequence of transactions processed by the SPM algorithm to find frequent itemsets. The research submits an experiment to discover copy/paste in the text source and it gave good results in a reasonable and acceptable time.", "num_citations": "5\n", "authors": ["1249"]}
{"title": "The Presence, Trends, and Causes of Security Vulnerabilities in Operating Systems of IoT\u2019s Low-End Devices\n", "abstract": " Internet of Things Operating Systems (IoT OSs) run, manage and control IoT devices. Therefore, it is important to secure the source code for IoT OSs, especially if they are deployed on devices used for human care and safety. In this paper, we report the results of our investigations of the security status and the presence of security vulnerabilities in the source code of the most popular open source IoT OSs. Through this research, three Static Analysis Tools (Cppcheck, Flawfinder and RATS) were used to examine the code of sixteen different releases of four different C/C++ IoT OSs, with 48 examinations, regarding the presence of vulnerabilities from the Common Weakness Enumeration (CWE). The examination reveals that IoT OS code still suffers from errors that lead to security vulnerabilities and increase the opportunity of security breaches. The total number of errors in IoT OSs is increasing from version to the next, while error density, ie, errors per 1K of physical Source Lines of Code (SLOC) is decreasing chronologically for all IoT Oss, with few exceptions. The most prevalent vulnerabilities in IoT OS source code were CWE-561, CWE-398 and CWE-563 according to Cppcheck,(CWE-119!/CWE-120), CWE-120 and CWE-126 according to Flawfinder, and CWE-119, CWE-120 and CWE-134 according to RATS. Additionally, the CodeScene tool was used to investigate the development of the evolutionary properties of IoT OSs and the relationship between them and the presence of IoT OS vulnerabilities. CodeScene reveals strong positive correlation between the total number of security errors within IoT OSs and SLOC, as well as strong negative\u00a0\u2026", "num_citations": "2\n", "authors": ["1249"]}
{"title": "Investigating the use of deep neural networks for software defect prediction\n", "abstract": " Many software projects are shipped to customers containing defects. Defective software cost money, time, and lives. To reduce this harm, software companies allocate testing and quality assurance budgets. The enormous sizes of modern software pose challenges to traditional testing approaches due to the need for scalability. Defect prediction models have been used to direct testing efforts to probable causes of defects in the software. Early approaches for software defect prediction relied on statistical approaches to classify software modules and decide whether each module is a defect-prone module or not. Lately, many researchers used machine learning techniques to train a model that can classify software modules to defect-prone modules and not defect-prone modules. Starting from the new millennium, neural networks and deep learning won many competitions in machine learning applications. However, the\u00a0\u2026", "num_citations": "2\n", "authors": ["1249"]}
{"title": "Behavio2Auth: Sensor-based Behavior Biometric Authentication for Smartphones\n", "abstract": " Many mobile applications use mobile built-in sensors to provide users with a plethora of services to collect and store a mass of sensitive data. These mobile devices need to be protected from unauthorized access and allow access for legitimate users only. In this paper, the problem of unauthorized access is addressed by identifying the user during activities under two considered scenarios: walking and sitting. We authenticate users continuously and implicitly based on micro-movements by leveraging the typing activity information on the screen. These micro-movements come from the user's typing on the touchscreen while walking or sitting. Accelerometer data were analyzed to capture these micro-movements and build the proposed authentication model, named Behavio2Auth. Assuming that each individual has a distinct movement pattern, this hypothesis is used to differentiate between users. A set of experiments\u00a0\u2026", "num_citations": "2\n", "authors": ["1249"]}
{"title": "Reverse Engineering Legacy User Interfaces Using Interaction Traces\n", "abstract": " \u201cBy the time you will finish your thesis, the systems, tools and/or prototypes that you have developed will be legacy systems and you will need to reverse engineer them in order to understand, migrate and/or reengineer them.\u201d Anonymous", "num_citations": "2\n", "authors": ["1249"]}
{"title": "Beware of the Vulnerability! How Vulnerable are GitHub's Most Popular PHP Applications?\n", "abstract": " The presence of software vulnerabilities is a serious threat to any software project. Exploiting them can compromise system availability, data integrity, and confidentiality. Unfortunately, many open source projects go for years with undetected ready-to-exploit critical vulnerabilities. In this study, we investigate the presence of software vulnerabilities in open source projects and the factors that influence this presence. We analyzed the top 100 open source PHP applications in GitHub using a static analysis vulnerability scanner to examine how common software vulnerabilities are. We also discussed which vulnerabilities are most present and what factors contribute to their presence. We found that 27% of these projects are insecure, with a median number of 3 vulnerabilities per vulnerable project. We found that the most common type is injection vulnerabilities, which made 58% of all detected vulnerabilities. Out of these\u00a0\u2026", "num_citations": "1\n", "authors": ["1249"]}
{"title": "Parallel and distributed code clone detection using sequential pattern mining\n", "abstract": " This research presents a parallel and distributed data mining approach to code clone detection. It aims to prove the value and importance of deploying parallel and distributed computing for real-time large scale code clone detection. It is implemented this approach in a family of clone detectors, called PD EgyCD (Parallel and Distributed Egypt Clone Detector). In this approach, This research builds on an earlier work of the authors for code clone and plagiarism detection using sequential pattern mining by adding parallelism and distribution to our earlier tool EgyCD. Our approach uses data mining through a tailored Apriori-based algorithm for code clone detection. And it uses parallelization and distribution to achieve excellent performance to scale up to clone detection on very large systems. This approach has been implemented as a database application which leverages the capabilities of modern database tools. Two versions have been developed of this distributed technique. The first one uses client-server technique in which all clients and the server deal with only one database. The second one uses agents where each client acts as a separate agent and has its own database and after working on a sub-problem, it submits its partial solution to the server to finally get the complete solution (set of code clones). Experiments show that agents technique is faster than clientserver one. Distribution enhances performance very much. Speed improvement is a function of the number of clients/agents used. Our conclusion is that data mining, combined with parallel and distributed computing, can efficiently be deployed for code clone detection of very\u00a0\u2026", "num_citations": "1\n", "authors": ["1249"]}
{"title": "User Interface Reverse Engineering in support of Mifgration to the Web\n", "abstract": " Legacy systems constitute valuable assets to the organizations that own them. Nowadays, there is an increased demand to make them accessible through the World-Wide-Web motivated by the need to support e-commerce related activities. As a result, the problem of legacy-interface migration is becoming extremely important. In the context of the CELLEST project, we have developed a novel method for migrating legacy user interfaces to web-accessible platforms. Its novelty lies in that it models the system\u2019s dynamic behavior based on traces of the users interaction with the system, instead of focusing on the system code structure. Furthermore it proposes a model of the users\u2019 tasks, in terms of the users\u2019 navigation of the interface and the information they exchange with the system, as the intermediate abstraction on which the forward engineering phase is based. In this paper, we discuss our interface-migration method, we illustrate it with examples and we discuss the results of our experimentation with it.", "num_citations": "1\n", "authors": ["1249"]}