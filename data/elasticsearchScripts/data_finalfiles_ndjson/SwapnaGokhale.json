{"title": "Architecture-based software reliability analysis: Overview and limitations\n", "abstract": " With the growing size and complexity of software applications, research in the area of architecture-based software reliability analysis has gained prominence. The purpose of this paper is to provide an overview of the existing research in this area, critically examine its limitations, and suggest ways to address the identified limitations", "num_citations": "288\n", "authors": ["465"]}
{"title": "Quantifying the closeness between program components and features\n", "abstract": " One of the most important steps towards effective software maintenance of a large complicated system is to understand how program features are spread over the entire system and their interactions with the program components. However, we must first be able to represent an abstract feature in terms of some concrete program components. In this paper, we use an execution slice-based technique to identify the basic blocks which are used to implement a program feature. Three metrics are then defined, based on this identification, to determine quantitatively, the disparity between a program component and a feature, the concentration of a feature in a program component, and the dedication of a program component to a feature. The computations of these metrics are automated by incorporating them in a tool (\u03c7Suds), which makes the use of our metrics immediately applicable in real-life contexts. We demonstrate the\u00a0\u2026", "num_citations": "116\n", "authors": ["465"]}
{"title": "Web robot detection techniques: overview and limitations\n", "abstract": " Most modern Web robots that crawl the Internet to support value-added services and technologies possess sophisticated data collection and analysis capabilities. Some of these robots, however, may be ill-behaved or malicious, and hence, may impose a significant strain on a Web server. It is thus necessary to detect Web robots in order to block undesirable ones from accessing the server. Such detection is also essential to ensure that the robot traffic is considered appropriately in the performance and capacity planning of Web servers. Despite a variety of Web robot detection techniques, there is no consensus regarding a single technique, or even a specific \u201ctype\u201d of technique, that performs well in practice. Therefore, to aid in the development of a practically applicable robot detection technique, this survey presents a critical analysis and comparison of the prevalent detection approaches. We propose a\u00a0\u2026", "num_citations": "106\n", "authors": ["465"]}
{"title": "A simulation approach to structure-based software reliability analysis\n", "abstract": " Structure-based techniques enable an analysis of the influence of individual components on the application reliability. In an effort to ensure analytical tractability, prevalent structure-based analysis techniques are based on assumptions which preclude the use of these techniques for reliability analysis during the testing and operational phases. In this paper, we develop simulation procedures to assess the impact of individual components on the reliability of an application in the presence of fault detection and repair strategies that may be employed during testing. We also develop simulation procedures to analyze the application reliability for various operational configurations. We illustrate the potential of simulation procedures using several examples. Based on the results of these examples, we provide novel insights into how testing and repair strategies can be tailored depending on the application structure to\u00a0\u2026", "num_citations": "105\n", "authors": ["465"]}
{"title": "Human sensing for smart cities\n", "abstract": " Smart cities are powered by the ability to self-monitor and respond to signals and data feeds from heterogeneous physical sensors. These physical sensors, however, are fraught with interoperability and dependability challenges. Moreover, they also cannot shed light on human emotions and factors that impact smart city initiatives. Yet everyday, millions of city dwellers share their observations, thoughts, feelings, and experiences about their city through social media updates. This paper describes how citizens can serve as human sensors in providing supplementary, alternate, and complementary sources of information for smart cities. It presents a methodology, based on a probabilistic language model, to extract the perceptions that may be relevant to smart city initiatives from social media updates. Geo-tagged tweets collected over a two-month period from New York City are used to illustrate the potential of social\u00a0\u2026", "num_citations": "58\n", "authors": ["465"]}
{"title": "Selecting open source software projects to teach software engineering\n", "abstract": " Aspiring software engineers must be able to comprehend and evolve legacy code, which is challenging because the code may be poorly documented, ill structured, and lacking in human support. These challenges of understanding and evolving existing code can be illustrated in academic settings by leveraging the rich and varied volume of Open Source Software (OSS) code. To teach SE with OSS, however, it is necessary to select uniform projects of appropriate size and complexity. This paper reports on our search for suitable OSS projects to teach an introductory SE course with a focus on maintenance and evolution. The search turned out to be quite labor intensive and cumbersome, contrary to our expectations that it would be quick and simple. The chosen projects successfully demonstrated the maintenance challenges, highlighting the promise of using OSS. The burden of selecting projects, however, may\u00a0\u2026", "num_citations": "41\n", "authors": ["465"]}
{"title": "A comparison of web robot and human requests\n", "abstract": " Sophisticated Web robots sport a wide variety of functionality and visiting characteristics, constituting a significant percentage of the requests serviced by a Web server. Unlike human clients that retrieve information off a site by navigating links and ignoring irrelevant information, Web robots may collect many different types of resources, and employ varying navigation strategies to find the knowledge on the site they desire. Thus, the resource request patterns of their visits are unpredictable and cannot be inferred based on our knowledge of human request patterns. In this paper, we perform an analysis on the types of resources requested by Web robots using recent Web logs from an academic Web server. We study the distribution of response sizes and response codes, the types of resources requested, and popularity of resources for requests from Web robots. Throughout, we contrast our findings against human\u00a0\u2026", "num_citations": "40\n", "authors": ["465"]}
{"title": "Performance and availability analysis of an e-commerce site\n", "abstract": " To encourage customers to prefer e-commerce services over their on-site counterparts, it is necessary that e-commerce services be offered with superior performance and availability. A systematic methodology to enable a quantitative analysis of the performance and availability attributes of an e-commerce site can assist in identifying bottlenecks and suggesting strategies for improvement. The development of such a methodology is the subject of this paper. The analysis methodology derives expressions for performance and availability metrics of a site which are important from a customer's perspective, namely, the session length and session availability. The methodology is hierarchical, in the first step expressions are derived for each user group based on the navigational pattern of the group. The navigational pattern of a user group is represented by a customer behavior model graph (CBMG), which is mapped to a\u00a0\u2026", "num_citations": "39\n", "authors": ["465"]}
{"title": "Exploring cost and reliability tradeoffs in architectural alternatives using a genetic algorithm\n", "abstract": " The shifting trends in software systems from custom-built to specification, and homogeneous to object-oriented and component-based, have necessitated the development of new approaches for their analysis and evaluation. Correspondingly, the last few years have seen a number of architecture-based techniques employing analytical methods, simulation and experimentation to characterize the behavior of such systems. Whereas most of the previously reported efforts were focussed on the evaluation of software systems using architecture-based techniques, the utility of these techniques in the design phase to evaluate a set of competing alternatives remains largely unexplored. In this paper, we develop an optimization framework founded on the architecture-based analysis techniques, and describe how the framework can be used to evaluate cost and reliability tradeoffs using a genetic algorithm. The choice of\u00a0\u2026", "num_citations": "39\n", "authors": ["465"]}
{"title": "Efficient software reliability analysis with correlated component failures\n", "abstract": " Correlated component failures (COCOF) may impact the reliability of a software application, and hence these types of failures must be explicitly incorporated into reliability analysis. The influence of COCOF on application reliability must be analyzed within the context of the application architecture. Contemporary reliability analysis approaches that incorporate COCOF, however, cannot scale to even moderate-sized software applications. This paper presents an efficient, scalable approach to analyze the reliability of a component-based software system, considering COCOF within the context of its architecture. The effectiveness of the approach is illustrated through two experimental studies. The results indicate that the approach is simple and efficient, and hence can be applied to large systems to identify correlations that impede system reliability.", "num_citations": "38\n", "authors": ["465"]}
{"title": "Signaling performance of SIP based VoIP: A measurement-based approach\n", "abstract": " VoIP must offer the same level of performance and reliability as PSTN, if it were to replace the traditional PSTN. An important indicator of the performance of VoIP is the signaling or call set up delay. Most of the prior research efforts concerning the signaling performance consider long distance calls routed over public wide area networks. Due to the many outstanding issues associated with using VoIP over public networks, however, VoIP is considered to be a viable option primarily in the case of corporate intranets and tunnels. When VoIP is used over corporate intranets and tunnels, the influence of factors such as workload of end user devices, and the number of proxies along the routing path may be prominent and this influence needs to be determined. Also, corporate VoIP service is likely to be offered across heterogeneous network infrastructures using solutions from multiple vendors. As a result, it is necessary to\u00a0\u2026", "num_citations": "37\n", "authors": ["465"]}
{"title": "Social media enabled human sensing for smart cities\n", "abstract": " Smart city initiatives rely on real-time measurements and data collected by a large number of heterogenous physical sensors deployed throughout a city. Physical sensors, however, are fraught with interoperability, dependability, management and political challenges. Furthermore, these sensors are unable to sense the opinions and emotional reactions of citizens that invariably impact smart city initiatives. Yet everyday, millions of dwellers and visitors of a city share their observations, thoughts, feelings and experiences, or in other words, their perceptions about their city through social media updates. This paper reasons why \u201chuman sensors\u201d, namely, citizens that share information about their surroundings via social media can supplement, complement, or even replace the information measured by physical sensors. We present a methodology based on probabilistic language modeling to extract and visualize such\u00a0\u2026", "num_citations": "31\n", "authors": ["465"]}
{"title": "A non-homogeneous Markov software reliability model with imperfect repair\n", "abstract": " This paper reviews existing non-homogeneous Poisson process (NHPP) software reliability models and their limitations, and proposes a more powerful non-homogeneous Markov model for the fault detection/removal problem. In addition, this non-homogeneous Markov model allows for the possibility of a finite time to repair a fault and for imperfections in the repair process. The proposed scheme provides the basis for decision making both during the testing and the operational phase of the software product. Software behavior in the operational phase and the development test phase are related and the release time formulae are derived. Illustrations of the proposed model are provided.", "num_citations": "29\n", "authors": ["465"]}
{"title": "Static and dynamic distance metrics for feature-based code analysis\n", "abstract": " In this paper we present metrics to determine the distance between the features of a software system. Such a measurement can elucidate how features of the system being examined are close to each other. We first use an execution slice-based technique to identify a set of code (basic blocks in our case) that is used to implement each feature. Then, depending on whether the execution frequency of each block is considered during the construction of such sets of code, a static as well as a dynamic distance is computed for each pair of features. These two types of distance differ in that the former computes the distance between two features only by how these features are implemented in the system, while the latter also takes into account how each feature is executed based on a user's operational profile. In other words, the static distance quantitatively gives the closeness of two features from the system implementation\u00a0\u2026", "num_citations": "28\n", "authors": ["465"]}
{"title": "A classification framework for web robots\n", "abstract": " The behavior of modern web robots varies widely when they crawl for different purposes. In this article, we present a framework to classify these web robots from two orthogonal perspectives, namely, their functionality and the types of resources they consume. Applying the classification framework to a year\u2010long access log from the UConn SoE web server, we present trends that point to significant differences in their crawling behavior.", "num_citations": "27\n", "authors": ["465"]}
{"title": "Modeling interference for apache spark jobs\n", "abstract": " To maximize resource utilization and system throughput, hardware resources are often shared across multiple Apache Spark jobs through virtualization techniques in cloud platforms. However, while the performance of these jobs running in virtualized environment can be negatively affected due to interference caused by resource contention, it is nontrivial to predict the effect of interference on job performance in such settings, which is critical for efficient scheduling of such jobs and performance troubleshooting. To address this challenge, in this paper, we develop analytical models to estimate the effect of interference among multiple Apache Spark jobs running concurrently on job execution time in virtualized cloud environment. We evaluated the accuracy of our models using four real-life applications (e.g., Page rank, K-means, Logistic regression, and Word count) on a 6 node cluster while running up to four jobs\u00a0\u2026", "num_citations": "25\n", "authors": ["465"]}
{"title": "From test count to code coverage using the lognormal failure rate\n", "abstract": " When testing software, both effort and delay costs are related to the number of tests developed and executed. However the benefits of testing are related to coverage achieved and defects discovered. We establish a novel relationship between test costs and the benefits, specifically between the quantity of testing and test coverage, based on the Lognormal Failure Rate model. We perform a detailed study of how code coverage increases as a function of number of test cases executed against the 29 files of an application termed SHARPE. Distinct, known coverage patterns are available for 735 tests against this application. By simulating execution of those tests in randomized sequences we determined the average empirical coverage as a function of number of test cases executed for SHARPE as a whole and for each of its individual files. Prior research suggests the branching nature of software causes code\u00a0\u2026", "num_citations": "25\n", "authors": ["465"]}
{"title": "Software defect rediscoveries: a discrete lognormal model\n", "abstract": " Corrective software maintenance, which consists of fixing defects that escape detection and manifest as field failures, is expensive, yet vital to ensuring customer satisfaction. To allocate and use maintenance resources effectively, it is necessary to understand the defect occurrence phenomenon in the field. A preliminary analysis of the defect occurrence data suggests that software defects vary in rate from corner cases, which may occur only once, to the pervasive, which occur many times. This suggests that the distribution of occurrence counts is heavy-tailed. Theoretical reasons and mounting evidence indicate that the distribution of defect occurrence rates is lognormal. We hypothesize that the distribution of occurrence rates is lognormal, and further hypothesize that the distribution of the number of occurrence counts follows the discrete-lognormal, also known as the Poisson-lognormal. We confirm that hypothesis\u00a0\u2026", "num_citations": "24\n", "authors": ["465"]}
{"title": "Software reliability model with bathtub-shaped fault detection rate\n", "abstract": " This paper proposes a software reliability model with a bathtub-shaped fault detection rate. We discuss how the inherent characteristics of the software testing process support the three phases of the bathtub; the first phase with a decreasing fault detection rate arises from the removal of simple, yet frequent faults like syntax errors and typos; the second phase possesses a constant fault detection rate marking the beginning of functional requirements testing; the third and final code comprehension stage exhibits an increasing fault detection rate because testers are now familiar with the system and can focus their attention on the outstanding and as yet untested portions of code. We also discuss how eliminating one of the testing phases gives rise to the burn-in model, which is a special case of the bathtub model. We compare the performance of the bathtub and burn-in models with the three classical software reliability\u00a0\u2026", "num_citations": "23\n", "authors": ["465"]}
{"title": "Optimal allocation of testing effort considering software architecture\n", "abstract": " The growing dependence of society on software systems places a high premium on their reliable operation. Moreover, the stringent reliability expectations imposed on these systems must be achieved despite their increasing size and complexity, and decreasing resources available for their development and maintenance. To mitigate these dual challenges, a systematic approach to guide the allocation of resources to the components of a software system is necessary. This paper presents an optimization framework which considers the contribution of each component to system reliability to determine the amount of effort to be allocated to each component, towards the ultimate objective of achieving the specified system reliability target with minimal effort. We assume that the contribution of a component to system reliability is governed by two factors: the system architecture, and the effort-reliability relationship of the\u00a0\u2026", "num_citations": "22\n", "authors": ["465"]}
{"title": "Discovering new trends in web robot traffic through functional classification\n", "abstract": " This paper proposes a novel functional classification scheme to understand and analyze web robot traffic. The scheme is rooted in the recognition that the crawling behavior of a robot on a site is primarily governed byits intended purpose or functionality. We apply the classification rules to analyze web server access logs from the University of Connecticut School of Engineering domain. The analysis results indicate how the classification scheme can provide insights into the robot traffic based on their functionality.", "num_citations": "22\n", "authors": ["465"]}
{"title": "Replicators: Transformations to address model scalability\n", "abstract": " In Model Integrated Computing, it is desirable to evaluate different design alternatives as they relate to issues of scalability. A typical approach to address scalability is to create a base model that captures the key interactions of various components (i.e., the essential properties and connections among modeling entities). A collection of base models can be adorned with necessary information to characterize their replication. In current practice, replication is accomplished by scaling the base model manually. This is a time-consuming process that represents a source of error, especially when there are deep interactions between model components. As an alternative to the manual process, this paper presents the idea of a replicator, which is a model transformation that expands the number of elements from the base model and makes the correct connections among the generated modeling elements. The paper\u00a0\u2026", "num_citations": "22\n", "authors": ["465"]}
{"title": "Performance and reliability analysis ofweb server software architectures\n", "abstract": " Our increasing reliance on the information and services provided by modern Web servers mandates that these services be offered with superior performance and reliability. The architecture of a Web server has a profound impact on its performance and reliability. One of the dimensions used to characterize the architecture of a Web server is the processing model employed in the server, which describes the type of process or threading model used to support a Web server operation. The main options for a processing model are process-based, thread-based or a hybrid of the process-based and the thread-based models. These options have unique advantages and disadvantages in terms of their performance and reliability tradeoffs. In this paper we propose an analysis methodology based on the stochastic reward net (SRN) modeling paradigm to quantify the performance and the reliability tradeoffs in the process\u00a0\u2026", "num_citations": "21\n", "authors": ["465"]}
{"title": "Cost constrained reliability maximization of software systems\n", "abstract": " Architecture-based techniques have been largely used for the reliability assessment of software systems. However, these techniques also enable the exploration of cost/reliability tradeoffs and evaluation of a set of competing architectural alternatives. This paper presents an optimization framework based on an evolutionary algorithm (EA) which can be used to explore cost/reliability tradeoffs based on software architecture. Evolutionary algorithm was used as an optimization technique because of the discontinuous search space, usually nonlinear but monotonic relation between the cost and reliability of individual modules comprising the software, and complex software architectures giving rise to nonlinear dependencies between individual module reliabilities and the overall application reliability. We illustrate the use of the EA using a case study, where the results of the EA are compared with those obtained from\u00a0\u2026", "num_citations": "21\n", "authors": ["465"]}
{"title": "An integrated method for real time and offline web robot detection\n", "abstract": " Recent academic and industry reports confirm that web robots dominate the traffic seen by web servers across the Internet. Because web robots crawl in an unregulated fashion, they may threaten the privacy, function, performance, and security of web servers. There is therefore a growing need to be able to identify robot visitors automatically, in offline and in real time, to assess their impact and to potentially protect web servers from abusive bots. Yet contemporary detection approaches, which rely on syntactic log analysis, finding statistical variations between robot and human traffic, analytical learning techniques, or complex software modifications may not be realistic to implement or remain effective as the behavior of robots evolve over time. Instead, this paper presents a novel detection approach that relies on the differences in the resource request patterns of web robots and humans. It rationalizes why differences\u00a0\u2026", "num_citations": "20\n", "authors": ["465"]}
{"title": "Queuing models for field defect resolution process\n", "abstract": " This paper explores a novel application of queuing theory to the corrective software maintenance problem to support quantitative balancing between resources and responsiveness. Initially, we provide a detailed description of the states a defect traverses from find to fix and a definition and justification of mean time to resolution as a useful process metric. We consider the effect of queuing system structures, priority levels and priority disciplines on the differential mean times to resolution of defects of different severities. We find that modeling the defect resolution capacity of a software engineering group as n identical M/M/1 servers provides a flexible and realistic approximation to the queuing behavior of four different organizations. We consider three queuing disciplines. Though purely preemptive and non-preemptive priority disciplines may be suited for other groups, our data was best fit by a mixed discipline, one in\u00a0\u2026", "num_citations": "20\n", "authors": ["465"]}
{"title": "Performance analysis of an asynchronous web server\n", "abstract": " Concurrency can be implemented in a Web server using synchronous and asynchronous mechanisms offered by the underlying operating system. Compared to the synchronous mechanisms, the asynchronous mechanisms are attractive because they provide the benefit of concurrency while alleviating much of the overhead and complexity of multithreading. The proactor pattern in middleware, which effectively encapsulates the asynchronous mechanisms supported by the operating system, can be used to implement a high performance Web server. In this paper, we present a queuing model of an asynchronous Web server implemented using the proactor pattern. We then describe a decomposition strategy to enable the application of the model in practical scenarios. We demonstrate the use of the model to guide configuration and provisioning decisions with several examples", "num_citations": "20\n", "authors": ["465"]}
{"title": "Software reliability analysis incorporating second-order architectural statistics\n", "abstract": " Architecture-based techniques for reliability assessment of software applications have received increased attention in the past few years due to the advent of component-based software development paradigm. Most of the prior research efforts in architecture-based analysis use the composite solution approach to solve the architecture-based models in order to estimate application reliability. Though the composite solution approach produces an accurate estimate of application reliability, it suffers from several drawbacks. The most notable drawback of the composite solution approach is that it does not allow an analysis of the sensitivity of the application reliability to the reliabilities of the components comprising the application and the application structure. The hierarchical solution approach on the other hand, has the potential of overcoming the drawbacks of the composite approach. However, in the present form, the\u00a0\u2026", "num_citations": "19\n", "authors": ["465"]}
{"title": "Integrating open source software into software engineering curriculum: Challenges in selecting projects\n", "abstract": " Software Engineering (SE) projects that emphasize maintenance and evolution can emulate industrial challenges and prepare students for careers in the software industry. Designing maintenance-centric SE projects, however, is difficult because software code upon which these projects must be based is not readily available. Open Source Software (OSS) can alleviate this issue by offering a rich and varied volume of code. This rich diversity of OSS projects, however, presents the greatest hurdle in seamlessly selecting suitable projects for integration. To better understand the scope of this diversity, initially, we propose to manually select uniformly difficult projects of appropriate complexity. Ultimately, based on the experiences and insights acquired through the manual selection, we envision the development of a systematic methodology based on software metrics to ease the project selection process. Such a\u00a0\u2026", "num_citations": "18\n", "authors": ["465"]}
{"title": "Heuristic component placement for maximizing software reliability\n", "abstract": " In this chapter, we present a methodology for architecture-based software reliability analysis considering interface failures. The methodology generates an analytical reliability function that expresses application reliability in terms of the reliabilities and visit statistics of the components and interfaces comprising the application. Based on the analytical reliability function, we then present an optimization approach that produces a desirable deployment configuration of the application components given the application architecture and the component and interface reliabilities, subject to two types of constraints. The first type of constraint is the node size constraint and is concerned with the physical limit of the nodes, where a single node cannot accommodate more than a certain maximum number of components. The second type of constraint is the component location constraint, and is concerned with component\u00a0\u2026", "num_citations": "18\n", "authors": ["465"]}
{"title": "Distributed QoS routing for backbone overlay networks\n", "abstract": " In recent years, overlay networks have emerged as an attractive alternative for supporting value-added services. Due to the difficulty of supporting end-to-end QoS purely in end-user overlays, backbone overlays for QoS support have been proposed. In this paper, we describe a backbone QoS overlay network architecture for scalable, efficient and practical QoS support. In this architecture, we advocate the notion of QoS overlay network (referred to as QSON) as the backbone service domain. The design of QSON relies on well-defined business relationships between the QSON provider, network service providers and end users. A key challenge in making QSON a reality consists of efficiently determining routes for end user QoS flows based on the service level agreements between the QSON provider and network service providers. In this paper, we propose and present a scalable and distributed QoS routing\u00a0\u2026", "num_citations": "18\n", "authors": ["465"]}
{"title": "Accurate reliability prediction based on software structure\n", "abstract": " ACCURATE RELIABILITY PREDICTION BASED ON SOFTWARE STRUCTURE Swapna S. Gokhale Dept. of Computer Science and Engineering Univ. of Connecticut Storrs, CT 06269 Email: ssg@engr.uconn.edu Phone: (860) 486-2772 ABSTRACT Software reliability growth models (SRGMs) are inade- quate to assess the reliability of modern, heterogeneous, componentbased software systems since these models treat the system as a black box and model its input/output behavior without looking into its internal structure. De- velopment of techniques to assess the reliability of a componentbased software system (which may be assem- bled from a variety of components, some picked offthe shelf, some developed inhouse and some developed con- tractually), based on its structure is thus absolutely essen- tial. Most of the prior efforts in the area of structurebased reliability assessment use the composite solution to the '\u2026", "num_citations": "18\n", "authors": ["465"]}
{"title": "Architecture-based software reliability with error propagation and recovery\n", "abstract": " Most of the contemporary architecture-based software reliability analysis approaches assume that a component failure leads to system failure. These approaches ignore the positive impact of error recovery methods incorporated in the components, which may allow the components to recover from propagated errors, on system reliability. Thus, the system reliability estimate produced by these approaches is pessimistic. This paper presents an approach to assess the error recovery methods embedded in the system components on system reliability, within the context of the system architecture. The proposed approach enjoys low model complexity and scalability, which fosters its application to systems with a large number of components. The results indicate that the approach identifies the critical components that should be equipped with robust error recovery mechanisms to improve system reliability efficiently.", "num_citations": "17\n", "authors": ["465"]}
{"title": "Evaluating an early software engineering course with projects and tools from open source software\n", "abstract": " We developed a software engineering course that emphasizes code maintenance and evolution by having students reverse engineer and modify open-source projects. To evaluate whether this course had the desired effects on student learning, we analyze pre-and post-course survey data using qualitative methods. This analysis, in combination with other data, suggests that the students gained an appreciation and understanding of software maintenance, documentation, and tool use.", "num_citations": "17\n", "authors": ["465"]}
{"title": "A multiplicative model of software defect repair times\n", "abstract": " We hypothesize that software defect repair times can be characterized by the Laplace Transform of the Lognormal (LTLN) distribution. This hypothesis is rooted in the observation that software defect repair times are influenced by the multiplicative interplay of several factors, and the lognormal distribution is a natural choice to model rates of occurrence of such phenomenon. Conversion of the lognormal rate distribution to an occurrence time distribution yields the LTLN. We analyzed a total of more than 10,000 software defect repair times collected over nine products at Cisco Systems to confirm our LTLN hypothesis. Our results also demonstrate that the LTLN distribution provides a statistically better fit to the observed repair times than either of the two most widely used repair time distributions, namely, the lognormal and the exponential. Moreover, we show that the repair times of subsets of defects, partitioned\u00a0\u2026", "num_citations": "17\n", "authors": ["465"]}
{"title": "Software failure rate and reliability incorporating repair policies\n", "abstract": " Reliability of a software application, its failure rate and the residual number of faults in an application are the three most important metrics that provide a quantitative assessment of the failure characteristics of an application. Typically, one of many stochastic models known as software reliability growth models (SRGMs) is used to describe the failure behavior of an application in its testing phase, and obtain an estimate of the above metrics. In order to ensure analytical tractability, SRGMs are based on an assumption of instantaneous repair and thus the estimates of the metrics obtained using SRGMs tend to be optimistic. In practice, fault repair activity consumes a nonnegligible amount of time and resources. Also, repair may be conducted according to many policies which are reflective of the schedule and budget constraints of a project. A few research efforts that have sought to incorporate repair into SRGMs are\u00a0\u2026", "num_citations": "17\n", "authors": ["465"]}
{"title": "Quantifying the variance in application reliability\n", "abstract": " A notable drawback of the existing architecture-based reliability assessment techniques is that they only obtain a point estimate of application reliability and do not attempt to quantify the variance in the estimate. The variance in the reliability estimate of an application represents the risk associated with the estimate. Ideally, the variance should be zero, but in practice it is inevitable due to the following two factors: (i) variances in the reliability estimates of components comprising the application, and (ii) architectural characteristics of the application. Quantifying the variance in the reliability estimate of an application provides an indication of the degree of risk associated with the estimate, and can also suggest an appropriate variance reduction strategy. We present a technique to quantify the variance in the reliability estimate of an application based on its architecture. Our technique generates analytical functions which\u00a0\u2026", "num_citations": "17\n", "authors": ["465"]}
{"title": "Analysis of software reliability and performance\n", "abstract": " With the steadily growing power and reliability of hardware, software has been identified as a major stumbling block in achieving desired levels of system dependability. Conventional approaches for the assessment of software reliability suffer from several limitations and are based on various stringent and unrealistic assumptions. The emphasis of this dissertation is to relax some of the stringent and unrealistic assumptions underlying the software reliability models, discuss some of their limitations and propose new approaches to overcome these limitations. In particular, we focus on the following three objectives:", "num_citations": "16\n", "authors": ["465"]}
{"title": "Importance measures for modular software with uncertain parameters\n", "abstract": " Importance measures provide a sense of the relative priorities of components and can be used to guide the allocation of resources for cost\u2010effective improvement of system reliability starting from the early phases. Uncertainties in model parameters, prevalent in the early phases, can introduce errors into these measures, and hence, importance assessment must account for these parametric uncertainties. In this paper, a framework for importance assessment of a software system within the context of its architecture is proposed. The framework includes two methods; the first one systematically quantifies the confidence intervals in the model parameters, and the second one offers an analytical approach for importance analysis. The two methods in conjunction address the issue of importance assessment in the face of parametric uncertainties. Illustration of the framework using a banking application demonstrates the\u00a0\u2026", "num_citations": "15\n", "authors": ["465"]}
{"title": "Linux bugs: Life cycle, resolution and architectural analysis\n", "abstract": " Efforts to improve application reliability can be irrelevant if the reliability of the underlying operating system on which the application resides is not seriously considered. An important first step in improving the reliability of an operating system is to gain insights into why and how the bugs originate, contributions of the different modules to the bugs, their distribution across severities, the different ways in which the bugs may be resolved and the impact of bug severities on their resolution times. To acquire this insight, we conducted an extensive analysis of the publicly available bug data on the Linux kernel over a period of seven years. We also justify and explain the statistical bug occurrence trends observed from the data, using the architecture of the Linux kernel as an anchor. The statistical analysis of the Linux bug data suggests that the Linux kernel may draw significant benefits from the continual reliability\u00a0\u2026", "num_citations": "15\n", "authors": ["465"]}
{"title": "Cloud incident data: An empirical analysis\n", "abstract": " This paper presents an empirical analysis of cloud incidents reported in the Cloutage.org database. The trend, causes, and impact of three types of incidents, namely, Outage, Vulnerability, and, failure during automatic updates (Auto Fail) were examined. Service availability was also analyzed based on the outage duration data. The analysis suggested that: (i) Outages and Vulnerabilities grow exponentially, while Auto Fail incidents show only a linear increase, (ii) Outages are caused by various sources of failures, Vulnerabilities primarily due to the lack of filtering inputs and most Auto Fail incidents are false positives, (iii) Many outages affected multiple, related services, some cascaded into additional ones during resolution, and the impact of some transcended organizational boundaries, and (iv) Availability of cloud services is less than 99%, and for free services such as Email it could be as low as 84%. The paper\u00a0\u2026", "num_citations": "14\n", "authors": ["465"]}
{"title": "Software reliability with architectural uncertainties\n", "abstract": " Architecture-based software reliability analysis can provide early identification of critical components which can then be targeted for cost-effective reliability improvement of the application. However, an important challenge in conducting this analysis early in the life cycle is that it is nearly impossible to estimate the architectural and component parameters with certainty. The issue of estimating software application reliability in the presence of uncertain component reliabilities has been addressed in the previous research. In this paper we consider the estimation of software reliability in the presence of architectural uncertainties. We present a methodology to estimate the confidence levels in the architectural parameters using limited testing or simulation data based on the theory of confidence intervals of the multinomial distribution. The sensitivity of the system reliability to uncertain architectural parameters can then be\u00a0\u2026", "num_citations": "14\n", "authors": ["465"]}
{"title": "Model-driven generative techniques for scalable performability analysis of distributed systems\n", "abstract": " The ever increasing societal demand for the timely availability of newer and feature-rich but highly dependable network-centric applications imposes the need for these applications to be constructed by the composition, assembly and deployment of off-the-shelf infrastructure and domain-specific services building blocks. Service oriented architecture (SOA) is an emerging paradigm to build applications in this manner by defining a choreography of loosely coupled building blocks. However, current research in SOA does not yet address the per for mobility (i.e., performance and dependability) challenges of these modern applications. Our research is developing novel mechanisms to address these challenges. We initially focus on the composition and configuration of the infrastructure hosting the individual services. We illustrate the use of domain-specific modeling languages and model weavers to model infrastructure\u00a0\u2026", "num_citations": "14\n", "authors": ["465"]}
{"title": "Optimal software release time incorporating fault correction\n", "abstract": " The \"stopping rule\" problem which involves determining an optimal release time for a software application at which costs justify the stop test decision has been addressed by several researchers. However, most of these research efforts assume instantaneous fault correction, an assumption that underlies many software reliability growth models, and hence provide optimistic predictions of both the cost at release and the release time. In this paper, we present an economic cost model which takes into consideration explicit fault correction in order to provide realistic predictions of release time and release cost. We also present a methodology to compute the failure rate of the software in the presence of fault correction, which is necessary in order to apply the cost model. We illustrate the utility of the cost model to provide realistic predictions of release time and cost with a case study.", "num_citations": "14\n", "authors": ["465"]}
{"title": "Software application design based on architecture, reliability and cost\n", "abstract": " In this paper we present an optimization framework based on an evolutionary algorithm to design a modular software application taking into account its architecture, reliability and cost. The specific design problem that we address is the reliability maximization of a software application subject to a specified cost constraint. Evolutionary algorithm (EA) is used as an optimization technique because of the discontinuous search space, usually nonlinear but monotonic relation between cost and reliability of individual modules comprising the software application and complex software architectures giving rise to nonlinear dependencies between individual module reliabilities and the overall application reliability. We demonstrate how the EA can be effectively and efficiently to design a software application using three case studies.", "num_citations": "13\n", "authors": ["465"]}
{"title": "Understanding social effects in online networks\n", "abstract": " Understanding the motives behind people's interactions online can offer sound bases to predict how a social network may evolve and also support a host of applications. We hypothesize that three offline social factors, namely, stature, relationship strength, and egocentricity may also play an important role in driving users' interactions online. Therefore, we study the influence of these three social factors in online interactions by analyzing the transitivity in triads or three-way relationships among users. Analyzing transitivity through the lens of triad census for four popular social networks, namely, Facebook, Twitter, YouTube and Slashdot, we find that: (i) users' interactions are largely influenced by intermediary relations, which enhances the mediators' stature; (ii) the strength of offline relationships plays a salient role in transitivity of relations online; and (iii) egocentricity, embodied in over-active and popular users, has a\u00a0\u2026", "num_citations": "12\n", "authors": ["465"]}
{"title": "Resource Provisioning in an E-commerce Application\n", "abstract": " It is the responsibility of an e-commerce provider to provision resources to ensure that the performance of the services provided by an e-commerce application is acceptable. A quantitative estimate of the performance expected from the site, obtained from the expected workload imposed on the application, may allow the provider to make informed decisions with respect to the right level of resources so that acceptable service performance may be offered in a cost-effective manner. In this paper we first present a systematic methodology to estimate the performance expected from an e-commerce application based on the representation of the e-commerce workload. Subsequently, we discuss how the estimate of the expected application performance could guide resource provisioning decisions. We illustrate the performance estimation and resource provisioning methodology using the TPC-W benchmark.", "num_citations": "12\n", "authors": ["465"]}
{"title": "An analytical approach to performance analysis of an asynchronous web server\n", "abstract": " Concurrency can be implemented in a Web server using synchronous or asynchronous mechanisms provided by the underlying operating system. Compared to the synchronous mechanisms, asynchronous mechanisms are attractive because they provide the benefit of concurrency while alleviating much of the overhead and compleXity of multi-threading. The Proactor pattern in middleware, which encapsulates the asynchronous mechanisms provided by the operating system, can be used to implement a high performance Web server. The performance eXpectations imposed on a Web server make it necessary to analyze its performance prior to deployment. While the performance of a server can be measured after implementation, design-time performance analysis, conducted early in the life cycle, can enable informed configuration and provisioning choices. A model-based approach can be used for such design\u00a0\u2026", "num_citations": "12\n", "authors": ["465"]}
{"title": "Reliability Analysis of Pipe and Filter Architecture Style.\n", "abstract": " Architecture\u2013based reliability analysis is necessary for a software application that is developed using the component\u2013based software development paradigm. Prevalent architecture\u2013based analysis techniques represent the application architecture by a Markov process, which may be adequate in the context of an application with a general\u2013purpose architecture. The Markov process, however, is not adequate to represent the architecture of an application which follows a specific architecture style, and hence prevalent techniques cannot be used to analyze the reliability of such an application. In this paper we develop a reliability analysis methodology for an application which follows one such architecture style, namely, the pipe and filter architecture style. We consider two variants of the topological organization of the pipes and filters in an application. In the first variant the pipes and filters are organized into a linear topology, whereas the second variant consists of a linear topology with a feedback loop. The objective of the reliability analysis methodology is to develop an analytical function which expresses the overall application reliability in terms of the reliabilities of the pipes and filters, and the characteristics of the topological organization of the pipes and filters. We illustrate the potential of the methodology to obtain a reliability estimate as well as to facilitate sensitivity analysis for a Document Analysis and Understanding Application which follows the pipe and filter architecture style. To the best of our knowledge, this is the first comprehensive effort to combine two mature, yet independent research areas, namely, software reliability analysis and\u00a0\u2026", "num_citations": "12\n", "authors": ["465"]}
{"title": "Estimating system reliability with correlated component failures\n", "abstract": " Correlated component failures, recognised to be a major impediment in the power of redundancy to improve system reliability, need to be factored into the reliability assessment of fault-tolerant systems. In this paper, we present a methodology to derive an approximate analytical expression for mean reliability of on-demand systems with correlated failures. Applying the methodology, we derive expressions for the mean reliability of common redundant structures, namely, k-out-of-n, series-parallel and bridge systems. We illustrate the potential of these expressions to systematically assess the influence of pairwise component correlations on system reliability. An examination of the error between the system reliability estimates obtained from the analytical expressions to those obtained from simulations indicates that our method accurately captures the trends in system reliability and can hence provide a computationally\u00a0\u2026", "num_citations": "11\n", "authors": ["465"]}
{"title": "Effect of unreliable nodes on QoS routing\n", "abstract": " A number of QoS routing algorithms have been proposed to address the dual objective of selecting feasible paths through the network with enough resources to satisfy a connections' QoS request, while simultaneously utilizing network resources efficiently. However, these routing algorithms and the guarantees they provide do not consider the possibility of node and link failures. The failure of a node or a link along a path can disrupt the continuity of an on-going session and potentially terminate the session. Hence the problem of QoS routing should be extended to incorporate reliability and fault-tolerance requirements. We study the impact of unreliable nodes on QoS routing. We describe a scheme to restore the flows that are disrupted due to node failures to alternate paths. Two prioritized restoration policies, one to maximize the number of disrupted flows that can be restored, and the other to maximize the\u00a0\u2026", "num_citations": "11\n", "authors": ["465"]}
{"title": "Teaching software maintenance with open source software: Experiences and lessons\n", "abstract": " Software Engineering (SE) careers are overwhelmingly devoted to the maintenance and evolution of existing, large software systems, where the key challenge is code comprehension especially in the face of inadequate documentation and support. SE courses must thus prepare students to meet this challenge. Open Source Software (OSS) furnishes a valuable source of realistic, sizeable projects for inculcating the appreciation and skills involved in code comprehension and evolution. This paper describes experiences and lessons learnt in using OSS projects to teach an introductory, sophomore/junior-level SE course with an emphasis on comprehension, maintenance, and evolution. Students' reactions and undertakings, acquired through participant observation and homework assignments, suggest that OSS can meaningfully illustrate comprehension and evolution difficulties. Finally, it describes the\u00a0\u2026", "num_citations": "10\n", "authors": ["465"]}
{"title": "Triads, transitivity, and social effects in user interactions on facebook\n", "abstract": " Most computational techniques that analyze Online Social Networks (OSNs) aim to discover patterns in a network's structure and the behavior of its users, but do not seek to understand how people's motives lead to these patterns. Studying the social effects that cause these patterns, however, can produce deeper insights that may transcend a specific network and are generically applicable. Therefore, a more promising approach is to anchor computational techniques to the underlying social effects that can explain the reasons behind why users interact the way they do. In this paper, we discover how the social effects of stature, relationship strength, and egocentricity shape the interactions among Facebook users. These effects are explored through transitivity in triads, which are network units that capture dynamics among triples of users. The analysis suggests that Facebook interactions are influenced by users with\u00a0\u2026", "num_citations": "10\n", "authors": ["465"]}
{"title": "Searching for heavy tails in Web robot traffic\n", "abstract": " This paper presents a study on whether the heavy-tailed trends reported in Web traffic are present in the traffic generated by Web robots. The study is motivated by three factors: (i) a significant volume of Web server traffic can now be attributed to Web robots, (ii) the Web is continuing to evolve into a semantic and service-oriented environment where Web robots will play a central role, and (iii) there are fundamental differences in the way robots and humans visit a site and search for information and these differences may lead to contrasts in the statistical patterns of the robots' requests compared to humans. We analyze Web robot traffic from a two-year access log from a Web server in the academic domain and study whether the response sizes, request inter-arrival times, and inter-session times exhibit heavy-tailed properties. In a multi-faceted analysis of the data we find that the response sizes and request inter-arrival\u00a0\u2026", "num_citations": "10\n", "authors": ["465"]}
{"title": "System availability analysis considering hardware/software failure severities\n", "abstract": " Model-based analysis is a well-established approach to assess the influence of several factors on system availability within the context of system structure. Prevalent availability models in the literature consider all failures to be equivalent in terms of their consequences on system services. In other words, all the failures are assumed to be of the same level of severity. In practice, failures are typically classified into multiple severity levels, where failures belonging to the highest severity level cause a complete loss of service, while failures belonging to levels below the highest level enable the system to operate in a degraded mode. This makes it necessary to consider the influence of failure severities on system availability. In this paper we present a Markov model which considers failure severities of the components of the system in conjunction with its structure. The model also incorporates the repair of the components\u00a0\u2026", "num_citations": "10\n", "authors": ["465"]}
{"title": "Design and implementation of an analytical framework for interference aware job scheduling on apache spark platform\n", "abstract": " Apache Spark is one of the recently popularized open-source platforms that is increasingly being used for large-scale data analytic applications. However, while performance prediction in such systems is important for efficient job scheduling and optimizing resource allocation, interference among multiple Apache Spark jobs running concurrently in a virtualized environment makes it extremely difficult, which is addressed in this paper. Towards that, first, we develop data-driven analytical models to estimate the effect of interference among multiple Apache Spark jobs on job execution time in virtualized cloud environments. Next, we present the design of an interference aware job scheduling algorithm leveraging the developed analytical framework. We evaluated the accuracy of our models using four real-life applications (e.g., Page rank, K-means, Logistic regression, and Word count) on a 6 node cluster while\u00a0\u2026", "num_citations": "9\n", "authors": ["465"]}
{"title": "Detecting web robots using resource request patterns\n", "abstract": " A significant proportion of Web traffic is now attributed to Web robots, and this proportion is likely to grow over time. These robots may threaten the security, privacy, functionality, and performance of a Web server due to their unregulated crawling behavior. Therefore, to assess their impact, it must be possible to accurately detect Web robot requests. Contemporary detection approaches, however, may cease to be effective as the behavior of both robots and humans evolves. In this paper, we present a novel detection approach that is based on the contrasts in the resource request patterns of robots and humans. The proposed scheme, which relies on an invariant behavioral difference between humans and robots, builds on the lessons from contemporary approaches. We demonstrate that the proposed approach can accurately detect Web robots and argue that it is expected to remain effective even as they continue\u00a0\u2026", "num_citations": "9\n", "authors": ["465"]}
{"title": "Security and performance analysis of a passenger screening checkpoint for mass-transit systems\n", "abstract": " During the past decade, the international community has witnessed several attacks on forms of mass transportation such as train stations and subways. The Department of Homeland Security requested that we develop methods to assess the security of mass transit in order to mitigate the vulnerability of the nation's public transportation systems. We present a methodology to quantify the impact of imposing screening on mass transit, which considers both security and delays incurred on the traveling public. We demonstrate the approach through a case study, the Fairfield Metro Station in Fairfield, Connecticut. Our results indicate that rigorous aviation-style screening will slow the flow of passengers drastically. We also show how to use the approach to identify where faster screening technologies can improve passenger throughput while ensuring security. The approach can thus be used to identify areas where\u00a0\u2026", "num_citations": "9\n", "authors": ["465"]}
{"title": "Efficient system reliability with correlated component failures\n", "abstract": " Correlated component failures (CCF) degrade system reliability, and hence, these failures must be explicitly incorporated into the reliability analysis process. Several contemporary efforts consider CCF, however, most of these approaches introduce an exponential number of parameters and are computationally intensive because they require a complete characterization of the joint distribution of the components. As a result, these approaches are not scalable and cannot be applied to large systems. This paper presents an efficient approach to analyze system reliability considering CCF. The approach introduces only a quadratic number of parameters and is computationally efficient. The effectiveness of the approach is illustrated through a series of examples. The results indicate that the approach is both simple and efficient and can be applied to large systems.", "num_citations": "9\n", "authors": ["465"]}
{"title": "Classifying Web Robots by K-means Clustering.\n", "abstract": " Sophisticated Web robots, sporting a variety of functionality and unique traffic characteristics, constitute a significant percentage of request and bandwidth volume serviced by a Web server. To adequately prepare Web servers for this continuous rise in Web robots, it is necessary to gain deeper insights into their traffic properties. In this paper, we propose to classify Web robots according to their workload characteristics, using K-means clustering as the underlying partitioning technique. We demonstrate how our approach can allow an examination of Web robot traffic from new perspectives by applying it to classify Web robots extracted from a year-long server log collected from the Univ. of Connecticut School of Engineering domain.", "num_citations": "9\n", "authors": ["465"]}
{"title": "Software defect repair times: a multiplicative model\n", "abstract": " We analyzed over 10,000 software defect repair times collected for nine products at Cisco Systems, to confirm our hypothesis that software defect repair times can be characterized by the Laplace Transform of the Lognormal (LTLN) distribution. This hypothesis originated from the observation that software defect repair times are influenced by the multiplicative interplay of several factors. The Lognormal distribution is a natural choice to model rates of occurrence of such phenomenon. Conversion of the Lognormal rate distribution to an occurrence time distribution yields the LTLN. Our results also confirm that the LTLN distribution provides a statistically better fit to the observed repair times than either of the two most widely used repair time distributions, the lognormal and the exponential.", "num_citations": "9\n", "authors": ["465"]}
{"title": "Location inference of social media posts at hyper-local scale\n", "abstract": " This paper describes an approach to infer the location of a social media post at a hyper-local scale based on its content, conditional to the knowledge that the post originates from a larger area such as a city or even a state. The approach comprises three components: (i) a discriminative classifier, namely, Logistic Regression (LR) which selects from a set of most probable sub-regions from where a post might have originated, (ii) a clustering technique, namely, k-means, that adaptively partitions the larger geographic region into sub regions based on the density of the posts, and (iii) a range of techniques to extract a set of hyper-local words from the posts to be fed as features to the LR classifier. The approach is evaluated on a large corpus of tweets collected from Twitter over the NYC, Washington DC, and state of Connecticut regions. The results show that our approach can geo-locate tweets within 1:72 km for NYC\u00a0\u2026", "num_citations": "8\n", "authors": ["465"]}
{"title": "Architecture-based reliability analysis with uncertain parameters\n", "abstract": " Architecture-based reliability analysis has gained prominence in the recent years as a way to predict the reliability of a software application during the design phase, before an investment is made in any implementation. To apply this analysis, the parameters comprising the architectural model must be estimated using the limited data and knowledge available during the design phase. These estimates, as a result, are inherently uncertain. Contemporary approaches, however, do not consider these uncertainties, and hence, may produce inaccurate reliability results. This paper presents a Bayesian approach to systematically consider parametric uncertainties in architecture-based analysis. The novelty of this approach lies in determining credible intervals for the model parameters as a function of their posterior distributions. By leveraging these intervals, we illustrate how to:(i) quantify the impact of uncertainty in a specific parameter on the system reliability estimate;(ii) evaluate when a sufficient amount of data has been collected to reduce the uncertainty to an acceptable level; and (iii) assess the impact of prior knowledge regarding the parameters in improving the system reliability estimate.", "num_citations": "8\n", "authors": ["465"]}
{"title": "POSAML: A visual modeling language for middleware provisioning\n", "abstract": " Next generation distributed applications are often hosted on heterogeneous platforms including different kinds of middleware. Due to the applications\u2019 growing functional complexity and their multiple quality of service (QoS) requirements, system developers are increasingly facing a substantial number of middleware provisioning challenges, which include configuring, optimizing and validating the middleware platforms for QoS properties. Traditional techniques for middleware provisioning tend to use non-intuitive, low-level and technology-specific approaches, which are tedious, error prone, and non-reusable across different technologies. Quite often the middleware provisioning activities are carried out by different actors without much interaction among them, which results in an iterative trial-and-error process to provisioning. Higher level abstractions, particularly those that use visual models, are effective in\u00a0\u2026", "num_citations": "8\n", "authors": ["465"]}
{"title": "Performance analysis of the reactor pattern in network services\n", "abstract": " The growing reliance on services provided by software applications places a high premium on the reliable and efficient operation of these applications. A number of these applications follow the event-driven software architecture style since this style fosters evolvability by separating event handling from event demultiplexing and dispatching functionality. The event demultiplexing capability, which appears repeatedly across a class of event-driven applications, can be codified into a reusable pattern, such as the reactor pattern. In order to enable performance analysis of event-driven applications at design time, a model is needed that represents the event demultiplexing and handling functionality that lies at the heart of these applications. In this paper, we present a model of the reactor pattern based on the well-established stochastic reward net (SRN) modeling paradigm. We discuss how the model can be used to\u00a0\u2026", "num_citations": "8\n", "authors": ["465"]}
{"title": "A model-driven performance analysis framework for distributed, performance-sensitive software systems\n", "abstract": " Large-scale, distributed, performance-sensitive software (DPSS) systems from the basis of mission- and often safety-critical applications. DPSS systems comprise of many independent artifacts, such as network/bus interconnects, many coordinated local and remote endsystems, and multiple layers of software. DPSS systems demand multiple, simultaneous predictable performance requirements such as end-to-end latencies, throughput, reliability and security, while also requiring the ability to control and adapt operating characteristics for applications with respect to such features as time, quantity of information, and quality of service (QoS) properties including predictability, controllability, and adaptability of operating characteristics for applications with respect to such features as time, quantity of information, accuracy, confidence and synchronization. All these issues become highly volatile in large-scale DPSS\u00a0\u2026", "num_citations": "8\n", "authors": ["465"]}
{"title": "Accurate local estimation of geo-coordinates for social media posts\n", "abstract": " Associating geo-coordinates with the content of social media posts can enhance many existing applications and services and enable a host of new ones. Unfortunately, a majority of social media posts are not tagged with geo-coordinates. Even when location data is available, it may be inaccurate, very broad or sometimes fictitious. Contemporary location estimation approaches based on analyzing the content of these posts can identify only broad areas such as a city, which limits their usefulness. To address these shortcomings, this paper proposes a methodology to narrowly estimate the geo-coordinates of social media posts with high accuracy. The methodology relies solely on the content of these posts and prior knowledge of the wide geographical region from where the posts originate. An ensemble of language models, which are smoothed over non-overlapping sub-regions of a wider region, lie at the heart of the methodology. Experimental evaluation using a corpus of over half a million tweets from New York City shows that the approach, on an average, estimates locations of tweets to within just 2.15km of their actual positions.", "num_citations": "7\n", "authors": ["465"]}
{"title": "Hierarchical availability analysis of multi-tiered Web applications\n", "abstract": " Multi-tiered Web applications must offer their services with superior availability in order to encourage customers to choose online services over the traditional brick-and-mortar options. A systematic, quantitative analysis is the first step in ensuring that multi-tiered Web applications meet their high availability expectations. This paper proposes a hierarchical, model-based methodology to assess the availability of multi-tiered Web applications. The hierarchical approach partitions the analysis into three levels, which allows a systematic consideration of several factors that are relevant to application availability without encountering the issues of model complexity and intractability. We illustrate the approach via experimentation using the TPC-W benchmark. We also demonstrate how the approach could be used to guide resource provisioning decisions.", "num_citations": "7\n", "authors": ["465"]}
{"title": "Model-based performance analysis using block coverage measurements\n", "abstract": " The primary advantage of model-based performance analysis is its ability to facilitate sensitivity and predictive analysis, in addition to providing an estimate of the application performance. To conduct model-based analysis, it is necessary to build a performance model of an application which represents the application structure in terms of the interactions among its components, using an appropriate modeling paradigm. While several research efforts have been devoted to the development of the theoretical aspects of model-based analysis, its practical applicability has been limited despite the advantages it offers. This limited practical applicability is due to the lack of techniques available to estimate the parameters of the performance model of the application. Since the model parameters cannot be estimated in a realistic manner, the results obtained from model-based analysis may not be accurate.In this paper, we\u00a0\u2026", "num_citations": "7\n", "authors": ["465"]}
{"title": "Model replication: transformations to address model scalability\n", "abstract": " In model\u2010driven engineering, it is often desirable to evaluate different design alternatives as they relate to scalability issues of the modeled system. A typical approach to address scalability is model replication, which starts by creating base models that capture the key entities as model elements and their relationships as model connections. A collection of base models can be adorned with necessary information to characterize a specific scalability concern as it relates to how the base modeling elements are replicated and connected together. In current modeling practice, such a model replication is usually accomplished by scaling the base model manually. This is a time\u2010consuming process that represents a source of error, especially when there are deep interactions between model components. As an alternative to the manual process, this paper presents the idea of automated model replication through a model\u00a0\u2026", "num_citations": "7\n", "authors": ["465"]}
{"title": "Reliability evaluation of reconfigurable conveyor systems\n", "abstract": " Conveyor systems are critical in automation applications such as material handling and packaging. Networked embedded devices, such as tiny microcontrollers that are integrated with sensors, actuators and low power radio transceivers, significantly impact the design of future such systems by offering flexible topologies, reduced wiring costs and distributed controllers. Because the application domains of such systems are critical, these systems are expected to have high reliabilities. As a result, to exploit the benefits offered by the emerging technologies, a study of the reliability of such systems that are regulated by an integrated, embedded, distributed collection of microcontrollers is important. We present a model-based approach to evaluate the reliability of composable conveyor systems based on the fault tree modeling paradigm. We illustrate how the methodology provides valuable guidance to navigate the\u00a0\u2026", "num_citations": "7\n", "authors": ["465"]}
{"title": "Linux bugs: life cycle and resolution analysis\n", "abstract": " Efforts to improve application reliability can fade if the reliability of the underlying operating system on which the application resides is not seriously considered. An important first step in improving the reliability of an operating system is to first gain insights into why and how the bugs originate, contributions of the different modules to the bugs, their distribution across severities, the different ways in which the bug may be resolved and the impact of bug severity on the resolution time. To gain this insight we conducted an extensive analysis of the publicly available bug data on the Linux kernel over a period of seven years. Our observations suggest that the Linux kernel may draw significant benefits from the continual reliability improvement efforts of its developers. These efforts, however, are disproportionately targeted towards popular configurations and hardware platforms, due to which the reliability of these\u00a0\u2026", "num_citations": "7\n", "authors": ["465"]}
{"title": "Importance Measures for a Modular Software System (Short Paper)\n", "abstract": " Importance measures of a system provide a sense of the relative priorities of the components from a system reliability perspective. These measures can thus be used to identify critical components and to guide the allocation of resources so that the system reliability can be improved in a cost effective manner. Importance measures are widely used in many engineered hardware and electro-mechanical systems. Their use in the engineering of software systems, however, is lacking. In this paper we develop an analytical methodology to compute the importance measures of a software system. The analytical treatment facilitates an assessment of the sensitivity of the importance measures of a software system to the uncertainties in its architectural parameters, component reliabilities, and operational profiles. The capability of enabling sensitivity analysis, which is a key strength of our approach, is crucial to the adaptation\u00a0\u2026", "num_citations": "7\n", "authors": ["465"]}
{"title": "Dynamic Scheduling of Multiple Hidden Markov Model-Based Sensors.\n", "abstract": " Complex applications involving threat detection, such as multi-target tracking and unmanned aerial vehicles for surveillance in remote or hostile environments, include heterogeneous sensors, which trade off performance (eg, detection, identification, and tracking accuracies) versus the sensor usage cost (eg, power and bandwidth consumption, distance traveled, risk of exposure, deployment requirements). The objective of dynamic sensor scheduling is to judiciously allocate sensing resources to exploit the individual sensors\u2019 capabilities, while minimizing their usage cost. As an example, consider a target identification scenario where an incoming aircraft needs to be identified as an enemy or a friendly target using active or passive sensors available at a surveillance station [16]. This scenario requires sensor scheduling because active sensors (eg, radar) tend to reveal clues about the location of the surveillance station to a potential enemy aircraft, whereas the more stealthy passive sensors tend to be inaccurate [16]. Thus, in this case, the sensor scheduling algorithm needs to trade-off accuracy versus risk of exposure. As another example, unmanned aerial vehicles (UAVs) are preferred assets for monitoring nearly all the intelligence, surveillance, and reconnaissance (ISR) activities; however, they cannot be deployed in large numbers due to their limited availability. Thus, astute allocation of scarce resources is a major issue in sensor scheduling.In this paper, we consider the sensor scheduling problem faced by an ISR officer of an expeditionary strike group (ESG) in coordinating the use of surveillance assets (sensors) to improve situational\u00a0\u2026", "num_citations": "7\n", "authors": ["465"]}
{"title": "Performance analysis of a middleware demultiplexing pattern\n", "abstract": " A key enabler of the recently adopted, assembly-centric development approach for distributed real-time software systems is QoS-enabled middleware, which provides reusable building blocks in the form of design patterns that codify solutions to commonly recurring problems. These patterns can be customized by choosing an appropriate set of configuration parameters. The configuration options of a pattern exert a strong influence on system performance, which especially for real-time systems is of paramount importance. Despite this significant influence, currently there are no techniques available to analyze performance at design time, prior to the use of a pattern in a system. Many software systems are based on an event-driven paradigm, primarily because it fosters evolvability and composability. The event demultiplexing and dispatching capabilities that are uniform across such systems are encapsulated in the\u00a0\u2026", "num_citations": "7\n", "authors": ["465"]}
{"title": "Web server performance analysis\n", "abstract": " The Web Wide Web (WWW) has experienced an exponential growth during the last ten years and in the current era Web servers represent one of the most important sources of information and services. Web servers, which are typically based on the HTTP protocol running over TCP/IP, are expected to serve millions of transaction requests per day at an acceptable level of performance. Due to the stringent performance expectations imposed on a Web server, it is important to analyze its performance for different levels of load prior to deployment. Model-based analysis, which consists of capturing the relevant aspects of a Web server into an appropriate model, validating the model and then using the validated model to predict the performance for different settings and load levels can be used for this purpose. A number of research efforts have focused on performance modeling and analysis of Web servers. Slothouber\u00a0\u2026", "num_citations": "7\n", "authors": ["465"]}
{"title": "Dynamic code coverage metrics: a lognormal perspective\n", "abstract": " The logical interrelationship between different code coverage types has been well studied, but less so their evolution through time or test. We study the dynamic relationship of four coverage types, namely, block, decision, c-use and p-use by comparing their growth using empirical coverage data generated from extensive testing of a software application with 35 KLOC of code. Our results indicate that as testing increases, the growth trends for each coverage type are surprisingly similar. Not only is each trend consistent with an underlying lognormal distribution of event rate, but also the parameters of the fitted lognormal distributions are closely related. Within the limits of the data, we find quantitative relations between the four coverage types. The paper thus takes a significant step in linking concepts from prior studies of software test sufficiency, test efficiency, and reliability in the context of software execution", "num_citations": "7\n", "authors": ["465"]}
{"title": "Performability analysis of a pipeline software architecture\n", "abstract": " An architecture style defines a commonly occurring pattern of the structural organization of components and connectors such that the impact of the pattern on the different non functional attributes is known. It is possible to make a qualitative assessment of whether a specific style is conducive to the nonfunctional attribute of performance based on the knowledge and experience in the use of the style. It is necessary however, that such a qualitative judgment be substantiated by a quantitative estimate obtained using a systematic performance analysis approach, since architecture styles are likely to be used in domains where application performance will play a critical role in ensuring its widespread use. Recognizing this need, several research efforts have focused on the development of quantitative performance analysis approaches for general purpose software architectures as well as for architecture styles. A notable\u00a0\u2026", "num_citations": "7\n", "authors": ["465"]}
{"title": "An efficient method to schedule tandem of real-time tasks in cluster computing with possible processor failures\n", "abstract": " Each computer system in operation today is certain to experience faults in its operational lifetime. There is more than one source, which can cause faults, for example environmental factors, communication, interaction with users, or hardware or software design flaws randomly build into the system. A reliable system is the one that would continue operation albeit in a degraded mode despite the presence of faults. In this paper, we introduce an efficient method to schedule a tandem of real-time tasks in cluster computing with possible processor failures. The method is based on developing an objective function that combines different scheduling goals. This objective function is used to guide the search algorithm to find an efficient solution. The proposed algorithm consists of three terms, reliability, deadline and tasks grouping to minimize remote communication among tasks. The proposed method creates a scheduling\u00a0\u2026", "num_citations": "7\n", "authors": ["465"]}
{"title": "A Model Driven Approach Towards Improving the Performance of Apache Spark Applications\n", "abstract": " Apache Spark applications often execute in multiple stages where each stage consists of multiple tasks running in parallel. However, prior efforts noted that the execution time of different tasks within a stage can vary significantly for various reasons (e.g., inefficient partition of input data), and tasks can be distributed unevenly across worker nodes for different reasons (e.g., data co-locality). While these problems are well-known, it is nontrivial to predict and address them effectively. In this paper we present an analytical model driven approach that can predict the possibility of such problems by executing an application with a limited amount of input data and recommend ways to address the identified problems by repartitioning input data (in case of task straggler problem) and/or changing the locality configuration setting (in case of skewed task distribution problem). The novelty of our approach lies in automatically\u00a0\u2026", "num_citations": "6\n", "authors": ["465"]}
{"title": "Learning facilitating leadership\n", "abstract": " This paper explains how engineering students at a Danish university acquired the necessary skills to become emergent facilitators of organisational development. The implications of this approach are discussed and related to relevant viewpoints and findings in the literature. The methodology deployed for this paper is empirical and conceptual. A specific facilitation project carried out by six international engineering students is presented. The importance of combining cognitive, emotional and synergistic skills is highlighted on the basis of this example, the authors\u2019 extensive experience in teaching facilitation and the literature. These types of skills are most effectively acquired by combining conceptual lectures, classroom exercises and the facilitation of groups in a real-life context. The paper also reflects certain \u2018shadow sides\u2019 related to facilitation observed by the students and discussed in the literature. The\u00a0\u2026", "num_citations": "6\n", "authors": ["465"]}
{"title": "Teaching software engineering from a maintenance-centric view using open-source software\n", "abstract": " Software engineering (SE) careers are disproportionately devoted towards maintaining and evolving existing large systems, rather than building them from ground up. To address this focus, we have developed a maintenance-centric SE course that provides students experience in the maintenance and evolution of realistic software projects. For this purpose, we use Open-Source Software (OSS) which is freely available, as a source of realistic software projects.", "num_citations": "6\n", "authors": ["465"]}
{"title": "Leveraging UML for security engineering and enforcement in a collaboration on duty and adaptive workflow model that extends NIST RBAC\n", "abstract": " To facilitate collaboration in the patient-centered medical home (PCMH), our prior work extended the NIST role-based access control (RBAC) model to yield a formal collaboration on duty and adaptive workflow (COD/AWF) model. The next logical step is to place this work into the context of an integrated software process for security engineering from design through enforcement. Towards this goal, we promote a secure software engineering process that leverages an extended unified modeling language (UML) to visualize COD/AWF policies to achieve a solution that separates concerns while still providing the means to securely engineer dynamic collaborations for applications such as the PCMH. Once defined, these collaboration UML diagrams can be utilized to generate the corresponding aspect oriented policy code upon which the enforcement mechanism can be applied to at runtime.", "num_citations": "6\n", "authors": ["465"]}
{"title": "Software reliability models incorporating testing effort\n", "abstract": " Explicitly relating the effectiveness of fault detection to the effort expended in testing, achieved by incorporating testing effort into software reliability models has been the focus of many research efforts. Although the literature is replete with these \u201ctesting effort models,\u201d their development appears to be ad hoc and disconnected. The objective of this survey is to propose a framework to classify testing effort models, aimed at identifying their commonalities and highlighting their differences. We conclude the article with a brief discussion of the limitations of the prevalent works in this domain, which also identify directions for future research.", "num_citations": "6\n", "authors": ["465"]}
{"title": "Fuzzy fault tree analysis of crane wire rope\n", "abstract": " Wire rope is a very useful and long lasting structural element when properly used and maintained. For a failure such as that of crane wire rope, it is often very difficult to estimate precise failure rates or failure probabilities of individual components or failure events. Conventional fault tree analysis (FTA) is based on the probability assumption. When the failure probability of a system is extremely small or necessary statistical data from the system is scarce, it is very difficult or impossible to evaluate its reliability and safety with conventional fault tree analysis (FTA) techniques. To overcome this disadvantage, fuzzy sets theory is introduced. The reliability of basic events is considered as type fuzzy numbers with weighted exponents. A weighted exponent represents the assuring measure of the decision-maker to the membership functions of basic events. From the logical relationship between different events in the fault tree and fuzzy operators AND and OR, the fuzzy probability of the top event is obtained. Finally, fuzzy fault tree analysis of crane wire rope is given to illustrate the proposed method.", "num_citations": "6\n", "authors": ["465"]}
{"title": "Software reliability analysis with component-level fault tolerance\n", "abstract": " This paper presents, an approach to assess the reliability of an application taking into consideration component-level fault tolerance is developed. Fault tolerance mechanisms represent an effective way of improving the reliability of a software application. When fault tolerance is employed for only a subset of application components it is necessary to analyze the reliability of the application taking into consideration the impact of component-level fault tolerance within the context of application architecture.", "num_citations": "6\n", "authors": ["465"]}
{"title": "Integrating path computation and precomputation for quality-of-service provisioning\n", "abstract": " QoS provisioning involves reserving resources along a suitable path through the network for the entire lifetime of a connection. To assess the suitability of the possible paths, their QoS metrics which indicate the available resources, should be computed. The QoS metrics can be computed on-demand after the arrival of a QoS connection request or precomputed. On-demand path computation is superior to path precomputation in that suitable paths may be determined using most up to date network state. However, the delay incurred in determining a suitable path on demand may be unacceptable. Path precomputation overcomes the drawback of long delay of on-demand path computation. In addition, path precomputation may be preferred over on-demand path computation for scalability issues, when the number of expected QoS connection requests is very large. Typically, path precomputation needs to be\u00a0\u2026", "num_citations": "6\n", "authors": ["465"]}
{"title": "Signaling performance of SIP based VoIP\n", "abstract": " Swapna Gokhale and Jijun Lu Dept. of Computer Science and Engineering University of Connecticut Storrs, CT 06269, USA Email:{ssg, jil03003}@ engr. uconn. edu", "num_citations": "6\n", "authors": ["465"]}
{"title": "Teaching software engineering from a maintenance-centric view\n", "abstract": " Software engineering (SE) careers are overwhelmingly devoted to maintenance and evolution of existing large software systems, rather than building such systems from ground up. Code comprehension, especially in the face of inadequate documentation and support, is a key challenge in efficiently conducting these maintenance activities. Therefore, SE courses in the computing curricula must adequately prepare the students to meet this challenge. We believe that Open Source Software (OSS) furnishes a useful source of realistic, sizeable projects for inculcating the appreciation and skills useful for comprehension and maintenance. We evaluate and observe that an OSS-based SE course emphasizing maintenance and evolution expanded students' appreciation of the difficulty in comprehending existing code and designs, especially when not well documented. We hope that this learned appreciation will\u00a0\u2026", "num_citations": "5\n", "authors": ["465"]}
{"title": "Reliable operating systems: Overview and techniques\n", "abstract": " Traditional operating systems\u2019 architectures are primarily focused on achieving superior performance, albeit at the expense of sacrificing reliability. This approach is clearly inadequate, given our growing dependence on computer systems for essential and critical services. Reliability should thus be considered as a first-class citizen alongside functionality and performance, while designing future operating systems. The present paper summarizes the current trends and best practices employed to improve the reliability of operating systems. We also draw attention to the limitations and drawbacks of the prevalent approaches, discuss outstanding issues and propose some promising research directions, which may be explored to build highly reliable operating systems.", "num_citations": "5\n", "authors": ["465"]}
{"title": "Quantifying the impact of architectural uncertainties on system reliability\n", "abstract": " QUANTIFYING THE IMPACT OF ARCHITECTURAL UNCERTAINTIES ON SYSTEM RELIABILITY Lance Fiondella and Swapna S. Gokhale Dept. of Computer Science and Engineering University of Connecticut Storrs, Connecticut, USA {l\ufb01ondella,ssg}@engr.uconn.edu ABSTRACT Architecture-based software reliability analysis can pro- vide early identi\ufb01cation of critical components for cost- effective reliability improvement. However, an important challenge in conducting this analysis early in the life cycle is that it is nearly impossible to estimate the architectural and component parameters with certainty. It is then neces- sary to determine the impact of parametric uncertainties on the analysis results, prior to basing resource provisioning and allocation decisions on these results. Earlier research focused on addressing the impact of uncertain component parameters on system reliability. The issue of assessing the impact -\u2026", "num_citations": "5\n", "authors": ["465"]}
{"title": "Architecture-based software reliability analysis incorporating concurrency\n", "abstract": " With the growing complexity of software applications and increasing reliance on the services provided by these applications, architecture-based reliability analysis has become the focus of several recent research efforts. Most of the prevalent research in this area does not consider simultaneous or concurrent execution of application components. Concurrency, however, may be common in modern software applications. Thus, reliability analysis considering concurrent component execution within the context of the application architecture is necessary for contemporary software applications. This paper presents an architecture-based reliability analysis methodology for concurrent software applications. Central to the methodology is a state space approach, based on discrete time Markov chains (DTMCs), to represent the application architecture taking into consideration simultaneous component execution. A closed\u00a0\u2026", "num_citations": "5\n", "authors": ["465"]}
{"title": "Response time analysis of a middleware event demultiplexing pattern for network services\n", "abstract": " Society is becoming increasingly reliant on the services provided by distributed, performance sensitive software systems. These systems demand multiple simultaneous quality of service (QoS) properties. A key enabler in recent successes in the development of such systems has been middleware, which comprises reusable building blocks. Typically, a large number of configuration options are available for each building block when composing a system end-to-end. The choice of the building blocks and their configuration options have an impact on the performance of the services provided by the systems. Currently, the effect of these choices can be determined only very late in the lifecycle, which can be detrimental to system development costs and schedules. In order to enable the right design choices, a systematic methodology to analyze the performance of these systems at design time is necessary. Such a\u00a0\u2026", "num_citations": "5\n", "authors": ["465"]}
{"title": "Routing metrics for best-effort traffic\n", "abstract": " Most of the QoS routing research in the literature attempts to increase the performance of QoS traffic without explicitly considering the performance of the best-effort traffic in the network. However, although the trend in the development and use of real-time multimedia applications is on the rise, traditional data applications are still expected to be a dominant percentage of the Internet traffic. Developing routing schemes for best-effort traffic when it coexists with QoS traffic to provide an acceptable performance to the best-effort traffic is thus critical. In this paper we study the effectiveness of the link utilization metric for routing best-effort traffic in a network that supports both QoS as well as best-effort traffic. The main contribution of this paper lies in demonstrating the feasibility of using the link utilization metric and quantifying the improvement in performance so obtained under various distributions of network load. Our\u00a0\u2026", "num_citations": "5\n", "authors": ["465"]}
{"title": "Interplay between video recommendations, categories, and popularity on YouTube\n", "abstract": " YouTube is one of the most popular video sharing sites, which allows users to upload, view, rate, share and comment on videos. Three of the many YouTube features, namely, tagging of videos into different categories, their popularity ratings, and video recommendations are notable to promoting users' experience on this site. Therefore, this paper examines the interplay between these features through three research questions. We find that about 40% of the video recommendations come from categories other than that of the original video, with Entertainment being the most preferred cross-linked category; (ii) popularity measures including the number of views and comments strongly impact video recommendations; and (iii) video categorization has a higher influence on video recommendations compared to their popularity ratings. Taken together, these three findings suggest that users should carefully consider the\u00a0\u2026", "num_citations": "4\n", "authors": ["465"]}
{"title": "Mining Social Capital on Online Social Networks with Strong and Weak Ties\n", "abstract": " The concept of social capital refers to the advantage that is created from the structures of an actor's social ties within a network. This paper presents an approach to investigate the extent to which a particular network is characterized by brokerage or closure social capital based on triadic analysis. We split each network into two, respectively comprising of strong and weak ties. To facilitate this splitting, we measure the strength of the tie among a pair actors based on the reciprocity of their relationship and the number of their shared or mutual friends. We hypothesize that the network composed of strong ties is expected to be rich in closure triads, whereas the network composed of weak ties is expected to be rich in brokerage triads. We test our hypotheses on four popular online social networks (OSNs), namely, Facebook, Twitter, Slashdot and YouTube. Empirical analysis reports that most networks composed of strong\u00a0\u2026", "num_citations": "4\n", "authors": ["465"]}
{"title": "Centrality and cluster analysis of yelp mutual customer business graph\n", "abstract": " This paper proposes a novel approach to understand customer relationships among businesses and the type of information that can be inferred from these relationships. Our approach is grounded in a unique method of constructing a mutual customer business graph, where businesses are represented by nodes and the weight of the edge connecting two businesses reflects the strength of their mutual customer population, which is estimated based on the reviews from the Yelp academic data set. We construct and analyze these mutual customer business graphs for cities of Las Vegas and Phoenix using centrality and spectral analysis techniques. Centrality analysis computes unweighted and weighted versions of degree and PageRank graph measures; the results reveal that businesses with high graph centralities also tend to be geographically central relative to other businesses. Spectral clustering partitions the\u00a0\u2026", "num_citations": "4\n", "authors": ["465"]}
{"title": "Understanding students' preferences of software engineering projects\n", "abstract": " Students in a maintenance-centric, introductory software engineering course were expected to understand, analyze and extend an open source software project of their choice, selected from a limited set of prepared applications. Students fell into two groups: those who chose a project based on its perceived and estimated difficulty, and those who chose a project based on the appeal of the subject matter. Students in both groups, however, cited value for themselves in terms of enhanced learning experience, and for users in terms of increased benefit, as reasons for their selection. These insights into students' thinking can guide future efforts in selecting projects that can simultaneously support the learning objectives as well as motivate the students, not only in software engineering but also in broader computing courses.", "num_citations": "4\n", "authors": ["465"]}
{"title": "Data analytics for power utility storm planning\n", "abstract": " As the world population grows, recent climatic changes seem to bring powerful storms to populated areas. The impact of these storms on utility services is devastating. Hurricane Sandy is a recent example of the enormous damages that storms can inflict on infrastructure, society, and the economy. Quick response to these emergencies represents a big challenge to electric power utilities. Traditionally utilities develop preparedness plans for storm emergency situations based on the experience of utility experts and with limited use of historical data. With the advent of the Smart Grid, utilities are incorporating automation and sensing technologies in their grids and operation systems. This greatly increases the amount of data collected during normal and storm conditions. These data, when complemented with data from weather stations, storm forecasting systems, and online social media, can be used in analyses for enhancing storm preparedness for utilities. This paper presents a data analytics approach that uses real-world historical data to help utilities in storm damage projection. Preliminary results from the analysis are also included.", "num_citations": "4\n", "authors": ["465"]}
{"title": "Analytic Model of Screening Times at Airport Security Checkpoints\n", "abstract": " Security checkpoints at airports across the United States are essential for preventing passengers with dangerous weapons, explosives, and other threats from boarding airplanes. However, the multiple screening technologies and speeds of passengers lead to unpredictable and sometimes long waiting times. Security agencies and airport managers must find ways to minimize screening times at checkpoints without compromising the security of aviation transportation. This paper introduces an analytic model that derives the distribution of completion times for passengers through a security checkpoint, given its architecture, passenger profiles, and expected service times at checkpoint components. By varying the model's parameters and checkpoint architecture, security agencies and airport managers can quickly understand how the end-to-end completion times of passengers are affected by policy changes and\u00a0\u2026", "num_citations": "4\n", "authors": ["465"]}
{"title": "Long range dependence (LRD) in the arrival process of Web robots\n", "abstract": " There is strong evidence to suggest that a significant proportion of traffic on Web servers, across many domains, can be attributed to Web robots. With the advent of the Social Web, widespread use of semantic Web technologies, and development of service-oriented Web applications, it is expected that this proportion will only rise over time. One of the most important distinctions between robots and humans is the pattern with which they request resources from a Web server. In this paper, we examine the arrival process of Web robot requests across Web servers from three diverse domains. We find that, regardless of the domain, Web robot traffic exhibits long range dependence (LRD) similar to human traffic. We discuss why, at least in some cases, LRD in robot traffic may not be generated by heavy-tailed response sizes as in the case of human traffic.", "num_citations": "4\n", "authors": ["465"]}
{"title": "Checkpointing for the RESTART problem in Markov networks\n", "abstract": " We apply the known formulae of the RESTART problem to Markov models of software (and many other) systems, and derive new equations. We show how checkpoints might be included, with their resultant performance under RESTART. The result is a complete procedure for finding the mean, variance, and tail behavior of the job completion time as a function of the failure rate. We also provide a detailed example.", "num_citations": "4\n", "authors": ["465"]}
{"title": "Architecture-based Reliability Analysis of Concurrent Software Applications using Stochastic Reward Nets.\n", "abstract": " Architecture-based reliability analysis of software applications has been the focus of several recent research efforts, as these applications continue to grow in size and complexity. Prevalent research in the area of architecturebased analysis predominantly focuses on sequential applications; with a few exceptions that address concurrency. Concurrency, however, is very common in modern software applications that are developed using the object-oriented or the component-based software development paradigms. Reliability analysis considering concurrency within the context of the application architecture is thus necessary for modern software applications. Our preliminary approach to analyze the reliability of a concurrent software application suffers from state-space explosion; due to which it cannot be applied to practical software applications. In this paper, we offer a methodology based on the Stochastic Reward\u00a0\u2026", "num_citations": "4\n", "authors": ["465"]}
{"title": "Architecture-based assessment of software reliability\n", "abstract": " With the growing advent of object-oriented and component-based software development paradigms, architecture-based software reliability analysis has emerged as an attractive alternative to the conventional black-box analysis based on software reliability growth models. The primary advantage of the architecture-based approach is that it explicitly relates the application reliability to component reliabilities, which eases the identification of components that are critical from a reliability perspective. Furthermore, these techniques can be used for an early assessment of the application reliability. These two features together can provide valuable information to practitioners and architects who design software applications, and managers who plan the allocation of resources to achieve the desired reliability targets in a cost effective manner.The objective of this tutorial is to discuss techniques to assess the reliability of a\u00a0\u2026", "num_citations": "4\n", "authors": ["465"]}
{"title": "Performance Analysis of the Active Object Pattern in Middleware.\n", "abstract": " A number of enterprises are turning towards the Service Oriented Architecture (SOA) approach for their systems due to the number of benefits it offers. A key enabling technology for the SOA-based approach is middleware, which comprises of reusable building blocks based on design patterns. These building blocks can be configured in numerous ways and the configuration options of a pattern can have a profound impact on system performance. A performance analysis methodology which can be used to assess this influence at design time can guide the selection of patterns and their configuration options and thus alleviate the possibility of performance problems arising later in the life cycle. This paper presents a model-based performance analysis methodology for a system built using the Active Object (AO) pattern. The AO pattern is chosen because it lies at the heart of an important class of producer/consumer and publish/subscribe systems. Central to the methodology is a queuing model which captures the internal architecture of an AO-based system. Using an implementation of the queuing model in CSIM, we illustrate the value of the methodology to guide the selection of configuration and provisioning options for a stock broker system.", "num_citations": "4\n", "authors": ["465"]}
{"title": "Modeling and Agent-Based Simulation of Organization in a Stochastic Environment\n", "abstract": " This paper describes a generic model and agent-based simulation to facilitate the analysis of interplay of information collection task identification and decision making task execution processes, as well as the information flow behaviors in organizations in the face of stochastic mission environments. In these mission environments, task arrivals are stochastic, the characteristics of tasks are not known a priori, but maybe inferred to a certain degree by undertaking the information collection or task identification processes. Through the information collection processes the organization collects the relevant attributes of tasks to estimate the resources necessary for their execution. This information is then used to allocate resources effectively for the execution of tasks. Our model, following structural contingency theory, depicts an organization as consisting of an information-processing, communication and coordination structure that is designed to achieve a specific set of goals, and is comprised of individuals with different information collecting and task execution capabilities. We develop a simulation toolkit based on a discrete event simulator, specifically the ANY LOGIC R simulation package, to quantify the performance of an organization based on this model. We illustrate our approach using a number of coordinating organizational structures operating in a stochastic mission environment.Descriptors:", "num_citations": "4\n", "authors": ["465"]}
{"title": "Posaml: A visual modeling framework for middleware provisioning\n", "abstract": " Effective provisioning of next generation distributed applications hosted on diverse middleware platforms incurs significant challenges due to the applications' growing complexity and quality of service (QoS) requirements. An effective provisioning of the middleware platform includes a composition and configuration of the middleware services that meets the application QoS requirements under expected workloads. Traditional techniques for middleware provisioning tend to use non-intuitive, low-level and technology-specific approaches, which are tedious, error prone, non-reusable and not amenable to ease of QoS validation. Additionally, most often the configuration activities of the middleware platform tend to be decoupled from the QoS validation stages resulting in an iterative trial-and-error process between the two phases. This paper describes the design of a visual modeling language called POSAML (patterns\u00a0\u2026", "num_citations": "4\n", "authors": ["465"]}
{"title": "Software failure intensity, reliability and optimal stopping time incorporating repair policies\n", "abstract": " Reliability of a software application, its failure intensity and the residual number of faults are three important metrics that provide a quantitative assessment of the failure characteristics of an application. Ultimately, it is also necessary, based on these metrics, to determine an optimal release time at which costs justify the stop test decision. Typically, one of the many stochastic models known as software reliability growth models (SRGMs) is used to characterize the failure behavior of an application to provide estimates of the failure intensity, residual number of faults, reliability, and optimal release time and cost. To ensure analytical tractability, SRGMs assume instantaneous repair and thus the estimates of these metrics obtained using SRGMs tend to be optimistic. In practice, repair activity consumes a non trivial amount of time and resources. Also, repair may be conducted according to many policies which reflect the\u00a0\u2026", "num_citations": "4\n", "authors": ["465"]}
{"title": "The marginal value of increased testing: An empirical analysis using four code coverage measures\n", "abstract": " This paper presents an empirical comparison of the growth characteristics of four code coverage measures, block, decision, c-use and p-use, as testing is increased. Due to the theoretical foundations underlying the lognormal software reliability growth model, we hypothesize that the growth for each coverage measure is lognormal. Further, since for a given program the breadth and the depth of the different coverage measures are similar, we expect that the parameters of the lognormal coverage growth model for each of the four coverage measures to be similar. We confirm these hypotheses using coverage data generated from extensive testing of an application which has 30 KLOC. We then discuss how the lognormal coverage growth function could be used to control the testing process and to guide decisions about when to stop testing, since it can provide an estimate of the marginal testing effort necessary to achieve a given level of improvement in the coverage.", "num_citations": "4\n", "authors": ["465"]}
{"title": "A Discrete Lognormal model for software defects affecting QoP\n", "abstract": " Many computer and network security crises arise due to the exploitation of software defects and are only remedied by their repair. Thus the effect of security related software defects and their occurrence rates is an important aspect of Quality of Protection (QoP). Existing arguments and evidence suggests that the distribution of occurrence rates of software defects is lognormal and that the first occurrence times of defects follows the Laplace transform of the lognormal. We extend this research to hypothesize that the distribution of occurrence counts of security related defects follows the Discrete Lognormal. We find that the observed occurrence counts for three sets of defect data relating specifically to network security are consistent with our hypothesis. The paper thus demonstrates how the existing concepts and techniques in software reliability engineering may be applied to study the occurrence phenomenon of security related defects that impact QoP.", "num_citations": "4\n", "authors": ["465"]}
{"title": "Towards integrated provisioning of QoS overlay network\n", "abstract": " The Internet is one of the most successful technologies in last century. Within less than four decades (starting from the first packet-switched computer network, ARPAnet), it has evolved into an extremely popular commercial infrastructure, and has a significant impact on almost all aspects of our lives and our society. With the dramatic advances in multimedia technologies and the increasing popularity of real-time applications, recently, quality of service (QoS) support in the Internet has been in a great demand. However, due to many historical reasons, today\u2019s Internet primarily provides best-effort connectivity service. To enhance the best-effort service model to provide QoS, researchers have proposed many seminal architectures, represented by IntServ [26] and DiffServ [25]. Unfortunately, due to many critical factors, realizing these QoS architectures in the Internet is unlikely to be feasible in the long run. In addition, there are no right economic models for these proposed architectures: although some ISPs might be interested in providing QoS in their own domains, there are no strong incentives for them to support QoS for users in other domains which are not their customers. Then, a challenging question faced by the researchers in the community is: what would be a practical solution for QoS support in the Internet? In the past few years, overlay networks have emerged as an alternative mechanism for supporting valueadded services such as fault tolerance [9], multicasting [38], and security [72]. Many of these overlays are end-user overlays, namely, overlays are constructed purely among the end hosts without support from any other intermediate\u00a0\u2026", "num_citations": "4\n", "authors": ["465"]}
{"title": "An integrated approach for QoS provisioning and monitoring\n", "abstract": " An important aspect of providing QoS to realtime multimedia applications is QoS provisioning. QoS provisioning consists of determining a path through the network, which has adequate resources to satisfy the QoS requirements of a connection with global efficiency in network resource utilization, and then reserving resources along this path for the duration of the connection. However, since performance degradation is inevitable due to the weakening or the failure of a network element, provisioning alone is not sufficient to ensure QoS for the entire duration of the connection. Hence, an equally important aspect of providing QoS to multimedia applications is QoS monitoring, so that performance degradation can be anticipated before it occurs or detected immediately upon occurrence. This can trigger provisioning of resources along an alternate suitable path to which the connection can be rerouted. Since QoS\u00a0\u2026", "num_citations": "4\n", "authors": ["465"]}
{"title": "Measuring distance between program features\n", "abstract": " We present a metric to determine the distance between the features of a software system. Such a measurement can elucidate how features of the system being examined are close to each other. We first use an execution slice-based technique to identify a set of code (basic blocks in our case) that is used to implement each feature. Then, depending on whether the execution frequency of each block is considered during the construction of such sets of code, a static as well as a dynamic distance is computed for each pair of features. These two types of distance differ in that the former computes the distance between two features only by how these features are implemented in the system, while the latter also takes into account how each feature is executed based on a user's operational profile. In other words, the static distance quantitatively gives the closeness of two features from the system implementation point of\u00a0\u2026", "num_citations": "4\n", "authors": ["465"]}
{"title": "Mining Emotions on Plutchik's Wheel\n", "abstract": " Tweets embed rich information about users' moods, emotions and feelings. Mining for these latent emotions can offer clues about users' affective state on a broad range of topics ranging from their mental health to political opinions. This paper proposes a supervised machine learning approach to detect emotions from tweets. The approach is built around a Crowdfiower data set of 40,000 tweets labeled with 13 distinct emotions. These 13 labels were mapped to emotions guided by the Plutchik's wheel, and are further organized into pairs of polar opposites leading to four binary classification problems: Love vs. Hate, Joy vs. Sadness, Trust vs. Disgust, and Anticipation vs. Surprise. For each classification problem, five supervised machine learning models were trained on a combination of linguistic and metadata features extracted from the tweets. The performance of these models is evaluated using sensitivity\u00a0\u2026", "num_citations": "3\n", "authors": ["465"]}
{"title": "Project-based learning in a Probabilistic Analysis course in the CS curriculum\n", "abstract": " This paper presents our experience in using a project-based learning approach in a senior-level class, which is not a traditional candidate for learning through projects in the CS curriculum. The objective of this class was to expose students to computer science applications of probabilistic analysis and stochastic processes. The class project expected the students to apply the concepts of Markov Chains constructed during in-class lectures to develop an understanding of the ranking algorithms used by the widely popular search giant, Google. These ranking algorithms sort web pages according to their relevance to a search query. Based on their understanding, the students were also expected to identify and document some strategies that can be employed by owners to improve the rank of their web sites. We found that our project, that guided the students through the four phases of project-based learning, led to\u00a0\u2026", "num_citations": "3\n", "authors": ["465"]}
{"title": "Analysis of structural social capital in online social networks\n", "abstract": " Structural capital focuses on the advantages derived from the patterns of an actor's connections within a social network. This paper investigates structural social capital in the form of brokerage and closure through the lens of triadic analysis in four popular and public online social networks, namely, Facebook, Twitter, Slashdot, and You Tube. Through a ratio of the census of open and closed triads, the analysis reveals that these four networks are overwhelmingly rich in brokerage capital. Brokerage triads are further classified into three types according to the role of the broker: (i) Tertius Gaudens (broker as a competitor), (ii) Tertius Inguens (broker as a coordinator), and (iii) Conduit (broker as a mediator). Closure triads are further classified according to the number of mutual dyadic connections. Exploring the distributions of brokerage and closure classes and their constituent triads reveals that for each online social\u00a0\u2026", "num_citations": "3\n", "authors": ["465"]}
{"title": "Understanding Common Perceptions from Online Social Media\n", "abstract": " Modern society habitually uses online social media services to publicly share observations, thoughts, opinions, and beliefs at any time and from any location. These geotagged social media posts may provide aggregate insights into people's perceptions on a bad range of topics across a given geographical area beyond what is currently possible through services such as Yelp and Foursquare. This paper develops probabilistic language models to investigate whether collective, topic-based perceptions within a geographical area can be extracted from the content of geotagged Twitter posts. The capability of the methodology is illustrated using tweets from three areas of different sizes. An application of the approach to support power grid restoration following a storm is presented.", "num_citations": "3\n", "authors": ["465"]}
{"title": "Discovering perceptions in online social media: A probabilistic approach\n", "abstract": " People across the world habitually turn to online social media to share their experiences, thoughts, ideas, and opinions as they go about their daily lives. These posts collectively contain a wealth of insights into how masses perceive their surroundings. Therefore, extracting people's perceptions from social media posts can provide valuable information about pertinent issues such as public transportation, emergency conditions, and even reactions to political actions or other activities. This paper proposes a novel approach to extract such perceptions from a corpus of social media posts originating from a given broad geographical region. The approach divides the broad region into a number of sub-regions, and trains language models over social media conversations within these sub-regions. Using Bayesian and geo-smoothing methods, the ensemble of language models can be queried with phrases embodying a\u00a0\u2026", "num_citations": "3\n", "authors": ["465"]}
{"title": "Data loss: An empirical analysis in search of best practices for prevention\n", "abstract": " The cost an convenience of cloud computing has motivated many organizations to migrate their data and application services to cloud platforms, yet information security concerns persist. Data loss incidents experienced by organizations over the past decade offer many valuable lessons, which are highly relevant to the present trend of storing sensitive data in clouds. This paper presents our analysis of the DataLoss db, an Open Source Foundation project to collect information on data loss incidents throughout the world. The historical probability of recovering data after it is lost is low, suggesting that the design and implementation of an enterprise wide data protection plan may be the most effective method to mitigate the risk of data loss. Our analysis includes an examination of the cases where data was recovered, drawing insights where better data protection policies and procedures could avoid similar losses in the\u00a0\u2026", "num_citations": "3\n", "authors": ["465"]}
{"title": "Participatory paradigms: Promises and challenges for urban transportation\n", "abstract": " Participatory paradigms such as online social networks and crowdsourced platforms count on riders of urban transportation systems to provide information that can be of value to transportation agencies, while alleviating the cost and reliability challenges of traditional, physical, sensor-based solutions in obtaining such information. This paper presents a novel methodology to mine online social network updates to obtain this data from the citizens and riders of a city\u2019s urban transportation system. The authors illustrate how the methodology can extract information from Twitter posts across New York City that the Metropolitan Transit Authority (MTA) can utilize to improve its service. Having demonstrated the promise of using online social networks as a way to harvest participatory data in the transportation arena, the paper concludes with a discussion of outstanding challenges that must be resolved before participatory paradigms can become a viable alternative to traditional solutions.", "num_citations": "3\n", "authors": ["465"]}
{"title": "A Model-driven Approach for Price/Performance Tradeoffs in Cloud-based MapReduce Application Deployment.\n", "abstract": " This paper describes preliminary work in developing a modeldriven approach to conducting price/performance tradeoffs for Cloudbased MapReduce application deployment. The need for this work stems from the significant variability in both the MapReduce application characteristics and price/performance characteristics of the underlying cloud platform. Our approach involves a model-based machine learning capability that trains itself from executing a variety of MapReduce applications on different cloud service providers, and in turn providing useful price/performance tradeoff information to MapReduce application users. Additionally, the model-based platform serves to automate the deployment of a MapReduce application to the cloud by incorporating the tradeoff choices.", "num_citations": "3\n", "authors": ["465"]}
{"title": "Error reduction by confusing characters discrimination for online handwritten Japanese character recognition\n", "abstract": " To reduce the classification errors of online handwritten Japanese character recognition, we propose a method for confusing characters discrimination with little additional costs. After building confusing sets by cross validation using a baseline quadratic classifier, a logistic regression (LR) classifier is trained to discriminate the characters in each set. The LR classifier uses subspace features selected from existing vectors of the baseline classifier, thus has no extra parameters except the weights, which consumes a small storage space compared to the baseline classifier. In experiments on the TUAT HANDS databases with the modified quadratic discriminant function (MQDF) as baseline classifier, the proposed method has largely reduced the confusion caused by non-Kanji characters.", "num_citations": "3\n", "authors": ["465"]}
{"title": "An agent-based simulation model for organizational analysis\n", "abstract": " In many fields, including engineering, management, and organizational science, simulation-based computational organization theory has been used to gain insight into the degree of match congruence between the organization people, work processes and structure and the tasks carried out by the organization. Simulation helps identify the bottlenecks, and improve the quality and efficiency of an organization. In this paper, we propose an approach based on the congruence model for analyzing and simulating the performance of an organization in project-based mission environments. In our model, organizations are constructed in terms of interacting components, namely, work and agents. The organizational structure depicts the grouping of agents, and the hierarchical arrangement of the groups. The congruence model of organizational behavior is based on the degree to which different components of the organization fit together. We use a discrete event simulator, specifically the Extendtrademark simulation package, to quantify the performance of an organization based on this model. We illustrate our approach using a symbolic example of an air operations center organization.Descriptors:", "num_citations": "3\n", "authors": ["465"]}
{"title": "A Discrete Lognormal Model for Software Defects Affecting Quality of Protection\n", "abstract": " Many computer and network security crises arise because of the exploitation of software defects and are remedied only by repair. The effect of security related software defects and their occurrence rates is an important aspect of Quality of Protection (QoP). Existing arguments and evidence suggests that the distribution of occurrence rates of software defects is lognormal and that the first occurrence times of defects follows the Laplace transform of the lognormal. We extend this research to hypothesize that the distribution of occurrence counts of security related defects follows the Discrete Lognormal. We find that the observed occurrence counts for three sets of defect data relating specifically to network security are consistent with our hypothesis. This paper demonstrates how existing concepts and techniques in software reliability engineering can be applied to study the occurrence phenomenon of security\u00a0\u2026", "num_citations": "3\n", "authors": ["465"]}
{"title": "QoS assurance of next generation network (NGN) applications\n", "abstract": " A Next Generation Network (NGN) application is an application that provides a value\u2013added service seamlessly across an integrated network consisting of a combination of packet\u2013switched, circuit\u2013switched, wireless and wireline networks. Such an application is likely to be developed using a set of open, standard APIs. These APIs provide a level of abstraction to shield the application programmers from the underlying complexities of heterogeneous networks. As a result, these APIs facilitate rapid service development by allowing an application developer to focus on the functionality (that is, what is provided), rather than on the implementation (that is, how it is provided). The importance of open, standard APIs in encouraging the growth of a community of application developers is evident from a number of initiatives to develop such APIs and environments providing these APIs. These initiatives include JAIN [2] TM, 1, OSA/PARLAY [3], and 3GPP [1]. Although these APIs facilitate rapid service creation by eliminating the need to understand the peculiarities of the underlying network infrastructures, the quality assurance of these services will require a service developer to become fully cognizant of such details. This requirement, which is primarily due to the lack of a consistent methodology for quality assurance, significantly mitigates the advantage of decreased development time gained by using open, standard APIs. A systematic framework for QoS assurance based on open, standard APIs provides an integrated approach for rapid service creation as well as quality assurance of these services and is the focus of this paper. The QoS assurance\u00a0\u2026", "num_citations": "3\n", "authors": ["465"]}
{"title": "QoS Monitoring of Voice-over-IP Services\n", "abstract": " Voice over IP (VoIP) is an attractive choice for voice transmittal compared to traditional circuit-switching technology for the following reasons: lower equipment cost, integration of voice and data applications, lower bandwidth requirements, widespread availability of IP, and the promise of novel, value-added services [1]. In future, VoIP services will be expected to operate seamlessly over a converged network referred to as a Next Generation Network (NGN), comprising a combination of heterogeneous network infrastructures, including packet-switched, circuit-Switched, wireless, and wireline networks.ABSTRACT", "num_citations": "2\n", "authors": ["465"]}
{"title": "Automated quantitative analysis of open-ended survey responses for transportation planning\n", "abstract": " Many organizations, including transportation agencies, use open-ended questions on their surveys to provide respondents an opportunity to narrate their concerns in their own words. Although these narrative responses contain wealth of useful information, most organizations ignore them because they are complex to interpret and analyze in automated manner. Therefore, this paper proposes an approach to extract useful knowledge from narrative survey responses. Posing the problem as a multi-label classification, the approach uses a Naive Bayes classifier to label open-ended responses as per the topics into which the forced choice responses are grouped. The approach is illustrated using narrative responses to a Customer Metro North Classification Survey. The classifier accuracy ranges from 75%-80% on an average, which is roughly only 10%-20% worse compared to human classifiers. Some categories\u00a0\u2026", "num_citations": "2\n", "authors": ["465"]}
{"title": "Social analysis of the SEKE co-author network\n", "abstract": " We extract the co-author network over the entire history of the SEKE conference from 1988 through 2014. In this network, authors represent nodes and a pair of authors is connected by an edge if they have co-authored at least one article over the entire duration. We analyze this network using socio-centric and ego-centric network methods to study the extent to which the authors are involved in the SEKE community, and the patterns of collaboration between them. Socio-centric analysis reveals that most authors publish a very small number of articles, and collaborate within tightly knit circles. In fact, only a tiny fraction of the authors consistently return to SEKE to disseminate their research. Ego-centric measures of centrality confirm these findings by revealing that only a small percentage of the authors are structurally dominant, and influence the flow of communication among others. Based on these findings, we believe that strategically SEKE could benefit from cultivating a wider base of influential authors, promoting broader collaborations, and encouraging one-time authors to return.", "num_citations": "2\n", "authors": ["465"]}
{"title": "How social network APIs have ended the age of privacy\n", "abstract": " Online Social Networks (OSNs) have captured our imagination by offering a revolutionary medium for communication and sharing. Skeptics, however, contend that these OSNs pose a grave threat to privacy. This paper seeks to examine the veracity of this skepticism by analyzing the APIs of six popular OSNs for their propensity to violate user privacy. Our analysis lends substantial support to this skepticism by finding that OSNs:(i) facilitate an extensive collection of user information;(ii) provide default access to information of new users;(iii) do not seek comprehensive permissions;(iv) request these permissions ambiguously; and (v) offer privacy settings that enable only limited control.", "num_citations": "2\n", "authors": ["465"]}
{"title": "A methodology to evaluate the availability of reconfigurable conveyor systems\n", "abstract": " Networked sensor-actuator systemspsila technologies significantly impact the design of future automation systems. Since the automation domain is critical, there is a need to ensure high availability of the system. A model-based approach to evaluate system availability offers systematic, quantitative guidance on the effectiveness of different topologies and failure recovery mechanisms. We present a methodology to evaluate the availability of a simple class of automation systems, namely reconfigurable conveyor systems in which each conveyor unit is regulated by a local micro-controller. Microcontrollers in different units interact via wireless communications. The availability of the conveyor system depends on the correct functioning of each unit in the system. We assume that only the sensors, actuators and mechanical components of units can fail, i.e., the microcontrollers and radio transceivers do not fail. Because the\u00a0\u2026", "num_citations": "2\n", "authors": ["465"]}
{"title": "System availability analysis considering failure severities\n", "abstract": " Model-based analysis is commonly used to assess the influence of different factors on system availability. Most of the availability models reported in the literature consider the impact of redundancy, fault tolerance, and system structure. However, these models treat all system failures to be equivalent or at the same level of severity. In practice, it is well-known that failures are classified into multiple severity levels according to their impact on the system's ability to deliver its services. System availability is thus influenced by only some rather than all failures. To obtain an accurate availability estimate it is then necessary to incorporate failure severities into the analysis. In this paper we present a system availability model which considers failure severities of the hardware and software components of the system in an integrated manner. Based on the model we obtain closed form expressions which relate system availability to the failure and repair parameters of the hardware and software components comprising the system. For a given choice of failure parameters, we discuss how the closed form expressions could be used to select the repair parameters to achieve specified target system availability and to establish bounds on system availability. We illustrate the potential of the model by applying it to the failure data collected during the acceptance testing of a satellite system.", "num_citations": "2\n", "authors": ["465"]}
{"title": "Model-driven performance analysis methodology for distributed software systems\n", "abstract": " A key enabler of the recently popularized, assembly-centric development approach for distributed real-time software systems is QoS-enabled middleware, which provides reusable building blocks in the form of design patterns that codify solutions to commonly recurring problems. These patterns can be customized by choosing an appropriate set of configuration parameters. The configuration options of the patterns exert a strong influence on system performance, which is of paramount importance in many distributed software systems. Despite this considerable influence, currently there is a lack of significant research to analyze performance of middleware at design time, where performance issues can be resolved at a much earlier stage of the application life cycle and with substantially less costs. The present project seeks to develop a performance analysis methodology for design-time performance analysis for\u00a0\u2026", "num_citations": "2\n", "authors": ["465"]}
{"title": "Software reliability\n", "abstract": " The influence of computer systems on our day\u2010to\u2010day lives has increased dramatically since their advent almost 60 years ago. The ever\u2010increasing power and reliability of hardware has encouraged the development of novel and innovative software applications. However, as the reliability of hardware continues to improve steadily, software reliability is becoming a major stumbling block in achieving desired levels of system dependability. The literature is replete with numerous software failures in the case of mission\u2010critical and medical systems, which result in a loss of life and reliable software operation is undoubtedly important for these systems. However, as economies of industrialized nations become increasingly dependent on software, its reliable operation becomes crucial for the success of business critical systems as well. A NIST study estimates that the U.S. economy loses $59.5 billion per year or 0.6% of\u00a0\u2026", "num_citations": "2\n", "authors": ["465"]}
{"title": "An efficient QoS distribution monitoring scheme\n", "abstract": " In end-to-end monitoring, traffic measurements are recorded at the source and destination nodes. In QoS distribution monitoring, the measurements are recorded at several intermediate nodes along a connection. End-to-end monitoring is conceptually simple and easy to implement, but not capable of localizing the degradation. Thus, any corrective action has to be applied end-to-end, which can be prohibitively expensive. Although QoS distribution monitoring holds the promise of being able to localize the degradation to a single domain, prevalent QoS distribution monitoring techniques do not consistently provide this capability. They also consume excessive resources in the monitoring process and rely on assumptions which are unlikely to hold in practice. In this paper we describe the design tradeoffs associated with QoS distribution monitoring and highlight the limitations of the existing research. We then describe\u00a0\u2026", "num_citations": "2\n", "authors": ["465"]}
{"title": "Variance expressions for software reliability growth models\n", "abstract": " This paper derives a generic function to express the variance in the application reliability estimate in terms of the variances in the parameter estimates for the finite failure non homogeneous Poisson process (NHPP) class of software reliability growth models SRGMs. Based on the generic expression, we then derive an expression to obtain the variance in the application reliability estimate for one of the most influential SRGMs, namely, the Goel-Okumoto model. We illustrate the potential of the variance expression derived for the Goel-Okumoto model with several examples.", "num_citations": "2\n", "authors": ["465"]}
{"title": "A Parsimonious and Practical Approach to Detecting Offensive Speech\n", "abstract": " With the proliferation of hateful and offensive speech on social media platforms such as Twitter, machine learning approaches to detect such toxic content have gained prominence. Despite these advances, real-time detection of such speech, while it is being shared on these platforms, remains a challenge for two reasons. First, these approaches train complex models on a plethora of features, which calls into question their computational efficiency for real-time deployment. Moreover, they require sizeable, manually annotated data sets from the same context, and annotating large data sets is extremely time-consuming, error-prone and cumbersome. This paper proposes a parsimonious and practical approach for the detection of offensive speech that alleviates these challenges. The approach is parsimonious because through a comprehensive evaluation of commonly used machine learning models (Logistic\u00a0\u2026", "num_citations": "1\n", "authors": ["465"]}
{"title": "Monitoring the Perception of Covid-19 Vaccine using Topic Models\n", "abstract": " Despite the unprecedented international collaboration to develop a Covid-19 vaccine at lightning speed, concerns remain whether the vaccine will be embraced by the population at large, calling into question its effectiveness at controlling the pandemic. This paper mines public outlook on the Covid-19 vaccine using unsupervised topic modeling. Tweets were collected a week following the announcement of Operation Warp Speed and were cleaned and labeled as anti-vaxx and pro-vaxx. Topic modeling was applied separately to these anti-vaxx and pro-vaxx groups of tweets. Anti-vaxx tweets lead to four themes: (i) lack of safety especially due to rushed development, (ii) conspiracy and conflict of interest, (iii) ideology, globalism and new world order, and (iv) loss of personal choice and freedom. Pro-vaxxers hope to convince skeptics by touting prior success of immunizations and express concern at the rise of anti\u00a0\u2026", "num_citations": "1\n", "authors": ["465"]}
{"title": "Analysis and Classification of Vaccine Dialogue in the Coronavirus Era\n", "abstract": " As the coronavirus tears through our global community, the world pins its hopes on the expedient availability of a safe and effective vaccine. In the U.S, however, mere mention of vaccines galvanizes a community that remains steadfastly opposed to them. This paper analyzes the vaccine dialogue on Twitter in the coronavirus era, using the data collected a week after President Trump's announcement of Operation Warp Speed. These tweets are explored in three ways. Informal opinion mining reveals both concerns and support; the anti-vaxx community is vociferous in opposing the vaccine, spreading misinformation, spinning conspiracies and whipping hysteria. Significant hesitation about the safety of the Covid-19 vaccine is also expressed in particular because of its rapid deployment. The pro-vaxx community counters this opposition by pointing to prior successes of immunizations as well as by mocking the anti\u00a0\u2026", "num_citations": "1\n", "authors": ["465"]}
{"title": "Quantifying the Relationship Between Health Outcomes and Unhealthy Habits.\n", "abstract": " Chronic health outcomes impact the quality of life of affected individuals and their families and also lead to huge health care costs. Most of the chronic health outcomes can be attributed to few unhealthy behaviors, however, the extent to which these behaviors can explain the variation in the common outcomes is not known. This paper explores the relationship between:(i) unhealthy behaviors using principal components analysis; and (ii) unhealthy behaviors and chronic health outcomes using multiple linear regression. The 500 Cities data, released by the Center for Disease Control, forms the basis of this investigation. PCA suggests that the unhealthy behaviors can be projected along two dimensions, each punctuated by the common age of occurrence. The results of linear regression are consistent with expectations for some outcomes, but reveal unexpected trends for the others.", "num_citations": "1\n", "authors": ["465"]}
{"title": "Analysis of Global Technology Price Index\n", "abstract": " The permeation of technological gadgets into our societal fabric means that easy and affordable access to these gadgets is no longer a luxury. It then becomes essential to empower modern consumers with global trends in the prices of many common gadgets to enable them to make smart decisions on where and how to acquire these devices. This paper presents an analysis of the inaugural Global Technology Price Index published by Linio.com. The methodology consists of statistical analysis followed by an investigation of the geographical trends using k-means clustering. Statistical analysis indicates that the price variability is the highest for smartphones, namely, Android and iPhone. Moreover, no single country or a region is a universal affordable source for all the 14 devices included in the index. Clustering analysis divides the countries into Affordable and Expensive groups; where the collective prices of most\u00a0\u2026", "num_citations": "1\n", "authors": ["465"]}
{"title": "Keyword-based semi-supervised text classification\n", "abstract": " Industrial organizations generate massive volumes of data during their routine business and production activities. Such data may be structured (numerical or categorical), or it may be unstructured and textual. Both structured and unstructured data contain a wealth of knowledge that can help organizations improve their operations. Organizations find it easy to automatically extract knowledge from structured data. Unstructured data, however, must be mined and interpreted manually which is cumbersome, error-prone and time consuming. This paper focuses on how to automatically analyze unstructured text data to extract important business value. It proposes a semi-supervised natural language (NL) approach to analyze a corpus of documents associated with accounts receivable disputes at a large corporation. The name semi-supervised derives from the philosophy underlying the methodology, where a set of\u00a0\u2026", "num_citations": "1\n", "authors": ["465"]}
{"title": "Comparing Health Outcomes in San Francisco and Boston Metro Areas\n", "abstract": " In many tangible and intangible ways, the Boston and San Francisco metro areas are very alike. This paper investigates whether this resemblance between the two metro areas leads to similar health outcomes. Health outcomes, estimated at the level of census tracts and collected as a part of the 500 cities project are compared to assess this similarity. The named cities of Boston and San Francisco along with four other suburban cities from each metro area, which have nearly equal numbers of census tracts and populations are considered in the comparison. Based on the 13 health outcomes, census tracts from these ten cities are grouped into three clusters using k-means clustering. These clusters are labeled \"Poor\", \"Fair\" and \"Good\" based on their mean prevalence of health outcomes. A quantitative assessment of the distribution of the census tracts among these three clusters shows that the Boston metro has a\u00a0\u2026", "num_citations": "1\n", "authors": ["465"]}
{"title": "Arion: A Model-Driven Middleware for Minimizing Data Loss in Stream Data Storage\n", "abstract": " In large-scale data stream management systems, sampling rate of different sensors can change quickly in response to changed execution environment. However, such changes can cause significant load imbalance on the back-end servers, leading towards performance degradation and data loss. To address this challenge, in this paper, we present a model-driven middleware service (i.e., Arion) that uses a two-step approach to minimize data loss. Specifically, Arion constructs models and algorithms for overload prediction for heterogeneous systems (where different streams can have different sampling rates and message sizes) leveraging limited execution traces from homogeneous systems (where each stream has the same sampling rate and message size). Subsequently, when an overload condition is predicted (or detected), Arion first leverages the a priori constructed models to identify the streams (if any) that\u00a0\u2026", "num_citations": "1\n", "authors": ["465"]}
{"title": "Reliability analysis of underwater sensor network packet transmission\n", "abstract": " Underwater sensor networks pose unique challenges to the design of reliable communication. Due to the high bit error rates experienced in this environment, achieving a compromise between reliability and energy efficiency has become a fundamental problem. In this paper, an objective metric to analyze the reliability of various packet transmission methods available for use in underwater sensor networks is developed. Earlier frameworks comparing competing alternatives have made the simplifying assumption that loss rat e is homogeneous across the entire network. Such a simplification contradicts the fact that wireless sensor networks exhibit properties such as link asymmetry and are also influenced by phenomenon like radio irregularity. In light of these realities, it is necessary to relax the existing modeling assumptions to produce a more general framework for assessing the various potential solutions. Drawing\u00a0\u2026", "num_citations": "1\n", "authors": ["465"]}
{"title": "Efficient reliability analysis of concurrent software applications considering software architecture\n", "abstract": " Architecture-based reliability analysis of software applications is gaining prominence as it can provide valuable guidance to software architects during early design phases. Concurrent component execution is common among modern software applications, and hence, reliability analysis considering concurrency within the context of software architecture is essential. Our preliminary analysis approach considering concurrency suffers from state-space explosion; due to which it cannot be applied to practical software applications. This paper proposes solutions to the model specification and solution challenges arising from the state-space explosion problem. The specification challenge is alleviated using the Stochastic Reward Net (SRN) modelling paradigm which can intuitively and concisely represent concurrent software architecture at a higher level of abstraction. The computational challenge is alleviated by\u00a0\u2026", "num_citations": "1\n", "authors": ["465"]}
{"title": "Industry-wise Analysis of Security Breaches in Data Loss Incidents.\n", "abstract": " Cloud computing offers many conveniences at reduced costs, which has motivated many organizations to migrate their data and services to cloud platforms. Security breaches that plague these cloud transactions, however, can threaten widespread migration to the cloud. In this paper, we study data loss incidents over the last decade to discover the vulnerabilities that organizations face when moving their data and computations to the cloud. These data loss incidents are collected and documented in the DataLoss project led by the Open Source Foundation. Our industry-wise analysis focuses on understanding the types of breaches and data loss modes encountered by organizations in the business, educational, government, and medical sectors. For each industrial sector, we also measure the impact of these breaches in terms of the number of lost records and the estimated costs. We summarize the findings of our\u00a0\u2026", "num_citations": "1\n", "authors": ["465"]}
{"title": "Security and Performance Analysis of Passenger Screening for Mass-transit\n", "abstract": " During the past decade, the international community has witnessed several attacks on forms of mass transportation such as train stations and subways. The Department of Homeland Security requested that we develop methods to assess the security of mass transit in order to mitigate the vulnerability of the nation\u2019s public transportation systems. We present a methodology to quantify the impact of imposing screening on mass transit, which considers both security and delays incurred on the traveling public. We demonstrate the approach through a case study, the Fairfield Metro Station in Fairfield, Connecticut. Our results indicate that rigorous aviation-style screening will slow the flow of passengers drastically. We also show how to use the approach to identify where faster screening technologies can improve passenger throughput while ensuring security. The approach can thus be used to identify areas where\u00a0\u2026", "num_citations": "1\n", "authors": ["465"]}
{"title": "Resource Allocation for a Modular Software System.\n", "abstract": " Most existing software optimization research assumes advance knowledge of the component parameters. Perfect future knowledge of fault detection is an unnecessary oversimplification and greatly diminishes the applicability of the previously proposed techniques. While efficient resource allocation is essential, these studies ignore the importance of components within a software\u2019s architecture. In this paper we present an adaptive optimization procedure that periodically assesses the testing process and allocates time to components. Using a case study, we illustrate how our approach adapts, increasing resources to a component when more failures are observed during testing. The results suggest that this interactive method is responsive to clusters of faults encountered during testing and can help to minimize the time needed to optimize software.", "num_citations": "1\n", "authors": ["465"]}
{"title": "Application of the Lognormal Distribution to Software Reliability Engineering\n", "abstract": " The theoretical foundations underlying the lognormal distribution suggest that it can be applied to important problems in software reliability engineering. Further, an overwhelming amount of evidence has emerged to confirm the lognormal hypothesis. In this chapter we provide an overview of the lognormal distribution and its transformations. We then summarize the previous evidence, place the most recent evidence in context, discuss the set of issues to which it has been successfully applied, and call attention to the unified perspective the lognormal affords. Finally, we outline potential applications of the lognormal and suggest enabling research.", "num_citations": "1\n", "authors": ["465"]}
{"title": "Adequacy of Composite Parametric Software Reliability Models.\n", "abstract": " Several composite parametric models have been proposed for software reliability analysis. These models enhance the classical software reliability models by explicitly incorporating the impact of testing effort on the fault detection process. The adequacy of composite models is commonly assessed using the Mean Square Error (MSE) criterion, which measures the ability of a model to explain the observed failure behavior. Many composite models perform better than the classical models on which they are based, according to the MSE. However, this goodness of fit sacrifices the parsimony of the original model and usually decreases the composite model\u2019s predictive power. In this paper we suggest the use of the Akaike Information Criteria (AIC) to assess model adequacy. The use of the AIC is motivated by its inherent feature to penalize models based on the number of model parameters. This serves to deter against overly complicated models that may fit the observed data well, but ultimately result in poor predictions. An illustrative comparison of the different models shows that the composite models fare worse than the classical ones according to the AIC. This indicates that the predictive capability of the composite models may be poorer than the classical models. We thus conclude that several classical, simpler software reliability models still remain relevant to modeling the fault detection process during testing.", "num_citations": "1\n", "authors": ["465"]}
{"title": "A Hierarchical Availability Analysis of Multi-tiered Web Applications\n", "abstract": " We propose a hierarchical availability analysis methodology for multi-tiered Web applications. The methodology partitions the analysis into three levels, namely, server, request and session, and considers only the relevant factors at each level. The levels are connected using a hierarchical approach; the results obtained from one level are propagated for use in the analysis at the next one. The methodology thus decouples the different factors that influence availability and yet provides an integrated framework to consider them simultaneously.", "num_citations": "1\n", "authors": ["465"]}
{"title": "An Analytical Approach for Reliability Analysis of Pipeline Software Architecture.\n", "abstract": " Architecture styles represent commonly occurring patterns of the structural organization of components and connectors of an application. A number of such styles have been identified and analyzed extensively for different non functional attributes including performance, maintainability, flexibility, and modifiability. The issue of reliability analysis of architecture styles, however, has been relatively less investigated. This paper presents a reliability analysis methodology for one such architecture style, namely, the pipe and filter style. Two variants of the topological organization of the pipes and filters, namely, linear topology without feedback and with feedback where the feedback loop is incorporated either to improve quality or to improve reliability are considered. The methodology derives analytical expressions for application reliability which incorporate the impact of (i) error propagation and downstream error correction, and (ii) deterministic number of iterations through the feedback loop, with filter reliabilities a function of the number of iterations. The potential of the methodology to obtain a reliability estimate and to facilitate sensitivity analysis is illustrated using an industrial case study of a Document Understanding and Analysis Application.", "num_citations": "1\n", "authors": ["465"]}
{"title": "A master-slave network architecture to support QoS routing\n", "abstract": " QoS routing is concerned with determining a path through the network that has the potential of satisfying the constraints of a QoS connection while simultaneously achieving global efficiency in network resource utilization. Existing QoS routing techniques suffer from excessive computation, communication and storage overheads and/or routing delay, which hinders their applicability in large networks consisting of multiple domains. In this paper we present a novel master-slave network architecture that provides a comprehensive solution to the problem of QoS routing. The architecture combines the existing routing techniques in an intelligent manner, such that the composite routing technique mitigates the overheads associated with each one of the constituent techniques. Due to this the composite routing technique can be used in a large network, and hence the architecture provides a unified solution to the problem of\u00a0\u2026", "num_citations": "1\n", "authors": ["465"]}