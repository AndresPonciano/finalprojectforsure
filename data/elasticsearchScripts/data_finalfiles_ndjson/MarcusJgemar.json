{"title": "Bandwidth Measurement using Performance Counters for Predictable Multicore Software\n", "abstract": " Memory contention is one of the largest sources of inter-core interference in statically partitioned multicore systems, and the contention reduces the overall performance of applications and causes unpredictable execution-times. A first step in achieving predictable execution is to accurately measure the amount of consumed memory bandwidth for each application. Such measurements can be used to track down bottlenecks, provide better partitioning among cores, and ultimately be used to arbitrate and police access to the memory bus. We propose to use hardware performance counters to continuously track the memory-bandwidth consumed by different applications executing in parallel. In this paper we describe ongoing efforts exploring suitable performance counters on core-level and on system-on-chip level for the 8-core Freescale P4080 processor. The aim is to accurately and efficiently track consumed\u00a0\u2026", "num_citations": "19\n", "authors": ["729"]}
{"title": "Towards feedback-based generation of hardware characteristics\n", "abstract": " In large complex server-like computer systems it is difficult to characterise hardware usage in early stages of system development. Many times the applications running on the platform are not ready at the time of platform deployment leading to postponed metrics measurement. In our study we seek answers to the questions:(1) Can we use a feedback-based control system to create a characteristics model of a real production system?(2) Can such a model be sufficiently accurate to detect characteristics changes instead of executing the production application?The model we have created runs a signalling application, similar to the production application, together with a PID-regulator generating L1 and L2 cache misses to the same extent as the production system. Our measurements indicate that we have managed to mimic a similar environment regarding cache characteristics. Additionally we have applied the model on a software update for a production system and detected characteristics changes using the model. This has later been verified on the complete production system, which in this study is a large scale telecommunication system with a substantial market share.", "num_citations": "13\n", "authors": ["729"]}
{"title": "Testing performance-isolation in multi-core systems\n", "abstract": " In this paper we present a methodology to be used for quantifying the level of performance isolation for a multi-core system. We have devised a test that can be applied to breaches of isolation in different computing resources that may be shared between different cores. We use this test to determine the level of isolation gained by using the Jailhouse hypervisor compared to a regular Linux system in terms of CPU isolation, cache isolation and memory bus isolation. Our measurements show that the Jailhouse hypervisor provides performance isolation of local computing resources such as CPU. We have also evaluated if any isolation could be gained for shared computing resources such as the system wide cache and the memory bus controller. Our tests show no measurable difference in partitioning between a regular Linux system and a Jailhouse partitioned system for shared resources. Using the Jailhouse\u00a0\u2026", "num_citations": "12\n", "authors": ["729"]}
{"title": "Measurement-based evaluation of data-parallelism for OpenCV feature-detection algorithms\n", "abstract": " We investigate the effects on the execution time, shared cache usage and speed-up gains when using datapartitioned parallelism for the feature detection algorithms available in the OpenCV library. We use a data set of three different images which are scaled to six different sizes to exercise the different cache memories of our test architectures. Our measurements reveal that the algorithms using the default settings of OpenCV behave very differently when using data-partitioned parallelism. Our investigation shows that the executions of the algorithms SURF, Dense and MSER correlate to L3-cache usage and they are therefore not suitable for data-partitioned parallelism on multicore CPUs. Other algorithms: BRISK, FAST, ORB, HARRIS, GFTT, SimpleBlob and SIFT, do not correlate to L3-cache in the same extent, and they are therefore more suitable for data-partitioned parallelism. Furthermore, the SIFT algorithm\u00a0\u2026", "num_citations": "10\n", "authors": ["729"]}
{"title": "A scheduling architecture for enforcing quality of service in multi-process systems\n", "abstract": " There is a massive deployment of multi-core CPUs. It requires a significant drive to consolidate multiple services while still achieving high performance on these off-the-shelf CPUs. Each function had earlier an own execution environment, which guaranteed a certain Quality of Service (QoS). Consolidating multiple services can give rise to shared resource congestions, resulting in lower and non-deterministic QoS. We describe a method to increase the overall system performance by assisting the operating system process scheduler to utilize shared resources more efficiently. Our method utilizes hardware- and system-level performance counters to profile the shared resource usage of each process. We also use a big-data approach to analyzing statistics from many nodes. The outcome of the analysis is a decision support model that is utilized by the process scheduler when allocating and scheduling process. Our\u00a0\u2026", "num_citations": "10\n", "authors": ["729"]}
{"title": "Cognitively Sustainable ICT with Ubiquitous Mobile Services-Challenges and Opportunities\n", "abstract": " Information and Communication Technology (ICT) has led to an unprecedented development in almost all areas of human life. It forms the basis for what is called \"the cognitive revolution\" -- a fundamental change in the way we communicate, feel, think and learn based on an extension of individual information processing capacities by communication with other people through technology. This so-called \"extended cognition\" shapes human relations in a radically new way. It is accompanied by a decrease of shared attention and affective presence within closely related groups. This weakens the deepest and most important bonds, that used to shape human identity. Sustainability, both environmental and social (economic, technological, political and cultural) is one of the most important issues of our time. In connection with \"extended cognition\" we have identified a new, basic type of social sustainability that everyone\u00a0\u2026", "num_citations": "10\n", "authors": ["729"]}
{"title": "Identifying Evolution Problems for Large Long Term Industrial Evolution Systems\n", "abstract": " Large infrastructure systems with a life time of more than 30 years, such as telecommunication or power transmission systems, are difficult to maintain since they suffer from the end-of-life plague of software, hardware and knowledge. Large companies have traditionally tackled this problem successfully, but maybe not with complete efficiency in all cases. We find system evolution to be an increasingly interesting problem as infrastructure becomes more complicated. Our increasingly complex and advanced society demands more of the infrastructure making system evolution an interesting alternative to system replacement. From the point of view of the ISO/IEC 15288 development process we have identified life cycle issues connected to long life time scenarios and the different life cycle stages. In this paper we contribute with a modification of the utilisation and support stage in ISO/IEC 15288 into an evolution stage\u00a0\u2026", "num_citations": "8\n", "authors": ["729"]}
{"title": "Run-time cache-partition controller for multi-core systems\n", "abstract": " The current trend in automotive systems is to integrate more software applications into fewer ECU's to decrease the cost and increase efficiency. This means more applications share the same resources which in turn can cause congestion on resources such as such as caches. Shared resource congestion may cause problems for time critical applications due to unpredictable interference among applications. It is possible to reduce the effects of shared resource congestion using cache partitioning techniques, which assign dedicated cache lines to different applications. We propose a cache partition controller called LLC-PC that uses the Palloc page coloring framework to decrease the cache partition sizes for applications during runtime. LLC-PC creates cache partitioning directives for the Palloc tool by evaluating the performance gained from increasing the cache partition size. We have evaluated LLC-PC using 3\u00a0\u2026", "num_citations": "6\n", "authors": ["729"]}
{"title": "Automatic message compression with overload protection\n", "abstract": " In this paper, we show that it is possible to increase the message throughput of a large-scale industrial system by selectively compress messages. The demand for new high-performance message processing systems conflicts with the cost effectiveness of legacy systems. The result is often a mixed environment with several concurrent system generations. Such a mixed environment does not allow a complete replacement of the communication backbone to provide the increased messaging performance. Thus, performance-enhancing software solutions are highly attractive. Our contribution is (1) an online compression mechanism that automatically selects the most appropriate compression algorithm to minimize the message round trip time; (2) a compression overload mechanism that ensures ample resources for other processes sharing the same CPU. We have integrated 11 well-known compression algorithms\u00a0\u2026", "num_citations": "5\n", "authors": ["729"]}
{"title": "Resource Depedency Analysis in Multi-Core Systems\n", "abstract": " In this paper, we evaluate different methods for statistical determination of application resource dependency in multi-core systems. We measure the performance counters of an application during run-time and create a system resource usage profile. We then use the resource profile to evaluate the application dependency on the specific resource. We discuss and evaluate two methods to process the data, including moving average filter and partitioning the data into smaller segments in order to interpret data for correlation calculations. Our aim with this study is to evaluate and create a generalizeable methods for automatic determination of resource dependencies. The final outcome of the methods used in this study is the answer to the question: \"To what resources is this application dependent on?\". The recommendation of this tool will be used in conjunction with our last-level cache partitioning controller (LLC-PC), to\u00a0\u2026", "num_citations": "4\n", "authors": ["729"]}
{"title": "Utilizing Hardware Monitoring to Improve the Performance of Industrial Systems\n", "abstract": " THE drastically increasing use of Information and Communications Tech-nology has resulted in a growing demand for network capacity. In thisLicentiate thesis, we show how to monitor, model and finally improve network performance for large industrial systems. We also show how to use modeling techniques to move performance testing to an earlier design phase, with the aim to reduce the total development time of large systems. Our first contribution is a low-intrusive method for long-term hardware characteristic measurements of production nodes located at customer sites. Our second contribution is a technique to mimic the hardware usage of a production environment by creating a characteristics model. The cloned environment makes function test suites more realistic. The goal when creating the model is to reduce the system development time by moving late-stage performance testing to early design phases thereby improving the quality of the test environment. The third and final contribution is a network performance improvement where we dynamically trade computational capacity for a message round-trip time reduction when there are CPU cycles to spare. We have implemented an automatic feedback controlled mechanism for transparent message compression resulting in improved messaging performance between interconnected network nodes. Our mechanism continuously evaluates eleven compression algorithms on message stream content and network congestion level. The message subsystem will use the compression algorithm that provides the lowest messaging time. If the message content or network load change, a new\u00a0\u2026", "num_citations": "4\n", "authors": ["729"]}
{"title": "Adaptive Online Feedback Controlled Message Compression\n", "abstract": " Communication is a vital part of computer systems today. One current problem is that computational capacity is growing faster than the bandwidth of interconnected computers. Maximising performance is a key objective for industries, both on new and existing software systems, which further extends the need for more powerful systems at the cost of additional communication. Our contribution is to let the system selectively choose the best compression algorithm from a set of available algorithms if it provides a better overall system performance. The online selection mechanism can adapt to a changing environment such as temporary network congestion or a change of message content while still selecting the optimal algorithm. Additionally, is autonomous and does not require any human intervention making it suitable for large-scale systems. We have implemented and evaluated this autonomous selection and\u00a0\u2026", "num_citations": "4\n", "authors": ["729"]}
{"title": "Technical Report: Feedback-Based Generation of Hardware Characteristics\n", "abstract": " In large complex server-like computer systems it is difficult to characterise hardware usage in early stages of system development. Many times the applications running on the platform are not ready at the time of platform deployment leading to postponed metrics measurement. In our study we seek answers to the questions:(1) Can we use a feedbackbased control system to create a characteristics model of a real production system?(2) Can such a model be sufficiently accurate to detect characteristics changes instead of executing the production application?The model we have created runs a signalling application, similar to the production application, together with a PID-regulator generating L1 and L2 cache misses to the same extent as the production system. Our measurements indicate that we have managed to mimic a similar environment regarding cache characteristics. Additionally we have applied the model on a software update for a production system and detected characteristics changes using the model. This has later been verified on the complete production system, which in this study is a large scale telecommunication system with a substantial market share.", "num_citations": "3\n", "authors": ["729"]}
{"title": "Process scheduling in a processing system having at least one processor and shared hardware resources\n", "abstract": " A method for enabling scheduling of processes in a processing system having at least one processor and associated hardware resources, at least one of the hardware resources being shared by at least two of the processes. The method is characterized by controlling execution of a process based on a usage bound of the number of allowable accesses, by the process, to a shared hardware resource by halting execution of the process when the number of allowable accesses has been reached, and enabling idle mode or start of execution of a next process. In this way, costly hardware overprovisioning and/or the need for shutting down processor cores can be avoided. By controlling execution of a process based on a usage bound of the number of allowable accesses to a shared hardware resource, instead of simply dividing CPU time between processes, highly efficient shared-resource-based process scheduling\u00a0\u2026", "num_citations": "2\n", "authors": ["729"]}
{"title": "Mallocpool: Improving Memory Performance Through Contiguously TLB Mapped Memory\n", "abstract": " Many computer systems allocate and free many memory chunks over the application lifespan. One problem with allocating many chunks is that they may not be contiguously allocated causing a massive strain on caches, translation lookaside buffers (TLB), and the memory subsystem. We have devised a method that preallocates a large memory fragment, mapping it with a variable size TLB, and then allocate subsequently requested chunks from that fragment. Our method has two advantages. The first is that all chunks allocated by malloc() are allocated contiguously, thus allowing a better cache-locality. The second advantage is that we can map the whole memory region with one variable size TLB reducing much of the 4kB TLB strain. These two advantages drastically improve the memory access performance. We have implemented our method in a Linux library which we can either dynamically or statically link to\u00a0\u2026", "num_citations": "2\n", "authors": ["729"]}
{"title": "Investigating execution-characteristics of feature-detection algorithms\n", "abstract": " We discuss how to obtain information of execution characteristics, such as parallelizability and memory utilization, with the final aim to improve the performance and predictability of feature and corner detection algorithms for use in e.g. robotics and autonomous machines. Our aim is to obtain a better understanding of how computer vision algorithms use hardware resources and how to improve the time predictability and execution time of such algorithms when executing on multi-core CPUs. We evaluate a fork-join model applicable to feature detection algorithms and present a method for measuring how well the algorithm performance correlates with hardware resource usage. We have applied our method to the Featured from Accelerated Segment Test (FAST) algorithm. Our characterization of FAST reveals that it is an algorithm with excellent parallelism opportunities, resulting in an almost linear speed-up per core\u00a0\u2026", "num_citations": "2\n", "authors": ["729"]}
{"title": "Automatic Multi-Core Cache Characteristics Modelling\n", "abstract": " When updating low-level software for large computer systems it is di cult to verify whether performance requirements are met or not. Common practice is to measure the performance only when the new software is fully developed and has reached system veri cation. Since this gives long lead-times it becomes costly to remedy performance problems. Our contribution is that we have deployed a new method to synthesise production workload. We have, using this method, created a multi-core cache characteristics model. We have validated our method by deploying it in a production system as a case study. The result shows that the method is su ciently accurate to detect changes and mimic cache characteristics and performance, and thus giving early characteristics feedback to engineers. We have also applied the model to a real software update detecting changes in performance characteristics similar to the real system.", "num_citations": "2\n", "authors": ["729"]}