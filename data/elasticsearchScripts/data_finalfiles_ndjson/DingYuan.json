{"title": "lprof: A nonintrusive request flow profiler for distributed systems\n", "abstract": " Applications implementing cloud services, such as HDFS, Hadoop YARN, Cassandra, and HBase, are mostly built as distributed systems designed to scale. In order to analyze and debug the performance of these systems effectively and efficiently, it is essential to understand the performance behavior of service requests, both in aggregate and individually.", "num_citations": "139\n", "authors": ["1823"]}
{"title": "Non-intrusive performance profiling for entire software stacks based on the flow reconstruction principle\n", "abstract": " Understanding the performance behavior of distributed server stacks at scale is non-trivial. The servicing of just a single request can trigger numerous sub-requests across heterogeneous software components; and many similar requests are serviced concurrently and in parallel. When a user experiences poor performance, it is extremely difficult to identify the root cause, as well as the software components and machines that are the culprits.", "num_citations": "87\n", "authors": ["1823"]}
{"title": "Don\u2019t Get Caught in the Cold, Warm-up Your JVM: Understand and Eliminate JVM Warm-up Overhead in Data-Parallel Systems\n", "abstract": " Many widely used, latency sensitive, data-parallel distributed systems, such as HDFS, Hive, and Spark choose to use the Java Virtual Machine (JVM), despite debate on the overhead of doing so. This paper analyzes the extent and causes of the JVM performance overhead in the above mentioned systems. Surprisingly, we find that the warm-up overhead, ie, class loading and interpretation of bytecode, is frequently the bottleneck. For example, even an I/O intensive, 1GB read on HDFS spends 33% of its execution time in JVM warm-up, and Spark queries spend an average of 21 seconds in warm-up.", "num_citations": "72\n", "authors": ["1823"]}
{"title": "An analysis of performance evolution of Linux's core operations\n", "abstract": " This paper presents an analysis of how Linux's performance has evolved over the past seven years. Unlike recent works that focus on OS performance in terms of scalability or service of a particular workload, this study goes back to basics: the latency of core kernel operations (eg, system calls, context switching, etc.). To our surprise, the study shows that the performance of many core operations has worsened or fluctuated significantly over the years. For example, the select system call is 100% slower than it was just two years ago. An in-depth analysis shows that over the past seven years, core kernel subsystems have been forced to accommodate an increasing number of security enhancements and new features. These additions steadily add overhead to core kernel operations but also frequently introduce extreme slowdowns of more than 100%. In addition, simple misconfigurations have also severely impacted\u00a0\u2026", "num_citations": "11\n", "authors": ["1823"]}
{"title": "CLP\n", "abstract": " : Efficient and Scalable Search on Compressed Text Logs YScope CLP Page 1 : Efficient and Scalable Search on Compressed Text Logs YScope Kirk Rodrigues, Yu Luo, Ding Yuan CLP Page 2 Compressed Log Processor Can search compressed logs \u2026without decompression \u2026with good performance Lossless log compression \u2026better than general-purpose compressors 4.72%2.35% 0% 20% 40% 60% 80% 100% Space Utilization (%) Uncompressed Gzip CLP 2x Page 3 The Log Management Pipeline 4 YScope 2020-01-02T03:04:05.006 INFO Task task_12 assigned to container: [NodeAddress:172.128.0.41, ContainerID:container_15], operation took 0.335 seconds 2020-01-02T03:04:05.006 INFO Task task_12 assigned to container: [NodeAddress:172.128.0.41, ContainerID:container_15], operation took 0.335 seconds 2020-01-02T03:04:05.006 INFO Task task_12 assigned to container: [NodeAddress:\u2026", "num_citations": "4\n", "authors": ["1823"]}
{"title": "Log processing and analysis\n", "abstract": " A log of execution of an executable program is obtained. Log messages contained in the log are parsed to generate object identifiers representative of instances of programmatic elements in the executable program. Relationships among the object identifiers are identified. A representation of identified relationships is constructed and outputted as, for example, a visual representation.", "num_citations": "3\n", "authors": ["1823"]}
{"title": "Non-Intrusive Failure Reproduction for Distributed Systems using the Partial Trace Principle\n", "abstract": " Complex and unforeseen failures in distributed systems must be diagnosed and replicated in a development environment so that developers can understand the underlying problem and verify the resolution. System logs often form the only source of diagnostic information, and developers reconstruct a failure using manual guesswork. This is an unpredictable and time-consuming process which can lead to costly service outages while a failure is repaired. This paper describes EC, a tool capable of reconstructing near-minimal failure reproduction steps from log files and system bytecode, without human involvement. Unlike existing solutions that use symbolic execution to search for the entire path leading to the failure, EC is based on the observation of the Partial Trace Principle, which states that programmers do not simulate the entire execution to understand the failure, but follow a combination of control and data dependencies to reconstruct a simplified trace only containing events that are likely to be relevant to the failure. EC follows a set of carefully designed rules to infer a chain of causally dependent events leading to the failure symptom while aggressively skipping unrelated code paths to avoid the pathexplosion overheads of symbolic execution models.", "num_citations": "1\n", "authors": ["1823"]}
{"title": "Don\u2019t Get Caught In the Cold, Warm-up Your JVM\n", "abstract": " Many widely used, latency sensitive, data-parallel distributed systems, such as HDFS, Hive, and Spark choose to use the Java Virtual Machine (JVM) despite debate on the overhead of doing so. By thoroughly studying the JVM performance overhead in the abovementioned systems, we found that the warm-up overhead, ie, class loading and interpretation of bytecode, is frequently the bottleneck. For example, even an I/O intensive, 1 GB read on HDFS spends 33% of its execution time in JVM warm-up, and Spark queries spend an average of 21 seconds in warmup. The findings on JVM warm-up overhead reveal a contradiction between the principle of parallelization, ie, speeding up long-running jobs by parallelizing them into short tasks, and amortizing JVM warm-up overhead through long tasks. We therefore developed HotTub, a new JVM that reuses a pool of already warm JVMs across multiple applications. The speed-up is significant: for example, using HotTub results in up to 1.8 x speed-ups for Spark queries, despite not adhering to the JVM specification in edge cases.The performance of data-parallel distributed systems has been heavily studied in the past decade, and numerous improvements have been made to the performance of these systems. A recent trend is to further process latency sensitive, interactive queries with these systems. However, there is a lack of understanding of the JVM\u2019s performance implications in these workloads. Consequently, almost every discussion on the implications of the JVM\u2019s performance results in heated debate. For example, the developers of Hypertable, an in-memory key-value store, use C++ because\u00a0\u2026", "num_citations": "1\n", "authors": ["1823"]}