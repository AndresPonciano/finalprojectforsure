{"title": "X10: an object-oriented approach to non-uniform cluster computing\n", "abstract": " It is now well established that the device scaling predicted by Moore's Law is no longer a viable option for increasing the clock frequency of future uniprocessor systems at the rate that had been sustained during the last two decades. As a result, future systems are rapidly moving from uniprocessor to multiprocessor configurations, so as to use parallelism instead of frequency scaling as the foundation for increased compute capacity. The dominant emerging multiprocessor structure for the future is a Non-Uniform Cluster Computing (NUCC) system with nodes that are built out of multi-core SMP chips with non-uniform memory hierarchies, and interconnected in horizontally scalable cluster configurations such as blade servers. Unlike previous generations of hardware evolution, this shift will have a major impact on existing software. Current OO language facilities for concurrent and distributed programming are\u00a0\u2026", "num_citations": "1968\n", "authors": ["254"]}
{"title": "Design of machinery: an introduction to the synthesis and analysis of mechanisms and machines\n", "abstract": " This text is intended for the kinematics and dynamics of machinery topics which are often given as a single course, or two-course sequence, in the junior year of most mechanical engineering programs. The usual prerequisites are first courses in statics, dynamics, and calculus. Usually, the first semester, or portion, is devoted to kinematics, and the second to dynamics of machinery. These courses are ideal vehicles for introducing the mechanical engineering student to the process of design, since mechanisms tend to beintuitive for the typical mechanical engineering student to visualize and create", "num_citations": "1678\n", "authors": ["254"]}
{"title": "Partitioning and scheduling parallel programs for execution on multiprocessors\n", "abstract": " There are three fundamental problems to be solved in the execution of a parallel program on a multiprocessor--identifying the parallelism in the program, partitioning the program into tasks and scheduling the tasks on processors. Whereas the problem of identifying parallelism is a programming language issue, the partitioning and scheduling problems are intimately related to parameters of the target multiprocessor, like the number of processors and synchronisation and communication overhead. It is desirable for the partitioning and scheduling to be performed automatically, so that the same parallel program can execute efficiently on different multiprocessors. This dissertation presents two solutions to the partitioning and scheduling problems. The first approach is based on a macro-dataflow model, where the program is partitioned into tasks at compile-time and the tasks are scheduled on processors at run-time\u00a0\u2026", "num_citations": "1174\n", "authors": ["254"]}
{"title": "Baring it all to software: Raw machines\n", "abstract": " The most radical of the architectures that appear in this issue are Raw processors-highly parallel architectures with hundreds of very simple processors coupled to a small portion of the on-chip memory. Each processor, or tile, also contains a small bank of configurable logic, allowing synthesis of complex operations directly in configurable hardware. Unlike the others, this architecture does not use a traditional instruction set architecture. Instead, programs are compiled directly onto the Raw hardware, with all units told explicitly what to do by the compiler. The compiler even schedules most of the intertile communication. The real limitation to this architecture is the efficacy of the compiler. The authors demonstrate impressive speedups for simple algorithms that lend themselves well to this architectural model, but whether this architecture will be effective for future workloads is an open question.", "num_citations": "980\n", "authors": ["254"]}
{"title": "Linear scan register allocation\n", "abstract": " We describe a new algorithm for fast global register allocation called linear scan. This algorithm is not based on graph coloring, but allocates registers to variables in a single linear-time scan of the variables' live ranges. The linear scan algorithm is considerably faster than algorithms based on graph coloring, is simple to implement, and results in code that is almost as efficient as that obtained using more complex and time-consuming register allocators based on graph coloring. The algorithm is of interest in applications where compile time is a concern, such as dynamic compilation systems, \u201cjust-in-time\u201d compilers, and interactive development environments.", "num_citations": "496\n", "authors": ["254"]}
{"title": "Space-time scheduling of instruction-level parallelism on a raw machine\n", "abstract": " Increasing demand for both greater parallelism and faster clocks dictate that future generation architectures will need to decentralize their resources and eliminate primitives that require single cycle global communication. A Raw microprocessor distributes all of its resources, including instruction streams, register files, memory ports, and ALUs, over a pipelined two-dimensional mesh interconnect, and exposes them fully to the compiler. Because communication in Raw machines is distributed, compiling for instruction-level parallelism (ILP) requires both spatial instruction partitioning as well as traditional temporal instruction scheduling. In addition, the compiler must explicitly manage all communication through the interconnect, including the global synchronization required at branch points. This paper describes RAWCC, the compiler we have developed for compiling general-purpose sequential programs to the\u00a0\u2026", "num_citations": "383\n", "authors": ["254"]}
{"title": "Habanero-Java: the new adventures of old X10\n", "abstract": " In this paper, we present the Habanero-Java (HJ) language developed at Rice University as an extension to the original Java-based definition of the X10 language. HJ includes a powerful set of task-parallel programming constructs that can be added as simple extensions to standard Java programs to take advantage of today's multi-core and heterogeneous architectures. The language puts a particular emphasis on the usability and safety of parallel constructs. For example, no HJ program using async, finish, isolated, and phaser constructs can create a logical deadlock cycle. In addition, the future and data-driven task variants of the async construct facilitate a functional approach to parallel programming. Finally, any HJ program written with async, finish, and phaser constructs that is data-race free is guaranteed to also be deterministic.", "num_citations": "303\n", "authors": ["254"]}
{"title": "Determining average program execution times and their variance\n", "abstract": " This paper presents a general framework for determining average program execution times and their variance, based on the program's interval structure and control dependence graph. Average execution times and variance values are computed using frequency information from an optimized counter-based execution profile of the program.", "num_citations": "252\n", "authors": ["254"]}
{"title": "On estimating and enhancing cache effectiveness\n", "abstract": " In this paper, we consider automatic analysis of a program's cache usage to achieve greater cache effectiveness. We show how to estimate efficiently the number of distinct cache lines used by a given loop in a nest of loops. Given this estimate of the number of cache lines needed, we can estimate the number of cache misses for a nest of loops. Our estimates can be used to guide program transformations such as loop interchange to achieve greater cache effectiveness. We present simulation results that show our estimates are reasonable for simple cases such as matrix multiply. We analyze the array sizes for which our estimates differ from our simulation results, and provide recommendations on how to handle such arrays in practice.", "num_citations": "235\n", "authors": ["254"]}
{"title": "JCUDA: A programmer-friendly interface for accelerating Java programs with CUDA\n", "abstract": " A recent trend in mainstream desktop systems is the use of general-purpose graphics processor units (GPGPUs) to obtain order-of-magnitude performance improvements. CUDA has emerged as a popular programming model for GPGPUs for use by C/C++ programmers. Given the widespread use of modern object-oriented languages with managed runtimes like Java and C#, it is natural to explore how CUDA-like capabilities can be made accessible to those programmers as well. In this paper, we present a programming interface called JCUDA that can be used by Java programmers to invoke CUDA kernels. Using this interface, programmers can write Java codes that directly call CUDA kernels, and delegate the responsibility of generating the Java-CUDA bridge codes and host-device data transfer calls to the compiler. Our preliminary performance results show that this interface can deliver significant\u00a0\u2026", "num_citations": "225\n", "authors": ["254"]}
{"title": "SLAW: A scalable locality-aware adaptive work-stealing scheduler\n", "abstract": " This paper introduces SLAW, a Scalable Locality-aware Adaptive Work-stealing scheduler. The SLAW scheduler is designed to address two common limitations in current work-stealing schedulers: use of a fixed task scheduling policy and locality-obliviousness due to randomized stealing. Past work has demonstrated the pros and cons of using fixed scheduling policies, such as work-first and help-first, in different cases without a clear win for one policy over the other. The SLAW scheduler addresses this limitation by supporting both work-first and help-first policies simultaneously. It does so by using an adaptive approach that selects a scheduling policy on a per-task basis at runtime. The SLAW scheduler also establishes bounds on the stack and heap space needed to store tasks. The experimental results for the benchmarks studied in this paper show that SLAW's adaptive scheduler achieves 0.98? to 9.2? speedup\u00a0\u2026", "num_citations": "221\n", "authors": ["254"]}
{"title": "Compile-time partitioning and scheduling of parallel programs\n", "abstract": " Partitioning and scheduling techniques are necessary to implement parallel languages on multiprocessors. Multiprocessor performance is maximized when parallelism between tasks is optimally traded off with communication and synchronization overhead. We present compile-time partitioning and scheduling techniques to achieve this trade-off.", "num_citations": "221\n", "authors": ["254"]}
{"title": "Array SSA form and its use in parallelization\n", "abstract": " Static single assignment (SSA) form for scalars has been a significant advance. It has simplified the way we think about scalar variables. It has simplified the design of some optimizations and has made other optimizations more effective. Unfortunately none of this can be be said for SSA form for arrays. The current SSA processing of arrays views an array as a single object. But the kinds of analyses that sophisticated compilers need to perform on arrays, for example those that drive loop parallelization, are at the element level. Current SSA form for arrays is incapable of providing the element-level data flow information required for such analyses. In this paper, we introduce an Array SSA form that captures precise element-level data flow information for array variables in all cases. It is general and simple, and coincides with standard SSA form when applied to scalar variables. It can also be used for structures and other\u00a0\u2026", "num_citations": "215\n", "authors": ["254"]}
{"title": "Work-first and help-first scheduling policies for async-finish task parallelism\n", "abstract": " Multiple programming models are emerging to address an increased need for dynamic task parallelism in applications for multicore processors and shared-address-space parallel computing. Examples include OpenMP 3.0, Java Concurrency Utilities, Microsoft Task Parallel Library, Intel Thread Building Blocks, Cilk, X10, Chapel, and Fortress. Scheduling algorithms based on work stealing, as embodied in Cilk's implementation of dynamic spawn-sync parallelism, are gaining in popularity but also have inherent limitations. In this paper, we address the problem of efficient and scalable implementation of X10's async-finish task parallelism, which is more general than Cilk's spawn-sync parallelism. We introduce a new work-stealing scheduler with compiler support for async-finish task parallelism that can accommodate both work-first and help-first scheduling policies. Performance results on two different multicore SMP\u00a0\u2026", "num_citations": "206\n", "authors": ["254"]}
{"title": "Phasers: a unified deadlock-free construct for collective and point-to-point synchronization\n", "abstract": " Coordination and synchronization of parallel tasks is a major source of complexity in parallel programming. These constructs take many forms in practice including mutual exclusion in accesses to shared resources, termination detection of child tasks, collective barrier synchronization, and point-to-point synchronization. In this paper, we introduce phasers, a new coordination construct that unifies collective and point-to-point synchronizations. We establish two safety properties for phasers: deadlock-freedom and phase-ordering. Performance results obtained from a portable implementation of phasers on three different SMP platforms demonstrate that phasers can deliver superior performance to existing barrier implementations, in addition to the productivity benefits that result from their generality and safety properties.", "num_citations": "184\n", "authors": ["254"]}
{"title": "Doe advanced scientific computing advisory subcommittee (ascac) report: top ten exascale research challenges\n", "abstract": " Exascale computing systems are essential for the scientific fields that will transform the 21st century global economy, including energy, biotechnology, nanotechnology, and materials science. Progress in these fields is predicated on the ability to perform advanced scientific and engineering simulations, and analyze the deluge of data. On July 29, 2013, ASCAC was charged by Patricia Dehmer, the Acting Director of the Office of Science, to assemble a subcommittee to provide advice on exascale computing. This subcommittee was directed to return a list of no more than ten technical approaches (hardware and software) that will enable the development of a system that achieves the Department's goals for exascale computing. Numerous reports over the past few years have documented the technical challenges and the non\u00ac-viability of simply scaling existing computer designs to reach exascale. The technical challenges revolve around energy consumption, memory performance, resilience, extreme concurrency, and big data. Drawing from these reports and more recent experience, this ASCAC subcommittee has identified the top ten computing technology advancements that are critical to making a capable, economically viable, exascale system.", "num_citations": "172\n", "authors": ["254"]}
{"title": "Collective loop fusion for array contraction\n", "abstract": " In this paper we propose a loop fusion algorithm specifically designed to increase opportunities for array contraction. Array contraction is an optimization that transforms array variables into scalar variables within a loop nest. In contrast to array elements, scalar variables have better cache behavior and can be allocated to registers. In past work we investigated loop interchange and loop reversal as optimizations that increase opportunities for array contraction [13]. This paper extends this work by including the loop fusion optimization. The fusion method discussed in this paper uses the maxflow-mincut algorithm to do loop clustering. Our collective loop fusion algorithm is efficient, and we demonstrate its usefulness for array contraction with a simple example.", "num_citations": "172\n", "authors": ["254"]}
{"title": "Partitioning parallel programs for macro-dataflow\n", "abstract": " Abstract Partitioni,~.. teclm/qucs are necessary to execute functional Inegra~ at\u2022 coarse Srenel~ ty. Fine srenul~ ty execution is inefficient on general purpose multiprocettars. There Ls\u2022 trado-off between pm'elkllsm and the overhead of exploiting parallelism. W e~ t\u2022 compile-time partitioning approach to achieve~ U~ le~ off.", "num_citations": "157\n", "authors": ["254"]}
{"title": "Customizable domain-specific computing\n", "abstract": " To meet computing needs and overcome power density limitations, the computing industry has entered the era of parallelization. However, highly parallel, general-purpose computing systems face serious challenges in terms of performance, energy, heat dissipation, space, and cost. We believe that there is significant opportunity to look beyond parallelization and focus on domain-specific customization to bring significant power-performance efficiency improvement.", "num_citations": "143\n", "authors": ["254"]}
{"title": "Retargeting optimized code by matching tree patterns in directed acyclic graphs\n", "abstract": " An optimizing, compiler that performs retargetable object code generation for a specific processor by matching tree patterns in directed acyclic graphs derived from the source code.", "num_citations": "143\n", "authors": ["254"]}
{"title": "Automatic partitioning of a program dependence graph into parallel tasks\n", "abstract": " In this paper, we describe a general interprocedural framework for partitioning a program dependence graph into parallel tasks for execution on a multiprocessor system. Partitioning techniques are necessary to execute a parallel program at the appropriate granularity for a given target multiprocessor. The problem is to determine the best trade-off between parallelism and overhead. It is desirable for the partitioning to be performed automatically, so that the programmer can write a parallel program without being burdened by details of the overhead target multiprocessor, and so that the same parallel program can be made to execute efficiently on different multiprocessors. For each procedure, the partitioning algorithm attempts to minimize the estimated parallel execution time. The estimated parallel execution time reflects a trade-off between parallelism and overhead and is minimized at an optimal intermediate\u00a0\u2026", "num_citations": "142\n", "authors": ["254"]}
{"title": "Location consistency-a new memory model and cache consistency protocol\n", "abstract": " Existing memory models and cache consistency protocols assume the memory coherence property which requires that all processors observe the same ordering of write operations to the same location. In this paper, we address the problem of defining a memory model that does not rely on the memory coherence assumption and also the problem of designing a cache consistency protocol based on such a memory model. We define a new memory consistency model, called Location Consistency (LC), in which the state of a memory location is modeled as a partially ordered multiset (pomset) of write and synchronization operations. We prove that LC is strictly weaker than existing memory models, but is still equivalent to stronger models for the common case of parallel programs that have no data races. We also describe a new multiprocessor cache consistency protocol based on the LC memory model. We prove that\u00a0\u2026", "num_citations": "139\n", "authors": ["254"]}
{"title": "Exascale software study: Software challenges in extreme scale systems\n", "abstract": " Extreme Scale processors containing hundreds or even thousands of cores will challenge current operating system (OS) practices. Many of the fundamental assumptions that underlie current OS technology are based on design assumptions that are no longer valid for a Extreme Scale processor containing thousands of cores. In the context of Exascale system requirements, as machines grow in scale and complexity, techniques to make the most effective use of network, memory, processor, and energy resources are becoming increasingly important. In its role as gate-keeper to all these resources, the OS becomes a major obstacle in allowing the application to view the hardware in accordance with the Extreme Scale Execution Model outlined in Section 3.1. A baseline challenge for the exascale software stack is: how to reduce OS overheads without compromising the need to protect hardware state from errant or malicious software. Execution models that support more asynchrony will be necessary to hide latency. Such execution models will also require more carefully coordinated scheduling to balance resource utilization", "num_citations": "135\n", "authors": ["254"]}
{"title": "Hierarchical place trees: A portable abstraction for task parallelism and data movement\n", "abstract": " Modern computer systems feature multiple homogeneous or heterogeneous computing units with deep memory hierarchies, and expect a high degree of thread-level parallelism from the software. Exploitation of data locality is critical to achieving scalable parallelism, but adds a significant dimension of complexity to performance optimization of parallel programs. This is especially true for programming models where locality is implicit and opaque to programmers. In this paper, we introduce the hierarchical place tree (HPT) model as a portable abstraction for task parallelism and data movement. The HPT model supports co-allocation of data and computation at multiple levels of a memory hierarchy. It can be viewed as a generalization of concepts from the Sequoia and X10 programming models, resulting in capabilities that are not supported by either. Compared to Sequoia, HPT supports three kinds of data\u00a0\u2026", "num_citations": "133\n", "authors": ["254"]}
{"title": "Optimized unrolling of nested loops\n", "abstract": " In this paper, we address the problems of automatically selecting unroll factors for perfectly nested loops, and generating compact code for the selected unroll factors. Compared to past work, the contributions of our work include a) a more detailed cost model that includes ILP and 1-cache considerations, b) a new code generation algorithm for unrolling nested loops that generates more compact code (with fewer remainder loops) than the unroll-and-jam transformation, and c) a new algorithm for efficiently enumerating feasible unroll vectors.", "num_citations": "127\n", "authors": ["254"]}
{"title": "Automatic selection of high-order transformations in the IBM XL FORTRAN compilers\n", "abstract": " The IBM ASTI optimizer provides the foundation for high-order transformations and automatic shared-memory parallelization in the latest IBM XL FORTRAN (XLF) compilers for RS/6000\u2122 and PowerPC\u00ae uniprocessors and symmetric multiprocessors (SMPs), and for automatic distributed-memory parallelizationin the IBM XL High-Performance FORTRAN (XLHPF) compiler for the SP2\u2122 distributed-memory multiprocessor. In this paper, we describe how the transformer component of the ASTI optimizer automatically selects high-order transformations for a given input program and a target uniprocessor, so as to improve utilization of the memory hierarchy (including cache and registers) and instruction-level parallelism. Our solution is centered on a quantitative approach in which optimization problems are formulated using quantitative cost models. The loop and data transformations currently employed by the ASTI\u00a0\u2026", "num_citations": "120\n", "authors": ["254"]}
{"title": "May-happen-in-parallel analysis of X10 programs\n", "abstract": " X10 is a modern object-oriented programming language designed for high performance, high productivity programming of parallel and multi-core computer systems. Compared to the lower-level thread-based concurrency model in the Java TM language, X10 has higher-level concurrency constructs such as async, atomic and finish built into the language to simplify creation, analysis and optimization of parallel programs. In this paper, we introduce a new algorithm for May-Happen-in-Parallel (MHP) analysis of X10 programs. The analysis algorithm is based on simple path traversals in the Program Structure Tree, and does not rely on pointer alias analysis of thread objects as in MHP analysis for Java programs. We introduce a more precise definition of the MHP relation than in past work by adding condition vectors that identify execution instances for which the MHP relation holds, instead of just returning a single\u00a0\u2026", "num_citations": "119\n", "authors": ["254"]}
{"title": "The RAW compiler project\n", "abstract": " Compilers today are capable of inferring detailed information about program parallelism and analyzing whole-program behavior. However, the traditional interface between the compiler and the processor, as defined by the instruction set architecture (ISA), is unable to communicate much of the compiler knowledge to the processor. The approach taken by modern processors such as superscalars is to incorporate purely run-time algorithms in their hardware to perform analyses and optimizations such as detection of instruction-level parallelism. However, these complex hardware implementations can only exploit a small fraction of the parallelism information available to the compiler.The Raw architecture developed at MIT aims to maximally utilize the compiler by fully exposing the hardware and by delegating the hardware\u2019s control completely to the software system. The Raw microprocessor, a set of simple RISC-like processor tiles interconnected with a high-speed 2D mesh network, does not provide hardware implementations for any of the complex algorithms found in conventional microprocessors. Instead, the compiler and the run-time software system fully orchestrate the Raw hardware resources, and they implement run-time analyses and optimizations tailored to the need of each individual application. This novel approach provides many opportunities and challenges for the Raw compiler and run-time system.", "num_citations": "112\n", "authors": ["254"]}
{"title": "Integrating asynchronous task parallelism with MPI\n", "abstract": " Effective combination of inter-node and intra-node parallelism is recognized to be a major challenge for future extreme-scale systems. Many researchers have demonstrated the potential benefits of combining both levels of parallelism, including increased communication-computation overlap, improved memory utilization, and effective use of accelerators. However, current \u201chybrid programming\u201d approaches often require significant rewrites of application code and assume a high level of programmer expertise. Dynamic task parallelism has been widely regarded as a programming model that combines the best of performance and programmability for shared-memory programs. For distributed-memory programs, most users rely on efficient implementations of MPI. In this paper, we propose HCMPI (Habanero-C MPI), an integration of the Habanero-C dynamic task-parallel programming model with the widely used MPI\u00a0\u2026", "num_citations": "107\n", "authors": ["254"]}
{"title": "X10: concurrent programming for modern architectures\n", "abstract": " Two major trends are converging to reshape the landscape of concurrent object-oriented programming languages. First, trends in modern architectures (multi-core, accelerators, high performance clusters such as Blue Gene) are making concurrency and distribution inescapable for large classes of OO programmers. Second, experience with first-generation concurrent OO languages (eg Java threads and synchronization) have revealed several drawbacks of unstructured threads with lock-based synchronization.", "num_citations": "102\n", "authors": ["254"]}
{"title": "Understanding reuse, performance, and hardware cost of dnn dataflow: A data-centric approach\n", "abstract": " The data partitioning and scheduling strategies used by DNN accelerators to leverage reuse and perform staging are known as dataflow, which directly impacts the performance and energy efficiency of DNN accelerators. An accelerator micro architecture dictates the dataflow (s) that can be employed to execute layers in a DNN. Selecting a dataflow for a layer can have a large impact on utilization and energy efficiency, but there is a lack of understanding on the choices and consequences of dataflow, and of tools and methodologies to help architects explore the co-optimization design space.", "num_citations": "100\n", "authors": ["254"]}
{"title": "Software challenges in extreme scale systems\n", "abstract": " Computer systems anticipated in the 2015\u20132020 timeframe are referred to as Extreme Scale because they will be built using massive multi-core processors with 100's of cores per chip. The largest capability Extreme Scale system is expected to deliver Exascale performance of the order of 10 18 operations per second. These systems pose new critical challenges for software in the areas of concurrency, energy efficiency and resiliency. In this paper, we discuss the implications of the concurrency and energy efficiency challenges on future software for Extreme Scale Systems. From an application viewpoint, the concurrency and energy challenges boil down to the ability to express and manage parallelism and locality by exploring a range of strong scaling and new-era weak scaling techniques. For expressing parallelism and locality, the key challenges are the ability to expose all of the intrinsic parallelism and locality\u00a0\u2026", "num_citations": "100\n", "authors": ["254"]}
{"title": "S-cave: Effective ssd caching to improve virtual machine storage performance\n", "abstract": " A unique challenge for SSD storage caching management in a virtual machine (VM) environment is to accomplish the dual objectives: maximizing utilization of shared SSD cache devices and ensuring performance isolation among VMs. In this paper, we present our design and implementation of S-CAVE, a hypervisor-based SSD caching facility, which effectively manages a storage cache in a Multi-VM environment by collecting and exploiting runtime information from both VMs and storage devices. Due to a hypervisor's unique position between VMs and hardware resources, S-CAVE does not require any modification to guest OSes, user applications, or the underlying storage system. A critical issue to address in S-CAVE is how to allocate limited and shared SSD cache space among multiple VMs to achieve the dual goals. This is accomplished in two steps. First, we propose an effective metric to determine the\u00a0\u2026", "num_citations": "96\n", "authors": ["254"]}
{"title": "Method of, system for, and computer program product for performing weighted loop fusion by an optimizing compiler\n", "abstract": " An integer programming formulation for weighted loop fusion is presented. Loop fusion is a well-known program transformation that has shown to be effective in reducing loop overhead and improving register and cache locality. Weighted loop fusion is the problem of finding a legal partition of loop nests into fusible clusters so as to minimize the total inter-cluster edge weights. Past work has shown that the weighted loop fusion problem is NP-hard. Despite the NP-hardness property, the present invention provides optimal solutions that may be found efficiently, in the context of an optimizing compiler, for weighted loop fusion problem sizes that occur in practice. An integer programming formulation for weighted loop fusion with a problem size (number of variables and constraints) that is linearly proportional to the size of the input weighted loop fusion problem is also presented. The integer programming formulation may\u00a0\u2026", "num_citations": "94\n", "authors": ["254"]}
{"title": "A general framework for iteration-reordering loop transformations\n", "abstract": " This paper describes a general framework for representing iteration-reordering transformations. These transformations can be both matrix-based and non-matrix-based. Transformations are defined by rules for mapping dependence vectors, rules for mapping loop bound expressions, and rules for creating new initialization statements. The framework is extensible, and can be used to represent any iteration-reordering transformation. Mapping rules for several common transformations are included in the paper.", "num_citations": "93\n", "authors": ["254"]}
{"title": "Combining register allocation and instruction scheduling\n", "abstract": " We formulate combined register allocation and instruction scheduling within a basic block as a single optimization problem, with an objective cost function that more directly captures the primary measure of interest in code optimization\u2014the completion time of the last instruction. We show that although a simple instance of the combined problem is NP-hard, the combined problem is much easier to solve approximately than graph coloring, which is a common formulation used for the register allocation phase in phase-ordered solutions.Using our framework, we devise a simple and effective heuristic algorithm for the combined problem. This algorithm is called the;-Combined Heuristic; parameters and provide relative weightages for controlling register pressure and instruction parallelism considerations in the combined heuristic. Preliminary experiments indicate that the combined heuristic yields improvements in the range of 16-21% compared to the phase-ordered solutions, when the input graphs contain balanced amount of register pressure and instruction-level parallelism.", "num_citations": "92\n", "authors": ["254"]}
{"title": "Savina-an actor benchmark suite: Enabling empirical evaluation of actor libraries\n", "abstract": " This paper introduces the Savina benchmark suite for actor-oriented programs. Our goal is to provide a standard benchmark suite that enables researchers and application developers to compare different actor implementations and identify those that deliver the best performance for a given use-case. The benchmarks in Savina are diverse, realistic, and represent compute (rather than I/O) intensive applications. They range from popular micro-benchmarks to classical concurrency problems to applications that demonstrate various styles of parallelism. Implementations of the benchmarks on various actor libraries are made publicly available through an open source release. This will allow other developers and researchers to compare the performance of their actor libraries on these common set of benchmarks.", "num_citations": "90\n", "authors": ["254"]}
{"title": "System, method, and program product for instruction scheduling in the presence of hardware lookahead accomplished by the rescheduling of idle slots\n", "abstract": " Instructions are scheduled for execution by a processor having a lookahead buffer by identifying an idle slot in a first instruction schedule of a first basic block of instructions, and by rescheduling the idle slot later in the first instruction schedule. The idle slot is rescheduled by determining if the first basic block of instructions may be rescheduled into a second instruction schedule in which the identified idle slot is scheduled later than in the first instruction schedule. The first basic block of instructions is rescheduled by determining a completion deadline of the first instruction schedule, decreasing the completion deadline, and determining the second instruction schedule based on the decreased completion deadline. Deadlines are determined by computing a rank of each node of a DAG corresponding to the first basic block of instructions; constructing an ordered list of the DAG nodes in nondecreasing rank order; and\u00a0\u2026", "num_citations": "89\n", "authors": ["254"]}
{"title": "The habanero multicore software research project\n", "abstract": " Multiple programming models are emerging to address an increased need for dynamic task parallelism in multicore shared-memory multiprocessors. This poster describes the main components of Rice University's Habanero Multicore Software Research Project, which proposes a new approach to multicore software enablement based on a two-level programming model consisting of a higher-level coordination language for domain experts and a lower-level parallel language for programming experts.", "num_citations": "87\n", "authors": ["254"]}
{"title": "Habanero-Java library: a Java 8 framework for multicore programming\n", "abstract": " With the advent of the multicore era, it is clear that future growth in application performance will primarily come from increased parallelism. We believe parallelism should be introduced early into the Computer Science curriculum to educate students on the fundamentals of parallel computation. In this paper, we introduce the newly-created Habanero-Java library (HJlib), a pure Java 8 library implementation of the pedagogic parallel programming model [12]. HJlib has been used in teaching a sophomore-level course titled\" Fundamentals of Parallel Programming\" at Rice University.", "num_citations": "84\n", "authors": ["254"]}
{"title": "Reducing the overhead of dynamic compilation\n", "abstract": " The execution model for mobile, dynamically\u2010linked, object\u2010oriented programs has evolved from fast interpretation to a mix of interpreted and dynamically compiled execution. The primary motivation for dynamic compilation is that compiled code executes significantly faster than interpreted code. However, dynamic compilation, which is performed while the application is running, introduces execution delay. In this paper we present two dynamic compilation techniques that enable high performance execution while reducing the effect of this compilation overhead. These techniques can be classified as (1) decreasing the amount of compilation performed, and (2) overlapping compilation with execution. We first present and evaluate lazy compilation, an approach used in most dynamic compilation systems in which individual methods are compiled on\u2010demand upon their first invocation. This is in contrast to eager\u00a0\u2026", "num_citations": "84\n", "authors": ["254"]}
{"title": "Optimal weighted loop fusion for parallel programs\n", "abstract": " Nimrod Megiddo Vivek Sarkar IBM Almaden Research Center MIT Laboratory for Computer Science and Tel Aviv University and IBM Software Solutions Division", "num_citations": "82\n", "authors": ["254"]}
{"title": "Data-driven tasks and their implementation\n", "abstract": " Dynamic task parallelism has been identified as a prerequisite for improving productivity and performance on future many-core processors. In dynamic task parallelism, computations are created dynamically and the runtime scheduler is responsible for scheduling the computations across processor cores. The sets of task graphs that can be supported by a dynamic scheduler depend on the underlying task primitives in the parallel programming model, with various classes of fork-join structures used most often in practice. However, many researchers have advocated the benefits of more general task graph structures, and have shown that the use of these task graph structures can lead to improved performance. In this paper, we propose an extension to task parallelism called Data-Driven Tasks (DDTs) that can be used to create arbitrary task graph structures. Unlike a normal task that starts execution upon creation, a\u00a0\u2026", "num_citations": "79\n", "authors": ["254"]}
{"title": "Multi-core implementations of the concurrent collections programming model\n", "abstract": " In this paper we introduce the Concurrent Collections programming model, which builds on past work on TStreams [8]. In this model, programs are written in terms of high-level application-specific operations. These operations are partially ordered according to only their semantic constraints. These partial orderings correspond to data flow and control flow.This approach supports an important separation of concerns. There are two roles involved in implementing a parallel program. One is the role of a domain expert, the developer whose interest and expertise is in the application domain, such as finance, genomics, or numerical analysis. The other is the tuning expert, whose interest and expertise is in performance, including performance on a particular platform. These may be distinct individuals or the same individual at different stages in application development. The tuning expert may in fact be software (such as a static or dynamic optimizing compiler). The Concurrent Collections programming model separates the work of the domain expert (the expression of the semantics of the computation) from the work of the tuning expert (selection and mapping of actual parallelism to a specific architecture). This separation simplifies the task of the domain expert. Writing in this language does not require any reasoning about parallelism or any understanding of the target architecture. The domain expert is concerned only with his or her area of expertise (the semantics of the application). This separation also simplifies the work of the tuning expert. The tuning expert is given the maximum possible freedom to map the computation onto the target architecture\u00a0\u2026", "num_citations": "78\n", "authors": ["254"]}
{"title": "Optimization of array accesses by collective loop transformations\n", "abstract": " In this paper, we investigate the problem of optimizing array accesses across a collection of loops. We demonstrate that a good solution to such a problem should be based on an optimization scheme, called collective loop transformations, that considers all loops simultaneously. In particular, loop reversal, loop interchange and loop fusion are performed collectively on a set of loop nests. The main impact of these transformations is an optimization called array contraction, that saves space and time by converting an array variable into a scalar variable or a buffer containing a small number of scalar variables.This optimization is applicable to general-purpose highperformance architectures. For a multiprocessor architecture, array contraction is performed by executing the producer and consumer loops on separate processors, and by using a smaller buffer for the array communication. For a uniprocessor architecture\u00a0\u2026", "num_citations": "78\n", "authors": ["254"]}
{"title": "Hadoopcl: Mapreduce on distributed heterogeneous platforms through seamless integration of hadoop and opencl\n", "abstract": " As the scale of high performance computing systems grows, three main challenges arise: the programmability, reliability, and energy efficiency of those systems. Accomplishing all three without sacrificing performance requires a rethinking of legacy distributed programming models and homogeneous clusters. In this work, we integrate Hadoop MapReduce with OpenCL to enable the use of heterogeneous processors in a distributed system. We do this by exploiting the implicit data parallelism of mappers and reducers in a MapReduce system. Combining Hadoop and OpenCL provides 1) an easy-to-learn and flexible application programming interface in a high level and popular programming language, 2) the reliability guarantees and distributed file system of Hadoop, and 3) the low power consumption and performance acceleration of heterogeneous processors. This paper presents HadoopCL: an extension to\u00a0\u2026", "num_citations": "75\n", "authors": ["254"]}
{"title": "A compiler framework for restructuring data declarations to enhance cache and TLB effectiveness\n", "abstract": " It has been observed that memory access performance can be improved by restructuring data declarations, using simple transformations such as array dimension padding and inter-array padding (array alignment) to reduce the number of misses in the cache and TLB (translation lookaside buffer). These transformations can be applied to both static and dynamic array variables. In this paper, we provide a padding algorithm for selecting appropriate padding amounts, which takes into account various cache and TLB effects collectively within a single framework. In addition to reducing the number of misses, we identify the importance of reducing the impact of cache miss jamming by spreading cache misses more uniformly across loop iterations.", "num_citations": "75\n", "authors": ["254"]}
{"title": "X10: Programming for hierarchical parallelism and non-uniform data access\n", "abstract": " The challenges faced by current and future-generation largescale systems include: 1) Frequency wall: inability to follow past frequency scaling trends, 2) Memory wall: inability to support a coherent uniform-memory access model with reasonable performance, and 3) Scalability wall: inability to utilize all levels of available parallelism in the system. These challenges manifest themselves as both performance and productivity issues in the use of large-scale systems. X10 is an experimental modern object-oriented programming language being developed to help address the second and third of these three challenges. It is intended for highperformance, high-productivity programming of large-scale computer systems with non-uniformities in data access and coherence, as well as multiple heterogeneous levels of parallelism. This paper provides a summary of the X10 language and programming model, and discusses its applicability to future processor architectures and runtime systems.", "num_citations": "73\n", "authors": ["254"]}
{"title": "Analytical bounds for optimal tile size selection\n", "abstract": " In this paper, we introduce a novel approach to guide tile size selection by employing analytical models to limit empirical search within a subspace of the full search space. Two analytical models are used together: 1) an existing conservative model, based on the data footprint of a tile, which ignores intra-tile cache block replacement, and 2) an aggressive new model that assumes optimal cache block replacement within a tile. Experimental results on multiple platforms demonstrate the practical effectiveness of the approach by reducing the search space for the optimal tile size by 1,307\u00d7 to 11,879\u00d7 for an Intel Core-2-Quad system; 358\u00d7 to 1,978\u00d7 for an Intel Nehalem system; and 45\u00d7 to 1,142\u00d7 for an IBM Power7 system. The execution of rectangularly tiled code tuned by a search of the subspace identified by our model achieves speed-ups of up to 1.40\u00d7 (Intel Core-2 Quad), 1.28\u00d7 (Nehalem) and 1.19\u00d7\u00a0\u2026", "num_citations": "72\n", "authors": ["254"]}
{"title": "Parallel program graphs and their classification\n", "abstract": " We categorize and compare different representations of program dependence graphs, including the Control Flow Graph (CFG) which is a sequential representation lacking data dependences, the Program Dependence Graph (PDG) which is a parallel representation of a sequential program and is comprised of control and data dependences, and more generally, the Parallel Program Graph (PPG) which is a parallel representation of sequential and (inherently) parallel programs and is comprised of parallel control flow edges and synchronization edges. PPGs are classified according to their graphical structure and properties related to deadlock detection and serializability.", "num_citations": "72\n", "authors": ["254"]}
{"title": "Compact representations for control dependence\n", "abstract": " Recently the Program Dependence Graph (PDG) has been shown useful as a basis for solving a variety of problems, including optimization[FOW87], vectorization[BB89], translation to dataflow machines [OBM90], code generation for VLIW machines [GS87a, GS87b], program transformation[Se189, CF89], merging versions of programs [HPR87], and automatic detection and management of parallelism [ABC* 87, ABC* 88, CFS89]. The edges of the PDG consist of control dependence and data dependence edges. The data dependence edges represent the essential data flow relationships of a program [Kuc?\u2018~]. In this paper, we examine the control dependence aspect of the PDG, which summarizes essential control flow relationships in a program. Informally, for nodes X and Y in CFG, Y is control dependent on X if during execution, X can directly affect whether Y is executed. We improve the space and time required\u00a0\u2026", "num_citations": "72\n", "authors": ["254"]}
{"title": "Deadlock-free scheduling of X10 computations with bounded resources\n", "abstract": " In this paper, we address the problem of guaranteeing the absence of physical deadlock in the execution of a parallel program using the async, finish, atomic, and place constructs from the X10 language. First, we extend previous work-stealing memory bound results for fully strict multi-threaded computations to terminally strict multithreaded computations in which one activity may wait for completion of a descendant activity (as in X10's async and finish constructs), not just an immediate child (as in Cilk's spawn and sync constructs). This result establishes physical dead-lock freedom for SMP deployments. Second, we introduce a new class of X10 deployments for clusters, which builds on an underlying Active Message network and the new concept of Doppelg\u00e4nger mode execution of X10 activities. Third, we use this new class of deployments to establish physical deadlock freedom for deployments on clusters of\u00a0\u2026", "num_citations": "69\n", "authors": ["254"]}
{"title": "The Open Community Runtime: A runtime system for extreme scale computing\n", "abstract": " The Open Community Runtime (OCR) is a new runtime system designed to meet the needs of extreme-scale computing. While there is growing support for the idea that future execution models will be based on dynamic tasks, there is little agreement on what else should be included. OCR minimally adds events for synchronization and relocatable data-blocks for data management to form a complete system that supports a wide range of higher-level programming models. This paper lays out the fundamental concepts behind OCR and compares OCR performance to that from MPI for two simple benchmarks. OCR has been developed within an open community model with features supporting flexible algorithm expression weighed against the expected realities of extreme-scale computing: power-constrained execution, aggressive growth in the number of compute resources, deepening memory hierarchies and a low\u00a0\u2026", "num_citations": "67\n", "authors": ["254"]}
{"title": "Efficient selection of vector instructions using dynamic programming\n", "abstract": " Accelerating program performance via SIMD vector units is very common in modern processors, as evidenced by the use of SSE, MMX, VSE, and VSX SIMD instructions in multimedia, scientific, and embedded applications. To take full advantage of the vector capabilities, a compiler needs to generate efficient vector code automatically. However, most commercial and open-source compilers fall short of using the full potential of vector units, and only generate vector code for simple innermost loops. In this paper, we present the design and implementation of an auto-vectorization framework in the back-end of a dynamic compiler that not only generates optimized vector code but is also well integrated with the instruction scheduler and register allocator. The framework includes a novel compile-time efficient dynamic programming-based vector instruction selection algorithm for straight-line code that expands\u00a0\u2026", "num_citations": "66\n", "authors": ["254"]}
{"title": "Immutability specification and its applications\n", "abstract": " A location is said to be immutable if its value and the values of selected locations reachable from it are guaranteed to remain unchanged during a specified time interval. We introduce a framework for immutability specification, and discuss its application to code optimization. Compared with a final declaration, an immutability assertion in our framework can express a richer set of immutability properties along three dimensions\u2014lifetime, reachability and context. We present a framework for processing and verifying immutability annotations in Java, as well as extending optimizations so as to exploit immutability information. Preliminary experimental results show that a significant number (61%) of read accesses could potentially be classified as immutable in our framework. Further, use of immutability information yields substantial reductions (33\u201399%) in the number of dynamic read accesses, and also measurable\u00a0\u2026", "num_citations": "65\n", "authors": ["254"]}
{"title": "Mapping a data-flow programming model onto heterogeneous platforms\n", "abstract": " In this paper we explore mapping of a high-level macro data-flow programming model called Concurrent Collections (CnC) onto heterogeneous platforms in order to achieve high performance and low energy consumption while preserving the ease of use of data-flow programming. Modern computing platforms are becoming increasingly heterogeneous in order to improve energy efficiency. This trend is clearly seen across a diverse spectrum of platforms, from small-scale embedded SOCs to large-scale super-computers. However, programming these heterogeneous platforms poses a serious challenge for application developers. We have designed a software flow for converting high-level CnC programs to the Habanero-C language. CnC programs have a clear separation between the application description, the implementation of each of the application components and the abstraction of hardware platform\u00a0\u2026", "num_citations": "64\n", "authors": ["254"]}
{"title": "Dependence analysis for Java\n", "abstract": " We describe a novel approach to performing data dependence analysis for Java in the presence of Java\u2019s \u201cnon-traditional\u201d language features such as exceptions, synchronization, and memory consistency. We introduce new classes of edges in a dependence graph to model code motion constraints arising from these language features. We present a linear-time algorithm for constructing this augmented dependence graph for an extended basic block.", "num_citations": "63\n", "authors": ["254"]}
{"title": "Experiences using control dependence in PTRAN\n", "abstract": " Experiences using control dependence in PTRAN | Selected papers of the second workshop on Languages and compilers for parallel computing ACM Digital Library home ACM home Google, Inc. (search) Advanced Search Browse About Sign in Register Advanced Search Journals Magazines Proceedings Books SIGs Conferences People More Search ACM Digital Library SearchSearch Advanced Search Browse Browse Digital Library Collections More HomeBrowse by TitleProceedingsSelected papers of the second workshop on Languages and compilers for parallel computingExperiences using control dependence in PTRAN ARTICLE Experiences using control dependence in PTRAN Share on Authors: Ron K. Cytron profile image Ron Cytron View Profile , Jeanne Ferrante profile image J. Ferrante View Profile , Vivek Sarkar profile image V. Sarkar View Profile Authors Info & Affiliations Publication: Selected of \u2013.\u2026", "num_citations": "62\n", "authors": ["254"]}
{"title": "Integrating task parallelism with actors\n", "abstract": " This paper introduces a unified concurrent programming model combining the previously developed Actor Model (AM) and the task-parallel Async-Finish Model (AFM). With the advent of multi-core computers, there is a renewed interest in programming models that can support a wide range of parallel programming patterns. The proposed unified model shows how the divide-and-conquer approach of the AFM and the no-shared mutable state and event-driven philosophy of the AM can be combined to solve certain classes of problems more efficiently and productively than either of the aforementioned models individually. The unified model adds actor creation and coordination to the AFM, while also enabling parallelization within actors. This paper describes two implementations of the unified model as extensions of Habanero-Java and Habanero-Scala. The unified model adds to the foundations of parallel programs\u00a0\u2026", "num_citations": "59\n", "authors": ["254"]}
{"title": "Extended linear scan: An alternate foundation for global register allocation\n", "abstract": " In this paper, we extend past work on Linear Scan register allocation, and propose two Extended Linear Scan (ELS) algorithms that retain the compile-time efficiency of past Linear Scan algorithms while delivering performance that can match or surpass that of Graph Coloring. Specifically, this paper makes the following contributions:               \u2013 We highlight three fundamental theoretical limitations in using Graph Coloring as a foundation for global register allocation, and introduce a basic Extended Linear Scan algorithm, ELS                 0, which addresses all three limitations for the problem of Spill-Free Register Allocation.               \u2013 We introduce the ELS                 1 algorithm which extends ELS                 0 to obtain a greedy algorithm for the problem of Register Allocation with Total Spills.               \u2013 Finally, we present experimental results to compare the Graph Coloring and Extended Linear Scan algorithms\u00a0\u2026", "num_citations": "59\n", "authors": ["254"]}
{"title": "Synergistic challenges in data-intensive science and exascale computing: DOE ASCAC data subcommittee report\n", "abstract": " Synergistic Challenges in Data-Intensive Science and Exascale Computing: DOE ASCAC Data Subcommittee Report \u2014 Northwestern Scholars Skip to main navigation Skip to search Skip to main content Northwestern Scholars Logo Help & FAQ Home Experts Organizations Research Output Grants Core Facilities Search by expertise, name or affiliation Synergistic Challenges in Data-Intensive Science and Exascale Computing: DOE ASCAC Data Subcommittee Report J. Chen, Alok Choudhary, S. Feldman, B. Hendrickson, CR Johnson, R. Mount, V. Sarkar, V. White, D. Williams Electrical and Computer Engineering Research output: Book/Report \u203a Other report Overview Original language English Publisher Department of Energy Office of Science State Published - Mar 2013 Cite this APA Author BIBTEX Harvard Standard RIS Vancouver Chen, J., Choudhary, A., Feldman, S., Hendrickson, B., Johnson, CR, Mount, R., .\u2026", "num_citations": "58\n", "authors": ["254"]}
{"title": "HabaneroUPC++ a Compiler-free PGAS Library\n", "abstract": " The Partitioned Global Address Space (PGAS) programming models combine shared and distributed memory features, providing the basis for high performance and high productivity parallel programming environments. UPC++[39] is a very recent PGAS implementation that takes a library-based approach and avoids the complexities associated with compiler transformations. However, this implementation does not support dynamic task parallelism and only relies on other threading models (eg, OpenMP or pthreads) for exploiting parallelism within a PGAS place.", "num_citations": "57\n", "authors": ["254"]}
{"title": "Phaser accumulators: A new reduction construct for dynamic parallelism\n", "abstract": " A reduction is a computation in which a common operation, such as a sum, is to be performed across multiple pieces of data, each supplied by a separate task. We introduce phaser accumulators, a new reduction construct that meshes seamlessly with phasers to support dynamic parallelism in a phased (iterative) setting. By separating reduction computations into the parts of sending data, performing the computation itself, and retrieving the result, we enable overlap of communication and computation in a manner analogous to that of split-phase barriers. Additionally, this separation enables exploration of implementation strategies that differ as to when the reduction itself is performed: eagerly when the data is supplied, or lazily when a synchronization point is reached. We implement accumulators as extensions to phasers in the Habanero dialect of the X10 programming language. Performance evaluations of the\u00a0\u2026", "num_citations": "57\n", "authors": ["254"]}
{"title": "An experiment in measuring the productivity of three parallel programming languages\n", "abstract": " In May 2005, a 4.5 day long productivity study was performed at the Pittsburgh Supercomputing Center as part of the IBM HPCS/PERCS project, comparing the productivity of three parallel programming languages: C+ MPI, UPC, and the IBM PERCS project\u2019s x10 language. 27 subjects were divided into 3 comparable groups (one per language) and all were asked to parallelize the same serial algorithm: Smith-Waterman local sequence matching\u2013a bio-informatics kernel inspired from the Scalable Synthetic Compact Applications (SSCA) Benchmark, number 1. Two days of tutorials were given for each language, followed by two days of intense parallel programming for the main problem, and a half day of exit interviews. The study participants were mostly Science and CS students from the University of Pittsburgh, with limited or no parallel programming experience.There were two typical ways of solving the sequence matching problem: a wavefront algorithm, which was not scalable because of data dependencies, and yet posed programming challenges because of the frequent synchronization requirements. However, the given problem was shown to also have a subtle domain-specific property, which allowed some ostensible data dependences to be ignored in exchange for redundant computation, and boosted scalability by a great extent. This property was also given as a written hint to all the participants, encouraging them to obtain higher degrees of", "num_citations": "57\n", "authors": ["254"]}
{"title": "An analytical model for loop tiling and its solution\n", "abstract": " The authors address the problem of estimating the performance of loop tiling, an important program transformation for improved memory hierarchy utilization. We introduce an analytical model for estimating the memory cost of a loop nest as a rational polynomial in tile size variables. We also present a constant-time algorithm for finding an optimal solution to the model (i.e., for selecting optimal tile sizes) for the case of doubly nested loops. This solution can be applied to tiling of three loops by performing an iterative search on the value of the first tile size variable, and using the constant-time algorithm at each point in the search to obtain optimal tile size values for the remaining two loops. Our solution is efficient enough to be used in production-quality optimizing compilers, and has been implemented in the IBM XL Fortran product compilers. This solution can also be used by processor designers to efficiently predict\u00a0\u2026", "num_citations": "56\n", "authors": ["254"]}
{"title": "Analysis and optimization of explicity parallel programs using the parallel program graph representation\n", "abstract": " Major changes in processor architecture over the last decade have created a demand for new compiler optimization technologies. Optimizing compilers have risen to this challenge by steadily increasing the uniprocessor performance gap between optimized compiled and unoptimized compiled code to a level that already exceeds the performance gap between two successive generations of processor hardware. These traditional optimizations [2] have been developed in the context of sequential programs--the assumption of sequential control flow is intrinsic to the definition of basic optimization data structures such as the control flow graph (CFG), and pervades all the optimization algorithms. As more and more programs are written in explicitly parallel programming languages, it becomes essential to extend the scope of sequential analysis and optimization techniques to explicitly parallel programs. This extension is\u00a0\u2026", "num_citations": "56\n", "authors": ["254"]}
{"title": "Compilation techniques for parallel systems\n", "abstract": " Over the past two decades tremendous progress has been made in both the design of parallel architectures and the compilers needed for exploiting parallelism on such architectures. In this paper we summarize the advances in compilation techniques for uncovering and effectively exploiting parallelism at various levels of granularity. We begin by describing the program analysis techniques through which parallelism is detected and expressed in form of a program representation. Next compilation techniques for scheduling instruction level parallelism (ILP) are discussed along with the relationship between the nature of compiler support and type of processor architecture. Compilation techniques for exploiting loop and task level parallelism on shared-memory multiprocessors (SMPs) are summarized. Locality optimizations that must be used in conjunction with parallelization techniques for achieving high\u00a0\u2026", "num_citations": "53\n", "authors": ["254"]}
{"title": "POSC\u2014a partitioning and optimizing SISAL compiler\n", "abstract": " Single-assignment languages like SISAL offer parallelism at all levels\u2014among arbitrary operations, conditionals, loop iterations, and function calls. All control and data dependencies are local, and can be easily determined from the program. Various studies of SISAL programs have shown that they contain massive amounts of potential parallelism. There are two major challenges in converting this potential parallelism into real speedup on multiprocessor systems. First, it is important to carefully select the useful parallelism in a SISAL program, so as to obtain good speedup by trading off parallelism with overhead. Second, it is important to do sequential optimizations, so that the sequential components (tasks) of the SISAL program have comparable execution times with sequential languages such as Fortran, Pascal and C. The POSC compiler system described in this paper addresses both issues by integrating\u00a0\u2026", "num_citations": "53\n", "authors": ["254"]}
{"title": "Dynamic optimistic interprocedural analysis: A framework and an application\n", "abstract": " In this paper, we address the problem of dynamic optimistic interprocedural analysis. Our goal is to build on past work on static interprocedural analysis and dynamic optimization by combining their advantages. We present a framework for performing dynamic optimistic interprocedural analysis. the framework is designed to be used in the context of dynamic class loading and dynamic compilation, and includes mechanisms for event notification (on class loading and method compilation) and dependence tracking (to enable invalidation of optimistic assumptions). We illustrate the functionality of the framework by using it to be used in the context of dynamic class loading and dynamic compilation, and includes mechanisms for event notification (on class loading and method compilaton) and dependence tracking (to enable invalidation of optimistic assumptions). We illustrate the functionality of the framework by using it\u00a0\u2026", "num_citations": "52\n", "authors": ["254"]}
{"title": "Compiling and optimizing java 8 programs for gpu execution\n", "abstract": " GPUs can enable significant performance improvements for certain classes of data parallel applications and are widely used in recent computer systems. However, GPU execution currently requires explicit low-level operations such as 1) managing memory allocations and transfers between the host system and the GPU, 2) writing GPU kernels in a low-level programming model such as CUDA or OpenCL, and 3) optimizing the kernels by utilizing appropriate memory types on the GPU. Because of this complexity, in many cases, only expert programmers can exploit the computational capabilities of GPUs through the CUDA/OpenCL languages. This is unfortunate since a large number of programmers use high-level languages, such as Java, due to their advantages of productivity, safety, and platform portability, but would still like to exploit the performance benefits of GPUs. Thus, one challenging problem is how to\u00a0\u2026", "num_citations": "51\n", "authors": ["254"]}
{"title": "Communication optimizations for distributed-memory X10 programs\n", "abstract": " X10 is a new object-oriented PGAS (Partitioned Global Address Space) programming language with support for distributed asynchronous dynamic parallelism that goes beyond past SPMD message-passing models such as MPI and SPMD PGAS models such as UPC and Co-Array Fortran. The concurrency constructs in X10 make it possible to express complex computation and communication structures with higher productivity than other distributed-memory programming models. However, this productivity often comes at the cost of high performance overhead when the language is used in its full generality. This paper introduces high-level compiler optimizations and transformations to reduce communication and synchronization overheads in distributed-memory implementations of X10 programs. Specifically, we focus on locality optimizations such as scalar replacement and task localization, combined with\u00a0\u2026", "num_citations": "50\n", "authors": ["254"]}
{"title": "X10: an experimental language for high productivity programming of scalable systems\n", "abstract": " It is well established that application development productivity is a significant bottleneck in the time to solution for obtaining production applications on High-End Computing (HEC) systems. Previously, we introduced a simple model for defining application development productivity in the presence of multiple expertise levels, and used this model to motivate the programming model and tools solution being pursued in the IBM PERCS project [9]. In this paper, we describe X10, an experimental language that embodies a new parallel programming model serves as the foundation for multiple productivity-improving technologies in PERCS ranging from visualization and refactoring tools to static and dynamic optimizing compilers.", "num_citations": "48\n", "authors": ["254"]}
{"title": "Jalape\u00f1o-a compiler-supported Java virtual machine for servers\n", "abstract": " In this paper, we give an overview of the Jalape~ no Java Virtual Machine (JVM) research project at the IBM TJ Watson Research Center. The goal of Jalape~ no is to expand the frontier of JVM technologies for server machines. As reported in the paper, several of the design and implementation decisions in Jalape~ no depend heavily on compiler support. Two noteworthy features of the Jalape~ no JVM are as follows. First, the Jalape~ no JVM takes a compile-only approach to program execution. Instead of providing both an interpreter and a JIT compiler as in other JVMs, bytecodes are always translated to machine code before they are executed. Second, the Jalape~ no JVM is itself implemented in Java! This design choice brings with it several advantages as well as technical challenges. The Jalape~ no project was initiated in January 1998 and is work-in-progress. This paper summarizes our design decisions and early experiences in working towards our goal of building a high-performance JVM f...", "num_citations": "48\n", "authors": ["254"]}
{"title": "System, method, and program product for loop instruction scheduling hardware lookahead\n", "abstract": " Improved scheduling of instructions within a loop for execution by a computer system having hardware lookahead is provided. A dependence graph is constructed which contains all the nodes of a dependence graph corresponding to the loop, but which only contains loop-independent dependence edges. A start node simulating a previous iteration of the loop may be added to the dependence graph, and an end node simulating a next iteration of the loop may also added to the dependence graph. A loop-independent edge between a source node and the start node is added to the dependence graph, and a loop-independent edge between a sink node and the end node is added to the dependence graph. Loop-carried edges which satisfy a computed lower bound on the time required for a single loop iteration are eliminated from a dependence graph, and loop-carried edges which do not satisfy the computed lower\u00a0\u2026", "num_citations": "47\n", "authors": ["254"]}
{"title": "Chunking parallel loops in the presence of synchronization\n", "abstract": " Modern languages for shared-memory parallelism are moving from a bulk-synchronous Single Program Multiple Data (SPMD) execution model to lightweight Task Parallel execution models for improved productivity. This shift is intended to encourage programmers to express the ideal parallelism in an application at a fine granularity that is natural for the underlying domain, while delegating to the compiler and runtime system the job of extracting coarser-grained useful parallelism for a given target system. A simple and important example of this separation of concerns between ideal and useful parallelism can be found in chunking of parallel loops, where the programmer expresses ideal parallelism by declaring all iterations of a loop to be parallel and the implementation exploits useful parallelism by executing iterations of the loop in sequential chunks.", "num_citations": "46\n", "authors": ["254"]}
{"title": "Intermediate language extensions for parallelism\n", "abstract": " An Intermediate Language (IL) specifies a program at a level of abstraction that includes precise semantics for state updates and control flow, but leaves unspecified the low-level software and hardware mechanisms that will be used to implement the semantics. Past ILs have followed the Von Neumann execution model by making sequential execution the default, and by supporting parallelism with runtime calls for lower-level mechanisms such as threads and locks. Now that the multicore trend is making parallelism the default execution model for all software, it behooves us as a community to study the fundamental requirements in parallel execution models and explore how they can be supported by first-class abstractions at the IL level.", "num_citations": "42\n", "authors": ["254"]}
{"title": "Dynamic task parallelism with a GPU work-stealing runtime system\n", "abstract": " NVIDIA\u2019s Compute Unified Device Architecture (CUDA) enabled GPUs become accessible to mainstream programming. Abundance of simple computational cores and high memory bandwidth make GPUs ideal candidates for data parallel applications. However, its potential for executing applications that combine task and data parallelism has not been explored in detail. CUDA does not provide a viable interface for creating dynamic tasks and handling load balancing issues. Any support for such has to be orchestrated entirely by the CUDA programmer today.             In this work, we introduce a finish-async style API to GPU device programming as first step towards task parallelism. We present the design and implementation details of our new intra-device inter-SM work-stealing runtime system. We compare performance results using our runtime to direct execution on the device as well as past work on GPU\u00a0\u2026", "num_citations": "42\n", "authors": ["254"]}
{"title": "Interprocedural load elimination for dynamic optimization of parallel programs\n", "abstract": " Load elimination is a classical compiler transformation that is increasing in importance for multi-core and many-core architectures. The effect of the transformation is to replace a memory access, such as a read of an object field or an array element, by a read of a compiler-generated temporary that can be allocated in faster and more energy-efficient storage structures such as registers and local memories (scratchpads). Unfortunately, current just-in-time and dynamic compilers perform load elimination only in limited situations. In particular, they usually make worst-case assumptions about potential side effects arising from parallel constructs and method calls. These two constraints interact with each other since parallel constructs are usually translated to low-level runtime library calls. In this paper, we introduce an interprocedural load elimination algorithm suitable for use in dynamic optimization of parallel programs\u00a0\u2026", "num_citations": "42\n", "authors": ["254"]}
{"title": "Practical permissions for race-free parallelism\n", "abstract": " Type systems that prevent data races are a powerful tool for parallel programming, eliminating whole classes of bugs that are both hard to find and hard to fix. Unfortunately, it is difficult to apply previous such type systems to \u201creal\u201d programs, as each of them are designed around a specific synchronization primitive or parallel pattern, such as locks or disjoint heaps; real programs often have to combine multiple synchronization primitives and parallel patterns. In this work, we present a new permissions-based type system, which we demonstrate is practical by showing that it supports multiple patterns (e.g., task parallelism, object isolation, array-based parallelism), and by applying it to a suite of non-trivial parallel programs. Our system also has a number of theoretical advances over previous work on permissions-based type systems, including aliased write permissions and a simpler way to store permissions in\u00a0\u2026", "num_citations": "41\n", "authors": ["254"]}
{"title": "A transformation framework for optimizing task-parallel programs\n", "abstract": " Task parallelism has increasingly become a trend with programming models such as OpenMP 3.0, Cilk, Java Concurrency, X10, Chapel and Habanero-Java (HJ) to address the requirements of multicore programmers. While task parallelism increases productivity by allowing the programmer to express multiple levels of parallelism, it can also lead to performance degradation due to increased overheads. In this article, we introduce a transformation framework for optimizing task-parallel programs with a focus on task creation and task termination operations. These operations can appear explicitly in constructs such as async, finish in X10 and HJ, task, taskwait in OpenMP 3.0, and spawn, sync in Cilk, or implicitly in composite code statements such as foreach and ateach loops in X10, forall and foreach loops in HJ, and parallel loop in OpenMP. Our framework includes a definition of data dependence in task-parallel\u00a0\u2026", "num_citations": "40\n", "authors": ["254"]}
{"title": "Regmutex: Inter-warp gpu register time-sharing\n", "abstract": " Registers are the fastest and simultaneously the most expensive kind of memory available to GPU threads. Due to existence of a great number of concurrently executing threads, and the high cost of context switching mechanisms, contemporary GPUs are equipped with large register files. However, to avoid over-complicating the hardware, registers are statically assigned and exclusively dedicated to threads for the entire duration of the thread's lifetime. This decomposition takes into account the maximum number of live registers at any given point in the GPU binary although the points at which all the requested registers are used may constitute only a small fraction of the whole program. Therefore, a considerable portion of the register file remains under-utilized. In this paper, we propose a software-hardware co-mechanism named RegMutex (Register Mutual Exclusion) to share a subset of physical registers between\u00a0\u2026", "num_citations": "39\n", "authors": ["254"]}
{"title": "CnC-CUDA: declarative programming for GPUs\n", "abstract": " The computer industry is at a major inflection point in its hardware roadmap due to the end of a decades-long trend of exponentially increasing clock frequencies. Instead, future computer systems are expected to be built using homogeneous and heterogeneous many-core processors with 10\u2019s to 100\u2019s of cores per chip, and complex hardware designs to address the challenges of concurrency, energy efficiency and resiliency. Unlike previous generations of hardware evolution, this shift towards many-core computing will have a profound impact on software. These software challenges are further compounded by the need to enable parallelism in workloads and application domains that traditionally did not have to worry about multiprocessor parallelism in the past. A recent trend in mainstream desktop systems is the use of graphics processor units (GPUs) to obtain order-of-magnitude performance\u00a0\u2026", "num_citations": "39\n", "authors": ["254"]}
{"title": "Delegated isolation\n", "abstract": " Isolation---the property that a task can access shared data without interference from other tasks---is one of the most basic concerns in parallel programming. In this paper, we present Aida, a new model of isolated execution for parallel programs that perform frequent, irregular accesses to pointer-based shared data structures. The three primary benefits of Aida are dynamism, safety and liveness guarantees, and programmability. First, Aida allows tasks to dynamically select and modify, in an isolated manner, arbitrary fine-grained regions in shared data structures, all the while maintaining a high level of concurrency. Consequently, the model can achieve scalable parallelization of regular as well as irregular shared-memory applications. Second, the model offers freedom from data races, deadlocks, and livelocks. Third, no extra burden is imposed on programmers, who access the model via a simple, declarative\u00a0\u2026", "num_citations": "38\n", "authors": ["254"]}
{"title": "Method of, system for, and computer program product for minimizing loop execution time by optimizing block/tile sizes\n", "abstract": " An optimized set of block sizes for a nest of loops for improved data locality is determined by estimating a memory cost per iteration as a function of the block sizes and selecting a set of block sizes that yield a minimum memory cost per iteration subject to a plurality of constraints. An objective function is an estimated memory cost per iteration as a function of the block sizes. An optimal solution is provided by evaluating the objective function for candidate points in the iteration space. These candidate points comprise: corner points of the iteration space, intersections between constraints and the edges of the iteration space, zero-derivative points for the curve that is the intersection of constraints with the objective function, intersection points between constraints, and local optimum of the objective function. Alternatively, an optimized set of block sizes for a nest of loops may be determined by iteratively searching the\u00a0\u2026", "num_citations": "37\n", "authors": ["254"]}
{"title": "Location consistency: Stepping beyond the barriers of memory coherence and serializability\n", "abstract": " A memory consistency model represents a binding\" contract\" between software and hardware in a shared-memory multiprocessor system. It is important to provide a memory consistency model that is easy to understand and that also facilitates efficient implementation. The memory consistency model that has been most commonly used in past work is sequential consistency (SC), which requires the execution of a parallel program to appear as some interleaving of the memory operations on a sequential machine. To reduce the rigid constraints of the SC model, several relaxed consistency models have been proposed, notably weak ordering (or weak consistency)(WC), release consistency (RC), data-race-free-0, and data-race-free-1. These models allow performance optimizations to be correctly applied, while guaranteeing that sequential consistency is retained for a specified class of programs. We call these models SCderived models. A central assumption in the definitions of all SC-derived memory consist...", "num_citations": "37\n", "authors": ["254"]}
{"title": "Oil and water can mix: An integration of polyhedral and ast-based transformations\n", "abstract": " Optimizing compilers targeting modern multi-core machines require complex program restructuring to expose the best combinations of coarse- and fine-grain parallelism and data locality. The polyhedral compilation model has provided significant advancements in the seamless handling of compositions of loop transformations, thereby exposing multiple levels of parallelism and improving data reuse. However, it usually implements abstract optimization objectives, for example \"maximize data reuse\", which often does not deliver best performance, e.g., The complex loop structures generated can be detrimental to short-vector SIMD performance. In addition, several key transformations such as pipeline-parallelism and unroll-and-jam are difficult to express in the polyhedral framework. In this paper, we propose a novel optimization flow that combines polyhedral and syntactic/AST-based transformations. It generates\u00a0\u2026", "num_citations": "36\n", "authors": ["254"]}
{"title": "Automatic parallelization for symmetric shared-memory multiprocessors\n", "abstract": " The trend in workstation hardware is towards symmetric shared-memory multiprocessors (SMPs). User expectations are for (largely) automatic exploitation of parallelism on an SMP, similar to automatic exploitation of modern processor features such as caches and instruction scheduling. In this paper, we present our solution to automatic SMP parallelization. Our solution is unique in its robust support for unbalanced processor loads and nesting of parallel loops and parallel sections, in conjunction with its tight integration with high-order transformations for improved uniprocessor performance, so that the speedup due to parallelism is truly a multiplicative speedup over highly optimized uniprocessor execution times.", "num_citations": "36\n", "authors": ["254"]}
{"title": "A concurrent execution semantics for parallel program graphs and program dependence graphs\n", "abstract": " In this paper, we present a concurrent execution semantics for Parallel Program Graphs (PPGs), a general parallel program representation that includes Program Dependence Graphs (PDGs) and sequential programs. We believe that this semantics is natural to the programmer's way of thinking, and that it also provides a suitable execution model for efficient implementation on real architectures. To demonstrate the robustness of our semantics, we prove a Reordering Theorem which states that a PPG's semantics does not depend on the order in which parallel nodes are executed, and an Equivalence Theorem which states that the semantics of a sequential program is identical to the semantics of its PDG.", "num_citations": "36\n", "authors": ["254"]}
{"title": "Modeling the conflicting demands of parallelism and temporal/spatial locality in affine scheduling\n", "abstract": " The construction of effective loop nest optimizers and parallelizers remains challenging despite decades of work in the area. Due to the increasing diversity of loop-intensive applications and to the complex memory/computation hierarchies in modern processors, optimization heuristics are pulled towards conflicting goals, highlighting the lack of a systematic approach to optimizing locality and parallelism. Acknowledging these conflicting demands on loop nest optimization, we propose an algorithmic template capable of modeling the multi-level parallelism and the temporal/spatial locality of multiprocessors and accelerators. This algorithmic template orchestrates a collection of parameterizable, linear optimization problems over a polyhedral space of semantics-preserving transformations. While the overall problem is not convex, effective algorithms can be derived from this template delivering unprecedented\u00a0\u2026", "num_citations": "34\n", "authors": ["254"]}
{"title": "Reducing task creation and termination overhead in explicitly parallel programs\n", "abstract": " There has been a proliferation of task-parallel programming systems to address the requirements of multicore programmers. Current production task-parallel systems include Cilk++, Intel Threading Building Blocks, Java Concurrency, .Net Task Parallel Library, OpenMP 3.0, and current research task-parallel languages include Cilk, Chapel, Fortress, X10, and Habanero-Java (HJ). It is desirable for the programmer to express all the parallelism intrinsic to their algorithm in their code for forward scalability and portability, but the overhead incurred by doing so can be prohibitively large in today's systems. In this paper, we address the problem of reducing the total amount of overhead incurred by a program due to excessive task creation and termination. We introduce a transformation framework to optimize task-parallel programs with finish, forall and next statements. Our approach includes elimination of redundant task\u00a0\u2026", "num_citations": "34\n", "authors": ["254"]}
{"title": "PTRAN\u2014the IBM parallel translation system\n", "abstract": " PTRAN\u2014the IBM parallel translation system | Parallel functional languages and compilers ACM Digital Library Logo ACM Logo Google, Inc. (search) Advanced Search Browse About Sign in Register Advanced Search Journals Magazines Proceedings Books SIGs Conferences People More Search ACM Digital Library SearchSearch Advanced Search Browse Browse Digital Library Collections More HomeBrowse by TitleBooksParallel functional languages and compilersPTRAN\u2014the IBM parallel translation system chapter PTRAN\u2014the IBM parallel translation system Share on Author: Vivek Sarkar profile image Vivek Sarkar View Profile Authors Info & Affiliations Publication: Parallel functional languages and compilersMay 1991 Pages 309\u2013391https://doi.org/10.1145/107214.129260 6citation 0 Downloads Metrics Total Citations6 Total Downloads0 Last 12 Months0 Last 6 weeks0 Get Citation Alerts New Citation Alert \u2026", "num_citations": "34\n", "authors": ["254"]}
{"title": "North Korea now\n", "abstract": " North Korea now Page 1 August 2018 \u2013 vol 34 \u2013 no 4 every two months print ISSN 0268-540X online ISSN 1467-8322 available online at www.wileyonlinelibrary.com/journal/anth anthropology today Director of the RAI: David Shankland Editor: Gustaaf Houtman Editorial Consultant: Sean Kingston Reviews Editor: Hayder Al-Mohammad News Editor: Matthias Kloft Copy Editor: Miranda Irving Production Consultant: Dominique Remars Editorial Panel: Monique Borgerhoff Mulder, Candis Callison, Richard Fardon, Alma Gottlieb, Hugh Gusterson, Ulf Hannerz, Michael Herzfeld, Thomas Hylland Eriksen, Solomon Katz, G\u00edsli P\u00e1lsson, Jo\u00e3o de Pina-Cabral, Gustavo Lins Ribeiro, Catherine Lutz, Howard Morphy, John Postill, Alexander F. Robertson, Nancy Scheper-Hughes, Cris Shore, Michael Wesch Editorial address: Please read Notes to Contributors before making submissions (http:// www.therai.org.uk/at/). : http:// at... -.\u2026", "num_citations": "33\n", "authors": ["254"]}
{"title": "Cooperative scheduling of parallel tasks with general synchronization patterns\n", "abstract": " In this paper, we address the problem of scheduling parallel tasks with general synchronization patterns using a cooperative runtime. Current implementations for task-parallel programming models provide efficient support for fork-join parallelism, but are unable to efficiently support more general synchronization patterns such as locks, futures, barriers and phasers. We propose a novel approach to addressing this challenge based on cooperative scheduling with one-shot delimited continuations (OSDeConts) and event-driven controls (EDCs). The use of OSDeConts enables the runtime to suspend a task at any point (thereby enabling the task\u2019s worker to switch to another task) whereas other runtimes may have forced the task\u2019s worker to be blocked. The use of EDCs ensures that identification of suspended tasks that are ready to be resumed can be performed efficiently. Furthermore, our approach is more\u00a0\u2026", "num_citations": "33\n", "authors": ["254"]}
{"title": "Synchronization using counting semaphores\n", "abstract": " This paper studies the optimization problem of enforcing a dependence graph with the minimum number of synchronization operations. For a dependence graph with N vertices, it is shown that binary semaphores may require \u039f (N 2) operations, compared to \u039f (N) operations for counting semaphores. Though the optimization problem of using the minimum number of counting semaphore operations is shown to be NP-complete, we present an approximation algorithm that is observed to be very close to optimal (within 0.5%) on small, randomly generated dependence graphs. A surprising property of the problem is that the inclusion (rather than removal) of transitive edges can actually help reduce the number of synchronization operations.", "num_citations": "32\n", "authors": ["254"]}
{"title": "Machine-learning-based performance heuristics for runtime cpu/gpu selection\n", "abstract": " High-level languages such as Java increase both productivity and portability with productive language features such as managed runtime, type safety, and precise exception semantics. Additionally, Java 8 provides parallel stream APIs with lambda expressions to facilitate parallel programming for mainstream users of multi-core CPUs and many-core GPUs. These high-level APIs avoid the complexity of writing natively running parallel programs with OpenMP and CUDA/OpenCL through Java Native Interface (JNI). The adoption of such high-level programming models offers opportunities for enabling compilers to perform parallel-aware optimizations and code generation.", "num_citations": "31\n", "authors": ["254"]}
{"title": "Compiler-driven data layout transformation for heterogeneous platforms\n", "abstract": " Modern heterogeneous systems comprise of CPU cores, GPU cores, and in some cases, accelerator cores. Each of these computational cores have very different memory hierarchies, making it challenging to efficiently map the data structures of an application to these memory hierarchies automatically. In this paper, we present a compiler-driven data layout transformation framework for heterogeneous platforms. We integrate our data layout framework with the data parallel construct, forasync, of Habanero-C and enable the same source code to be compiled with different data layouts for various architectures. The programmer or an auto-tuner specifies a schema of the data layout. Our compiler infrastructure generates efficient code for different architectures based on the meta information provided in the schema. Our experimental results show significant benefits from the compiler-driven data layout\u00a0\u2026", "num_citations": "31\n", "authors": ["254"]}
{"title": "Declarative aspects of memory management in the concurrent collections parallel programming model\n", "abstract": " Concurrent Collections (CnC) is a declarative parallel language that allows the application developer to express their parallel application as a collection of high-level computations called steps that communicate via single-assignment data structures called items.", "num_citations": "30\n", "authors": ["254"]}
{"title": "False sharing elimination by selection of runtime scheduling parameters\n", "abstract": " False sharing can be a source of significant overhead on shared-memory multiprocessors. Several program restructuring techniques to reduce false sharing have been proposed in past work. In this paper, we propose an approach for elimination of false sharing based solely on selection of runtime schedule parameters for parallel loops. This approach leads to more portable code since only the schedule parameters need to be changed to target different multiprocessors. Also, the guarantee of elimination (rather than reduction) of false sharing in a parallel loop can significantly reduce the bookkeeping overhead in some memory consistency mechanisms. We present some preliminary experimental results for this approach.", "num_citations": "30\n", "authors": ["254"]}
{"title": "Pedagogy and tools for teaching parallel computing at the sophomore undergraduate level\n", "abstract": " As the need for multicore-aware programmers rises in both science and industry, Computer Science departments in universities around the USA are having to rethink their parallel computing curriculum. At Rice University, this rethinking took the shape of COMP 322, an introductory parallel programming course that is required for all Bachelors students. COMP 322 teaches students to reason about the behavior of parallel programs, educating them in both the high level abstractions of task-parallel programming as well as the nitty gritty details of working with threads in Java.In this paper, we detail the structure, principles, and experiences of COMP 322, gained from 6 years of teaching parallel programming to second-year undergraduates. We describe in detail two particularly useful tools that have been integrated into the curriculum: the HJlibparallel programming library and the Habanero Autograder for parallel\u00a0\u2026", "num_citations": "29\n", "authors": ["254"]}
{"title": "An extended polyhedral model for SPMD programs and its use in static data race detection\n", "abstract": " Despite its age, SPMD (Single Program Multiple Data) parallelism continues to be one of the most popular parallel execution models in use today, as exemplified by OpenMP for multicore systems and CUDA and OpenCL for accelerator systems. The basic idea behind the SPMD model, which makes it different from task-parallel models, is that all logical processors (worker threads) execute the same program with sequential code executed redundantly and parallel code executed cooperatively. In this paper, we extend the polyhedral model to enable analysis of explicitly parallel SPMD programs and provide a new approach for static detection of data races in SPMD programs using the extended polyhedral model. We evaluate our approach using 34 OpenMP programs from the OmpSCR and PolyBench-ACC (PolyBench-ACC derives from the PolyBench benchmark suite and provides OpenMP, OpenACC\u00a0\u2026", "num_citations": "29\n", "authors": ["254"]}
{"title": "Polyhedral optimizations of explicitly parallel programs\n", "abstract": " The polyhedral model is a powerful algebraic framework that has enabled significant advances to analysis and transformation of sequential affine (sub)programs, relative to traditional AST-based approaches. However, given the rapid growth of parallel software, there is a need for increased attention to using polyhedral frameworks to optimize explicitly parallel programs. An interesting side effect of supporting explicitly parallel programs is that doing so can also enable optimization of programs with unanalyzable data accesses within a polyhedral framework. In this paper, we address the problem of extending polyhedral frameworks to enable analysis and transformation of programs that contain both explicit parallelism and unanalyzable data accesses. As a first step, we focus on OpenMP loop parallelism and task parallelism, including task dependences from OpenMP 4.0. Our approach first enables conservative\u00a0\u2026", "num_citations": "29\n", "authors": ["254"]}
{"title": "In-register parameter caching for dynamic neural nets with virtual persistent processor specialization\n", "abstract": " Dynamic neural networks enable higher representation flexibility compared to networks with a fixed architecture and are extensively deployed in problems dealing with varying input-induced network structure, such as those in Natural Language Processing. One of the standard optimizations used in static net training is persistency of recurrent weights on the chip. In dynamic nets, possibly-inhomogeneous computation graph for every input prevents caching recurrent weights in GPU registers. Therefore, existing solutions suffer from excessive recurring off-chip memory loads as well as compounded kernel launch overheads leading to underutilization of GPU SMs. In this paper, we present a software system that enables persistency of weight matrices during the training of dynamic neural networks on the GPU. Before the training begins, our approach named Virtual Persistent Processor Specialization (VPPS\u00a0\u2026", "num_citations": "28\n", "authors": ["254"]}
{"title": "Minimum lock assignment: A method for exploiting concurrency among critical sections\n", "abstract": " In this paper we propose a lock assignment technique to simplify the mutual exclusion enforcement in multithreaded programs. Programmers are allowed to annotate the regions of code that are expected to be mutually exclusive as critical sections, without using explicit locks. The compiler then automatically infers an assignment of the minimum number of locks to critical sections by solving the Minimum Lock Assignment (MLA) problem so as to enforce mutual exclusion without any loss of concurrency. We show that the MLA problem is NP-hard. We have proposed a heuristic to solve the MLA problem, and tested the optimality of the heuristic with the Integer Linear Programming (ILP) solver. We have also tested the efficiency of the heuristic using scientific applications, from which we obtain up to 30% performance gain with respect to the programs in which all critical sections are controlled by a single lock.", "num_citations": "28\n", "authors": ["254"]}
{"title": "Method of, system for, and computer program product for providing quick fusion in WHERE constructs\n", "abstract": " FORTRAN WHERE construct compilation and optimization is provided by excluding an assignment statement containing a transformational intrinsic function from loop fusion of the WHERE construct. To perform this loop fusion, intrastatement dependence analysis is performed within each assignment statement of the WHERE construct, and then interstatement dependence analysis is performed between each assignment statement and assignment statements subsequent to the assignment statement. Responsive to this dependence analysis, pairs of assignment statements which may not be fused into a single loop are identified, and non-fusion boundaries between adjacent assignment statements where assignment statements preceding a non-fusion boundary and assignment statements subsequent to the non-fusion boundary may not be fused into a single loop are identified. This fusion analysis yields a loop\u00a0\u2026", "num_citations": "27\n", "authors": ["254"]}
{"title": "Accelerating habanero-java programs with opencl generation\n", "abstract": " The initial wave of programming models for general-purpose computing on GPUs, led by CUDA and OpenCL, has provided experts with low-level constructs to obtain significant performance and energy improvements on GPUs. However, these programming models are characterized by a challenging learning curve for non-experts due to their complex and low-level APIs. Looking to the future, improving the accessibility of GPUs and accelerators for mainstream software developers is crucial to bringing the benefits of these heterogeneous architectures to a broader set of application domains. A key challenge in doing so is that mainstream developers are accustomed to working with high-level managed languages, such as Java, rather than lower-level native languages such as C, CUDA, and OpenCL.", "num_citations": "26\n", "authors": ["254"]}
{"title": "Optimization of lattice Boltzmann simulation with graphics-processing-unit parallel computing and the application in reservoir characterization\n", "abstract": " Shale permeability is sufficiently low to require an unconventional scale of stimulation treatments, such as very-large-volume, high-rate, multistage hydraulic-fracturing applications. Upscaling of hydrocarbon transport processes in shales is challenging because of the low permeability and strong heterogeneity. Rock characterization with high-resolution imaging [X-ray tomography and scanning electron microscope (SEM)] is usually highly localized and contains significant uncertainties because of the small field of view. Therefore, an effective high-performance computing method is required to collect information over a larger scale to meet the ergodicity requirement in upscaling. The lattice Boltzmann (LB) method has received significant attention in computational fluid dynamics because of its capability in coping with complicated boundary conditions. A combination of high-resolution imaging and LB simulation is a\u00a0\u2026", "num_citations": "25\n", "authors": ["254"]}
{"title": "Automatic data layout generation and kernel mapping for cpu+ gpu architectures\n", "abstract": " The ubiquity of hybrid CPU+ GPU architectures has led to renewed interest in automatic data layout generation owing to the fact that data layouts have a large impact on performance, and that different data layouts yield the best performance on CPUs vs. GPUs. Unfortunately, current programming models still fail to provide an effective solution to the problem of automatic data layout generation for CPU+ GPU processors. Specifically, the interaction among wholeprogram data layout optimizations, data movement optimizations, and mapping of kernels across heterogeneous cores pose a major challenge to current programming systems. In this paper, we introduce a novel two-level hierarchical formulation of the data layout and kernel mapping problem for modern heterogeneous architectures. The bottom level formulation deals with the data layout problem for a parallel code region on a given processor, which is\u00a0\u2026", "num_citations": "25\n", "authors": ["254"]}
{"title": "Language extensions in support of compiler parallelization\n", "abstract": " In this paper, we propose an approach to automatic compiler parallelization based on language extensions that is applicable to a broader range of program structures and application domains than in past work. As a complement to ongoing work on high productivity languages for explicit parallelism, the basic idea in this paper is to make sequential languages more amenable to compiler parallelization by adding enforceable declarations and annotations. Specifically, we propose the addition of annotations and declarations related to multidimensional arrays, points, regions, array views, parameter intents, array and object privatization, pure methods, absence of exceptions, and gather/reduce computations. In many cases, these extensions are also motivated by best practices in software engineering, and can also contribute to performance improvements in sequential code. A detailed case study of the Java\u00a0\u2026", "num_citations": "25\n", "authors": ["254"]}
{"title": "Experiences with an smp implementation for x10 based on the java concurrency utilities\n", "abstract": " } &\u00a9 9 &%\u00a3\u00a6 Ia&\u00a9\u00a6\u00a3\u00a6 Ia&S\u00a1 0\u00a5\u00a4 $2 p\u00a1 0&% p\u00a3\u00a6@ BA\u00a9\u00a6@ B \u00a7 S\u00a1\"! G\u00a1 0\u00a3 $ \u00a7 ! G70! $\u00a5 3 25$\u00a6\u00a4\u00a5 \u00e0 \u00f3 u \u00a7 d \u00f4\u00cb \u00cc\u00ee\u00d1S SS 1\u00c0a\u00ef6 ww G\u00f0S\u00e2S HG w H $ f 8! $\u00a5'\u00a5 p\u00a3\u00a6\u00a5 q\u00a3\u00a6 \u00a7 S\u00a1'70H (% 4% 8 (T! $\u00a5 q! $ \u00a7 yH\u00a1 0 \u00a7 \u00a9\u00a5'\u00a3\u00a6 \u00e4 \u00a7 B\u00a1 0\u00a1 0&\u00a9 u at9 \u00ee\u00d1S SS 1\u00c0a\u00ef6 ww G\u00f0S\u00e2S HG w H $8! $\u00a5'\u00a5 sg} &\u00a9@! $\u00a3\u00a6 \u00a7 2Q4\u00a9 \u00a7 \u00a9 8x\u00a1 0\u00a3 $ \u00a7 ! 1\u00a3 D\u00a1\u00a2 C\u00a4\u00a3\u00a6 \u00a7 \u00a1 0&\u00a9\u00a3\u00a6\u00a5 yH\u00a1 0 \u00a7 %\u00a5'\u00a3 $ \u00a7 E\u00a3\u00a6\u00a5\u00a1 0&%! $#%\u00a3\u00a6\u00a3 D\u00a1 rC 2517\u00a1 0&\u00a9 U\u00a1 0&670! a (T AjS\u00e4\u00a1 0P\u00a3\u00a6 \u00a7 \u00a9 8 70! $\u00a5'! $ \u00a7 \u00a9((6 8 70! 1\u00a5'x\u00a1 0&\u00a9 U \u00a7 H4%@ U# j 7 $2 q\u00a1 0&670! a (6\u00a5 3\u00a3 \u00a7 X\u00a1 0&\u00a9 p AjS $ c! 1\u00a5 3 70 iS4\u00a9\u00a3 D70 (# HC\u00a1 0&% q sot uo 704\u00a9 \u00a7 S\u00a1 0\u00a3\u00a6@ B\u00a5 C%\u00a5\u00a1 0@ Ps \u00f3\u00a2 2! 1 \u00a7 d sot uU! $8\u00a1 0\u00a3\u00a6 FH\u00a3 D\u00a1 rCP Aj x7'25 $70@ B\u00a5 g! x#% S8\" vH\u00a3\u00a6 \u00a7 \u00a9 IB \u00e4Aj x7\"! 1\u00a1 0\u00a3\u00a6 \u00e4 \u00a7 \u00d9~\u00ac\u00ea \u00ba5\u00aa\u00daHb $\u00b6\u00b7 $'\" \u00dc $79\u00ac j\u00dc'\u00eb $ h xb\u00a9\u00a1 0&% \u00a7 X\u00a1 0&% 9 w! F $!\u00a1 0&% 70! a () xy% 8 4%\u00a1 0\u00a3\u00a6 \u00a7 \u00a9 I\u00a1 0&\u00a9 sot ud! 18\u00a1 0\u00a3\u00a6 FH\u00a3\u00a6\u00a1\u00a2 C\u00a4\u00a3\u00a6\u00a6\u00a4! $\u00a6\u00a5'f#\u00a9\u00a6 S8xv! $ \u00a7 \u00a9(\u00a4 \u00a7 \u00a9\u00a1 0&% 70! $(%\u00a5\u00a4\u00a3\u00a6 3# j! a (\u00a9(6 (U\u00a1 09\u00a1 0&\u00a9 g AjS\u00e4! 1\u00a5 h \u00a7 \u00a9(%(sp\u00a2 2y\u00a1 0&\u00a9\u00a4 \u00a7 H4\u00a9@ o# j 7m $2#\u00a9\u00a6 S8\" vw (! $8\u00a1 0\u00a3\u00a6 FH\u00a3 D\u00a1 0\u00a3\u00a5! $ \u00a7 \u00a9(\u00a1 0&% 70! a (%\u00a5\" 9# j 8$@ B\u00a5 o! P\u00a5'\u00a3 I $ \u00a7 \u00a9\u00a3 DY8! $ \u00a7 S\u00a1 x# j1\u00a1'\u00a1 0 xV \u00a7 \u00a9 8\" vjbw g\u00a4\u00a3\u00a6 8$ \u00a7 \u00a9\u00a5'\u00a3(% 73!@ B170 u\u00a5'$ A\u00a9 &%\u00a3\u00a5\u00a1 0\u00a3\u00a6 8! G\u00a1 0 ()\u00a3\u00a6@ BA%@ B \u00a7 % V\u00a1\"! 1\u00a1 0\u00a3\u00a6 \u00e4 \u00a7 \u00a3\u00a6 \u00a7 \u00a4 &%\u00a3\u00a6 8x&9\u00a1 0&\u00a9 m 8$ \u00a7 H\u00a1 0\u00a3\u00a6 \u00a7 H4! G\u00a1 0\u00a3\u00a6 \u00e4 \u00a7 $2\u00a9! g#% S8\" vw (! 18\u00a1 0\u00a3\u00a6 F6\u00a3 D\u00a1\u00a2 C9\u00a3\u00a6\u00a5\u00a5'70\u00a3! $\u00a6\u00a3\u00a6 r (x\u00a1 0&\u00a9 70# HCo \u00a7 \u00a9! $#\u00a9\u00a6\u00a3\u00a6 \u00a7 \u00a9 I9\u00a3 D\u00a1 0\u00a5 gw! Fa! q\u00a1 0&670! a (U\u00a1 0p (% 9\u00a5'\u00e4@ B xV\u00a1 0&\u00a9\u00a3\u00a6 \u00a7 \u00a9 Io\u00a6\u00a5'\u00a1 0\u00a3\u00a6\u00a6\u00a1 0&\u00a9! $8 x\u00a1 0\u00a3 FH\u00a3 D\u00a1 rCX# j 8 \u00e4@ B\u00a5\u00a4 4\u00a9 \u00a7 H#% S8\" vw (s \u00f3 z \u00f5% G a\u00f6H a\u00c0a\u00e16 ww \u00f6\u00a9 G S\u00c4S\u00f7 1 H $ H x\u00a3\u00a6\u00a5 g 4\u00a9\u00a5'(o\u00a1 09 &% \u00e4 (x sot u9! $8\u00a1 0\u00a3\u00a6 FH\u00a3 D\u00a1 0\u00a3\u00a6\u00a5 70! a (HC 2Q $7 X y6 8 46\u00a1 0\u00a3\u00a6 \u00e4 \u00a7 \u00bf# SCE\u00a1 0&\u00a9 \u00f4\u00cb \u00cc\u00ee\u00d1w SS $ \u00c0a\u00efH ww 1\u00f0S\u00e2w HGSH 1\u00a9 s\u00a1 \u00edy \u00dfq Aj\u00e4\u00a6\u00a3\u00a6 8 CX\u00a3\u00a6\u00a5 3 4\u00a9\u00a5'(\u00e4 \u00a7 X\u00a1 0&\u00a9 p\u00a0\u2026", "num_citations": "25\n", "authors": ["254"]}
{"title": "Enabling sparse constant propagation of array elements via array SSA form\n", "abstract": " We present a new static analysis technique based on Array SSA form [6]. Compared to traditional SSA form, the key enhancement in Array SSA form is that it deals with arrays at the element level instead of as monolithic objects. In addition, Array SSA form improves the \u03c6 function used for merging scalar or array variables in traditional SSA form. The computation of a \u03c6 function in traditional SSA form depends on the program\u2019s control flow in addition to the arguments of the \u03c6 function. Our improved \u03c6 function (referred to as a \u03c6 function) includes the relevant control flow information explicitly as arguments through auxiliary variables that are called @ variables.               The @ variables and \u03c6 functions were originally introduced as run-time computations in Array SSA form. In this paper, we use the element-level \u03c6 functions in Array SSA form for enhanced static analysis. We use Array SSA form to extend past\u00a0\u2026", "num_citations": "25\n", "authors": ["254"]}
{"title": "T2S-Tensor: Productively generating high-performance spatial hardware for dense tensor computations\n", "abstract": " We present a language and compilation framework for productively generating high-performance systolic arrays for dense tensor kernels on spatial architectures, including FPGAs and CGRAs. It decouples a functional specification from a spatial mapping, allowing programmers to quickly explore various spatial optimizations for the same function. The actual implementation of these optimizations is left to a compiler. Thus, productivity and performance are achieved at the same time. We used this framework to implement several important dense tensor kernels. We implemented dense matrix multiply for an Arria-10 FPGA and a research CGRA, achieving 88% and 92% of the performance of manually written, and highly optimized expert (ninja\") implementations in just 3% of their engineering time. Three other tensor kernels, including MTTKRP, TTM and TTMc, were also implemented with high performance and low\u00a0\u2026", "num_citations": "24\n", "authors": ["254"]}
{"title": "Folding of tagged single assignment values for memory-efficient parallelism\n", "abstract": " The dynamic-single-assignment property for shared data accesses can establish data race freedom and determinism in parallel programs. However, memory management is a well known challenge in making dynamic-single-assignment practical, especially when objects can be accessed through tags that can be computed by any step.               In this paper, we propose a new memory management approach based on user-specified folding functions that map logical dynamic-single -assignment (DSA) tags into dynamic-multiple-assignment (DMA) tags. We also compare folding with get-counts, an approach in which the user specifies a reference count for each single-assignment value. The context for our work is parallel programming models in which shared data accesses are coordinated by put/get operations on tagged DSA data structures. These models include dataflow programs with I-structures\u00a0\u2026", "num_citations": "24\n", "authors": ["254"]}
{"title": "Comparing the usability of library vs. language approaches to task parallelism\n", "abstract": " In this paper, we compare the usability of a library approach with a language approach to task parallelism. There are many practical advantages and disadvantages to both approaches. A key advantage of a library-based approach is that it can be deployed without requiring any change in the tool chain, including compilers and IDEs. However, the use of library APIs to express all aspects of task parallelism can lead to code that is hard to understand and modify. A key advantage of a language-based approach is that the intent of the programmer is easier to express and understand, both by other programmers and by program analysis tools. However, a language-based approach usually requires the standardization of new constructs and (possibly) of new keywords. In this paper, we compare the java. util. concurrent (juc) library [14] from Java 7 and the Habanero-Java (HJ)[16] language, supported by our experiences\u00a0\u2026", "num_citations": "24\n", "authors": ["254"]}
{"title": "Hierarchical phasers for scalable synchronization and reductions in dynamic parallelism\n", "abstract": " The phaser construct is a unification of collective and point-to-point synchronization with dynamic parallelism. This construct gives each task the option of synchronizing on a phaser in signal-only/wait-only mode for producer/consumer synchronization or signal-wait mode for barrier synchronization. A phaser accumulator is a reduction construct that works with phasers in a phased setting. Phasers and accumulators support dynamic parallelism i.e., they allow dynamic addition and removal of tasks from the synchronizations and reductions that they support. Past implementations of phasers and phaser accumulators have used a single master task to advance a phaser to the next phase and to perform computations for lazy reductions, while also supporting dynamic parallelism. Though the single master approach provides an effective solution for modest levels of parallelism, it quickly becomes a scalability bottleneck\u00a0\u2026", "num_citations": "24\n", "authors": ["254"]}
{"title": "Mapping iterative task graphs on distributed memory machines\n", "abstract": " This paper addresses the problem of scheduling iterative task graphs on distributed memory architectures with nonzero communication overhead. The proposed algorithm incorporates techniques of software pipelining, graph unfolding and directed acyclic graph scheduling. The goal of optimization is to minimize overall parallel time, which is achieved by balancing processor loads, exploring task parallelism within and across iterations, overlapping communication and computation, and eliminating unnecessary communication. This paper gives a method to execute static schedules, studies the sensitivity of run-time performance when weights are not estimated accurately at compile-time, and presents experimental results to demonstrate the effectiveness of this approach. 1 Introduction Many scientific applications can be viewed as the repeated execution of a set of computational tasks and can be modeled by iterative task graphs (ITGs). Mapping weighted iterative task graphs on messagepassing archi...", "num_citations": "24\n", "authors": ["254"]}
{"title": "Instruction reordering for fork-join parallelism\n", "abstract": " Any execution of a parallel program must satisfy the program\u2019s control dependence3 [FOW87] and data d\u2019rpendences [KKP* 81]. Broadly speaking, the mechanisms available for satisfying dependences fall into two classes-control seqzLencing and data synchronization. In control sequencing, dependences are satisfied by using parallel or sequential control structures like fork-join, doall, cobegincoend, and even sequential begin-end. In data synchwnization, dependences are satisfied by using synchrcnization objects like binary semaphores, counting seamphores and I-structures. It is desirable for the compiler to automatically generate parallel code that correctly and efficiently satisfies all the proigram\u2019s dependences, using whatever control sequencing and data synchronization mechanisms are available for the target architecture. In this paper, we investigate the problem of generatingPermission to copy without fee\u00a0\u2026", "num_citations": "24\n", "authors": ["254"]}
{"title": "Race detection in two dimensions\n", "abstract": " Dynamic race detection is a program analysis technique for detecting errors caused by undesired interleavings of concurrent tasks. A primary challenge when designing efficient race detection algorithms is to achieve manageable space requirements. State-of-the-art algorithms for unstructured parallelism require \u0398 (n) space per monitored memory location, where n is the total number of tasks. This is a serious drawback when analyzing programs with many tasks. In contrast, algorithms for programs with a series-parallel (SP) structure require only \u0398 (1) space. Unfortunately, it is currently not well understood if there are classes of parallelism beyond SP that can also benefit from and be analyzed with \u0398 (1) space complexity. In this work, we show that structures richer than SP graphs, namely, that of two-dimensional (2D) lattices, can also be analyzed in \u0398 (1) space. Toward that (a) we extend Tarjan\u2019s algorithm for\u00a0\u2026", "num_citations": "23\n", "authors": ["254"]}
{"title": "Dynamic determinacy race detection for task parallelism with futures\n", "abstract": " Existing dynamic determinacy race detectors for task-parallel programs are limited to programs with strict computation graphs, where a task can only wait for its descendant tasks to complete. In this paper, we present the first known determinacy race detector for non-strict computation graphs, constructed using futures. The space and time complexity of our algorithm are similar to those of the classical SP-bags algorithm, when using only structured parallel constructs such as spawn-sync and async-finish. In the presence of point-to-point synchronization using futures, the complexity of the algorithm increases by a factor determined by the number of future task creation and get operations as well as the number of non-tree edges in the computation graph. The experimental results show that the slowdown factor observed for our algorithm relative to the sequential version is in the range of 1.00 \u2013 9.92, which\u00a0\u2026", "num_citations": "23\n", "authors": ["254"]}
{"title": "Polyhedral optimizations for a data-flow graph language\n", "abstract": " This paper proposes a novel optimization framework for the Data-Flow Graph Language (DFGL), a dependence-based notation for macro-dataflow model which can be used as an embedded domain-specific language. Our optimization framework follows a \u201cdependence-first\u201d approach in capturing the semantics of DFGL programs in polyhedral representations, as opposed to the standard polyhedral approach of deriving dependences from access functions and schedules. As a first step, our proposed framework performs two important legality checks on an input DFGL program \u2014 checking for potential violations of the single-assignment rule, and checking for potential deadlocks. After these legality checks are performed, the DFGL dependence information is used in lieu of standard polyhedral dependences to enable polyhedral transformations and code generation, which include automatic loop\u00a0\u2026", "num_citations": "23\n", "authors": ["254"]}
{"title": "Method of, system for, and computer program product for efficient identification of private variables in program loops by an optimizing compiler\n", "abstract": " Privatization or identification of private variables in single-entry strongly connected regions or program loops by the use of dummy identity assignment statements. Dummy identity assignment statements, V= V, are inserted in the header block and postexit blocks of each single-entry strongly connected region for each variable V with a definition in the single-entry strongly connected region (SCR). USE functions of the dummy identity assignment statements are determined. The dummy definition of a variable in an SCR header block is used to test if there exists a loop-carried flow dependence on the variable in the SCR. Dummy definitions of the variable in postexit blocks of the SCR are used to test if the variable would require to be copied out on exit from the SCR. The dummy definition of the variable in header block of the SCR is used to test if only the last iteration value of the variable needs to be copied out. A\u00a0\u2026", "num_citations": "23\n", "authors": ["254"]}
{"title": "Swat: A programmable, in-memory, distributed, high-performance computing platform\n", "abstract": " The field of data analytics is currently going through a renaissance as a result of ever-increasing dataset sizes, the value of the models that can be trained from those datasets, and a surge in flexible, distributed programming models. In particular, the Apache Hadoop and Spark programming systems, as well as their supporting projects (eg HDFS, SparkSQL), have greatly simplified the analysis and transformation of datasets whose size exceeds the capacity of a single machine. While these programming models facilitate the use of distributed systems to analyze large datasets, they have been plagued by performance issues. The I/O performance bottlenecks of Hadoop are partially responsible for the creation of Spark. Performance bottlenecks in Spark due to the JVM object model, garbage collection, interpreted/managed execution, and other abstraction layers are responsible for the creation of additional\u00a0\u2026", "num_citations": "22\n", "authors": ["254"]}
{"title": "A practical approach to DOACROSS parallelization\n", "abstract": " Loops with cross-iteration dependences (doacross loops) often contain significant amounts of parallelism that can potentially be exploited on modern manycore processors. However, most production-strength compilers focus their automatic parallelization efforts on doall loops, and consider doacross parallelism to be impractical due to the space inefficiencies and the synchronization overheads of past approaches. This paper presents a novel and practical approach to automatically parallelizing doacross loops for execution on manycore-SMP systems. We introduce a compiler-and-runtime optimization called dependence folding that bounds the number of synchronization variables allocated per worker thread (processor core) to be at most the maximum depth of a loop nest being considered for automatic parallelization. Our approach has been implemented in a development version of the IBM XL Fortran\u00a0\u2026", "num_citations": "22\n", "authors": ["254"]}
{"title": "Maestro: A data-centric approach to understand reuse, performance, and hardware cost of dnn mappings\n", "abstract": " The efficiency of an accelerator depends on three factors-mapping, deep neural network (DNN) layers, and hardware-constructing extremely complicated design space of DNN accelerators. To demystify such complicated design space and guide the DNN accelerator design for better efficiency, we propose an analytical cost model, MAESTRO. MAESTRO receives DNN model description and hardware resources information as a list, and mapping described in a data-centric representation we propose as inputs. The data-centric representation consists of three directives that enable concise description of mappings in a compiler-friendly form. MAESTRO analyzes various forms of data reuse in an accelerator based on inputs quickly and generates more than 20 statistics including total latency, energy, throughput, etc., as outputs. MAESTRO's fast analysis enables various optimization tools for DNN accelerators such as\u00a0\u2026", "num_citations": "21\n", "authors": ["254"]}
{"title": "The design and implementation of the habanero-java parallel programming language\n", "abstract": " The Habanero-Java language extends sequential Java with a simple but powerful set of constructs for multicore parallelism. Its implementation includes a compiler that generates standard Java classfiles, a runtime system that builds on the java. util. concurrent library, an IDE (DrHJ) that extends DrJava, and a new data-race detection tool.", "num_citations": "21\n", "authors": ["254"]}
{"title": "Data merging for shared-memory multiprocessors\n", "abstract": " An efficient software cache consistency mechanism for shared-memory multiprocessors that supports multiple writers and works for cache lines of any size is described. The mechanism relies on the fact that for a correct program only the global memory needs a consistent view of the shared data between synchronization points. The delayed consistency mechanism allows arbitrary use of data blocks between synchronizations. In contrast to other mechanisms, the mechanism needs no modification to the processor hardware or any assistance from the programmer or compiler. The processors can use normal cache management policies. Since no special action is needed to use the shared data, the processors are free to act almost as if they are all running out of a single cache. The global memory units are nearly identical to those on currently available machines. Only a small amount of hardware and/or software is\u00a0\u2026", "num_citations": "21\n", "authors": ["254"]}
{"title": "Hadoopcl2: Motivating the design of a distributed, heterogeneous programming system with machine-learning applications\n", "abstract": " Machine learning (ML) algorithms have garnered increased interest as they demonstrate improved ability to extract meaningful trends from large, diverse, and noisy data sets. While research is advancing the state-of-the-art in ML algorithms, it is difficult to drastically improve the real-world performance of these algorithms. Porting new and existing algorithms from single-node systems to multi-node clusters, or from architecturally homogeneous systems to heterogeneous systems, is a promising optimization technique. However, performing optimized ports is challenging for domain experts who may lack experience in distributed and heterogeneous software development. This work explores how challenges in ML application development on heterogeneous, distributed systems shaped the development of the HadoopCL2 (HCL2) programming system. ML applications guide this work because they exhibit features that\u00a0\u2026", "num_citations": "20\n", "authors": ["254"]}
{"title": "Bounded memory scheduling of dynamic task graphs\n", "abstract": " It is now widely recognized that increased levels of parallelism is a necessary condition for improved application performance on multicore computers. However, as the number of cores increases, the memory-per-core ratio is expected to further decrease, making per-core memory efficiency of parallel programs an even more important concern in future systems. For many parallel applications, the memory requirements can be significantly larger than for their sequential counterparts and, more importantly, their memory utilization depends critically on the schedule used when running them.", "num_citations": "20\n", "authors": ["254"]}
{"title": "Method and system for generating compact code for the loop unrolling transformation\n", "abstract": " A loop unrolling trasformation specified by loop unrolling factors UF [1],..., UF [k] is performed on a perfect nest of k multiple loops to produce an unrolled loop representation as follows. Moving from the outermost loop to the innermost loop of the nest, the unroll factor UF [j] of the current loop is examined. First, the separate unrolled loop body is expanded by the specified unroll factor UF [j]. Second, the loop header for the current loop is adjusted so that if the loop's iteration count, COUNT [j], is known to be less than or equal to the unroll factor, UP [j], then the loop header is simply an assignment of the index variable to the lower-bound expression; otherwise, the loop header is adjusted so that the unrolled loop's iteration count equals. left brkt-bot. COUNT [J]/UF [J]. right brkt-bot. a rounded down truncation of the division. Third, a remainder loop nest is generated, if needed. The size of the generated code when unrolling\u00a0\u2026", "num_citations": "20\n", "authors": ["254"]}
{"title": "Location consistency: Stepping beyond the memory coherence barrier\n", "abstract": " In this paper, we introduce a new memory consistency model called Location Consistency (LC). The LC model uses a novel approach for de ning memory consistency. The state of a memory location is modeled as a partially ordered multiset (pomset) of write operations and synchronization operations. The partial orders are determined solely by the ordering constraints imposed by the program being executed. We illustrate how the LC model can enable more compiler and hardware performance optimizations to be applied, compared to other memory consistency models which rely on the memory coherence assumption.", "num_citations": "20\n", "authors": ["254"]}
{"title": "A simple and efficient implmentation approach for single assignment languages\n", "abstract": " Functional and single assignment languages have semantically pure features that do not permit side effects. This lack of side effects makes detection of parallelism in programs much easier. However, the same property poses a challenge in implementing these languages efficiently. A preliminary implementation of a compiler for the single assignment language, SISAL, is described. The compiler uses reference counts for memory management and copy avoidance. Performance results on a wide range of benchmark programs show that SISAL programs compiled by our implementation run 2-3 times slower on average than the same programs written in C, Pascal, and Fortran, and orders of magnitude faster than other implementations of single assignment languages. Extensions of these techniques for multiprocessor implementations are proposed.", "num_citations": "20\n", "authors": ["254"]}
{"title": "Reactive power constrained OPF scheduling with 2-D locational marginal pricing\n", "abstract": " The objective of this paper is to suggest a robust optimal power flow (OPF) framework to perform locational marginal pricing under the scarcity of reactive power. The classical OPF models employed for market clearing either assume infinite reactive power support or make a fully independent representation of the reactive power load. In contrast, a potentially more accurate power flow model is adopted in this paper recognizing the dependence of the level of reactive power consumption on the level of active power consumption. The relationship is primarily modeled by employing the concept of power factor. In addition, there can be load requests with different power factors from the same location. The corresponding locational marginal prices (LMPs) are found to vary not only spatially but also according to power factors. Thus, a two-dimensional LMP variation is finally obtained. A consistent definition of financial\u00a0\u2026", "num_citations": "19\n", "authors": ["254"]}
{"title": "A study of a software cache implementation of the openmp memory model for multicore and manycore architectures\n", "abstract": " This paper is motivated by the desire to provide an efficient and scalable software cache implementation of OpenMP on multicore and manycore architectures in general, and on the IBM CELL architecture in particular. In this paper, we propose an instantiation of the OpenMP memory model with the following advantages: (1) The proposed instantiation prohibits undefined values that may cause problems of safety, security, programming and debugging. (2) The proposed instantiation is scalable with respect to the number of threads because it does not rely on communication among threads or a centralized directory that maintains consistency of multiple copies of each shared variable. (3) The proposed instantiation avoids the ambiguity of the original memory model definition proposed on the OpenMP Specification 3.0.               We also introduce a new cache protocol for this instantiation, which can be\u00a0\u2026", "num_citations": "19\n", "authors": ["254"]}
{"title": "Optimized lock assignment and allocation: A method for exploiting concurrency among critical sections\n", "abstract": " One of the major performance and productivity issues in parallel programming arises from the use of lock/unlock operations or critical sections to enforce mutual exclusion. When programmers manage multiple fine-grained locks explicitly, they run the risk of introducing data races or creating deadlocks. When they use coarsegrained locks or critical sections, they run the risk of losing scalability in parallel performance. Ideally, we would like to give the programmers the best of both worlds\u2013the convenience of coarsegrained locks or critical sections combined with the scalability of fined-grained locks. We propose to achieve this ideal by (1) letting programmers focus on the correctness of the application by using coarse-grained unnamed critical sections for mutual exclusion, and (2) letting the compiler maximize the concurrency among critical sections by selecting an assignment of compiler-managed fine-grained locks\u00a0\u2026", "num_citations": "19\n", "authors": ["254"]}
{"title": "PIPES: a language and compiler for task-based programming on distributed-memory clusters\n", "abstract": " Applications running on clusters of shared-memory computers are often implemented using OpenMP+MPI. Productivity can be vastly improved using task-based programming, a paradigm where the user expresses the data and control-flow relations between tasks, offering the runtime maximal freedom to place and schedule tasks. While productivity is increased, high-performance execution remains challenging: the implementation of parallel algorithms typically requires specific task placement and communication strategies to reduce internode communications and exploit data locality. In this work, we present a new macro-dataflow programming environment for distributed-memory clusters, based on the Intel Concurrent Collections (CnC) runtime. Our language extensions let the user define virtual topologies, task mappings, task-centric data placement, task and communication scheduling, etc. We introduce a\u00a0\u2026", "num_citations": "18\n", "authors": ["254"]}
{"title": "A survey of sparse matrix-vector multiplication performance on large matrices\n", "abstract": " We contribute a third-party survey of sparse matrix-vector (SpMV) product performance on industrial-strength, large matrices using: (1) The SpMV implementations in Intel MKL, the Trilinos project (Tpetra subpackage), the CUSPARSE library, and the CUSP library, each running on modern architectures. (2) NVIDIA GPUs and Intel multi-core CPUs (supported by each software package). (3) The CSR, BSR, COO, HYB, and ELL matrix formats (supported by each software package).", "num_citations": "18\n", "authors": ["254"]}
{"title": "Application development productivity challenges for high-end computing\n", "abstract": " Application development productivity can be a significant bottleneck in the time to solution for deploying production applications on High-End Computing (HEC) systems. We discuss the fundamental reasons for these productivity barriers, and outline the solution being pursued for application development productivity in the IBM PERCS project (Productive Easy-touse Reliable Computing Systems). We also introduce a simple model for defining application development productivity, and use this model to guide our choice of solutions.", "num_citations": "18\n", "authors": ["254"]}
{"title": "Selectors: Actors with multiple guarded mailboxes\n", "abstract": " The actor programming model is based on asynchronous message passing and offers a promising approach for developing reliable concurrent systems. However, lack of guarantees to control the order in which messages are processed next by an actor makes implementing synchronization and coordination patterns difficult. In this work, we address this issue by introducing our extension to the actor model called selectors. Selectors have multiple mailboxes and each mailbox is guarded ie it can be enabled or disabled to affect the order in which messages are processed. The view of having guarded mailboxes is inspired by condition variables where a thread checks whether a condition is true before continuing its execution.", "num_citations": "17\n", "authors": ["254"]}
{"title": "Test-driven repair of data races in structured parallel programs\n", "abstract": " A common workflow for developing parallel software is as follows: 1) start with a sequential program, 2) identify subcomputations that should be converted to parallel tasks, 3) insert synchronization to achieve the same semantics as the sequential program, and repeat steps 2) and 3) as needed to improve performance. Though this is not the only approach to developing parallel software, it is sufficiently common to warrant special attention as parallel programming becomes ubiquitous. This paper focuses on automating step 3), which is usually the hardest step for developers who lack expertise in parallel programming. Past solutions to the problem of repairing parallel programs have used static-only or dynamic-only approaches, both of which incur significant limitations in practice. Static approaches can guarantee soundness in many cases but are limited in precision when analyzing medium or large-scale software\u00a0\u2026", "num_citations": "17\n", "authors": ["254"]}
{"title": "Expressing doacross loop dependences in OpenMP\n", "abstract": " OpenMP is a widely used programming standard for a broad range of parallel systems. In the OpenMP programming model, synchronization points are specified by implicit or explicit barrier operations within a parallel region. However, certain classes of computations, such as stencil algorithms, can be supported with better synchronization efficiency and data locality when using doacross parallelism with point-to-point synchronization than wavefront parallelism with barrier synchronization. In this paper, we propose new synchronization constructs to enable doacross parallelism in the context of the OpenMP programming model. Experimental results on a 32-core IBM Power7 system using four benchmark programs show performance improvements of the proposed doacross approach over OpenMP barriers by factors of 1.4\u00d7 to 5.2\u00d7 when using all 32 cores.", "num_citations": "17\n", "authors": ["254"]}
{"title": "Efficient data race detection for async-finish parallelism\n", "abstract": " A major productivity hurdle for parallel programming is the presence of data races. Data races can lead to all kinds of harmful program behaviors, including determinism violations and corrupted memory. However, runtime overheads of current dynamic data race detectors are still prohibitively large (often incurring slowdowns of 10\u00d7 or more) for use in mainstream software development.               In this paper, we present an efficient dynamic race detection algorithm that handles both the async-finish task-parallel programming model used in languages such as X10 and Habanero Java (HJ) and the spawn-sync constructs used in Cilk.               We have implemented our algorithm in a tool called TaskChecker and evaluated it on a suite of 12 benchmarks. To reduce overhead of the dynamic analysis, we have also implemented various static optimizations in the tool. Our experimental results indicate that our\u00a0\u2026", "num_citations": "17\n", "authors": ["254"]}
{"title": "Permission regions for race-free parallelism\n", "abstract": " It is difficult to write parallel programs that are correct. This is because of the potential for data races, when parallel tasks access shared data in complex and unexpected ways. A classic approach to addressing this problem is dynamic race detection, which has the benefits of working transparently to the programmer and not raising any false alarms. Unfortunately, dynamic race detection is very slow in practice; further, it can only detect low-level races, not high-level races which are also known as atomicity violations. In this paper, we present a new approach to dynamic detection of data races and atomicity violations based on the concept of permission regions, which are regions of code that have permission to read or write certain variables. Dynamic checks are used to ensure that no conflicting permission regions execute in parallel, thereby allowing the granularity of checks to be adjusted according to the\u00a0\u2026", "num_citations": "16\n", "authors": ["254"]}
{"title": "Register-sensitive selection, duplication, and sequencing of instructions\n", "abstract": " In this paper, we present a new framework for selecting, duplicating and sequencing instructions so as to decrease register pressure. The motivation for this work is to target current and future high-performance processors where reductions in register pressure in the compiled programs can lead to improved performance.", "num_citations": "16\n", "authors": ["254"]}
{"title": "Optimized two-level parallelization for GPU accelerators using the polyhedral model\n", "abstract": " While GPUs play an increasingly important role in today's high-performance computers, optimizing GPU performance continues to impose large burdens upon programmers. A major challenge in optimizing codes for GPUs stems from the two levels of hardware parallelism, blocks and threads; each of these levels has significantly different characteristics, requiring different optimization strategies.", "num_citations": "15\n", "authors": ["254"]}
{"title": "Load balancing prioritized tasks via work-stealing\n", "abstract": " Work-stealing schedulers focus on minimizing overhead in task scheduling. Consequently, they avoid features, such as task priorities, which can add overhead to the implementation. Thus in such schedulers, low priority tasks may be scheduled earlier, delaying the execution of higher priority tasks and possibly increasing overall execution time.                 In this paper, we develop a decentralized work-stealing scheduler that dynamically schedules fixed-priority tasks in a non-preemptive manner. We adhere, as closely as possible, to the priority order while scheduling tasks by accepting some overhead to preserve order. Our approach uses non-blocking operations, is workload independent, and we achieve performance even in the presence of fine-grained tasks. Experimental results show that the Java implementation of our scheduler performs favorably compared to other schedulers (priority and non\u00a0\u2026", "num_citations": "15\n", "authors": ["254"]}
{"title": "Dfgr an intermediate graph representation for macro-dataflow programs\n", "abstract": " In this paper we propose a new intermediate graph representation for macro-dataflow programs, DFGR, which is capable of offering a high-level view of applications for easy programmability, while allowing the expression of complex applications using dataflow principles. DFGR makes it possible to write applications in a manner that is oblivious of the underlying parallel runtime, and can easily be targeted by both programming systems and domain experts. In addition, DFGR can use further optimizations in the form of graph transformations, enabling the coupling of static and dynamic scheduling and efficient task composition and assignment, for improved scalability and locality. We show preliminary performance results for an implementation of DFGR on a shared memory runtim system, offering speedups of up to 11\u00d7 on 12 cores, for complex graphs.", "num_citations": "15\n", "authors": ["254"]}
{"title": "Determinacy and repeatability of parallel program schemata\n", "abstract": " The concept of \"determinism\" of parallel programs and parallel systems has received a lot of attention since the dawn of computing, with multiple proposals for formal and informal definitions of deterministic execution. In this paper, we present precise definitions of two related properties of program schemata - determinacy and repeatability. A key advantage of providing definitions for schemata rather than concrete programs is that it simplifies the task for programmers and tools to check these properties. The definitions of these properties are provided for schemata arising from data flow programs and task-parallel programs, thereby also establishing new relationships between the two models. Our hope is that these definitions will help provide a framework for enabling more precise definitions of determinism in future work.", "num_citations": "15\n", "authors": ["254"]}
{"title": "Unifying barrier and point-to-point synchronization in OpenMP with phasers\n", "abstract": " OpenMP is a widely used standard for parallel programing on a broad range of SMP systems. In the OpenMP programming model, synchronization points are specified by implicit or explicit barrier operations. However, certain classes of computations such as stencil algorithms need to specify synchronization only among particular tasks/threads so as to support pipeline parallelism with better synchronization efficiency and data locality than wavefront parallelism using all-to-all barriers. In this paper, we propose two new synchronization constructs in the OpenMP programming model, thread-level phasers and iteration level phasers to support various synchronization patterns such as point-to-point synchronizations and sub-group barriers with neighbor threads. Experimental results on three platforms using numerical applications show performance improvements of phasers over OpenMP barriers of up to 1.74\u00a0\u2026", "num_citations": "15\n", "authors": ["254"]}
{"title": "Locality analysis for distributed shared-memory multiprocessors\n", "abstract": " This paper studies the locality analysis problem for shared-memory multiprocessors, a class of parallel machines that has experienced steady and rapid growth in the past few years. The focus of this work is on estimation of the memory performance of a loop nest for a given set of computation and data distributions. We assume a distributed shared-memory multiprocessor model. We discuss how to estimate the total number of cache misses (compulsory misses, conflict misses, capacity misses), and also the fractions of these cache misses that result in local vs. remote memory accesses. The goal of our work is to use this performance estimation to guide automatic and semi-automatic selection of data distributions and loop transformations in programs written for future shared-memory multiprocessors. This paper also includes simulation results as validation of our analysis method.", "num_citations": "15\n", "authors": ["254"]}
{"title": "Optimized distributed work-stealing\n", "abstract": " Work-stealing is a popular approach for dynamic load balancing of task-parallel programs. However, as has been widely studied, the use of classical work-stealing algorithms on massively parallel and distributed supercomputers introduces several performance issues. One such issue is the overhead of failed steals (communicating with a victim that has no work), which is far more severe in the distributed context than within a single SMP node. Due to the cost of inter-node communication, it is critical to reduce the number of failed steals in a distributed context. Prior work has demonstrated that load-aware victim processor selection can reduce the number of failed steals, but it cannot eliminate the failed steals completely.In this paper, we present two different load-aware implementations of distributed work-stealing algorithm in HabaneroUPC++ PGAS library - BaselineWS and SuccessOnlyWS. BaselineWS follows\u00a0\u2026", "num_citations": "14\n", "authors": ["254"]}
{"title": "Declarative tuning for locality in parallel programs\n", "abstract": " Optimized placement of data and computation for locality is critical for improving performance and reducing energy consumption on modern computing systems. However, for most programming models, modifying data and computation placements typically requires rewriting large portions of the application, thereby posing a huge performance portability challenge in today's rapidly evolving architecture landscape. In this paper we present TunedCnC, a novel, declarative and flexible CnC tuning framework for controlling the spatial and temporal placement of data and computation by specifying hierarchical affinity groups and distribution functions. TunedCnC emphasizes a separation of concerns: the domain expert specifies a parallel application by defining data and control dependences, while the tuning expert specifies how the application should be executed on a given architecture - defining when and where for\u00a0\u2026", "num_citations": "14\n", "authors": ["254"]}
{"title": "Combining Register Allocation and Instruction Scheduling (Technical Summary)\n", "abstract": " Modern optimizing compilers contain several optimization phases, including register allocation and instruction scheduling which have received widespread attention in past academic and industrial research. It is now generally recognized that the separation between the register allocation and instruction scheduling phases leads to significant problems, such as poor optimization for cases that are ill-suited to the specific phase-ordering selected by the compiler, and additional conceptual and software-engineering complexities in trying to adjust one phase to take into account cost considerations from the other. The goal of our research is to address these problems by building on past work and combining register allocation and instruction scheduling into a single phase. Henceforth, we refer to our model of these problems as CRISP (for Combined Register allocation and Instruction Scheduling Problem). The main contributions of this paper are as follows:", "num_citations": "14\n", "authors": ["254"]}
{"title": "A pluggable framework for composable hpc scheduling libraries\n", "abstract": " Driven by the increasing diversity of current and future HPC hardware and software platforms, the HPC community has seen a dramatic increase in research and development efforts into the composability of discrete software systems. While modularity is often desirable from a software engineering, quality assurance, and maintainability perspective, the barriers between software components often hide optimization opportunities. Recent examples of work in composable HPC software include GPU-Aware MPI, OpenMP's target directive, Lithe, HCMPI, and MVAPICH's unified communication runtime. These projects all deal with breaking down the walls between software or hardware components in order to achieve performance, programmability, and/or portability gains. However, they also generally focus on composing only specific types of HPC software and have limited extensability. In this paper, we present work on\u00a0\u2026", "num_citations": "13\n", "authors": ["254"]}
{"title": "Integrating asynchronous task parallelism with openshmem\n", "abstract": " Partitioned Global Address Space (PGAS) programming models combine shared and distributed memory features, and provide a foundation for high-productivity parallel programming using lightweight one-sided communications. The OpenSHMEM programming interface has recently begun gaining popularity as a lightweight library-based approach for developing PGAS applications, in part through its use of a symmetric heap to realize more efficient implementations of global pointers than in other PGAS systems. However, current approaches to hybrid inter-node and intra-node parallel programming in OpenSHMEM rely on the use of multithreaded programming models (e.g., pthreads, OpenMP) that harness intra-node parallelism but are opaque to the OpenSHMEM runtime. This OpenSHMEM+X approach can encounter performance challenges such as bottlenecks on shared resources, long pause\u00a0\u2026", "num_citations": "13\n", "authors": ["254"]}
{"title": "LLVM-based communication optimizations for PGAS programs\n", "abstract": " While Partitioned Global Address Space (PGAS) programming languages such as UPC/UPC++, CAF, Chapel and X10 provide high-level programming models for facilitating large-scale distributed-memory parallel programming, it is widely recognized that compiler analysis and optimization for these languages has been very limited, unlike the optimization of SMP models such as OpenMP. One reason for this limitation is that current optimizers for PGAS programs are specialized to different languages. This is unfortunate since communication optimization is an important class of compiler optimizations for PGAS programs running on distributed-memory platforms, and these optimizations need to be performed more widely. Thus, a more effective approach would be to build a language-independent and runtime-independent compiler framework for optimizing PGAS programs so that new communication optimizations\u00a0\u2026", "num_citations": "13\n", "authors": ["254"]}
{"title": "Inter-iteration scalar replacement using array ssa form\n", "abstract": " In this paper, we introduce novel simple and efficient analysis algorithms for scalar replacement and dead store elimination that are built on Array SSA form, a uniform representation for capturing control and data flow properties at the level of array or pointer accesses. We present extensions to the original Array SSA form representation to capture loop-carried data flow information for arrays and pointers. A core contribution of our algorithm is a subscript analysis that propagates array indices across loop iterations. Compared to past work, this algorithm can handle control flow within and across loop iterations and degrade gracefully in the presence of unanalyzable subscripts. We also introduce code transformations that can use the output of our analysis algorithms to perform the necessary scalar replacement transformations (including the insertion of loop prologues and epilogues for loop-carried reuse). Our\u00a0\u2026", "num_citations": "13\n", "authors": ["254"]}
{"title": "On the importance of an end-to-end view of memory consistency in future computer systems\n", "abstract": " The main purpose of a memory consistency model is to serve as an agreement between hardware system designers and software developers on the semantics of memory operations so as to ensure correct execution of user programs. However, the bulk of past work on memory consistency models has been pursued from the hardware viewpoint. In this viewpoint, a memory consistency model is used to specify certain behavioral properties that are guaranteed by uniprocessor/multiprocessor hardware e.g., the memory coherence property. In this paper, we argue that it is essential to adopt an end-to-end view of memory consistency that can be understood at all levels of software and hardware. We believe that this is possible with a memory consistency model based on partial order execution semantics \u2014 such as the Location Consistency (LC) model \u2014 rather than on the memory coherence assumption.", "num_citations": "13\n", "authors": ["254"]}
{"title": "An automatically partitioning compiler for sisal\n", "abstract": " In this paper, we describe a compiler that automatically repackages SISAL programs to achieve the grain size needed to run efficiently on different multiprocessors. The compiler is based on an existing implementation for SISAL on the Sequent Balance multiprocessor, and on previous work on automatic partitioning of SISAL programs. The granularity of parallelism in the existing Sequent implementation is defined by language constructs (eg, all function calls are spawned as separate tasks), causing the programming style to dramatically effect multiprocessor performance. It is desirable for the partitioning to be performed automatically, so that the same program can be made to execute efficiently on different multiprocessors and the programmer need not be concerned with granularity issues when developing his program. This is the goal of our automatically partitioning compiler. The partitioner operates on IF1, a graphical intermediate language for SISAL programs. It also takes as input a list of parameters describing the target multiprocessor. The partitioner's output consists of IF1 annotations to specify which calls to user-defined functions should be executed as parallel tasks, which Forall expressions should be executed as parallel loops and their appropriate chunk sizes. the Sequent Balance implementation has been retargetted to the Alliant, multi-VAX and Cray more\u00bb", "num_citations": "13\n", "authors": ["254"]}
{"title": "Implementation and evaluation of OpenSHMEM contexts using OFI libfabric\n", "abstract": " HPC system and processor architectures are trending toward increasing numbers of cores and tall, narrow memory hierarchies. As a result, programmers have embraced hybrid parallel programming as a means of tuning for such architectures. While popular HPC communication middlewares, such as MPI, allow the use of threads, most fall short of fully-integrating threads with the communication model. The OpenSHMEM contexts proposal promises thread isolation and direct mapping of threads to network resources; however, fully realizing these potentials will be dependent upon support for efficient threaded communication through the underlying layers of the networking stack. In this paper, we explore the mapping of OpenSHMEM contexts to the new OpenFabrics Interfaces (OFI) libfabric communication layer and use the libfabric GNI provider to access the Aries interconnect. We describe the design of\u00a0\u2026", "num_citations": "12\n", "authors": ["254"]}
{"title": "Exploring compiler optimization opportunities for the OpenMP 4.\u00d7 accelerator model on a POWER8+ GPU platform\n", "abstract": " While GPUs are increasingly popular for high-performance computing, optimizing the performance of GPU programs is a time-consuming and non-trivial process in general. This complexity stems from the low abstraction level of standard GPU programming models such as CUDA and OpenCL: programmers are required to orchestrate low-level operations in order to exploit the full capability of GPUs. In terms of software productivity and portability, a more attractive approach would be to facilitate GPU programming by providing high-level abstractions for expressing parallel algorithms.OpenMP is a directive-based shared memory parallel programming model and has been widely used for many years. From OpenMP 4.0 onwards, GPU platforms are supported by extending OpenMP's high-level parallel abstractions with accelerator programming. This extension allows programmers to write GPU programs in standard C\u00a0\u2026", "num_citations": "12\n", "authors": ["254"]}
{"title": "Data layout optimization for portable performance\n", "abstract": " This paper describes a new approach to managing data layouts to optimize performance for array-intensive codes. Prior research has shown that changing data layouts (e.g., interleaving arrays) can improve performance. However, there have been two major reasons why such optimizations are not widely used in practice: (1) the challenge of selecting an optimized layout for a given computing platform, and (2) the cost of re-writing codes to use different layouts for different platforms. We describe a source-to-source code transformation process that enables the generation of different codes with different array interleavings from the same source program, controlled by data layout specifications that are defined separately from the program. Performance results for multicore versions of the benchmarks show significant benefits on four different computing platforms (up\u00a0to  for IRSmk, up\u00a0to  for SRAD and\u00a0\u2026", "num_citations": "12\n", "authors": ["254"]}
{"title": "The Eureka programming model for speculative task parallelism\n", "abstract": " In this paper, we describe the Eureka Programming Model (EuPM) that simplifies the expression of speculative parallel tasks, and is especially well suited for parallel search and optimization applications. The focus of this work is to provide a clean semantics for, and efficiently support, such\" eureka-style\" computations (EuSCs) in general structured task parallel programming models. In EuSCs, a eureka event is a point in a program that announces that a result has been found. A eureka triggered by a speculative task can cause a group of related speculative tasks to become redundant, and enable them to be terminated at well-defined program points. Our approach provides a bound on the additional work done in redundant speculative tasks after such a eureka event occurs. We identify various patterns that are supported by our eureka construct, which include search, optimization, convergence, and soft real-time deadlines. These different patterns of computations can also be safely combined or nested in the EuPM, along with regular task-parallel constructs, thereby enabling high degrees of composability and reusability. As demonstrated by our implementation, the EuPM can also be implemented efficiently. We use a cooperative runtime that uses delimited continuations to manage the termination of redundant tasks and their synchronization at join points. In contrast to current approaches, EuPM obviates the need for cumbersome manual refactoring by the programmer that may (for example) require the insertion of if checks and early return statements in every method in the call chain. Experimental results show that solutions using the EuPM\u00a0\u2026", "num_citations": "12\n", "authors": ["254"]}
{"title": "Interprocedural strength reduction of critical sections in explicitly-parallel programs\n", "abstract": " In this paper, we introduce novel compiler optimization techniques to reduce the number of operations performed in critical sections that occur in explicitly-parallel programs. Specifically, we focus on three code transformations: 1) Partial Strength Reduction (PSR) of critical sections to replace critical sections by non-critical sections on certain control flow paths; 2) Critical Load Elimination (CLE) to replace memory accesses within a critical section by accesses to scalar temporaries that contain values loaded outside the critical section; and 3) Non-critical Code Motion (NCM) to hoist thread-local computations out of critical sections. The effectiveness of the first two transformations is further increased by interprocedural analysis. The effectiveness of our techniques has been demonstrated for critical section constructs from three different explicitly-parallel programming models - the isolated construct in Habanero Java (HJ\u00a0\u2026", "num_citations": "12\n", "authors": ["254"]}
{"title": "User-specified and automatic data layout selection for portable performance\n", "abstract": " This paper describes a new approach to managing array data layouts to optimize performance for scientific codes. Prior research has shown that changing data layouts (eg, interleaving arrays) can improve performance. However, there have been two major reasons why such optimizations are not widely used:(1) the need to select different layouts for different computing platforms, and (2) the cost of re-writing codes to use to new layouts. We describe a source-to-source translation process that allows us to generate codes with different array interleavings, based on a data layout specification. We used this process to generate 19 different data layouts for an ASC benchmark code (IRSmk) and 32 different data layouts for the DARPA UHPC challenge application (LULESH). Performance results for multicore versions of the benchmarks with different layouts show significant benefits on four computing platforms (IBM POWER7, AMD APU, Intel Sandybridge, IBM BG/Q). For IRSmk, our results show performance improvements ranging from 22.23\u00d7 on IBM POWER7 to 1.10\u00d7 on Intel Sandybridge. For LULESH, we see improvements ranging from 1.82\u00d7 on IBM POWER7 to 1.02\u00d7 on Intel Sandybridge. We also developed a new optimization algorithm to recommend a layout for an input source program and specific target machine characteristics. Our results show that the performance of this automated layout algorithm outperforms the manual layouts in one case and performs within 10% of the best architecture-specific layout in all the other cases, but one.", "num_citations": "12\n", "authors": ["254"]}
{"title": "Improving demand response and bid-consistency of price outcome in the security-constrained dispatch scheduling\n", "abstract": " The primary objective of this paper is to design a security-constrained locational marginal pricing framework with specific emphasis on creating clear incentive for the demand side to make participation in serving the reserve option. Both the contingency forms, such as line outage and generator outage, are taken into account. The reserve services from the demand side are identified from the respective energy requests only, based upon a concept of load service security. Different prices are established for generating and load entities. Those are defined as generator and load locational marginal prices (LMPs), respectively. The load LMPs are further differentiated based upon the service security levels requested by different load entities. In the case of any system stress due to security constraints, the load prices go higher for a higher level of service security. Therefore, the necessary price signal is generated to control\u00a0\u2026", "num_citations": "12\n", "authors": ["254"]}
{"title": "Habanero-scala: Async-finish programming in scala\n", "abstract": " With the advent of the multicore era, it is clear that improvements in application performance will require parallelism. Programming models that utilize multiple cores offer promising directions for the future where core counts are expected to increase. There is, hence, a renewed interest in programming models that simplify the reasoning and writing of efficient parallel programs. In this paper, we present Habanero-Scala (HS) which implements a generic task parallel programming model that can be used to parallelize both regular and irregular applications. HS is a library extension of Scala and supports a hybrid programming model combining the previously developed Async/Finish Model (AFM) and Actor Model (AM). HS extends Scala\u2019s actor-based concurrency model with creation of lightweight tasks embodied in async, future, and foreach constructs; termination detection using the finish construct; locality in the form of places; weak isolation using isolated blocks; and task coordination patterns using phasers, data-driven futures. HS offers a simpler parallel programming model compared to writing programs using threads and adds to the tools available for the programmer to aid in productivity and performance while developing parallel software.", "num_citations": "12\n", "authors": ["254"]}
{"title": "Hardware and software tradeoffs for task synchronization on manycore architectures\n", "abstract": " Manycore architectures \u2013 hundreds to thousands of cores per processor \u2013 are seen by many as a natural evolution of multicore processors. To take advantage of this massive parallelism in practice requires a productive parallel programming model, and an efficient runtime for the scheduling and coordination of concurrent tasks. A critical prerequisite for an efficient runtime is a scalable synchronization mechanism to support task coordination at different levels of granularity.               This paper describes the implementation of a high-level synchronization construct called phasers on the IBM Cyclops64 manycore processor, and compares phasers to lower-level synchronization primitives currently available to Cyclops64 programmers. Phasers support synchronization of dynamic tasks by allowing tasks to register and deregister with a phaser object. It provides a general unification of point-to-point and collective\u00a0\u2026", "num_citations": "12\n", "authors": ["254"]}
{"title": "Automatic parallelization of pure method calls via conditional future synthesis\n", "abstract": " We introduce a novel approach for using futures to automatically parallelize the execution of pure method calls. Our approach is built on three new techniques to address the challenge of automatic parallelization via future synthesis: candidate future synthesis, parallelism benefit analysis, and threshold expression synthesis. During candidate future synthesis, our system annotates pure method calls as async expressions and synthesizes a parallel program with future objects and their type declarations. Next, the system performs a parallel benefit analysis to determine which async expressions may need to be executed sequentially due to overhead reasons, based on execution profile information collected from multiple test inputs. Finally, threshold expression synthesis uses the output from parallelism benefit analysis to synthesize predicate expressions that can be used to determine at runtime if a specific pure\u00a0\u2026", "num_citations": "11\n", "authors": ["254"]}
{"title": "Heterogeneous work-stealing across CPU and DSP cores\n", "abstract": " Due to the increasing power constraints and higher and higher performance demands, many vendors have shifted their focus from designing high-performance computer nodes using powerful multicore general-purpose CPUs, to nodes containing a smaller number of general-purpose CPUs aided by a larger number of more power-efficient special purpose processing units, such as GPUs, FPGAs or DSPs. While offering a lower power-to-performance ratio, unfortunately, such heterogeneous systems are notoriously hard to program, forcing the users to resort to lower-level direct programming of the special purpose processors and manually managing data transfer and synchronization between the parts of the program running on general-purpose CPUs and on special-purpose processors. In this paper, we present HC-K2H, a programming model and runtime system for the Texas Instruments Keystone II Hawking\u00a0\u2026", "num_citations": "11\n", "authors": ["254"]}
{"title": "Heterogeneous Habanero-C (H2C): a portable programming model for heterogeneous processors\n", "abstract": " Heterogeneous architectures with their diverse architectural features impose significant programmability challenges. Existing programming systems involve non-trivial learning and are not productive, not portable, and are challenging to tune for performance. In this paper, we introduce Heterogeneous Habanero-C (H2C), which is an implementation of the Habanero execution model for modern heterogeneous (CPU + GPU) architectures. The H2C language provides high-level constructs to specify the computation, communication, and synchronization in a given application. H2C also implements novel constructs for task partitioning and locality. The H2C (source-to-source) compiler and runtime framework efficiently map these high-level constructs onto the underlying heterogeneous platform, which can include multiple CPU cores and multiple GPU devices, possibly from different vendors. Experimental evaluations of\u00a0\u2026", "num_citations": "11\n", "authors": ["254"]}
{"title": "Compiler support for work-stealing parallel runtime systems\n", "abstract": " Multiple programming models are emerging to address an increased need for dynamic task parallelism in applications for multicore processors and shared-address-space parallel computing. Examples include OpenMP 3.0, Java Concurrency Utilities, Microsoft Task Parallel Library, Intel Threading Building Blocks, Cilk, X10, Chapel, and Fortress. Scheduling algorithms based on work-stealing, as embodied in Cilk's implementation of dynamic spawn-sync parallelism, are gaining in popularity but also have inherent limitations. In this thesis, we focus on the compiler support needed to extend work-stealing for dynamic async-finish task parallelism as in X10 and HabaneroJava (HJ). We also discuss the compiler support needed for work-stealing with both the work-first and help-first policies. Performance results obtained using our compiler and the HJ work-stealing runtime show significant improvement compared to\u00a0\u2026", "num_citations": "11\n", "authors": ["254"]}
{"title": "Unified polyhedral modeling of temporal and spatial locality\n", "abstract": " Despite decades of work in this area, the construction of effective loop nest optimizers and parallelizers continues to be challenging due to the increasing diversity of both loop-intensive application workloads and complex memory/computation hierarchies in modern processors.  The lack  of  a  systematic  approach  to  optimizing  locality  and  parallelism,  with  a  well-founded  data locality  model,  is  a  major  obstacle  to  the  design  of  optimizing  compilers  coping  with  the  variety of software and hardware.  Acknowledging the conflicting demands on loop nest optimization, we propose a new unified algorithm for optimizing parallelism and locality in loop nests, that is capable  of  modeling  temporal  and  spatial  effects  of  multiprocessors  and  accelerators  with  deep memory hierarchies and multiple levels of parallelism.  It orchestrates a collection of parameterizable optimization  problems  for  locality  and  parallelism  objectives  over  a  polyhedral  space  of semantics-preserving transformations.  The overall problem is not convex and is only constrained by semantics preservation.  We discuss the rationale for this unified algorithm, and validate it on a collection of representative computational kernels/benchmarks.", "num_citations": "10\n", "authors": ["254"]}
{"title": "Elastic tasks: Unifying task parallelism and SPMD parallelism with an adaptive runtime\n", "abstract": " In this paper, we introduce elastic tasks, a new high-level parallel programming primitive that can be used to unify task parallelism and SPMD parallelism in a common adaptive scheduling framework. Elastic tasks are internally parallel tasks and can run on a single worker or expand to take over multiple workers. An elastic task can be an ordinary task or an SPMD region that must be executed by one or more workers simultaneously, in a tightly coupled manner.                 This paper demonstrates the following benefits of elastic tasks: (1)\u00a0they offer theoretical guarantees: in a work-stealing environment computations complete in expected time , where \u00a0=\u00a0# of elastic tasks, W\u00a0=\u00a0work, S\u00a0=\u00a0span, P\u00a0=\u00a0# cores. (2) they offer performance benefits in practice by co-scheduling tightly coupled parallel/SPMD subcomputations within a single elastic task, and (3) they can adapt at runtime to the state of\u00a0\u2026", "num_citations": "10\n", "authors": ["254"]}
{"title": "Isolation for nested task parallelism\n", "abstract": " Isolation--the property that a task can access shared data without interference from other tasks--is one of the most basic concerns in parallel programming. Whilethere is a large body of past work on isolated task-parallelism, the integration of isolation, task-parallelism, and nesting of tasks has been a difficult and unresolved challenge. In this pa- per, we present a programming and execution model called Otello where isolation is extended to arbitrarily nested parallel tasks with irregular accesses to heap data. At the same time, no additional burden is imposed on the programmer, who only exposes parallelism by creating and synchronizing parallel tasks, leaving the job of ensuring isolation to the underlying compiler and runtime system. Otello extends our past work on Aida execution model and the delegated isolation mechanism [22] to the setting of nested parallelism. The basic runtime construct in Aida and Otello\u00a0\u2026", "num_citations": "10\n", "authors": ["254"]}
{"title": "Hj-hadoop: an optimized mapreduce runtime for multi-core systems\n", "abstract": " We introduces HabaneroJava-Hadoop, an extension to the HadoopMapReduce system that is optimized for multi-core machines. HJ-Hadoop exploits intra-JVM parallelism that increases memory efficiency of each node. Results show a significant improvement in the amount of data each MapReduce job could process and load balance across cores for certain applications.", "num_citations": "10\n", "authors": ["254"]}
{"title": "Speculative execution of parallel programs with precise exception semantics on gpus\n", "abstract": " General purpose computing on GPUs (GPGPU) can enable significant performance and energy improvements for certain classes of applications. However, current GPGPU programming models, such as CUDA and OpenCL, are only accessible by systems experts through low-level C/C++ APIs. In contrast, large numbers of programmers use high-level languages, such as Java, due to their productivity advantages of type safety, managed runtimes and precise exception semantics. Current approaches to enabling GPGPU computing in Java and other managed languages involve low-level interfaces to native code that compromise the semantic guarantees of managed languages, and are not readily accessible to mainstream programmers.                 In this paper, we propose compile-time and runtime technique for accelerating Java programs with automatic generation of OpenCL while preserving precise\u00a0\u2026", "num_citations": "10\n", "authors": ["254"]}
{"title": "The flexible preconditions model for macro-dataflow execution\n", "abstract": " In this paper, we propose the flexible preconditions model for macro-dataflow execution. Our approach unifies two current approaches for managing task dependences, eager execution vs. Strict preconditions. When one of the two outperforms the other, flexible preconditions can always attain, and possibly surpass, the performance of the better approach. This work focuses on the performance of parallel programming models based on macro-dataflow, in which applications are composed of tasks and inter-task dependences. Data-flow models usually make a choice between specifying the task dependences before task creation (as strict preconditions), or during task execution, when they are actually needed (eager execution). This paper shows how the choice between eager execution and strict preconditions affects the performance, memory consumption and expressiveness of macro-dataflow applications. The\u00a0\u2026", "num_citations": "10\n", "authors": ["254"]}
{"title": "An optimal asynchronous scheduling algorithm for software cache consistency\n", "abstract": " We present a linear time algorithm for scheduling iterations of a loop that has no loop-carried dependences. The algorithm is optimal in the sense that any p consecutive iterations in the schedule can be executed simultaneously without any possibility of false sharing, where p is the number of processors, and the algorithm uses at most two wait synchronizations per iteration. Our algorithm is asynchronous in the sense that it allows the loop iterations to be scheduled greedily so that no processor will ever be kept idle if it could be safely executing an iteration. This property makes our algorithm superior to a \"barrier\" type algorithm, in which barrier synchronizations are used instead of pairwise signal-wait synchronizations.<>", "num_citations": "10\n", "authors": ["254"]}
{"title": "Processor Scheduling Algorithms for Constraint-Satisfaction Search Problems\n", "abstract": " Constraint-satisfaction problems arise frequently in Artificial Intelligence and engineering design applications. These problems are computationally intensive and would significantly benefit from speedup through parallel processing. In this paper, we investigate parallelizations of the Forward-Checker algorithm, which is known to be an efficient sequential algorithm for constraint-satisfaction problems. We present two parallel algorithms--the Threshold Depth-First Priority (TDFP) and the Breadth-First List Scheduling (BFLS) algorithms. Simulation results show that both algorithms are suitable for solving constraint-satisfaction problems in parallel, and yield near-linear speedup even beyond 100 processors. The best choice of algorithm depends on the amount of imbalance in the search problem and the overhead in the target multiprocessor.", "num_citations": "10\n", "authors": ["254"]}
{"title": "Using polyhedral analysis to verify OpenMP applications are data race free\n", "abstract": " Among the most common and hardest to debug types of bugs in concurrent systems are data races. In this paper, we present an approach for verifying that an OpenMP program is data race free. We use polyhedral analysis to verify those parts of the program where we detect parallel affine loop nests. We show the applicability of polyhedral analysis with analysis-enabling program transformations for data race detection in HPC applications. We evaluate our approach with the dedicated data race benchmark suite DataRaceBench and the LLNL Proxy Application AMG2013 which consists of 75,000 LOC. Our evaluation shows that polyhedral analysis can classify 40% of the DataRaceBench 1.2.0 benchmarks as either data race free or having data races and verify that 41 of the 114 (36%) loop nests of AMG2013 are data race free.", "num_citations": "9\n", "authors": ["254"]}
{"title": "A preliminary study of compiler transformations for graph applications on the Emu system\n", "abstract": " Unlike dense linear algebra applications, graph applications typically suffer from poor performance because of 1) inefficient utilization of memory systems through random memory accesses to graph data, and 2) overhead of executing atomic operations. Hence, there is a rapid growth in improving both software and hardware platforms to address the above challenges. One such improvement in the hardware platform is a realization of the Emu system, a thread migratory and near-memory processor. In the Emu system, a thread responsible for computation on a datum is automatically migrated over to a node where the data resides without any intervention from the programmer. The idea of thread migrations is very well suited to graph applications as memory accesses of the applications are irregular. However, thread migrations can hurt the performance of graph applications if overhead from the migrations dominates\u00a0\u2026", "num_citations": "9\n", "authors": ["254"]}
{"title": "Graph500 on openshmem: Using a practical survey of past work to motivate novel algorithmic developments\n", "abstract": " Graph500 is an open specification of a graph-based benchmark for high-performance computing (HPC). The core computational kernel of Graph500 is a breadth-first search of an undirected graph. Unlike many other HPC benchmarks, Graph500 is therefore characterized by heavily irregular and fine-grain computation, memory accesses, and network communication. Therefore, it can serve as a more realistic stress test of modern HPC hardware, software, and algorithmic techniques than other benchmarking efforts.", "num_citations": "9\n", "authors": ["254"]}
{"title": "Design, verification and applications of a new read-write lock algorithm\n", "abstract": " Coordination and synchronization of parallel tasks is a major source of complexity in parallel programming. These constructs take many forms in practice including directed barrier and point-to-point synchronizations, termination detection of child tasks, and mutual exclusion in accesses to shared resources.", "num_citations": "9\n", "authors": ["254"]}
{"title": "Interfacing Chapel with traditional HPC programming languages\n", "abstract": " Chapel is a high-level parallel programming language that implements a partitioned global address space model (PGAS). Programs written in this programming model have traditionally been selfcontained entities written entirely in one language. While this approach enables the compiler to produce better performing code by doing whole program optimization, it also carries a risk of positioning PGAS languages as \u201cisland\u201d programming languages. In this paper we present a tool that lets Chapel programs call functions and instantiate objects written in C, C++, Fortran 77\u20132008, Java and Python. Our tool creates language bindings that are binary-compatible with those generated by the Babel language interoperability tool. The scientific community maintains a large amount of code (mathematical libraries, solvers and numerical models) written in legacy languages. With the help of our tool, users will gain access to their existing codebase with minimal effort and through a well-defined interface. Knowing the demands of the target audience, we support the full Babel array API. A particular contribution of this paper is that we expose Chapel\u2019s distributed data types through our interface and make them accessible to external functions implemented in traditional serial programming languages. We anticipate applying similar concepts to other PGAS languages in the future.", "num_citations": "9\n", "authors": ["254"]}
{"title": "Improving access to shared data in a partitioned global address space programming model\n", "abstract": " of this thesis and to lend or sell such copies for private, scholarly or scientific research purposes only. The author reserves all other publication and other rights in association with the copyright in the thesis, and except as herein before provided, neither the thesis nor any substantial portion thereof may be printed or otherwise reproduced in any material form whatever without the author\u2019s prior written permission.", "num_citations": "9\n", "authors": ["254"]}
{"title": "Enhanced bitwidth-aware register allocation\n", "abstract": " Embedded processors depend on register files for performance, just like general-purpose processors in desktop and server systems. However, unlike general-purpose processors, the power consumption of register files poses a significant challenge for embedded processors, making it desirable for embedded processors to use as few registers as possible. Past research has indicated the potential for leveraging bitwidth analysis and bitwidth-aware register allocation to reduce register usage in embedded applications.               This paper makes the following contributions in evaluating and enhancing bitwidth-aware register allocation for embedded applications. First, we compare the Tallam-Gupta bitwidth analysis with an idealized limit study, and show significant opportunities for enhancements. Second, we show how bitwidth-aware register allocation can be enhanced by enhanced bitwidth analysis for\u00a0\u2026", "num_citations": "9\n", "authors": ["254"]}
{"title": "Detecting MPI usage anomalies via partial program symbolic execution\n", "abstract": " MPI is a message passing based programming model for distributed-memory parallelism that is widely used for programming supercomputers. However, debugging and verification of MPI programs is generally recognized to be a deep technical challenge. This challenge is further exacerbated by a recent increase in the use of nonblocking MPI operations for improved scalability, since incorrect use of these operations can introduce new classes of bugs related to data races. In this paper, we introduce a new debugging approach based on partial symbolic execution to identify anomalies in MPI usage. Our approach avoids the false positives inherent in static analysis, while still scaling to large programs. Further, our approach can be applied to incomplete programs and explore multiple execution paths, thereby leading to increased coverage compared with dynamic approaches. An experimental comparison with state\u00a0\u2026", "num_citations": "8\n", "authors": ["254"]}
{"title": "Topkapi: parallel and fast sketches for finding top-K frequent elements\n", "abstract": " Identifying the top-K frequent items in a collection or data stream is one of the most common and important operations in large data processing systems. As a result, several solutions have been proposed to solve this problem approximately. We observe that the existing algorithms, although theoretically sound, are suboptimal from the performance perspective because of their limitations in exploiting parallelism in modern distributed compute settings. In particular, for identifying top-K frequent items, Count-Min Sketch (CMS) has an excellent update time, but lacks the important property of reducibility which is needed for exploiting available massive data parallelism. On the other end, the popular Frequent algorithm (FA) leads to reducible summaries but its update costs are significant. In this paper, we present Topkapi, a fast and parallel algorithm for finding top-K frequent items, which gives the best of both worlds, ie, it is reducible and has fast update time similar to CMS. Topkapi possesses strong theoretical guarantees and leads to significant performance gains due to increased parallelism, relative to past work.", "num_citations": "8\n", "authors": ["254"]}
{"title": "Formalization of Habanero phasers using Coq\n", "abstract": " Phasers pose an interesting synchronization mechanism that generalizes many collective synchronization patterns seen in parallel programming languages, including barriers, clocks, and point-to-point synchronization using latches or semaphores. This work characterizes scheduling constraints on phaser operations, by relating the execution state of two tasks that operate on the same phaser. We propose a formalization of Habanero phasers, May-Happen-In-Parallel, and Happens\u2013Before relations for phaser operations, and show that these relations conform with the semantics. Our formalization and proofs are fully mechanized using the Coq proof assistant, and are available online.", "num_citations": "8\n", "authors": ["254"]}
{"title": "Static data race detection for SPMD programs via an extended polyhedral representation\n", "abstract": " SPMD kernel with barriers1//tid\u2212 Thread id 2//T\u2212 Total number of threads 3# pragma omp parallel shared (A){4 for (int i= 0; i< N; i++){5 for (int j= 0; j< N; j++){6 int temp= A [tid+ i+ j];//S1 7# pragma omp barrier 8 A [tid]+= temp;//S2", "num_citations": "8\n", "authors": ["254"]}
{"title": "Auto-grading for parallel programs\n", "abstract": " Fundamentals of Parallel Programming (COMP 322) is a required course for all Computer Science majors at Rice University. It introduces students to several basic concepts of parallel programming and parallel algorithms and follows a\" pedagogic approach that exposes students to intellectual challenges in parallel software without enmeshing them in jargon and lower-level details of today's parallel systems\". The material from COMP 322 has also been used in related courses at other universities including Harvey Mudd College and Brigham Young University.", "num_citations": "8\n", "authors": ["254"]}
{"title": "Characterizing application execution using the open community runtime\n", "abstract": " Exascale and extreme-scale systems impose fundamental new requirements on software to target platforms with severe energy, data movement and resiliency constraints within and across nodes, and large degrees of homogeneous and heterogeneous parallelism and locality within a node. These challenges have led to the exploration of a diverse range of many-core processor architectures and memory hierarchies for future systems, that differ quite dramatically from current systems. As a result, there is still a lack of consensus as to what the main tenets should be for the execution model and low-level runtime system in future architectures. The Open Community Runtime (OCR) was created to engage the broader community of software and hardware researchers in identifying these underlying principles. While there is broad support for including dynamic task parallelism as one of the pillars of future execution models, there is currently little agreement on what else should be included. OCR proposes an approach to complete this picture by adding events and relocatable data-blocks as two additional pillars to build on, and shows how the three concepts (tasks, events, data blocks) can be combined in very general ways to support a wide range of higher-level programming constructs. In this paper, we focus on the use of OCR for application characterization. Despite the fact that the development of OCR is still at an early stage, we are fortunate that a large number of applications have already been implemented using the OCR APIs. We study the behavior of OCR implemen-", "num_citations": "8\n", "authors": ["254"]}
{"title": "Hj-opencl: Reducing the gap between the jvm and accelerators\n", "abstract": " Recently there has been increasing interest in supporting execution of Java Virtual Machine (JVM) applications on accelerator architectures, such as GPUs. Unfortunately, there is a large gap between the features of the JVM and those commonly supported by accelerators. Examples of important JVM features include exceptions, dynamic memory allocation, use of arbitrary composite objects, file I/O, and more. Recent work from our research group tackled the first feature in that list, JVM exception semantics [14]. This paper continues along that path by enabling the acceleration of JVM parallel regions that include object references and dynamic memory allocation.", "num_citations": "8\n", "authors": ["254"]}
{"title": "Polyhedral transformations of explicitly parallel programs\n", "abstract": " Polyhedral Transformations of Explicitly Parallel Programs Page 1 1/42 Polyhedral Transformations of Explicitly Parallel Programs Prasanth Chatarasi, Jun Shirako, Vivek Sarkar Habanero Extreme Scale Software Research Group Department of Computer Science Rice University January 19, 2015 Prasanth Chatarasi, Jun Shirako, Vivek Sarkar IMPACT Workshop, 19 Jan 2015 Page 2 2/42 Introduction 1 Introduction 2 Explicit Parallelism and Motivation 3 Our Approach 4 Preliminary Results 5 Related Work 6 Conclusions, Future work and Acknowledgments Prasanth Chatarasi, Jun Shirako, Vivek Sarkar IMPACT Workshop, 19 Jan 2015 Page 3 3/42 Introduction Introduction Software with explicit parallelism is on rise Two major compiler approaches for program optimizations AST-based Polyhedral-based Past work on transformations of parallel programs using AST-based approaches Eg, [Nicolau et.al 2009], [et.al ] of \u2026", "num_citations": "8\n", "authors": ["254"]}
{"title": "A decoupled non-SSA global register allocation using bipartite liveness graphs\n", "abstract": " Register allocation is an essential optimization for all compilers. A number of sophisticated register allocation algorithms have been developed over the years. The two fundamental classes of register allocation algorithms used in modern compilers are based on Graph Coloring (GC) and Linear Scan (LS). However, these two algorithms have fundamental limitations in terms of precision. For example, the key data structure used in GC-based algorithms, the interference graph, lacks information on the program points at which two variables may interfere. The LS-based algorithms make local decisions regarding spilling, and thereby trade off global optimization for reduced compile-time and space overheads. Recently, researchers have proposed Static Single Assignment (SSA)-based decoupled register allocation algorithms that exploit the live-range split points of the SSA representation to optimally solve the spilling\u00a0\u2026", "num_citations": "8\n", "authors": ["254"]}
{"title": "Integrating mpi with asynchronous task parallelism\n", "abstract": " This paper describes a programming model that integrates intra-node asynchronous task parallelism with inter-node MPI communications to address the hybrid parallelism challenges faced by future extreme scale systems. We explore the integration of MPI\u2019s blocking and non-blocking communications with lightweight tasks. We also provide the implementation details of a non-blocking runtime execution model based on computation and communication workers.", "num_citations": "8\n", "authors": ["254"]}
{"title": "Challenges in code optimization of parallel programs\n", "abstract": " Code optimization has a rich history that dates back over half a century, and includes deep innovations that arose in response to changing trends in hardware and programming languages. These innovations have contributed significantly to programmer productivity by reducing the effort that programmers spend on hand-implementing code optimizations and by enabling code to be more portable. Often these innovations were accompanied by paradigm shifts in the foundations of compilers led by the introduction of new ideas such as interprocedural whole program analysis, coloring-based register allocation, static single assignment form, array dependence analysis, pointer alias analysis, loop transformations, adaptive profile-directed optimizations, and dynamic compilation.             In this talk, we claim that the current multicore trend in the computer industry is forcing a new paradigm shift in compilers to\u00a0\u2026", "num_citations": "8\n", "authors": ["254"]}
{"title": "Analyzable atomic sections: Integrating fine-grained synchronization and weak consistency models for scalable parallelism\n", "abstract": " A key source of complexity in parallel programming arises from ne-grained synchronizations which appear in the form of lock/unlock or critical sections. Not only are these constructs complicated to understand and debug, but they are also often an impediment to achieving scalable parallelism because of the overhead of the underlying synchronization operations and their accompanying data consistency operations. In this paper, we propose the use of analyzable atomic sections as a parallel programming construct that can simplify the use of ne-grained synchronization, while delivering scalable parallelism by using a weak memory consistency model. We use OpenMP as the base programming model in this paper, and show how the OpenMP memory model can be formalized by using the pomset abstraction in the Location Consistency (LC) memory model. We then show how OpenMP can be extended with analyzable atomic sections, and use two examples to motivate the potential for scalable parallelism with this extension. i", "num_citations": "8\n", "authors": ["254"]}
{"title": "Baring it all to software: The raw machine\n", "abstract": " Rapid advances in technology force a quest for computer architectures that exploit new opportunities and shed existing mechanisms that do not scale. Current architectures, such as hardware scheduled superscalars, are already hitting performance and complexity limits and cannot be scaled indefinitely. The Reconfigurable Architecture Workstation (Raw) is a simple, wire-efficient architecture that scales with increasing VLSI gate densities and attempts to provide performance that is at least comparable to that provided by scaling an existing architecture, but that can achieve orders of magnitude more performance for applications in which the compiler can discover and statically schedule fine-grain parallelism.", "num_citations": "8\n", "authors": ["254"]}
{"title": "Enabling resilience in asynchronous many-task programming models\n", "abstract": " Resilience is an imminent issue for next-generation platforms due to projected increases in soft/transient failures as part of the inherent trade-offs among performance, energy, and costs in system design. In this paper, we introduce a comprehensive approach to enabling application-level resilience in Asynchronous Many-Task (AMT) programming models with a focus on remedying Silent Data Corruption (SDC) that can often go undetected by the hardware and OS. Our approach makes it possible for the application programmer to declaratively express resilience attributes with minimal code changes, and to delegate the complexity of efficiently supporting resilience to our runtime system. We have created a prototype implementation of our approach as an extension to the Habanero C/C++ library (HClib), where different resilience techniques including task replay, task replication, algorithm-based fault\u00a0\u2026", "num_citations": "7\n", "authors": ["254"]}
{"title": "Transitive joins: a sound and efficient online deadlock-avoidance policy\n", "abstract": " We introduce a new online deadlock-avoidance policy, Transitive Joins (TJ), that targets programs with dynamic task parallelism and arbitrary join operations. In this model, a computation task can asynchronously spawn new tasks and selectively join (block) on any task for which it has a handle. We prove that TJ soundly guarantees the absence of deadlock cycles among the blocking join operations. We present an algorithm for dynamically verifying TJ and show that TJ results in fewer false positives than the state-of-the-art policy, Known Joins (KJ). We evaluate an implementation of our verifier in comparison to prior work. The evaluation results show that instrumenting a program with a TJ verifier incurs geometric mean overheads of only 1.06\u00d7 in execution time and 1.09\u00d7 in memory usage, which is better overall than existing KJ verifiers. TJ is a practical online deadlock-avoidance policy that is applicable to a wide\u00a0\u2026", "num_citations": "7\n", "authors": ["254"]}
{"title": "OpenMP as a high-level specification language for parallelism\n", "abstract": " While OpenMP is the de facto standard of shared memory parallel programming models, a number of alternative programming models and runtime systems have arisen in recent years. Fairly evaluating these programming systems can be challenging and can require significant manual effort on the part of researchers. However, it is important to facilitate these comparisons as a way of advancing both the available OpenMP runtimes and the research being done with these novel programming systems.                 In this paper we present the OpenMP-to-X framework, an open source tool for mapping OpenMP constructs and APIs to other parallel programming systems. We apply OpenMP-to-X to the HClib parallel programming library, and use it to enable a fair and objective comparison of performance and programmability among HClib, GNU OpenMP, and Intel OpenMP. We use this investigation to expose\u00a0\u2026", "num_citations": "7\n", "authors": ["254"]}
{"title": "A distributed selectors runtime system for java applications\n", "abstract": " The demand for portable mainstream programming models supporting scalable, reactive and versatile distributed computing is growing dramatically with the proliferation of manycore/heterogeneous processors on portable devices and cloud computing clusters that can be elastically and dynamically allocated. With such changes, distributed software systems and applications are shifting towards service oriented architectures (SOA) that consist of largely decoupled, dynamically replaceable components and connected via loosely coupled, interactive networks that may exhibit more complex coordination and synchronization patterns.", "num_citations": "7\n", "authors": ["254"]}
{"title": "A composable deadlock-free approach to object-based isolation\n", "abstract": " A widely used principle in the design of concurrent programs is isolation \u2013 the property that a task can operate on shared data without interference from other tasks. In this paper, we introduce a new approach to object-based isolation that is guaranteed to be deadlock-free, while still retaining the rollback benefits of transactions. Further, our approach differentiates between read and write accesses in its concurrency control mechanisms. Finally, since the generality of our approach precludes the use of static ordering for deadlock avoidance, our runtime ensures deadlock-freedom by detecting and resolving deadlocks at runtime automatically, without involving the programmer.", "num_citations": "7\n", "authors": ["254"]}
{"title": "Finish accumulators: An efficient reduction construct for dynamic task parallelism\n", "abstract": " Parallel reductions represent a common pattern for computing the aggregation of an associative and commutative operation, such as summation, across multiple pieces of data supplied by parallel tasks. In this poster, we introduce finish accumulators, a unified construct that supports predefined and user-defined parallel reductions for dynamic task parallelism. Finish accumulators are designed to be integrated into structured task parallelism constructs, such as the async and finish constructs found in the X10 and Habanero-Java (HJ) languages, so as to guarantee determinism for accumulation and to avoid any possible race conditions in referring to intermediate results. In contrast to lower-level reduction constructs such as atomic variables, the high-level semantics of finish accumulators allows for a wide range of implementations with different accumulation policies, e.g., eager-computation vs. lazy\u00a0\u2026", "num_citations": "7\n", "authors": ["254"]}
{"title": "DrHJ: a lightweight pedagogic IDE for Habanero Java\n", "abstract": " The Java language and runtime environment has had a profound worldwide impact on computer software since its introduction nearly two decades ago. It has enabled the creation of a rich ecosystem of libraries, frameworks, and tools that promises to deliver significant value for many years to come. Consequently, a wide range of Interactive Development Environments (IDEs) have emerged to increase the productivity of Java programmers. They vary in functionality based on the expertise level assumed for their target user base. The Eclipse Java Development Tools (JDT) project offers a rich set of power tools for experienced programmers, but can be harder for novice programmers to set up and use. In contrast, IDEs such as DrJava [2] and BlueJ [16] have been developed primarily for use in introductory programming courses.", "num_citations": "7\n", "authors": ["254"]}
{"title": "Phaser beams: Integrating stream parallelism with task parallelism\n", "abstract": " Phaser Beams: Integrating Stream Parallelism with Task Parallelism Page 1 Phaser Beams: Integrating Stream Parallelism with Task Parallelism X10 Workshop June 4th, 2011 Jun Shirako, David M. Peixotto, Dragos-Dumitru Sbirlea and Vivek Sarkar Rice University Page 2 Introduction \u2022 Stream Languages \u2013 Natures to explicitly specify streaming parallelism in a stream graph \u2022 Filter (node): Computation unit \u2022 Stream (edge): Flow of data among filters \u2013 Lack of dynamic parallelism \u2022 Fixed stream graphs w/o dynamic reconfiguration \u2022 Task Parallel Languages \u2013 Support of dynamic task parallelism \u2022 Task: Dynamically created/terminated lightweight thread \u2013 Chapel, Cilk, Fortress, Habanero-Java/C, Intel Threading Building Blocks, Java Concurrency Utilities, Microsoft Task Parallel Library, OpenMP 3.0 and X10 \u2013 Lack of support for efficient streaming communication among tasks \u2022 Address the gap between two \u2022 Phaser -\u2026", "num_citations": "7\n", "authors": ["254"]}
{"title": "Code optimization of parallel programs: evolutionary vs. revolutionary approaches\n", "abstract": " Code optimization has a rich history that dates back over half a century. Over the years, it has contributed deep innovations to address challenges posed by new computer system and programming language features. Examples of the former include optimizations for improved register utilization, instruction-level parallelism, vector parallelism, multiprocessor parallelism and memory hierarchy utilization. Examples of the latter include optimizations for procedural, object-oriented, functional and domain-specific languages as well as dynamic optimization for managed runtimes. These optimizations have contributed significantly to programmer productivity by reducing the effort that programmers need to spend on hand-implementing code optimizations and by enabling code to be more portable, especially as programming models and computer architectures change. While compiler frameworks are often able to\u00a0\u2026", "num_citations": "7\n", "authors": ["254"]}
{"title": "Optimized lock assignment and allocation for productivity: A method for exploiting concurrency among critical sections\n", "abstract": " One of the major productivity issues in parallel programming arises from the use of lock/unlock operations or atomic/critical sections to enforce mutual exclusion. Not only are these constructs complicated to understand and debug, but they are also often an impediment to achieving scalable parallelism. In this paper, we propose to give the programmer the convenience of critical sections combined with the scalability of fine-grained locks, by solving two technical problems related to optimized assignment of locks to critical sections. The first problem, Minimum Lock Assignment (MLA), addresses the problem of finding the minimum number of locks needed to enforce mutual exclusion among interfering critical sections without any loss of concurrency. The second problem, K-Lock Allocation (K-LA) addresses the problem of allocating a fixed number (K) of locks to critical sections so as to minimize the serialization overhead. i", "num_citations": "7\n", "authors": ["254"]}
{"title": "X10: An object oriented aproach to non-uniform cluster computing\n", "abstract": " X10: An object oriented aproach to non-uniform cluster computing | Companion to the 20th annual ACM SIGPLAN conference on Object-oriented programming, systems, languages, and applications ACM Digital Library Logo ACM Logo Google, Inc. (search) Advanced Search Browse About Sign in Register Advanced Search Journals Magazines Proceedings Books SIGs Conferences People More Search ACM Digital Library SearchSearch Advanced Search splash Conference Proceedings Upcoming Events Authors Affiliations Award Winners More HomeConferencesSPLASHProceedingsOOPSLA '05X10: An object oriented aproach to non-uniform cluster computing ARTICLE X10: An object oriented aproach to non-uniform cluster computing Share on Author: Vivek Sarkar profile image Vivek Sarkar View Profile Authors Info & Affiliations Publication: OOPSLA '05: Companion to the 20th annual ACM SIGPLAN on -, , , '\u2026", "num_citations": "7\n", "authors": ["254"]}
{"title": "Lightweight object-oriented shared variables for distributed applications on the Internet\n", "abstract": " This paper describes a lightweight yet powerful approach for writing distributed applications using shared variables. Our approach, called SHAREHOLDER, is inspired by the flexible and intuitive model of information access common to the World Wide Web. The distributed applications targeted by our approach all share a weak consistency model and loose transaction semantics, similar to a user's model of accessing email, bulletin boards, chat rooms, etc. on the Internet. The SHAREHOLDER infrastructure has several advantages. Its highly object-oriented view of shared variables simplifies their initialization and configuration. A shared variable's distribution mechanism is specified through an associated configuration object, and the programmer does not need to write any extra code to implement the sharing mechanism. These configuration objects can be initialized at run-time, allowing tremendous flexibility in\u00a0\u2026", "num_citations": "7\n", "authors": ["254"]}
{"title": "Misim: An end-to-end neural code similarity system\n", "abstract": " Code similarity systems are integral to a range of applications from code recommendation to automated construction of software tests and defect mitigation. In this paper, we present Machine Inferred Code Similarity (MISIM), a novel end-toend code similarity system that consists of two core components. First, MISIM uses a novel context-aware semantic structure, which is designed to aid in lifting semantic meaning from code syntax. Second, MISIM provides a neural-based code similarity scoring algorithm, which can be implemented with various neural network architectures with learned parameters. We compare MISIM to three stateof-the-art code similarity systems:(i) code2vec,(ii) Neural Code Comprehension, and (iii) Aroma. In our experimental evaluation across 45,780 programs, MISIM consistently outperformed all three systems, often by a large factor (upwards of 40.6\u00d7).", "num_citations": "6\n", "authors": ["254"]}
{"title": "Marvel: A data-centric compiler for dnn operators on spatial accelerators\n", "abstract": " The efficiency of a spatial DNN accelerator depends heavily on the compiler and its cost model ability to generate optimized mappings for various operators of DNN models on to the accelerator's compute and memory resources. But, existing cost models lack a formal boundary over the operators for precise and tractable analysis, which poses adaptability challenges for new DNN operators. To address this challenge, we leverage the recently introduced Maestro Data-Centric (MDC) notation. We develop a formal understanding of DNN operators whose mappings can be described in the MDC notation, because any mapping adhering to the notation is always analyzable by the MDC's cost model. Furthermore, we introduce a transformation for translating mappings into the MDC notation for exploring the mapping space. Searching for the optimal mappings is challenging because of the large space of mappings, and this challenge gets exacerbated with new operators and diverse accelerator configurations.To address this challenge, we propose a decoupled off-chip/on-chip approach that decomposes the mapping space into off-chip and on-chip subspaces, and first optimizes the off-chip subspace followed by the on-chip subspace. The motivation for this decomposition is to reduce the size of the search space dramatically and also to prioritize the optimization of off-chip data movement, which is 2-3 orders of magnitude more compared to the on-chip data movement. We implemented our approach in a tool called {\\em Marvel}, and another major benefit of our approach is that it is applicable to any DNN operator conformable with the MDC notation.", "num_citations": "6\n", "authors": ["254"]}
{"title": "OMPSan: static verification of OpenMP\u2019s data mapping constructs\n", "abstract": " OpenMP offers directives for offloading computations from CPU hosts to accelerator devices such as GPUs. A key underlying challenge is in efficiently managing the movement of data across the host and the accelerator. User experiences have shown that memory management in OpenMP programs with offloading capabilities is non-trivial and error-prone.                 This paper presents OMPSan (OpenMP Sanitizer) \u2013 a static analysis-based tool that helps developers detect bugs from incorrect usage of the map clause, and also suggests potential fixes for the bugs. We have developed an LLVM based data flow analysis that validates if the def-use information of the array variables are respected by the mapping constructs in the OpenMP program. We evaluate OmpSan over some standard benchmarks and also show its effectiveness by detecting commonly reported bugs.", "num_citations": "6\n", "authors": ["254"]}
{"title": "Optimized execution of parallel loops via user-defined scheduling policies\n", "abstract": " On-node parallelism continues to increase in importance for high-performance computing and most newly deployed supercomputers have tens of processor cores per node. These higher levels of on-node parallelism exacerbate the impact of load imbalance and locality in parallel computations, and current programming systems notably lack features to enable efficient use of these large numbers of cores or require users to modify codes significantly. Our work is motivated by the need to address application-specific load balance and locality requirements with minimal changes to application codes.", "num_citations": "6\n", "authors": ["254"]}
{"title": "GPUIterator: bridging the gap between Chapel and GPU platforms\n", "abstract": " PGAS (Partitioned Global Address Space) programming models were originally designed to facilitate productive parallel programming at both the intra-node and inter-node levels in homogeneous parallel machines. However, there is a growing need to support accelerators, especially GPU accelerators, in heterogeneous nodes in a cluster. Among high-level PGAS programming languages, Chapel is well suited for this task due to its use of locales and domains to help abstract away low-level details of data and compute mappings for different compute nodes, as well as for different processing units (CPU vs. GPU) within a node.", "num_citations": "6\n", "authors": ["254"]}
{"title": "Performance evaluation of OpenMP's target construct on GPUs-exploring compiler optimisations\n", "abstract": " OpenMP is a directive-based shared memory parallel programming model and has been widely used for many years. From OpenMP 4.0 onwards, GPU platforms are supported by extending OpenMP's high-level parallel abstractions with accelerator programming. This extension allows programmers to write GPU programs in standard C/C++ or Fortran languages, without exposing too many details of GPU architectures. However, such high-level programming models generally impose additional program optimisations on compilers and runtime systems. Otherwise, OpenMP programs could be slower than fully hand-tuned and even naive implementations with low-level programming models like CUDA. To study potential performance improvements by compiling and optimising high-level programs for GPU execution, in this paper, we: 1) evaluate a set of OpenMP benchmarks on two NVIDIA Tesla GPUs (K80 and\u00a0\u2026", "num_citations": "6\n", "authors": ["254"]}
{"title": "Adha: Automatic data layout framework for heterogeneous architectures\n", "abstract": " Data layouts play a crucial role in determining the performance of a given application running on a given architecture. Existing parallel programming frameworks for both multicore and heterogeneous systems leave the onus of selecting a data layout to the programmer. Therefore, shifting the burden of data layout selection to optimizing compilers can greatly enhance programmer productivity and application performance. In this work, we introduce ADHA: a two-level hierarchal formulation of the data layout problem for modern heterogeneous architectures. We have created a reference implementation of ADHA in the Heterogeneous Habanero-C (H2C) parallel programming system. ADHA shows significant performance benefits of up to 6.92\u00d7 compared to manually specified layouts for two benchmark programs running on a CPU+GPU heterogeneous platform.", "num_citations": "6\n", "authors": ["254"]}
{"title": "Automatic vector instruction selection for dynamic compilation\n", "abstract": " Accelerating program performance via short SIMD vector units is very common in modern processors, as evidenced by the use of SSE, MMX, and AltiVec SIMD instructions in multimedia, scientific, and embedded applications. To take full advantage of the vector capabilities, a compiler needs to generate efficient vector code automatically. However, most commercial and open-source compilers still fall short of using the full potential of vector units, and only generate vector code for simple loop nests. In this poster, we present the design and implementation of an auto-vectorization framework in the back-end of a dynamic compiler that not only generates optimized vector code but is also well integrated with the instruction scheduler and register allocator. Additionally, we describe a vector instruction selection algorithm based on dynamic programming. Our results obtained in JikesRVM dynamic compilation environment\u00a0\u2026", "num_citations": "6\n", "authors": ["254"]}
{"title": "Programming challenges for petascale and multicore parallel systems\n", "abstract": " This decade marks a resurgence for parallel computing with high-end systems moving to petascale and mainstream systems moving to multi-core processors. Unlike previous generations of hardware evolution, this shift will have a major impact on existing software. For petascale, it is widely recognized by application experts that past approaches based on domain decomposition will not scale to exploit the parallelism available in future high-end systems. For multicore, it is acknowledged by hardware vendors that enablement of mainstream software for execution on multiple cores is the major open problem that needs to be solved in support of this hardware trend. These software challenges are further compounded by an increased adoption of high performance computing in new application domains that may not fit the patterns of parallelism that have been studied by the community thus far. In this talk, we\u00a0\u2026", "num_citations": "6\n", "authors": ["254"]}
{"title": "Combined instruction scheduling and register allocation\n", "abstract": " In this paper, we describe a novel framework for expressing an optimization problem that simultaneously addresses instruction scheduling and register allocation, referred to as CRISP. By modeling spill-costs directly in the scheduling problem, CRISP permits the design of algorithms whose objective is to exploit the available instruction level parallelism --- the traditional goal of instruction scheduling --- while lowering the cost of register spilling at the same time. Currently, these optimizations are performed in separate phases and interact in ways that are not characterized very well, leading to phase-ordering problems. We also develop a fast heuristic in this paper for solving this combined optimization in the context of basic-blocks; our algorithm runs in time O ( E N) where the basic block of N instructions has E edges; this time includes all preprocessing costs. In comparison to conventional phase-ordered approaches\u00a0\u2026", "num_citations": "6\n", "authors": ["254"]}
{"title": "Future high performance computing capabilities: Summary report of the advanced scientific computing advisory committee (ascac) subcommittee\n", "abstract": " The ASCAC Subcommittee on Future High Performance Computing (HPC) Capabilities has reviewed opportunities and challenges related to the most promising technologies that are currently planned for the post-exascale (2020's) and post-Moore (2030's and beyond) timeframes. We briefly summarize in this report the key findings and recommendations from this review, from the perspective of planning and research directions that need to be given priority to prepare for the very significant challenges that await us in the post-Moore computing era. An overarching concern that emerged from the subcommittee's deliberations is that DOE has lost considerable momentum in funding and sustaining a research pipeline in the applied math and computer science areas that should have been the\" seed corn\" for preparing for these future challenges, and it is therefore critical to correct this gap as soon as possible. While the subcommittee understands the paramount importance of DOE's commitment to deliver exascale capabilities, we believe that it is essential for DOE ASCR to fund research and development that looks beyond the Exascale Computing Project (ECP) horizon so as to ensure our nation's continued leadership in HPC and we were glad to learn about the ASCAC exascale transition subcommittee in this regard.", "num_citations": "5\n", "authors": ["254"]}
{"title": "Cost-driven thread coarsening for GPU kernels\n", "abstract": " Directive-based programming models like OpenACC provide a higher level abstraction and low overhead approach of porting existing applications to GPGPUs and other heterogeneous HPC hardware. Such programming models increase the design space exploration possible at the compiler level to exploit specific features of different architectures. We observed that traditional applications designed for latency optimized out-of-order pipelined CPUs do not exploit the throughput optimized in-order pipelined GPU architecture efficiently. In this paper we develop a model to estimate the memory throughput of a given application. Then we use the loop interleave transformation to improve the memory bandwidth utilization of a given kernel.", "num_citations": "5\n", "authors": ["254"]}
{"title": "Preparing an online java parallel computing course\n", "abstract": " While multi-core platforms are now ubiquitous in all areas of information technology, from enterprise software engineering to mobile app development, parallel computing education is still lagging behind the demand for skilled parallel programmers. At many universities today, parallel and concurrent computing is still not part of the core curriculum because of resistance to major curriculum changes. Many other universities lack the necessary educators or infrastructure to teach a comprehensive parallel computing course. Furthermore, even addressing these issues would do nothing towards supporting software professionals who have already entered the work force and have no plans to return to school. To address this broad need for a standalone, publically available, comprehensive, and easily accessible course on parallel computing, we have developed an online offering packaged as a Coursera Specialization\u00a0\u2026", "num_citations": "5\n", "authors": ["254"]}
{"title": "DOE Advanced Scientific Computing Advisory Committee (ASCAC) Report: Exascale Computing Initiative Review\n", "abstract": " On November 19, 2014, the Advanced Scientific Computing Advisory Committee (ASCAC) was charged with reviewing the Department of Energy\u2019s conceptual design for the Exascale Computing Initiative (ECI). In particular, this included assessing whether there are significant gaps in the ECI plan or areas that need to be given priority or extra management attention. Given the breadth and depth of previous reviews of the technical challenges inherent in exascale system design and deployment, the subcommittee focused its assessment on organizational and management issues, considering technical issues only as they informed organizational or management priorities and structures. This report presents the observations and recommendations of the subcommittee.", "num_citations": "5\n", "authors": ["254"]}
{"title": "A comparative study on LQR and H\u221econtrol for damping oscillations in power system network considering different operating points\n", "abstract": " This paper presents the comparative study on damping of oscillations in interconnected power system network considering multiple operating points using Linear Quadratic Regulator (LQR) and H \u221e  control techniques. The LQR control strategy is designed in such a way that the required states are to be known and H \u221e  control strategy is designed by using linear matrix inequality(LMI) approach. The control techniques are implemented for base case operating point and multiple operating points such as line outage, load change. Both controller performances are tested on the New England 39-bus 10-machine system in time domain and frequency domain by using MATLAB/Simulink environment. From the results it is observed that H \u221e  control design has shown promising results over the LQR design for different operating points. However, the LQR control design gives good results over the H \u221e  control design if\u00a0\u2026", "num_citations": "5\n", "authors": ["254"]}
{"title": "Dynamic determinism checking for structured parallelism\n", "abstract": " Determinism is a powerful property, making it much easier to design, implement, and debug parallel programs when it can be guaranteed. There has been much work on guaranteeing determinism of programs, both through dynamic and static approaches. The dynamic approaches tend to have high overheads, while the static approaches can be difficult for non-experts to understand and can require high programmer effort in the form of typing annotations and code restructuring to fit the static type system. In this work, we present a compiler and runtime system that ensures determinism of application code, assuming that libraries satisfy their determinism specifications. In our system, the library writer includes determinism specifications in the form of annotations on the methods in the library. When these libraries are used by application programmers, our compiler will insert dynamic checks where necessary to ensure determinism at runtime. We demonstrate that our inserted dynamic checks lead to an average (geometric mean) slowdown of only 1.26\u00d7 on 16 cores for our benchmark, thereby establishing the practicality of our approach.", "num_citations": "5\n", "authors": ["254"]}
{"title": "Subregion analysis and bounds check elimination for high level arrays\n", "abstract": " For decades, the design and implementation of arrays in programming languages has reflected a natural tension between productivity and performance. Recently introduced HPCS languages (Chapel, Fortress and X10) advocate the use of high-level arrays for improved productivity. For example, high-level arrays in the X10 language support rank-independent specification of multidimensional loop and array computations using regions and points. Three aspects of X10 high-level arrays are important for productivity but pose significant performance challenges: high-level accesses are performed through point objects rather than integer indices, variables containing references to arrays are rank-independent, and all subscripts in a high-level array access must be checked for bounds violations.               The first two challenges have been addressed in past work. In this paper, we address the third challenge of\u00a0\u2026", "num_citations": "5\n", "authors": ["254"]}
{"title": "Array optimizations for parallel implementations of high productivity languages\n", "abstract": " This paper presents an interprocedural rank analysis algorithm to automatically infer ranks of arrays in X10, a language that supports rank-independent specification of loop and array computations using regions and points. We use the rank analysis information to enable storage transformations on arrays. We evaluate a transformation that converts high-level multidimensional X10 arrays into lower-level multidimensional Java arrays, when legal to do so. Preliminary performance results for a set of parallel computational benchmarks on a 64-way AIX Power5+ SMP machine show that our optimizations deliver performance that rivals the performance of lower-level, hand-tuned code with explicit loops and array accesses, and up to two orders of magnitude faster than unoptimized, high-level X10 programs. The results show that our optimizations also help improve the scalability of X10 programs by demonstrating that\u00a0\u2026", "num_citations": "5\n", "authors": ["254"]}
{"title": "Optimizing array accesses in high productivity languages\n", "abstract": " One of the outcomes of DARPA\u2019s HPCS program has been the creation of three new high productivity languages: Chapel, Fortress, and X10. While these languages have introduced improvements in language expressiveness and programmer productivity, several technical challenges still remain in delivering high performance with these languages. In the absence of optimization, the high-level language constructs that improve productivity can result in order-of-magnitude runtime performance degradations.               This paper addresses the problem of efficient code generation for high level array accesses in the X10 language. Two aspects of high level array accesses in X10 are important for productivity but also pose significant performance challenges: the high level accesses are performed through Point objects rather than integer indices, and variables containing references to arrays are rank\u00a0\u2026", "num_citations": "5\n", "authors": ["254"]}
{"title": "X10: an Experimental Language for High Productivity Programming of Scalable Systems\n", "abstract": " It is well established that application development pro-ductivity is a significant bottleneck in the time to solution for obtaining production applications on High-End Computing (HEC) systems. Previously, we introduced a simple model for defining application development productivity in the presence of multiple expertise levels, and used this model to motivate the programming model and tools solution be-ing pursued in the IBM PERCS project [9]. In this paper, we describe X10, an experimental language that embodies a new parallel programming model serves as the foundation for multiple productivity-improving technologies in PERCS ranging from visualization and refactoring tools to static and dynamic optimizing compilers. 1", "num_citations": "5\n", "authors": ["254"]}
{"title": "Lightweight object-oriented shared variables for cluster computing in Java\n", "abstract": " This paper describes a lightweight yet powerful approach for writing distributed applications in Java using shared variables. Our approach, called S hare H older, is inspired by the flexible and intuitive model of information access common to the World Wide Web. The distributed applications targeted by our approach all share a weak consistency model, similar to a user's model of accessing e-mail, bulletin boards, chat rooms, etc. on the Internet. The S hare H older infrastructure has several advantages. Its highly object-oriented view of shared variables simplifies their initialization and configuration. A shared variable's distribution mechanism is specified through an associated configuration object, and the programmer does not need to write any extra code to implement the sharing mechanism. These configuration objects can be initialized at run-time, allowing tremendous flexibility in dynamic control of distribution of\u00a0\u2026", "num_citations": "5\n", "authors": ["254"]}
{"title": "Porting DMRG++ scientific application to openpower\n", "abstract": " With the rapidly changing microprocessor designs and architectural diversity (multi-cores, many-cores, accelerators) for the next generation HPC systems, scientific applications must adapt to the hardware, to exploit the different types of parallelism and resources available in the architecture. To get the benefit of all the in-node hardware threads, it is important to use a single programming model to map and coordinate the available work to the different heterogeneous execution units in the node (e.g., multi-core hardware threads (latency optimized), accelerators (bandwidth optimized), etc.).                 Our goal is to show that we can manage the node complexity of these systems by using OpenMP for in-node parallelization by exploiting different \u201cprogramming styles\u201d supported by OpenMP 4.5 to program CPU cores and accelerators. Finding out the suitable programming-style (e.g., SPMD style, multi-level\u00a0\u2026", "num_citations": "4\n", "authors": ["254"]}
{"title": "Chapel-on-x: Exploring tasking runtimes for pgas languages\n", "abstract": " With the shift to exascale computer systems, the importance of productive programming models for distributed systems is increasing. Partitioned Global Address Space (PGAS) programming models aim to reduce the complexity of writing distributed-memory parallel programs by introducing global operations on distributed arrays, distributed task parallelism, directed synchronization, and mutual exclusion. However, a key challenge in the application of PGAS programming models is the improvement of compilers and runtime systems. In particular, one open question is how runtime systems meet the requirement of exascale systems, where a large number of asynchronous tasks are executed.", "num_citations": "4\n", "authors": ["254"]}
{"title": "Analysis of sparse matrix-vector multiply for large sparse linear systems\n", "abstract": " Discretization of the partial differential equations that govern the physics of multi-phase multi-component fluid flow and transport gives rise to large sparse linear systems for practical pore-scale simulation. In this work, we focus on a linear system arising from the discretization of the Cahn-Hilliard equation that governs the separation of a two-component mixture in the pore space. The discretization is performed using the discontinuous Galerkin method. The resulting nonlinear system is solved by use of the Newton's method, which entails multiple large sparse linear systems over Newton iterations course. The sparse linear systems are solved by use of an iterative linear solver. Iterative linear solvers approach the solution process by the computation of sparse matrix-vector (SpMV) products. SpMV products are computational bottlenecks for the simulation of large problems, since they are extremely memory bound. In\u00a0\u2026", "num_citations": "4\n", "authors": ["254"]}
{"title": "Efficient checkpointing of multi-threaded applications as a tool for debugging, performance tuning, and resiliency\n", "abstract": " Past work on application checkpointing systems has either focused on enabling application resiliency or as a tool for debugging (as in record-replay literature). Each of these use cases for checkpoints places different constraints on the constructed checkpointing system. When used for resiliency, checkpointing systems must minimize their interference with the running application. When used for record-replay and numerical debugging, checkpointing systems instead must focus on correlating the contents of a checkpoint to user-visible data structures in order to aid in the debugging process. Past literature has ignored the use of checkpoints in application performance tuning. While existing performance profiling tools enable the identification of hotspots in an application, creating full application checkpoints immediately prior to a hotspot enables rapid iteration on the performance of that hotspot. In this paper, we\u00a0\u2026", "num_citations": "4\n", "authors": ["254"]}
{"title": "A Case for Cooperative Scheduling in X10\u2019s Managed Runtime\n", "abstract": " A Case for Cooperative Scheduling in X10's Managed Runtime Page 1 A Case for Cooperative Scheduling in X10's Managed Runtime X10 Workshop 2014 June 12, 2014 Shams Imam, Vivek Sarkar Rice University Page 2 Task-Parallel Model 2 \u2022 Worker Threads source: http://www.deviantart.com/art/Randomness-20-178737664 Please ignore the DP on the cartoons Page 3 Task-Parallel Model 3 \u2022 Tasks, Work Queues, and Worker Threads \u2022 Runtime manages load balancing and synchronization source: http://www.deviantart.com/art/Randomness-20-178737664 Page 4 Synchronization Constraints 4 source: http://viper-x27.deviantart.com/art/Checkout-Lane-Guest-Comic-161795346 \u2022 Dependences between tasks \u2022 Prevent an executing task from making further progress \u2022 Needs to synchronize with other task(s) Page 5 X10 Synchronization 5 \u2022 Current synchronization constructs \u2022 Finish \u2022 Futures \u2022 Clocks \u2022 Atomic \u2022 ? \u2026", "num_citations": "4\n", "authors": ["254"]}
{"title": "Finish Accumulators: a Deterministic Reduction Construct for Dynamic Task Parallelism\n", "abstract": " Parallel reductions represent a common pattern for computing the aggregation of an associative and commutative operation, such as summation, across multiple pieces of data supplied by parallel tasks. In this paper, we introduce finish accumulators, a unified construct that supports predefined and user-defined deterministic reductions for dynamic async-finish task parallelism. Finish accumulators are designed to be integrated into terminally strict models of task parallelism as in the X10 and Habanero-Java (HJ) languages, which is more general than fully strict models of task parallelism found in Cilk and OpenMP. In contrast to lower-level reduction constructs such as atomic variables, the high-level semantics of finish accumulators allows for a wide range of implementations with different accumulation policies, eg, eager-computation vs. lazycomputation. The best implementation can thus be selected based on a given application and the target platform that it will execute on. We have integrated finish accumulators into the Habanero-Java task parallel language, and used them in both research and teaching. In addition to their higherlevel semantics, experimental results demonstrate that our Java-based implementation of finish accumulators delivers comparable or better performance for reductions relative to Java\u2019s atomic variables and concurrent collection libraries.", "num_citations": "4\n", "authors": ["254"]}
{"title": "COMP 322: Principles of Parallel Programming\n", "abstract": " > Transformation between viewpoint; time vs space; 2D vs 3D> Data parallel algorithms image and volume processing> Computational geometry surfaces and modeling> Image-based modeling and rendering> Global illumination and lighting> Ray tracing/Ray casting> Visualization of output> Advanced rendering> Advanced graphics> Image processing> Video processing> Rich media> High definition", "num_citations": "4\n", "authors": ["254"]}
{"title": "Program analysis for safety guarantees in a Java virtual machine written in Java\n", "abstract": " In this paper, we report on our experiences with guaranteeing GC-pointer safety when using unsafe low-level language extensions to implement a JVM in Java. We give an overview of the original unsafe language extensions that were defined for use by Jalape\u00f1o implementers, and introduce sanitized replacements that capture common idioms while also guaranteeing GC-pointer safety. We also outline some simple static and dynamic checks for correct usage of low-level operations, and examine how code containing low-level operations can be optimized correctly and effectively.", "num_citations": "4\n", "authors": ["254"]}
{"title": "Optimized execution of fortran 90 array language on symmetric shared-memory multiprocessors\n", "abstract": " Past compilers have found it challenging to implement Fortran 90 array language on symmetric shared-memory multiprocessors (SMPs) so as to match, let alone beat, the performance of comparable Fortran 77 scalar loops. This is in spite of the fact that the semantics of array language is implicitly concurrent and the semantics of scalar loops is implicitly sequential. A well known obstacle to efficient execution of array language lies in the overhead of using array temporaries to obey the fetch-before-store semantics of array language. We observe that another major obstacle to supporting array language efficiently arises from the fact that most past compilers attempted to compile and optimize each array statement in isolation.               In this paper, we describe a solution for optimized compilation of Fortran 90 array language for execution on SMPs. Our solution optimizes scalarized loops and scalar loops in a\u00a0\u2026", "num_citations": "4\n", "authors": ["254"]}
{"title": "Loop transformations for hierarchical parallelism and locality\n", "abstract": " The increasing depth of memory and parallelism hierarchies in future scalable computer systems poses many challenges to parallelizing compilers. In this paper, we address the problem of selecting and implementing iteration-reordering loop transformations for hierarchical parallelism and locality. We present a two-pass algorithm for selecting sequences of Block, Unimodular, Parallel, and Coalesce transformations for optimizing locality and parallelism for a specified parallelism hierarchy model. These general transformation sequences are implemented using a framework for iteration-reordering loop transformations that we developed in past work [15].", "num_citations": "4\n", "authors": ["254"]}
{"title": "A one year retrospective on a mooc in parallel, concurrent, and distributed programming in java\n", "abstract": " Much progress has been made on integrating parallel programming into the core Computer Science curriculum of top-tier universities in the United States. For example, \"COMP 322: Introduction to Parallel Programming\" at Rice University is a required course for all undergraduate students pursuing a bachelors degree. It teaches a wide range of parallel programming paradigms, from task-parallel to SPMD to actor-based programming. However, courses like COMP 322 do little to support members of the Computer Science community that need to develop these skills but who are not currently enrolled in a four-year program with parallel programming in the curriculum. This group includes (1) working professionals, (2) students at USA universities without parallel programming courses, or (3) students in countries other than the USA without access to a parallel programming course. To serve these groups, Rice\u00a0\u2026", "num_citations": "3\n", "authors": ["254"]}
{"title": "A unified runtime for pgas and event-driven programming\n", "abstract": " A well-recognized characteristic of extreme scale systems is that their computation bandwidths far exceed their communication bandwidths. PGAS runtimes have proven to be effective in enabling efficient use of communication bandwidth for many applications, due to their efficient support for short non- blocking one-sided messages. However, they were not designed for exploiting the massive levels of intra-node parallelism found in extreme scale systems. Thus, a key question in the use of PGAS runtimes on extreme scale platforms is: what kind of node-level runtime system should be integrated with PGAS runtimes? In this paper, we explore the premise that event-driven intra- node runtimes could be promising candidates for integration with PGAS runtimes, due to their ability to overlap useful computation with background long-latency operations. For this exploration, we use OpenSHMEM as an exemplar of PGAS\u00a0\u2026", "num_citations": "3\n", "authors": ["254"]}
{"title": "FY18 ASC P&EM L2 Milestone 6362: Local Failure Local Recovery (LFLR) Resiliency for Asynchronous Many Task (AMT) Programming and Execution Models: Executive Summary.\n", "abstract": " The overall goal of this work was to perform an in-depth analysis of resilience schemes adapted to the Asynchronous Many-Task (AMT) programming and execution model with the goal of informing the Sandia Advanced Simulation and Computing (ASC) program's application development strategy for next generation platforms (NGPs).", "num_citations": "3\n", "authors": ["254"]}
{"title": "Using dynamic compilation to achieve ninja performance for cnn training on many-core processors\n", "abstract": " Convolutional Neural Networks (CNNs) represent a class of Deep Neural Networks that is growing in importance due to their state-of-the-art performance in pattern recognition tasks in various domains, including image recognition, speech recognition, and natural language processing. However, CNNs are very time consuming to train due to the computationally intensive nature of their convolution operations. Typically, a convolution operation is exposed as a library API that duplicates and reorganizes input tensors under-the-hood in order to leverage existing matrix-matrix multiplication (GEMM) BLAS routines. Unfortunately, this widely-used approach suffers not only from memory expansion but also from memory bandwidth limitations. Moreover, although there has been a significant amount of past work on optimizing CNNs on GPUs, those approaches are not directly applicable to many-core CPU\u00a0\u2026", "num_citations": "3\n", "authors": ["254"]}
{"title": "Gt-race: graph traversal based data race detection for asynchronous many-task parallelism\n", "abstract": " Asynchronous Many-Task (AMT) parallelism is growing in popularity because of its promise to support future platforms with new heterogeneity and resiliency requirements. It supports the construction of parallel programs with fine-grained tasks, thereby enabling portability across a wide range of platforms. However, applications written for AMT parallelism still remain vulnerable to data races, and existing data race detection tools are unsuitable for AMT programs because they either incur intractably large overheads or are limited to restricted task structures such as fork-join parallelism.                 In this paper, we propose GT-Race, a new graph-traversal based data race detector for AMT parallelism. It leverages the computation graph data structure, which encodes the general happens-before structures in AMT programs. After introducing a baseline algorithm for data race detection, we propose key\u00a0\u2026", "num_citations": "3\n", "authors": ["254"]}
{"title": "Static cost estimation for data layout selection on GPUs\n", "abstract": " Performance modeling provides mathematical models and quantitative analysis for designing and optimizing computer systems. In high performance architectures, high-latency memory accesses often dominate execution time in many classes of applications. Thus, performance modeling for memory accesses of high performance architectures has been an important research topic. In high performance computation, data layout can significantly affect the efficiency of memory access operations. In recent years, the problem of data layout selection has been well studied on various parallel CPU and some GPU architectures. GPUs have memory hierarchies different from multi-core CPUs. While data layout selection on GPUs has been inspected by several existing projects, there is still a lack of a mathematical cost model for data layout selection on GPUs. This motivates us to investigate static cost analysis methods that\u00a0\u2026", "num_citations": "3\n", "authors": ["254"]}
{"title": "Brief announcement: dynamic determinacy race detection for task parallelism with futures\n", "abstract": " Existing dynamic determinacy race detectors for task-parallel programs are limited to programs with strict computation graphs, where a task can only wait for its descendant tasks to complete. In this paper, we present the first known determinacy race detector for non-strict computation graphs with futures. The space and time complexity of our algorithm are similar to those of the classical SP-bags algorithm, when using only structured parallel constructs such as spawn-sync and async-finish. In the presence of point-to-point synchronization using futures, the complexity of the algorithm increases by a factor determined by the number of future operations, which includes future task creation and future get operations. The experimental results show that the slowdown factor observed for our algorithm relative to the sequential version is in the range of 1.00 x to 9.92 x, which is very much in line with slowdowns experienced\u00a0\u2026", "num_citations": "3\n", "authors": ["254"]}
{"title": "Model Checking Task Parallel Programs Using Gradual Permissions (N)\n", "abstract": " Habanero is a task parallel programming model that provides correctness guarantees to the programmer. Even so, programs may contain data races that lead to non-determinism, which complicates debugging and verification. This paper presents a sound algorithm based on permission regions to prove data race and deadlock freedom in Habanero programs. Permission regions are user annotations to indicate the use of shared variables over spans of code. The verification algorithm restricts scheduling to permission region boundaries and isolation to reduce verification cost. The effectiveness of the algorithm is shown in benchmarks with an implementation in the Java Pathfinder (JPF) model checker. The implementation uses a verification specific library for Habanero that is tested using JPF for correctness. The results show significant reductions in cost, where cost is controlled with the size of the permission\u00a0\u2026", "num_citations": "3\n", "authors": ["254"]}
{"title": "JPF verification of Habanero Java programs using gradual type permission regions\n", "abstract": " The Habanero Java Library (HJ-lib) is a Java 8 library implementation of the Habanero Java (HJ) programming model. Calls into this pure Java library provide support for all HJ primitives, including async, finish, and phasers. In previous work, we presented VR, a custom verification run-time designed to be used within Java Pathfinder (JPF) to verify a subset of HJ programs. In this work, we present VR-lib, a library implementation of HJ, which supports verification of a larger subset of programs than VR. Additionally, we present the implementation of gradually typed permission regions (GPRs). PRs provide a building block for dynamically detecting violations of conditions sufficient to guarantee race-freedom. Lastly, we present results for benchmarks using PRs in combination with VR-lib to verify HJ programs.", "num_citations": "3\n", "authors": ["254"]}
{"title": "X10\u2014new opportunities for compilerdriven performance via a new programming model\n", "abstract": " Language and Virtual Machine Challenges for Large-scale Parallel Systems Page 1 X10: New opportunities for Compiler-Driven Performance via a new Programming Model X10: New opportunities for Compiler-Driven Performance via a new Programming Model Kemal Ebcioglu Vijay Saraswat Vivek Sarkar IBM TJ Watson Research Center {kemal,vsaraswat,vsarkar}@us.ibm.com Compiler-Driven Performance Workshop --- CASCON 2004 Oct 6, 2004 This work has been supported in part by the Defense Advanced Research Projects Agency (DARPA) under contract No. NBCH30390004. Kemal Ebcioglu Vijay Saraswat Vivek Sarkar IBM TJ Watson Research Center {kemal,vsaraswat,vsarkar}@us.ibm.com Compiler-Driven Performance Workshop --- CASCON 2004 Oct 6, 2004 This work has been supported in part by the Defense Advanced Research Projects Agency (DARPA) under contract No. NBCH30390004. 2 (\u2026", "num_citations": "3\n", "authors": ["254"]}
{"title": "Enhanced parallelization via analyses and transformations on Array SSA form\n", "abstract": " Array SSA form is a version of SSA form that captures precise element-level data flow information for array variables. As an example of program analysis using Array SSA form, we presented a conditional constant propagation algorithm that can lead to discovery of a larger set of constants than previous algorithms that analyze only scalar variables [7]. As an example of program transformation using Array SSA form, we showed that making its elementlevel functions manifest at run-time can increase the set of parallelizable loops [6].In this paper, we extend our past work by showing how Array SSA form can be used to perform analyses and transformations that go beyond loop parallelization. These transformations (which include region/task parallelism, pipeline parallelism, speculation, and general forms of code reordering) rely on two important properties of Array SSA form. First, because the operation is truly functional in Array SSA form, it can be subjected to classical optimizations and transformations (eg, copy propagation, reassociation, code reordering) like other operations. Second, Array SSA form, like scalar SSA form, creates a factoring of reaching definitions. However, in the array case this factoring can be refined by using element-level information. Specifically, we present a static analysis technique called resolution that reduces the number of definitions that appear to reach a use. Resolution is global in scope and is distinct from traditional dependence analysis which focuses on array references in loops.", "num_citations": "3\n", "authors": ["254"]}
{"title": "Anticipatory instruction scheduling\n", "abstract": " We present the first provably optimal algorithm for a special case of anticipatory instruction scheduling for a trace of basic blocks on a machine with arbitrary size lookahead windows. We extend this result for the version of the problem in which a trace of basic blocks is cent ained wit hin a loop. In addition, we discuss how to modify these special-case optimal algorithms to obtain heuristics for the more general (but NP-hard) problems that occur in practice,", "num_citations": "3\n", "authors": ["254"]}
{"title": "Is there a future for functional languages in parallel programming?\n", "abstract": " This panel provides an outlook for the role of functional languages in parallel programming from the perspective of researchers who are actively involved in the design and implementation of parallel functional languages. The panel focuses on developments in parallel programming and functional programming and their interactions. The panelists discuss the impact of these developments on the future of parallel functional languages from the perspective of their research groups.< >", "num_citations": "3\n", "authors": ["254"]}
{"title": "An ownership policy and deadlock detector for promises\n", "abstract": " Task-parallel programs often enjoy deadlock freedom under certain restrictions, such as the use of structured join operations, as in Cilk and X10, or the use of asynchronous task futures together with deadlock-avoiding policies such as Known Joins or Transitive Joins. However, the promise, a popular synchronization primitive for parallel tasks, does not enjoy deadlock-freedom guarantees. Promises can exhibit deadlock-like bugs; however, the concept of a deadlock is not currently well-defined for promises.", "num_citations": "2\n", "authors": ["254"]}
{"title": "Towards Chapel-based Exascale Tree Search Algorithms: dealing with multiple GPU accelerators\n", "abstract": " Tree-based search algorithms applied to combinatorial optimization problems are highly irregular and time consuming when it comes to solving big instances. Solving such instances efficiently requires the use of massively parallel distributed-memory supercomputers. According to recent Top 500 trends, the degree of parallelism in these supercomputers continues to increase in size and complexity, with millions of heterogeneous (mainly CPU-GPU) cores. Harnessing this scale of computing resources raises at least three challenging issues which are described and addressed in this paper. Indeed, as a step towards exascale computing, we revisit the design and implementation of tree search algorithms dealing with multiple GPUs, in addition to scalability and productivity-awareness using Chapel. The proposed algorithm exploits Chapel's distributed iterators by combining a partial search strategy with pre-compiled CUDA kernels for more efficient exploitation of the intra-node parallelism. Extensive experimentation on big N-Queens problem instances using 24 GPUs shows that up to 90% of the linear speedup can be achieved.", "num_citations": "2\n", "authors": ["254"]}
{"title": "Intrepydd: performance, productivity, and portability for data science application kernels\n", "abstract": " Major simultaneous disruptions are currently under way in both hardware and software. In hardware,``extreme heterogeneity''has become critical to sustaining cost and performance improvements after Moore's Law, but poses productivity and portability challenges for developers. In software, the rise of large-scale data science is driven by developers who come from diverse backgrounds and, moreover, who demand the rapid prototyping and interactive-notebook capabilities of high-productivity languages like Python.", "num_citations": "2\n", "authors": ["254"]}
{"title": "Integrating inter-node communication with a resilient asynchronous many-task runtime system\n", "abstract": " Achieving fault tolerance is one of the significant challenges of exascale computing due to projected increases in soft/transient failures. While past work on software-based resilience techniques typically focused on traditional bulk-synchronous parallel programming models, we believe that Asynchronous Many-Task (AMT) programming models are better suited to enabling resiliency since they provide explicit abstractions of data and tasks which contribute to increased asynchrony and latency tolerance. In this paper, we extend our past work on enabling application-level resilience in single node AMT programs by integrating the capability to perform asynchronous MPI communication, thereby enabling resiliency across multiple nodes. We also enable resilience against fail-stop errors where our runtime will manage all re-execution of tasks and communication without user intervention. Our results show that we are\u00a0\u2026", "num_citations": "2\n", "authors": ["254"]}
{"title": "OmpMemOpt: Optimized Memory Movement for Heterogeneous Computing\n", "abstract": " The fast development of acceleration architectures and applications has made heterogeneous computing the norm for high-performance computing. The cost of high volume data movement to the accelerators is an important bottleneck both in terms of application performance and developer productivity. Memory management is still a manual task performed tediously by expert programmers. In this paper, we develop a compiler analysis to automate memory management for heterogeneous computing. We propose an optimization framework that casts the problem of detection and removal of redundant data movements into a partial redundancy elimination (PRE) problem and applies the lazy code motion technique to optimize these data movements. We chose OpenMP as the underlying parallel programming model and implemented our optimization framework in the LLVM toolchain. We evaluated it with ten\u00a0\u2026", "num_citations": "2\n", "authors": ["254"]}
{"title": "MISIM: A Novel Code Similarity System\n", "abstract": " Code similarity systems are integral to a range of applications from code recommendation to automated software defect correction. We argue that code similarity is now a first-order problem that must be solved. To begin to address this, we present machine Inferred Code Similarity (MISIM), a novel end-to-end code similarity system that consists of two core components. First, MISIM uses a novel context-aware semantic structure, which is designed to aid in lifting semantic meaning from code syntax. Second, MISIM provides a neural-based code similarity scoring algorithm, which can be implemented with various neural network architectures with learned parameters. We compare MISIM to three state-of-the-art code similarity systems: (i)code2vec, (ii)Neural Code Comprehension, and (iii)Aroma. In our experimental evaluation across 328,155 programs (over 18 million lines of code), MISIM has 1.5x to 43.4x better accuracy than all three systems.", "num_citations": "2\n", "authors": ["254"]}
{"title": "Exploring a multi-resolution GPU programming model for Chapel\n", "abstract": " There is a growing need to support accelerators, especially GPU accelerators, since they are a common source of performance improvement in HPC clusters. As for GPU programming with Chapel, typically programmers first start with writing forall loops and run these loops on CPUs as a proof-of-concept. If the resulting CPU performance is not sufficient for their needs, their next step could be to try the automatic compiler-based GPU code generation techniques [1], [2]. For portions that remain as performance bottlenecks, even after automatic compilation approaches, the next step is to consider writing GPU kernels using CUDA/HIP/OpenCL and invoking these kernels from the Chapel program using the GPUIterator [3], [4] and Chapel's C interoperability feature.", "num_citations": "2\n", "authors": ["254"]}
{"title": "Understanding Reuse, Performance, and Hardware Cost of DNN Dataflows: A Data-Centric Approach Using MAESTRO\n", "abstract": " SettingUnderstanding Reuse, Performance, and Hardware Cost of DNN Dataflows: A Data-Centric Approach Using MAESTRO", "num_citations": "2\n", "authors": ["254"]}
{"title": "Experimental Insights from the Rogues Gallery\n", "abstract": " The Rogues Gallery is a new deployment for understanding next-generation hardware with a focus on unorthodox and uncommon technologies. This testbed project was initiated in 2017 in response to Rebooting Computing efforts and initiatives. The Gallery's focus is to acquire new and unique hardware (the rogues) from vendors, research labs, and start-ups and to make this hardware widely available to students, faculty, and industry collaborators within a managed data center environment. By exposing students and researchers to this set of unique hardware, we hope to foster cross-cutting discussions about hardware designs that will drive future performance improvements in computing long after the Moore's Law era of cheap transistors ends. We have defined an initial vision of the infrastructure and driving engineering challenges for such a testbed in a separate document, so here we present highlights of the\u00a0\u2026", "num_citations": "2\n", "authors": ["254"]}
{"title": "Valence: variable length calling context encoding\n", "abstract": " Many applications, including program optimizations, debugging tools, and event loggers, rely on calling context to gain additional insight about how a program behaves during execution. One common strategy for determining calling contexts is to use compiler instrumentation at each function call site and return sites to encode the call paths and store them in a designated area of memory. While recent works have shown that this approach can generate precise calling context encodings with low overhead, the encodings can grow to hundreds or even thousands of bytes to encode a long call path, for some applications. Such lengthy encodings increase the costs associated with storing, detecting, and decoding call path contexts, and can limit the effectiveness of this approach for many usage scenarios.", "num_citations": "2\n", "authors": ["254"]}
{"title": "Integrating data layout transformations with the polyhedral model\n", "abstract": " Integrating Data Layout Transformations with the Polyhedral Model Page 1 Integrating Data Layout Transformations with the Polyhedral Model IMPACT 2019 January 23rd, 2019 Jun Shirako and Vivek Sarkar Georgia Institute of Technology Page 2 2 Loop Transformations \u2022 Change the statement order of program (ie, loop structures) \u2022 Impact on temporal/spatial locality and parallelism \u2022 Use dependence analysis to identify legal transformations \u2022 Best loop transformation depend on hardware and data layout \u2022 Large body of work since 1980\u2019s, including \u2022 AST-based loop transformations \u2022 Loop fusion/distribution, permutation, skewing, tiling, and etc. \u2022 Sequence of individual transformations applied to AST \u2022 Polyhedral transformations \u2022 Linear algebraic framework to generalize loop transformations \u2022 Unified and formalized as affine scheduling problems Page 3 Loop Transformations 3 // Input for (i = 0; i < ni; i++) for (j = 0; [\u2026", "num_citations": "2\n", "authors": ["254"]}
{"title": "A Unified Approach to Variable Renaming for Enhanced Vectorization\n", "abstract": " Despite the fact that compiler technologies for automatic vectorization have been under development for over four decades, there are still considerable gaps in the capabilities of modern compilers to perform automatic vectorization for SIMD units. One such gap can be found in the handling of loops with dependence cycles that involve memory-based anti (write-after-read) and output (write-after-write) dependences. Past approaches, such as variable renaming and variable expansion, break such dependence cycles by either eliminating or repositioning the problematic memory-based dependences. However, the past work suffers from three key limitations: (1) Lack of a unified framework that synergistically integrates multiple storage transformations, (2) Lack of support for bounding the additional space required to break memory-based dependences, and (3) Lack of support for integrating these storage transformations with other code\u00a0\u2026", "num_citations": "2\n", "authors": ["254"]}
{"title": "SHCOLL-A Standalone Implementation of OpenSHMEM-Style Collectives API\n", "abstract": " The performance of collective operations has a large impact on overall performance in many HPC applications. Implementing multiple algorithms and selecting optimal one depending on message size and the number of processes involved in the operation is essential to achieve good performance. In this paper, we will present SHCOLL, a collective routines library that was developed on top of OpenSHMEM API point to point operations: puts, gets, atomic memory update, and memory synchronization routines. The library is designed to serve as a plug-in\u00a0to OpenSHMEM implementations and will be used by the OSSS OpenSHMEM reference implementation to support OpenSHMEM collective operations. In this paper, we describe the algorithms that have been incorporated in the implementation of each OpenSHMEM API collective routine and evaluate them on a Cray XC30 system. For long messages\u00a0\u2026", "num_citations": "2\n", "authors": ["254"]}
{"title": "Using CAASCADE and CrayPAT for analysis of HPC applications\n", "abstract": " We describe our work on integrating CAASCADE with CrayPAT to obtain both static and dynamic information on characteristics of high-performance computing (HPC) applications. CAASCADE---Compiler-Assisted Application Source Code Analysis and Database---is a system we are developing to extract features of application from its source code by utilizing compiler plugins. CrayPAT enable us to add runtime-based information to CAASCADE's feature detection. We present results from analysis of HPC applications.", "num_citations": "2\n", "authors": ["254"]}
{"title": "Dammp: A distributed actor model for mobile platforms\n", "abstract": " While mobile computing has seen a trend towards miniaturization and energy savings for a number of years, the available hardware parallelism in mobile devices has at the same time continued to increase. Overall, mobile devices remain resource constrained on power consumption and thermal dissipation. Aggregating the computing capabilities of multiple mobile devices in a distributed and dynamic setting, opens the possibilities for performance improvements, longer aggregate battery life and novel dynamic and distributed applications.", "num_citations": "2\n", "authors": ["254"]}
{"title": "Formalization of phase ordering\n", "abstract": " Phasers pose an interesting synchronization mechanism that generalizes many collective synchronization patterns seen in parallel programming languages, including barriers, clocks, and point-to-point synchronization using latches or semaphores. This work characterizes scheduling constraints on phaser operations, by relating the execution state of two tasks that operate on the same phaser. We propose a formalization of Habanero phasers, May-Happen-In-Parallel, and Happens-Before relations for phaser operations, and show that these relations conform with the semantics. Our formalization and proofs are fully mechanized using the Coq proof assistant, and are available online.", "num_citations": "2\n", "authors": ["254"]}
{"title": "Extending Polyhedral Model for Analysis and Transformation of OpenMP Programs\n", "abstract": " The polyhedral model is a powerful algebraic framework that has enabled significant advances in analysis and transformation of sequential affine (sub)programs, relative to traditional AST-based approaches. However, given the rapid growth of parallel software, there is a need for increased attention to using polyhedral compilation techniques to analyze and transform explicitly parallel programs. In our PACT'15 paper titled \"Polyhedral Optimizations of Explicitly Parallel Programs\" [1, 2], we addressed the problem of analyzing and transforming programs with explicit parallelism that satisfy the serial-elision property, i.e., the property that removal of all parallel constructs results in a sequential program that is a valid (albeit inefficient) implementation of the parallel program semantics.In this poster, we address the problem of analyzing and transforming more general OpenMP programs that do not satisfy the serial-elision\u00a0\u2026", "num_citations": "2\n", "authors": ["254"]}
{"title": "HJ-Viz: a new tool for visualizing, debugging and optimizing parallel programs\n", "abstract": " The proliferation of multicore processors warrants parallelism as the future of computing, increasing the demand to write parallel programs for increased application performance. Previous experience has shown that writing explicitly parallel programs is inherently more difficult than writing sequential programs. Programmers need parallel programming models, constructs, and tools that can simplify writing of parallel programs. In this poster, we present an innovative new tool, HJ-Viz, which generates interactive Computation Graphs (CGs) of parallel programs by analyzing event logs. The visual feedback is valuable for a programmer to efficiently optimize program logic and to eliminate the presence of potential bugs which may otherwise be difficult to detect. For example, in cases of deadlocks, HJ-Viz enables users to visualize and easily diagnose the deadlock scenario.", "num_citations": "2\n", "authors": ["254"]}
{"title": "DOE Advanced Scientific Advisory Committee (ASCAC): Workforce Subcommittee Letter\n", "abstract": " Simulation and computing are essential to much of the research conducted at the DOE national laboratories. Experts in the ASCR\u00ac relevant Computing Sciences, which encompass a range of disciplines including Computer Science, Applied Mathematics, Statistics and domain Computational Sciences, are an essential element of the workforce in nearly all of the DOE national laboratories. This report seeks to identify the gaps and challenges facing DOE with respect to this workforce. This letter is ASCAC\u2019s response to the charge of February 19, 2014 to identify disciplines in which significantly greater emphasis in workforce training at the graduate or postdoctoral levels is necessary to address workforce gaps in current and future Office of Science mission needs.", "num_citations": "2\n", "authors": ["254"]}
{"title": "Degas: Dynamic exascale global address space\n", "abstract": " Approach:\u201cRethink\u201d algorithms to optimize for data movement\u2022 New class of communication-\u2010optimal algorithms\u2022 Most codes are not bandwidth limited, but many should be Challenges: How general are these algorithms?\u2022 Can they be automated and for what types of loops?\u2022 How much benefit is there in practice?", "num_citations": "2\n", "authors": ["254"]}
{"title": "Scnc: Efficient unification of streaming with dynamic task parallelism\n", "abstract": " Stream processing is a special form of the dataflow execution model that offers extensive opportunities for optimization and automatic parallelization. To take full advantage of the paradigm, however, typically requires programmers to learn a new language and re-implement their applications. This work shows that it is possible to exploit streaming as a safe and automatic optimization of a more general dataflow-based model-one in which computation kernels are written in standard, general-purpose languages and organized as a coordination graph. We propose Streaming Concurrent Collections (SCnC), a streaming system that can efficiently run a subset of programs supported by Concurrent Collections (CnC). CnC is a general purpose parallel programming paradigm with a task-parallel look and feel but based on dataflow graph principles. Its expressivity extends to any arbitrary task graph. Integration of these\u00a0\u2026", "num_citations": "2\n", "authors": ["254"]}
{"title": "Languages and Compilers for Parallel Computing: 23rd International Workshop, LCPC 2010, Houston, TX, USA, October 7-9, 2010. Revised Selected Papers\n", "abstract": " This book constitutes the thoroughly refereed post-proceedings of the 23rd International Workshop on Languages and Compilers for Parallel Computing, LCPC 2010, held in Houston, TX, USA, in October 2010. The 18 revised full papers presented were carefully reviewed and selected from 47 submissions. The scope of the workshop spans foundational results and practical experience, and targets all classes of parallel platforms including concurrent, multithreaded, multicore, accelerated, multiprocessor, and cluster systems", "num_citations": "2\n", "authors": ["254"]}
{"title": "Programmability issues\n", "abstract": " Programming models are at the very center of our effort to address the exascale challenge. They are the key interface that will allow the separation of the programmers\u2019 concerns from those of system designers, potentially at different levels of granularity. Any such model must meet the extensive needs of application developers and be supported by the entire software stack. Considerable research is needed to define and implement the programming and execution models for such systems. Whereas evolutionary approaches may best support the migration of existing application software, revolutionary models may be best suited to providing extreme-scale performance for new applications on emerging architectures. Both approaches should be explored.", "num_citations": "2\n", "authors": ["254"]}
{"title": "Optimization of array accesses by collective loop transformations\n", "abstract": " In this paper, we investigate the problem of optimizing array accesses across a collection of loops. We demonstrate that a good solution to such a problem should be based on an optimization scheme, called collective loop transformations, that considers all loops simultaneously. In particular, loop reversal, loop interchange and loop fusion are performed collectively on a set of loop nests. The main impact of these transformations is an optimization called array contraction, that saves space and time by converting an array variable into a scalar variable or a bu er containing a small number of scalar variables.This optimization is applicable to general-purpose high-performance architectures. For a multiprocessor architecture, array contraction is performed by executing the producer and consumer loops on separate processors, and by using a smaller bu er for the array communication. For a uniprocessor architecture, array contraction is performed by fusing the producer and consumer loops into a single loop, and using scalar variables for the array communication. In both architectures, our goal is to recon gure the loops so as to maximize the number of arrays that bene t from array contraction.", "num_citations": "2\n", "authors": ["254"]}
{"title": "Reverse synthesis compilation for architectural research\n", "abstract": " This paper discusses the development of compilation strategies for DEL architectures and tools to assist in the evaluation of their efficiency. Compilation is divided into a series of independent simpler problems. To explore optimization of code for DEL compilers, two intermediate representations are employed. One of these representations is at a lower level than target machine instructions. Machine-independent optimization is performed on this intermediate representation. The other interrncdiate representation has been specifically designed for compiler retargelability. It is at a higher level than the target machine. Target code generation is performed by reverse synthesis followed by atlribuledparsing. This technique demonstrates the feasibility of using automated table-driven code generation techniques for infexibZe architectures.", "num_citations": "2\n", "authors": ["254"]}
{"title": "OpenMP application experiences: porting to accelerated nodes\n", "abstract": " As recent enhancements to the OpenMP specification become available in its implementations, there is a need to share the results of experimentation in order to better understand the OpenMP implementation\u2019s behavior in practice, to identify pitfalls, and to learn how the implementations can be effectively deployed in scientific codes. We report on experiences gained and practices adopted when using OpenMP to port a variety of ECP applications, mini-apps and libraries based on different computational motifs to accelerator-based leadership-class high-performance supercomputer systems at the United States Department of Energy. Additionally, we identify important challenges and open problems related to the deployment of OpenMP. Through our report of experiences, we find that OpenMP implementations are successful on current supercomputing platforms and that OpenMP is a promising programming model\u00a0\u2026", "num_citations": "1\n", "authors": ["254"]}
{"title": "Task-graph scheduling extensions for efficient synchronization and communication\n", "abstract": " Task graphs have been studied for decades as a foundation for scheduling irregular parallel applications and incorporated in many programming models including OpenMP. While many high-performance parallel libraries are based on task graphs, they also have additional scheduling requirements, such as synchronization within inner levels of data parallelism and internal blocking communications. In this paper, we extend task-graph scheduling to support efficient synchronization and communication within tasks. Compared to past work, our scheduler avoids deadlock and oversubscription of worker threads, and refines victim selection to increase the overlap of sibling tasks. To the best of our knowledge, our approach is the first to combine gang-scheduling and work-stealing in a single runtime. Our approach has been evaluated on the SLATE high-performance linear algebra library. Relative to the LLVM OMP\u00a0\u2026", "num_citations": "1\n", "authors": ["254"]}
{"title": "ARBALEST: Dynamic Detection of Data Mapping Issues in Heterogeneous OpenMP Applications\n", "abstract": " From OpenMP 4.0 onwards, programmers can offload code regions to accelerators by using the target offloading feature. However, incorrect usage of target offloading constructs may incur data mapping issues. A data mapping issue occurs when the host fails to observe updates on the accelerator or vice versa. It may further lead to multiple memory issues such as use of uninitialized memory, use of stale data, and data race. To the best of our knowledge, currently there is no prior work on dynamic detection of data mapping issues in heterogeneous OpenMP applications.In this paper, we identify possible root causes of data mapping issues in OpenMP\u2019s standard memory model and the unified memory model. We find that data mapping issues primarily result from incorrect settings of map and nowait clauses in target offloading constructs. Further, the novel unified memory model introduced in OpenMP 5.0 cannot\u00a0\u2026", "num_citations": "1\n", "authors": ["254"]}
{"title": "Vyasa: A high-performance vectorizing compiler for tensor convolutions on the Xilinx AI Engine\n", "abstract": " Xilinx's AI Engine is a recent industry example of energy-efficient vector processing that includes novel support for 2D SIMD datapaths and shuffle interconnection network. The current approach to programming the AI Engine relies on a C/C++ API for vector intrinsics. While an advance over assembly-level programming, it requires the programmer to specify a number of low-level operations based on detailed knowledge of the hardware. To address these challenges, we introduce Vyasa, a new programming system that extends the Halide DSL compiler to automatically generate code for the AI Engine. We evaluated Vyasa on 36 CONV2D workloads, and achieved geometric means of 7.6 and 24.2 MACs/cycle for 32-bit and 16-bit operands (which represent 95.9% and 75.6% of the peak performance respectively).", "num_citations": "1\n", "authors": ["254"]}
{"title": "Context-Aware Parse Trees\n", "abstract": " The simplified parse tree (SPT) presented in Aroma, a state-of-the-art code recommendation system, is a tree-structured representation used to infer code semantics by capturing program \\emph{structure} rather than program \\emph{syntax}. This is a departure from the classical abstract syntax tree, which is principally driven by programming language syntax. While we believe a semantics-driven representation is desirable, the specifics of an SPT's construction can impact its performance. We analyze these nuances and present a new tree structure, heavily influenced by Aroma's SPT, called a \\emph{context-aware parse tree} (CAPT). CAPT enhances SPT by providing a richer level of semantic representation. Specifically, CAPT provides additional binding support for language-specific techniques for adding semantically-salient features, and language-agnostic techniques for removing syntactically-present but semantically-irrelevant features. Our research quantitatively demonstrates the value of our proposed semantically-salient features, enabling a specific CAPT configuration to be 39\\% more accurate than SPT across the 48,610 programs we analyzed.", "num_citations": "1\n", "authors": ["254"]}
{"title": "High Performance Multilevel Graph Partitioning on GPU\n", "abstract": " Graph partitioning is a common computational phase in many application domains, including social network analysis, data mining, scheduling, and VLSI design. The significant SIMT compute power of a GPU makes it an appropriate platform to exploit data parallelism in graph partitioning and accelerate the computation. However, irregular, non-uniform, and data-dependent graph partitioning sub-tasks pose multiple challenges for efficient GPU utilization. Some of these challenges include load imbalance, non-coalesced memory accesses, and warp execution inefficiency. In this paper, we describe an effective and methodological approach to enable multi-level graph partitioning on GPUs. Our solution avoids thread divergence and balances the load over GPU threads by dynamically assigning appropriate number of threads to process the graph vertices and their irregular sized neighbors. Our design is autonomous\u00a0\u2026", "num_citations": "1\n", "authors": ["254"]}
{"title": "MiniApp for Density Matrix Renormalization Group Hamiltonian Application Kernel\n", "abstract": " We present two miniapps that implement the core computational kernel of the DMRG++ application, a generic C++ code that implements the Density Matrix Renormalization Group (DMRG) algorithm. The DMRG++ core Kronecker multiplication kernel is formulated using a batched BLAS approach, with implementation that targets both multi-core CPUs using OpenMP and GPGPU using the MAGMA library. The kernel evaluates the matrix-vector multiplication of the target Hamiltonian matrix used in Lanczos algorithm for computing the lowest eigenvalue and eigenvector. The Hamiltonian matrix is expressed compactly as sums of Kronecker products of small dense matrices. We demonstrate improved performance of the miniapp on synthetic problem, and show the performance of the DMRG++ application using a plugin based on the miniapp. We also present an OpenMP miniapp that explores the use of nested\u00a0\u2026", "num_citations": "1\n", "authors": ["254"]}
{"title": "HOOVER: Distributed, Flexible, and Scalable Streaming Graph Processing on OpenSHMEM\n", "abstract": " Many problems can benefit from being phrased as a graph processing or graph analytics problem: infectious disease modeling, insider threat detection, fraud prevention, social network analyis, and more. These problems all share a common property: the relationships between entitites in these systems are crucial to understanding the overall behavior of the systems themselves. However, relations are rarely if ever static. As our ability to collect information on those relations improve (e.g. on financial transactions in fraud prevention), the value added by large-scale, high-performance, dynamic/streaming (rather than static) graph analysis becomes significant.                 This paper introduces HOOVER, a distributed software framework for large-scale, dynamic graph modeling and analyis. HOOVER sits on top of OpenSHMEM, a PGAS programming system, and enables users to plug in application-specific logic\u00a0\u2026", "num_citations": "1\n", "authors": ["254"]}
{"title": "Compile-Time Library Call Detection Using CAASCADE and XALT\n", "abstract": " CAASCADE  \u2014 Compiler-Assisted Application Source Code Analysis and DatabasE\u2014is a tool that summarizes the use of parallel programming language features in application source code using compiler technology. This paper discusses the library detection capability within CAASCADE to find information about the usage of scientific libraries within the source code. The information that CAASCADE collects provides insights into the usage of library calls in an applications. CAASCADE can classify the APIs by scientific libraries (e.g. LAPACK, BLAS, FFTW, etc). It can also detect the context in which a library API is being invoked, for example within a serial or multi-threaded region. To collect this information, CAASCADE uses compiler plugins that summarize procedural information and uses Apache Spark to do inter-procedural analysis to reconstruct call chains. In addition to this, we also integrated\u00a0\u2026", "num_citations": "1\n", "authors": ["254"]}
{"title": "Exploration of Supervised Machine Learning Techniques for Runtime Selection of CPU vs. GPU Execution in Java Programs\n", "abstract": " While multi-core CPUs and many-core GPUs are both viable platforms for parallel computing, programming models for them can impose large burdens upon programmers due to their complex and low-level APIs. Since managed languages like Java are designed to be run on multiple platforms, parallel language constructs and APIs such as Java 8 Parallel Stream APIs can enable high-level parallel programming with the promise of performance portability for mainstream (\u201cnon-ninja\u201d) programmers. To achieve this goal, it is important for the selection of the hardware device to be automated rather than be specified by the programmer, as is done in current programming models. Due to a variety of factors affecting performance, predicting a preferable device for faster performance of individual kernels remains a difficult problem. While a prior approach uses machine learning to address this challenge, there is\u00a0\u2026", "num_citations": "1\n", "authors": ["254"]}
{"title": "A marshalled data format for pointers in relocatable data blocks\n", "abstract": " As future computing hardware progresses towards extreme-scale technology, new challenges arise for addressing heterogeneous compute and memory resources, for providing application resilience in the presence of more frequent failures, and for working within strict energy constraints. While C++ has gained popularity in recent years within the HPC community, some concepts of object-oriented program design may be at odds with the techniques we use to address the challenges of extreme-scale computing. In this work, we focus on the challenges related to using aggregate data structures that include pointer values within a programming model where the runtime may frequently relocate data, and traditional serialization techniques are not practical. We propose and evaluate a marshalled encoding for relocatable data blocks, and present a C++ library and other tools to simplify the work of the application\u00a0\u2026", "num_citations": "1\n", "authors": ["254"]}
{"title": "Fine-grained parallelism in probabilistic parsing with Habanero Java\n", "abstract": " Structured prediction algorithms-used when applying machine learning to tasks like natural language parsing and image understanding-present some opportunities for fine-grained parallelism, but also have problem-specific serial dependencies. Most implementations exploit only simple opportunities such as parallel BLAS, or embarrassing parallelism over input examples. In this work we explore an orthogonal direction: using the fact that these algorithms can be described as specialized forward-chaining theorem provers [1], [2], and implementing fine-grained parallelization of the forward-chaining mechanism. We study context-free parsing as a simple canonical example, but the approach is more general.", "num_citations": "1\n", "authors": ["254"]}
{"title": "Exploiting Implicit Parallelism in Dynamic Array Programming Languages\n", "abstract": " We have built an interpreter for the array programming language J. The interpreter exploits implicit data parallelism in the language to achieve good parallel speedups on a variety of benchmark applications.", "num_citations": "1\n", "authors": ["254"]}
{"title": "BMS-CnC: Bounded memory scheduling of dynamic task graphs\n", "abstract": " It is now widely recognized that increased levels of parallelism is a necessary condition for improved application performance on multicore computers. However, as the number of cores increases, the memory-per-core ratio is expected to further decrease, making per-core memory efficiency of parallel programs an even more important concern in future systems. For many parallel applications, the memory requirements can be significantly larger than for their sequential counterparts and, more importantly, their memory utilization depends critically on the schedule used when running them. To address this problem we propose bounded memory scheduling (BMS) for parallel programs expressed as dynamic task graphs, in which an upper bound is imposed on the program\u2019s peak memory. Using the inspector/executor model, BMS tailors the set of allowable schedules to either guarantee that the program can be executed within the given memory bound, or throw an error during the inspector phase without running the computation if no feasible schedule can be found. Since solving BMS is NP-hard, we propose an approach in which we first use our heuristic algorithm, and if it fails we fall back on a more expensive optimal approach which is sped up by the best-effort result of the heuristic. Through evaluation on seven benchmarks, we show that BMS gracefully spans the spectrum between fully parallel and serial execution with decreasing memory bounds. Comparison with OpenMP shows that BMS-CnC can execute in 53% of the memory required by OpenMP while running at 90% (or more) of OpenMP\u2019s performance.", "num_citations": "1\n", "authors": ["254"]}
{"title": "Oil and Water can mix! Experiences with integrating Polyhedral and AST-based Transformations\n", "abstract": " The polyhedral model is an algebraic framework for affine program representations and transformations for enhancing locality and parallelism. Compared with traditional AST-based transformation frameworks, the polyhedral model can easily handle imperfectly nested loops and complex data dependences within and across loop nests in a unified framework. On the other hand, AST-based transformation frameworks for locality and parallelism have a long history that dates back to early vectorizing and parallelizing compilers. They can be used to efficiently perform a wide range of transformations including hierarchical parametric tiling, parallel reduction, scalar replacement and unroll-and-jam, and the implemented loop transformations are more compact (with smaller code size) than polyhedral frameworks. While many members of the polyhedral and AST-based transformation camps see the two frameworks as a mutually exclusive either-or choice, our experience has been that both frameworks can be integrated in a synergistic manner. In this paper, we present our early experiences with integrating polyhedral and AST-based transformations. Our preliminary experiments demonstrate the benefits of the proposed combined approach relative to Pluto, a pure polyhedral framework for locality and parallelism optimizations.", "num_citations": "1\n", "authors": ["254"]}
{"title": "CnC-Python: multicore programming with high productivity\n", "abstract": " We present CnC-Python (CP), an approach to implicit multicore parallelism based on Intel's Concurrent Collections model. CP enables programmers to achieve task, data and pipeline parallelism in a declarative fashion while only being required to describe the program as a coordination graph with serial Python code for individual nodes.", "num_citations": "1\n", "authors": ["254"]}
{"title": "The Platform-Aware Compilation Environment: Status and Future Directions\n", "abstract": " The Platform-Aware Compilation Environment (PACE) is an ambitious attempt to construct a portable compiler that produces code capable of achieving high levels of performance on new architectures. The key strategies in PACE are the design and development of an optimizer and runtime system that are parameterized by system characteristics, the automatic measurement of those characteristics, the extensive use of measured performance data to help drive optimization, and the use of machine learning to improve the long-term effectiveness of the compiler and runtime system.", "num_citations": "1\n", "authors": ["254"]}
{"title": "Habanero-java extensions for scientific computing\n", "abstract": " Mainstream object-oriented languages such as Java and C#, through the use of object-oriented abstractions and managed runtimes (virtual machines), have significantly improved productivity and portability in multiple application domains. However, despite many attempts in the past, the effect of these improvements on high-performance numeric computations has been limited. In this report, we describe the results and lessons learned from a one-year joint study between researchers in an industrial company (BHP Billiton) and an academic institution (Rice University) to port Dipole1D, an open source Fortran 90 application for 1D forward modeling of an arbitrarily located and oriented electric dipole transmitter, to Java with a goal of gaining efficient sequential and multicore implementations. Our primary conclusions from this study are as follows: 1) a standard library-based implementation of Fortran 90 primitives in\u00a0\u2026", "num_citations": "1\n", "authors": ["254"]}
{"title": "Establishing causality as a desideratum for memory models and transformations of parallel programs\n", "abstract": " In this paper, we establish a notion of causality that should be used as a desideratum for memory models and code transformations of parallel programs. We introduce a Causal Acyclic Consistency (CAC) model which is weak enough to allow various useful code transformations, yet still strong enough to prevent any execution that exhibits \u201ccausal cycles\u201d that may be caused by the Java Memory Model (JMM)[18].For memory models, we introduce a graph model called causality graph that can be used to analyze if a particular program execution violates causality. By using causality graph, we show that a popular memory model (such as the Java memory model) can lead to program executions that exhibit causality violations with respect to our notion of causality.", "num_citations": "1\n", "authors": ["254"]}
{"title": "Customizable domain-specific computing.\n", "abstract": " In this article, we introduce ongoing research in the newly established Center for Domain-Specific Computing (CDSC), which consists of twelve faculty members of diverse backgrounds from multiple disciplines, including computer science and engineering, electrical engineering, medicine, and applied mathematics, from four universities: UCLA (the lead institution), Rice, UC Santa Barbara, and Ohio State. The goal of this project is to look beyond parallelization and to focus on domain-specific customization as the next disruptive technology to bring orders-of-magnitude power-performance efficiency improvement to important application domains. CDSC is primarily funded by the National Science Foundation with an award from the 2009 Expeditions in Computing Program [1].", "num_citations": "1\n", "authors": ["254"]}
{"title": "Unconstrained static scheduling with communication weights\n", "abstract": " In this paper, we present some new theoretical results for unconstrained static scheduling with communication weights, i.e. multiprocessor scheduling of tasks with no precedence constraints, but with arbitrary communication and computation weights. The results are obtained for a cost function that extends completion time with a simple model of communication overhead. This cost function and its variants have been studied in past work. The main results of this paper are as follows: (1) it is shown that no single\u2010pass priority\u2010list algorithm can yield a constant performance bound for this cost function, (2) a two\u2010pass approach is proposed as a heuristic solution, (3) the two\u2010pass approach is shown to have a performance bound of (1 + \u03b4), where \u03b4 is the performance bound for the first step (scheduling on an unbounded number of processors), and (4) it is shown that no greedy\u2010merge clustering algorithm can deliver a\u00a0\u2026", "num_citations": "1\n", "authors": ["254"]}
{"title": "Reducing the overhead of compilation delay\n", "abstract": " The execution model for mobile dynamically-linked object--oriented programs has evolved from fast interpretation to a mix of interpreted and dynamically compiled execution.  The primary motivation for dynamic compilation is that compiled code executes significantly faster than interpreted code. However, since dynamic compilation is performed while the application is running, the biggest challenge in using dynamic compilation is to reduce its overhead so as not to mitigate the runtime improvement that it delivers. Techniques for reducing dynamic compilation overhead can be classified as (1) decreasing the amount of compilation performed, or (2) overlapping compilation with useful work. In this paper, we first evaluate the effectiveness of Lazy Compilation as a technique for decreasing the amount of compilation performed. In lazy compilation, individual methods are compiled on demand (when called), thus avoiding the load-time delay of compiling all methods when a new class/module is loaded.  Our experimental results (obtained by executing the specJVM Java programs on the Jalapeno JVM) show that lazy compilation results in compilation of 57% to 63% fewer methods, and a reduction in compilation time of approximately 30%, when compared to load-time compilation. Next, we present Profile-driven Background Compilation as a new technique for overlapping compilation with execution.  The motivation for background compilation is to use idle cycles in multiprocessor systems to overlap compilation with application execution.  Profile information is used to prioritize methods as candidates for background compilation.  Our results show that\u00a0\u2026", "num_citations": "1\n", "authors": ["254"]}
{"title": "Programming, compilation, and resource management issues for multithreading (panel session II)\n", "abstract": " HALSTEAD: The topic of this panel is, to put it very briefly, the software side of the issue. We've talked a lot about multithreaded architectures. David Culler has given us a little bit of a transition into thinking about things from the software side. But whether or not you think that software makes the hardware mechanisms redundant, or simply that the existence of various hardware mechanisms creates new challenges on the software side, the purpose of our panel discussion is really to address the question of what else changes, besides the hardware, when you consider multithreaded computers.I'm very happy to have been able to assemble the following panel for this discussion, and I'd actually like to invite you folks to come up and sit in the hot seat. We have David Callahan, who I think of as one of the chief thinkers (especially on the software side) for Tera Computer; we have Jack Dennis, who has contributed all kinds\u00a0\u2026", "num_citations": "1\n", "authors": ["254"]}
{"title": "Automatic localization for distributed-memory multiprocessors using a shared-memory compilation framework\n", "abstract": " We outline an approach for compiling for distributed-memory multiprocessors that is inherited from compiler technologies for shared-memory multiprocessors. We believe that this approach to compiling for distributed-memory machines as promising because it is a logical extension of the shared-memory parallel programming model, a model that is easier for programmers to work with, and that has been studied in great detail as a target for parallelizing and optimizing compilers. In particular, the paper focuses on the localization step, and presents optimal localization algorithms for a single global DOALL, and for special-case structures of multiple global DOALLs.< >", "num_citations": "1\n", "authors": ["254"]}
{"title": "Beyond the data parallel paradigm: issues and options\n", "abstract": " Currently, the predominant approach in compiling a program for parallel execution on a distributed memory multiprocessor is driven by the data parallel paradigm, in which user-specified data mappings are used to derive computation mappings via ad hoc rules such as owner-computes. We explore a more general approach which is driven by the selection of computation mappings from the program dependence constraints, and by the selection of dynamic data mappings from the localization constraints in different computation phases of the program. We state the optimization problems addressed by this approach and outline the solution methods that can be used. We believe that this approach provides promising solutions beyond what can be achieved by the data parallel paradigm. The paper outlines the general program model assumed for this work, states the optimization problems addressed by the approach\u00a0\u2026", "num_citations": "1\n", "authors": ["254"]}
{"title": "Compile-Time Partitioning and Scheduling of Parallel Programs. Extended Summary\n", "abstract": " One of the biggest challenges facing language designers and implementors is to develop languages that will be suitable for use on multiprocessors. A wide variety of multiprocessor architectures are currently being built and that variety is expected to increase. Very little software is available to exploit parallelism and speed-up the execution of individual programs. What little work has been done, has focused largely on special-purpose parallelism eg vectorization or very limited classes of architectures. To understand the issues involved in taking advantage of parallelism and to evaluate architectures, we need to find compilation techniques for fairly general-purpose languages these techniques need to be adapted to a wide variety of architectures. Only then will it be possible to compare various languages, alternative architectures, and their interaction with different applications. To compile and execute a program in a parallel fashion on a multiprocessor, there are three fundamental problems to be solved 1 Identification of potential parallelism-discovering parallel computations in the program. 2 Partitioning the program into tasks-each task represents a serially-executed component that can execute in parallel with other tasks. 3 Scheduling the execution of tasks-the tasks must be scheduled for execution on multiple processors.Descriptors:", "num_citations": "1\n", "authors": ["254"]}