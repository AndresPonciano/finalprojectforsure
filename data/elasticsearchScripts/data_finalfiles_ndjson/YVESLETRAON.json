{"title": "A state-of the-art survey & testbed of fuzzy AHP (FAHP) applications\n", "abstract": " As a practical popular methodology for dealing with fuzziness and uncertainty in Multiple Criteria Decision-Making (MCDM), Fuzzy AHP (FAHP) has been applied to a wide range of applications. As of the time of writing there is no state of the art survey of FAHP, we carry out a literature review of 190 application papers (i.e., applied research papers), published between 2004 and 2016, by classifying them on the basis of the area of application, the identified theme, the year of publication, and so forth. The identified themes and application areas have been chosen based upon the latest state-of-the-art survey of AHP conducted by [Vaidya, O., & Kumar, S. (2006). Analytic hierarchy process: An overview of applications. European Journal of operational research, 169(1), 1\u201329.]. To help readers extract quick and meaningful information, the reviewed papers are summarized in various tabular formats and charts. Unlike\u00a0\u2026", "num_citations": "289\n", "authors": ["304"]}
{"title": "Model-based tests for access control policies\n", "abstract": " We present a model-based approach to testing access control requirements. By using combinatorial testing, we first automatically generate test cases from and without access control policies-i.e., the model- and assess the effectiveness of the test suites by means of mutation testing. We also compare them to purely random tests. For some of the investigated strategies, non-random tests kill considerably more mutants than the same number of random tests. Since we rely on policies only, no information on the application is required at this stage. As a consequence, our methodology applies to arbitrary implementations of the policy decision points.", "num_citations": "100\n", "authors": ["304"]}
{"title": "Comparison of metadata quality in open data portals using the Analytic Hierarchy Process\n", "abstract": " The quality of metadata in open data portals plays a crucial role for the success of open data. E-government, for example, have to manage accurate and complete metadata information to guarantee the reliability and foster the reputation of e-government to the public. Measuring and comparing the quality of open data is not a straightforward process because it implies to take into consideration multiple quality dimensions whose quality may vary from one another, as well as various open data stakeholders who \u2013 depending on their role/needs \u2013 may have different preferences regarding the dimensions\u2019 importance. To address this Multi-Criteria Decision Making (MCDM) problem, and since data quality is hardly considered in existing e-government models, this paper develops an Open Data Portal Quality (ODPQ) framework that enables end-users to easily and in real-time assess/rank open data portals. From a\u00a0\u2026", "num_citations": "87\n", "authors": ["304"]}
{"title": "A tridimensional approach for studying the formal verification of model transformations\n", "abstract": " In Model Driven Engineering (MDE), models are first-class citizens, and model transformation is MDE's \"heart and soul\". Since model transformations are executed for a family of conforming models, their validity becomes a crucial issue. This paper proposes to explore the question of the formal verification of model transformation properties through a tri-dimensional approach: the transformation involved, the properties of interest addressed, and the formal verification techniques used to establish the properties. This work allows a better understanding of the expected properties for a particular transformation, and facilitates the identification of the suitable tools and techniques for enabling their verification.", "num_citations": "84\n", "authors": ["304"]}
{"title": "Testability measurements for data flow designs\n", "abstract": " The paper focuses on data flow designs. It presents a testability measurement based on the controllability/observability pair of attributes. A case study provided by AEROSPATIALE illustrates the testability analysis of an embedded data flow design. Applying such an analysis during the specification stage allows detection of weaknesses and appraisal of improvements in terms of testability.", "num_citations": "52\n", "authors": ["304"]}
{"title": "Experimental comparisons between implicit and explicit implementations of discrete-time sliding mode controllers: Towards chattering suppression in output and input signals\n", "abstract": " This paper presents a set of experimental results concerning the sliding mode control of an electro-pneumatic system. Two discrete-time control strategies are considered for the implementation of the discontinuous part of the sliding mode controller: explicit and implicit discretizations. While the explicit implementation is known to generate numerical chattering [6], [7], [12], [13], the implicit one is expected to significantly reduce chattering while keeping the accuracy. The experimental results reported in this work remarkably confirm that the implicit discrete-time sliding mode supersedes the explicit ones, with several important features: chattering in the control input is almost eliminated (while the explicit and saturated controllers behave like high-frequency bang-bang inputs), the input magnitude depends only on the perturbation size and is largely independent of the controller gain and sampling time.", "num_citations": "49\n", "authors": ["304"]}
{"title": "From hardware to software testability\n", "abstract": " This paper presents the application of some hardware testability concepts to data-flow software. Testability is concerned with three difficulties: generating test sets, interpreting test results and diagnosing faults. This threefold aspect of testability is discussed and estimates are proposed.", "num_citations": "45\n", "authors": ["304"]}
{"title": "Formal verification techniques for model transformations: A tridimensional classification\n", "abstract": " In Model Driven Engineering (MDE), models are first-class citizens, and model transformation is MDE's \"heart and soul\". Since model transformations are executed for a family of (conforming) models, their validity becomes a crucial issue. This paper proposes to explore the question of the formal verification of model transformation properties through a tridimensional approach: the transformation involved, the properties of interest addressed, and the formal verification techniques used to establish the properties. This work is intended for a double audience. For newcomers, it provides a tutorial introduction to the field of formal verification of model transformations. For readers more familiar with formal methods and model transformations, it proposes a literature review (although not systematic) of the contributions of the field. Overall, this work allows to better understand the evolution, trends and current practice in the domain of model transformation verification. This work opens an interesting research line for building an engineering of model transformation verification guided by the notion of model transformation intent.", "num_citations": "39\n", "authors": ["304"]}
{"title": "A systematic review on the engineering of software for ubiquitous systems\n", "abstract": " Context: Software engineering for ubiquitous systems has experienced an important and rapid growth, however the vast research corpus makes it difficult to obtain valuable information from it.Objective: To identify, evaluate, and synthesize research about the most relevant approaches addressing the different phases of the software development life cycle for ubiquitous systems.Method: We conducted a systematic literature review of papers presenting and evaluating approaches for the different phases of the software development life cycle for ubiquitous systems. Approaches were classified according to the phase of the development cycle they addressed, identifying their main concerns and limitations.Results: We identified 128 papers reporting 132 approaches addressing issues related to different phases of the software development cycle for ubiquitous systems. Most approaches have been aimed at addressing\u00a0\u2026", "num_citations": "38\n", "authors": ["304"]}
{"title": "Analyzing testability on data flow designs\n", "abstract": " High testability is a strongly desired feature of software, since it tends to make the validation phase more efficient in exposing faults during testing, and consequently it increases the quality of the end-product. Furthermore, testability is a criterion of crucial importance to software developers, since the sooner it can be estimated, the better the software architecture will be organized to improve subsequent maintenance. This paper is concerned with the testability of data flow software designs, its definition, and the axiomatization of its expected behavior. This behavior is expressed in relation to basic operations that are applicable on designs, and to the dedicated test strategies which are selected. Measurements are proposed which are consistent with the stated axioms. The whole approach is demonstrated using design specifications of embedded software developed in the avionics industry.", "num_citations": "33\n", "authors": ["304"]}
{"title": "Analyzing complex data in motion at scale with temporal graphs\n", "abstract": " Modern analytics solutions succeed to understand and predict phenomenons in a large diversity of software systems, from social networks to Internet-of-Things platforms. This success challenges analytics algorithms to deal with more and more complex data, which can be structured as graphs and evolve over time. However, the underlying data storage systems that support large-scale data analytics, such as time-series or graph databases, fail to accommodate both dimensions, which limits the integration of more advanced analysis taking into account the history of complex graphs, for example.This paper therefore introduces a formal and practical definition of temporal graphs. Temporal graphs provide a compact representation of time-evolving graphs that can be used to analyze complex data in motion. In particular, we demonstrate with our open-source implementation, named GREYCAT, that the performance of temporal graphs allows analytics solutions to deal with rapidly evolving large-scale graphs.", "num_citations": "30\n", "authors": ["304"]}
{"title": "A framework for testing peer-to-peer systems\n", "abstract": " Developing peer-to-peer (P2P) systems is hard because they must be deployed on a high number of nodes, which can be autonomous, refusing to answer to some requests or even unexpectedly leaving the system. Such volatility of nodes is a common behavior in P2P system and can be interpreted as fault during tests.In this paper, we propose a framework for testing P2P systems. This framework is based on the individual control of nodes, allowing test cases to precisely control the volatility of nodes during execution.We validated this framework through implementation and experimentation on an open-source P2P system.", "num_citations": "28\n", "authors": ["304"]}
{"title": "Permissioned blockchain frameworks in the industry: A comparison\n", "abstract": " Permissioned and private blockchain platforms are increasingly used in today\u2019s industry. This paper provides a comprehensive and comparative study of the 5 major frameworks (Fabric, Ethereum, Quorum, MultiChain and R3 Corda) with regard to the community activities, performance, scalability, privacy and adoption criteria. Based on a literature review, this study shows that even if Fabric is promising, the final selection of a framework for a specific case-study is always a trade-off. Finally, lessons learnt are given for industrial practitioners and researchers.", "num_citations": "27\n", "authors": ["304"]}
{"title": "Measuring inconsistency and deriving priorities from fuzzy pairwise comparison matrices using the knowledge-based consistency index\n", "abstract": " The fuzzy analytic hierarchy process (AHP) is a widely applied multiple-criteria decision-making (MCDM) technique, making it possible to tackle vagueness and uncertainty arising from decision makers, especially in a pairwise comparison process. Indeed, as the human brain reasons with uncertain rather than precise information, pairwise comparisons may involve some degree of inconsistency, which must be correctly managed to guarantee a coherent result/ranking. Several consistency indexes for fuzzy pairwise comparison matrices (FPCMs) have been proposed in the literature. However, some scholars argue that most of these fail to be axiomatically grounded, which may lead to misleading results. To overcome this lack of an axiomatically grounded index, a new index is proposed in this paper, referred to as the knowledge-based consistency index (KCI). A comparative study of the proposed index with an\u00a0\u2026", "num_citations": "24\n", "authors": ["304"]}
{"title": "Open data portal quality comparison using AHP\n", "abstract": " During recent years, more and more Open Data becomes available and used as part of the Open Data movement. However, there are reported issues with the quality of the metadata in data portals and the data itself. This is a serious risk that could disrupt the Open Data project, as well as e-government initiatives since the data quality needs to be managed to guarantee the reliability of e-government to the public. First quality assessment frameworks emerge to evaluate the quality for a given dataset or portal along various dimensions (eg, information completeness). Nonetheless, a common problem with such frameworks is to provide meaningful ranking mechanisms that are able to integrate several quality dimensions and user preferences (eg, a portal provider is likely to have different quality preferences than a portal consumer). To address this multi-criteria decision making problem, our research work applies AHP\u00a0\u2026", "num_citations": "23\n", "authors": ["304"]}
{"title": "The next evolution of MDE: a seamless integration of machine learning into domain modeling\n", "abstract": " Machine learning algorithms are designed to resolve unknown behaviors by extracting commonalities over massive datasets. Unfortunately, learning such global behaviors can be inaccurate and slow for systems composed of heterogeneous elements, which behave very differently, for instance as it is the case for cyber-physical systems and Internet of Things applications. Instead, to make smart decisions, such systems have to continuously refine the behavior on a per-element basis and compose these small learning units together. However, combining and composing learned behaviors from different elements is challenging and requires domain knowledge. Therefore, there is a need to structure and combine the learned behaviors and domain knowledge together in a flexible way. In this paper we propose to weave machine learning into domain modeling. More specifically, we suggest to decompose\u00a0\u2026", "num_citations": "21\n", "authors": ["304"]}
{"title": "Efficient distributed test architectures for large-scale systems\n", "abstract": " Typical testing architectures for distributed software rely on a centralized test controller, which decomposes test cases in steps and deploy them across distributed testers. The controller also guarantees the correct execution of test steps through synchronization messages. These architectures are not scalable while testing large-scale distributed systems due to the cost of synchronization management, which may increase the cost of a test and even prevent its execution. This paper presents a distributed architecture to synchronize the test execution sequence. This approach organizes the testers in a tree, where messages are exchanged among parents and children. The experimental evaluation shows that the synchronization management overhead can be reduced by several orders of magnitude. We conclude that testing architectures should scale up along with the distributed system under test.", "num_citations": "21\n", "authors": ["304"]}
{"title": "Impact of system partitioning on test cost\n", "abstract": " Well known as an important influence on other parts of a design, hardware-software partitioning also significantly affects test cost. This article provides a method of measuring partitioning's impact based on the cost to test each unit-level component. An Aerospatiale case study illustrates the use of this method as a design.", "num_citations": "20\n", "authors": ["304"]}
{"title": "Testing obligation policy enforcement using mutation analysis\n", "abstract": " The support of obligations with access control policies allows the expression of more sophisticated requirements such as usage control, availability and privacy. In order to enable the use of these policies, it is crucial to ensure their correct enforcement and management in the system. For this reason, this paper introduces a set of mutation operators for obligation policies. The paper first identifies key elements in obligation policy management, then presents mutation operators which injects minimal errors which affect these aspects. Test cases are qualified w.r.t. their ability in detecting problems, simulated by mutation, in the interactions between policy management and the application code. The use of policy mutants as substitutes for real flaws enables a first investigation of testing obligation policies in a system. We validate our work by providing an implementation of the mutation process: the experiments conducted\u00a0\u2026", "num_citations": "19\n", "authors": ["304"]}
{"title": "Testing peer-to-peer systems\n", "abstract": " Peer-to-peer (P2P) offers good solutions for many applications such as large data sharing and collaboration in social networks. Thus, it appears as a powerful paradigm to develop scalable distributed applications, as reflected by the increasing number of emerging projects based on this technology. However, building trustworthy P2P applications is difficult because they must be deployed on a large number of autonomous nodes, which may refuse to answer to some requests and even leave the system unexpectedly. This volatility of nodes is a common behavior in P2P systems and may be interpreted as a fault during tests (i.e., failed node). In this work, we present a framework and a methodology for testing P2P applications. The framework is based on the individual control of nodes, allowing test cases to precisely control the volatility of nodes during their execution. We validated this framework through\u00a0\u2026", "num_citations": "19\n", "authors": ["304"]}
{"title": "O-mi/o-df standards as interoperability enablers for industrial internet: A performance analysis\n", "abstract": " The Industrial Internet should provide means to create ad hoc and loosely coupled information flows between objects, users, services, and business domain systems. However, today's technologies and products often feed `vertical silos' (e.g., vertical/siloed apps), which inevitably result in multiple and non-interoperable systems. Standardization will play an ever-increasing part in enabling information to flow between such vertically-oriented closed systems. This paper presents recent IoT messaging standards, notably O-MI (Open Messaging Interface) and O-DF (Open Data Format), whose initial requirements were defined for enhanced collaboration and interoperability in product lifecycle management. The performance of those standards is evaluated in terms of efficiency ratio, defined as the percentage of payload over traffic load. A first analytical model of the efficiency ratio based on the required/basic standard\u00a0\u2026", "num_citations": "18\n", "authors": ["304"]}
{"title": "Proficient: Productivity tool for semantic interoperability in an open iot ecosystem\n", "abstract": " The Internet of Things (IoT) is promising to open up opportunities for businesses to offer new services to uncover untapped needs. However, before taking advantage of such opportunities, there are still challenges ahead, one of which is the development of strategies to abstract from the heterogeneity of APIs that shape today's IoT. It is becoming increasingly complex for developers and smart connected objects to efficiently discover, parse, aggregate and process data from disparate information systems, as different protocols, data models, and serializations for APIs exist on the market. Standards play an indisputable role in reducing such a complexity, but will not solve all problems related to interoperability. For example, it will remain a permanent need to help and guide data/service providers to efficiently describe the data/services they would like to expose to the IoT. This paper presents PROFICIENT, a productivity\u00a0\u2026", "num_citations": "16\n", "authors": ["304"]}
{"title": "Linked vocabulary recommendation tools for internet of things: a survey\n", "abstract": " The Semantic Web emerged with the vision of eased integration of heterogeneous, distributed data on the Web. The approach fundamentally relies on the linkage between and reuse of previously published vocabularies to facilitate semantic interoperability. In recent years, the Semantic Web has been perceived as a potential enabling technology to overcome interoperability issues in the Internet of Things (IoT), especially for service discovery and composition. Despite the importance of making vocabulary terms discoverable and selecting the most suitable ones in forthcoming IoT applications, no state-of-the-art survey of tools achieving such recommendation tasks exists to date. This survey covers this gap by specifying an extensive evaluation framework and assessing linked vocabulary recommendation tools. Furthermore, we discuss challenges and opportunities of vocabulary recommendation and related tools\u00a0\u2026", "num_citations": "15\n", "authors": ["304"]}
{"title": "Generic cloud platform multi-objective optimization leveraging models@ run. time\n", "abstract": " Cloud computing promises scalable hosting by offering an elastic management of virtual machines which run on top of hardware data centers. This elastic management as a cornerstone of PaaS (Platform As A Service) has to deal with trade-offs between conflicting requirements such as cost and quality of service. Solving such trade-offs is a challenging problem. Indeed, most of PaaS providers consider only one optimization axis or ad-hoc multi-objective resolution techniques using domain specific heuristics.", "num_citations": "15\n", "authors": ["304"]}
{"title": "Towards an automatic diagnosis for high-level design validation\n", "abstract": " In this paper, we focus on high level design diagnosis. A novel diagnosis strategy is presented which allows faults to be automatically located. Given a system under test, this method effectively restricts the suspected parts in order to correct the detected faults.", "num_citations": "15\n", "authors": ["304"]}
{"title": "Automated functional test case synthesis from Thales industrial requirements\n", "abstract": " Test case generation and specification validation are essential concerns for the software industry in its continuous search for productivity improvement and quality mastering. We present the approach developed in the MUTATION project for functional test case synthesis in the THALES industrial context. A two-step approach is proposed, which automates most of the process in a continuous way. The first step consists in expressing the requirements in the requirements description language (RDL) textual formalism and completing, disambiguating and validating them through simulation. Test objectives are then automatically derived from the RDL formalisation. The second step synthesises functional test cases through combining the test objectives produced at step 1 and symbolic path computation on a UML detailed design model of the application, using the academic tool AGATHA (French acronym for toolset\u00a0\u2026", "num_citations": "14\n", "authors": ["304"]}
{"title": "[Engineering Paper] Enabling the Continuous Analysis of Security Vulnerabilities with VulData7\n", "abstract": " Studies on security vulnerabilities require the analysis, investigation and comprehension of real vulnerable code instances. However, collecting and experimenting with a sufficient number of such instances is challenging. To cope with this issue, we developed VulData7, an extensible framework and dataset of real vulnerabilities, automatically collected from software archives. The current version of the dataset contains all reported vulnerabilities (in the NVD database) of 4 security critical open source systems, i.e., Linux Kernel, WireShark, OpenSSL, SystemD. For each vulnerability, VulData7 provides the vulnerability report data (description, CVE number, CWE number, CVSS severity score and others), the vulnerable code instance (list of versions), and when available its corresponding patches (list of fixing commits) and the files (before and after fix). VulData7 is automated, flexible and easily extensible. Once\u00a0\u2026", "num_citations": "13\n", "authors": ["304"]}
{"title": "A training-resistant anomaly detection system\n", "abstract": " Modern network intrusion detection systems rely on machine learning techniques to detect traffic anomalies and thus intruders. However, the ability to learn the network behaviour in real-time comes at a cost: malicious software can interfere with the learning process, and teach the intrusion detection system to accept dangerous traffic. This paper presents an intrusion detection system (IDS) that is able to detect common network attacks including but not limited to, denial-of-service, bot nets, intrusions, and network scans. With the help of the proposed example IDS, we show to what extent the training attack (and more sophisticated variants of it) has an impact on machine learning based detection schemes, and how it can be detected.", "num_citations": "13\n", "authors": ["304"]}
{"title": "Privacy challenges in ambient intelligence systems\n", "abstract": " Today, privacy is a key concept. It is also one which is rapidly evolving with technological advances, and there is no consensus on a single definition for it. In fact, the concept of privacy has been defined in many different ways, ranging from the \u201cright to be left alone\u201d to being a \u201ccommodity\u201d that can be bought and sold. In the same time, powerful Ambient Intelligence (AmI) systems are being developed, that deploy context-aware, personalised, adaptive and anticipatory services. In such systems personal data is vastly collected, stored, and distributed, making privacy preservation a critical issue. The human-centred focus of AmI systems has prompted the introduction of new kinds of technologies, eg Privacy Enhancing Technologies (PET), and methodologies, eg Privacy by Design (PbD), whereby privacy concerns are included in the design of the system. One particular application field, where privacy preservation is of\u00a0\u2026", "num_citations": "13\n", "authors": ["304"]}
{"title": "GREYCAT: Efficient what-if analytics for data in motion at scale\n", "abstract": " Over the last few years, data analytics shifted from adescriptive era, confined to the explanation of past events, to the emergence of predictive techniques. Nonetheless, existing predictive techniques still fail to effectively explore alternative futures, which continuously diverge from current situations when exploring the effects of what-if decisions. Enabling prescriptive analytics therefore calls for the design of scalable systems that can cope with the complexity and the diversity of underlying data models. In this article, we address this challenge by combining graphs and time series within a scalable storage system that can organize a massive amount of unstructured and continuously changing data into multi-dimensional data models, called Many-Worlds Graphs. We demonstrate that our open source implementation, GreyCat, can efficiently fork and update thousands of parallel worlds composed of millions of\u00a0\u2026", "num_citations": "12\n", "authors": ["304"]}
{"title": "Enriching a situation awareness framework for IoT with knowledge base and reasoning components\n", "abstract": " The importance of system-level context- and situation awareness increases with the growth of the Internet of Things (IoT). This paper proposes an integrated approach to situation awareness by providing a semantically rich situation model together with reliable situation inference based on Context Spaces Theory (CST) and Situation Theory (ST). The paper discusses benefits of integrating the proposed situation awareness framework with knowledge base and efficient reasoning techniques taking into account uncertainty and incomplete knowledge about situations. The paper discusses advantages and impact of proposed context adaptation in dynamic IoT environments. Practical issues of two-way mapping between IoT messaging standards and CST are also discussed.", "num_citations": "12\n", "authors": ["304"]}
{"title": "Towards semantic interoperability in an open IoT ecosystem for connected vehicle services\n", "abstract": " A present challenge in today's Internet of Things (IoT) ecosystem is to enable interoperability across heterogeneous systems and service providers. Restricted access to data sources and services limits the capabilities of a smart city to improve social, environmental and economic aspects. Interoperability in the IoT is concerned with both, messaging interfaces and semantic understanding of heterogeneous data. In this paper, the first building blocks of an emerging open IoT ecosystem developed at the EU level are presented. Semantic web technologies are applied to the existing messaging components to support and improve semantic interoperability. The approach is demonstrated with a proof-of-concept for connected vehicle services in a smart city setting.", "num_citations": "12\n", "authors": ["304"]}
{"title": "Micro-billing framework for IoT: Research & Technological foundations\n", "abstract": " In traditional product companies, creating value meant identifying enduring customer needs and manufacturing well-engineered solutions. Two hundred and fifty years after the start of the Industrial Revolution, this pattern of activity plays out every day in a connected world where products are no longer one-and-done. Making money is not anymore limited to physical product sales, other downstream revenue streams become possible (e.g., service-based information, Apps). Nonetheless, it is still challenging to stimulate the IoT market by enabling IoT stakeholders (from organizations to an individual persons) to make money out of the information that surrounds them. Generally speaking, there is a lack of micro-billing frameworks and platforms that enable IoT stakeholders to publish/discover, and potentially sell/buy relevant and useful IoT information items. This paper discusses important aspects that need to be\u00a0\u2026", "num_citations": "12\n", "authors": ["304"]}
{"title": "Under pressure benchmark for ddbms availability\n", "abstract": " The availability of Distributed Database Management Systems (DDBMS) is related to the probabilityof being up and running at a given point in time and to the management of failures. One well-known and widelyused mechanism to ensure availability is replication, which includes performance impact on maintaining data replicasacross the DDBMS's machine nodes. Benchmarking can be used to measure such impact. In this article, we present abenchmark that evaluates the performance of DDBMS, considering availability through replication, called Under Pres-sure Benchmark (UPB). The UPB measures performance with di? erent degrees of replication upon a high-throughputdistributed workload, combined with failures. The UPB methodology increases the evaluation complexity from a sta-ble system scenario to a complex one with di? erent load sizes and replicas. We validate our benchmark with threehigh-throughput in-memory DDBMS: VoltDB, NuoDB and Dbms-X.", "num_citations": "12\n", "authors": ["304"]}
{"title": "Peer-to-peer load testing\n", "abstract": " Nowadays the large-scale systems are common-place in any kind of applications. The popularity of the web created a new environment in which the applications need to be highly scalable due to the data tsunami generated by a huge load of requests (i.e., connections and business operations). In this context, the main question is to validate how far the web applications can deal with the load generated by the clients. Load testing is a technique to analyze the behavior of the system under test upon normal and heavy load conditions. In this work we present a peer-to-peer load testing approach to isolate bottleneck problems related to centralized testing drivers and to scale up the load. Our approach was tested in a DBMS as study case and presents satisfactory results.", "num_citations": "12\n", "authors": ["304"]}
{"title": "Towards a unified approach to the testability of co-designed systems\n", "abstract": " The paper deals with the testability analysis of dataflow co designed systems. As a data flow specification is independent from the hardware/software implementation choice, a uniform approach may be used to evaluate the specification with respect to testability. The difficulty of generating test sets, and of detecting and diagnosing faults is discussed and estimated. We chose to use an existing hardware testability model which is suitable for data flow software specification; this model, based on information transfers, is called the Information Transfer Graph. A real case study supplied by Aerospatiale illustrates the proposed testability estimates.", "num_citations": "12\n", "authors": ["304"]}
{"title": "Security@Runtime: A Flexible MDE Approach to Enforce Fine-grained Security Policies\n", "abstract": " In this paper, we present a policy-based approach for automating the integration of security mechanisms into Java-based business applications. In particular, we introduce an expressive Domain Specific modeling Language (Dsl), called Security@Runtime, for the specification of security configurations of targeted systems. The Security@Runtime                 Dsl supports the expression of authorization, obligation and reaction policies, covering many of the security requirements of modern applications. Security requirements specified in security configurations are enforced using an application-independent Policy Enforcement Point Pep)- Policy Decision Point (Pdp) architecture, which enables the runtime update of security requirements. Our work is evaluated using two systems and its advantages and limitations are discussed.", "num_citations": "11\n", "authors": ["304"]}
{"title": "Introducing conviviality as a new paradigm for interactions among IT objects\n", "abstract": " [en] The Internet of Things allows people and objects to seamlessly interact, crossing the bridge between real and virtual worlds. Newly created spaces are heterogeneous; social relations naturally extend to smart objects. Conviviality has recently been introduced as a social science concept for ambient intelligent systems to highlight soft qualitative requirements like user friendliness of systems. Roughly, more opportunities to work with other people increase the conviviality. In this paper, we first propose the conviviality concept as a new interaction paradigm for social exchanges between humans and Information Technology (IT) objects, and extend it to IT objects among themselves. Second, we introduce a hierarchy for IT objects social interactions, from low-level one-way interactions to high-level complex interactions. Then, we propose a mapping of our hierarchy levels into dependence networks-based conviviality classes. In particular, low levels without cooperation among objects are mapped to lower conviviality classes, and high levels with complex cooperative IT objects are mapped to higher conviviality classes. Finally, we introduce new conviviality measures for the Internet of Things, and an iterative process to facilitate cooperation among IT objects, thereby the conviviality of the system. We use a smart home as a running example.", "num_citations": "11\n", "authors": ["304"]}
{"title": "Assessing the impact of attacks on opc-ua applications in the industry 4.0 era\n", "abstract": " The advent of the Internet of Things (IoT) is leading to create \u201cSystem-of-Systems\u201d, where disparate information systems, sensors, devices, people and software solutions are used altogether. Industrial companies tend to apply the same concept for gaining in productivity. Several consortia, such as the German initiative Industrie 4.0, recommend to use the OPCUA framework to manage interoperability issues. Associate to the convergence of the Operational Technology (OT) and the Information Technology (IT) domains, it may lead to create a new attack surface, that needs to be apprehended. The contribution is therefore to identify, based on the specifications, the threats and countermeasures that may occur/be applied when using OPC-UA in an Industry 4.0 environment and to highlight the impact of the eavesdropping and message flooding attacks on an OPC-UA application implemented on a real testbed.", "num_citations": "10\n", "authors": ["304"]}
{"title": "Cloud providers viability: how to address it from an IT and legal perspective?\n", "abstract": " A major part of the commercial Internet is moving towards a cloud paradigm. This phenomenon has a drastic impact on the organizational structures of enterprises and introduces new challenges that must be properly addressed to avoid major setbacks. One such challenge is that of cloud provider viability, that is, the reasonable certainty that the Cloud Service Provider (CSP) will not go out of business, either by filing for bankruptcy or by simply shutting down operations, thus leaving its customers stranded without an infrastructure and, depending on the type of cloud service used, even without their applications or data. This article attempts to address the issue of cloud provider viability, proposing some ways of mitigating the problem both from a technical and from a legal perspective.", "num_citations": "10\n", "authors": ["304"]}
{"title": "Testability analysis of co-designed systems\n", "abstract": " This paper focus on the testability analysis of co-designed data-flow specifications. The co-designed specification level implies a high level testability analysis, independent of the implementation choices. With respect to testability, the difficulties of generating test sets, detecting and diagnosing faults are discussed and estimates are proposed. A hardware modelling, based on information transfers and called the Information Transfer Graph, is adapted to the specifications. A real case study supplied by Aerospatiale illustrates all the evaluations.", "num_citations": "10\n", "authors": ["304"]}
{"title": "Efficiently computing the likelihoods of cyclically interdependent risk scenarios\n", "abstract": " Quantitative risk assessment provides a holistic view of risk in an organisation, which is, however, often biased by the fact that risk shared by several assets is encoded multiple times in a risk analysis. An apparent solution to this issue is to take all dependencies between assets into consideration when building a risk model. However, existing approaches rarely support cyclic dependencies, although assets that mutually rely on each other are encountered in many organisations, notably in critical infrastructures. To the best of our knowledge, no author has provided a provably efficient algorithm (in terms of the execution time) for computing the risk in such an organisation, notwithstanding that some heuristics exist.This paper introduces the dependency-aware root cause (DARC) model, which is able to compute the risk resulting from a collection of root causes using a poly-time randomised algorithm, and concludes\u00a0\u2026", "num_citations": "9\n", "authors": ["304"]}
{"title": "Inroads in Testing Access Control\n", "abstract": " In the last few years, a plethora of research has addressed security testing issues. Several commercial tools have emerged to provide security testing services. Software security testing goes beyond functional testing to reveal flaws and vulnerabilities in software design and behavior. Access control is a major pillar in computer security. This chapter pursues the goal of describing the landscape in the research area of access control testing. We provide an outline of the different existing research over the literature according to the taxonomy reflecting the different phases of common software testing processes (generation, selection, prioritization, quality assessment, regression). We also provide an outline of some existing initiatives that support usage control besides access control by testing obligation policies. Finally, we point out future research directions that emerge from the current research study. Through this work\u00a0\u2026", "num_citations": "9\n", "authors": ["304"]}
{"title": "A rule-based contextual reasoning platform for ambient intelligence environments\n", "abstract": " The special characteristics and requirements of intelligent environments impose several challenges to the reasoning processes of Ambient Intelligence systems. Such systems must enable heterogeneous entities operating in open and dynamic environments to collectively reason with imperfect context information. Previously we introduced Contextual Defeasible Logic (CDL) as a contextual reasoning model that addresses most of these challenges using the concepts of context, mappings and contextual preferences. In this paper, we present a platform integrating CDL with Kevoree, a component-based software framework for Dynamically Adaptive Systems. We explain how the capabilities of Kevoree are exploited to overcome several technical issues, such as communication, information exchange and detection, and explain how the reasoning methods may be further extended. We illustrate our approach\u00a0\u2026", "num_citations": "9\n", "authors": ["304"]}
{"title": "Software diagnosability\n", "abstract": " This paper is concerned with diagnosability, its definition and the axiomatization of its expected behaviour. The intuitive expected behaviour of diagnosability is defined relative to basic operations that are applicable on software designs. A diagnosability measurement is proposed which is consistent with the stated axioms. The diagnosability metric is based on an analysis of the design structure: fault location effort and precision are measured for a given testing context. Compromises between global test difficulty and diagnostic precision are illustrated on part of a data-flow software design. Throughout the paper, we develop a case study.", "num_citations": "9\n", "authors": ["304"]}
{"title": "Testability-oriented hardware/software partitioning\n", "abstract": " In this paper a test-based hardware/software partitioning approach is presented for a co-design specification. Depending on the hardware or software implementation choice for each unit-level component, the test cost for the whole system is evaluated. The unit test costs are estimated by means of mutation-based analysis with respect to the implementation choices.", "num_citations": "9\n", "authors": ["304"]}
{"title": "Continuous identification in smart environments using wrist-worn inertial sensors\n", "abstract": " In this paper, we propose a new approach capable of performing continuous identification of users in home and office environments based on hand and arm motion patterns obtained from a wrist-worn inertial measurement unit (IMU). Different from state-of-the-art methods, our approach is not constrained to particular types of movements, gestures, or activities, thus allowing users to perform freely and unconstrained their daily routines while the identification takes place. We evaluate our approach by conducting an in the lab study and two in-situ studies, one in home environment and one in office environment. Our studies involved a total of 29 different participants and the data collected corresponds to approximately 256 hours. The results obtained in the studies indicate that our approach is able to perform continuous user identification with an accuracy of 0.88 for office environments and 0.71 for the average size of a\u00a0\u2026", "num_citations": "8\n", "authors": ["304"]}
{"title": "Stress testing of transactional database systems\n", "abstract": " Transactional database management systems (DBMS) have been successful at supporting traditional transaction processing workloads. However, web-based applications that tend to generate huge numbers of concurrent business operations are pushing DBMS performance over their limits, thus threatening overall system availability. Then, a crucial question is how to test DBMS performance under heavy workload conditions. Answering this question requires a testing methodology to set up non-biased conditions for pushing a particular DBMS over its normal performance limits (i.e., to stress it). In this article, we present a stress testing methodology for DBMS to search for defects in supporting very heavy workloads. Our methodology leverages distributed testing techniques and takes into account the various biases that may affect the test results. It progressively increases the workload along with several tuning steps up to a stress condition. We validate our methodology with empirical studies on two popular DBMS (one proprietary, one open-source) and detail the defects that have been found.", "num_citations": "8\n", "authors": ["304"]}
{"title": "XSS-FP: Browser fingerprinting using HTML parser quirks\n", "abstract": " There are many scenarios in which inferring the type of a client browser is desirable, for instance to fight against session stealing. This is known as browser fingerprinting. This paper presents and evaluates a novel fingerprinting technique to determine the exact nature (browser type and version, eg Firefox 15) of a web-browser, exploiting HTML parser quirks exercised through XSS. Our experiments show that the exact version of a web browser can be determined with 71% of accuracy, and that only 6 tests are sufficient to quickly determine the exact family a web browser belongs to.", "num_citations": "8\n", "authors": ["304"]}
{"title": "Verifying access control in statecharts\n", "abstract": " Access control is one of the main security mechanisms for software applications. It ensures that all accesses conform to a predefined access control policy. It is important to check that the access control policy is well implemented in the system. When following an MDD methodology it may be necessary to check this early during the development lifecycle, namely when modeling the application. This paper tackles the issue of verifying access control policies in statecharts. The approach is based on the transformation of a statechart into an Algebraic Petri net to enable checking access control policies and identifying potential inconsistencies with an OrBAC set of access control policies. Our method allows locating the part of the statechart that is causing the problem. The approach has been successfully applied to a Library Management System. Based on our proposal a tool for performing the transformation and localization of errors in the statechart has been implemented.", "num_citations": "7\n", "authors": ["304"]}
{"title": "Testing peers' volatility\n", "abstract": " Peer-to-peer (P2P) is becoming a key technology for software development, but still lacks integrated solutions to build trust in the final software, in terms of correctness and security. Testing such systems is difficult because of the high numbers of nodes which can be volatile. In this paper, we present a framework for testing volatility of P2P systems. The framework is based on the individual control of peers, allowing test cases to precisely control the volatility of peers during execution. We validated our framework through implementation and experimentation on two open-source P2P systems. Through experimentation, we analyze the behavior of both systems on different conditions of volatility and show how the framework is able to detect implementation problems.", "num_citations": "7\n", "authors": ["304"]}
{"title": "Meta-modelling meta-learning\n", "abstract": " Although artificial intelligence and machine learning are currently extremely fashionable, applying machine learning on real-life problems remains very challenging. Data scientists need to evaluate various learning algorithms and tune their numerous parameters, based on their assumptions and experience, against concrete problems and training data sets. This is a long, tedious, and resource expensive task. Meta-learning is a recent technique to overcome, i.e. automate this problem. It aims at using machine learning itself to automatically learn the most appropriate algorithms and parameters for a machine learning problem. As it turns out, there are many parallels between meta-modelling\u2014in the sense of model-driven engineering\u2014and meta-learning. Both rely on abstractions, the meta data, to model a predefined class of problems and to define the variabilities of the models conforming to this definition. Both are\u00a0\u2026", "num_citations": "6\n", "authors": ["304"]}
{"title": "Cloud providers viability\n", "abstract": " A major part of the commercial Internet is moving toward the cloud paradigm. This phenomenon has a drastic impact on the organizational structures of enterprizes and introduces new challenges that must be properly addressed to avoid major setbacks. One such challenge is that of cloud provider viability, that is, the reasonable certainty that the Cloud Service Provider (CSP) will not go out of business, either by filing for bankruptcy or by simply shutting down operations, thus leaving its customers stranded without an infrastructure and, depending on the type of cloud service used, even without their applications or data. This article attempts to address the issue of cloud provider viability, defining a possible way of modeling viability as a non-functional requirement and proposing some approaches that can be used to mitigate the problem, both from a technical and from a legal perspective. By introducing a\u00a0\u2026", "num_citations": "6\n", "authors": ["304"]}
{"title": "Conviviality-driven access control policy\n", "abstract": " Nowadays many organizations experience security incidents due to unauthorized access to information. To reduce the risk of such incidents, security policies are often employed to regulate access to information. Such policies, however, are often too restrictive, and users do not have the rights necessary to perform assigned duties. As a consequence, access control mechanisms are perceived by users as a barrier and thus bypassed, making the system insecure. In this paper, we draw a bridge between the social concept of conviviality and access control. Conviviality has been introduced as a social science concept for ambient intelligence and multi-agent systems to highlight soft qualitative requirements like user-friendliness of systems. To bridge the gap between conviviality and security, we propose a methodological framework for updating and adapting access control policies based on conviviality\u00a0\u2026", "num_citations": "6\n", "authors": ["304"]}
{"title": "Optimizing multi-objective evolutionary algorithms to enable quality-aware software provisioning\n", "abstract": " Elasticity is a key feature for cloud infrastructures to continuously align allocated computational resources to evolving hosted software needs. This is often achieved by relaxing quality criteria, for instance security or privacy because quality criteria are often conflicting with performance. As an example, software replication could improve scalability and uptime while decreasing privacy by creating more potential leakage points. The conciliation of these conflicting objectives has to be achieved by exhibiting trade-offs. Multi-Objective Evolutionary Algorithms (MOEAs) have shown to be suitable candidates to find these trade-offs and have been even applied for cloud architecture optimizations. Still though, their runtime efficiency limits the widespread adoption of such algorithms in cloud engines, and thus the consideration of quality criteria in clouds. Indeed MOEAs produce many dead-born solutions because of the\u00a0\u2026", "num_citations": "6\n", "authors": ["304"]}
{"title": "Information dependencies in MCS: Conviviality-based model and metrics\n", "abstract": " Information exchange among heterogenous entities is common in most distributed systems. To facilitate information exchange, we first need ways to evaluate it. The concept of conviviality was recently introduced to model and measure cooperation among agents. In this paper, we use conviviality to model and measure information dependencies in distributed systems modeled as Multi-Context Systems. Then, we apply our findings to resolve inconsistencies among participating entities.", "num_citations": "6\n", "authors": ["304"]}
{"title": "Popularity-driven ontology ranking using qualitative features\n", "abstract": " Efficient ontology reuse is a key factor in the Semantic Web to enable and enhance the interoperability of computing systems. One important aspect of ontology reuse is concerned with ranking most relevant ontologies based on a keyword query. Apart from the semantic match of query and ontology, the state-of-the-art often relies on ontologies\u2019 occurrences in the Linked Open Data (LOD) cloud to determine relevance. We observe that ontologies of some application domains, in particular those related to Web of Things (WoT), often do not appear in the underlying LOD datasets used to define ontologies\u2019 popularity, resulting in ineffective ranking scores. This motivated us to investigate\u00a0\u2013\u00a0based on the problematic WoT case\u00a0\u2013\u00a0whether the scope of ranking models can be extended by relying on qualitative attributes instead of an explicit popularity feature. We propose a novel approach to ontology ranking by (i\u00a0\u2026", "num_citations": "5\n", "authors": ["304"]}
{"title": "Raising time awareness in model-driven engineering: Vision paper\n", "abstract": " The conviction that big data analytics is a key for the success of modern businesses is growing deeper, and the mobilisation of companies into adopting it becomes increasingly important. Big data integration projects enable companies to capture their relevant data, to efficiently store it, turn it into domain knowledge, and finally monetize it. In this context, historical data, also called temporal data, is becoming increasingly available and delivers means to analyse the history of applications, discover temporal patterns, and predict future trends. Despite the fact that most data that today's applications are dealing with is inherently temporal, current approaches, methodologies, and environments for developing these applications don't provide sufficient support for handling time. We envision that Model-Driven Engineering (MDE) would be an appropriate ecosystem for a seamless and orthogonal integration of time into\u00a0\u2026", "num_citations": "5\n", "authors": ["304"]}
{"title": "Building lifecycle management system for enhanced closed loop collaboration\n", "abstract": " In the past few years, the architecture, engineering and construction (AEC) industry has carried out efforts to develop BIM (Building Information Modelling) facilitating tools and standards for enhanced collaborative working and information sharing. Lessons learnt from other industries and tools such as PLM (Product Lifecycle Management) \u2013 established tool in manufacturing to manage the engineering change process \u2013 revealed interesting potential to manage more efficiently the building design and construction processes. Nonetheless, one of the remaining challenges consists in closing the information loop between multiple building lifecycle phases, e.g. by capturing information from middle-of-life processes (i.e., use and maintenance) to re-use it in end-of-life processes (e.g., to guide disposal decision making). Our research addresses this lack of closed-loop system in the AEC industry by proposing an\u00a0\u2026", "num_citations": "5\n", "authors": ["304"]}
{"title": "A state machine for database non-functional testing\n", "abstract": " Over the last decade, large amounts of concurrent transactions have been generated from different sources, such as, Internet-based systems, mobile applications, smart-homes and cars. High-throughput transaction processing is becoming commonplace, however there is no testing technique for validating non functional aspects of DBMS under transaction flooding workloads. In this paper we propose a database state machine to represent the states of DBMS when processing concurrent transactions. The state transitions are forced by increasing concurrency of the testing workload. Preliminary results show the effectiveness of our approach to drive the system among different performance states and to find related defects.", "num_citations": "5\n", "authors": ["304"]}
{"title": "Access control enforcement testing\n", "abstract": " A policy-based access control architecture comprises Policy Enforcement Points (PEPs), which are modules that intercept subjects access requests and enforce the access decision reached by a Policy Decision Point (PDP), the module implementing the access decision logic. In applications, PEPs are generally implemented manually, which can introduce errors in policy enforcement and lead to security vulnerabilities. In this paper, we propose an approach to systematically test and validate the correct enforcement of access control policies in a given target application. More specifically, we rely on a two folded approach where a static analysis of the target application is first made to identify the sensitive accesses that could be regulated by the policy. The dynamic analysis of the application is then conducted using mutation to verify for every sensitive access whether the policy is correctly enforced. The dynamic\u00a0\u2026", "num_citations": "5\n", "authors": ["304"]}
{"title": "Preventing data leakage in service orchestration\n", "abstract": " Web Services are currently the base of a lot a e-commerce applications. Nevertheless, clients often use these services without knowing anything about their internals. Moreover, they have no clue about the use of their personal data inside the global applications. In this paper, we offer the opportunity to the user to specify constraints on the use of its personal data. To ensure the privacy of data at runtime, we define a distributed security policy model. This policy is configured at runtime by the user of the BPEL program. This policy is enforced within a BPEL interpreter, and ensures that no information flow can be produced from the user data to unauthorized services. However, the dynamic aspects of web services lead to situations where the policy prohibits the nominal operation of orchestration (e.g., when using a service that is unknown by the user). To solve this problem, we propose to let user to dynamically permit\u00a0\u2026", "num_citations": "5\n", "authors": ["304"]}
{"title": "LOVBench: Ontology ranking benchmark\n", "abstract": " Ontology search and ranking are key building blocks to establish and reuse shared conceptualizations of domain knowledge on the Web. However, the effectiveness of proposed ontology ranking models is difficult to compare since these are often evaluated on diverse datasets that are limited by their static nature and scale. In this paper, we first introduce the LOVBench dataset as a benchmark for ontology term ranking. With inferred relevance judgments for more than 7000 queries, LOVBench is large enough to perform a comparison study using learning to rank (LTR) with complex ontology ranking models. Instead of relying on relevance judgments from a few experts, we consider implicit feedback from many actual users collected from the Linked Open Vocabularies (LOV) platform. Our approach further enables continuous updates of the benchmark, capturing the evolution of ontologies\u2019 relevance in an ever\u00a0\u2026", "num_citations": "4\n", "authors": ["304"]}
{"title": "A case driven study of the use of time series classification for flexibility in industry 4.0\n", "abstract": " With the Industry 4.0 paradigm comes the convergence of the Internet Technologies and Operational Technologies, and concepts, such as Industrial Internet of Things (IIoT), cloud manufacturing, Cyber-Physical Systems (CPS), and so on. These concepts bring industries into the big data era and allow for them to have access to potentially useful information in order to optimise the Overall Equipment Effectiveness (OEE); however, most European industries still rely on the Computer-Integrated Manufacturing (CIM) model, where the production systems run as independent systems (ie, without any communication with the upper levels). Those production systems are controlled by a Programmable Logic Controller, in which a static and rigid program is implemented. This program is static and rigid in a sense that the programmed routines cannot evolve over the time unless a human modifies it. However, to go further in terms of flexibility, we are convinced that it requires moving away from the aforementioned old-fashioned and rigid automation to a ML-based automation, ie, where the control itself is based on the decisions that were taken by ML algorithms. In order to verify this, we applied a time series classification method on a scale model of a factory using real industrial controllers, and widened the variety of parts the production line has to treat. This study shows that satisfactory results can be obtained only at the expense of the human expertise (ie, in the industrial process and in the ML process). View Full-Text", "num_citations": "4\n", "authors": ["304"]}
{"title": "Knowledge-based consistency index for fuzzy pairwise comparison matrices\n", "abstract": " Fuzzy AHP is today one of the most used Multiple Criteria Decision-Making (MCDM) techniques. The main argument to introduce fuzzy set theory within AHP lies in its ability to handle uncertainty and vagueness arising from decision makers (when performing pairwise comparisons between a set of criteria/alternatives). As humans usually reason with granular information rather than precise one, such pairwise comparisons may contain some degree of inconsistency that needs to be properly tackled to guarantee the relevance of the result/ranking. Over the last decades, several consistency indexes designed for fuzzy pairwise comparison matrices (FPCMs) were proposed, as will be discussed in this article. However, for some decision theory specialists, it appears that most of these indexes fail to be properly \u201caxiomatically\u201d founded, thus leading to misleading results. To overcome this, a new index, referred to as KCI\u00a0\u2026", "num_citations": "4\n", "authors": ["304"]}
{"title": "\u00c9valuation de la fiabilit\u00e9 d'une table de hachage distribu\u00e9e construite dans un plan hyperbolique\n", "abstract": " Une table de hachage distribu\u00e9e doit pouvoir acheminer les messages de requ\u00eate en supportant le passage \u00e0 l'\u00e9chelle. Bien que plusieurs solutions existent d\u00e9j\u00e0, elles n\u00e9cessitent souvent une topologie pr\u00e9d\u00e9finie entre les noeuds ainsi que des tables de routage. Nous propo-sons d'utiliser un algorithme de routage glouton bas\u00e9 sur des coordonn\u00e9es virtuelles provenant du plan hyperbolique afin de construire une table de hachage distribu\u00e9e ayant une topologie quelconque et ne requi\u00e9rant pas de table de routage. Nous d\u00e9finissons \u00e0 l'aide de cet algorithme un nouveau syst\u00e8me de table de hachage distribu\u00e9e fiable et supportant le passage \u00e0 l'\u00e9chelle. Nous fournissons une analyse des co\u00fbts de complexit\u00e9 et nous \u00e9valuons ses performances par des simulations en les comparant \u00e0 des solutions existantes. Les r\u00e9sultats montrent que notre syst\u00e8me apporte de la flexibilit\u00e9 aux noeuds tout en restant fiable et extensible en pr\u00e9sence de remous.", "num_citations": "4\n", "authors": ["304"]}
{"title": "Composing models at two modeling levels to capture heterogeneous concerns in requirements\n", "abstract": " Requirements specification is initially scattered in numerous partial models (viewpoints), describing heterogeneous concerns (typically functional and non-functional ones). To define these concerns, requirements analysts prefer describing them separately with metamodels so that they can be properly identified, reused and tooled. The production of one unified view of requirements from separate viewpoints is a complex issue which requires a composition process working at two levels of modeling. At the meta-level, separate \u201cof-the-shelf\u201d metamodels allow defining either concerns or variation in the operational semantics. These metamodels have to be composed into a core metamodel, which captures the information and semantics needed for expressing and analyzing the requirements of a dedicated application domain (e.g. real-time critical systems, telecom services). At the instance-level, viewpoints are\u00a0\u2026", "num_citations": "4\n", "authors": ["304"]}
{"title": "Analyse conjointe logiciel/mat\u00e9riel de la testabilit\u00e9 de syst\u00e8mes flot de donn\u00e9es\n", "abstract": " Ce travail concerne la qualite de specifications flot de donnees mixtes logiciel-materiel. Le langage plus particulierement etudie est le langage sao (specification assistee par ordinateur) actuellement utilise dans les domaines avionique et spatial. Les facteurs que nous cherchons a analyser sont la testabilite et la diagnosabilite de la specification. L'enjeu de la testabilite, definie comme facilite a tester, apparait dans l'opposition a priori entre l'efficacite exigee de la phase de validation et le cout qu'elle occasionne, sachant qu'il faut bien tester pour obtenir une bonne confiance dans le systeme. Le but de l'analyse de testabilite est de rendre compatibles ces exigences de confiance et de reduction des couts: un systeme plus testable doit pouvoir etre teste plus efficacement a moindre cout. Comme l'activite de diagnostic s' integre pleinement au processus de validation, nous nous sommes aussi interesses a la diagnosabilite du systeme, definie comme la facilite a localiser les fautes detectees. Pour elaborer une analyse de testabilite et diagnosabilite, il faut pouvoir l'estimer, la mesurer. C'est le principal objet de ce travail que de definir des mesures de testabilite et de diagnosabilite aidant a la detection d'enventuelles faiblesses de la specification et a l'elaboration de la strategie de test la plus adaptee. Pour cela, le comportement attendu des mesures est specifie, puis le modele et les mesures sont definis formellement. Les mesures sont verifiees theoriquement par rapport au comportement attendu intuitivement. Ceci garantit la conformite des mesures avec l'intuition et est complete par une premiere verification experimentale. On distingue les\u00a0\u2026", "num_citations": "4\n", "authors": ["304"]}
{"title": "Automated search for configurations of deep neural network architectures\n", "abstract": " Deep Neural Networks (DNNs) are intensively used to solve a wide variety of complex problems. Although powerful, such systems require manual configuration and tuning. To this end, we view DNNs as configurable systems and propose an end-to-end framework that allows the configuration, evaluation and automated search for DNN architectures. Therefore, our contribution is threefold. First, we model the variability of DNN architectures with a Feature Model (FM) that generalizes over existing architectures. Each valid configuration of the FM corresponds to a valid DNN model that can be built and trained. Second, we implement, on top of Tensorflow, an automated procedure to deploy, train and evaluate the performance of a configured model. Third, we propose a method to search for configurations and demonstrate that it leads to good DNN models. We evaluate our method by applying it on image classification tasks (MNIST, CIFAR-10) and show that, with limited amount of computation and training, our method can identify high-performing architectures (with high accuracy). We also demonstrate that we outperform existing state-of-the-art architectures handcrafted by ML researchers. Our FM and framework have been released %and are publicly available to support replication and future research.", "num_citations": "3\n", "authors": ["304"]}
{"title": "O-mi/o-df vs. mqtt: A performance analysis\n", "abstract": " Over the past decade, a flourishing number of concepts and architectural shifts appeared such as Industrial Internet of Things (IIoT), Industrial CPS or even Industry 4.0. Unfortunately, today's IoT as well as Industry 4.0 environments, look more like a collection of isolated \u201cIntranets of Things\u201d, also referred to as \u201cvertical silos\u201d, rather than a federated infrastructure. Breaking down these silos is a key challenge in both the IoT and Industry 4.0 communities. This paper is intended to present and discuss two open and standardised messaging protocols designed for IoT applications, namely: MQTT and O-MI/O-DF. First, a traffic load's analytical model derived from the MQTT standard specifications is presented. Second, a comparison study between MQTT and O-MI/O-DF standards is carried out based on a real-life industrial implementation. This study brings a deep understanding of the extent to which these protocols are\u00a0\u2026", "num_citations": "3\n", "authors": ["304"]}
{"title": "Dynamic risk analyses and dependency-aware root cause model for critical infrastructures\n", "abstract": " Critical Infrastructures are known for their complexity and the strong interdependencies between the various components. As a result, cascading effects can have devastating consequences, while foreseeing the overall impact of a particular incident is not straight-forward at all and goes beyond performing a simple risk analysis. This work presents a graph-based approach for conducting dynamic risk analyses, which are programmatically generated from a threat model and an inventory of assets. In contrast to traditional risk analyses, they can be kept automatically up-to-date and show the risk currently faced by a system in real-time. The concepts are applied to and validated in the context of the smart grid infrastructure currently being deployed in Luxembourg.", "num_citations": "3\n", "authors": ["304"]}
{"title": "Towards a Full Support of Obligations In XACML\n", "abstract": " Policy-based systems rely on the separation of concerns, by implementing independently a software system and its associated security policy.               XACML (eXtensible Access Control Markup Language) proposes a conceptual architecture and a policy language to reflect this ideal design of policy-based systems.However, while rights are well-captured by authorizations, duties, also called obligations, are not well managed by XACML architecture. The current version of XACML lacks (1) well-defined syntax to express obligations and (2) an unified model to handle decision making w.r.t. obligation states and the history of obligations fulfillment/violation. In this work, we propose an extension of XACML reference model that integrates obligation states in the decision making process. We have extended XACML language and architecture for a better obligations support and have shown how obligations are\u00a0\u2026", "num_citations": "3\n", "authors": ["304"]}
{"title": "Empirical investigation of the web browser attack surface under cross-site scripting: An urgent need for systematic security regression testing\n", "abstract": " One of the major threats against web applications is Cross-Site Scripting (XSS). The final target of XSS attacks is the client running a particular web browser. During this last decade, several competing web browsers (IE, Netscape, Chrome, Firefox) have evolved to support new features. In this paper, we explore whether the evolution of web browsers is done using systematic security regression testing. Beginning with an analysis of their current exposure degree to XSS, we extend the empirical study to a decade of most popular web browser versions. We use XSS attack vectors as unit test cases and we propose a new method supported by a tool to address this XSS vector testing issue. The analysis on a decade releases of most popular web browsers including mobile ones shows an urgent need of XSS regression testing. We advocate the use of a shared security testing benchmark as a good practice and propose\u00a0\u2026", "num_citations": "3\n", "authors": ["304"]}
{"title": "Preventing overloading incidents on smart grids: A multiobjective combinatorial optimization approach\n", "abstract": " Cable overloading is one of the most critical disturbances that may occur in smart grids, as it can cause damage to the distribution power lines. Therefore, the circuits are protected by fuses so that, the overload could trip the fuse, opening the circuit, and stopping the flow and heating. However, sustained overloads, even if they are below the safety limits, could also damage the wires. To prevent overload, smart grid operators can switch the fuses on or off to protect the circuits, or remotely curtail the over-producing/over-consuming users. Nevertheless, making the most appropriate decision is a daunting decision-making task, notably due to contractual and technical obligations. In this paper, we define and formulate the overloading prevention problem as a Multiobjective Mixed Integer Quadratically Constrained Program. We also suggest a solution method using a combinatorial optimization approach with a state-of\u00a0\u2026", "num_citations": "2\n", "authors": ["304"]}
{"title": "Artificial mutation inspired hyper-heuristic for runtime usage of multi-objective algorithms\n", "abstract": " In the last years, multi-objective evolutionary algorithms (MOEA) have been applied to different software engineering problems where many conflicting objectives have to be optimized simultaneously. In theory, evolutionary algorithms feature a nice property for runtime optimization as they can provide a solution in any execution time. In practice, based on a Darwinian inspired natural selection, these evolutionary algorithms produce many deadborn solutions whose computation results in a computational resources wastage: natural selection is naturally slow. In this paper, we reconsider this founding analogy to accelerate convergence of MOEA, by looking at modern biology studies: artificial selection has been used to achieve an anticipated specific purpose instead of only relying on crossover and natural selection (i.e., Muller et al [18] research on artificial mutation of fruits with X-Ray). Putting aside the analogy with natural selection , the present paper proposes an hyper-heuristic for MOEA algorithms named Sputnik 1 that uses artificial selective mutation to improve the convergence speed of MOEA. Sputnik leverages the past history of mutation efficiency to select the most relevant mutations to perform. We evaluate Sputnik on a cloud-reasoning engine, which drives on-demand provisioning while considering conflicting performance and cost objectives. We have conducted experiments to highlight the significant performance improvement of Sputnik in terms of resolution time.", "num_citations": "2\n", "authors": ["304"]}
{"title": "Sputnik: Elitist Artifical Mutation Hyper-heuristic for Runtime Usage of Multi-objective Evolutionary Algorithms\n", "abstract": " In the last years, multi-objective evolutionary algorithms (MOEA) have been applied to different software engineering problems where many conflicting objectives have to be optimized simultaneously. In theory, evolutionary algorithms feature a nice property for runtime optimization as they can provide a solution in any execution time. In practice, based on a Darwinian inspired natural selection, these evolutionary algorithms produce many deadborn solutions whose computation results in a computational resources wastage: natural selection is naturally slow. In this paper, we reconsider this founding analogy to accelerate convergence of MOEA, by looking at modern biology studies: artificial selection has been used to achieve an anticipated specific purpose instead of only relying on crossover and natural selection (ie, Muller et al [18] research on artificial mutation of fruits with X-Ray). Putting aside the analogy with natural selection, the present paper proposes an hyper-heuristic for MOEA algorithms named Sputnik 1 that uses artificial selective mutation to improve the convergence speed of MOEA. Sputnik leverages the past history of mutation efficiency to select the most relevant mutations to perform. We evaluate Sputnik on a cloud-reasoning engine, which drives on-demand provisioning while considering conflicting performance and cost objectives. We have conducted experiments to highlight the significant performance improvement of Sputnik in terms of resolution time.", "num_citations": "2\n", "authors": ["304"]}
{"title": "R-core: a rule-based contextual reasoning platform for AmI\n", "abstract": " [en] In this paper we present R-CoRe; a rule-based contextual reasoning platform for Ambient Intelligence environments. R-CoRe integrates Contextual Defeasible Logic (CDL) and Kevoree, a component-based software platform for Dynamically Adaptive Systems. Previously, we explained how this integration enables to overcome several reasoning and technical issues that arise from the imperfect nature of context knowledge, the open and dynamic nature of Ambient Intelligence environments, and the restrictions of wireless communications. Here, we focus more on technical aspects related to the architecture of R-Core, and demonstrate its use in Ambient Assisted Living.", "num_citations": "2\n", "authors": ["304"]}
{"title": "Acquisition et analyse des exigences pour le d\u00e9veloppement logiciel: une approche dirig\u00e9e par les mod\u00e8les.\n", "abstract": " Dans cette th\u00e8se, nous nous int\u00e9ressons \u00e0 la d\u00e9finition d'une plate-forme industrielle favorisant une meilleure int\u00e9gration des techniques de v\u00e9rification et de validation des exigences au sein des processus de d\u00e9veloppement. Cette plate-forme, appel\u00e9e R2A (pour Requirements To Analysis) est enti\u00e8rement construite \u00e0 l'aide de technologies issues de l'ing\u00e9nierie dirig\u00e9e par les mod\u00e8les. Le c\u0153ur de la plate-forme est un processus de composition de mod\u00e8les \u00e0 deux niveaux de mod\u00e9lisation. Au niveau instance, il produit une sp\u00e9cification globale des exigences \u00e0 partir d'une collection de sp\u00e9cifications d'exigences partielles, h\u00e9t\u00e9rog\u00e8nes et potentiellement incoh\u00e9rentes. Au niveau de la conception de la plate-forme (niveau meta), il produit le formalisme interne de la plate-forme (types d'information pouvant \u00eatre captur\u00e9e et fonctionnalit\u00e9s support\u00e9es) \u00e0 partir de composants de conception embarquant des s\u00e9mantiques op\u00e9rationnelles, de composition et de d\u00e9ploiement. Ce processus favorise l'adaptabilit\u00e9 de la plate-forme \u00e0 des contextes industriels vari\u00e9s. L'obtention d'une sp\u00e9cification globale des exigences (i) autorise l'application des techniques modernes de v\u00e9rification et de validation pour la d\u00e9tection d'incoh\u00e9rences et (ii) favorise une approche de d\u00e9veloppement dirig\u00e9e par les mod\u00e8les (MDD) d\u00e8s les premi\u00e8res \u00e9tapes du d\u00e9veloppement logiciel (synchronisation exigences et artefacts de d\u00e9veloppement aval). Dans sa version actuelle, la plate-forme est sp\u00e9cialis\u00e9e pour le contexte industriel de France T\u00e9l\u00e9com. Elle supporte quatre langages de description des exigences : les diagrammes d'activit\u00e9 et de classes UML, un langage naturel\u00a0\u2026", "num_citations": "2\n", "authors": ["304"]}
{"title": "XML to manage source engineering in object-oriented development: An example\n", "abstract": " In software engineering, XML to date has mostly been used to support three sub-activities: documentation management, data interchange and lightweight data storage. In this position paper, we give an example of using XML technology as the infrastructure for the integrated management of all core software development information.For several years now we have been developing the concept of Design for Testability based on\" Design by Contract Approach\" and a self-documented and self-testable class model. Since two years we have proposed a master document type that captures all relevant information for this classes, ie documentation, contracts, tests, and so on. This document is defined by an XML DTD and we have tentatively named the resulting markup language OOPML: Object-Oriented Programming Markup Language. Currently, we are building a dedicated software development framework for several object-oriented languages based on this markup language.", "num_citations": "2\n", "authors": ["304"]}
{"title": "TRIDENT: A Three-Steps Strategy to Digitise an Industrial System for Stepping into Industry 4.0\n", "abstract": " Nowadays, industrial companies are engaging their global transition toward the fourth industrial revolution (the so-called Industry 4.0). The main objective is to increase the Overall Equipment Effectiveness (OEE), by collecting, storing and analysing production data. The challenge to be tackled is to collect and make available data from the production units in a real-time and standardised manner. This paper proposes a strategy to digitise an industrial system, that can be used regardless the industrial environment. This strategy is applied on a real case-study and deployed on an industrial assembly line. The evaluation has been led by measuring the performance of three standards (i.e. OPC-UA, MQTT and O-MI/O-DF) highlighted by both industrials and academics. The study points out the i) feasibility of applying our strategy and ii) the suitability of 2 out of 3 standards to meet the requirements (in particular, in terms of\u00a0\u2026", "num_citations": "1\n", "authors": ["304"]}
{"title": "Weaving rules into models@ run. time for embedded smart systems\n", "abstract": " Smart systems are characterised by their ability to analyse measured data in live and to react to changes according to expert rules. Therefore, such systems exploit appropriate data models together with actions, triggered by domain-related conditions. The challenge at hand is that smart systems usually need to process thousands of updates to detect which rules need to be triggered, often even on restricted hardware like a Raspberry Pi. Despite various approaches have been investigated to efficiently check conditions on data models, they either assume to fit into main memory or rely on high latency persistence storage systems that severely damage the reactivity of smart systems. To tackle this challenge, we propose a novel composition process, which weaves executable rules into a data model with lazy loading abilities. We quantitatively show, on a smart building case study, that our approach can handle, at low\u00a0\u2026", "num_citations": "1\n", "authors": ["304"]}
{"title": "Intra-query adaptivity for mapreduce query processing systems\n", "abstract": " MapReduce query processing systems translate a query statement into a query plan, consisting of a set of MapReduce jobs to be executed in distributed machines. During query translation, these query systems uniformly allocate computing resources to each job by delegating the same tuning to the entire query plan. However, jobs may implement their own collection of operators, which lead to different usage of computing resources. In this paper we propose an adaptive tuning mechanism that enables setting specific resources to each job within a query plan. Our adaptive mechanism relies on a data structure that maps jobs to tuning codes by analyzing source code and log files. This adaptive mechanism allows delegating specific resources to the query plan at runtime as the data structure hosts specific pre-computed tuning codes.", "num_citations": "1\n", "authors": ["304"]}
{"title": "A Survey of Formal Verification Techniques for Model Transformations: A Tridimensional Classification\n", "abstract": " A Survey of Formal Verification Techniques for Model Transformations: A Tridimensional Classification - Amrani Moussa Open Repository and Bibliography Login Home Help? EN FR University of Luxembourg Library You are here: ORBi lu Detailled reference Reference : A Survey of Formal Verification Techniques for Model Transformations: A Tridimensiona... Document type : Scientific journals : Article Discipline(s) : Engineering, computing & technology : Computer science To cite this reference: http://hdl.handle.net/10993/14627 Title : A Survey of Formal Verification Techniques for Model Transformations: A Tridimensional Classification Language : English Author, co-author : Amrani, Moussa mailto [University of Luxembourg > Interdisciplinary Centre for Security, Reliability and Trust (SNT) > >] L\u00facio, L\u00e9vi [] Selim, Gehan [] Combemale, Beno\u00eet [] Dingel, J\u00fcrgen [] Vangheluwe, Hans [] Le Traon, Yves mailto [University (\u2026", "num_citations": "1\n", "authors": ["304"]}
{"title": "A PEP-PDP Architecture to Monitor and Enforce Security Policies in Java Applications\n", "abstract": " Security of Java-based applications is crucial to many businesses today. In this paper, we propose an approach to completely automate the generation of a security architecture inside of a target Java application where advanced security policies can be enforced. Our approach combines the use of Aspect-Oriented Programming with the Policy Enforcement Point (PEP) - Policy Decision Point (PDP) paradigm and allows the runtime update of policies.", "num_citations": "1\n", "authors": ["304"]}
{"title": "User Data Confidentiality in an Orchestration of Web Services.\n", "abstract": " Web Services are currently the base of a lot a e-commerce applications. Nevertheless, the clients often use these services without knowing anything about their internals. Moreover, they have no clue about the use of their personal data inside the global applications. BPEL (Business Process Execution Language) is a programming language for orchestrating Web Services within Service-Oriented Architecture (SOA). As one feature of SOAs is the dynamic discovery of services actually used during execution, a BPEL user does not know prior to the execution how, and by who, the data he provides will be used. In this paper, we offer the opportunity to the user to specify constraints on the use of its personal data. To ensure the privacy of data at runtime, we define a distributed security policy model. This policy is configured at runtime by the user of the BPEL program. This policy is enforced within a BPEL interpreter, and ensures that no information flow can be produced from the user data to unauthorized services. However, the dynamic aspects of the web services lead to situations where the policy prohibits the nominal operation of the orchestration (eg, when using a service that is unknown by the user). To solve this problem, we propose to the user to dynamically permit exceptional unauthorized flows. In order to make its decision, the user is provided with all information necessary for decision-making. An implementation inside the Orchestra BPEL interpreter illustrates our approach and exhibits the CPU overhead induced by the security mechanisms.", "num_citations": "1\n", "authors": ["304"]}
{"title": "Protection des donn\u00e9es utilisateurs dans une orchestration de web-services\n", "abstract": " Archive ouverte HAL - Protection des donn\u00e9es utilisateurs dans une orchestration de Web-Services Acc\u00e9der directement au contenu Acc\u00e9der directement \u00e0 la navigation Toggle navigation CCSD HAL HAL HALSHS TEL M\u00e9diHAL Liste des portails AUR\u00e9HAL API Data Documentation Episciences.org Episciences.org Revues Documentation Sciencesconf.org Support hal Accueil D\u00e9p\u00f4t Consultation Les derniers d\u00e9p\u00f4ts Par type de publication Par discipline Par ann\u00e9e de publication Par structure de recherche Les portails de l'archive Recherche Documentation hal-00536660, version 1 Communication dans un congr\u00e8s Protection des donn\u00e9es utilisateurs dans une orchestration de Web-Services Thomas Demongeot 1 Eric Totel 2 Val\u00e9rie Viet Triem Tong 2 Yves Le Traon 3 D\u00e9tails 1 CELAR - Centre d'\u00c9lectronique de l'ARmement 2 SUPELEC-Campus Rennes 3 Uni.lu - Universit\u00e9 du Luxembourg Type de document : : [] \u2026", "num_citations": "1\n", "authors": ["304"]}
{"title": "Analyses Automatiques pour le Test de Programmes Orient\u00e9s Aspect\n", "abstract": " La programmation orient\u00e9e aspects s\u00e9pare les diff\u00e9rentes pr\u00e9occupations pour am\u00e9liorer la modularit\u00e9. Ce paradigme introduit de nouveaux d\u00e9fis pour le test. Afin de proposer des solutions efficaces \u00e0 ces probl\u00e8mes, nous avons \u00e9tudi\u00e9 l'utilisation d'AspectJ dans 38 projets libres. Ces observations ont r\u00e9v\u00e9l\u00e9 que les aspects sont peu utilis\u00e9, et avec pr\u00e9caution, et que la programmation orient\u00e9e aspect diminue la testabilit\u00e9 en augmentant le couplage. Nous avons d\u00e9velopp\u00e9 une analyse statique de l'impact des aspects sur les cas de test. Le but est de d\u00e9terminer statiquement quels sont les cas de test qui sont impact\u00e9s par l'introduction d'un aspect et qui doivent \u00eatre modifi\u00e9s pour prendre en compte les changements. Cette analyse outill\u00e9e, vise \u00e0 limiter le temps d'ex\u00e9cution des cas de test et permet au testeur de se concentrer sur les cas de test qui doivent \u00eatre modifi\u00e9s. Nous proposons une approche, impl\u00e9ment\u00e9e dans un outil nomm\u00e9 AdviceTracer, pour tester les expressions de point de coupe s\u00e9par\u00e9ment des greffons. Cet outil permet la d\u00e9finition d'un oracle sp\u00e9cifiquement adapt\u00e9 \u00e0 la d\u00e9tection d'erreurs dans un point de coupe. Ceci permet \u00e0 l'oracle de tester si un greffon a \u00e9t\u00e9 correctement tiss\u00e9, sans d\u00e9pendre du comportement du greffon. Dans le cadre de cette th\u00e8se nous avons eu besoin de diff\u00e9rents outils d'analyse. AjMetrics est un outil qui mesure diverses m\u00e9triques sur les programmes orient\u00e9s aspect en utilisant un formalisme. AjMutator est un outil d'analyse de mutations de programmes orient\u00e9s aspect, qui est capable d'ins\u00e9rer des erreurs dans les expressions de point de coupe.", "num_citations": "1\n", "authors": ["304"]}
{"title": "Chameleon: The Performance Tuning Tool for MapReduce Query Processing Systems\n", "abstract": " Chameleon is a tuning advisor to support performance tuning decision-making of MapReduce administrators and users. In MapReduce query processing, a query is translated into a set of jobs, ie, query plan. For administrators, Chameleon can be a powerful tool for observing query plan workloads and their impact in large-cluster machine setups in terms of computing resource consumptions. For users, Chameleon provides a set of functionalities to tune query plans and observe performance improvements while testing different tuning knobs. Chameleon embeds a tuning mechanism based on a hash index to map jobs to tuning knobs. The hash index allows users defining specific tuning knobs for jobs, while a clustering algorithm is responsible for finding jobs with similar resource consumptions to receive the same tuning. In this demonstration we outline the functionalities of Chameleon and allow users interacting with it by sending and tuning queries for auditing performance improvements.", "num_citations": "1\n", "authors": ["304"]}