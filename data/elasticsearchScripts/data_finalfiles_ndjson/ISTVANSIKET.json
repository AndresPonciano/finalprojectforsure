{"title": "Empirical validation of object-oriented metrics on open source software for fault prediction\n", "abstract": " Open source software systems are becoming increasingly important these days. Many companies are investing in open source projects and lots of them are also using such software in their own work. But, because open source software is often developed with a different management style than the industrial ones, the quality and reliability of the code needs to be studied. Hence, the characteristics of the source code of these projects need to be measured to obtain more information about it. This paper describes how we calculated the object-oriented metrics given by Chidamber and Kemerer to illustrate how fault-proneness detection of the source code of the open source Web and e-mail suite called Mozilla can be carried out. We checked the values obtained against the number of bugs found in its bug database - called Bugzilla - using regression and machine learning methods to validate the usefulness of these\u00a0\u2026", "num_citations": "1102\n", "authors": ["85"]}
{"title": "Extracting facts from open source software\n", "abstract": " Open source software systems are becoming increasingly important these days. Many companies are investing in open source projects and lots of them are also using such software in their own work. But because open source software is often developed without proper management, the quality and reliability of the code may be uncertain. The quality of the code needs to be measured and this can be done only with the help of proper tools. We describe a framework called Columbus with which we calculate the object oriented metrics validated by Basili et al. for illustrating how fault-proneness detection from the open source Web and e-mail suite called Mozilla can be done. We also compare the metrics of several versions of Mozilla to see how the predicted fault-proneness of the software system changed during its development. The Columbus framework has been further developed recently with a compiler wrapping\u00a0\u2026", "num_citations": "85\n", "authors": ["85"]}
{"title": "Qualitygate sourceaudit: A tool for assessing the technical quality of software\n", "abstract": " Software systems are evolving continuously in order to fulfill the ever-changing business needs. This endless modification, however, decreases the internal quality of the system over time. This phenomena is called software erosion, which results in higher development, testing, and operational costs. The SourceAudit tool presented in this paper helps managing the technical risks of software deterioration by allowing imme-diate, automatic, and objective assessment of software quality. By monitoring the high-level technical quality of systems it is possible to immediately perform the necessary steps needed to reduce the effects of software erosion, thus reaching higher maintainability and lower costs in the mid and long-term. The tool measures source code maintainability according to the ISO/IEC 25010 based probabilistic software maintainability model called ColumbusQM. It gives a holistic view on software quality\u00a0\u2026", "num_citations": "30\n", "authors": ["85"]}
{"title": "A public unified bug dataset for java\n", "abstract": " Background: Bug datasets have been created and used by many researchers to build bug prediction models.Aims: In this work we collected existing public bug datasets and unified their contents.Method: We considered 5 public datasets which adhered to all of our criteria. We also downloaded the corresponding source code for each system in the datasets and performed their source code analysis to obtain a common set of source code metrics. This way we produced a unified bug dataset at class and file level that is suitable for further research (eg to be used in the building of new bug prediction models). Furthermore, we compared the metric definitions and values of the different bug datasets.Results: We found that (i) the same metric abbreviation can have different definitions or metrics calculated in the same way can have different names,(ii) in some cases different tools give different values even if the metric\u00a0\u2026", "num_citations": "22\n", "authors": ["85"]}
{"title": "Source meter sonar qube plug-in\n", "abstract": " The SourceMeter Sonar Qube plug-in is an extension of Sonar Qube, an open-source platform for managing code quality made by Sonar Source S.A, Switzerland. The plug-in extends the built-in Java code analysis engine of Sonar Qube with Front End ART's high-end Java code analysis engine. Most of Sonar Qubes original analysis results are replaced (including the detected source code duplications), while the range of available analyses is extended with a number of additional metrics and issue detectors. Additionally, the plug-in offers new GUI features on the Sonar Qube dashboard and drilldown views, making the Sonar Qube user experience more comfortable and the work with the tool more productive.", "num_citations": "22\n", "authors": ["85"]}
{"title": "Effect of object oriented refactorings on testability, error proneness and other maintainability attributes\n", "abstract": " Refactoring (object-oriented) code aims to improve some of the quality attributes of the software system under maintenance. However, as any other changes to a system, refactoring actions may have side-effects too, which need to be taken carefully into account during their implementation. Consequences of refactoring are only moderately discussed in literature. In this work, we emphasize the importance of documenting such consequences, by reviewing relevant literature and giving our views on the topic. We do this in three steps: first, we investigate how high level quality attributes (including testability and other maintainability aspects based on the ISO 9126 standard and fault proneness) can be estimated based on measurable metrics in the code, like complexity, size, and coupling. In the second step, we examine what effect of individual refactoring techniques can have on such metrics. Finally, we combine these\u00a0\u2026", "num_citations": "15\n", "authors": ["85"]}
{"title": "Towards building method level maintainability models based on expert evaluations\n", "abstract": " The maintainability of software systems is getting more and more attention both from researchers and industrial experts. This is due to its direct impact on development costs and reliability of the software.             Many models exist for estimating maintainability by aggregating low level source code metrics. However, very few of them are able to predict the maintainability on method level; even fewer take subjective human opinions into consideration. In this paper we present a new approach to create method level maintainability prediction models based on human surveys using regression techniques.             We performed three different surveys and compared the derived prediction models. Our regression models were built based on approximately 150000 answers of 268 persons. These models were able to estimate the maintainability of methods with a 0.72 correlation and a 0.83 mean absolute error on a\u00a0\u2026", "num_citations": "12\n", "authors": ["85"]}
{"title": "Prediction models for performance, power, and energy efficiency of software executed on heterogeneous hardware\n", "abstract": " Heterogeneous computer environments are becoming commonplace so it is increasingly important to understand how and where we could execute a given algorithm the most efficiently. In this paper we propose a methodology that uses both static source code metrics, and dynamic execution time, power, and energy measurements to build gain ratio prediction models. These models are trained on special benchmarks that have both sequential and parallel implementations and can be executed on various computing elements, e.g., on CPUs, GPUs, or FPGAs. After they are built, however, they can be applied to a new system using only the system\u2019s static source code metrics which are much more easily computable than any dynamic measurement. We found that while estimating a continuous gain ratio is a much harder problem, we could predict the gain category (e.g., \u201cslight improvement\u201d or \u201clarge\u00a0\u2026", "num_citations": "8\n", "authors": ["85"]}
{"title": "A public unified bug dataset for java and its assessment regarding metrics and bug prediction\n", "abstract": " Bug datasets have been created and used by many researchers to build and validate novel bug prediction models. In this work, our aim is to collect existing public source code metric-based bug datasets and unify their contents. Furthermore, we wish to assess the plethora of collected metrics and the capabilities of the unified bug dataset in bug prediction. We considered 5 public datasets and we downloaded the corresponding source code for each system in the datasets and performed source code analysis to obtain a common set of source code metrics. This way, we produced a unified bug dataset at class and file level as well. We investigated the diversion of metric definitions and values of the different bug datasets. Finally, we used a decision tree algorithm to show the capabilities of the dataset in bug prediction. We found that there are statistically significant differences in the values of the original and\u00a0\u2026", "num_citations": "5\n", "authors": ["85"]}
{"title": "Evaluating the effectiveness of object-oriented metrics for bug prediction\n", "abstract": " In our experiments we examined the general relationship between object-oriented metrics and the fault-proneness of classes. We analyzed a large open-source program called Mozilla, calculated 58 object-oriented metrics for Mozilla at the class level\\cite FSG04, collected the reported and corrected bugs from the bug tracking system of Mozilla and associated them with the classes. We applied logistic regression to examine which metrics could be used to predict the fault proneness of the classes. We found that 17 of the 58 object-oriented metrics were useful predictors, but to a different extent. The CBO (Coupling Between Object classes) metric was the best, but it was only slightly better than NOI (Number of Outgoing Invocations) and RFC (Response Set for a Class), which proved useful as well. We also examined the metrics in terms of their categories and we found that coupling metrics were the best predictors for finding bugs, but the complexity and size metrics also gave good results. On the other hand, in tests all the inheritance-related metrics were statistically insignificant.", "num_citations": "3\n", "authors": ["85"]}
{"title": "Challenges of SonarQube Plug-In Maintenance\n", "abstract": " The SONARQUBE TM  platform is a widely used open-source tool for continuous code quality management. It provides an API to extend the platform with plug-ins to upload additional data or to enrich its functionalities. The SourceMeter plug-in for SONARQUBE TM  platform integrates the SourceMeter static source code analyzer tool into the SONARQUBE TM  platform, i.e., uploads the analysis results and extends the GUI to be able to present the new results. The first version of the plug-in was released in 2015 and was compatible with the corresponding SONARQUBE TM  version. However, the platform - and what is more important, its API - have evolved a lot since then, therefore the plug-in had to be adapted to the new API. It was not just a slight adjustment, though, because we had to redesign and reimplement the whole UI and, at the same time, perform significant alterations in other parts of the plug-in as well\u00a0\u2026", "num_citations": "2\n", "authors": ["85"]}
{"title": "Employing Partial Least Squares Regression with Discriminant Analysis for Bug Prediction\n", "abstract": " Forecasting defect proneness of source code has long been a major research concern. Having an estimation of those parts of a software system that most likely contain bugs may help focus testing efforts, reduce costs, and improve product quality. Many prediction models and approaches have been introduced during the past decades that try to forecast bugged code elements based on static source code metrics, change and history metrics, or both. However, there is still no universal best solution to this problem, as most suitable features and models vary from dataset to dataset and depend on the context in which we use them. Therefore, novel approaches and further studies on this topic are highly necessary. In this paper, we employ a chemometric approach - Partial Least Squares with Discriminant Analysis (PLS-DA) - for predicting bug prone Classes in Java programs using static source code metrics. To our best knowledge, PLS-DA has never been used before as a statistical approach in the software maintenance domain for predicting software errors. In addition, we have used rigorous statistical treatments including bootstrap resampling and randomization (permutation) test, and evaluation for representing the software engineering results. We show that our PLS-DA based prediction model achieves superior performances compared to the state-of-the-art approaches (i.e. F-measure of 0.44-0.47 at 90% confidence level) when no data re-sampling applied and comparable to others when applying up-sampling on the largest open bug dataset, while training the model is significantly faster, thus finding optimal parameters is much easier. In terms\u00a0\u2026", "num_citations": "1\n", "authors": ["85"]}
{"title": "Systematic comparison of six open-source Java call graph construction tools\n", "abstract": " Call graphs provide the groundwork for numerous analysis algorithms and tools. However, in practice, their construction may have several ambiguities, especially for object-oriented programming languages like Java. The characteristics of the call graphs\u2013which are influenced by building requirements such as scalability, efficiency, completeness, and precision\u2013can greatly affect the output of the algorithms utilizing them. Therefore, it is important for developers to know a well-defined set of criteria based on which they can choose the most appropriate call graph builder tool for their static analysis applications. In this paper, we studied and compared six static call graph creator tools for Java. Our aim was to identify linguistic and technical properties that might induce differences in the generated call graphs besides the obvious differences caused by the various call graph construction algorithms. We evaluated the tools on multiple real-life open-source Java systems and performed a quantitative and qualitative assessment of the resulting graphs. We have shown how different outputs could be generated by the different tools. By manually analyzing the differences found on larger programs, we also found differences that we did not expect based on our preliminary assumptions.", "num_citations": "1\n", "authors": ["85"]}
{"title": "Code clones: good, bad, or ugly?\n", "abstract": " Code cloning is often considered as a bad practice in programming, especially from the point of view of maintenance. In this paper we present the results of an empirical study which revealed inverse correspondence between cloning and coupling. Since low coupling is conventionally regarded favourable for maintenance, this finding appears slightly suprising at first sight. In this paper we study the possible benefits of code cloning. Based on a theoretical structural analysis of the effect of cloning, we draw conclusions on the actual, restricted ways cloning is applied in practice.", "num_citations": "1\n", "authors": ["85"]}
{"title": "Szoftver term\u00e9k metrik\u00e1k alkalmaz\u00e1sa a szoftverkarbantart\u00e1s ter\u00fclet\u00e9n\n", "abstract": " Szoftver term\u00e9k metrik\u00e1k alkalmaz\u00e1sa a szoftverkarbantart\u00e1s ter\u00fclet\u00e9n - SZTE Doktori Repozit\u00f3rium SZTE Contenta Nyit\u00f3lap A repozit\u00f3riumr\u00f3l B\u00f6ng\u00e9sz\u00e9s, \u00c9v B\u00f6ng\u00e9sz\u00e9s, Tudom\u00e1nyter\u00fcletek B\u00f6ng\u00e9sz\u00e9s, Doktori iskola B\u00f6ng\u00e9sz\u00e9s, Szerz\u0151 Bejelentkez\u00e9s adatfelt\u00f6lt\u00e9shez Regisztr\u00e1ci\u00f3 adatfelt\u00f6lt\u00e9shez English Szoftver term\u00e9k metrik\u00e1k alkalmaz\u00e1sa a szoftverkarbantart\u00e1s ter\u00fclet\u00e9n Siket, Istv\u00e1n Szoftver term\u00e9k metrik\u00e1k alkalmaz\u00e1sa a szoftverkarbantart\u00e1s ter\u00fclet\u00e9n. [Disszert\u00e1ci\u00f3] (K\u00e9ziratban) [img] El\u0151n\u00e9zet PDF (disszert\u00e1ci\u00f3) Download (2MB) | El\u0151n\u00e9zet [img] El\u0151n\u00e9zet PDF (t\u00e9zis) Download (356kB) | El\u0151n\u00e9zet [img] El\u0151n\u00e9zet PDF (t\u00e9zis) Download (98kB) | El\u0151n\u00e9zet [img] PDF (mell\u00e9klet) Restricted to Csak az arch\u00edvum karbantart\u00f3ja Download (2MB) M\u0171 t\u00edpusa: Disszert\u00e1ci\u00f3 (Doktori \u00e9rtekez\u00e9s) Publik\u00e1ci\u00f3ban haszn\u00e1lt n\u00e9v: Siket, Istv\u00e1n Idegen nyelv\u0171 c\u00edm: Applying Software Product Metrics in Software Maintenance Doktori iskola(\u2026", "num_citations": "1\n", "authors": ["85"]}