{"title": "Time-dependent error-detection rate model for software reliability and other performance measures\n", "abstract": " This paper presents a stochastic model for the software failure phenomenon based on a nonhomogeneous Poisson process (NHPP). The failure process is analyzed to develop a suitable meanvalue function for the NHPP; expressions are given for several performance measures. Actual software failure data are analyzed and compared with a previous analysis.", "num_citations": "2425\n", "authors": ["454"]}
{"title": "Software reliability models: Assumptions, limitations, and applicability\n", "abstract": " A number of analytical models have been proposed during the past 15 years for assessing the reliability of a software system. In this paper we present an overview of the key modeling approaches, provide a critical analysis of the underlying assumptions, and assess the limitations and applicability of these models during the software development cycle. We also propose a step-by-step procedure for fitting a model and illustrate it via an analysis of failure data from a medium-sized real-time command and control software system.", "num_citations": "1219\n", "authors": ["454"]}
{"title": "Introduction to mechanics and symmetry\n", "abstract": " Symmetry and mechanics have been close partners since the time of the founding masters: Newton, Euler, Lagrange, Laplace, Poisson, Jacobi, Hamilton, Kelvin, Routh, Riemann, Noether, Poincar\u00e9, Einstein, Schr\u00f6dinger, Cartan, Dirac, and to this day, symmetry has continued to play a strong role, especially with the modern work of Kolmogorov, Arnold, Moser, Kirillov, Kostant, Smale, Souriau, Guillemin, Sternberg, and many others. This book is about these developments, with an emphasis on concrete applications that we hope will make it accessible to a wide variety of readers, especially senior undergraduate and graduate students in science and engineering.The geometric point of view in mechanics combined with solid analysis has been a phenomenal success in linking various diverse areas, both within and across standard disciplinary lines. It has provided both insight into fundamental issues in mechanics (such as variational and Hamiltonian structures in continuum mechanics, fluid mechanics, and plasma physics) and provided useful tools in specific models such as new stability and bifurcation criteria using the energy-Casimir and energy-momentum methods, new numerical codes based on geometrically exact update procedures and variational integrators, and new reorientation techniques in control theory and robotics.", "num_citations": "820\n", "authors": ["454"]}
{"title": "Optimum release time for software systems based on reliability and cost criteria\n", "abstract": " The problem of determining the optimum time when testing can stop and the system can be considered ready for operational use is considered. This decision, of course, depends on the model for the software failure phenomenon and the criterion used for evaluating system readiness. With the time dependent error detection rate model of Goel and Okumoto, two criteria are investigated: software reliability and total expected cost. Based on the cost criterion, an optimum release policy is derived and its sensitivity to the model parameters is studied. The results are illustrated by numerical examples.", "num_citations": "359\n", "authors": ["454"]}
{"title": "Determination of ARL and a contour nomogram for CUSUM charts to control normal mean\n", "abstract": " The Average Run Length of a Cusum chart for controlling a normal mean is calculated by solving the systems of linear equations which approximate the integral equations for the required quantities. The accuracy of approximation by this method is numerically evaluated and the results are compared with those obtained by other approximate methods. The construction and use of a new nomogram based on the contours of Average Run Lengths La . and Lr  drawn in the h\u221an/\u03c3\u2014|\u03bc \u2013 k|\u221an/\u03c3 plane is discussed. Numerical examples are given to illustrate the flexibility and convenience provided by this nomogram in the design of Cusum charts.", "num_citations": "190\n", "authors": ["454"]}
{"title": "An algorithm for the determination of the economic design of-charts based on Duncan's model\n", "abstract": " An algorithm for the determination of the economic design of -charts based on Duncan's model is described in this paper. This algorithm consists of solving an implicit equation in design variables n (sample size) and k (control limit factor) and an explicit equation for h (sampling interval). The use of this algorithm not only yields the exact optimum but also provides valuable information so that the sensitivity of the optimum loss-cost (L*) can be evaluated. Loss-cost contours are used to discuss the nature of the loss-cost surface and the effect of the design variables. The effect of two parameters, the delay factor (e), and the average time for an assignable cause to occur (1/\u03bb), on the optimum design is evaluated. Numerical examples are used for illustrations.", "num_citations": "182\n", "authors": ["454"]}
{"title": "Empirical data modeling in software engineering using radial basis functions\n", "abstract": " Many empirical studies in software engineering involve relationships between various process and product characteristics derived via linear regression analysis. We propose an alternative modeling approach using radial basis functions (RBFs) which provide a flexible way to generalize linear regression function. Further, RBF models possess strong mathematical properties of universal and best approximation. We present an objective modeling methodology for determining model parameters using our recent SG algorithm, followed by a model selection procedure based on generalization ability. Finally, we describe a detailed RBF modeling study for software effort estimation using a well-known NASA dataset.", "num_citations": "169\n", "authors": ["454"]}
{"title": "An analysis of recurrent software errors in a real-time control system\n", "abstract": " This paper presents an analysis of actual software error data from a large-scale software project using the imperfect debugging model proposed by Goel and Okumoto [1]. The model parameters are estimated from the actual data and the values predicted from the model are compared with the observed values. Joint confidence regions for the parameters are also constructed which permit a study of the sensitivity of predictions.", "num_citations": "121\n", "authors": ["454"]}
{"title": "A Markovian model for reliability and other performance measures of software systems*\n", "abstract": " Several studies have been undertaken in recent years to investigate the software failure phenomenon, with the objective of developing analytical models for quantitative assessment of software performance. Most of these studies assume that the times between software failures follow an exponential distribution with a failure rate that depends on the number of errors in the system (see, for example, Jelinski and Moranda, 5 Littlewood and Verrall 6 and Shooman 11 ). A key assumption made in most of these studies is that the errors are removed with certainty when detected. However, as pointed out in Miyamoto 7 and Thayer et al., 12 errors are not always corrected when detected. The existing models do not provide a solution for such situations.", "num_citations": "115\n", "authors": ["454"]}
{"title": "Economically optimum design of CUSUM charts\n", "abstract": " We present a procedure for the economic design of Cusum charts to control the mean of a process with a normally distributed quality characteristic. A model is derived which gives the long-ran average cost as a function of both the design parameters of the chart and the cost and risk factors associated with the process. The \u201cpattern-search\u201d technique is employed to determine the optimum values of the sample size, the sampling interval and the decision limit. The cost surfaces are investigated, numerically, to study the effects of changes in the design parameters and in some of the cost and risk factors.", "num_citations": "109\n", "authors": ["454"]}
{"title": "Nonparametric (distribution\u2010free) quality control charts\n", "abstract": " Control charts that are typically based on the assumption of a specific form of a parametric distribution, such as the normal, are called parametric control charts. In many applications, however, there is not enough information to justify this assumption and control charts that do not depend on a particular distributional assumption are desirable. Nonparametric or distribution\u2010free control charts can serve this broader purpose. A key advantage of nonparametric charts is that its in\u2010control run length distribution is the same for all continuous process distributions. This means, for example, that the false alarm rate and the in\u2010control average run length of a nonparametric chart is the same for all continuous distributions. This is not true for parametric control charts in general and consequently their in\u2010control robustness can be a legitimate concern. Nonparametric charts are often more robust and efficient than their normal\u00a0\u2026", "num_citations": "80\n", "authors": ["454"]}
{"title": "Software error detection model with applications\n", "abstract": " This paper deals with the modeling of software errors encountered in a small and a large software system. A deterministic analysis of software failure process is presented to obtain an appropriate mean value function for a nonhomogeneous Poisson process. Several quantitative measures for software quality assessment are also proposed. Statistical techniques of inference about unknown parameters are discussed and detailed analyses of software error data from two systems are presented.", "num_citations": "59\n", "authors": ["454"]}
{"title": "Testing Object-Oriented Software: Life Cicle Solutionss\n", "abstract": " The rise of object-oriented (00) software development seems to have helped improve software quality, but it has not, of course, eliminated all possibility of error. Thus, software testing is at least as important today as it ever has been. Testing, as a centerpiece of quality assurance efforts, only increases in value as society becomes more and more reliant on software. Testing 00 components and even 00 systems is not very difficult once you know how to do it. But until now there have been few accounts of how to do it well and none that adequately characterize testing across the life cycle of large 00 software production efforts. This book combines insights from research on 00 testing with insights from industrial testing efforts to produce an account that should be valuable to anyone interested in the theory and practice of testing 00 software. This book provides extensive coverage of testing methods applicable to 00 software development, as well as discussions of underlying concepts and technical underpinnings that enable you to devise additional techniques of your own. It is unlikely that you will apply every test, process, review criterion, or metric described in this book to your software project. If you have a small project, it is unlikely that you will apply more than a few of them. But the breadth of coverage allows you to select and customize them with full knowledge of the alternatives and of the options available if you need to extend testing efforts.", "num_citations": "58\n", "authors": ["454"]}
{"title": "Models for hardware-software system operational-performance evaluation\n", "abstract": " Stochastic models for hardware-software systems are developed and used to study their performance as a function of hardware-software failure and maintenance rates. Expressions are derived for the distribution of time to a specified number of software errors, system occupancy probabilities, system reliability, availability, and average availability. The behavior of these measures is investigated via numerical examples.", "num_citations": "55\n", "authors": ["454"]}
{"title": "A Guidebook for Software Reliability Assessment.\n", "abstract": " The purpose of this guidebook is to provide state-of-the-art information about the selection and use of existing software reliability models. Towards this objective, we have presented a brief summary of the available models backed by a detailed discussion of most of the models in the appendices One of the difficulties in choosing a model is to find a match between the testing environment and a class of models. To help a user in this process, we have presented a detailed discussion of most of the assumptions that characterize the various software reliability models. The process of developing a model has been explained in detail and illustrated via numerical examples. AuthorDescriptors:", "num_citations": "49\n", "authors": ["454"]}
{"title": "A time dependent error detection rate model for a large scale software system\n", "abstract": " CiNii \u8ad6\u6587 - A time dependent error detection rate model for a large scale software system CiNii \u56fd\u7acb\u60c5\u5831\u5b66\u7814\u7a76\u6240 \u5b66\u8853\u60c5\u5831\u30ca\u30d3\u30b2\u30fc\u30bf[\u30b5\u30a4\u30cb\u30a3] \u65e5\u672c\u306e\u8ad6\u6587\u3092\u3055\u304c\u3059 \u5927\u5b66\u56f3\u66f8\u9928\u306e\u672c\u3092\u3055\u304c\u3059 \u65e5\u672c\u306e\u535a\u58eb\u8ad6\u6587\u3092\u3055\u304c\u3059 \u65b0\u898f\u767b\u9332 \u30ed\u30b0\u30a4\u30f3 English \u691c\u7d22 \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u9589\u3058\u308b \u30bf\u30a4\u30c8\u30eb \u8457\u8005\u540d \u8457\u8005ID \u8457\u8005\u6240\u5c5e \u520a\u884c\u7269\u540d ISSN \u5dfb\u53f7\u30da\u30fc\u30b8 \u51fa\u7248\u8005 \u53c2\u8003\u6587\u732e \u51fa\u7248\u5e74 \u5e74\u304b\u3089 \u5e74 \u307e\u3067 \u691c\u7d22 \u691c\u7d22 \u691c\u7d22 CiNii\u7a93\u53e3\u696d\u52d9\u306e\u518d\u958b\u306b\u3064\u3044\u3066 A time dependent error detection rate model for a large scale software system GOEL AL \u88ab\u5f15\u7528\u6587\u732e: 1\u4ef6 \u8457\u8005 GOEL AL \u53ce\u9332\u520a\u884c\u7269 Proc.3rd USA-Japan Computer Conference Proc.3rd USA-Japan Computer Conference, 35-40, 1978 \u88ab \u5f15\u7528\u6587\u732e: 1\u4ef6\u4e2d 1-1\u4ef6\u3092 \u8868\u793a 1 \u30c6\u30b9\u30c8\u7fd2\u719f\u6027\u3092\u8003\u616e\u3057\u305f\u30bd\u30d5\u30c8\u30a6\u30a7\u30a2\u4fe1\u983c\u5ea6\u6210\u9577\u30e2\u30c7\u30eb\u3068\u305d\u306e\u9069\u5408\u6027 \u8a55\u4fa1\u306b\u95a2\u3059\u308b\u8003\u5bdf \u85e4\u539f \u9686\u6b21 , \u5c71\u7530 \u8302 \u96fb\u5b50\u60c5\u5831\u901a\u4fe1\u5b66\u4f1a\u8ad6\u6587\u8a8c. A, \u57fa\u790e\u30fb\u5883\u754c 00083(00002), 188-195, 2000-02-25 \u53c2\u8003\u6587\u732e6\u4ef6 \u88ab\u5f15\u7528\u6587\u732e5\u4ef6 Tweet \u5404\u7a2e\u30b3\u30fc\u30c9 NII\u8ad6\u6587ID(NAID) \u8cc7\u6599\u2026", "num_citations": "45\n", "authors": ["454"]}
{"title": "Availability and other performance measures of software systems under imperfect maintenance\n", "abstract": " In this paper we develop a model for the operational phase of a software system which incorporates the uncertainty of error removal and the time spent in correcting errors. Expressions for various measures of software performance, e.g. distribution of time to a specified number of remaining errors, the expected number of errors detected and corrected in time t, and software system availability are derived. Numerical examples are used to illustrate these results.", "num_citations": "38\n", "authors": ["454"]}
{"title": "Time-dependent error-detection rate model for software and other performance measures\n", "abstract": " CiNii \u8ad6\u6587 - Time-dependent error-detection rate model for software and other performance measures CiNii \u56fd\u7acb\u60c5\u5831\u5b66\u7814\u7a76\u6240 \u5b66\u8853\u60c5\u5831\u30ca\u30d3\u30b2\u30fc\u30bf[\u30b5\u30a4\u30cb\u30a3] \u65e5\u672c\u306e\u8ad6\u6587\u3092\u3055\u304c\u3059 \u5927\u5b66 \u56f3\u66f8\u9928\u306e\u672c\u3092\u3055\u304c\u3059 \u65e5\u672c\u306e\u535a\u58eb\u8ad6\u6587\u3092\u3055\u304c\u3059 \u65b0\u898f\u767b\u9332 \u30ed\u30b0\u30a4\u30f3 English \u691c\u7d22 \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u9589\u3058\u308b \u30bf\u30a4\u30c8\u30eb \u8457\u8005\u540d \u8457\u8005ID \u8457\u8005\u6240\u5c5e \u520a\u884c\u7269\u540d ISSN \u5dfb\u53f7\u30da\u30fc\u30b8 \u51fa\u7248\u8005 \u53c2\u8003 \u6587\u732e \u51fa\u7248\u5e74 \u5e74\u304b\u3089 \u5e74\u307e\u3067 \u691c\u7d22 \u691c\u7d22 \u691c\u7d22 CiNii\u7a93\u53e3\u696d\u52d9\u306e\u518d\u958b\u306b\u3064\u3044\u3066 Time-dependent error-detection rate model for software and other performance measures GOEL AL \u88ab\u5f15\u7528\u6587\u732e: 1\u4ef6 \u8457\u8005 GOEL AL \u53ce\u9332\u520a\u884c\u7269 IEEE Transactions on Reliability IEEE Transactions on Reliability R-28(3), 465-484, 1979 \u88ab\u5f15\u7528\u6587\u732e: 1\u4ef6\u4e2d 1-1\u4ef6\u3092 \u8868\u793a 1 \u30de\u30eb\u30c1\u30b5\u30a4\u30c8\u306b\u304a\u3051\u308b\u30bd\u30d5\u30c8\u30a6\u30a7\u30a2\u306e\u904b\u7528\u4fe1\u983c\u5ea6 \u30e2\u30c7\u30eb \u677e\u5c3e\u8c37 \u5fb9 \u96fb\u5b50\u60c5\u5831\u901a\u4fe1\u5b66\u4f1a\u6280\u8853\u7814\u7a76\u5831\u544a. CS, \u901a\u4fe1\u65b9\u5f0f 99(621), 7-12, 2000-02-18 \u53c2\u8003 \u6587\u732e13\u4ef6 Tweet \u5404\u7a2e\u30b3\u30fc\u30c9 NII\u8ad6\u6587ID(NAID) 10022242021 \u8cc7\u6599\u7a2e\u5225 \u96d1\u8a8c\u8ad6\u6587 \u30c7\u30fc\u30bf\u63d0\u4f9b\u5143 /(\u2026", "num_citations": "36\n", "authors": ["454"]}
{"title": "Gaussian radial basis functions for simulation metamodeling\n", "abstract": " The paper presents a novel approach for developing simulation metamodels using Gaussian radial basis functions. This approach is based on some recently developed mathematical results for radial basis functions. It is systematic, explicitly controls the underfitting and overfitting tradeoff, and uses a fast computational algorithm that requires minimal human involvement. This approach is illustrated by developing metamodels for the M/M/1 queueing system.", "num_citations": "35\n", "authors": ["454"]}
{"title": "A comparative and economic investigation of X and cumulative sum control charts\n", "abstract": " 5.1 Design Based on ARL Criterion 5.1. 1 Ewan and Kemp Nomogram 5.1. 2 The Contour Nomogram 5.2 Design Based on Cost Criterion 5.2. 1 Derivation of the Cost Model 5.2. 2\" Direct Search Solution 5.2. 3 An Example 5.3 Analysis of Minimum Cost Designs 5.3. 1 Design Variables and Loss-Cost Surfaces 5.3. 2 Effect of Shift Parameter, 6", "num_citations": "33\n", "authors": ["454"]}
{"title": "Software Reliability Modelling and Estimation Techniques.\n", "abstract": " This report presents the results of the software reliability modelling and estimation research. Two new models of very general applicability are introduced and the necessary mathematical and practical details are developed in this report. A new methodology for determining when to stop testing and start using software is described and developed. A new model for analyzing the operational performance of a combined hardware-software system is reported even though it was not a part of the original research plan.Descriptors:", "num_citations": "30\n", "authors": ["454"]}
{"title": "When to stop testing and start using software?\n", "abstract": " During the last decade, numerous studies have been undertaken to quantify the failure process of large scale software systems. (see for example, references 1-12.) An important objective of these studies is to predict software performance and use the information for decision making. An important decision of practical concern is the determination of the amount of time that should be spent in testing. This decision of course will depend on the model used for describing the failure phenomenon and the criterion used for determining system readiness. In this paper we present a cost model based on the time dependent fault detection rate model of Goel and Okumoto (4,5) and describe a policy that yields the optimal value of test time T.", "num_citations": "28\n", "authors": ["454"]}
{"title": "Cumulative sum control charts\n", "abstract": " The classical control chart* procedures as proposed by Shewhart (39) are based on a one-point rule where the process is said to be out of control if the last plotted point falls outside the control limits*. Such procedures are equivalent to the repeated application of the fixed sample-size test. The advantages of these charts are their simplicity and their ability to detect large changes quickly. The main disadvantage is that they are slow in signaling small or moderate changes. Several modifications to the one-point rule have been proposed to overcome this disadvantage and to improve the performance of these procedures.A fundamental change in the classical procedure was proposed by Page (30), who suggested constructing control charts based on sums of observations rather than individual observations. This system of charting takes full advantage of the historical record and provides a rapid means of detecting\u00a0\u2026", "num_citations": "25\n", "authors": ["454"]}
{"title": "A note on availability for a finite interval\n", "abstract": " A definition of availability having a probabilistic guarantee for a finite interval is made. Examples and asymptotic solution are presented.", "num_citations": "24\n", "authors": ["454"]}
{"title": "A Summary of the Discussion on\" An Analysis of Competing Software Reliability Models\"\n", "abstract": " In March 1978, Schick and Wolverton published a paper [81 in the IEEE TRANSACTIONS ON SOFTWARE ENGINEERING. Moranda [6] criticized several aspects of this paper. His critique was reviewed by Littlewood [31 and rebutted by Schick and Wolverton [9]. The purpose of this note is to summarize and comment on the main points raised in [61,[81, and [9].SCHICK AND WOLVERTON PAPER In this paper [81 the authors evaluated the strengths and weaknesses of several software reliability models. In par-ticular, they discussed 1) the Schick and Wolverton model (SW model), which was first introduced in [7], 2) the Jelinski and Moranda model (JM model)[21, and 3) the Weibull model proposed by Wagoner [121. The SW model for the error-time distribution is based on a discretely decreasing hazard function z (tg) as a linear func-", "num_citations": "21\n", "authors": ["454"]}
{"title": "Screening of Prosopis Germplasm for Afforestation of Degraded Soil Sites: Performance, Leaf Nutrient Status and Influence on Soil Properties\n", "abstract": " Performance of nine species of Prosopiswas evaluated in species site interaction trials in order to identify promising species for afforestation of alkaline soil site (pH > 9). Eight of these species (P affinis, P. chilensis, P. flexuosus, P. glandulosa, P. lampa, P. pallida, P. siliquastrumand P. tamarugo)were exotic while one (P. juliflora)was a local species. Differences in leaf trait and thorn characters among species were well marked. Out of the nine species, only five had thorn-less plants. Their frequency varied from 5 to 51% with maximum in P. siliquastrum. P. tamarugocould not survive in the field and most of its plants died during the first year itself. P. affinisand P. pallida, both from Peru, showed outstanding performance and tolerance to soil sod-icity. P. affiniswas found to be significantly superior (P < 0.05, 0.01) among all the species investigated while P. pallidaranked next to it. P. juliflorawas moderate in its\u00a0\u2026", "num_citations": "18\n", "authors": ["454"]}
{"title": "A Time Dependent Error Detection Rate Model for Software Performance Assessment with Applications.\n", "abstract": " The objective of this study was to develop a parsimonious model whose parameters have a physical interpretation, and which can be used to predict various quantitative measures for software performance assessment. With this objective, the behavior of the software failure counting process Nt, t equal to or greater than zero has been studied. It is shown that Nt can be well described by a non-homogeneous Poisson process NHPP with a two parameter exponentially decaying error detection rate. Several measures, such as the number of failures by some prespecified time, the number of errors remaining in the system at a future time, and software reliability during a mission, have been proposed in this report. Models for software performance assessment are also derived. Two methods are developed to estimate model parameters from either failure count data or times between failures. A goodness-of-fit test is also developed to check the adequacy of the fitted model. Finally, actual failure data are analyzed from two DOD software systems. One is a large command and control system and the other a Naval data analysis system. AuthorDescriptors:", "num_citations": "18\n", "authors": ["454"]}
{"title": "Optimum release time for software systems\n", "abstract": " This paper deals with the problem of determining the optimum time when testing can stop and the system be considered ready for operational use. This decision, of course, depends on the model for the software failure phenomenon and the criterion used for evaluating system readiness. Using the non-homogeneous Poisson process model of Goel and Okumoto [4], two criteria are investigated, viz the software reliability and total expected cost. A simple cost model is developed and the optimum release policy, based on cost criterion, is derived. Numerical examples are used to illustrate the results.", "num_citations": "17\n", "authors": ["454"]}
{"title": "A guidebook for software reliability assessment\n", "abstract": " A guidebook for software reliability assessment with emphasis on the initial version which will serve as an interim guide and on the nonhomogeneous Poisson process software reliability prediction model is presented. The guidebook will specify procedures for a project type guidance such as avionics, command, and control, and will indicate how to organize the collected software data for model input as a function of the model(s) selected. The nonhomogeneous Poisson process model is described, noting the expressions for the cumulative number of software errors and software error data analyses. Finally, software project characteristics and a description of the data set are given, concluding that the widespread distribution of the draft handbooks will lead to a MIL-HDBK that will be widely used.", "num_citations": "14\n", "authors": ["454"]}
{"title": "Special issue on software reliability\n", "abstract": " CiNii \u8ad6\u6587 - Special issue on software reliability CiNii \u56fd\u7acb\u60c5\u5831\u5b66\u7814\u7a76\u6240 \u5b66\u8853\u60c5\u5831\u30ca\u30d3\u30b2\u30fc\u30bf[\u30b5\u30a4\u30cb\u30a3 ] \u65e5\u672c\u306e\u8ad6\u6587\u3092\u3055\u304c\u3059 \u5927\u5b66\u56f3\u66f8\u9928\u306e\u672c\u3092\u3055\u304c\u3059 \u65e5\u672c\u306e\u535a\u58eb\u8ad6\u6587\u3092\u3055\u304c\u3059 \u65b0\u898f\u767b\u9332 \u30ed\u30b0\u30a4\u30f3 English \u691c\u7d22 \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u9589\u3058\u308b \u30bf\u30a4\u30c8\u30eb \u8457\u8005\u540d \u8457\u8005ID \u8457\u8005\u6240\u5c5e \u520a\u884c\u7269\u540d ISSN \u5dfb\u53f7 \u30da\u30fc\u30b8 \u51fa\u7248\u8005 \u53c2\u8003\u6587\u732e \u51fa\u7248\u5e74 \u5e74\u304b\u3089 \u5e74\u307e\u3067 \u691c\u7d22 \u691c\u7d22 \u691c\u7d22 Special issue on software reliability GOEL AL \u88ab\u5f15\u7528\u6587\u732e: 1\u4ef6 \u8457\u8005 GOEL AL \u53ce\u9332\u520a\u884c\u7269 IEEE Trans. Software Engineering IEEE Trans. Software Engineering SE-11(12), 1409-1517, 1985 \u88ab\u5f15\u7528\u6587\u732e: 1\u4ef6\u4e2d 1-1\u4ef6\u3092 \u8868\u793a 1 \u969c\u5bb3\u306e\u5927\u304d\u3055\u3092\u8003\u616e\u3057\u305f\u30bd\u30d5\u30c8\u30a6\u30a7\u30a2\u306e\u4fe1\u983c\u6027\u5b9f\u8a3c\u8a66\u9a13 \u6fa4\u7530 \u6e05 , \u4e09\u9053 \u5f18\u660e \u96fb\u5b50\u60c5\u5831\u901a\u4fe1\u5b66\u4f1a \u8ad6\u6587\u8a8c. A, \u57fa\u790e\u30fb\u5883\u754c 00081(00001), 98-109, 1998-01-25 \u53c2\u8003\u6587\u732e14\u4ef6 \u88ab\u5f15\u7528\u6587\u732e3\u4ef6 Tweet \u5404\u7a2e\u30b3\u30fc\u30c9 NII\u8ad6\u6587ID(NAID) 10000062084 NII\u66f8\u8a8cID(NCID) AA00668120 \u8cc7\u6599\u7a2e\u5225 \u96d1\u8a8c \u8ad6\u6587 \u30c7\u30fc\u30bf\u63d0\u4f9b\u5143 CJP\u5f15\u7528 \u66f8\u304d\u51fa\u3057 RefWorks\u306b\u66f8\u304d\u51fa\u3057 EndNote\u306b\u66f8\u304d\u51fa\u3057 Mendeley\u306b/\u3067\u2026", "num_citations": "13\n", "authors": ["454"]}
{"title": "Software reliability and readiness assessment based on the non-homogeneous Poisson process\n", "abstract": " This chapter addresses the inten-elated issues of software reliability and readiness assessment based on open and closed software problem reports (SPRs). It describes a systematic methodology consisting of the following three steps: use of the Laplace trend statistic to determine reliability growth or decay, fitting non-homogeneous Poisson process (NHPP) models and reliability or readiness assessment. The mathematical framework pertinent to the Laplace statistic and the NHPP models is discussed at length. SPR data from commercial and military systems are used throughout the chapter for illustration and explanation.", "num_citations": "11\n", "authors": ["454"]}
{"title": "Interactive segmentation of medical images using belief propagation with level sets\n", "abstract": " In this paper, we propose an interactive segmentation method to apply user information during the segmentation of a specific anatomic structure. This method is formulated to use belief propagation to minimize a global cost function according to local level sets. The propagation starts with one user labeled point, and iteratively extends the user information from the labeled pixel to its neighborhood by calculating the beliefs of the pixels in the same level as the labeled pixel. Since the segmentation relies on both local user information and global image features, it is less interrupted by noise, and works well even the target is not obvious to its neighbor. The promising segmentation results also show that our method is robust to the objects with high shape variation and inhomogeneous intensity value appearance.", "num_citations": "10\n", "authors": ["454"]}
{"title": "Foreword Software Reliability\n", "abstract": " EADERS of this TRANSACTIONS are well aware of the mportance of software reliability. While they may differ about the ways to measure it or about the ways to achieve it, they would certainly agree about the need to ensure the presence of this attribute in a software system. The two parts of this special issue on Software Reliability (December 1985 and January 1986) address several'aspects relevant to this critical software characteristic. The current issue deals with software reliability estimation, new modeling techniques, management issues, testing and verification, and fault-tolerant programs. Software reliability can be viewed as the probability that a given program will operate correctly in a specified en-vironment for a specified duration. Pursuant to this view, several models have been proposed for estimating the re-liability of programs. These can be broadly categorized as software reliability growth models and\u00a0\u2026", "num_citations": "10\n", "authors": ["454"]}
{"title": "Testing C++ classes\n", "abstract": " We present a semi-exhaustive approach to testing of a C++ class as a unit of testing. The general approach is outlined by an algorithm. This approach determines the correctness of a class by considering the manipulation of each of its data members by all possible sequences of member functions of the class. Member functions manipulating a data member are divided into different categories and each category is tested individually. A preliminary analysis of the algorithm determines the maximum number of test cases required for a given class.", "num_citations": "9\n", "authors": ["454"]}
{"title": "Modeling software component criticality using a machine learning approach\n", "abstract": " During software development, early identification of critical components is of much practical significance since it facilitates allocation of adequate resources to these components in a timely fashion and thus enhance the quality of the delivered system. The purpose of this paper is to develop a classification model for evaluating the criticality of software components based on their software characteristics. In particular, we employ the radial basis function machine learning approach for model development where our new, innovative algebraic algorithm is used to determine the model parameters. For experiments, we used the USA-NASA metrics database that contains information about measurable features of software systems at the component level. Using our principled modeling methodology, we obtained parsimonious classification models with impressive performance that involve only design metrics\u00a0\u2026", "num_citations": "8\n", "authors": ["454"]}
{"title": "Knowledge discovery and validation in software metrics databases\n", "abstract": " The explosive growth of commercial and scientific databases has outpaced our ability to manually analyze and interpret this data. The newly emerging interdisciplinary field of knowledge discovery in databases (KDD), provides methodologies for seeking valuable and useful information from these databases. In this paper, we describe a methodology for identifying high fault modules in software metrics databases. It employs radial basis function model for the data mining phase of the KDD process based on our newly developed algorithm. We use the well-known bootstrap method for model validation and accuracy estimation of the classification task. As an example, a genuine problem from NASA software database is explored.", "num_citations": "8\n", "authors": ["454"]}
{"title": "Radial basis functions: an algebraic approach (with data mining applications)\n", "abstract": " Radial basis functions have now become a popular model for classification and prediction tasks. Most algorithms for their design, however, are basically iterative and lead to irreproducible results. In this tutorial, we present an innovative new approach (Shin-Goel algorithm) for the design and evaluation of the RBF model. It is based on purely algebraic concepts and yields reproducible designs. Use of this algorithm is demonstrated on some benchmark data sets, and data mining applications in software engineering and cancer class prediction are described.", "num_citations": "7\n", "authors": ["454"]}
{"title": "Software engineering data analysis techniques (tutorial)\n", "abstract": " The purpose of this tutorial is to provide a comprehensive coverage of software engineering data analysis techniques. It will briefly cover the basic product and process metrics, their description, use and interpretation. A systematic approach for analyzing and interpreting software engineering data will be introduced. It explicitly recognizes that metrics data tend to have high dimensionality, are highly correlated and suffer from redundancy. The techniques to be presented become progressively more sophisticated in terms of the underlying theory and analysis as well as in their ability to provide insights into the software project and the development process. The specific techniques to be covered are: statistical analyses; regression modeling; stochastic models; classification trees; and neural networks. Our emphasis is on the so-called data mining techniques within the KDD (knowledge discovery in databases) framework.", "num_citations": "7\n", "authors": ["454"]}
{"title": "Stoghastig behavior of an intermittently used system\n", "abstract": " This paper discusses the stochastic behavior of an intermittently used System. Of interest are (i) the distribution ofthe disappointment time,(ii) the expected number of disappointments during afinite interval, and (iii) the pointwise avaiiability. This paper d\u00e9rives the above three measures applying a Markov renewal process. A few examples are also presented.", "num_citations": "7\n", "authors": ["454"]}
{"title": "Parsimonious classifiers for software quality assessment\n", "abstract": " Modeling to predict fault-proneness of software modules is an important area of research in software engineering. Most such models employ a large number of basic and derived metrics as predictors. This paper presents modeling results based on only two metrics, lines of code and cyclomatic complexity, using radial basis functions with Gaussian kernels as classifiers. Results from two NASA systems are presented and analyzed.", "num_citations": "6\n", "authors": ["454"]}
{"title": "Formal specifications and reliability: an experimental study\n", "abstract": " An experimental study was undertaken to assess the improvement in program quality by using formal specifications. Specifications in the Z notation were developed for a simple but realistic anti-missile system. These specifications were then used to develop two versions in C by two programmers. Another set of three versions in Ada were independently developed from informal specifications in English. A comparison of the reliability of the resulting programs suggests the advantages of using formal specifications in terms of number of errors detected. Also, several errors that have been known to occur in earlier experiments dealing with this system were avoided by the use of formal specifications.<>", "num_citations": "6\n", "authors": ["454"]}
{"title": "An Experimental Investigation into Software Reliability\n", "abstract": " This report presents the results of an experiment investigating the effect of Fortran and Ada languages on program reliability. The experimental design employed was a full factorial design, ie, a design in two variables, each at two levels. The problem used in the experiment was the Launch Interceptor Program LIP, a simple but realistic anti-missile system. Reliability comparisons between Ada and Fortran programs were based on the total number of errors as well as on errors found during various testing phases. Some comparisons were also based on error density, the number of errors per 100 non-comment lines of code. It was found that on the average, the Ada programs had about 70 percent less errors than the Fortran ones. If errors during unit testing were excluded, the Ada program had about 78 percent less errors. Similar differences were found for data based on error causes and error types. Keywords Software Reliability, Software measurement.Descriptors:", "num_citations": "6\n", "authors": ["454"]}
{"title": "Design of reliability acceptance sampling plans based upon prior distribution\n", "abstract": " The problem considered is the development of single sample reliability acceptance plans for the case when the failure distribution is exponential with mean whose distribution is assumed to be the conjugate inverted gamma density. Various risk criteria that arise because of the incorporation of a prior distribution for the mean are discussed along with the problem of the choice of appropriate risk criteria to meet the producer's and consumer's true interests in various practical situations. Plan design for a specified pair of producer-consumer risks consists of obtaining the values of test time and acceptance number that satisfy the specified risks for the given prior distribution. These values have been obtained numerically to construct two test plans for two application examples.", "num_citations": "5\n", "authors": ["454"]}
{"title": "Summary of Technical Progress on Bayesian Software Prediction Models.\n", "abstract": " Work has been completed on development of Bayesian Software Correction Limit Policies designed to determine the optimum time value that minimizes the long run average cost of debugging at two levels-correction action undertaken by the programmer Phase I and action undertaken by a system analyst or system designer, if the error is not corrected in Phase I Phase II. Two models are developed-one assuming the cost of observations of error occurrence and correction time, prior to implementation of the optimum policy, is negligible the other incorporating the cost of observations. Work was also completed on an Imperfect Software Debugging Model that assumes errors are not corrected with certainty. By assuming the initial number of errors, probability of successfully correcting an error, and constant error occurence rate are all known, formulas for such quantities as distribution of time to completely debugged software, distribution of time to a specified number of remaining errors, and expected number of errors detected by time t can be derived. Work is currently in progress in extending the Imperfect Debugging Model to incorporate error correction time, estimation of model parameters and development of a Bayesian model developing bivariate software reliability models where system errors are classified as serious and non-serious development of empirical models for software error data development of software reliability demonstration plans for making acceptreject decisions for software packages and investigating the effects of changes in prior distributions andor model parameters on quantities of interest. AuthorDescriptors:", "num_citations": "5\n", "authors": ["454"]}
{"title": "Software effort prediction\n", "abstract": " The development of effort models has been an area of considerable research in software engineering. Most early models were algorithmic and derived relationships empirically between effort and project characteristics, mostly using regression analysis. Recently, they have been complemented by machine learning and by neural net techniques. Various comparative studies have also been pursued to evaluate the predictive performance of different approaches. This article first highlights the main effort\u2010prediction techniques and comparative studies and then discusses the model development process and some relevant issues. An illustrative example is employed to develop support vector prediction models for data from several industrial projects.", "num_citations": "4\n", "authors": ["454"]}
{"title": "Relating operational software reliability and workload: results from an experimental study\n", "abstract": " This paper presents some results from an experimental and analytical study to investigate the failure behavior of operational software for several types of workload. The authors are primarily interested in the so-called highly reliable software where enough evidence exists to indicate a lack of known faults. For purposes of this study, a \"gold\" version of a well known program was used in the experimental study. Judiciously selected errors were introduced, one at a time, and each mutated program was executed for three types of workload: constant; random; and systematically varying. Analytical analyses of the resulting failures were undertaken and are summarized here. The case when workload is random is of particular interest in this paper. An analytical model for the resulting failure phenomenon based on the Wald equation is found to give excellent results.", "num_citations": "4\n", "authors": ["454"]}
{"title": "A parallel compilation technique based on grammar partitioning\n", "abstract": " A new data partitioning scheme for parallel compilation is presented. The scheme is based on partitioning the grammar, which in turn effectively decomposes the language specification into multiple subsets. The scheme requires definition and development of specialized subcompilers. A simple and practical method is suggested for the implementation of such subcompilers using Parser Generators. The proposed scheme is illustrated through an example, using the grammar of Pascal language. A comparison with other methods such as functional decomposition, reveals that the proposed methodology has a larger potential speed-up factor.", "num_citations": "4\n", "authors": ["454"]}
{"title": "Reliability and Other Performance Measures of Computer Software\n", "abstract": " Sauf mention contraire ci-dessus, le contenu de cette notice bibliographique peut \u00eatre utilis\u00e9 dans le cadre d\u2019une licence CC BY 4.0 Inist-CNRS/Unless otherwise stated above, the content of this bibliographic record may be used under a CC BY 4.0 licence by Inist-CNRS/A menos que se haya se\u00f1alado antes, el contenido de este registro bibliogr\u00e1fico puede ser utilizado al amparo de una licencia CC BY 4.0 Inist-CNRS", "num_citations": "4\n", "authors": ["454"]}
{"title": "System and method for generating micro-array data classification model using radial basis functions\n", "abstract": " The present invention relates to a radial basis function classifier generating system and method to classify gene expression pattern appearing on micro-array for functional property. In the present invention, the \u2018representation coverage\u2019to be represented by classifier and the \u2018representation precision\u2019, instead of various variables, are set to be input variables and other variables required to generate classifier are automatically determined based on the given values of the input variables. Developer's selection of the values of variables is minimized and the unnecessary trial-and-errors are reduced. Developers understand easily meaning of such input variables and can predict the result of the selection of variables. Accordingly, the trial-and-errors due to meaningless selection of the values of the variables are reduced, so the classifier generation process can be optimized.", "num_citations": "3\n", "authors": ["454"]}
{"title": "Reliability analysis of a fault-tolerant multi-bus multiprocessor system\n", "abstract": " The paper proposes a fault-tolerant multi-bus system called binomial multi-bus architecture and analyses its reliability using a combinatorial method involving unreliability and mutually exclusive partitioning. Some attractive features of this architecture include low interconnection complexity, and a high degree of fault-tolerance. It is found that the probability of bus survival has a greater influence on the system reliability than the processors or memory modules. Comparison with the fully connected multi-bus architecture shows an important result, i.e. at least one-third of the connection cost is reduced at the expense of only a very slight decrease in the system reliability. Thus, the proposed architecture exhibits a strong potential to be a cost-effective interconnection network.< >", "num_citations": "3\n", "authors": ["454"]}
{"title": "The PASM parallel processing system: Hardware design and operating system concepts\n", "abstract": " Many of today's scientific and industrial problems require enormous computing power. Since circuit switching speeds are reaching fundamental limits, avenues to speed up computations other than that using faster components are being explored. One such avenue is the use of parallelism. PASM is a dynamically reconfigurable SIMD/MIMD parallel processing system design for up to 1,024 processing elements (PEs). It can be dynamically reconfigured to work as one or more SIMD (single instruction stream-multiple data stream) and/or MIMD (multiple instruction stream-multiple data stream) machines. A prototype with 30 MC68000 microprocessors, including 16 PEs in the computational engine, has been designed and constructed. The design of the prototype hardware is described, as well as the design tradeoffs that were made. Extending the current prototype by the addition of a Network Interface Unit (NIU) to\u00a0\u2026", "num_citations": "3\n", "authors": ["454"]}
{"title": "Error modelling applications in software quality assurance\n", "abstract": " This paper presents the results of a two-phased experiment conducted by Rome Air Development Center and Syracuse University to demonstrate the potential applicability of software error prediction models in performing formalized qualification testing of a software package. First, decisions based upon the predictions of three software error prediction models will be compared with actual program decisions for a large command and control software development project. Classical and Bayesian demonstration tests are used to make accept/reject decisions about the software system. Finally, the results of the two phases will be compared and some conclusions drawn as to the potential use of these predictive techniques to software quality assurance.", "num_citations": "3\n", "authors": ["454"]}
{"title": "Use of Computers in Teaching Statistics to Engineering Students\n", "abstract": " Anrit L. Goel Syracuse University Syracuse, New York use of Building on previous experience with the computers in teaching statistical courses at the University of Wisconsin and at Syracuse University, we incorporated computers into a two-senester sequence in Probability and Statistics during the fall of 1969. In the following paragraphs ve shall describe how such usage has been integrated into the course curriculum, what initial observations have been nade, and what our plans are for the future. We feel that the ideas presented here, even though based on rather linited experience, could be of help to educators who may be starting out on similar endeavors.There are at least three wa ys in which teaching and learning efficiency can be substantially increased through the use of the computer. First, the computer, being a fast and reliable computational device, enables the teacher to use real life data in homework assignments without loading the students with long hours of coaplicated calculations. The inportance of using actual data in statistical teaching cannot be over-enphasized. Second, the computer has made it possible to enploy sinulation to demonstrate, clarify and reinforce the theoretical concepts developed in the classroon. The third, and by far the most inportant feature is that computer-oriented approach stiaulates badly needed student interest in the subject matter. The use of tine-sharing consoles provides healthy computer-student interface which tends to encourage the student to learn by\" experimentation\", rather than by\" menorization\". Furthermore, believe, that such an approach inspires students to do\" research\" of their own.", "num_citations": "3\n", "authors": ["454"]}
{"title": "A RBF Classifier Based Framework for Software Quality Evaluation\n", "abstract": " Traditionally statistical methods such as regression analysis have been used for software quality evaluation models relating fault occurrences to descriptive metrics of a module. Classification trees have also been employed which generate metrics-based rules for identifying \u201cfaulty\u201d modules. In this paper we develop radial basis function clas-sifiers for early identification of fault-prone modules. Our use of RBF model is motivated by two considerations. First, the RBF possesses impressive properties of universal and best approximation. Second, the classifier which our recently developed design algorithm leads to is con-sistent and objective because of its strong mathematical basis. Here consistency means that there is no randomness in the design process. As a case study, we develop module classifiers in a well studied NASA software metrics database.", "num_citations": "2\n", "authors": ["454"]}
{"title": "Analysis of truncated SPRT- Exponential case(Sequential Probability Ratio Tests)\n", "abstract": " This paper discusses the application of a new procedure for the design and study of truncated sequential probability ratio tests for reliability testing when the underlying distribution is exponential. Effects of changes in the truncation point, the truncation region, and the decision boundaries on the risks and the expected test time are investigated for some selected test plans from MIL-STD-781B.", "num_citations": "2\n", "authors": ["454"]}
{"title": "COMMENT ON\" ECONOMICALLY OPTIMUM DESIGN OF CUSUM CHARTS\"\n", "abstract": " This note questions the authors' claim for optimality in designing cusum charts using the heuristic pattern search method.", "num_citations": "2\n", "authors": ["454"]}
{"title": "Software Quality Classification Models\n", "abstract": " This article addresses the development of analytical models to identify fault\u2010prone components. Because of the lack of physical underpinnings, the software development mechanism is not understood well enough to derive theory\u2010based models. Therefore, these models are obtained from experimental or historical data after an iterative process. Some commonly used techniques and a recent methodology based on radial basis functions are described. A case study is presented to illustrate the model's development, evaluation, and selection process.", "num_citations": "1\n", "authors": ["454"]}
{"title": "Parameter Estimation for Software Reliability Models Based on Delayed S-Shaped NHPP\n", "abstract": " Stochastic models based on the non-homogeneous Pos-sion process are increasingly being used for software reliability assessment. A major difficulity in their practical use is the estimation of model parameters which are obtained by numerical methods and are generally very sensitive to the initial values. In this paper we address this problem for the delayed S-shaped NHPP model but our results are also applicable to other pop-ular NHPP models. functions (3, 5). Many investigators have found that at least one of these three mean value functions can be used to describe the failure process in most situations. In some cases, all three are applicable but one may give better results than others. One major difficulty in the practical use of these reliability models is the estimation of the model parameters. In general, the estimation equations have to be solved numerically and the results tend to be very sensitive to the initial values chosen for the numerical procedure. In this study, we address this problem for the DSS model by first studying the characteristic points of the theoretical mean value function and its derivatives. Then we derive some relationships between the model parameters and the characteristic points. Finally, we use these relationships and the data derived trend test to develop guidelines for determining good initial val-ues for the model parameters.", "num_citations": "1\n", "authors": ["454"]}
{"title": "An error-specific approach to testing\n", "abstract": " The main objective of software testing in the software development life cycle is to verify conformance of the implemented software with its intended requirements. Such requirements include system requirements, and programming requirements. Non-conformance with such requirements causes what are known as software errors. Specifying an appropriate testing strategy to expose software errors is still an art. Traditional approaches do succeed in revealing many errrors but none is powerful enough to expose all errors. The best that is hoped for is to use a specific test strategy to expose a specific error type in specific program locations. This limitation is exploited to develop a new approach to software testing which is called an error-specific testing (EST) strategy. Error specific testing is in fact a dual to the traditional testing approaches.", "num_citations": "1\n", "authors": ["454"]}
{"title": "The application of fractional factorial techniques: to the design of software experiments, or, confounding the issues\n", "abstract": " We are interested in evaluating the effects of software tools and methods on cost and errors over the life-cycle of a software project. The adopted approach is to perform controlled experiments. The experiments are designed to be conducted for selected variables using standard experimental design strategies. We discuss the use of the fractional factorial approach to a specific design for a selected experiment. The Appendix contains the list of variables which were considered in the design of this experiment.", "num_citations": "1\n", "authors": ["454"]}
{"title": "Design of Reliability Test Plans Based Upon Prior Distribution.\n", "abstract": " A simple procedure using tabulated values for the design of reliability acceptance sampling plans is described in this report. The failure distribution is considered to be exponential with parameter theta and the prior distribution of theta is taken to be inverted gamma. This procedure is applicable when acceptreject decisions are to be made regarding a sequence of systems or lots. For specified combinations of producer-consumer risks, the procedure described here can be used for the design of a truncated single sample plan for a system. Other single sample plans can be obtained by using the equivalence relations given in this report. The use of the design tables is illustrated via numerical examples. AuthorDescriptors:", "num_citations": "1\n", "authors": ["454"]}
{"title": "Bayesian Software Prediction Models. Volume 1. An Imperfect Debugging Model for Reliability and other Quantitative Measures of Software Systems\n", "abstract": " In this report a stochastic model for software failure phenomena is developed for the case when the errors are not corrected with certainty. Expressions for several quantities of interest are derived to establish quantitative measures for software performance assessment. Approximations for large-scale software systems using a gamma distribution are also discussed. Numerical examples are used to illustrate the computations and usefulness of various quantities.Descriptors:", "num_citations": "1\n", "authors": ["454"]}
{"title": "Bayesian software prediction models. Volume 1: An imperfect debugging model for reliability and other quantitative measures of software systems[Final Technical Report, Dec\u00a0\u2026\n", "abstract": " In this report a stochastic model for software failure phenomena is developed for the case when the errors are not corrected with certainty. Expressions for several quantities of interest are derived to establish quantitative measures for software performance assessment. Approximations for large-scale software systems using a gamma distribution are also discussed. Numerical examples are used to illustrate the computations and usefulness of various quantities.", "num_citations": "1\n", "authors": ["454"]}
{"title": "An analytical model for information processing systems\n", "abstract": " Information processing systems are an important sector in the application of computer technology to the fields of on-line data processing and the management of large complicated data bases. With an increase in the number of systems being designed and used, there has been an increasing emphasis on the modeling and performance evaluation of such systems. In Reference 4 Nunamaker presented a procedure for the design and optimization methodology of such systems. In some cases, the evaluation procedure was reduced to the evaluation of file structure and data base organization. Kobayashi gave an algebraic modeling of information structures. In this paper, we carry out the study of mean response time by taking into consideration host processor environment, user characteristics and the data base structure. Total system response time is the main goal of this investigation.", "num_citations": "1\n", "authors": ["454"]}
{"title": "A COMPARATIVE AND ECONOMIC INVESTIGATION OF SHEWHART X-CHARTS AND CUMULATIVE SUM CONTROL CHARTS\n", "abstract": " Reproduced with permission of the copyright owner. Further reproduction prohibited without permission. Page 1 Reproduced with permission of the copyright owner. Further reproduction prohibited without permission. Page 2 Reproduced with permission of the copyright owner. Further reproduction prohibited without permission. Page 3 Reproduced with permission of the copyright owner. Further reproduction prohibited without permission. Page 4 Reproduced with permission of the copyright owner. Further reproduction prohibited without permission. Page 5 Reproduced with permission of the copyright owner. Further reproduction prohibited without permission. Page 6 Reproduced with permission of the copyright owner. Further reproduction prohibited without permission. Page 7 Reproduced with permission of the copyright owner. Further reproduction prohibited without permission. Page 8 Reproduced with of the . . \u2026", "num_citations": "1\n", "authors": ["454"]}