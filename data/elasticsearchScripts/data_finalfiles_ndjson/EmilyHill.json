{"title": "Towards automatically generating summary comments for java methods\n", "abstract": " Studies have shown that good comments can help programmers quickly understand what a method does, aiding program comprehension and software maintenance. Unfortunately, few software projects adequately comment the code. One way to overcome the lack of human-written summary comments, and guard against obsolete comments, is to automatically generate them. In this paper, we present a novel technique to automatically generate descriptive summary comments for Java methods. Given the signature and body of a method, our automatic comment generator identifies the content for the summary and generates natural language text that summarizes the method's overall actions. According to programmers who judged our generated comments, the summaries are accurate, do not miss important content, and are reasonably concise.", "num_citations": "426\n", "authors": ["981"]}
{"title": "Using natural language program analysis to locate and understand action-oriented concerns\n", "abstract": " Most current software systems contain undocumented high-level ideas implemented across multiple files and modules. When developers perform program maintenance tasks, they often waste time and effort locating and understanding these scattered concerns. We have developed a semi-automated concern location and comprehension tool, Find-Concept, designed to reduce the time developers spend on maintenance tasks and to increase their confidence in the results of these tasks. Find-Concept is effective because it searches a unique natural language-based representation of source code, uses novel techniques to expand initial queries into more effective queries, and displays search results in an easy-to-comprehend format. We describe the Find-Concept tool, the underlying program analysis, and an experimental study comparing Find-Concept's search effectiveness with two state-of-the-art lexical and\u00a0\u2026", "num_citations": "279\n", "authors": ["981"]}
{"title": "Automatically capturing source code context of nl-queries for software maintenance and reuse\n", "abstract": " As software systems continue to grow and evolve, locating code for maintenance and reuse tasks becomes increasingly difficult. Existing static code search techniques using natural language queries provide little support to help developers determine whether search results are relevant, and few recommend alternative words to help developers reformulate poor queries. In this paper, we present a novel approach that automatically extracts natural language phrases from source code identifiers and categorizes the phrases and search results in a hierarchy. Our contextual search approach allows developers to explore the word usage in a piece of software, helping them to quickly identify relevant program elements for investigation or to quickly recognize alternative words for query reformulation. An empirical evaluation of 22 developers reveals that our contextual search approach significantly outperforms the most\u00a0\u2026", "num_citations": "253\n", "authors": ["981"]}
{"title": "Mining source code to automatically split identifiers for software analysis\n", "abstract": " Automated software engineering tools (e.g., program search, concern location, code reuse, quality assessment, etc.) increasingly rely on natural language information from comments and identifiers in code. The first step in analyzing words from identifiers requires splitting identifiers into their constituent words. Unlike natural languages, where space and punctuation are used to delineate words, identifiers cannot contain spaces. One common way to split identifiers is to follow programming language naming conventions. For example, Java programmers often use camel case, where words are delineated by uppercase letters or non-alphabetic characters. However, programmers also create identifiers by concatenating sequences of words together with no discernible delineation, which poses challenges to automatic identifier splitting. In this paper, we present an algorithm to automatically split identifiers into\u00a0\u2026", "num_citations": "189\n", "authors": ["981"]}
{"title": "Exploring the neighborhood with dora to expedite software maintenance\n", "abstract": " Completing software maintenance and evolution tasks for today's large, complex software systems can be difficult, often requiring considerable time to understand the system well enough to make correct changes. Despite evidence that successful programmers use program structure as well as identifier names to explore software, most existing program exploration techniques use either structural or lexical identifier information. By using only one type of information, automated tools ignore valuable clues about a developer's intentions-clues critical to the human program comprehension process. In this paper, we present and evaluate a technique that exploits both program structure and lexical information to help programmers more effectively explore programs. Our approach uses structural information to focus automated program exploration and lexical information to prune irrelevant structure edges from\u00a0\u2026", "num_citations": "165\n", "authors": ["981"]}
{"title": "Applying concept analysis to user-session-based testing of web applications\n", "abstract": " The continuous use of the Web for daily operations by businesses, consumers, and the government has created a great demand for reliable Web applications. One promising approach to testing the functionality of Web applications leverages the user-session data collected by Web servers. User-session-based testing automatically generates test cases based on real user profiles. The key contribution of this paper is the application of concept analysis for clustering user sessions and a set of heuristics for test case selection. Existing incremental concept analysis algorithms are exploited to avoid collecting and maintaining large user-session data sets and to thus provide scalability. We have completely automated the process from user session collection and test suite reduction through test case replay. Our incremental test suite update algorithm, coupled with our experimental study, indicates that concept analysis\u00a0\u2026", "num_citations": "158\n", "authors": ["981"]}
{"title": "Automated replay and failure detection for web applications\n", "abstract": " User-session-based testing of web applications gathers user sessions to create and continually update test suites based on real user input in the field. To support this approach during maintenance and beta testing phases, we have built an automated framework for testing web-based software that focuses on scalability and evolving the test suite automatically as the application's operational profile changes. This paper reports on the automation of the replay and oracle components for web applications, which pose issues beyond those in the equivalent testing steps for traditional, stand-alone applications. Concurrency, nondeterminism, dependence on persistent state and previous user sessions, a complex application infrastructure, and a large number of output formats necessitate developing different replay and oracle comparator operators, which have tradeoffs in fault detection effectiveness, precision of analysis\u00a0\u2026", "num_citations": "141\n", "authors": ["981"]}
{"title": "AMAP: automatically mining abbreviation expansions in programs to enhance software maintenance tools\n", "abstract": " When writing software, developers often employ abbreviations in identifier names. In fact, some abbreviations may never occur with the expanded word, or occur more often in the code. However, most existing program comprehension and search tools do little to address the problem of abbreviations, and therefore may miss meaningful pieces of code or relationships between software artifacts. In this paper, we present an automated approach to mining abbreviation expansions from source code to enhance software maintenance tools that utilize natural language information. Our scoped approach uses contextual information at the method, program, and general software level to automatically select the most appropriate expansion for a given abbreviation. We evaluated our approach on a set of 250 potential abbreviations and found that our scoped approach provides a 57% improvement in accuracy over the current\u00a0\u2026", "num_citations": "125\n", "authors": ["981"]}
{"title": "Does a programmer's activity indicate knowledge of code?\n", "abstract": " The practice of software development can likely be improved if an externalized model of each programmer's knowledge of a particular code base is available. Some tools already assume a useful form of such a model can be created from data collected during development, such as expertise recommenders that use information about who has changed each file to suggest who might answer questions about particular parts of a system. In this paper, we report on an empirical study that investigates whether a programmer's activity can be used to build a model of what a programmer knows about a code base. In this study, nineteen professional Java programmers completed a series of questionnaires about the code on which they were working. These questionnaires were generated automatically and asked about program elements a programmer had worked with frequently and recently and ones that he had not. We\u00a0\u2026", "num_citations": "121\n", "authors": ["981"]}
{"title": "Identifying word relations in software: A comparative study of semantic similarity tools\n", "abstract": " Modern software systems are typically large and complex, making comprehension of these systems extremely difficult. Experienced programmers comprehend code by seamlessly processing synonyms and other word relations. Thus, we believe that automated comprehension and software tools can be significantly improved by leveraging word relations in software. In this paper, we perform a comparative study of six state of the art, English-based semantic similarity techniques and evaluate their effectiveness on words from the comments and identifiers in software. Our results suggest that applying English-based semantic similarity techniques to software without any customization could be detrimental to the performance of the client software tools. We propose strategies to customize the existing semantic similarity techniques to software, and describe how various program comprehension tools can benefit from\u00a0\u2026", "num_citations": "114\n", "authors": ["981"]}
{"title": "Design and evaluation of an automated aspect mining tool\n", "abstract": " Attention to aspect oriented programming (AOP) is rapidly growing as its benefits in large software system development and maintenance are increasingly recognized. However, existing large software systems, which could benefit most from refactoring into AOP, still remain unchanged in practice, due to the high cost of the refactoring. Automatic identification and extraction of aspects would not only enable migration of legacy systems to AOP, but also prevent current systems from accumulating scattered and duplicated code. In this paper, we present the design, implementation, and evaluation of an aspect mining analysis, which automatically identifies desirable candidates for refactoring into AOP, without requiring input from the user or predefined queries. By exploiting the program dependence graph and abstract syntax tree representations of a program, our analysis is able to automatically identify a much larger set of valuable refactoring candidates than current aspect mining techniques, as demonstrated by an empirical evaluation of our automatic mining analysis on two large software systems.", "num_citations": "106\n", "authors": ["981"]}
{"title": "An empirical comparison of test suite reduction techniques for user-session-based testing of web applications\n", "abstract": " Automated cost-effective test strategies are needed to provide reliable, secure, and usable Web applications. As a software maintainer updates an application, test cases must accurately reflect usage to expose faults that users are most likely to encounter. User-session-based testing is an automated approach to enhancing an initial test suite with real user data, enabling additional testing during maintenance as well as adding test data that represents usage as operational profiles evolve. Test suite reduction techniques are critical to the cost effectiveness of user-session-based testing because a key issue is the cost of collecting, analyzing, and replaying the large number of test cases generated from user-session data. We performed an empirical study comparing the test suite size, program coverage, fault detection capability, and costs of three requirements-based reduction techniques and three variations of\u00a0\u2026", "num_citations": "89\n", "authors": ["981"]}
{"title": "Improving source code search with natural language phrasal representations of method signatures\n", "abstract": " As software continues to grow, locating code for maintenance tasks becomes increasingly difficult. Software search tools help developers find source code relevant to their maintenance tasks. One major challenge to successful search tools is locating relevant code when the user's query contains words with multiple meanings or words that occur frequently throughout the program. Traditional search techniques, which treat each word individually, are unable to distinguish relevant and irrelevant methods under these conditions. In this paper, we present a novel search technique that uses information such as the position of the query word and its semantic role to calculate relevance. Our evaluation shows that this approach is more consistently effective than three other state of the art search techniques.", "num_citations": "73\n", "authors": ["981"]}
{"title": "Degree-of-knowledge: Modeling a developer's knowledge of code\n", "abstract": " As a software system evolves, the system's codebase constantly changes, making it difficult for developers to answer such questions as who is knowledgeable about particular parts of the code or who needs to know about changes made. In this article, we show that an externalized model of a developer's individual knowledge of code can make it easier for developers to answer such questions. We introduce a degree-of-knowledge model that computes automatically, for each source-code element in a codebase, a real value that represents a developer's knowledge of that element based on a developer's authorship and interaction data. We present evidence that shows that both authorship and interaction data of the code are important in characterizing a developer's knowledge of code. We report on the usage of our model in case studies on expert finding, knowledge transfer, and identifying changes of interest. We\u00a0\u2026", "num_citations": "68\n", "authors": ["981"]}
{"title": "Analysing source code: looking for useful verb\u2013direct object pairs in all the right places\n", "abstract": " The large time and effort devoted to software maintenance can be reduced by providing software engineers with software tools that automate tedious, error-prone tasks. However, despite the prevalence of tools such as IDEs, which automatically provide program information and automated support to the developer, there is considerable room for improvement in the existing software tools. The authors' previous work has demonstrated that using natural language information embedded in a program can significantly improve the effectiveness of various software maintenance tools. In particular, precise verb information from source code analysis is useful in improving tools for comprehension, maintenance and evolution of object-oriented code, by aiding in the discovery of scattered, action-oriented concerns. However, the precision of the extraction analysis can greatly affect the utility of the natural language information\u00a0\u2026", "num_citations": "63\n", "authors": ["981"]}
{"title": "Integrating natural language and program structure information to improve software search and exploration\n", "abstract": " Today's software is large and complex, with systems consisting of millions of lines of code. New developers to a software project face significant challenges in locating code related to their maintenance tasks of fixing bugs or adding new features. Developers can simply be assigned a bug and told to fix it\u2014even when they have no idea where to begin. In fact, research has shown that a developer typically spends more time locating and understanding code during maintenance than modifying it.", "num_citations": "54\n", "authors": ["981"]}
{"title": "On the use of stemming for concern location and bug localization in java\n", "abstract": " As the popularity of text-based source code search and analysis grows, the use of stemmers to strip suffixes has increased. Although widely investigated in the information retrieval community, the comparative effectiveness of stemmers in the domain of software is relatively unknown. In this paper, we investigate which of the well-known stemmers perform best in the domain of Java software for concern location and bug localization. For these two problems, we evaluate the use of stemming on over 500 search tasks for six different Java applications. Using MAP and Rank Measure, we conducted an overall qualitative study and a query-by-query quantitative study of the impact of stemming on retrieval effectiveness. As one might expect, our contribution demonstrates that how stemming affects retrieval performance is mediated by other factors, such as the use of tf-idf to filter commonly occurring terms and the precise\u00a0\u2026", "num_citations": "45\n", "authors": ["981"]}
{"title": "A case study of automatically creating test suites from web application field data\n", "abstract": " Creating effective test cases is a difficult problem, especially for web applications. To comprehensively test a web application's functionality, test cases must test complex application state dependencies and concurrent user interactions. Rather than creating test cases manually or from a static model, field data provides an inexpensive alternative to creating such sophisticated test cases. An existing approach to using field data in testing web applications is user-session-based testing. Previous user-session-based testing approaches ignore state dependences from multi-user interactions. In this paper, we propose strategies for leveraging web application field data to automatically create test cases that test various levels of multi-user interaction and state dependencies. Results from out preliminary case study of a publicly deployed web application show that these test case creation mechanisms are a promising testing\u00a0\u2026", "num_citations": "45\n", "authors": ["981"]}
{"title": "A comparison of stemmers on source code identifiers for software search\n", "abstract": " As the popularity of text-based source code analysis grows, the use of stemmers to strip suffixes has increased. Stemmers have been used to more accurately determine relevance between a keyword query and methods in source code for search, exploration, and bug localization. In this paper, we investigate which traditional stemmers perform best on the domain of software, specifically, Java source code. We compare the stemmers using two case studies: a comparative analysis of the unified word classes in terms of accuracy and completeness, as well as an investigation into the effectiveness of stemming for software search. Our results indicate that relative stemmer effectiveness varies with a software engineering tool such as search, justifying further research into this area.", "num_citations": "38\n", "authors": ["981"]}
{"title": "Web application testing with customized test requirements-an experimental comparison study\n", "abstract": " Test suite reduction uses test requirement coverage to determine if the reduced test suite maintains the original suite's requirement coverage. Based on observations from our previous experimental studies on test suite reduction, we believe there is a need for customized test requirements for Web applications. In this paper, we examine usage-based customized test requirements for the test suite reduction problem in Web application testing. We conduct an extensive experimental study to evaluate the tradeoffs between five classes of customized requirements with respect to reduced test suite size, program coverage and fault detection effectiveness. Our results show that the reduced suites' program coverage and fault detection effectiveness increases with the context or data associated with the reduction requirement. Based on our experimental results, we provide guidance to testers on the most useful test\u00a0\u2026", "num_citations": "37\n", "authors": ["981"]}
{"title": "CONQUER: A tool for NL-based query refinement and contextualizing code search results\n", "abstract": " Identifying relevant code to perform maintenance or reuse tasks is becoming increasingly difficult. Software systems continue to grow and evolve, and developers often find themselves searching within thousands to even millions of lines of code to identify code relevant to a particular maintenance task. Automated solutions are vital to help developers become more efficient at locating code to be modified when performing maintenance tasks. In order to address this need and help developers reduce the time spent finding and searching for relevant code, we have built an Eclipse-plug in, CONQUER, that helps developers identify relevant results by providing critical insight and context of how query words are used in the code. CONQUER leverages advanced natural language (NL) information in the source code to group, sort and display the results in a meaningful way. In addition, CONQUER analyzes the frequency\u00a0\u2026", "num_citations": "34\n", "authors": ["981"]}
{"title": "NL-based query refinement and contextualized code search results: A user study\n", "abstract": " As software systems continue to grow and evolve, locating code for software maintenance tasks becomes increasingly difficult. Source code search tools match a developer's keyword-style or natural language query with comments and identifiers in the source code to identify relevant methods that may need to be changed or understood to complete the maintenance task. In this search process, the developer faces a number of challenges: (1) formulating a query, (2) determining if the results are relevant, and (3) if the results are not relevant, reformulating the query. In this paper, we present a NL-based results view for searching source code for maintenance that helps address these challenges by integrating multiple feedback mechanisms into the search results view: prevalence of the query words in the result set, results grouped by NL-based information, as a result list, and suggested alternative query words. Our\u00a0\u2026", "num_citations": "28\n", "authors": ["981"]}
{"title": "Towards automatically generating descriptive names for unit tests\n", "abstract": " During maintenance, developers often need to understand the purpose of a test. One of the most potentially useful sources of information for understanding a test is its name. Ideally, test names are descriptive in that they accurately summarize both the scenario and the expected outcome of the test. Despite the benefits of being descriptive, test names often fall short of this goal. In this paper we present a new approach for automatically generating descriptive names for existing test bodies. Using a combination of natural-language program analysis and text generation, the technique creates names that summarize the test's scenario and the expected outcome. The results of our evaluation show that,(1) compared to alternative approaches, the names generated by our technique are significantly more similar to human-generated names and are nearly always preferred by developers,(2) the names generated by our\u00a0\u2026", "num_citations": "26\n", "authors": ["981"]}
{"title": "Natural language-based software analyses and tools for software maintenance\n", "abstract": " Significant portions of software life cycle resources are devoted to program maintenance, which motivates the development of automated techniques and tools to support the tedious, error-prone tasks. Natural language clues from programmers\u2019 naming in literals, identifiers, and comments can be leveraged to improve the effectiveness of many software tools. For example, they can be used to increase the accuracy of software search tools, improve the ability of program navigation tools to recommend related methods, and raise the accuracy of other program analyses by providing access to natural language information. This chapter focuses on how to capture, model, and apply the programmers\u2019 conceptual knowledge expressed in both linguistic information as well as programming language structure and semantics. We call this kind of analysis Natural Language Program Analysis (NLPA) since it combines\u00a0\u2026", "num_citations": "25\n", "authors": ["981"]}
{"title": "Introducing natural language program analysis\n", "abstract": " This research group presentation focuses on our work in extracting and utilizing natural language clues from source code to improve software maintenance tools. We demonstrate the valuable information that can be gained from a software system's identifiers, literals, and comments. We then present an overview of our extraction process, program representation, and a set of tools we have developedusing this natural language program analysis.", "num_citations": "20\n", "authors": ["981"]}
{"title": "Automatically generating test templates from test names (n)\n", "abstract": " Existing specification-based testing techniques require specifications that either do not exist or are too difficult to create. As a result, they often fall short of their goal of helping developers test expected behaviors. In this paper we present a novel, natural language-based approach that exploits the descriptive nature of test names to generate test templates. Similar to how modern IDEs simplify development by providing templates for common constructs such as loops, test templates can save time and lower the cognitive barrier for writing tests. The results of our evaluation show that the approach is feasible: despite the difficulty of the task, when test names contain a sufficient amount of information, the approach's accuracy is over 80% when parsing the relevant information from the test name and generating the template.", "num_citations": "19\n", "authors": ["981"]}
{"title": "Analyzing clusters of web application user sessions\n", "abstract": " User sessions provide valuable insight into the dynamic behavior of web applications. They also play a key role in user-session-based testing, which gathers user sessions in the field and replays selected sessions to test an evolving application. To decrease the testing and analysis effort, testers reduce the set of collected user sessions by either clustering user sessions by their shared URL attributes or by program coverage requirements-based reduction techniques. Clustering URL attributes can be considerably less expensive; however, the tradeoff may be that clustering is not representative of dynamic behavior similarities. This paper describes our analysis of user session data to reveal correlations between sessions clustered on the sessions' attributes and the relative dynamic behavior of the program for those sessions. The results of our analysis also motivate other clustering and test suite reduction techniques\u00a0\u2026", "num_citations": "19\n", "authors": ["981"]}
{"title": "GeoTagger: a collaborative and participatory environmental inquiry system\n", "abstract": " This note focuses on the motivation, approach, and the initial prototype implementation of Geotagger: a collaborative participatory environmental inquiry system. We situate the need for such a technology, and discuss related work--much of which is situated in the realm of citizen science. Our work uniquely distinguishes itself from many other citizen science applications in that it supports limited data collection and analysis, with the additional benefit of supporting social interactions and engagement through conversations about observed data. This is accomplished by creating friends and groups which are collaborators in the observational inquiry process.", "num_citations": "15\n", "authors": ["981"]}
{"title": "Part of speech tagging Java method names\n", "abstract": " Numerous software engineering tools for evolution and comprehension, including code search, comment generation, and analyzing bug reports, make use of part-of-speech (POS) information. However, many POS taggers are developed for, and trained on, natural language. In this paper, we investigate the accuracy of 9 POS taggers on over 200 source code identifiers taken from method names in open source Java programs. The set of taggers includes traditional POS taggers for English as well as some tuned to source code identifiers. Our results indicate that taggers tailored for source code are significantly more effective.", "num_citations": "14\n", "authors": ["981"]}
{"title": "Integrating customized test requirements with traditional requirements in web application testing\n", "abstract": " Existing test suite reduction techniques employed for testing web applications have either used traditional program coverage-based requirements or usage-based requirements. In this paper, we explore three different strategies to integrate the use of program coverage-based requirements and usage-based requirements in relation to test suite reduction for web applications. We investigate the use of usage-based test requirements for comparison of test suites that have been reduced based on program coverage-based test requirements. We examine the effectiveness of a test suite reduction process based on a combination of both usage-based and program coverage-based requirements. Finally, we modify a popular test suite reduction algorithm to replace part of its test selection process with selection based on usage-based test requirements. Our case study suggests that integrating program coverage-based and\u00a0\u2026", "num_citations": "13\n", "authors": ["981"]}
{"title": "Current developments in big data and sustainability sciences in mobile citizen science applications\n", "abstract": " Sustainability Sciences Studies is an interdisciplinary approach towards understanding how to develop a culture of conservation. This culture of conservation can be viewed from many different aspects, from the individual person's decisions to larger community's impacts. In all of this, we see large quantities of data in a push-pull relationship with each other, with stakeholders needing to have access to real-time generated data sets and analytics from populations as large as a metropolitan community and be able to respond to as well as disseminate information to these populations. In this paper, we present a survey of the literature in these areas. The pervasive and diverse nature of big data in these fields demonstrates the need for data scientists to collaborate to identify ways to address the common and disparate needs that different projects may have in relation to big data. We also briefly present a platform and\u00a0\u2026", "num_citations": "12\n", "authors": ["981"]}
{"title": "Coverage criteria for testing web applications\n", "abstract": " As web applications evolve and their usage increases, their complexity also increases, thus creating a great demand for techniques and tools to ensure well-tested, reliable applications. While program-based coverage and fault detection capability can be used to measure the quality of test suites, the dynamic characteristics of web applications motivate additional criteria to complement these traditional test adequacy criteria. This paper presents novel dynamic coverage criteria customized for web applications\u2014criteria intended for testing at a page level. Based on a changing universe of test requirements, as indicated by evolving usage of the application, our criteria avoid the difficulties of building an accurate static model of a web application\u2019s structure. We define a class of dynamic coverage criteria, present the subsumption relation among them, and describe two case studies to demonstrate their usefulness. Among other possible uses, the proposed criteria can be used to compare the quality of test suites, to select test cases, and to examine how usage of a web application changes over time. The proposed criteria complement traditional program coverage and fault detection capability criteria.", "num_citations": "12\n", "authors": ["981"]}
{"title": "Learning effective oracle comparator combinations for web applications\n", "abstract": " Web application testers need automated, effective approaches to validate the test results of complex, evolving Web applications. In previous work, we developed a suite of automated oracle comparators that focus on specific characteristics of a Web application's HTML response. We found that oracle comparators' effectiveness depends on the application's behavior. We also found that by combining the results of two oracle comparators, we could achieve better effectiveness than using a single oracle comparator alone. However, selecting the most effective oracle combination from the large suite of comparators is difficult. In this paper, we propose applying decision tree learning to identify the best combination of oracle comparators, based on the tester's effectiveness goals. Using decision tree learning, we train separately on four Web applications and identify the most effective oracle comparator for each application\u00a0\u2026", "num_citations": "11\n", "authors": ["981"]}
{"title": "Is an athletic approach the future of software engineering education?\n", "abstract": " Traditional software engineering education approaches--in-class lectures, unsupervised homework assignments, and occasional projects--create many opportunities for distraction. To address this problem, the authors have employed an approach that treats software engineering education more like athletic training.", "num_citations": "10\n", "authors": ["981"]}
{"title": "Investigating how to effectively combine static concern location techniques\n", "abstract": " As software systems continue to grow and evolve, locating code for maintenance tasks becomes increasingly difficult. Studies have shown that combining static global concern location techniques like search with more structure-based local techniques can improve effectiveness. However, no studies have yet investigated why this occurs. In this paper, we investigate why combining global and local techniques improves effectiveness, and under what conditions. We explore such questions as:\" What are the limits of lexical information in locating concerns?\",\" How far away does a local technique have to go to locate the remaining relevant elements?\", and\" How sensitive are these results to the query or scoring thresholds of the techniques?\". The results of our study can inform design decisions to maximize effective global and local combinations in future concern location techniques.", "num_citations": "10\n", "authors": ["981"]}
{"title": "On the use of positional proximity in IR-based feature location\n", "abstract": " As software systems continue to grow and evolve, locating code for software maintenance tasks becomes increasingly difficult. Recently proposed approaches to bug localization and feature location have suggested using the positional proximity of words in the source code files and the bug reports to determine the relevance of a file to a query. Two different types of approaches have emerged for incorporating word proximity and order in retrieval: those based on ad-hoc considerations and those based on Markov Random Field (MRF) modeling. In this paper, we explore using both these types of approaches to identify over 200 features in five open source Java systems. In addition, we use positional proximity of query words within natural language (NL) phrases in order to capture the NL semantics of positional proximity. As expected, our results indicate that the power of these approaches varies from one dataset to\u00a0\u2026", "num_citations": "9\n", "authors": ["981"]}
{"title": "Developing natural language-based program analyses and tools to expedite software maintenance\n", "abstract": " With as much as 60-90% of software life cycle resources spent on program maintenance, there is a critical need for automated software tools to help explore and understand today's large and complex software. One important source of information software maintenance tools can draw from is lexical information in comments and identifiers. Identifier names often communicate a programmer's intent when writing code, and help developers map real-world concepts to code during comprehension. My dissertation will develop specialized information retrieval techniques and natural language analyses for software so that software maintenance tools can take full advantage of the wealth of information in program identifiers, and integrate these techniques into software tools to expedite the maintenance activities of program exploration, concern location, and fault localization.", "num_citations": "8\n", "authors": ["981"]}
{"title": "Toward automatic summarization of arbitrary java statements for novice programmers\n", "abstract": " Novice programmers sometimes need to understand code written by others. Unfortunately, most software projects lack comments suitable for novices. The lack of comments have been addressed through automated techniques of generating comments based on program statements. However, these techniques lacked the context of how these statements function since they were aimed toward experienced programmers. In this paper, we present a novel technique towards automatically generating comments for Java statements suitable for novice programmers. Our technique not only goes beyond existing approaches to method summarization to meet the needs of novices, it also leverages API documentation when available. In an experimental study of 30 computer science undergraduate students, we observed explanations based on our technique to be preferred over an existing approach.", "num_citations": "7\n", "authors": ["981"]}
{"title": "Parallel genetic algorithms: An exploration of weather prediction through clustered computing\n", "abstract": " BACKGROUNDIn recent years, the price drop in off-the-shelf computer systems has enabled small institutions access to affordable supercomputing. Thus, there has been considerable growth in research into inherently parallel problems and methods, which are well suited to cluster computing environments. One of the most promising of these methods is the development of parallel genetic algorithms. Genetic algorithms are based on the theory of Darwinian evolution [Holland]. A large number of potential solutions to the problem are created and then competed against each other. The fittest solutions are combined to form new solutions and mutated to seek better solutions. Over time, an effective solution is developed [Xobitko]. Genetic algorithms are especially good with NP-complete and NP-hard problems, as well as problems which are not fully understood by the programmer.", "num_citations": "7\n", "authors": ["981"]}
{"title": "Exploring the use of concern element role information in feature location evaluation\n", "abstract": " Before making changes, programmers need to locate and understand source code that corresponds to specific functionality, i.e., Perform concern or feature location. Numerous concern and feature location techniques have been proposed, but to the best of our knowledge, no existing techniques or evaluations report information on what role a code element plays in the larger concern. In this paper, we report on two case studies that investigate two hypotheses on how evaluation studies of concern location techniques can be strengthened by utilizing concern role information: (1) by increasing agreement among human annotators for gold set establishment and (2) by providing richer information about the elements ranked as relevant by concern location techniques, which could help further improve the tools. We conducted a case study of 6 Java developers annotating 3 concerns with role information. When the\u00a0\u2026", "num_citations": "5\n", "authors": ["981"]}
{"title": "Scientific data infrastructure for sustainability science mobile applications\n", "abstract": " With the recent ecological and volatile climate issues, numerous concerns have arisen which have led to the rapid growth of sustainability sciences. In sustainability studies and related areas such as crisis data management, multiple communities need to interact and contribute data, and then have this data modeled for them in an effective manner. The National Science Foundation Advisory Committee for Environment Research and Education states that, \"Because of the complex relationships among people, ecosystems, and the biosphere, human health and well-being are closely linked to the integrity of local, regional and global ecosystems.\" In our work, we look towards developing a mobile application platform that allows data integration for multiple information sources that allows the user flexibility to learn about and actively participate in understanding and helping their environment. In this paper, we address\u00a0\u2026", "num_citations": "4\n", "authors": ["981"]}
{"title": "Developing Natural Language-based Software Analyses & Tools to Expedite Software Maintenance\n", "abstract": " Today\u2019s software is large and complex, with systems consisting of millions of lines of code. New developers to a software project face significant challenges in locating code related to their maintenance tasks of fixing bugs or adding new features. Developers can simply be handed a bug and told to fix it\u2013even when they have no idea where to begin.We can significantly reduce the cost of software maintenance by reducing the time and effort to find and understand code. In this talk, I will outline the challenges in finding and understanding code in a large software project as well as present some software engineering tools that can help. Specifically, I will present techniques that leverage the textual information from comments and identifiers as well as program structure to create intuitive software engineering and program comprehension tools.", "num_citations": "3\n", "authors": ["981"]}
{"title": "Web accessibility and section 508 compliance\n", "abstract": " With the recent addition of Section 508 to the Rehabilitation Act of 1973 (enacted August 1998, implemented June 2001), the need for accessible and Section 508 compliant Web sites has become imperative. Web Accessibility Evaluation and Repair Tools have been created to help Web developers make Web pages accessible, mitigating the enormity of this task. The focus of the project was to compare and contrast a number of software tools available that test and/or fix Web sites and individual Web pages for Web Accessibility and compliance with Section 508.", "num_citations": "3\n", "authors": ["981"]}
{"title": "Geotagger: A collaborative environmental inquiry platform\n", "abstract": " Geotagger is a collaborative environmental inquiry platform that enables children and adults alike the opportunity to observe the world around them, document that observation, share it, and encourage discussion around that tagged item of interest. The main objectives are to leverage the rampant use of and affinity for technology to encourage people to observe the natural world around them and to share and discuss that information with peers and colleagues. We present the Geotagger platform, share some initial feedback from various users, and we discuss how Geotagger has evolved based on that feedback.", "num_citations": "2\n", "authors": ["981"]}
{"title": "Towards automatically generating comments for Java methods\n", "abstract": " Studies have shown that good comments can help programmers quickly understand what a method does, aiding program comprehension and software maintenance. Unfortunately, few software projects adequately comment the code. One way to overcome the lack of human-written summary comments, and guard against obsolete comments, is to automatically generate them. In this paper, we present a novel technique to automatically generate descriptive summary comments for Java methods. Given the signature and body of a method, our automatic comment generator identifies the content for the summary and generates natural language text that summarizes the method\u2019s overall actions.  According to programmers who judged our generated comments, the summaries are accurate, do not miss important content, and are reasonably concise.", "num_citations": "2\n", "authors": ["981"]}
{"title": "Differentiating roles of program elements in action-oriented concerns\n", "abstract": " Many techniques have been developed to help programmers locate source code that corresponds to specific functionality, i.e., concern or feature location, as it is a frequent software maintenance activity. This paper proposes operational definitions for differentiating the roles that each program element of a concern plays with respect to the concern's implementation. By identifying the respective roles, we enable evaluations that provide more insight into comparative performance of concern location techniques. To provide definitions that are specific enough to be useful in practice, we focus on the subset of concerns that are action-oriented. We also conducted a case study that compares concern mappings derived from our role definitions with three developers' mappings across three concerns. The results suggest that our definitions capture the majority of developer-identified elements and that control-flow islands (i.e\u00a0\u2026", "num_citations": "1\n", "authors": ["981"]}
{"title": "Mobile interaction and query optimizationin a protein-ligand data analysis system\n", "abstract": " With current trends in integrating phylogenetic analysis into pharma-research, computing systems that integrate the two areas can help the drug discovery field. DrugTree is a tool that overlays ligand data on a protein-motivated phylogenetic tree. While initial tests of DrugTree are successful, it has been noticed that there are a number of lags concerning querying the tree. Due to the interleaving nature of the data, query optimization can become problematic since the data is being obtained from multiple sources, integrated and then presented to the user with the phylogenetic imposed upon the phylogenetic analysis layer. This poster presents our initial methodologies for addressing the query optimization issues. Our approach applies standards as well as uses novel mechanisms to help improve performance time.", "num_citations": "1\n", "authors": ["981"]}
{"title": "Evaluating a Software Word Usage Model for C+\n", "abstract": " Currently, there are many automatic and semiautomatic tools to expedite software maintenance; however, most of these tools rely solely on the structural model of the program, while disregarding any semantic information from the natural language used by the programmer. In previous work towards solving this problem, we develepod a Software Word Usage Model (SWUM) for Java. SWUM enables software engineering tools to apply linguistic relations between words to form a more complete interpretation of the program. Although SWUM is currently defined for Java, we believe that SWUM is capable of representing programs in different programming languages. This paper focuses on investigating the generality and extensibility of SWUM for programming languages beyond Java. The potential structural, semantic, and syntactic modifications of SWUM for other languages were examined, particularly analyzing the differences between Java and C++. We evaluated the effectiveness of the phrases generated from SWUM for C++ code, and modified the SWUM construction algorithm to handle C++ features as needed.", "num_citations": "1\n", "authors": ["981"]}
{"title": "Needed: a common denominator\n", "abstract": " GENERALLY, some degree of tension may be expected to exist between students and teachers in any school. One group is striving to gain the understanding of ideas and concepts to which the other group appears to hold the key. Students seek to attain standards set by teachers; teachers seek to impart knowledge sought by students. Such is the world of education and these tensions serve to keep the process alive and exciting. However, when the tensions rise to higher levels and encompass strong expressions of dissatisfaction with a program, disillusionment with subjects, and dissonance in values between learner and teacher, then the process of education becomes a contest filled with frustration and lacking in a sense of achievement.", "num_citations": "1\n", "authors": ["981"]}
{"title": "Teaching as guidance of learning\n", "abstract": " THERE are many definitions of teaching, some of them complex, but a very simple one is that it is any activity designed to influence learning toward a pre-determined goal. The learning may be cognitive, affective or psychomotor. Teaching has a distinctive connotation of rational explanation and critical dialogue, the goal of which is to Israel Sheffler. Conditions of Knowledg develop learning in which the student will be capable of backing his beliefs by appropriate and sufficient means\" 1). Teaching is a complex form of interaction between teacher and students or between students and is concerned with subject matter of some kind. It is the sort of interaction which exposes the teacher's underlying judgement to the critical evaluation of students and invites the student to form and submit his own judgement to critical appraisal (2). In this view the teacher is seen as a facilitator of learning rather than a transmitter of knowledge alone, and learners become active participants in the process of their own learning. Teaching is an essentially human and humane interaction. Kaoru Yamamoto. Humanizing College Teaching\"\" To teach is to touch someone's life in progress and in so doing, one hardly remains untouched oneself.\"(3)", "num_citations": "1\n", "authors": ["981"]}