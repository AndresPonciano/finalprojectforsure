{"title": "Open source software peer review practices\n", "abstract": " Peer review is seen as an important quality assurance mechanism in both industrial development and the open source software (OSS) community. The techniques for performing inspections have been well studied in industry; in OSS development, peer reviews are less well understood. We examine the two peer review techniques used by the successful, mature Apache server project: review-then-commit and commit-then-review. Using archival records of email discussion and version control repositories, we construct a series of metrics that produces measures similar to those used in traditional inspection experiments. Specifically, we measure the frequency of review, the level of participation in reviews, the size of the artifact under review, the calendar time to perform a review, and the number of reviews that find defects. We provide a comparison of the two Apache review techniques as well as a comparison of\u00a0\u2026", "num_citations": "269\n", "authors": ["170"]}
{"title": "The GNOME project: a case study of open source, global software development\n", "abstract": " Many successful free/open source software (FOSS) projects start with the premise that their contributors are rarely colocated, and as a consequence, these projects are cases of global software development (GSD). This article describes how the GNOME Project, a large FOSS project, has tried to overcome the disadvantages of GSD. The main goal of GNOME is to create a GUI desktop for Unix systems, and encompasses close to two million lines of code. More than 500 individuals (distributed across the world) have contributed to the project. This article also describes the software development methods and practices used by the members of the project, and its organizational structure. The article ends by proposing a list of practices that could benefit other global software development projects, both FOSS and commercial. Copyright \u00a9 2004 John Wiley & Sons, Ltd.", "num_citations": "231\n", "authors": ["170"]}
{"title": "On the use of visualization to support awareness of human activities in software development: a survey and a framework\n", "abstract": " This paper proposes a framework for describing, comparing and understanding visualization tools that provide awareness of human activities in software development. The framework has several purposes--it can act as a formative evaluation mechanism for tool designers; as an assessment tool for potential tool users; and as a comparison tool so that tool researchers can compare and understand the differences between various tools and identify potential new research areas. We use this framework to structure a survey of visualization tools for activity awareness in software development. Based on this survey we suggest directions for future research.", "num_citations": "201\n", "authors": ["170"]}
{"title": "How social and communication channels shape and challenge a participatory culture in software development\n", "abstract": " Software developers use many different communication tools and channels in their work. The diversity of these tools has dramatically increased over the past decade and developers now have access to a wide range of socially enabled communication channels and social media to support their activities. The availability of such social tools is leading to a participatory culture of software development, where developers want to engage with, learn from, and co-create software with other developers. However, the interplay of these social channels, as well as the opportunities and challenges they may create when used together within this participatory development culture are not yet well understood. In this paper, we report on a large-scale survey conducted with 1,449 GitHub users. We discuss the channels these developers find essential to their work and gain an understanding of the challenges they face using them\u00a0\u2026", "num_citations": "157\n", "authors": ["170"]}
{"title": "An empirical study of fine-grained software modifications\n", "abstract": " Software is typically improved and modified in small increments (we refer to each of these increments as a modification record\u2014MR). MRs are usually stored in a configuration management or version control system and can be retrieved for analysis. In this study we retrieved the MRs from several mature open software projects. We then concentrated our analysis on those MRs that fix defects and provided heuristics to automatically classify them. We used the information in the MRs to visualize what files are changed at the same time, and who are the people who tend to modify certain files. We argue that these visualizations can be used to understand the development stage of in which a project is at a given time (new features are added, or defects are being fixed), the level of modularization of a project, and how developers might interact between each other and the source code of a system.", "num_citations": "157\n", "authors": ["170"]}
{"title": "Will my patch make it? and how fast? case study on the linux kernel\n", "abstract": " The Linux kernel follows an extremely distributed reviewing and integration process supported by 130 developer mailing lists and a hierarchy of dozens of Git repositories for version control. Since not every patch can make it and of those that do, some patches require a lot more reviewing and integration effort than others, developers, reviewers and integrators need support for estimating which patches are worthwhile to spend effort on and which ones do not stand a chance. This paper crosslinks and analyzes eight years of patch reviews from the kernel mailing lists and committed patches from the Git repository to understand which patches are accepted and how long it takes those patches to get to the end user. We found that 33% of the patches makes it into a Linux release, and that most of them need 3 to 6 months for this. Furthermore, that patches developed by more experienced developers are more easily\u00a0\u2026", "num_citations": "148\n", "authors": ["170"]}
{"title": "Automating the measurement of open source projects\n", "abstract": " The proliferation of open source projects raises a number of vital economic, social, and software engineering questions that are subject of intense research. Based on experience analyzing numerous open source and commercial projects we propose a set of tools to support extraction and validation of software project data. Such tools would streamline empirical investigation of open source projects and make it possible to test existing and new theories about the nature of open source projects. Our software includes tools to extract and summarize information from mailing lists, CVS logs, ChangeLog files, and defect tracing databases. More importantly, it cross-links records from various data sources and identifies all contributors for a software change. We illustrate some of the capabilities by analyzing data from Ximian Evolution project.", "num_citations": "135\n", "authors": ["170"]}
{"title": "Contemporary peer review in action: Lessons from open source development\n", "abstract": " Do you use software peer reviews? Are you happy with your current code review practices? Even though formal inspection is recognized as one of the most effective ways to improve software quality, many software organizations struggle to effectively implement a formal inspection regime. Open source projects use an agile peer review process-based on asynchronous, frequent, incremental reviews that are carried out by invested codevelopers-that contrasts with heavyweight inspection processes. The authors describe lessons from the OSS process that transfer to proprietary software development. They also present a selection of popular tools that support lightweight, collaborative, code review processes and nonintrusive metric collection.", "num_citations": "125\n", "authors": ["170"]}
{"title": "Mining CVS repositories, the softChange experience.\n", "abstract": " CVS logs are a rich source of software trails (information left behind by the contributors to the development process, usually in the forms of logs). This paper describes how softChange extracts these trails, and enhances them. This paper also addresses some challenges that CVS fact extraction poses to researchers.", "num_citations": "114\n", "authors": ["170"]}
{"title": "A survey and evaluation of tool features for understanding reverse\u2010engineered sequence diagrams\n", "abstract": " Sequence diagrams can be valuable aids to software understanding. However, they can be extremely large and hard to understand in spite of using modern tool support. Consequently, providing the right set of tool features is important if the tools are to help rather than hinder the user. This paper surveys research and commercial sequence diagram tools to determine the features they provide to support program understanding. Although there has been significant effort in developing these tools, many of them have not been evaluated using human subjects. To begin to address this gap, a preliminary study was performed with a specially designed sequence diagram tool that implements the features found during the survey. On the basis of an analysis of the study results, we discuss the features that were found to be useful and relate these to the tasks performed. It concludes by proposing how future tools can be\u00a0\u2026", "num_citations": "105\n", "authors": ["170"]}
{"title": "Towards understanding twitter use in software engineering: preliminary findings, ongoing challenges and future questions\n", "abstract": " There has been some research conducted around the motivation for the use of Twitter and the value brought by micro-blogging tools to individuals and business environments. This paper builds on our understanding of how the phenomenon affects the population which birthed the technology: Software Engineers. We find that the Software Engineering community extensively leverages Twitter's capabilities for conversation and information sharing and that use of the tool is notably different between distinct Software Engineering groups. Our work exposes topics for future research and outlines some of the challenges in exploring this type of data.", "num_citations": "92\n", "authors": ["170"]}
{"title": "Using software trails to reconstruct the evolution of software\n", "abstract": " This paper describes a method to recover the evolution of a software system using its software trails: information left behind by the contributors to the development process of the product, such as mailing lists, Web sites, version control logs, software releases, documentation, and the source code. This paper demonstrates the use of this method by recovering the evolution of Ximian Evolution, a mail client for Unix. By extracting useful facts stored in these software trails and correlating them, it was possible to provide a detailed view of the history of this project. This view provides interesting insight into how an open source software project evolves and some of the practices used by its software developers. Copyright \u00a9 2004 John Wiley & Sons, Ltd.", "num_citations": "90\n", "authors": ["170"]}
{"title": "Peer review on open-source software projects: Parameters, statistical models, and theory\n", "abstract": " Peer review is seen as an important quality-assurance mechanism in both industrial development and the open-source software (OSS) community. The techniques for performing inspections have been well studied in industry; in OSS development, software peer reviews are not as well understood. To develop an empirical understanding of OSS peer review, we examine the review policies of 25 OSS projects and study the archival records of six large, mature, successful OSS projects. We extract a series of measures based on those used in traditional inspection experiments. We measure the frequency of review, the size of the contribution under review, the level of participation during review, the experience and expertise of the individuals involved in the review, the review interval, and the number of issues discussed during review. We create statistical models of the review efficiency, review interval, and effectiveness\u00a0\u2026", "num_citations": "86\n", "authors": ["170"]}
{"title": "Towards a unified catalog of hypermedia design patterns\n", "abstract": " There has been a recent increase in the number of published design patterns for hypermedia. Some of these patterns have been evolving, while others have remained untouched. This paper attempts to list all the patterns currently known, tracing the different publications in which they have appeared. The patterns are scrutinized and refined: some patterns are unified into one; some are deemed special cases of other patterns; some patterns are renamed. At the same time, we propose to rewrite the patterns in a vocabulary that is uniform, and to use similar pattern templates. We then discuss the creation of a design patterns system, which organizes the patterns and assists the designer in the process of recognizing the problems and their potential solutions. Finally we propose a subset of the patterns which should conform a catalog of basic patterns; this catalog will attempt to address the most common problems\u00a0\u2026", "num_citations": "80\n", "authors": ["170"]}
{"title": "A preliminary examination of code review processes in open source projects\n", "abstract": " This paper represents a first attempt to understand the code review processes used by open source projects. Although there have been many studies of open source projects [1, 2, 4], these studies have focused on the entire development process and community of the project or projects. We are not aware of any paper that has examined the review process of open source projects in depth or compared the review processes used among projects. We examined the stated and observed code review processes used by 11 open source projects; the four most interesting projects are discussed. Additionally, we examined the mature, well-known Apache server project in depth. We extracted the developer and commit mailing lists into a database in order to reconstruct and understand the review and patch processes of the Apache project. The paper is broken into six sections. The remainder of this section introduces our research questions. The second section introduces our research methodology and data extraction techniques. The third section discusses the actors in the process, introduces a general patch process, and contrasts the formal and observed processes used by GCC, Linux, Mozilla, and Apache. The fourth section discusses some observed review patterns. The fifth section quantitatively answers our research questions using the Apache data (although we discuss our findings, due to time and other considerations, we leave the development of hypotheses to future work). In the final section we present our conclusions and future work.", "num_citations": "59\n", "authors": ["170"]}
{"title": "Pannini: A New Projection for RenderingWide Angle Perspective Images.\n", "abstract": " The widely used rectilinear perspective projection cannot render realistic looking flat views with fields of view much wider than 70\u25e6. Yet 18th century artists known as \u2018view painters\u2019 depicted wider architectural scenes without visible perspective distortion. We have found no written records of how they did that, however, quantitative analysis of several works suggests that the key is a system for compressing horizontal angles while preserving certain straight lines important for the perspective illusion.We show that a simple double projection of the sphere to the plane, that we call the Pannini projection, can render images 150\u25e6 or more wide with a natural appearance, reminiscent of vedutismo perspective. We give the mathematical formulas for realizing it numerically, in a general form that can be adjusted to suit a wide range of subject matter and field widths, and briefly compare it to other proposed alternatives to the\u00a0\u2026", "num_citations": "37\n", "authors": ["170"]}
{"title": "A comparative exploration of freebsd bug lifetimes\n", "abstract": " In this paper, we explore the viability of mining the basic data provided in bug repositories to predict bug lifetimes. We follow the method of Lucas D. Panjer as described in his paper, Predicting Eclipse Bug Lifetimes. However, in place of Eclipse data, the FreeBSD bug repository is used. We compare the predictive accuracy of five different classification algorithms applied to the two data sets. In addition, we propose future work on whether there is a more informative way of classifying bugs than is considered by current bug tracking systems.", "num_citations": "36\n", "authors": ["170"]}
{"title": "Visualizing software architecture evolution using change-sets\n", "abstract": " When trying to understand the evolution of a software system it can be useful to visualize the evolution of the system's architecture. Existing tools for viewing architectural evolution assume that what a user is interested in can be described in an unbroken sequence of time, for example the changes over the last six months. We present an alternative approach that provides a lightweight method for examining the net effect of any set of changes on a system's architecture. We also present Motive, a prototype tool that implements this approach, and demonstrate how it can be used to answer questions about software evolution by describing case studies we conducted on two Java systems.", "num_citations": "35\n", "authors": ["170"]}
{"title": "How the R community creates and curates knowledge: a comparative study of stack overflow and mailing lists\n", "abstract": " One of the many effects of social media in software development is the flourishing of very large communities of practice where members share a common interest, such as programming languages, frameworks, and tools. These communities of practice use many different communication channels but little is known about how these communities create, share, and curate knowledge using such channels. In this paper, we report a qualitative study of how one community of practice---the R software development community---creates and curates knowledge associated with questions and answers (Q&A) in two of its main communication channels: the R-tag in Stack Overflow and the R-users mailing list. The results reveal that knowledge is created and curated in two main forms: participatory, where multiple members explicitly collaborate to build knowledge, and crowdsourced, where individuals work independently of\u00a0\u2026", "num_citations": "33\n", "authors": ["170"]}
{"title": "A framework for describing and understanding mining tools in software development\n", "abstract": " We propose a framework for describing, comparing and understanding tools for the mining of software repositories. The fundamental premise of this framework is that mining should be done by considering the specific needs of the users and the tasks to be supported by the mined information. First, different types of users have distinct needs, and these needs should be taken into account by tool designers. Second, the data sources available, and mined, will determine if those needs can be satisfied. Our framework is based upon three main principles: the type of user, the objective of the user, and the mined information. This framework has the following purposes: to help tool designers in the understanding and comparison of different tools, to assist users in the assessment of a potential tool; and to identify new research areas. We use this framework to describe several mining tools and to suggest future research\u00a0\u2026", "num_citations": "32\n", "authors": ["170"]}
{"title": "Using software distributions to understand the relationship among free and open source software projects\n", "abstract": " Success in the open source software world has been measured in terms of metrics such as number of downloads, number of commits, number of lines of code, number of participants, etc. These metrics tend to discriminate towards applications that are small and tend to evolve slowly. A problem is, however, how to identify applications in these latter categories that are important. Software distributions specify the dependencies needed to build and to run a given software application. We use this information to create a dependency graph of the applications contained in such a distribution. We explore the characteristics of this graph, and use it to define some metrics to quantify the dependencies (and dependents) of a given software application. We demonstrate that some applications that are invisible to the final user (such as libraries) are widely used by end-user applications. This graph can be used as a proxy to\u00a0\u2026", "num_citations": "31\n", "authors": ["170"]}
{"title": "The Semantic Web\n", "abstract": " 6. An evolution rules language allows inference rules to be given which allow a machine with a certain algorithm to convert documents from one RDF schema into another. 7. The logical layer turns a limited declarative language into a Turing-complete logical language, with inference and functions.", "num_citations": "31\n", "authors": ["170"]}
{"title": "A study of the contributors of PostgreSQL\n", "abstract": " This report describes some characteristics of the development team of PostgreSQL that were uncovered by analyzing the history of its software artifacts as recorded by the project's CVS repository.", "num_citations": "31\n", "authors": ["170"]}
{"title": "Experiences teaching a graduate course in open source software engineering\n", "abstract": " This paper describes the early experiences of a graduate course in open source software engineering at the Department of Computer Science at the University Victoria. It includes a description of the motivation for the course, its structure and evaluation methods. It concludes with a discussion of the lessons learned and its future.", "num_citations": "30\n", "authors": ["170"]}
{"title": "The evolution of the GNOME Project\n", "abstract": " The GNOME Project is an attempt to create a GUI desktop for Unix systems. Originally started by a handful of volunteers in 1996, GNOME has become the desktop of choice for Solaris, HP-UX, and Red Hat Linux, and it is currently developed by a team of approximately five hundred people around the world. The importance of GNOME to the Unix world has attracted the attention of several software companies who are actively participating in its development. At the same time, some of its volunteer developers have created enterprises who expect to sell services and products around GNOME. This extended abstract describes, first, the development model of GNOME, then the influence that private companies had had on the project: on one hand they are contributing a large amount of resources to the project, accelerating its development, and increasing its reliability and documentation; and on the other hand, the GNOME Foundation has been created to maintain the goal of the project to provide a free (as in freedom) software desktop for Unix, and avoid that the commercial interests of these partners could jeopardize the interests of the community.", "num_citations": "27\n", "authors": ["170"]}
{"title": "Working with\u2019monster\u2019traces: Building a scalable, usable, sequence viewer\n", "abstract": " In this position paper, we survey and identify tool features that provide cognitive support for reverse engineering and program comprehension of very large reverse engineered sequence diagrams. From these features we synthesize user requirements for a sequence diagram viewer, to which we add system requirements such as memory and processing scalability. We briefly describe a pluggable sequence viewer that meets these requirements and discuss some open questions that we are currently exploring.", "num_citations": "23\n", "authors": ["170"]}
{"title": "Microarray classification from several two-gene expression comparisons\n", "abstract": " We describe our contribution to the ICMLA2008 \u00bfAutomated Micro-Array Classification Challenge\u00bf. The design of our classifier is motivated by the special scenario encountered in molecular cancer classification based on the mRNA concentrations provided by gene microarray data. Our classifier is rank-based; it only depends on expression comparisons among selected pairs of genes. Such comparisons are invariant to most of the transformations involved in preprocessing and normalization. Every pair of genes determines a binary classifier - choose the class for which the observed ordering is most likely. Pairs are scored by maximizing accuracy. In our k-TSP (k-disjoint Top Scoring Pairs) classifier, k disjoint pairs of genes are learned from training data; the discriminant function is simply the difference in the number of votes for the two classes. This rule involves exactly 2k genes, is readily interpretable, and\u00a0\u2026", "num_citations": "21\n", "authors": ["170"]}
{"title": "How the R community creates and curates knowledge: an extended study of stack overflow and mailing lists\n", "abstract": " One of the effects of social media\u2019s prevalence in software development is the many flourishing communities of practice where users share a common interest. These large communities use many different communication channels, but little is known about how they create, share, and curate knowledge using such channels. In this paper, we report a mixed methods study of how one community of practice, the R software development community, creates and curates knowledge associated with questions and answers (Q&A) in two of its main communication channels: the R tag in Stack Overflow and the R-Help mailing list. The results reveal that knowledge is created and curated in two main forms: participatory, where multiple users explicitly collaborate to build knowledge, and crowdsourced, where individuals primarily work independently of each other. Moreover, we take a unique approach at slicing the data\u00a0\u2026", "num_citations": "20\n", "authors": ["170"]}
{"title": "Enhancing code for readability and comprehension using SGML\n", "abstract": " Reading and understanding programs is a key activity in software reengineering, development, and maintenance. The ability of people to understand programs is directly related to the ease with which the source code and documentation can be read. Thus, enhancements to the style of presentation should heighten this comprehensibility. We describe methods that use markup languages such as SGML to embed information about the syntax and semantics of a program in the program code, and then show how these can be used to enhance its presentation style. We also briefly discuss the extension of these markup language concepts to text databases, and indicate how they can support various structural views of the code through browsing techniques associated with database queries. 1 Introduction A key step in reengineering a software system is extracting the enterprise rules and meaning in order to re-implement the process using alternative software technologies. Reengineering requires a careful scannin...", "num_citations": "20\n", "authors": ["170"]}
{"title": "New Methods to Project Panoramas for Practical and Aesthetic Purposes.\n", "abstract": " Recent advances in digital photomontage have simplified the creation of extreme wide-angle views from a vantage point, including the recreation of the entire sphere (we will refer to these type of images as panoramas). In order to minimize the distortion from the point of view of the viewer, panoramas have been typically presented using curved displays (such as the original panoramas, by Barker, in 1787; or several cinematographic systems, such as Circle-Vision 360, still in use), and more recently with the help of the computer (such as the QuickTime VR format). Unfortunately requiring such systems restricts their use, and little research has been done in the representation of panoramas into a flat surface. In this paper we propose the use of several geographic map projections to project a panorama into a flat surface, both for realistic purposes (where the projection can be easily accepted as a faithful representation\u00a0\u2026", "num_citations": "19\n", "authors": ["170"]}
{"title": "Three hypermedia design patterns\n", "abstract": " Hypermedia authors are now acquiring experience to build good hypermedia systems and assess their quality, and this experience should be compiled and organized in order to be shared with other authors. We propose three hypermedia design patterns for hypermedia publishing: hyper-book, hyper-map and virtual product.", "num_citations": "18\n", "authors": ["170"]}
{"title": "The impact of context metrics on just-in-time defect prediction\n", "abstract": " Traditional just-in-time defect prediction approaches have been using changed lines of software to predict defective-changes in software development. However, they disregard information around the changed lines. Our main hypothesis is that such information has an impact on the likelihood that the change is defective. To take advantage of this information in defect prediction, we consider n-lines (n =\u20091,2,\u2026) that precede and follow the changed lines (which we call context lines), and propose metrics that measure them, which we call \u201cContext Metrics.\u201d Specifically, these context metrics are defined as the number of words/keywords in the context lines. In a large-scale empirical study using six open source software projects, we compare the performance of using our context metrics, traditional code churn metrics (e.g., the number of modified subsystems), our extended context metrics which measure not only\u00a0\u2026", "num_citations": "17\n", "authors": ["170"]}
{"title": "Flattening the Viewable Sphere.\n", "abstract": " The viewable sphere corresponds to the space that surrounds us. The evolution of photography and panoramic software and hardware has made it possible for anybody to capture the viewable sphere. It is now up to the artist to determine what can be done with this raw material. In this paper we explore the underdeveloped field of flat panoramas from an artistic point of view. We argue that its future lies in the exploration of conformal mappings, specialized software, and the interaction of its practitioners via the Internet.", "num_citations": "17\n", "authors": ["170"]}
{"title": "Hadez, a framework for the specification and verification of hypermedia applications\n", "abstract": " In recent years, several methodologies for the development of hypermedia applications have been proposed.   These methodologies are, primarily, guidelines to be followed during the design process.   They also indicate what deliverables should be created at each of their stages.   These products are usually informally specified - in the sense that they do not have formal syntax nor formally defined semantics - and they are not required to pass validity tests.        Hadez formally specifies the design of a hypermedia application, supports the verification of properties of the specification, and promotes the reuse of design.        Hadez is an object-oriented specification language with formal syntax and semantics.   Hadez is based on the formal specification languages Z and Z++, with extensions unique to hypermedia.   It uses set theory and first order predicate logic.   It divides the specification of a hypermedia application into three main parts: its conceptual schema, which describes the domain-specific data and its relationships; its structural schema, which describes how this data is combined and gathered into more complex entities, called composites; and the perspective schema, which uses Abstract Design Perspectives (artifacts unique to Hadez) to indicate how these composites are mapped to hyperpages, and how the user interacts with them.        Hadez provides a formal framework in which properties of a specification can be specified and answered.        The specification of an application should not constrain its implementation and, therefore, it is independent of the platform in which the application is to be presented.   As a consequence, the same\u00a0\u2026", "num_citations": "17\n", "authors": ["170"]}
{"title": "Open source peer review\u2013lessons and recommendations for closed source\n", "abstract": " Use asynchronous, frequent, incremental peer reviews conducted by invested experts supported by lightweight tools and non-intrusive metrics Although the effectiveness of software inspection (formal peer review) has a long and mostly supportive history, the thought of reviewing a large, unfamiliar software artifact over a period of weeks is something dreaded by both the author and the reviewers. The dislike of this cumbersome process is natural, but neither the formality nor the aversion are fundamental characteristics of peer review. The core idea of review is simply to get an expert peer to examine your work for problems that you are blind to. The actual process is much less important for finding defects than the expertise of the people involved [1]. While developers will acknowledge the value of inspection, most avoid it and adoption of traditional inspection practices remains relatively low [2],[3]. Surprisingly, peer review is a prevalent practice in Open Source Software (OSS) projects, which have minimalist processes and consist of intrinsically motivated developers. We examined over 100K peer reviews in case studies on the Apache", "num_citations": "16\n", "authors": ["170"]}
{"title": "Remixing visualization to support collaboration in software maintenance\n", "abstract": " We propose that collaborative software visualization can improve team software maintenance. We first review how visualization can support software maintenance from the perspectives of system understanding, process understanding and software evolution. From this, we conclude that visualization tools are rarely designed to provide explicit support for collaborative authoring and sharing of views. We then provide an overview of research from a computer supported cooperative work perspective, and propose that this research should be applied to software visualization. We explore the opportunities and challenges this research focus presents and conclude that more attention paid to the social aspects of software visualization should improve both individual and team processes in software maintenance.", "num_citations": "16\n", "authors": ["170"]}
{"title": "Intellectual property aspects of web publishing\n", "abstract": " This paper addresses how intellectual property affects the Web in general, and content publishing on the Web in particular. Before its commercialization, the Web was perceived as being free and unregulated; this assumption is no longer true. Nowadays, content providers need to know which practices on the Web can result in potential legal problems. The vast majority of Web sites are developed by individual such as technical writers or graphic artists, and small organizations, which receive limited or no legal advice. As a result, these Web sites are developed with little or no regard to the legal constraints of intellectual property law. In order to help this group of people, the paper tries to answer the following question: What are the (typical) legal issues for Web content providers to watch out for? This paper gives an overview of these legal issues for intellectual property (ie, copyrights, patents, and trademarks) and\u00a0\u2026", "num_citations": "15\n", "authors": ["170"]}
{"title": "A Formal Approach to Design Pattern Definition & Application\n", "abstract": " In this paper we present a formal approach to define and apply design patterns that is both process-and reuse-oriented. Initially we use a process program based on design pattern primitive tasks or constructors to describe how to instantiate a pattern. As we develop the patterns we introduce a formal model for the interconnected objects that constitute the instantiation. The formal model which is based on Abstract Data Views divides designs into both objects and views in order to maintain a separation of concerns. We have chosen a formal model for pattern definition and application since it allows us to specify the steps in pattern instantiation unambiguously and to reason about the completed design. Furthermore, a formal statement of the application of a design pattern can provide the foundation on which to build tools to assist the programmer in code generation.", "num_citations": "15\n", "authors": ["170"]}
{"title": "What is the gist? Understanding the use of public gists on GitHub\n", "abstract": " GitHub is a popular source code hosting site which serves as a collaborative coding platform. The many features of GitHub have greatly facilitated developers' collaboration, communication, and coordination. Gists are one feature of GitHub, which defines them as \"a simple way to share snippets and pastes with others.\" This three-part study explores how users are using Gists. The first part is a quantitative analysis of Gist metadata and contents. The second part investigates the information contained in a Gist: We sampled 750k users and their Gists (totalling 762k Gists), then manually categorized the contents of 398. The third part of the study investigates what users are saying Gists are for by reading the contents of web pages and twitter feeds. The results indicate that Gists are used by a small portion of GitHub users, and those that use them typically only have a few. We found that Gists are usually small and\u00a0\u2026", "num_citations": "14\n", "authors": ["170"]}
{"title": "GNOME, a case of open source global software development\n", "abstract": " The GNOME Project is an open source project which main goal is to create a GUI desktop for Unix systems, and encompases close to two million lines of code. It is composed by a group of more than 500 different contributors, distributed across the world Some companies employ several of these contributors with the hope of accelerating the development of the project, but many other contributors are volunteers. The project is divided into several dozen modules, ranging from libraries (such as GUI, CORBA, XML, etc) to core applications (such as email client, graphical editor, word processor, spreadsheet, etc). This paper describes the organization and management of the project and describes the infrastructure needed by a contributor, how contributors work as independently together, but still with a common goal. It also describes how requirement gathering takes place, and its unique administration structure, rooted in the GNOME Foundation, a body created solely to oversee the current and future development of the project.", "num_citations": "14\n", "authors": ["170"]}
{"title": "Formalizing the specification of web applications\n", "abstract": " As the size of Web applications grows, it becomes clear that we need better tools to deal with their growing complexity. The current trend has been to assist the developer during the implementation stage, with little or no emphasis in the design process. Formal specification languages allow the unambiguous description of the properties of a system without restricting its implementation. Formal languages can be used to verify properties about the design. We present in this paper Flash, a formal specification language for hypertext design. Based in set theory, Flash is a formal system that attempts to separate the different tasks faced during the design process. A Flash specification first formalizes the content of the application and its relationships. Then it collates that content into navigational composites. Finally, it specifies how those composites can be navigated. Each stage is clearly specified with precise\u00a0\u2026", "num_citations": "14\n", "authors": ["170"]}
{"title": "Perspectives on bugs in the debian bug tracking system\n", "abstract": " Bugs in Debian differ from regular software bugs. They are usually associated with packages, instead of software modules. They are caused and fixed by source package uploads instead of code commits. The majority are reported by individuals who appear in the bug database once, and only once. There also exists a small group of bug reporters with over 1,000 bug reports each to their name. We also explore our idea that a high bug-frequency for an individual package might be an indicator of popularity instead of poor quality.", "num_citations": "13\n", "authors": ["170"]}
{"title": "Experiments with the Z Interchange Format and SGML\n", "abstract": " Standards, if widely accepted, encourage the development of tools and techniques to process objects conforming to that standard. This paper describes a number of experiments using available tools to process text containing Z specifications adhering to the existing Z Interchange Format. The experiments resulted in tools that could be used in specific programming environments where Z was used to describe software systems.", "num_citations": "11\n", "authors": ["170"]}
{"title": "A multicollaborative push-caching http protocol for the WWW\n", "abstract": " We propose a caching protocol designed to automatically mirror heavily accessed WWW pages in a distributed and temporal fashion. The proposed caching mechanism differs from proxy type mechanisms in that it caches according to load pattern at the server side, instead of access patterns at the client-side LAN, in a Demand-based Document Dissemination (DDD) system fashion. This type of server initiated caching scheme has been termed push-caching. As well, the proposed caching scheme incorporates topological caching functions. The proposed protocol is orthogonal to other extensions to the HTTP protocol and other caching schemes already in use.", "num_citations": "11\n", "authors": ["170"]}
{"title": "Herding cats: a case study of release management in an open collaboration ecosystem\n", "abstract": " Release management in large-scale software development projects requires significant communication and coordination. It is particularly challenging in Free and Open Source Software (FOSS) ecosystems, in which hundreds of loosely connected developers and their projects need to be coordinated to release software to a schedule. To better understand this process and its challenges, we analyzed over two and half years of communication in the GNOME ecosystem and studied developers\u2019 interactions. We cataloged communication channels, categorized high level communication and coordination activities in one of them, and triangulated our results by interviewing developers. We found that a release schedule, influence instead of direct control, and diversity are factors that impact positively the release process in the GNOME ecosystem. Our results can help organizations build better large-scale teams\u00a0\u2026", "num_citations": "10\n", "authors": ["170"]}
{"title": "The right to a contribution: An exploratory survey on how organizations address it\n", "abstract": " Free and Open Source Software (FOSS) projects are characterized by the opportunity to attract external contributors, where contributions can be in any form of copyrightable material, such as code or documentation. In most of them it is understood that contributions would be licensed in similar or compatible terms than the project\u2019s license. Some projects require a copyright transfer from the contributor to an organization for the work contributed to a project, such documents are known as copyright assignment agreements. In a way, it is similar to the copyright transfer than some researchers grant to a publisher. In this work we present an exploratory survey of the multiple visions of copyright assignments, and aggregate them in a work that researchers and practitioners could use to get informed of the alternatives available in the literature. We expect that our findings help inform practitioners on legal concerns\u00a0\u2026", "num_citations": "10\n", "authors": ["170"]}
{"title": "A comparative evaluation of feature detectors on historic repeat photography\n", "abstract": " This study reports on the quantitative evaluation of a set of state-ofthe- art feature detectors in the context of repeat photography. Unlike most related work, the proposed study assesses the performance of feature detectors when intra-pair variations are uncontrolled and due to a variety of factors (landscape change, weather conditions, different acquisition sensors). There is no systematic way to model the factors inducing image change. The proposed evaluation is performed in the context of image matching, i.e. in conjunction with a descriptor and matching strategy. Thus, beyond just comparing the performance of these detectors, we also examine the feasibility of feature-based matching on repeat photography. Our dataset consists of a set of repeat and historic images pairs that are representative for the database created by the Mountain Legacy Project www.mountainlegacy.ca.", "num_citations": "10\n", "authors": ["170"]}
{"title": "A world of difference\n", "abstract": " Data show why inequality is a hot topic around the world. In the United States, the top 20% of earners take home a whopping 51% of income, and the share of income going to the top 1% has ballooned in the past 30 years, according to data from the U.S. Census Bureau and the World Top Incomes Database. Yet U.S. incomes are more equal than those of many countries, such as South Africa and Brazil, while Norway and Sweden are bastions of equality, as shown by data prepared especially for Science by researchers at the Luxembourg Income Study Center.", "num_citations": "9\n", "authors": ["170"]}
{"title": "The future of continuous integration in GNOME\n", "abstract": " In Free and Open Source Software (FOSS) projects based on Linux systems, the users usually install the software from distributions. The distributions act as intermediaries between software developers and users. Distributors collect the source code of the different projects and package them, ready to be installed by the users. Packages seems to work well for managing and distributing stable major and minor releases. It presents, however, various release management challenges for developers of projects with multiples dependencies not always available in the stable version of their systems. In projects like GNOME, composed of dozens of individual components, developers must build newer versions of the libraries and applications that their applications depend upon before working in their own projects. This process can be cumbersome for developers who are not programmers, such as user interaction designers\u00a0\u2026", "num_citations": "9\n", "authors": ["170"]}
{"title": "Developing marking support within Eclipse\n", "abstract": " In this paper, we describe marking features provided in Gild, a set of plug-ins to support education in Eclipse developed at the University of Victoria. We discuss our requirements gathering techniques, design process and the challenges experienced during development of this tool. We also consider the problematic nature of student evaluation, particularly within the context of introductory Computer Science courses.", "num_citations": "9\n", "authors": ["170"]}
{"title": "Legal concerns of web site reverse engineering\n", "abstract": " Researchers involved in Web site reverse engineering are often not aware of potential legal implications of using someone else's Web site for experimentation. Even if researchers are concerned with legal problems, there is little guidance available. This paper explores the legality of Web site reverse engineering with the intent to raise awareness among researchers about this issue. The discussed legal issues encompass copyright, contract, and trespass law.", "num_citations": "9\n", "authors": ["170"]}
{"title": "Dynamic views of SGML tagged documents\n", "abstract": " Product information is more frequently being delivered as hypertext webs or documents because of the availability of the World-Wide Web and the associated communications infrastructure. However, this type of document with its large number of files and hyperlinks can become very complex and present significant usability problems for the creator, maintainer and user. Because of this complexity it becomes extremely difficult to implement and maintain dynamic views of a document, a supposed advantage of a hyperlinked structure. In this paper we analyze some of the causes for these usability issues, and then describe some approaches that are being used to make significant improvements to this situation.", "num_citations": "9\n", "authors": ["170"]}
{"title": "cregit: Token-level blame information in git version control repositories\n", "abstract": " The blame feature of version control systems is widely used\u2014both by practitioners and researchers\u2014to determine who has last modified a given line of code, and the commit where this contribution was made. The main disadvantage of blame is that, when a line is modified several times, it only shows the last commit that modified it\u2014occluding previous changes to other areas of the same line. In this paper, we developed a method to increase the granularity of blame in git: instead of tracking lines of code, this method is capable of tracking tokens in source code. We evaluate its effectiveness with an empirical study in which we compare the accuracy of blame in git (per line) with our proposed blame-per-token method. We demonstrate that, in 5 large open source systems, blame-per-token is capable of properly identifying the commit that introduced a token with an accuracy between 94.5% and 99.2%, while\u00a0\u2026", "num_citations": "8\n", "authors": ["170"]}
{"title": "Herding cats in a FOSS ecosystem: a tale of communication and coordination for release management\n", "abstract": " Release management in large-scale software development projects requires significant communication and coordination. It is particularly challenging in Free and Open Source Software (FOSS) ecosystems, in which hundreds of loosely connected developers and their projects are coordinated to release software to a schedule. To better understand this process and its challenges, we analyzed over two and half years of communication in the GNOME ecosystem and studied developers\u2019 interactions. Through a case study, we cataloged communication channels, determined the main channel from which we categorized high level communication and coordination activities spanning five releases, and triangulated our results by interviewing ten key developers. We found that a release schedule, influence (instead of direct control), and diversity are the main factors that positively impact the release process in the GNOME ecosystem. We report a set of lessons learned that encapsulates our understanding of how the Release Management process function in a FOSS ecosystem, we learned that: (1) ensure that the release team follows the main communication channels used by developers, (2) provide a common place for coordination for an ecosystem, (3) consider including both good technical and social skills in a release team, (4) aim for a diverse release team, (5) based on lack of power, lobbying and consensus based management must be followed, (6) help the release team in the coordination process with a well defined schedule, and (7) release team work is different from regular software work. Our results can help organizations build better large\u00a0\u2026", "num_citations": "8\n", "authors": ["170"]}
{"title": "How to Wow with Photoshop Elements 5\n", "abstract": " Wouldn't it be great if you could have three of the world's top digital imaging experts sitting next to you at your computer, helping you navigate through the infinite possibilities available to you in Photoshop Elements 5? How to Wow with Photoshop Elements 5 is the next best thing. It combines the skills and techniques of Jack Davis and Mike McHugh along with the inspiring designs and creative projects of Wayne Rankin. All three are renowned authors, designers, and educators. Jack, Mike, and Wayne guide you step by step through real-world projects, with an emphasis on uncompromising quality, last-minute flexibility, and phenomenal speed. You'll learn tips and techniques on everything from sorting your digital photos to creating a digital scrapbook. Whether it's optimizing the color and tone of your image,... and turn your digital photographs into creative masterpieces! retouching cosmetic undesirables, reconstructing priceless heirlooms, or creating customized wrapping paper, you'll be given the tools at every stage of the process in order to learn How to Wow! The book begins with an introduction to the Elements 5 workspace, and then moves on to workflow (including file organizing, color settings, and Camera Raw), and then explores photo optimizing and retouching and repairing. Next comes the fun stuff with enhancing all aspects of your photography for maximum impact. You will produce stunning creative projects, including a digital watercolor and a multimedia VCD or DVD that you can watch on your home TV. You'll also create engaging presentations, and using the How to Wow presets, available on the CD accompanying this book, you\u00a0\u2026", "num_citations": "8\n", "authors": ["170"]}
{"title": "Merge\u2010Tree: Visualizing the integration of commits into Linux\n", "abstract": " With an average of more than 900 merges into the Linux kernel per release, many containing hundreds of commits and some containing thousands, maintenance of older versions of the kernel becomes nearly impossible. Various commercial products, such as the Android platform, run older versions of the kernel; due to security, performance, and changing hardware needs, maintainers must understand what changes (commits) are added to the current version of the kernel since the last time they inspected it to make the necessary patches. Current tools provide information about repositories through the directed acyclic graph (DAG) of the repository, which is helpful for smaller projects. However, with the scale and number of branches in the kernel, the DAG becomes overwhelming very quickly. Furthermore, the DAG contains every parents of every commit, while maintainers are more interested in how and when a\u00a0\u2026", "num_citations": "6\n", "authors": ["170"]}
{"title": "People analytics in software development\n", "abstract": " Developers are using more and more different channels and tools to collaborate, and integrations between these tools are becoming more prevalent. In turn, more data about developers\u2019 interactions at work will become available. These developments will likely make People Analytics \u2014 using data to show and improve how people collaborate\u00a0\u2014\u00a0more accessible and in turn more important for software developers. Even though developer collaboration has been the focus of several research groups and studies, we believe these changes will qualitatively change how some developers work. We provide an introduction to existing work in this field and outline where it could be headed.", "num_citations": "6\n", "authors": ["170"]}
{"title": "Using evolutionary annotations from change logs to enhance program comprehension\n", "abstract": " Evolutionary annotations are descriptions of how source code evolves over time. Typical source comments, given their static nature, are usually inadequate for describing how a program has evolved over time; instead, source code comments are typically a description of what a program currently does. We propose the use of evolutionary annotations as a way of describing the rationale behind changes applied to a given program (for example\" These lines were added to...\"). Evolutionary annotations can assist a software developer in the understanding of how a given portion of source code works by showing him how the source has evolved into its current form. In this paper we describe a method to automatically create evolutionary annotations from change logs, defect tracking systems and mailing lists. We describe the design of a prototype for Eclipse that can filter and present these annotations alongside their\u00a0\u2026", "num_citations": "6\n", "authors": ["170"]}
{"title": "SCC: automatic classification of code snippets\n", "abstract": " Determining the programming language of a source code file has been considered in the research community; it has been shown that Machine Learning (ML) and Natural Language Processing (NLP) algorithms can be effective in identifying the programming language of source code files. However, determining the programming language of a code snippet or a few lines of source code is still a challenging task. Online forums such as Stack Overflow and code repositories such as GitHub contain a large number of code snippets. In this paper, we describe Source Code Classification (SCC), a classifier that can identify the programming language of code snippets written in 21 different programming languages. A Multinomial Naive Bayes (MNB) classifier is employed which is trained using Stack Overflow posts. It is shown to achieve an accuracy of 75% which is higher than that with Programming Languages Identification (PLI a proprietary online classifier of snippets) whose accuracy is only 55.5%. The average score for precision, recall and the F1 score with the proposed tool are 0.76, 0.75 and 0.75, respectively. In addition, it can distinguish between code snippets from a family of programming languages such as C, C++ and C#, and can also identify the programming language version such as C# 3.0, C# 4.0 and C# 5.0.", "num_citations": "5\n", "authors": ["170"]}
{"title": "SCC++: predicting the programming language of questions and snippets of Stack Overflow\n", "abstract": " Stack Overflow is the most popular Q&A website among software developers. As a platform for knowledge sharing and acquisition, the questions posted on Stack Overflow usually contain a code snippet. Determining the programming language of a source code file has been considered in the research community; it has been shown that Machine Learning (ML) and Natural Language Processing (NLP) algorithms can be effective in identifying the programming language of source code files. However, determining the programming language of a code snippet or a few lines of source code is still a challenging task. Online forums such as Stack Overflow and code repositories such as GitHub contain a large number of code snippets. In this paper, we design and evaluate Source Code Classification (SCC++), a classifier that can identify the programming language of a question posted on Stack Overflow. The classifier\u00a0\u2026", "num_citations": "4\n", "authors": ["170"]}
{"title": "[engineering paper] scc: Automatic classification of code snippets\n", "abstract": " The following topics are dealt with: public domain software; software maintenance; Java; program diagnostics; learning (artificial intelligence); program compilers; C language; software reusability; source code (software); software engineering.", "num_citations": "4\n", "authors": ["170"]}
{"title": "Atlantis: Improving the analysis and visualization of large assembly execution traces\n", "abstract": " Assembly execution trace analysis is an effective approach for discovering potential software vulnerabilities. However, the size of the execution traces and the lack of source code makes this a manual, labor-intensive process. Instead of browsing billions of instructions one by one, software security analysts need higher-level information that can provide an overview of the execution of a program to assist in the identification of patterns of interest. The tool we present in this paper, Atlantis, is our trace analysis environment for multi-gigabyte assembly traces, and it contains a number of new features that make it particularly successful in meeting this goal. The contributions of this continuous work fall into three main categories: a) the ability to efficiently reconstruct and navigate the memory state of a program at any point in a trace; b) the ability to reconstruct and navigate functions and processes; and c) a powerful search\u00a0\u2026", "num_citations": "4\n", "authors": ["170"]}
{"title": "Opening Licensing and Databases\n", "abstract": " In the European Union databases have a special legal treatment that provides two levels of protection. A database is protected by copyright in the classical sense when it can be considered an intellectual work with a creative nature. Where databases represent mere collections of data without sufficient creativity to trigger copyright, EU jurisdictions protect the database under sui generis rights when substantial investment has been made in obtaining, verifying, or presenting the database contents according to Directive 96/9/EC. This system creates a substantial discrepancy between the situation of European countries and the rest of the world, and also affects those databases that have been released under open licenses. Not all of the currently available open licenses take account of the legal and practical implications of this discrepancy, and we should examine the consequences and options. The paper aims to\u00a0\u2026", "num_citations": "4\n", "authors": ["170"]}
{"title": "Apples Vs. Oranges? An exploration of the challenges of comparing the source code of two software systems\n", "abstract": " We attempt to compare the source code of two Java IDE systems: Netbeans and Eclipse. The result of this experiment shows that many factors, if ignored, could risk a bias in the results, and we posit various observations that should be taken into consideration to minimize such risk.", "num_citations": "4\n", "authors": ["170"]}
{"title": "The W3C and the WWW\n", "abstract": " The W3C and the WWW What types of infrastructure do we have available? The W3C W3C Seven Principles Page 1 The W3C and the WWW Where is the Web going? Daniel M. German Department of Computer Science University of Victoria 10\u20131 SEng 480b dmgerman@uvic.ca What types of infrastructure do we have available? 2 Delivery Infrastructure 2 Authoring Infrastructure 2 Development infrasturcture 10\u20132 SEng 480b dmgerman@uvic.ca The W3C 2 W3C\u2019s mission is to lead the Web to its full potential, 2 It developes developing technologies: 3 specifications, 3 guidelines 3 software 3 tools 10\u20133 SEng 480b dmgerman@uvic.ca W3C Seven Principles Universal Access Semantic Web Trust Interoperability Evolvability Decentralization Cooler Multimedia! 10\u20134 SEng 480b dmgerman@uvic.ca Page 2 Universal Access 2 W3C defines the Web as the universe of network-accessible information (available through , .\u2026", "num_citations": "4\n", "authors": ["170"]}
{"title": "Managing legal risks associated with intellectual property on the web\n", "abstract": " Intellectual Property (IP) has taken a prominent place on the web. Today's organisations need to know the ways in which their websites can be the target of costly IP litigation. Organisations also need to know how to manage and protect their own IP that they expose through their web presence. This paper provides an overview of the legal risks associated with IP on the web. Managing such risks begins with gaining a clear understanding of how to address the salient issues related to IP that any organisation has to take into account when it has a web presence or provides a service using the web. Towards this end, a comprehensive survey of existing IP case law in the context of web content is provided. The survey focuses on three essential IP areas: copyright, patents, and trademarks.", "num_citations": "4\n", "authors": ["170"]}
{"title": "A system of patterns for web navigation\n", "abstract": " In this paper we propose a system of design patterns for Web navigation. We have collected patterns already published in the literature, selected ten of them, refined them and identified the relationships among them. The selected patterns are rewritten in the Gang of Four (GoF) notation. They are implemented and integrated together leading to a framework intended to be used as the central part in developing data intensive Web applications.", "num_citations": "4\n", "authors": ["170"]}
{"title": "A longitudinal study on the maintainers' sentiment of a large scale open source ecosystem\n", "abstract": " Software development is a collaborative activity in which feelings and emotions can affect the developer's productivity, creativity, and contribution satisfaction. For example, the Linux Kernel Mailing List (LKML), which is used by subsystem maintainers to review patches sent by contributors, is known for its direct communication style, which is sometimes blamed as having a negative impact on contributors. In September 28, 2018, the kernel's lead maintainer, Linus Torvalds, announced that he would take a temporary break from the community, which led numerous members of the kernel community and observers from other communities to wonder to what extent this unexpected event could raise awareness about respectful interactions between community members. This paper performs an exploratory study in which we use an off-the-shelf sentiment mining tool to assess whether the maintainers' sentiment changed\u00a0\u2026", "num_citations": "3\n", "authors": ["170"]}
{"title": "On the Variability of the BSD and MIT Licenses\n", "abstract": " The MIT/X11 and the BSD are two of the most important family of Free and Open Source (FOSS) licenses. Because these licenses are to be inserted into the files that use it, and because they are expected to be changed by those who use them, their text has suffered alterations over time. Some of this variability is the result of licenses containing template fields which allow the license to be customized to include information such as the copyright holder name. Other variability can be attributed to changes in spelling, punctuation, and adding or removing conditions. This study empirically evaluated the extent that the BSD and MIT/X11 family of licenses are varied, and the manner and frequency in which license texts vary from the original definition. The study found that the BSD family has little variability, with a significant proportion fitting the common standard. The MIT/X11 family of licenses exhibited significantly\u00a0\u2026", "num_citations": "3\n", "authors": ["170"]}
{"title": "Doughty on the use and utility of government information and archives, 1933\n", "abstract": " In 1933, a Select Committee of Inquiry submitted a cost-cutting report to the federal government on the topic of government printing and publications. A copy of the report was sent to Dominion Archivist, Arthur Doughty. In his response to this report, Doughty attempted to convince the government of the benefits, uses and utility of archives, government records, and good information management. He did so through an almost stream-of-consciousness narrative, reporting on the many times that archives and government records had benefited the government. Doughty argued for the continued and continuing value of archival holdings. The criticisms he levied are familiar to modern users, the concerns he uttered are well-known, and his comments on the value of good records management could have been writtentoday. Indeed, some of his comments are just as applicable to modern digital copying. Doughty\u2019s commentary speaks across the generations as archivists will easily agree with his views on appropriate pay, records preservation, and the value of archives.R\u00c9SUM\u00c9En 1933, une commission sp\u00e9ciale d\u2019enqu\u00eate a soumis au gouvernement f\u00e9d\u00e9ral un rapport de r\u00e9duction des co\u00fbts pour l\u2019impression et la publication de documents gouvernementaux. Une copie de ce rapport a alors \u00e9t\u00e9 envoy\u00e9e \u00e0 l\u2019Archiviste du Dominion, Arthur Doughty. En r\u00e9ponse \u00e0 ce rapport, Doughty a tent\u00e9 de convaincre le gouvernement des avantages et de l\u2019utilit\u00e9 des archives et des documents gouvernementaux, ainsi que d\u2019une bonne gestion de documents. Pour ce faire, il a choisi comme instrument un r\u00e9cit presque sous la forme d\u2019un monologue int\u00e9rieur, dans lequel\u00a0\u2026", "num_citations": "3\n", "authors": ["170"]}
{"title": "A Commentary on Patrizia Gentile\u2019s \u201cResisted Access? National Security, the Access to Information Act, and Queer (ing) Archives\u201d\n", "abstract": " The Fall 2009 issue of Archivaria included an article by Dr. Patrizia Gentile, entitled \u201cResisted Access? National Security, the Access to Information Act, and Queer (ing) Archives,\u201d in which Dr. Gentile examines the Access to Information Act in the light of her own archival research into gay history. In doing so, Dr. Gentile suggests that the exemptions from release found in the Access to Information Act may be used to unfairly impede research, effect ively preventing a full and thorough understanding of the security service\u2019s past and possibly even future treatment of gay and lesbian Canadians. Since Dr. Gentile\u2019s article makes frequent reference to her research, which was carried out in the mid-to late-1990s at the then-National Archives of Canada, and she states \u201c[a] n analysis\u2026 illuminates how the Act, LAC, and ATIP officers themselves played a critical role in the researching and writing\u2026\u201d of her book (p. 146), in the spirit of full disclosure I must state that in the time in question I was employed by the National Archives as an ATIP (Access to Information and Privacy) officer, and that while I do not believe I ever met with her, I do remember her name and believe that it is possible that I worked on some of her requests to access archival records. In addition, she also cited an article of mine from Archivaria, dealing with the application of the Access to Information Act at the National Archives. In my own article I conceded that the legislation was imperfect and that its application to archival records could, at times, be difficult, and were that Dr. Gentile\u2019s position, there would be little reason to comment on this paper, except to say that yes, I agree. Unfortunately, in\u00a0\u2026", "num_citations": "3\n", "authors": ["170"]}
{"title": "Review early, Review often: A case study of Apache peer review\n", "abstract": " The mantra of the open source software (OSS) community regarding software releases is\" release early, release often\". We have found that a similar maxim applies to peer review conducted on the OSS Apache server project,\" review early, review often\". In practice, for source code,\" early\" means review the code before, or shortly after, it has been added to the communal or master code base.\" Often\" implies that the code should not be reviewed as a large finished artifact at infrequent intervals, but as a small, complete, specific solution to a defect or enhancement. Like most OSS practices, the review is done completely asynchronously using simple text-based tools, such as email. The high release frequency of open source projects represented a significant departure from the long release schedules of traditional projects-the Linux operating system was often released daily in its early days. The process of rapidly\u00a0\u2026", "num_citations": "3\n", "authors": ["170"]}
{"title": "Access and Privacy Legislation and the National Archives, 1983-1993: A Decade of ATIP\n", "abstract": " Freedom of information and privacy legislation has greatly changed and altered the manner in which information has been made available at the National Archives of Canada. Because the legislation is complicated, many of those concerned with the National Archives-staff and researchers alike-have little understanding of the mechanisms involved. This article attempts to remedy this lack of knowledge by closely examining the legislation, and placing it in the context of its first decade of application at the National Archives.The tenth anniversary of the promulgation of the Access to Information (AIA) and Privacy Acts (PA) in 1993 was marked by numerous newspaper articles, as the media explained how or why these Acts have or have not been a success.'For managers of federal government archives, though, and in particular for the National Archives of Canada, these Acts have meant more prosaic concerns than the intellectual pleasure of creating an avenue of access for the general public and media to federal government information. For government archives it has meant the creation of new Access to Information and Privacy (ATIP) offices and the diversion of funds from limited budgets to staff these offices in order to meet the regulatory obligations of the ATIP legi~ lation.~ In the words of Jean-Pierre Wallot, National Archivist of Canada:", "num_citations": "3\n", "authors": ["170"]}
{"title": "Clonecompass: Visualizations for exploring assembly code clone ecosystems\n", "abstract": " Assembly code analysis is an intensive process undertaken by security analysts and reverse engineers to discover vulnerabilities in existing software when source code is unavailable. Kam1n0 is an efficient code clone search engine that facilitates assembly code analysis. However, Kam1n0 search results can contain millions of function-clone pairs, and efficiently exploring and comprehensively understanding the resulting data can be challenging. This paper presents a design study whereby we collaborated with analyst stakeholders to identify requirements for a tool that visualizes and scales to millions of function-clone pairs. These requirements led to the design of an interactive visual tool, CloneCompass, consisting of novel TreeMap Matrix and Adjacency Matrix visualizations to aid in the exploration of assembly code clones extracted from Kam1n0. We conducted a preliminary evaluation with the analyst\u00a0\u2026", "num_citations": "2\n", "authors": ["170"]}
{"title": "Software patents: a replication study\n", "abstract": " Previous research has documented the legal and economic aspects of software patents. To study the evolution in the granting of software patents we reproduced and extended part of the empirical study on software patents conducted by Bessen and Hunt. The original study established a criteria to identify software patents, and provided a look at the evolution of patents granted until 2002. We present a simple approach to retrieve patents from the full text database provided by the United States Patent and Trademark Office (USPTO), which is freely accessible. We also present the evolution of software patents since the original study, and which we also present separated by major technological firms. Our research shows a continuous increase in the number of software patents granted higher, both in number of patents granted (in absolute numbers) and in proportion of overall patents (in relative terms). The relevance\u00a0\u2026", "num_citations": "2\n", "authors": ["170"]}
{"title": "Less May Be More: Copyleft, Right and the Case Law on APIs on Both Sides of the Atlantic\n", "abstract": " Like any relatively young area of law, copyright on software is surrounded by some legal uncertainty. Even more so in the context of copyleft open source licenses, since these licenses in some respects aim for goals that are the opposite of'regular'software copyright law. This article provides an analysis of the reciprocal effect of the GPL-family of copyleft software licenses (the GPL, LGPL and the AGPL) from a mostly copyright perspective as well as an analysis of the extent to which the SAS/WPL case affects this family of copyleft software licenses. In this article the extent to which the GPL and AGPL reciprocity clauses have a wider effect than those of the LGPL is questioned, while both the SAS/WPL jurisprudence and the Oracle vs Google case seem to affirm the LGPL's\" dynamic linking\" criterium. The net result is that the GPL may not be able to be more copyleft than the LGPL.", "num_citations": "2\n", "authors": ["170"]}
{"title": "Lisping Copyleft: A Close Reading of the Lisp LGPL\n", "abstract": " The idioms of both the General Public License (the\" GPL\") and the Lesser General Public License (the\" LGPL\") seem to be grounded in the C programming language. This article analyses the Lisp Lesser General Public License (colloquially and here referred to as the\" LLGPL\"), a specific attempt to apply the LGPL to a language with a programming paradigm and method of building and distributing programs that traditionally differs substantially from the approach of C. In addition, this article attempts to understand whether the LLGPL actually succeeds in its stated goal of translating the LGPL to the Lisp context or whether the LLGPL changes the requirements and philosophical moorings of the LGPL.", "num_citations": "2\n", "authors": ["170"]}
{"title": "Improving scans of black and white photographs by recovering the print maker's artistic intent\n", "abstract": " In this paper we propose a method that reverse engineers the aesthetic decisions made by a print maker to produce a print from a negative, namely cropping, contrast selection, and dodging-and-burning. It then re-applies this process to the electronic negative in order to achieve an electronic version of such print with better tonal range and detail than one produced by scanning the print. We then extend this method to restore a print by combining scans of different versions of the same image.", "num_citations": "2\n", "authors": ["170"]}
{"title": "The challenges of creating open source educational software: the Gild experience\n", "abstract": " This paper discusses Gild: An open source, Eclipse-based IDE for teaching and learning programming. Gild was designed to simplify and add pedagogical support to the Eclipse IDE to make it more appropriate for novice programmers and their instructors. Its development has greatly benefited from the ability to study, reuse, and modify existing Eclipse code. The core members of the Gild team are primarily researchers, making the maintenance of a growing code base difficult. It is challenging to create a community of developers because unlike most open source projects the developers (researchers) of Gild are not the main users (novice programmers) of Gild. To overcome this problem, we discuss techniques for making Gild more attractive to skilled developers (professors and graduate students). These techniques include improving instructor support in Gild and developing a grading perspective. We hope that these additions will attract able contributors and make Gild a self-sustain...", "num_citations": "2\n", "authors": ["170"]}
{"title": "A component-oriented framework for the implementation of navigational design patterns\n", "abstract": " In this paper, we describe a framework for the implementation of Web applications based on navigational design patterns. This framework is being implemented as a collection of Java classes that can be easily parameterized or inherited in order to instantiate a given design pattern.", "num_citations": "2\n", "authors": ["170"]}
{"title": "Architectural Patterns for Data Mediation in Web-centric Information Systems\n", "abstract": " For over two decades, Web-centric information management has considerably changed business processes of organisations in the private and public sector. The Web itself has emerged from a fairly static collection of hypertext documents to a dynamic network of objects providing information services, ie, the socalled Object-Web [6]. Today, many organisations provide access to their information systems (IS) by means of Web services. Moreover, the Web is no longer restricted to traditional computing hardware but it invades the world of mobile and smart devices. We can identify three main waves in the evolution of Web-centric systems. The first wave had its peak in the mid 90\u2019s; Organisations discovered the Web as an environment for marketing their products and publishing information to a growing community of users. The second, still ongoing wave has been focused on business-to-business e-commerce and federated Web services. The third wave of connectivity is on the horizon and targets the integration of \u201cevery-day-things\u201d like cell phones, PDAs, and household appliances as Web clients based on emerging W3C standards like UDDI and SOAP. In this paper, we discuss a collection of general architectural patterns that can be applied to systematically building such \u201cthird generation\u201d Web information systems. We then specialize these patterns with respect to current middleware solutions applied to a real world case study of a third generation Web IS in the domain of Health Care.", "num_citations": "2\n", "authors": ["170"]}
{"title": "Using Hadez to formally specify the Web museum of the National Gallery of Art\n", "abstract": " Hadez is a formal specification language for the design of data-intensive Web applications. In this paper, we use it to describe the Web museum of the National Gallery of Art (www. nga. gov). Hadez divides the specification of a Web application into three main parts: its conceptual schema, which describes the domain-specific data and its relationships; its structural schema, which describes how this data is combined and gathered into more complex entities, called composites; and the perspective schema, which uses abstract design perspectives to indicate how these composites are mapped to hyperpages, and how the user interacts with them. Hadez provides a formal framework in which properties of a specification can be specified and answered.", "num_citations": "2\n", "authors": ["170"]}
{"title": "LivePAGE-A multimedia database system to support World-Wide Web development\n", "abstract": " The rampant growth of the World-Wide Web (WWW) is largely a consequence of its simplicity. A typical person can quickly learn HTML and start creating WWW pages in an afternoon. As WWW sites become larger and more complex, this inherent simplicity causes multiple problems as many of the current tools and techniques are stretched to address issues they were not designed to handle. These problems are compounded by the rapid proliferation of solutions which are often fairly ad-hoc in nature. In this paper we present a layered model which through its tools and techniques provide a more disciplined approach to constructing and maintaining a WWW site. We then describe an implementation of this model which is based on two more mature technologies; SGML and relational database systems.", "num_citations": "2\n", "authors": ["170"]}
{"title": "Hypermedia Design Patterns\n", "abstract": " The popularity of hypermedia systems, such as the World Wide Web has promoted the creation of multiple hypermedia applications. The number, average size and complexity of such applications are growing at unexpected rates. Authors are now acquiring experience to build good hypermedia systems and assess their quality, and this experience should be compiled and organized in order to be shared with other authors. In software development, where similar kinds of creativity are prevalent, design patterns are being used to capture problem statements and potential solutions. We propose analogous hypermedia design patterns for hypermedia publishing. These patterns attempt to gather the experience of authors so that this experience can be reused by others producing improved systems in less time. This paper proposes a language to describe multimedia design patterns, including some examples.", "num_citations": "2\n", "authors": ["170"]}
{"title": "Predicting the Programming Language of Questions and Snippets of StackOverflow Using Natural Language Processing\n", "abstract": " Stack Overflow is the most popular Q&A website among software developers. As a platform for knowledge sharing and acquisition, the questions posted in Stack Overflow usually contain a code snippet. Stack Overflow relies on users to properly tag the programming language of a question and it simply assumes that the programming language of the snippets inside a question is the same as the tag of the question itself. In this paper, we propose a classifier to predict the programming language of questions posted in Stack Overflow using Natural Language Processing (NLP) and Machine Learning (ML). The classifier achieves an accuracy of 91.1% in predicting the 24 most popular programming languages by combining features from the title, body and the code snippets of the question. We also propose a classifier that only uses the title and body of the question and has an accuracy of 81.1%. Finally, we propose a classifier of code snippets only that achieves an accuracy of 77.7%. These results show that deploying Machine Learning techniques on the combination of text and the code snippets of a question provides the best performance. These results demonstrate also that it is possible to identify the programming language of a snippet of few lines of source code. We visualize the feature space of two programming languages Java and SQL in order to identify some special properties of information inside the questions in Stack Overflow corresponding to these languages.", "num_citations": "1\n", "authors": ["170"]}
{"title": "Archives and the Law\n", "abstract": " Twenty-five years have passed since Archivaria last devoted an entire issue to the theme of archives and the law. As the editor of Archivaria 18 stated in his introduction,\u201cThe relationship between archivists and \u2018the law\u2019is intri cate, varied, and largely unexplored.\u201d 1 These words are no less true in the first decade of the twenty-first century. Our society operates within a rule of law, and archivists and their institutions function within society. In acquir ing, preserving, and making available the documentary heritage of that soci ety, archivists are subject to the laws of the jurisdictions within which they operate. Some laws require that certain records be created and preserved for particular lengths of time; others specify the legal and regulatory framework within which records can be used in order to achieve public policy goals such as ensuring access, or protecting privacy and other rights. While legal issues affecting archivists have been further explored in the professional literature2 in the intervening quarter century, the digital environ ment has introduced a host of new issues to be addressed, and has signalled the need for the profession to re-examine traditional approaches to long-standing practices. The rapid adoption of information and communication technologies has resulted in a digital revolution that has changed the ways in which informa tion and records are created, transmitted, organized, located, preserved, and disposed of. In many arenas, digital technologies have challenged the ways in which laws apply to records and their use. Information can be created, used, and disseminated in ways that were never before possible, and laws have been\u00a0\u2026", "num_citations": "1\n", "authors": ["170"]}
{"title": "The Flow of Knowledge in Free and Open Source Communities\n", "abstract": " In this paper we present a survey of the methods used by a selection of successful free and open source projects to exchange, store and retrieve knowledge. In particular, we look into mailing lists, Internet Relay Chat, conferences, and code reviews. We explore how historical records left during the development become stored knowledge that can be subsequently retrieved. We also discuss the existence of meta-communities (composed of members of different communities) that allow knowledge to flow from one community to another.", "num_citations": "1\n", "authors": ["170"]}
{"title": "A formal specification language for hypermedia applications\n", "abstract": " As the size of hypermedia applications grows, it is clear that we need better tools for the designer to deal with their growing complexity. Formal languages allow the unambiguous description of the properties of a system without restricting their implementation. They can become the foundation for tool support in the design stage; these tools could then be used to verify properties about the design such as completeness, type-consistency or the characteristics of the corresponding navigation graph. We present in this paper HadeZ, a formal speci cation language for hypertext design. Based {as its counterpart Z and Object Z {in set theory, it is a mechanism to specify the data that composes an application and how its attributes are collated into hypermedia pages and therefore cross-linked. The aim of HadeZ is to unambiguously specify the design of an application without stating anything about how it must be implemented (what kind of repository to use, whether to use HTML or another platform), or the nal look-and-feel of it (what colours, what size of type) and to allow the designer to verify properties (such as the ones mentioned above) of a given design.", "num_citations": "1\n", "authors": ["170"]}
{"title": "A federated database for hypermedia development for the WWW\n", "abstract": " Large sites on the World-Wide Web (WWW) are difficult to manage and maintain because of the numerous files and interrelated hyperlinks or URLs. Current tools tend to focus on producing Web pages rather than on this maintenance problem. We propose an architecture for development of World-Wide Web sites that uses various types of databases to manage the information content of the Web pages while using a modeling layer whose responsibilities are to retrieve the content from the databases and to manage the presentational and navigational aspects of the the Web site. Thus, new material can be added to the Web site by adding content to the databases and then running the tools in the modeling layer. 1 Introduction The popularity of the World Wide Web (WWW) and the accessibility of simple-to-use, but powerful interfaces or browsers has prompted the development of literally millions of hypermedia applications. In many of these applications both the size and number of the source documents are s...", "num_citations": "1\n", "authors": ["170"]}
{"title": "Who Owns the Archives? A North American Perspective on Issues of Ownership and Control over Holdings of Archival Repositories\n", "abstract": " All those who work in archival repositories do so under a set of assumptions and beliefs. One of the paramount beliefs is that the archive \u201cowns\u201d the records in its custody. Certainly, it may be known that certain collections may have restrictions placed upon them by the donors, but these restrictions have generally been understood and accepted by the archives, in essence carried out under the archival imprimatur2. In any event, the records are physically held by the archival repository, and there is an old dictum which states that in questions of ownership\u2013\u201cpossession is 9/10th of the law\u201d\u2013implying that the key element in ownership lies with having physical possession of the material in question. The problem with this dictum though is the same found with all such dicta\u2013it paints too broadly a picture, with strokes which attempt to cover too wide and area.", "num_citations": "1\n", "authors": ["170"]}