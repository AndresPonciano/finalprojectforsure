{"title": "An integration of model checking with automated proof checking\n", "abstract": " Although automated proof checking tools for general-purpose logics have been successfully employed in the verification of digital systems, there are inherent limits to the efficient automation of expressive logics. If the expressiveness is constrained, there are useful logic fragments for which efficient decision procedures can be found. The model checking paradigm yields an important class of decision procedures for establishing temporal properties of finite-state systems. Model checking is remarkably effective for automatically verifying finite automata with relatively small state spaces, but is inadequate when the state spaces are either too large or unbounded. For this reason, it is useful to integrate the complementary technologies of model checking and proof checking. Such an integration has to be carried out in a delicate manner in order to be more than just the sum of the techniques. We describe an\u00a0\u2026", "num_citations": "303\n", "authors": ["1592"]}
{"title": "Abstract and model check while you prove\n", "abstract": " The construction of abstractions is essential for reducing large or infinite state systems to small or finite state systems. Boolean abstractions, where boolean variables replace concrete predicates, are an important class that subsume several abstraction schemes. We show how boolean abstractions can be constructed simply, efficiently, and precisely for infinite state systems while preserving properties in the full \u00b5-calculus. We also propose an automatic refinement algorithm which refines the abstraction until the property is verified or a counterexample is found. Our algorithm is implemented as a proof rule in the PVS verification system. With the abstraction proof rule, proof strategies combining deductive proof construction, model checking, and abstraction can be defined entirely within the PVS framework.", "num_citations": "248\n", "authors": ["1592"]}
{"title": "The PVS specification language\n", "abstract": " PVS is a Prototype Verification System for specifying and verifying digital systems. The PVS system consists of a specification language, a parser, a typechecker, a prover, specification libraries, and various browsing tools. This document primarily describes the specification language and is meant to be used as a reference manuaL The PVS User Guide [9] is to be consulted for information on how to use the system to develop specifications and proofs, The PVS Proof Checker manual [11] is a reference manual for the commands used to construct proofs.", "num_citations": "196\n", "authors": ["1592"]}
{"title": "ICS: Integrated Canonizer and Solver?\n", "abstract": " Decision procedures are at the core of many industrial-strength verification systems such as ACL2 [KM97], PVS [ORS92], or STeP [MtSg96]. Effective use of decision procedures in these verification systems require the management of large assertional contexts. Many existing decision procedures, however, lack an appropriate API for managing contexts and efficiently switching between contexts, since they are typically used in a fire-and-forget environment.               ICS (Integrated Canonizer and Solver) is a decision procedure developed at SRI International. It does not only efficiently decide formulas in a useful combination of theories but it also provides an API that makes it suitable for use in applications with highly dynamic environments such as proof search or symbolic simulation.               The theory decided by ICS is a quantifier-free, first-order theory with uninterpreted function symbols and a rich\u00a0\u2026", "num_citations": "182\n", "authors": ["1592"]}
{"title": "User guide for the PVS specification and verification system\n", "abstract": " The Prototype Veri cation System (PVS) provides an integrated environment for the development and analysis of formal speci cations, and supports a wide range of activities involved in creating, analyzing, modifying, managing, and documenting theories and proofs. This section provides a broad overview of PVS; the facilities provided by the system are discussed in the order a new user is likely to encounter them.", "num_citations": "175\n", "authors": ["1592"]}
{"title": "Little engines of proof\n", "abstract": " The automated construction of mathematical proof is a basic activity in computing. Since the dawn of the field of automated reasoning, there have been two divergent schools of thought. One school, best represented by Alan Robinson\u2019s resolution method, is based on simple uniform proof search procedures guided by heuristics. The other school, pioneered by Hao Wang, argues for problem-specific combinations of decision and semi-decision procedures. While the former school has been dominant in the past, the latter approach has greater promise. In recent years, several high quality inference engines have been developed, including propositional satisfiability solvers, ground decision procedures for equality and arithmetic, quantifier elimination procedures for integers and reals, and abstraction methods for finitely approximating problems over infinite domains. We describe some of these \u201clittle engines of\u00a0\u2026", "num_citations": "146\n", "authors": ["1592"]}
{"title": "The SAL language manual\n", "abstract": " \u042c \u042b \u0424 \u0430 \u0432 \u0439 \u043b \u0437 \u0433\u0436 \u0432 \u0430\u0430\u043d \u0437 \u0432 \u0432 \u0433\u0430\u0430 \u0433\u0436 \u0438 \u0433\u0432 \u043b \u0438 \u043a \u0430\u0430 \u0433 \u042b\u0438 \u0432 \u0433\u0436 \u042d\u0432 \u043a \u0436\u0437 \u0438\u043d \u0432 \u042c \u0433\u0431 \u0437 \u0420 \u0432\u043e \u0432 \u0436 \u0433 \u0438 \u042d\u0432 \u043a \u0436\u0437 \u0438\u043d \u0433 \u0430 \u0433\u0436\u0432 \u0438 \u0436 \u0430 \u043d\u041a \u042c \u043a \u0436\u0437 \u0433\u0432 \u0434\u0436 \u0437 \u0432\u0438 \u0436 \u0437 \u0438 \u0433\u0432 \u0439\u0436\u0436 \u0432\u0438\u0430\u043d \u0434\u0438 \u043d \u0438 \u0438\u0433\u0433\u0430\u0437 \u043a \u0430\u0433\u0434 \u0438 \u042b\u042a\u0421\u041a", "num_citations": "145\n", "authors": ["1592"]}
{"title": "Metamathematics, machines and G\u00f6del's proof\n", "abstract": " Mathematicians from Leibniz to Hilbert have sought to mechanise the verification of mathematical proofs. Developments arising out of G\u00f6del's proof of his incompleteness theorem showed that no computer program could automatically prove true all the theorems of mathematics. In practice, however, there are a number of sophisticated automated reasoning programs that are quite effective at checking mathematical proofs. Now in paperback, this book describes the use of a computer program to check the proofs of several celebrated theorems in metamathematics including G\u00f6del's incompleteness theorem and the Church-Rosser theorem. The computer verification using the Boyer-Moore theorem prover yields precise and rigorous proofs of these difficult theorems. It also demonstrates the range and power of automated proof checking technology. The mechanisation of metamathematics itself has important implications for automated reasoning since metatheorems can be applied by labour-saving devices to simplify proof construction. The book should be accessible to scientists and philosophers with some knowledge of logic and computing.", "num_citations": "144\n", "authors": ["1592"]}
{"title": "Effective theorem proving for hardware verification\n", "abstract": " The attractiveness of using theorem provers for system design verification lies in their generality. The major practical challenge confronting theorem proving technology is in combining this generality with an acceptable degree of automation. We describe an approach for enhancing the effectiveness of theorem provers for hardware verification through the use of efficient automatic procedures for rewriting, arithmetic and equality reasoning, and an off-the-shelf BDD-based propo-sitional simplifier. These automatic procedures can be combined into general-purpose proof strategies that can efficiently automate a number of proofs including those of hardware correctness. The inference procedures and proof strategies have been implemented in the PVS verification system. They are applied to several examples including an N-bit adder, the Saxe pipelined processor, and the benchmark Tamarack microprocessor\u00a0\u2026", "num_citations": "136\n", "authors": ["1592"]}
{"title": "The formal semantics of PVS\n", "abstract": " A specification language is a medium for expressing what is computed rather than how it is computed. Specification languages share some features with programming languages but are also different in several important ways. For our purpose, a specification language is a logic within which the behavior of computational systems can be formalized. Although a specification can be used to simulate the behavior of such systems, we mainly use specifications to state and prove system properties with mechanical assistance. We present the formal semantics of the specification language of SRI's Prototype Verification System(PVS). This specification language is based on the simply typed lambda calculus. The novelty in PVS is that it contains very expressive language features whose static analysis(eg, typechecking) requires the assistance of a theorem prover. The formal semantics illuminates several of the design considerations underlying PVS, particularly the interaction between theorem proving and typechecking. lll", "num_citations": "129\n", "authors": ["1592"]}
{"title": "Deconstructing shostak\n", "abstract": " Decision procedures for equality in a combination of theories are at the core of a number of verification systems. R.E. Shostak's (J. of the ACM, vol. 31, no. 1, pp. 1-12, 1984) decision procedure for equality in the combination of solvable and canonizable theories has been around for nearly two decades. Variations of this decision procedure have been implemented in a number of specification and verification systems, including STP, EHDM, PVS, STeP and SVC. The algorithm is quite subtle and a correctness argument for it has remained elusive. Shostak's algorithm and all previously published variants of it yield incomplete decision procedures. We describe a variant of Shostak's algorithm, along with proofs of termination, soundness and completeness.", "num_citations": "123\n", "authors": ["1592"]}
{"title": "A mechanical proof of the Church-Rosser theorem\n", "abstract": " The Church-Rosser theorem is a celebrated metamathematical result on the lambda calculus. We describe a formalization and proof of the Church-Rosser theorem that was carried out with the Boyer-Moore theorem prover. The proof presented in this paper is based on that of Tait and Martin-L\u00f6f. The mechanical proof illustrates the effective use of the Boyer-Moore theorem prover in proof checking difficult metamathematical proofs.", "num_citations": "114\n", "authors": ["1592"]}
{"title": "PVS: an experience report\n", "abstract": " PVS is a comprehensive interactive tool for specification and verification combining an expressive specification language with an integrated suite of tools for theorem proving and model checking. PVS has many academic and industrial users and has been applied to a wide range of verification tasks. In this note, we summarize some of its applications.", "num_citations": "108\n", "authors": ["1592"]}
{"title": "Verification of real-time systems using PVS\n", "abstract": " We present an approach to the verification of the real-time behavior of concurrent programs and describe its mechanization using the PVS proof checker. Our approach to real-time behavior extends previous verification techniques for concurrent programs by proposing a simple model for real-time computation and introducing a new operator for reasoning about absolute time. This model is formalized and mechanized within the higher-order logic of PVS. The interactive proof checker of PVS is used to develop the proofs of two illustrative examples: Fischer's real-time mutual exclusion protocol and a railroad crossing controller.", "num_citations": "108\n", "authors": ["1592"]}
{"title": "Combining theorem proving and model checking through symbolic analysis\n", "abstract": " Automated verification of concurrent systems is hindered by the fact that the state spaces are either infinite or too large for model checking, and the case analysis usually defeats theorem proving. Combinations of the two techniques have been tried with varying degrees of success. We argue for a specific combination where theorem proving is used to reduce verification problems to finite-state form, and model checking is used to explore properties of these reductions. This decomposition of the verification task forms the basis of the Symbolic Analysis Laboratory (SAL), a framework for combining different analysis tools for transition systems via a common intermediate language. We demonstrate how symbolic analysis can be an effective methodology for combining deduction and exploration.", "num_citations": "99\n", "authors": ["1592"]}
{"title": "Towards a duration calculus proof assistant in PVS\n", "abstract": " The Duration Calculus (DC) is an interval temporal logic for reasoning about real-time systems. This paper describes a tool for constructing DC specifications and checking DC proofs. The proof assistant is implemented by encoding the semantics of DC within the higher-order logic of a general-purpose specification and verification environment called PVS. We develop a Gentzen style sequent proof system for DC which we show to be sound with respect to the semantic encoding. We exploit the similarity between the sequent calculus of PVS and the sequent calculus of DC to obtain a DC proof system where the proofs are carried out in PVS at the semantic level, but appear as proofs in the DC proof system. The resulting system, called PC/DC, retains the specification and verification capabilities of PVS within the framework of the Duration Calculus. We present an example of a DC proof whose PC/DC\u00a0\u2026", "num_citations": "95\n", "authors": ["1592"]}
{"title": "Proof search in the intuitionistic sequent calculus\n", "abstract": " The use of Ilerbrand functions (sometimes called Skolemization) plays an important role in classical theorem proving and logic programming. We define a notion of Herbrand functions for the full intuitionistic predicate calculus. This definition is based on the view that the proof-theoretic role of Herbrand functions (to replace universal quantifiers), and of unification (to find instances corresponding to existential quantifiers), is to ensure the eigenvariable conditions on a sequent proof. The propositional impermutabilities that arise in the intuitionistic but not the classical sequent calculus motivate a generalization of the classical notion of Ilerbrand functions. This generalization of Herbrand functions also applies to the sequent calculus formalizations of logics other than intuitionistic predicate calculus.", "num_citations": "89\n", "authors": ["1592"]}
{"title": "Proof-checking metamathematics (theorem-proving)\n", "abstract": " Formal proof-checking has long been recognized as an interesting application of computers. It has often been claimed that significant proofs in mathematics cannot be checked using an automatic proof-checker, and that formal proofs lack the intuitive plausibility and insight that informal proofs possess. We argue against these claims by presenting machine-checked versions of some landmark proofs in metamathematics, such as those of the tautology theorem, Godel's incompleteness theorem, and the Church-Rosser theorem. These proofs were checked using the Boyer-Moore theorem power.", "num_citations": "85\n", "authors": ["1592"]}
{"title": "Abstract datatypes in PVS\n", "abstract": " PVS (Prototype Verification System) is a general-purpose environment for developing specifications and proofs. This document deals primarily with the abstract datatype mechanism in PVS which generates theories containing axioms and definitions for a class of recursive datatypes. The concepts underlying the abstract datatype mechanism are illustrated using ordered binary trees as an example. Binary trees are described by a PVS abstract datatype that is parametric in its value type. The type of ordered binary trees is then presented as a subtype of binary trees where the ordering relation is also taken as a parameter. We define the operations of inserting an element into, and searching for an element in an ordered binary tree; the bulk of the report is devoted to PVS proofs of some useful properties of these operations. These proofs illustrate various approaches to proving properties of abstract datatype operations. They also describe the built-in capabilities of the PVS proof checker for simplifying abstract datatype expressions.", "num_citations": "83\n", "authors": ["1592"]}
{"title": "Modular verification of SRT division\n", "abstract": " We describe a formal specification and verification in PVS for the general theory of SRT division, and for the hardware design of a specific implementation. The specification demonstrates how attributes of the PVS language (in particular, predicate subtypes) allow the general theory to be developed in a readable manner that is similar to textbook presentations, while the PVS table construct allows direct specification of the implementation's quotient look-up table. Verification of the derivations in the SRT theory and for the data path and look-up table of the implementation are highly automated and performed for arbitrary, but finite precision; in addition, the theory is verified for general radix, while the implementation is specialized to radix 4. The effectiveness of the automation derives from PVS's tight integration of rewriting with decision procedures for equality, linear arithmetic over integers and rationals, and\u00a0\u2026", "num_citations": "82\n", "authors": ["1592"]}
{"title": "Combining shostak theories\n", "abstract": " Ground decision procedures for combinations of theories are used in many systems for automated deduction. There are two basic paradigms for combining decision procedures. The Nelson-Oppen method combines decision procedures for disjoint theories by exchanging equality information on the shared variables. In Shostak\u2019s method, the combination of the theory of pure equality with canonizable and solvable theories is decided through an extension of congruence closure that yields a canonizer for the combined theory. Shostak\u2019s original presentation, and others that followed it, contained serious errors which were corrected for the basic procedure by the present authors. Shostak also claimed that it was possible to combine canonizers and solvers for disjoint theories. This claim is easily verifiable for canonizers, but is unsubstantiated for the case of solvers. We show how our earlier procedure can be\u00a0\u2026", "num_citations": "77\n", "authors": ["1592"]}
{"title": "Theory interpretations in PVS\n", "abstract": " The purpose of this task was to provide a mechanism for theory interpretations in a prototype verification system (PVS) so that it is possible to demonstrate the consistency of a theory by exhibiting an interpretation that validates the axioms. The mechanization makes it possible to show that one collection of theories is correctly interpreted by another collection of theories under a user-specified interpretation for the uninterpreted types and constants. A theory instance is generated and imported, while the axiom instances are generated as proof obligations to ensure that the interpretation is valid. Interpretations can be used to show that an implementation is a correct refinement of a specification, that an axiomatically defined specification is consistent, or that a axiomatically defined specification captures its intended models. In addition, the theory parameter mechanism has been extended with a notion of theory as parameter so that a theory instance can be given as an actual parameter to an imported theory. Theory interpretations can thus be used to refine an abstract specification or to demonstrate the consistency of an axiomatic theory. In this report we describe the mechanism in detail. This extension is a part of PVS version 3.0, which will be publicly released in mid-2001.", "num_citations": "76\n", "authors": ["1592"]}
{"title": "A tutorial on specification and verification using PVS\n", "abstract": " CiteSeerX \u2014 A Tutorial on Specification and Verification Using PVS Documents Authors Tables Log in Sign up MetaCart DMCA Donate CiteSeerX logo Documents: Advanced Search Include Citations Authors: Advanced Search Include Citations Tables: DMCA A Tutorial on Specification and Verification Using PVS (1993) Cached Download as a PDF Download Links [www.csl.sri.com] Save to List Add to Collection Correct Errors Monitor Changes by N. Shankar, et al. Citations: 14 - 0 self Summary Citations Active Bibliography Co-citation Clustered Documents Version History Share Facebook Twitter Reddit Bibsonomy OpenURL Abstract Keyphrases verification using pvs Powered by: Apache Solr About CiteSeerX Submit and Index Documents Privacy Policy Help Data Source Contact Us Developed at and hosted by The College of Information Sciences and Technology \u00a9 2007-2019 The Pennsylvania State \u2026", "num_citations": "74\n", "authors": ["1592"]}
{"title": "Mechanical verification of a generalized protocol for byzantine fault tolerant clock synchronization\n", "abstract": " Schneider [Sch87] generalizes a number of protocols for Byzantine fault-tolerant clock synchronization and presents a uniform proof for their correctness. We present a mechanical verification of Schneider's protocol leading to several significant clarifications and revisions. The verification was carried out with the Ehdm system [RvHO91] developed at the SRI Computer Science Laboratory. The mechanically checked proofs include the verification that the egocentric mean function used in Lamport and Melliar-Smith's Interactive Convergence Algorithm [LMS85] satisfies the requirements of Schneider's protocol. Our mechanical verification raises a number of issues regarding the verification of fault-tolerant, distributed, real-time protocols that are germane to the design of a special-purpose logic for such problems.", "num_citations": "73\n", "authors": ["1592"]}
{"title": "PVS: Combining specification, proof checking, and model checking\n", "abstract": " We claim that no single technique such as rewriting, BDDs, or model checking is effective for all aspects of hardware verification. Many examples need the careful integration of these techniques. We have shown some simple examples to illustrate the integration available in PVS. This combination of techniques has been applied to some larger examples such as an SRT divider and Rockwell-Collins AAMP series of processors. The automation available in PVS on these examples can be further improved through the use of more decision procedures (e.g., bit vectors) and better verification methodologies (e.g., abstraction, induction).", "num_citations": "67\n", "authors": ["1592"]}
{"title": "Lazy compositional verication\n", "abstract": " Existing methodologies for the verification of concurrent systems are effective for reasoning about global properties of small systems. For large systems, these approaches become expensive both in terms of computational and human effort. A compositional verification methodology can reduce the verification effort by allowing global system properties to be derived from local component properties. For this to work, each component must be viewed as an open system interacting with a well-behaved environment. Much of the emphasis in compositional verification has been on the assume-guarantee paradigm where component properties are verified contingent on properties that are assumed of the environment.We highlight an alternate paradigm called lazy composition where the component properties are proved by composing the component with an abstract environment.We present the main ideas\u00a0\u2026", "num_citations": "61\n", "authors": ["1592"]}
{"title": "Symbolic Analysis of Transition Systems?\n", "abstract": " We give a brief overview of the Symbolic Analysis Laboratory (SAL) project. SAL is a verification framework that is directed at analyzing properties of transition systems by combining tools for program analysis, model checking, and theorem proving. SAL is built around a small intermediate language that serves as a semantic representation for transition systems that can be used to drive the various analysis tools.", "num_citations": "56\n", "authors": ["1592"]}
{"title": "Towards mechanical metamathematics\n", "abstract": " Metamathematics is a source of many interesting theorems and difficult proofs. This paper reports the results of an experiment to use the Boyer-Moore theorem prover to proof-check theorems in metamathematics. We describe a First Order Logic due to Shoenfield and outline some of the theorems that the prover was able to prove about this logic. These include the tautology theorem which states that every tautology has a proof. Such proofs can be used to add new proof procedures to a proof-checking program in a sound and efficient manner.", "num_citations": "52\n", "authors": ["1592"]}
{"title": "Tool integration with the evidential tool bus\n", "abstract": " Formal and semi-formal tools are now being used in large projects both for development and certification. A typical project integrates many diverse tools such as static analyzers, model checkers, test generators, and constraint solvers. These tools are usually integrated in an ad hoc manner. There is, however, a need for a tool integration framework that can be used to systematically create workflows, to generate claims along with supporting evidence, and to maintain the claims and evidence as the inputs change. We present the Evidential Tool Bus (ETB) as a tool integration framework for constructing claims supported by evidence. ETB employs a variant of Datalog as a metalanguage for representing claims, rules, and evidence, and as a scripting language for capturing distributed workflows. ETB can be used to develop assurance cases for certifying complex systems that are developed and assured using\u00a0\u2026", "num_citations": "50\n", "authors": ["1592"]}
{"title": "Principles and Pragmatics of Subtyping in PVS\n", "abstract": " PVS (Prototype Verification System) is a mechanized framework for formal specification and interactive proof development. The PVS specification language is based on higher-order logic enriched with features such as predicate subtypes, dependent types, recursive datatypes, and parametric theories. Subtyping is a central concept in the PVS type system. PVS admits the definition of subtypes corresponding to nonzero integers, prime numbers, injective maps, order-preserving maps, and even empty subtypes. We examine the principles underlying the PVS subtype mechanism and its implementation and use.", "num_citations": "50\n", "authors": ["1592"]}
{"title": "A brief overview of PVS\n", "abstract": " PVS is now 15 years old, and has been extensively used in research, industry, and teaching. The system is very expressive, with unique features such as predicate subtypes, recursive and corecursive datatypes, inductive and coinductive definitions, judgements, conversions, tables, and theory interpretations. The prover supports a combination of decision procedures, automatic simplification, rewriting, ground evaluation, random test case generation, induction, model checking, predicate abstraction, MONA, BDDs, and user-defined proof strategies. In this paper we give a very brief overview of the features of PVS, some illustrative examples, and a summary of the libraries and PVS applications.", "num_citations": "47\n", "authors": ["1592"]}
{"title": "Automated deduction for verification\n", "abstract": " Automated deduction uses computation to perform symbolic logical reasoning. It has been a core technology for program verification from the very beginning. Satisfiability solvers for propositional and first-order logic significantly automate the task of deductive program verification. We introduce some of the basic deduction techniques used in software and hardware verification and outline the theoretical and engineering issues in building deductive verification tools. Beyond verification, deduction techniques can also be used to support a variety of applications including planning, program optimization, and program synthesis.", "num_citations": "40\n", "authors": ["1592"]}
{"title": "Solving linear arithmetic constraints\n", "abstract": " Linear arithmetic constraints in the form of equalities and inequalities constitute the vast majority of proof obligations that arise in embedded applications of theorem proving such as extended typechecking, software and hardware verification, and compiler optimization. Such constraints involve the conjunction of equalities, inequalities, and disequalities over arithmetic, uninterpreted functions, and other datatypes. Nelson presented a practical scheme for solving linear inequalities based on the simplex algorithm for linear programming. We extend Nelson\u2019s version of the simplex algorithm in a number of ways. Among these extensions are an improved treatment of the combination of restricted (non-negative slack) and unrestricted variables, a method for adding equalities and disequalities to a simplex tableau, an optimized method for propagating equality information, and efficient techniques for generating proofs and models. Our algorithms are supported by simple and rigorous correctness arguments.", "num_citations": "39\n", "authors": ["1592"]}
{"title": "Using decision procedures with a higher-order logic\n", "abstract": " In automated reasoning, there is a perceived trade-off between expressiveness and automation. Higher-order logic is typically viewed as expressive but resistant to automation, in contrast with first-order logic and its fragments. We argue that higher-order logic and its variants actually achieve a happy medium between expressiveness and automation, particularly when used as a front-end to a wide range of decision procedures and deductive procedures. We illustrate the discussion with examples from PVS, but some of the observations apply to other variants of higher-order logic as well.", "num_citations": "38\n", "authors": ["1592"]}
{"title": "SimCheck: a contract type system for Simulink\n", "abstract": " Matlab Simulink\u2122 is a member of a class of visual languages that are used for modeling and simulating physical and cyber-physical system. A Simulink model consists of blocks with input and output ports connected using links that carry signals. We provide a contract-based type system of Simulink with annotations and dimensions/units associated with ports and links. These contract types can capture invariants on signals as well as relations between signals. We define a contract-based verifier that checks the well formedness of Simulink blocks with respect to these contracts. This verifier generates proof obligations that are solved by SRI\u2019s Yices solver for satisfiability modulo theories (SMT). This translation can be used to detect basic type errors and violation of contracts, demonstrate counterexamples, generate test cases, or prove the absence of contract-based type errors. Our work is an initial step toward\u00a0\u2026", "num_citations": "34\n", "authors": ["1592"]}
{"title": "Writing PVS proof strategies\n", "abstract": " PVS (Prototype Verification System) is a comprehensive framework for writing formal logical specifications and constructing proofs. An interactive proof checker is a key component of PVS. The capabilities of this proof checker can be extended by defining proof strategies that are similar to LCF-style tactics. Commonly used proof strategies include those for discharging typechecking proof obligations, simplification and rewriting using decision procedures, and various forms of induction. We describe the basic building blocks of PVS proof strategies and provide a pragmatic guide for writing sophisticated strategies.", "num_citations": "32\n", "authors": ["1592"]}
{"title": "Design and verification for transportation system security\n", "abstract": " Cyber-security has emerged as a pressing issue for transportation systems. Studies have shown that attackers can attack modern vehicles from a variety of interfaces and gain access to the most safety-critical components. Such threats become even broader and more challenging with the emergence of vehicle-to-vehicle (V2V) and vehicle-to-infrastructure (V2I) communication technologies. Addressing the security issues in transportation systems requires comprehensive approaches that encompass considerations of security mechanisms, safety properties, resource constraints, and other related system metrics. In this work, we propose an integrated framework that combines hybrid modeling, formal verification, and automated synthesis techniques for analyzing the security and safety of transportation systems and carrying out design space exploration of both in-vehicle electronic control systems and vehicle-to\u00a0\u2026", "num_citations": "31\n", "authors": ["1592"]}
{"title": "The mechanical verification of a DPLL-based satisfiability solver\n", "abstract": " Recent years have witnessed dramatic improvements in the capabilities of propositional satisfiability procedures or SAT solvers. The speedups are the result of numerous optimizations including conflict-directed backjumping. We use the Prototype Verification System (PVS) to verify a satisfiability procedure based on the Davis\u2013Putnam\u2013Logemann\u2013Loveland (DPLL) scheme that features these optimizations. This exercise is a step toward the verification of an efficient implementation of the satisfiability procedure. Our verification of a SAT solver is part of a larger program of research to provide a secure foundation for inference using a verified reference kernel that contains a verified SAT solver. Our verification exploits predicate subtypes and dependent types in PVS to capture the specification and the key invariants.", "num_citations": "31\n", "authors": ["1592"]}
{"title": "Justifying equality\n", "abstract": " We consider the problem of finding irredundant bases for inconsistent sets of equalities and disequalities. These are subsets of inconsistent sets which do not contain any literals which do not contribute to the unsatisfiability in an essential way, and can therefore be discarded. The approach we are pursuing here is to decorate derivations with proofs and to extract irredundant sets of assumptions from these proofs. This requires specialized operators on proofs, but the basic inference systems are otherwise left unchanged. In particular, we include justifying inference systems for union-find structures and abstract congruence closure, but our constructions can also be applied to other inference systems such as Gaussian elimination.", "num_citations": "31\n", "authors": ["1592"]}
{"title": "Fair synchronous transition systems and their liveness proofs\n", "abstract": " We present a compositional semantics of synchronous systems that captures both safety and progress properties of such systems. The fair synchronous transitions systems (Fsts) model we introduce in this paper extends the basic \u03b1Sts model [KP96] by introducing operations for parallel composition, for the restriction of variables, and by addressing fairness. We introduce a weak fairness (justice) condition which ensures that any communication deadlock in a system can only occur through the need for external synchronization. We present an extended version of linear time temporal logic (Eltl) for expressing and proving safety and liveness properties of synchronous specifications, and provide a sound and compositional proof system for it.", "num_citations": "31\n", "authors": ["1592"]}
{"title": "The PVS prelude library\n", "abstract": " \u042c \u0428\u042e\u042b \u0428\u0436 \u0430\u0439 \u0424 \u0436 \u0436\u043d \u0437 \u0433\u0430\u0430 \u0438 \u0433\u0432 \u0433 \u0437 \u0438 \u0433\u0436 \u0437 \u0433\u0439\u0438 \u0430\u0433 \u0418 \u0439\u0432 \u0438 \u0433\u0432\u0437\u0418 \u0434\u0436 \u0438 \u0437\u0418 \u0437 \u0438\u0437\u0418 \u0432\u0439\u0431 \u0436\u0437\u0418 \u0432 \u0433\u0438 \u0436 \u0438 \u0438\u043d\u0434 \u0437\u041a \u042c \u0438 \u0433\u0436 \u0437 \u0432 \u0438 \u0434\u0436 \u0430\u0439 \u0430 \u0436 \u0436\u043d \u0436 \u043a \u0437 \u0430 \u0432 \u0430\u0430 \u0428\u042e\u042b \u0433\u0432\u0438 \u043c\u0438\u0437\u0418 \u0439\u0432\u0430 \u0438 \u0433\u0437 \u0436\u0433\u0431 \u0433\u0438 \u0436 \u0430 \u0436 \u0436 \u0437 \u0438 \u0438 \u043a \u0438\u0433 \u043c\u0434\u0430 \u0438\u0430\u043d \u0431\u0434\u0433\u0436\u0438 \u041a \u042c \u0437 \u0438 \u0433\u0436 \u0437 \u0430\u0437\u0433 \u0430\u0430\u0439\u0437\u0438\u0436 \u0438 \u043a \u0436 \u0433\u0439\u0437 \u0430 \u0432 \u0439 \u0438\u0439\u0436 \u0437 \u0433 \u0428\u042e\u042b \u0438 \u0438 \u0436 \u0439\u0437 \u0439\u0430 \u0433\u0436\u0431 \u0430 \u043e \u0438 \u0433\u0432 \u0437\u041a", "num_citations": "30\n", "authors": ["1592"]}
{"title": "A lazy approach to compositional verification\n", "abstract": " The verification of concurrent systems with a large number of interacting components poses a significant challenge to existing verification methodologies. We present a simple approach to compositional verification that allows global system properties to be established by considering only the behavior of the relevant system components. Each component specification describes the transitions due to the component itself and constrains the transitions due its environment. The composition of two specifications is an interleaving conjunction of the two specifications. Every property derived from a component specification remains true when this component is composed with other components. This compositional approach to verification is illustrated with the example of a buffer composed from two smaller buffers, and a real-time controller for a railroad crossing system composed from the subsystems for trains, the controller\u00a0\u2026", "num_citations": "30\n", "authors": ["1592"]}
{"title": "Automated Reasoning: Third International Joint Conference, IJCAR 2006, Seattle, WA, USA, August 17-20, 2006, Proceedings\n", "abstract": " This volume contains the papers presented at the Third International Joint Conference on Automated Reasoning (IJCAR 2006) held on August 17\u201320, 2006, in Seattle, Washington as part of the Federated Logic Conference (FLoC 2006). The IJCAR series of conferences is aimed at unifying the different research disciplines within automated reasoning. IJCAR 2006 is the fusion of several major conferences:", "num_citations": "28\n", "authors": ["1592"]}
{"title": "Efficiently executing PVS\n", "abstract": " Specification languages like PVS are designed to be expressive rather than executable. However, execution is useful for animating and validating specifications, and for automating certain kinds of proofs. A surprisingly large fragment of PVS turns out to be executable as a functional language with quite reasonable efficiency. Execution is achieved by generating Common Lisp code from PVS. The key step in generating efficient code is the use of static analysis to determine safe destructive array updates. Contents 1 Introduction 5 2 A Brief Summary of the PVS Specification Language 7 3 Executing PVS 13 3.1 Translating Primitive Operations................ 15 3.2 Translating Expressions..................... 16 3.3 Optimizing Unary to Multiary Function Application...... 19 3.3. 1 Examples......................... 20 3.4 Translating Recursive Datatypes................. 25 4 Optimizing Nondestructive to Destr...", "num_citations": "27\n", "authors": ["1592"]}
{"title": "SimCheck: An Expressive Type System for Simulink.\n", "abstract": " MATLAB Simulink is a member of a class of visual languages that are used for modeling and simulating physical and cyber-physical system. A Simulink model consists of blocks with input and output ports connected using links that carry signals. We extend the type system of Simulink with annotations and dimensions/units associated with ports and links. These types can capture invariants on signals as well as relations between signals. We define a type-checker that checks the wellformedness of Simulink blocks with respect to these type annotations. The type checker generates proof obligations that are solved by SRI\u2019s Yices solver for satisfiability modulo theories (SMT). This translation can be used to detect type errors, demonstrate counterexamples, generate test cases, or prove the absence of type errors. Our work is an initial step toward the symbolic analysis of MATLAB Simulink models.", "num_citations": "25\n", "authors": ["1592"]}
{"title": "Trust and automation in verification tools\n", "abstract": " On the one hand, we would like verification tools to feature powerful automation, but on the other hand, we also want to be able to trust the results with a high degree of confidence. The question of trust in verification tools has been debated for a long time. One popular way of achieving trust in verification tools is through proof generation. However, proof generation could hamstring both the functionality and the efficiency of the automation that can be built into these tools. We argue that trust need not be achieved at the expense of automation, and outline a lightweight approach where the results of untrusted verifiers are checked by a trusted offline checker. The trusted checker is a verified reference kernel that contains a satisfiability solver to support the robust and efficient checking of untrusted tools.", "num_citations": "25\n", "authors": ["1592"]}
{"title": "Steps towards mechanizing program transformations using PVS\n", "abstract": " PVS is a highly automated framework for specification and verification. We show how the language and deduction features of PVS can be used to formalize, mechanize, and apply some useful program transformation techniques. We examine two such examples in detail. The first is a fusion theorem due to Bird where the composition of a catamorphism (a recursive operation on the structure of a datatype) and an anamorphism (an operation that constructs instances of the datatype) are fused to eliminate the intermediate data structure. The second example is Wand's continuation-based transformation technique for deriving tail-recursive functions from non-tailrecursive ones. These examples illustrate the utility of the language and inference features of PVS in capturing these transformations in a simple, general, and useful form.", "num_citations": "25\n", "authors": ["1592"]}
{"title": "Automated software winnowing\n", "abstract": " The strong isolation guarantees of hardware virtualization have led to its widespread use. A consequence of this is that individual partitions contain much software that is designed to be used in a variety of environments and by a range of applications, while in practice only a limited subset is actually utilized. Similarly, the modular design of software has contributed greatly to the ability of application developers to quickly write sophisticated programs. However, in most instances only a small fraction of the functionality included in a particular software component is needed.", "num_citations": "23\n", "authors": ["1592"]}
{"title": "Formal verification of a combination decision procedure\n", "abstract": " Decision procedures for combinations of theories are at the core of many modern theorem provers such as ACL2, Ehdm, PVS, SIMPLIFY, the Stanford Pascal Verifier, STeP, SVC, and Z/Eves. Shostak, in 1984, published a decision procedure for the combination of canonizable and solvable theories. Recently, Ruess and Shankar showed Shostak\u2019s method to be incomplete and nonterminating, and presented a correct version of Shostak\u2019s algorithm along with informal proofs of termination, soundness, and completeness. We describe a formalization and mechanical verification of these proofs using the PVS verification system. The formalization itself posed significant challenges and the verification revealed some gaps in the informal argument.", "num_citations": "23\n", "authors": ["1592"]}
{"title": "Satisfiability modulo theories and assignments\n", "abstract": " The CDCL procedure for SAT is the archetype of conflict-driven procedures for satisfiability of quantifier-free problems in a single theory. In this paper we lift CDCL to CDSAT (Conflict-Driven Satisfiability), a system for conflict-driven reasoning in combinations of disjoint theories. CDSAT combines theory modules that interact through a global trail representing a candidate model by Boolean and first-order assignments. CDSAT generalizes to generic theory combinations the model-constructing satisfiability calculus (MCSAT) introduced by de Moura and Jovanovi\u0107. Furthermore, CDSAT generalizes the equality sharing (Nelson-Oppen) approach to theory combination, by allowing theories to share equality information both explicitly through equalities and disequalities, and implicitly through assignments. We identify sufficient conditions for the soundness, completeness, and termination of CDSAT.", "num_citations": "22\n", "authors": ["1592"]}
{"title": "The gradual verifier\n", "abstract": " Static verification traditionally produces yes/no answers. It either provides a proof that a piece of code meets a property, or a counterexample showing that the property can be violated. Hence, the progress of static verification is hard to measure. Unlike in testing, where coverage metrics can be used to track progress, static verification does not provide any intermediate result until the proof of correctness can be computed. This is in particular problematic because of the inevitable incompleteness of static verifiers.             To overcome this, we propose a gradual verification approach, GraVy. For a given piece of Java code, GraVy partitions the statements into those that are unreachable, or from which exceptional termination is impossible, inevitable, or possible. Further analysis can then focus on the latter case. That is, even though some statements still may terminate exceptionally, GraVy still computes a partial\u00a0\u2026", "num_citations": "22\n", "authors": ["1592"]}
{"title": "EFSMT: A logical framework for cyber-physical systems\n", "abstract": " The design of cyber-physical systems is challenging in that it includes the analysis and synthesis of distributed and embedded real-time systems for controlling, often in a nonlinear way, the environment. We address this challenge with EFSMT, the exists-forall quantified first-order fragment of propositional combinations over constraints (including nonlinear arithmetic), as the logical framework and foundation for analyzing and synthesizing cyber-physical systems. We demonstrate the expressiveness of EFSMT by reducing a number of pivotal verification and synthesis problems to EFSMT. Exemplary problems in this paper include synthesis for robust control via BIBO stability, Lyapunov coefficient finding for nonlinear control systems, distributed priority synthesis for orchestrating system components, and synthesis for hybrid control systems. We are also proposing an algorithm for solving EFSMT problems based on the interplay between two SMT solvers for respectively solving universally and existentially quantified problems. This algorithms builds on commonly used techniques in modern SMT solvers, and generalizes them to quantifier reasoning by counterexample-guided constraint strengthening. The EFSMT solver uses Bernstein polynomials for solving nonlinear arithmetic constraints.", "num_citations": "22\n", "authors": ["1592"]}
{"title": "A mechanized refinement proof for a garbage collector\n", "abstract": " We describe how the PVS verification system has been used to verify a safety property of a garbage collection algorithm. The safety property basically says that\\nothing but garbage is ever collected\". The proof is based on refinement mappings as suggested by Lamport. Although the algorithm is relatively simple, its parallel composition with a\\user\" program that (nearly) arbitrarily modifies the memory makes the verification quite challenging. The garbage collection algorithm and its composition with the user program is regarded as a concurrent system with two processes working on a shared memory. Such concurrent systems can be encoded in PVS as state transition systems, very similar to the model of, for example, TLA. The safety proof is formulated as a refinement, where the safety specification itself is formulated as state transition system and where the final algorithm is shown to be a refinement thereof. The algorithm is an excellent test-case for formal methods, be they based on theorem proving or model checking. Various hand-written proofs of the algorithm have been developed, some of which are wrong. David Russinoff has verified the algorithm in the Boyer-Moore prover, but his proof is not based on refinement, implying that his safety property cannot be appreciated without a glass box view of the algorithm, considering it's internal structure. Using refinement, however, the algorithm can be regarded as a black box.", "num_citations": "21\n", "authors": ["1592"]}
{"title": "Counterexample-Driven Model Checking\n", "abstract": " The generation of counterexamples is frequently touted as one of the primary advantages of model checking as a verification technique. However, the generation of trace-like counterexamples is limited to a small fragment of branching-time temporal logic. When model checking does succeed in verifying a property, there is typically no independently checkable witness that can be used as evidence for the verified property. We present a definition of witnesses, and, dually, counterexamples, for computation-tree logic (CTL), and describe a model checking algorithm that is based on the generation of evidence. Our model checking algorithm is local in the sense that it explores only the reachable states. It partitions the given initial set of states into those that do, and those that do not satisfy the given property, with a corresponding witness and counterexample that is independently verifiable. We have built a model checker based on these ideas that works quite efficiently despite the overhead of generating evidence.", "num_citations": "20\n", "authors": ["1592"]}
{"title": "Machine-assisted verification using theorem proving and model checking\n", "abstract": " Theorem proving and model checking are complementary approaches to the verification of hardware designs and software algorithms. In theorem proving, the verification task is one of showing that the formal description of the program implies the formal statement of a putative program property, while model checking demonstrates that the program is a model that satisfies the putative property. Theorem proving is completely general but typically requires significant human guidance, whereas model checking though restricted to a limited range of properties of small (essentially) finitestate systems, is largely automatic. This paper is a tutorial on the combined use of theorem proving and model checking as mechanized in the PVS specification and verification environment.", "num_citations": "20\n", "authors": ["1592"]}
{"title": "Modularity and refinement in inference systems\n", "abstract": " Combination decision procedures which decide the satisfiability of quantifier-free formulas in the union of several theories, are at the core of many proof engines. Correctness proofs for such decision procedures are often tours de force of metamathematical reasoning. We present a modular framework for constructing decision procedures over multiple theories by composing modules that are specific to the individual theories. Each inference module can be independently refined based on theory-specific assumptions. We outline a general theory of modularity for decision procedures and specialize it to the combination methods of Nelson and Oppen, Shostak, and Ghilardi. The formal concept of refinement for inference systems and modules makes it possible to develop correct implementations by discharging simple, incremental proof obligations. We also show in this mathematically precise sense that Shostak\u2019s\u00a0\u2026", "num_citations": "19\n", "authors": ["1592"]}
{"title": "Mechanical verification of a schematic byzantine clock synchronization algorithm\n", "abstract": " Schneider generalizes a number of protocols for Byzantine fault tolerant clock synchronization and presents a uniform proof for their correctness. The authors present a machine checked proof of this schematic protocol that revises some of the details in Schneider's original analysis. The verification was carried out with the EHDM system developed at the SRI Computer Science Laboratory. The mechanically checked proofs include the verification that the egocentric mean function used in Lamport and Melliar-Smith's Interactive Convergence Algorithm satisfies the requirements of Schneider's protocol.", "num_citations": "19\n", "authors": ["1592"]}
{"title": "A Semantic Embedding of the Dynamic Logic in PVS\n", "abstract": " \u042c \u0432 \u0437 \u0438\u0433 \u0422\u0433\u0437 \u0424 \u043a\u043d \u0433\u0436 \u0434\u0436\u0433\u043a \u0432 \u0431 \u0438 \u043a \u0438 \u0430 \u0430 \u0431 \u0432\u0438\u0418 \u043d \u0436 \u0431 \u0438\u0418 \u043b \u0438 \u0433\u0439\u0438 \u0438 \u0421 \u043b\u0433\u0439\u0430 \u0432 \u043a \u0436 \u0433\u0439\u0432 \u043b \u043d \u0438\u0433 \u0437\u0439\u0436\u043a \u043a \u0433\u0439\u0436 \u0431\u0433\u0432\u0438 \u0437\u041a \u0432 \u0438 \u0432 \u0437 \u0438\u0433 \u042b \u0431 \u0433\u0436 \u0437 \u0436 \u0432 \u043b \u0438 \u0431 \u0438 \u0433\u0437 \u0431 \u0438 \u0437\u041a \u0433\u0436 \u0438 \u0433\u0437 \u043b \u0433 \u0430 \u043a \u0432 \u0438\u0436 \u0438 \u0433\u0432\u0437 \u0438 \u0437 \u043a \u0436\u043d \u0431\u0434\u0433\u0436\u0438 \u0432\u0438 \u0438\u0433 \u040c\u0432 \u0437\u0433\u0431 \u0433 \u043d \u043b \u0430\u0430 \u0432 \u0438\u0433 \u0437 \u0436 \u0438 \u0431\u041a \u0432 \u0430\u0430\u043d \u0421 \u043b \u0432\u0438 \u0438\u0433 \u0438 \u0432 \u0430\u0430 \u042b\u0424 \u0437\u0438 \u040b\u0418 \u0439\u0438 \u0437\u0434 \u0430\u0430\u043d \u0438\u0433 \u0422\u0433 \u0432 \u042a\u0439\u0437 \u043d \u0432 \u0420 \u0437\u0437 \u0432 \u042b \u0433\u0436 \u0438 \u0436 \u0433\u0437\u0434 \u0438 \u0430 \u0438\u043d \u0432 \u0437\u0439\u0434\u0434\u0433\u0436\u0438 \u0439\u0436 \u0432 \u0431\u043d \u0437\u0438 \u043d\u041a \u042c \u0430 \u0437\u0438 \u0433 \u0438 \u0434 \u0433\u0434\u0430 \u0436\u0433\u0431 \u0436 \u0432\u0438 \u0432 \u0438 \u0438 \u0432 \u043a \u0436 \u0430 \u0438 \u0431 \u0433\u043b\u0432 \u0432 \u043b\u0433\u0436 \u0438\u0433 \u0431 \u0431\u043d \u0438\u0436 \u043a \u0430 \u0434\u0433\u0437\u0437 \u0430 \u0437 \u0437\u0433 \u0430 \u0436 \u0438 \u0438 \u0421 \u0433\u0439\u0430 \u0432 \u043a \u0436 \u040c\u0438 \u0438 \u0432 \u0438 \u0437 \u0434 \u0418 \u0438 \u0432 \u0437 \u0438\u0433 \u0430\u0430 \u0433 \u043d\u0433\u0439\u041a", "num_citations": "15\n", "authors": ["1592"]}
{"title": "Slicing SAL\n", "abstract": " Model checking has been successfully applied to verify nite-state systems albeit ones with small state-space. But most interesting systems have very large or in nite state-spaces. Automatic Abstraction techniques can help alleviate the state-space explosion problem to some extent. Another complementary approach is the use of program slicing to automatically remove portions of the input transition system irrelevant to the property being veri ed. This may result in state-space reduction. The reduced state system, if nite, may then be more amenable to model checking.", "num_citations": "15\n", "authors": ["1592"]}
{"title": "Automatic dimensional analysis of cyber-physical systems\n", "abstract": " The first step in building a cyber-physical system is the construction of a faithful model that captures the relevant behaviors. Dimensional consistency provides the first check on the correctness of such models and the physical quantities represented in it. Though manual analysis of dimensions is used in physical sciences to find errors in formulas, this approach does not scale to complex cyber-physical systems with many interacting components. We present DimSim, a tool to automatically check the dimensional consistency of a cyber-physical system modeled in Simulink. DimSim generates a set of constraints from the Simulink model for each subsystem in a modular way, and solves them using the Gauss-Jordan elimination method. The tool depends on user-provided dimension annotations, and it can detect both inconsistency and underspecification in the given dimensional constraints. In case of a\u00a0\u2026", "num_citations": "14\n", "authors": ["1592"]}
{"title": "Machine reading using markov logic networks for collective probabilistic inference\n", "abstract": " DARPA\u2019s Machine Reading project is directed at extracting specific information from natural language text such as events from news articles. We describe a component of FAUST, a system designed for machine reading, which combines stateof-the-art information extraction (IE), based on statistical parsing and local sentencewise analysis, with global article-wide inference using Markov Logic Networks (MLNs). We use the probabilistic first-order rules of MLNs to encode domain knowledge for resolving ambiguities and inconsistencies of the extracted information, and for merging multiple information sources. Experiments over a corpus of NFL news articles show that collective inference using the probabilistic relational MLN engine substantially improves the performance, according to F-measure, over individual IE engines.", "num_citations": "14\n", "authors": ["1592"]}
{"title": "Inference systems for logical algorithms\n", "abstract": " Logical algorithms are defined in terms of individual computation steps that are based on logical inferences. We present a uniform framework for formalizing logical algorithms based on inference systems. We present inference systems for algorithms such as resolution, the Davis\u2013Putnam\u2013Logemann\u2013Loveland procedure, equivalence and congruence closure, and satisfiability modulo theories. The paper is intended as an introduction to the use of inference systems for studying logical algorithms.", "num_citations": "14\n", "authors": ["1592"]}
{"title": "Proofs in conflict-driven theory combination\n", "abstract": " Search-based satisfiability procedures try to construct a model of the input formula by simultaneously proposing candidate models and deriving new formulae implied by the input. When the formulae are satisfiable, these procedures generate a model as a witness. Dually, it is desirable to have a proof when the formulae are unsatisfiable. Conflict-driven procedures perform nontrivial inferences only when resolving conflicts between the formulae and assignments representing the candidate model. CDSAT (Conflict-Driven SATisfiability) is a method for conflict-driven reasoning in combinations of theories. It combines solvers for individual theories as theory modules within a solver for the union of the theories. In this paper we endow CDSAT with lemma learning and proof generation. For the latter, we present two techniques. The first one produces proof objects in memory: it assumes that all theory modules produce\u00a0\u2026", "num_citations": "12\n", "authors": ["1592"]}
{"title": "Design and verification of multi-rate distributed systems\n", "abstract": " Multi-rate systems arise naturally in distributed settings where computing units execute periodically according to their local clocks and communicate among themselves via message passing. We present a systematic way of designing and verifying such systems with the assumption of bounded drift for local clocks and bounded communication latency. First, we capture the system model through an architecture definition language (called RADL) that has a precise model of computation and communication. The RADL paradigm is simple, compositional, and resilient against denial-of-service attacks. Our radler build tool takes the architecture definition and individual local functions as inputs and generate executables for the overall system as output. In addition, we present a modular encoding of multi-rate systems using calendar automata and describe how to verify real-time properties of these systems using SMT-based\u00a0\u2026", "num_citations": "12\n", "authors": ["1592"]}
{"title": "Combining model checking and deduction\n", "abstract": " There are two basic approaches to automated verification. In model checking, the system is viewed as a graph representing possible execution steps. Properties are established by exploring or traversing the graph structure. In deduction, both the system and its putative properties are represented by formulas in a logic, and the resulting proof obligations are discharged by decision procedures or by automated or semi-automated proof construction. Model checking sacrifices expressivity for greater automation, and with deduction it is vice versa. Newer techniques combine deductive and model-checking approaches to achieve greater scale, expressivity, and automation. We examine the logical foundations of the two approaches and explore their similarities, differences, and complementarities. The presentation is directed at students and researchers who are interested in understanding the research challenges\u00a0\u2026", "num_citations": "11\n", "authors": ["1592"]}
{"title": "A framework for high-assurance quasi-synchronous systems\n", "abstract": " The design of a complex cyber-physical system is centered around one or more models of computation (MoCs). These models define the semantic framework within which a network of sensors, controllers, and actuators operate and interact with each other. In this paper, we examine the foundations of a quasi-synchronous model of computation Our version of the quasi-synchronous model is inspired by the Robot Operating System (ROS). It consists of nodes that encapsulate computation and topic channels that are used for communicating between nodes. The nodes execute with a fixed period with possible jitter due to local clock drift and scheduling uncertainties, but are not otherwise synchronized. The channels are implemented through a mailbox semantics. In each execution step, a node reads its incoming mailboxes, applies a next-step operation to update its local state, and writes to all its outgoing mailboxes\u00a0\u2026", "num_citations": "11\n", "authors": ["1592"]}
{"title": "Accountable clouds\n", "abstract": " An increasing number of organizations are migrating their critical information technology services, from healthcare to business intelligence, into public cloud computing environments. However, even if cloud technologies are continuously evolving, they still have not reached a maturity level that allows them to provide users with high assurance about the security of their data beyond existent service level agreements (SLAs). To address this limitation, we propose a suite of mechanisms that enhances cloud computing technologies with more assurance capabilities. Assurance becomes a measurable property, quantified by the volume of evidence to audit and retain in a privacy-preserving and nonrepudiable fashion. By proactively collecting potential forensic evidence, the cloud becomes more accountable, while providing its regular services. In the case of a security breach, the cloud provides the appropriate reactive\u00a0\u2026", "num_citations": "11\n", "authors": ["1592"]}
{"title": "Mechanized verification of real-time systems using PVS\n", "abstract": " Some important properties of certain safety-critical systems can only be established by a careful analysis of their real-time behavior. These systems are typically also designed to meet other functional requirements where the real-time behavior is irrelevant. A suitable verification framework for such systems must therefore be capable of coping with functional as well as real time behavior. We present an approach to the verification of the real-time behavior of concurrent programs and describe its mechanization using the PVS proof checker. Our approach to real-time behavior extends previous verification techniques for concurrent programs by introducing a new operator for reasoning about absolute time. This operator returns the amount of time that has elapsed since the given predicate last held in the current computation. This approach is mechanized by first encoding the computational behavior of real-time systems\u00a0\u2026", "num_citations": "11\n", "authors": ["1592"]}
{"title": "Method for combining decision procedures\n", "abstract": " The method provides a sound and complete online decision method for the combination of canonizable and solvable theories together with uninterpreted function and predicate symbols. It also provides the representation of a solution state in terms of theory-wise solution sets that are used to capture the equality information extracted from the processed equalities. The method includes a context-sensitive canonizer that uses theory-specific canonizers and the solution state to obtain the canonical form of an expression with respect to the given equality information. Moreover, included is the variable abstraction operation for reducing and equality between term to an equality between variables and an enhanced solution state. The closure operation for propagating equality information between solution sets for individual theories uses the theory-specific solvers. The invention teaches a modular method for combining\u00a0\u2026", "num_citations": "10\n", "authors": ["1592"]}
{"title": "Conflict-driven satisfiability for theory combination: transition system and completeness\n", "abstract": " Many applications depend on solving the satisfiability of formul\u00e6 involving propositional logic and first-order theories, a problem known as Satisfiability Modulo Theory. This article presents a new method for satisfiability modulo a combination of theories, named CDSAT, for Conflict-Driven SATisfiability. CDSAT also solves Satisfiability Modulo Assignment problems that may include assignments to first-order terms. A conflict-driven procedure assigns values to variables to build a model, and performs inferences on demand in order to solve conflicts between assignments and formul\u00e6. CDSAT extends this paradigm to generic combinations of disjoint theories, each characterized by a collection of inference rules called theory module. CDSAT coordinates the theory modules in such a way that the conflict-driven reasoning happens in the union of the theories, not only in propositional logic. As there is no fixed hierarchy\u00a0\u2026", "num_citations": "9\n", "authors": ["1592"]}
{"title": "Code generation using a formal model of reference counting\n", "abstract": " Reference counting is a popular technique for memory management. It tracks the number of active references to a data object during the execution of a program. Reference counting allows the memory used by a data object to be freed when there are no active references to it. We develop the metatheory of reference counting by presenting an abstract model for a functional language with arrays. The model is captured by an intermediate language and its operational semantics, defined both with and without reference counting. These two semantics are shown to correspond by means of a bisimulation. The reference counting implementation allows singly referenced data objects to be updated in place, i.e., without copying. The main motivation for our model of reference counting is in soundly translating programs from a high-level functional language, in our case, an executable fragment of the PVS\u00a0\u2026", "num_citations": "9\n", "authors": ["1592"]}
{"title": "Automated verification using deduction, exploration, and abstraction\n", "abstract": " Computer programs are formal texts that are composed by programmers and executed by machines. Formal methods are used to predict the execution-time behavior of a program text through formal, symbolic calculation. Automation in the form of computer programs can be used to execute formal calculations so that they are reproducible and checkable. Deduction and exploration are two basic frameworks for the formal calculation of program properties. Both deduction and exploration have their limitations. We argue that these limitations can be overcome through a methodology for automated verification that uses property-preserving abstractions to bridge the gap between deduction and exploration. We introduce models, logics, and verification methods for transition systems, and outline a methodology based on the combined use of deduction, exploration, and abstraction.", "num_citations": "9\n", "authors": ["1592"]}
{"title": "Computer-aided computing\n", "abstract": " Formal program design methods are most useful when supported with suitable mechanization. This need for mechanization has long been apparent, but there have been doubts whether verification technology could cope with the problems of scale and complexity. Though there is very little compelling evidence either way at this point, several powerful mechanical verification systems are now available for experimentation. Using SRI's PVS as one representative example, we argue that the technology of mechanical verification is already quite effective. PVS derives its power from an integration of theorem proving with type-checking, decision procedures with interactive proof construction, and more recently, model checking with theorem proving. We discuss these individual aspects of PVS using examples, and motivate some of the challenges that lie ahead.", "num_citations": "9\n", "authors": ["1592"]}
{"title": "Introducing cyberlogic\n", "abstract": " Cyberlogic is an enabling foundation for building and analyzing protocols that involve the exchange of electronic forms of evidence. The key ideas underlying Cyberlogic are extremely simple. First, evidence is encoded by means of numbers using digital certificates and nonces. Second, predicates are signed by private keys so that a decryption of such a certificate with the corresponding public key is a proof or evidence for the assertion contained in the certificate. Third, protocols are distributed logic programs that gather evidence by using both ordinary predicates and digital certificates. These simple building blocks can be used to construct a rich variety of services in a variety of domains ranging from digital government to access control in computer systems.", "num_citations": "8\n", "authors": ["1592"]}
{"title": "Unifying verification paradigms\n", "abstract": " The field of formal methods is blessed with an overabundance of formalisms (functional, relational, automata-theoretic, modal, and temporal), techniques (resolution, rewriting, induction, and model checking), and application areas (hardware, reactive, fault-tolerant, real-time, and hybrid systems). No single verification approach has proven convincingly superior to the others. I argue that it is both necessary and desirable to develop a unified framework within which different approaches can coexist. The paper outlines some preliminary efforts in this direction in the context of SRI's PVS system. These efforts include the embedding of special-purpose formalisms (e.g., the Duration Calculus) into the general-purpose PVS logic, the integration of theorem proving with various forms of model checking, and the application of theorem proving and model checking to the analysis of tabular specifications.", "num_citations": "8\n", "authors": ["1592"]}
{"title": "JBernstein: A Validity Checker for Generalized Polynomial Constraints\n", "abstract": " Overview               Efficient and scalable verification of nonlinear real arithmetic constraints is essential in many automated verification and synthesis tasks for hybrid systems, control algorithms, digital signal processors, and mixed analog/digital circuits. Despite substantial advances in verification technology, complexity issues with classical decision procedures for nonlinear real arithmetic are still a major obstacle for formal verification of real-world applications.", "num_citations": "7\n", "authors": ["1592"]}
{"title": "A model-constructing framework for theory combination\n", "abstract": " This report presents a conflict-driven satisfiability inference system (CDSAT) for (quantifier-free) first-order logic modulo a generic combination of disjoint theories. We determine the requirements that the theories and their decision procedures need to satisfy for a CDSAT combination, generalizing both equality sharing and the MCSAT system of De Moura and Jovanovic, that was introduced for one generic theory and extended to a combination of specific disjoint theories. We prove soundness, completeness, and termination of CDSAT.", "num_citations": "6\n", "authors": ["1592"]}
{"title": "A Brief Introduction to the PVS2C Code Generator.\n", "abstract": " We present a brief tutorial on the PVS2C code generator for producing C code from an applicative fragment of the PVS specification language. This fragment roughly corresponds to a self-contained functional language. The tutorial covers the generation of C code for numeric data types and associated operations, arrays, recursive data types, and higherorder operations.", "num_citations": "5\n", "authors": ["1592"]}
{"title": "ModelRob: A Simulink Library for Model-Based Development of Robot Manipulators\n", "abstract": " Robot manipulators are widely used in many industrial automation applications. A robot manipulator moves the end-effector to the configuration instructed by the user. The user input from a master unit is transformed into the desired configuration through forward kinematics. This configuration is communicated to the robot controller, which employs inverse kinematics to transform the configuration into joint angles. The control algorithm is implemented as software and embedded into the robot controller. The software is typically written in traditional programming languages like C or C++. We introduce a Simulink Library ModelRob that provides basic building blocks to model kinematics of a robot manipulator. Availability of such a library enables Model-Based Development (MBD) of robot manipulator software, where the manipulator controller can be modeled using ModelRob library blocks, and production code can be\u00a0\u2026", "num_citations": "5\n", "authors": ["1592"]}
{"title": "Rewriting, inference, and proof\n", "abstract": " Rewriting is a form of inference, and one that interacts in several ways with other forms of inference such as decision procedures and proof search. We discuss a range of issues at the intersection of rewriting and inference. How can other inference procedures be combined with rewriting? Can rewriting be used to describe inference procedures? What are some of the theoretical challenges and practical applications of combining rewriting and inference? How can rewriters, decision procedures, and their combination be certified? We discuss these problems in the context of our ongoing effort to use PVS as a metatheoretic framework to construct a proof kernel for justifying the claims of theorem provers, rewriters, model checkers, and satisfiability solvers.", "num_citations": "5\n", "authors": ["1592"]}
{"title": "First-order cyberlogic\n", "abstract": " Cyberlogic [RS03] is an enabling foundation for building and analyzing protocols that involve the exchange of electronic forms of evidence. Protocols are distributed logic programs that gather evidence by using predicates and digital certificates. This document presents the underlying logic, based on first-order hereditary Harrop [Nad92] formulas, and some interesting examples beyond trust management.", "num_citations": "5\n", "authors": ["1592"]}
{"title": "The Parsley Data Format Definition Language\n", "abstract": " Any program that reads formatted input relies on parsing software to check the input for validity and transform it into a representation suitable for further processing. Many security vulnerabilities can be attributed to poorly defined grammars, incorrect parsing, and sloppy input validation. In contrast to programming languages, grammars for even common data formats such as ELF and PDF are typically context-sensitive and heterogenous. However, as in programming languages, a standard notation or language to express these data format grammars can address poor or ambiguous definitions, and the automated generation of correct-by-construction parsers from such grammar specifications can yield correct and type- and memory-safe data parsing routines. We present our ongoing work on developing such a data format description language. Parsley is a declarative data format definition language that combines\u00a0\u2026", "num_citations": "4\n", "authors": ["1592"]}
{"title": "A verified packrat parser interpreter for parsing expression grammars\n", "abstract": " Parsing expression grammars (PEGs) offer a natural opportunity for building verified parser interpreters based on higher-order parsing combinators. PEGs are expressive, unambiguous, and efficient to parse in a top-down recursive descent style. We use the rich type system of the PVS specification language and verification system to formalize the metatheory of PEGs and define a reference implementation of a recursive parser interpreter for PEGs. In order to ensure termination of parsing, we define a notion of a well-formed grammar. Rather than relying on an inductive definition of parsing, we use abstract syntax trees that represent the computational trace of the parser to provide an effective proof certificate for correct parsing and ensure that parsing properties including soundness and completeness are maintained. The correctness properties are embedded in the types of the operations so that the proofs can be\u00a0\u2026", "num_citations": "4\n", "authors": ["1592"]}
{"title": "Duality-based nested controller synthesis from STL specifications for stochastic linear systems\n", "abstract": " We propose an automatic synthesis technique to generate provably correct controllers of stochastic linear dynamical systems for Signal Temporal Logic (STL) specifications. While formal synthesis problems can be directly formulated as exists-forall constraints, the quantifier alternation restricts the scalability of such an approach. We use the duality between a system and its proof of correctness to partially alleviate this challenge. We decompose the controller synthesis into two subproblems, each addressing orthogonal concerns - stabilization with respect to the noise, and meeting the STL specification. The overall controller is a nested controller comprising of the feedback controller for noise cancellation and an open loop controller for STL satisfaction. The correct-by-construction compositional synthesis of this nested controller relies on using the guarantees of the feedback controller instead of the controller\u00a0\u2026", "num_citations": "4\n", "authors": ["1592"]}
{"title": "Declaratively processing provenance metadata\n", "abstract": " Systems that gather fine-grained provenance metadata must process and store large amounts of information. Filtering this metadata as it is collected has a number of benefits, including reducing the amount of persistent storage required and simplifying subsequent provenance queries. However, writing these filters in a procedural language is verbose and error prone. We propose a simple declarative language for processing provenance metadata and evaluate it by translating filters implemented in SPADE [9], an open-source provenance collection platform.", "num_citations": "4\n", "authors": ["1592"]}
{"title": "A tool bus for anytime verification\n", "abstract": " The science and technology of verification has advanced rapidly in recent years and it is now being used in substantive industrial projects. However, these opportunistic applications do not yet reflect a radical transformation of practice. Such a transformation can only come about through the flexible and pervasive use of formalization and formal analysis. Anytime Verification (AV) is a way of exploiting verification technology to the extent that it is fruitful while deriving the maximal benefit from it. The basic idea in AV is to systematically deepen the link between a design artifact and its putative properties to the level of rigor that is feasible. In AV, tools are used to the extent that they are robustly effective in supporting modeling, discharging proof obligations, optimizing code, and in generating counterexamples and test cases. The infrastructure for AV is provided by the Evidential Tool Bus (ETB) which supports interoperability between multiple inference tools in aid of specific verification tasks.In order for something to be usable, it must first be useful. Verification technology is getting there. We can now use it to support the development of software through modeling, test generation, type checking, static analysis, abstraction, optimization, and certification. But these advances have not yet reached the programming masses. This chasm between the possibilities raised by formal technologies and currently entrenched practice is not easily bridged. Modern verification techniques offer a lot of entry points for the prospective user. Some users might focus on the modeling and specification capabilities, while others focus on finding bugs and security vulnerabilities\u00a0\u2026", "num_citations": "4\n", "authors": ["1592"]}
{"title": "Verification by abstraction\n", "abstract": " Verification seeks to prove or refute putative properties of a given program. Deductive verification is carried out by constructing a proof that the program satisfies its specification, whereas model checking uses state exploration to find computations where the property fails. Model checking is largely automatic but is effective only for programs defined over small state spaces. Abstraction serves as a bridge between the more general deductive methods for program verification and the restricted but effective state exporation methods used in model checking. In verification by abstraction, deduction is used to construct a finite-state approximation of a program that preserves the property of interest. The resulting abstraction can be explored for offending computations through the use of model checking. We motivate the use of abstraction in verification and survey some of the recent advances.", "num_citations": "4\n", "authors": ["1592"]}
{"title": "Mechanical Verification of a Schematic Protocol for Byzantine Fault Tolerant Clock Synchronization\n", "abstract": " Schneider generalizes a number of protocols for Byzantine fault tolerant clock synchronization and presents a uniform proof for their correctness. We present a machine checked proof of this schematic protocol that revises some of the details in Schneider's original analysis. The verification was carried out with the Ehdm system developed at the SRI Computer Science Laboratory. The mechanically checked proofs include the verification that the egocentric mean function used in Lamport and Melliar-Smith's Interactive Convergence Algorithm satisfies the requirements of Schneider's protocol.", "num_citations": "4\n", "authors": ["1592"]}
{"title": "System and method using information based indicia for securing and authenticating transactions\n", "abstract": " A method, system, and apparatus for authenticating transactions and records is disclosed. An information-based indicium includes an article bearing a first identifier, wherein the first identifier substantially prevents a single user from accumulating multiple articles bearing the same first identifier, and a first digital certificate that is derived in part by encoding the first identifier, wherein the first digital certificate and the article may be presented together to authenticate the indicium by comparing the first digital certificate and the first identifier.", "num_citations": "3\n", "authors": ["1592"]}
{"title": "System Support for Forensic Inference\n", "abstract": " Digital evidence is playing an increasingly important role in prosecuting crimes. The reasons are manifold: financially lucrative targets are now connected online, systems are so complex that vulnerabilities abound and strong digital identities are being adopted, making audit trails more useful. If the discoveries of forensic analysts are to hold up to scrutiny in court, they must meet the standard for scientific evidence. Software systems are currently developed without consideration of this fact. This paper argues for the development of a formal framework for constructing \u201cdigital artifacts\u201d that can serve as proxies for physical evidence; a system so imbued would facilitate sound digital forensic inference. A case study involving a filesystem augmentation that provides transparent support for forensic inference is described.", "num_citations": "3\n", "authors": ["1592"]}
{"title": "The challenge of software verification\n", "abstract": " The Challenge of Software Verification Page 1 ' & $ % The Challenge of Software Verification N. Shankar shankar@csl.sri.com URL: http://www.csl.sri.com/shankar/ Computer Science Laboratory SRI International Menlo Park, CA 1 Page 2 ' & $ % Overview Professor Tony Hoare has proposed the goal of automatically verified software as a grand scientific challenge for computing. A series of workshops (funded by NCO HCSS through NSF) have been organized to explore the nature of this challenge. A preliminary workshop was held at SRI International in Washington DC in April 2004 and was attended by about 50 participants. A larger workshop was held recently (Feb 21\u201323, 2005) at SRI International in Menlo Park (chaired by Jay Misra, Greg Morrisett, and NS). A working conference will be held in Zurich, Switzerland during the week of Oct. 10, 2005 (www.vstte.ethz.ch). 2 Page 3 ' & $ % Outline \u2022 A brief history of \u2026", "num_citations": "3\n", "authors": ["1592"]}
{"title": "Industrial strength formal verification techniques for hardware designs\n", "abstract": " The past decade has seen tremendous progress in the application of formal methods for hardware design and verification. While a number of different techniques based on BDDs, symbolic simulation, special-purpose decision procedures, model checking, and theorem proving have been applied with varying degrees of success, no one technique by itself has proven to be effective enough to verify a complex register-transfer level design, such as a state-of-the-art microprocessor. To scale up formal verification to industrial-scale designs it is necessary to combine these complimentary techniques within a general logical environment that can support appropriate abstraction mechanisms. The Prototype Verification System (PVS) is an environment to support the exploration of such a combined approach to verification. PVS is designed to exploit the synergies between language and deduction, automation and\u00a0\u2026", "num_citations": "3\n", "authors": ["1592"]}
{"title": "The correctness of a code generator for a functional language\n", "abstract": " Code generation is gaining popularity as a technique to bridge the gap between high-level models and executable code. We describe the theory underlying the PVS2C code generator that translates functional programs written using the PVS specification language to standalone, efficiently executable C code. We outline a correctness argument for the code generator. The techniques used are quite generic and can be applied to transform programs written in functional languages into imperative code. We use a formal model of reference counting to capture memory management and safe destructive updates for a simple first-order functional language with arrays. We exhibit a bisimulation between the functional execution and the imperative execution. This bisimulation shows that the generated imperative program returns the same result as the functional program.", "num_citations": "2\n", "authors": ["1592"]}
{"title": "The semantics of Datalog for the Evidential Tool Bus\n", "abstract": " The Evidential Tool Bus (ETB) is a distributed framework for tool integration for the purpose of building and maintaining assurance cases. ETB employs Datalog as a metalanguage both for defining workflows and representing arguments. The application of Datalog in ETB differs in some significant ways from its use as a database query language. For example, in ETB Datalog predicates can be tied to external tool invocations. The operational treatment of such external calls is more expressive than the use of built-in predicates in Datalog. We outline the semantic characteristics of the variant of Datalog used in ETB and describe an abstract machine for evaluating Datalog queries.", "num_citations": "2\n", "authors": ["1592"]}
{"title": "Fixpoints and Search in PVS\n", "abstract": " The Knaster\u2013Tarski theorem asserts the existence of least and greatest fixpoints for any monotonic function on a complete lattice. More strongly, it asserts the existence of a complete lattice of such fixpoints. This fundamental theorem has a fairly straightforward proof. We use a mechanically checked proof of the Knaster\u2013Tarski theorem to illustrate several features of the Prototype Verification System (PVS). We specialize the theorem to the power set lattice, and apply the latter to the verification of a general forward search algorithm and a generalization of Dijkstra\u2019s shortest path algorithm. We use these examples to argue that the verification of even simple, widely used algorithms can depend on a fair amount of background theory, human insight, and sophisticated mechanical support.", "num_citations": "2\n", "authors": ["1592"]}
{"title": "Modular Verification of SRT Division\n", "abstract": " We describe a formal specification and mechanized verification in PVS of the general theory of SRT division along with a specific hardware realization of the algorithm. The specification demonstrates how attributes of the PVS language (in particular, predicate subtypes) allow the general theory to be developed in a readable manner that is similar to textbook presentations, while the PVS table construct allows direct specification of the implementation's quotient lookup table. Verification of the derivations in the SRT theory and for the data path and lookup table of the implementation are highly automated and performed for arbitrary but finite precision; in addition, the theory is verified for general radix, while the implementation is specialized to radix 4. The effectiveness of the automation stems from the tight integration in PVS of rewriting with decision procedures for equality, linear arithmetic over integers and rationals\u00a0\u2026", "num_citations": "2\n", "authors": ["1592"]}
{"title": "Integration of formal verification with real-time design\n", "abstract": " As computers play greater roles in critical functions of complex systems, an increased reliance on formal methods for verification of critical components will be required. However, formal methods are often criticized regarding their intractability to large problems domains and lack of mechanical support. The paper reports on preliminary research aimed at integrating formal verification techniques with an OO, real time CASE tool. We establish the feasibility of translating a system implementation model, expressed in the Real Time Object Oriented Modeling (ROOM) notation, into the Murphi model checker notation. Fragments of the translation algorithm are implemented in a proof of concept prototype; two formal analysis benchmarks are manually replicated using the translator definition. Methods for graphically expressing safety and timing constraints are demonstrated.", "num_citations": "2\n", "authors": ["1592"]}
{"title": "The PVS Proof Checker:...\n", "abstract": " ion and Verification System (Draft). Computer Science Laboratory, SRI International, Menlo Park, CA, February 1993.[SOR93] N. Shankar, S. Owre, and JM Rushby. PVS Tutorial. Computer Science Laboratory, SRI International, Menlo Park, CA, February 1993. Beta Release 58 REFERENCES rerun: Rerun a Proof or Partial Proof syntax:(rerun &optional proof) effect: This step can be used to rerun a partial or completed proof from a previous attempt or from another branch of the proof. This step is largely used automatically by the system when it queries as to whether the proof should be rerun. The proof argument can also be explicitly given by the user using either the Mx edit-proof or Mx show-proof commands to generate and edit such inputs. This step can be used to: 1. Restore a partial proof to the state when the proof was interrupted", "num_citations": "2\n", "authors": ["1592"]}
{"title": "The First Fifteen Years of the Verified Software Project\n", "abstract": " Edsger Dijkstra once remarked, with tongue firmly in cheek, that he and Tony Hoare both drew their notoriety from a clever algorithm and an oft-repeated quote. In Dijkstra\u2019s case, these were the shortest paths algorithm and the observation that \u2018testing can prove the presence of bugs but never their absence.\u2019For Hoare, the algorithm in question was Quicksort, and the quote:\u2018Here is a language so far ahead of its time, that it was not only an improvement on its predecessors, but also on nearly all its successors.\u2019Both Hoare and Dijkstra were motivated by the chal lenge of crafting software that was correct by construction. Dijkstra framed this challenge as one of using a finite brain and finite language to reason about infinite behaviors. 1 The scale of proof construction required to handle millions of lines of", "num_citations": "1\n", "authors": ["1592"]}
{"title": "Conflict-Driven Satisfiability for Theory Combination: Lemmas, Modules, and Proofs\n", "abstract": " Search-based satisfiability procedures try to build a model of the input formula by simultaneously proposing candidate models and deriving new formul\u00e6 implied by the input. Conflict-driven procedures perform nontrivial inferences only when resolving conflicts between formul\u00e6 and assignments representing the candidate model. CDSAT (Conflict-Driven SATisfiability) is a method for conflict-driven reasoning in unions of theories. It combines inference systems for individual theories as theory modules within a solver for the union of the theories. This article augments CDSAT with a more general lemma learning capability and with proof generation. Furthermore, theory modules for several theories of practical interest are shown to fulfill the requirements for completeness and termination of CDSAT. Proof generation is accomplished by a proof-carrying version of the CDSAT transition system, that produces proof objects in memory accommodating multiple proof formats. Alternatively, one can apply to CDSAT the LCF approach to proofs from interactive theorem proving, by defining a kernel of reasoning primitives that guarantees the correctness by construction of CDSAT proofs.", "num_citations": "1\n", "authors": ["1592"]}
{"title": "Contract-based verification of complex time-dependent behaviors in avionic systems\n", "abstract": " Avionic systems involve complex time-dependent behaviors across interacting components. This paper presents a contract-based approach for formally verifying these behaviors in a compositional manner. A unique feature of our contract-based tool is the support of architectural specification for multi-rate platforms. An abstraction technique has also been developed for properties related to variable time bounds. Preliminary results on applying this approach to the verification of an aircraft cabin pressure control system are promising.", "num_citations": "1\n", "authors": ["1592"]}
{"title": "Automated reasoning, fast and slow\n", "abstract": " Psychologists have argued that human behavior is the result of the interaction between two different cognitive modules. System\u00a01 is fast, intuitive, and error-prone, whereas System 2 is slow, logical, and reliable. When it comes to reasoning, the field of automated deduction has focused its attention on the slow System 2 processes. We argue that there is an increasing role for forms of reasoning that are closer to System\u00a01 particularly in tasks that involve uncertainty, partial knowledge, and discrimination. The interaction between these two systems of reasoning is also fertile ground for further exploration. We present some tentative and preliminary speculation on the prospects for automated reasoning in the style of System 1, and the synergy with the more traditional System\u00a02 strand of work. We explore this interaction by focusing on the use of cues in perception, reasoning, and communication.", "num_citations": "1\n", "authors": ["1592"]}
{"title": "Speaking Logic\n", "abstract": " Perhaps I can best describe my experience of doing mathematics in terms of a journey through a dark unexplored mansion. You enter the first room of the mansion and it\u2019s completely dark. You stumble around bumping into the furniture, but gradually you learn where each piece of furniture is. Finally, after six months or so, you find the light switch, you turn it on, and suddenly it\u2019s all illuminated. You can see exactly where you were. Then you move into the next room and spend another six months in the dark. So each of these breakthroughs, while sometimes they\u2019re momentary, sometimes over a period of a day or two, they are the culmination of and couldn\u2019t exist without the many months of stumbling around in the dark that precede them. Andrew Wiles1", "num_citations": "1\n", "authors": ["1592"]}
{"title": "Can we rely on Formal Methods?\n", "abstract": " We can all agree that safety-critical systems already contain software and hardware components of immense complexity. The reliability of these discrete systems is obviously a matter of concern to those involved in building or using such systems. We can also agree that there is a major gap between the reliability levels that are required of these systems and those that are obtainable using traditional hardware and software design methods. The basic question then is: Can formal methods play a useful role infilling this reliability gap? I will argue that formal methods technology has matured to a point where it ought to be a major component of any process for building reliable systems. Like any other technology, it will have to be used judiciously and in conjunction with other effective design tools and techniques. I will also argue that if it makes sense to apply formal methods, then it also makes sense to employ\u00a0\u2026", "num_citations": "1\n", "authors": ["1592"]}
{"title": "On Shostak\u2019s Combination of Decision Procedures\n", "abstract": " On Shostak\u2019s Combination of Decision Procedures Page 1 On Shostak\u2019s Combination of Decision Procedures H. Rue\u2580, N. Shankar, A. Tiwari \u0430 ruess,shankar,tiwari \u0431 @csl.sri.com http://www.csl.sri.com/. Computer Science Laboratory SRI International 333 Ravenswood Menlo Park, CA 94025 Shostak\u2019s Combination (p.1 of 121) Page 2 The Combination Problem Verification conditions typically are in combination of many theories. \u0432 Theory of equality \u0432 Arithmetic constraints \u0432 Lists, Arrays, Bitvectors, . . . Examples. \u0432 \u0433 \u0434 \u0435 \u0436\u0437 \u0439 \u0433 \u0435 \u0436 \u0434 \u0435 \u0438 \u0436 \u0433 \u0432 \u0438 \u0436 \u0435 \u0436 \u0438 \u0433 \u0435 \u0433 \u0433 \u0435 \u0436\u0437 \" !# \u0432 \u0438 \u0438 \u0433 \u0438 \u0436 \u0435 $ \u0436 \u0433 \u0436 \u0433 $ $ %& \u0437 \" !# Shostak\u2019s Combination (p.2 of 121) Page 3 West-Coast Theorem Proving Theorem provers which rely heavily on decision procedures for automating proofs. Historical. \u0432 Stanford Pascal Verifier \u0432 Boyer-Moore Theorem Prover \u0432 Shostak\u2019s Theorem Prover (STP) Current. (Outline) \u0432 Simplify, Java/ESC \u0432 Stanford Temporal \u2026", "num_citations": "1\n", "authors": ["1592"]}