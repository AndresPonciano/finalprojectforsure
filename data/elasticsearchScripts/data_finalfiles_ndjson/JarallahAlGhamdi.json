{"title": "Adaptive fuzzy logic-based framework for software development effort prediction\n", "abstract": " Algorithmic effort prediction models are limited by their inability to cope with uncertainties and imprecision present in software projects early in the development life cycle. In this paper, we present an adaptive fuzzy logic framework for software effort prediction. The training and adaptation algorithms implemented in the framework tolerates imprecision, explains prediction rationale through rules, incorporates experts knowledge, offers transparency in the prediction system, and could adapt to new environments as new data becomes available. Our validation experiment was carried out on artificial datasets as well as the COCOMO public database. We also present an experimental validation of the training procedure employed in the framework.", "num_citations": "122\n", "authors": ["1225"]}
{"title": "Automated nuclei segmentation of malignant using level sets\n", "abstract": " Segmentation of objects from a noisy and complex image is still a challenging task that needs to be addressed. This article proposed a new method to detect and segment nuclei to determine whether they are malignant or not (determination of the region of interest, noise removal, enhance the image, candidate detection is employed on the centroid transform to evaluate the centroid of each object, the level set [LS] is applied to segment the nuclei). The proposed method consists of three main stages: preprocessing, seed detection, and segmentation. Preprocessing stage involves the preparation of the image conditions to ensure that they meet the segmentation requirements. Seed detection detects the seed point to be used in the segmentation stage, which refers to the process of segmenting the nuclei using the LS method. In this research work, 58 H&E breast cancer images from the UCSB Bio\u2010Segmentation\u00a0\u2026", "num_citations": "68\n", "authors": ["1225"]}
{"title": "A data-centric design for n-tier architecture\n", "abstract": " The increasing heterogeneity, complexity, and distributed nature of deployment architectures only serve to compound the problems faced by software solutions. With the advent of the Internet and web technologies, system designers have had to reevaluate the applicability of n-tier architectures, and assess which technologies are appropriate at each tier. In this paper we evaluate the design issues of n-tier architecture. We debate whether it is good to move code or data to communicate messages between applications. Based on the latest technology of J2EE and .NET framework, we recommend a data centric design of n-tier architecture. Based on our experience we also suggest a modified interactive software process model.", "num_citations": "65\n", "authors": ["1225"]}
{"title": "OOMeter: A software quality assurance tool\n", "abstract": " This paper gives an overview of OOMeter, a software metric tool that accepts Java and C# source code as well as UML models in XMI format.", "num_citations": "60\n", "authors": ["1225"]}
{"title": "Machine aided malaria parasitemia detection in Giemsa-stained thin blood smears\n", "abstract": " Malaria parasitemia is the quantitative measurement of the parasites in the blood to grade the degree of infection. Light microscopy is the most well-known method used to examine the blood for parasitemia quantification. The visual quantification of malaria parasitemia is laborious, time-consuming and subjective. Although automating the process is a good solution, the available techniques are unable to evaluate the same cases such as anemia and hemoglobinopathies due to deviation from normal RBCs\u2019 morphology. The main aim of this research is to examine the microscopic images of stained thin blood smears using a variety of computer vision techniques, grading malaria parasitemia on independent factors (RBC\u2019s morphology). The proposed methodology is based on inductive approach, color segmentation of malaria parasites through adaptive algorithm of Gaussian mixture model (GMM). The\u00a0\u2026", "num_citations": "49\n", "authors": ["1225"]}
{"title": "MEASURING SOFTWARE COUPLING.\n", "abstract": " Coupling is is a software internal attribute that was correlated to important software quality attributes such as maintainability, traceability, and robustness. This work presents a method for measuring coupling in a software system using two matrices. The coupling calculation is performed in two major steps. The first step creates a matrix that captures the nature of the underlying software system. The second step produces a matrix that indicates the degree of coupling between each pair of system components, and also measures the overall system coupling.", "num_citations": "36\n", "authors": ["1225"]}
{"title": "Towards adaptive soft computing based software effort prediction\n", "abstract": " Algorithmic effort prediction models are limited by their inability to cope with imprecision present in software projects early in the development life cycle. A critical survey carried out by Satin and Ahmed [17] reveals the lack of adaptive soft computing based effort prediction systems that provide complete transparency to the prediction system building strategies. In addition, efforts to model the imprecision problem in one of the most widely used algorithmic model, COCOMO, have not been appropriate [17]. The components of COCOMO model were addressed independently. Integrating the individual component into a single prediction system remains an open question. In this paper, we present a transparent and adaptive fuzzy logic framework for effort prediction based on COCOMO. The training strategies we have implemented in the framework tolerates imprecision, explains prediction rationale through rules\u00a0\u2026", "num_citations": "34\n", "authors": ["1225"]}
{"title": "Probabilistic size proxy for software effort prediction: A framework\n", "abstract": " Software effort prediction is an important and challenging activity that takes place during the early stages of software development, where costing is needed. Software size estimate is one of the most popular inputs for software effort prediction models. Accordingly, providing a size estimate with good accuracy early in the lifecycle is very important; it is equally challenging too. Estimates that are computed early in the development lifecycle, when it is needed the most, are typically associated with uncertainty. However, none of the prominent software effort prediction techniques or software size metrics addresses this issue satisfactorily. In this paper, we propose a framework for developing probabilistic size proxies for software effort prediction using information from conceptual UML models created early in the software development lifecycle. The framework accounts for uncertainty in software size and effort prediction by\u00a0\u2026", "num_citations": "23\n", "authors": ["1225"]}
{"title": "Comparing and assessing programming languages: basis for a qualitative methodology\n", "abstract": " Manyatudies havecumpared andassemed pgmming languages. Unfortunately, a comprehemive methodology fm comparisonand~~ t of~@ Pll~~@ been developed. lle main reason behind the absence of such a methodology is that previous studies have Concentratedon comparing languages rather than M developing such a methodology. This paper lays the groundwork fm establishing a comprehemive qualitative methodology to compare and assess general-purpose~ 1-=.-of~-fmtures of the general-purposepmgmmn@ languages will be factored out as much as possible fm the comparison and assessment to be more awurate. Features that can not be identified precisely are not included. Features that are solely basedon other featuresare not incl~ either. llerefore, only fmldamentalfeatures am included.", "num_citations": "16\n", "authors": ["1225"]}
{"title": "Mining software repositories\u2013a comparative analysis\n", "abstract": " Despite of many Mining Software Repositories (MSR) tools in use, it is a relatively new research domain, which forms the basis of classifying various tools and comparing them. In this paper we present a comparative analysis of different tools for MSR, based on some existing and new criteria proposed in this paper. This will assist in determining an appropriate tool that performs the best for a given type of application and to use it directly, instead of relying on the usual trial-and-error approach. This work has several purposes; it acts as a formative evaluation mechanism for tool designers (by quickly understanding and comparing different tools), as an assessment tool for potential tool users (by simply going through the comparative analysis chart to know at a glance, the essential components needed to be incorporated into the intended tool) and as a comparative milestone so that MSR tool researchers can easily differentiate amongst a pool of tools, thereby identifying other new research avenues. The tabular presentation furthers the work by providing a quick index to the reader and a means for quick analysis of the desired tool.", "num_citations": "11\n", "authors": ["1225"]}
{"title": "Measuring architectural stability in object oriented software\n", "abstract": " This paper proposes a way to measure architectural stability of an object-oriented system by means of similarity metrics. These similarity metrics compare the architectures of successive releases of a system to the base version. We then generate a regression line from the changes in these similarity values. We derive a stability measure that gives higher values for stable architectures and lower values for less stable ones.", "num_citations": "10\n", "authors": ["1225"]}
{"title": "A study on the uncertainty inherent in class cohesion measurements\n", "abstract": " Software metrics are essential for component certification and for the development of high quality software in general. Accordingly, research in the area of software metrics has been active and a wide range of metrics has been proposed. However, the variety of metrics proposed for measuring the same quality attribute suggests that there may be some sort of inconsistencies among the measurements computed using these metrics. In this paper, we report a case study considering class cohesion as a quality attribute of concern. We present the results of our empirical investigation to confirm that prominent object-oriented class cohesion metrics provide inconsistent measurements. We also present an analysis of the uncertainty that should be considered in these class cohesion measurements due to their inter-inconsistencies. Considering such uncertainty, we provide a framework for associating a probability\u00a0\u2026", "num_citations": "8\n", "authors": ["1225"]}
{"title": "A Novel Fused Image Compression Technique Using DFT, DWT, and DCT.\n", "abstract": " Image compression processes moderate the number of bits essential to signify an image, which can improve the performance of systems during storage and transmission without compromising image quality. Accordingly, a new hybrid image compression technique has been suggested in this research. Three transform-based techniques discrete Fourier transform (DFT), discrete wavelet transform (DWT), and discrete cosine transform (DCT) have been combined for image compression to confer the good characteristics of these methods. The proposed method works for forfeiture compression and the attained results show that it works well compared to existing approaches. To test the level of compression, the quantitative measures of the peak signal-to-noise ratio (PSNR) & mean squared error (MSE) are used to ensure the effectiveness of the suggested system.", "num_citations": "6\n", "authors": ["1225"]}
{"title": "Can Cohesion Predict Fault Density?\n", "abstract": " Cohesion is an internal software attribute which depicts how well the components of a software module are connected. It is thought of as having effect on the quality of the software system. This paper presents the results of an empirical investigation of whether cohesion of object-oriented systems plays a role on software reliability. The paper presents a study of the correlation between cohesion measures and the number of defects in software systems, using seven open source projects.", "num_citations": "6\n", "authors": ["1225"]}
{"title": "Multithreaded parallelism with OpenMP\n", "abstract": " While multithreaded programming is an effective way to exploit concurrency, multithreaded programs are notoriously hard to program, debug and tune for performance. In this chapter, we present OpenMP shared memory programming as a viable alternative and a much simpler way to write multithreaded programs. We show through empirical results obtained by running, on a single processor machine, a simple matrix multiplication program written in OpenMP C that the drop in performance compared with the single threaded version even on a uniprocessor machine may be negligible. However, this is well compensated for by the increased programmer productivity resulting from the ease of programming, debugging, tuning and the relative ease of OpenMP skill acquisition.", "num_citations": "6\n", "authors": ["1225"]}
{"title": "Theoretical validation of cohesion metrics in object oriented systems\n", "abstract": " One of the fundamental concepts of object-oriented (OO) systems is to build high quality software especially in the software industry. Therefore, the software developed using this paradigm is expected to be more reliable, easier to maintain, and more likely to be reused. Furthermore, effective control of development projects requires the use of sophisticated metrics for software quality. This, in turn, has lead to an increasingly large number of new measures being proposed for quality design principles such as cohesion. We describe state-of-the-art of five cohesion metrics. Tight Class Cohesion (TCC) and Loose Class Cohesion (LCC) proposed by Bieman and Kang, and Class Connected Metric (CCM), Connected Class Connection Metric (CCCM), and Modify Class Connection Metric (MCCM) proposed by Wasiq. In order to assess these measures, four different frameworks are presented which examine the same concept in different ways. A critique analysis for these frameworks is also provided. Finally, we provide a summary of our results in the application of these frameworks to those metrics.", "num_citations": "5\n", "authors": ["1225"]}
{"title": "Measuring the coupling of procedural programs\n", "abstract": " Coupling is one of two attributes of software that have great impact on software quality. Quite a few methods have been established to quantify the measurement of coupling. This paper presents a new method that provides coupling measurement of procedural programs. The first step of this method populates a description matrix that describes the software system that is being evaluated capturing all system attributes that affect coupling. Factors that affect coupling were studied and a scheme to reflect them in the description matrix was developed. A method has also been developed to calculate coupling between each two components of the system. The second step uses this method to populate a coupling matrix that indicates the coupling measurement between each two components of the system. Other metrics such as calculating the overall coupling of the system can be evaluated from the generated matrix. One\u00a0\u2026", "num_citations": "5\n", "authors": ["1225"]}
{"title": "A load balanced distributed computing system\n", "abstract": " The main objective of this study is to transform a network of workstations into a load balanced distributed computing system (LBDCS). LBDCS is to improve the performance of generally underutilized timeshared workstations and highly CPU intensive independent or parallel applications. It affects the initial placement of the tasks and task migrations later during their executions. One of the important implementation features of LBDCS is that it does not use any intermediary such as PVM (parallel virtual machine) or MPI (message passing interface) for inter\u2010task communication. It defines various metrics to characterize the level of load and dynamically monitors the system and applications to detect the load imbalances. The employed load balancing algorithm makes use of predicted load indices which are computed as weighted averages of the past system and application loads. Performance analysis of the system\u00a0\u2026", "num_citations": "5\n", "authors": ["1225"]}
{"title": "Programming languages: a qualitative methodology for assessments and software metrics to measure syntactic properties\n", "abstract": " The objective of the research for this dissertation was to develop a qualitative methodology for comparing and assessing general-purpose programming languages and to develop software metrics that are useful for the programming languages community. The qualitative methodology was developed by accumulating features of programming languages from the software engineering perspective and the programming languages perspective. The developed qualitative methodology provides a framework for evaluating and comparing general-purpose programming languages. The software metrics developed during the research for this dissertation are valuable tools that help programming language implementors, designers, and researchers to observe, understand, and deal with interactions among components of programming languages. A clustering algorithm that also benefits the programming languages\u00a0\u2026", "num_citations": "5\n", "authors": ["1225"]}
{"title": "Metric suite for assuring the quality of ERP implementation and development\n", "abstract": " Success of ERP implementation and development relies heavily on careful and accurate measurement of quality goals identified as part of the quality assurance and quality control exercise. A recent report published by Gartner Research revealed that organizations usually measure the effectiveness of ERP implementation and development in terms of time and budget than other factors such as quality of process, quality of product, and stability of requirements. In this paper we provide details of the metric suite designed by the third party Quality Assurance consultant to improve the outcome and help to accomplish successful completion of ERP project launched by the Ministry of Education, Saudi Arabia. The paper provides details of the quality goals, metrics selection criteria and metric suite defined for the purpose of assuring and controlling quality of the ERP implementation and development. This will serve as a\u00a0\u2026", "num_citations": "4\n", "authors": ["1225"]}
{"title": "Analysis and Theoretical Validation of Object-oriented Coupling Metrics.\n", "abstract": " Various object-oriented (OO) coupling metrics have been proposed for capturing the level of class coupling in object-oriented systems. Since product complexity plays a major role in determining the quality of software, metrics developed to characterize internal attributes such as coupling need to be validated to determine the usefulness of the measures. In this paper, we present an analysis of some OO coupling metrics. An interaction coupling metric, the modified coupling metric (MCC), is proposed as an extension of coupling between object (CBO)[1] and message passing coupling (MPC)[2]. We also perform a theoretical validation of the suite of OO inheritance coupling metrics proposed in [3] using wellknown coupling properties.", "num_citations": "3\n", "authors": ["1225"]}
{"title": "Quality assurance model capturing ERP implementation facets through RVRA services\n", "abstract": " Success of any ERP project requires the clear understanding of the various implementation facets and careful and appropriate identification of systematic QA activities to provide adequate confidence that when rolled out the system will fulfill requirements for quality and achieve envisaged business value. In this effort, we unleash details of rigorous ERP QA model designed at Ministry of Education, Saudi Arabia that encompasses four facets of ERP implementation: Project Management, Change Management, Solution Management and Services Management. We provide specific details about review, validation/verification, reporting and advisory (RVRA) services that need to be considered in order to prepare the course for successful ERP implementation.", "num_citations": "2\n", "authors": ["1225"]}
{"title": "A taxonomy of 3D occluded objects recognition techniques\n", "abstract": " The overall performances of object recognition techniques under different condition (e.g., occlusion, viewpoint, and illumination) have been improved significantly in recent years. New applications and hardware are shifted towards digital photography, and digital media. This faces an increase in Internet usage requiring object recognition for certain applications; particularly occulded objects. However occlusion is still an issue unhandled, interlacing the relations between extracted feature points through image, research is going on to develop efficient techniques and easy to use algorithms that would help users to source images; this need to overcome problems and issues regarding occlusion. The aim of this research is to review recognition occluded objects algorithms and figure out their pros and cons to solve the occlusion problem features, which are extracted from occluded object to distinguish objects\u00a0\u2026", "num_citations": "1\n", "authors": ["1225"]}
{"title": "Modelling by a rational spline with interval shape control\n", "abstract": " Various models have been developed for the design of distinct objects and for applications such as font design, computer aided design (CAD), computer aided engineering (CAE), etc. Some methods are better suited for controlling the shape of the curve on an interval, while others are better suited for controlling the shape at individual control points. The work reviews C/sup 2/ rational splines with interval tension and extends this work for the modelling of interpolatory curves and surfaces through B-spline formulation.", "num_citations": "1\n", "authors": ["1225"]}