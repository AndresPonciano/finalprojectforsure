{"title": "Learning to detect phishing emails\n", "abstract": " Each month, more attacks are launched with the aim of making web users believe that they are communicating with a trusted entity for the purpose of stealing account information, logon credentials, and identity information in general. This attack method, commonly known as\" phishing,\" is most commonly initiated by sending out emails with links to spoofed websites that harvest information. We present a method for detecting these attacks, which in its most general form is an application of machine learning on a feature set designed to highlight user-targeted deception in electronic communication. This method is applicable, with slight modification, to detection of phishing websites, or the emails used to direct victims to these sites. We evaluate this method on a set of approximately 860 such phishing emails, and 6950 non-phishing emails, and correctly identify over 96% of the phishing emails while only mis-classifying\u00a0\u2026", "num_citations": "815\n", "authors": ["2106"]}
{"title": "GlOSS text-source discovery over the Internet\n", "abstract": " The dramatic growth of the Internet has created a new problem for users: location of the relevant sources of documents. This article presents a framework for (and experimentally analyzes a solution to) this problem, which we call the text-source discovery problem. Our approach consists of two phases. First, each text source exports its contents to a centralized service. Second, users present queries to the service, which returns an ordered list of promising text sources. This article describes GlOSS, Glossary of Servers Server, with two versions: bGlOSS, which provides a Boolean query retrieval model, and vGlOSS, which provides a vector-space retrieval model. We also present hGlOSS, which provides a  decentralized version of the system. We extensively describe the methodology for measuring the retrieval effectiveness of these systems and provide experimental evidence, based on actual data, that all three\u00a0\u2026", "num_citations": "442\n", "authors": ["2106"]}
{"title": "Scaling access to heterogeneous data sources with DISCO\n", "abstract": " Accessing many data sources aggravates problems for users of heterogeneous distributed databases. Database administrators must deal with fragile mediators, that is, mediators with schemas and views that must be significantly changed to incorporate a new data source. When implementing translators of queries from mediators to data sources, database implementers must deal with data sources that do not support all the functionality required by mediators. Application programmers must deal with graceless failures for unavailable data sources. Queries simply return failure and no further information when data sources are unavailable for query processing. The Distributed Information Search COmponent (Disco) addresses these problems. Data modeling techniques manage the connections to data sources, and sources can be added transparently to the users and applications. The interface between mediators\u00a0\u2026", "num_citations": "337\n", "authors": ["2106"]}
{"title": "The effectiveness of GIOSS for the text database discovery problem\n", "abstract": " The popularity of on-line document databases has led to a new problem: finding which text databases (out of many candidate choices) are the most relevant to a user. Identifying the relevant databases for a given query is the text database discovery problem. The first part of this paper presents a practical solution based on estimating the result size of a query and a database. The method is termed GlOSS\u2014Glossary of Servers Server. The second part of this paper evaluates the effectiveness of GlOSS based on a trace of real user queries. In addition, we analyze the storage cost of our approach.", "num_citations": "311\n", "authors": ["2106"]}
{"title": "Scaling heterogeneous databases and the design of disco\n", "abstract": " Access to large numbers of data sources introduces new problems for users of heterogeneous distributed databases. End users and application programmers must deal with unavailable data sources. Database administrators must deal with incorporating new sources into the model. Database implementers must deal with the translation of queries between query languages and schemas. The Distributed Information Search COmponent (Disco) addresses these problems. Query processing semantics are developed to process queries over data sources which do not return answers. Data modeling techniques manage connections to data sources. The component interface to data sources flexibly handles different query languages and translates queries. This paper describes (a) the distributed mediator architecture of Disco, (b) its query processing semantics, (C) the data model and its modeling of data source\u00a0\u2026", "num_citations": "300\n", "authors": ["2106"]}
{"title": "Incremental updates of inverted lists for text document retrieval\n", "abstract": " With the proliferation of the world's \u201cinformation highways\u201d a renewed interest in efficient document indexing techniques has come about. In this paper, the problem of incremental updates of inverted lists is addressed using a new dual-structure index. The index dynamically separates long and short inverted lists and optimizes retrieval, update, and storage of each type of list. To study the behavior of the index, a space of engineering trade-offs which range from optimizing update time to optimizing query performance is described. We quantitatively explore this space by using actual data and hardware in combination with a simulation of an information retrieval system. We then describe the best algorithm for a variety of criteria.", "num_citations": "295\n", "authors": ["2106"]}
{"title": "Performance of inverted indices in shared-nothing distributed text document information retrieval systems\n", "abstract": " The impact on query processing performance of various physical organizations for inverted lists is compared. A probabilistic mode of the database and queries is introduced. Simulation experiments determine which variables most strongly influence response time and throughput. This leads to a set of design tradeoffs over a range of hardware configurations and new parallel query processing strategies.< >", "num_citations": "216\n", "authors": ["2106"]}
{"title": "Field trial of tiramisu: crowd-sourcing bus arrival times to spur co-design\n", "abstract": " Crowd-sourcing social computing systems represent a new material for HCI designers. However, these systems are difficult to work with and to prototype, because they require a critical mass of participants to investigate social behavior. Service design is an emerging research area that focuses on how customers co-produce the services that they use, and thus it appears to be a great domain to apply this new material. To investigate this relationship, we developed Tiramisu, a transit information system where commuters share GPS traces and submit problem reports. Tiramisu processes incoming traces and generates real-time arrival time predictions for buses. We conducted a field trial with 28 participants. In this paper we report on the results and reflect on the use of field trials to evaluate crowd-sourcing prototypes and on how crowd sourcing can generate co-production between citizens and public services.", "num_citations": "209\n", "authors": ["2106"]}
{"title": "Self-Driving Database Management Systems\n", "abstract": " In the last two decades, both researchers and vendors have built advisory tools to assist database administrators (DBAs) in various aspects of system tuning and physical design. Most of this previous work, however, is incomplete because they still require humans to make the final decisions about any changes to the database and are reactionary measures that fix problems after they occur. What is needed for a truly \u201cself-driving\u201d database management system (DBMS) is a new architecture that is designed for autonomous operation. This is different than earlier attempts because all aspects of the system are controlled by an integrated planning component that not only optimizes the system for the current workload, but also predicts future workload trends so that the system can prepare itself accordingly. With this, the DBMS can support all of the previous tuning techniques without requiring a human to determine the right way and proper time to deploy them. It also enables new optimizations that are important for modern high-performance DBMSs, but which are not possible today because the complexity of managing these systems has surpassed the abilities of human experts. This paper presents the architecture of Peloton, the first selfdriving DBMS. Peloton\u2019s autonomic capabilities are now possible due to algorithmic advancements in deep learning, as well as improvements in hardware and adaptive database architectures.", "num_citations": "206\n", "authors": ["2106"]}
{"title": "Scrambling query plans to cope with unexpected delays\n", "abstract": " Accessing data from numerous widely distributed sources poses significant new challenges for query optimization and execution. Congestion and failures in the network can introduce highly variable response times for wide area data access. The paper is an initial exploration of solutions to this variability. We introduce a class of dynamic, run time query plan modification techniques that we call query plan scrambling. We present an algorithm that modifies execution plans on-the-fly in response to unexpected delays in obtaining initial requested tuples from remote sources. The algorithm both reschedules operators and introduces new operators into the query plan. We present simulation results that demonstrate how the technique effectively hides delays by performing other useful work while waiting for missing data to arrive.", "num_citations": "144\n", "authors": ["2106"]}
{"title": "Leveraging mediator cost models with heterogeneous data sources\n", "abstract": " Distributed systems require declarative access to diverse information sources. One approach to solving this heterogeneous distributed database problem is based on mediator architectures. In these architectures, mediators accept queries from users, process them with respect to wrappers, and return answers. Wrappers provide access to underlying sources. To efficiently process queries, the mediator must optimize the plan used for processing the query. In classical databases, cost-estimate based query optimization is effective. In a heterogeneous distributed databases, cost-estimate based query optimization is difficult to achieve because the underlying data sources do not export cost information. This paper describes a new method that permits the wrapper programmer to export cost estimates. For the wrapper programmer to describe all cost estimates may be impossible due to lack of information or burdensome\u00a0\u2026", "num_citations": "101\n", "authors": ["2106"]}
{"title": "The distributed information search component (Disco) and the World Wide Web\n", "abstract": " The Distributed Information Search COmponent (DISCO) is a prototype heterogeneous distributed database that accesses underlying data sources. The DISCO prototype currently focuses on three central research problems in the context of these systems. First, since the capabilities of each data source is different, transforming queries into subqueries on data source is difficult. We call this problem the weak data source problem. Second, since each data source performs operations in a generally unique way, the cost for performing an operation may vary radically from one wrapper to another. We call this problem the radical cost problem. Finally, existing systems behave rudely when attempting to access an unavailable data source. We call this problem the ungraceful failure problem. DISCO copes with these problems. For the weak data source problem, the database implementor defines precisely the capabilities of\u00a0\u2026", "num_citations": "96\n", "authors": ["2106"]}
{"title": "Dynamic query operator scheduling for wide-area remote access\n", "abstract": " Distributed databases operating over wide-area networks such as the Internet, must deal with the unpredictable nature of the performance of communication. The response times of accessing remote sources can vary widely due to network congestion, link failure, and other problems. In such an unpredictable environment, the traditional iterator-based query execution model performs poorly. We have developed a class of methods, called query scrambling, for dealing explicitly with the problem of unpredictable response times. Query scrambling dynamically modifies query execution plans on-the-fly in reaction to unexpected delays in data access. In this paper we focus on the dynamic scheduling of query operators in the context of query scrambling. We explore various choices for dynamic scheduling and examine, through a detailed simulation, the effects of these choices. Our experimental environment\u00a0\u2026", "num_citations": "83\n", "authors": ["2106"]}
{"title": "Query processing and inverted indices in shared-nothing text document information retrieval systems\n", "abstract": " The performance of distributed text document retrieval systems is strongly influenced by the organization of the inverted text. This article compares the performance impact on query processing of various physical organizations for inverted lists. We present a new probabilistic model of the database and queries. Simulation experiments determine those variables that most strongly influence response time and throughput. This leads to a set of design trade-offs over a wide range of hardware configurations and new parallel query processing strategies.", "num_citations": "71\n", "authors": ["2106"]}
{"title": "Intent-based information processing and updates\n", "abstract": " In various embodiments, a method for processing a user request is provided. The method may include receiving input data from a user including at least natural language associated with a user request; analyzing the user input data with an intermediary agent; selecting at least one form based on analyzing the user input data; and, executing at least one update based on at least one form.", "num_citations": "70\n", "authors": ["2106"]}
{"title": "Mobile transit information from universal design and crowdsourcing\n", "abstract": " Extensive interviews with riders of the Pittsburgh, Pennsylvania, bus system revealed that, as the top priority, riders wanted to know the actual arrival time of buses. Following a universal design approach, a system called Tiramisu was created to foster a greater sense of community between riders and transit bus service providers. The design focused on acquisition of crowdsourced information for bus location and bus fullness. On the basis of that input, the system predicted the arrival time of buses and provided a convenient platform for reporting problems and positive experiences within the transit system. The intention was to create a community of riders that materially participated in the delivery of the transit service. Tiramisu also supported specific information and reporting needs for riders with disabilities and thereby provided greater independent mobility around the community. An early field trial of Tiramisu\u00a0\u2026", "num_citations": "64\n", "authors": ["2106"]}
{"title": "Caching and database scaling in distributed shared-nothing information retrieval systems\n", "abstract": " A common class of existing information retrieval system provides access to abstracts. For example Stanford University, through its FOLIO system, provides access to the INSPECT database of abstracts of the literature on physics, computer science, electrical engineering, etc. In this paper this database is studied by using a trace-driven simulation. We focus on physical index design, inverted index caching, and database scaling in a distributed shared-nothing system. All three issues are shown to have a strong effect on response time and throughput. Database scaling is explored in two ways. One way assumes an \u201coptimal\u201d configuration for a single host and then linearly scales the database by duplicating the host architecture as needed. The second way determines the optimal number of hosts given a fixed database size.", "num_citations": "64\n", "authors": ["2106"]}
{"title": "Scalable query result caching for web applications\n", "abstract": " The backend database system is often the performance bottleneck when running web applications. A common approach to scale the database component is query result caching, but it faces the challenge of maintaining a high cache hit rate while efficiently ensuring cache consistency as the database is updated. In this paper we introduce Ferdinand, the first proxy-based cooperative query result cache with fully distributed consistency management. To maintain a high cache hit rate, Ferdinand uses both a local query result cache on each proxy server and a distributed cache. Consistency management is implemented with a highly scalable publish/subscribe system. We implement a fully functioning Ferdinand prototype and evaluate its performance compared to several alternative query-caching approaches, showing that our high cache hit rate and consistency management are both critical for Ferdinand's\u00a0\u2026", "num_citations": "60\n", "authors": ["2106"]}
{"title": "View update translation via deduction and annotation\n", "abstract": " First steps are taken in examining the view update problem in deductive databases. The class of recursive definite deductive databases is examined. A view update is defined as a statement of factual logical consequence of the deductive database. A translation is a minimal update on the facts of a deductive database such that the view update holds. The number of translations for a view update is exponential in the size of the database. Algorithms for view updates are presented and proven correct. They are based on SLD-resolution and are independent of the computation rule. Finally, as an example of a method for reducing the number of possible translations of a view update, rule annotations are introduced. A small number of unique annotations (proportional to the size of the database) is shown to produce unique translations of view updates.", "num_citations": "60\n", "authors": ["2106"]}
{"title": "Scaling heterogeneous databases and the design of DISCO\n", "abstract": " Access to large numbers of data sources introduces new problems for users of heterogeneous distributed databases. End users and application programmers must deal with unavailable data sources. Database administrators must deal with incorporating each new data source into the system. Database implementors must deal with the transformation of queries between query languages and schemas. The Distributed Information Search COmponent ({\\sc Disco}) addresses these problems. Query processing semantics give meaning to queries that reference unavailable data sources. Data modeling techniques manage connections to data sources. The component interface to data sources flexibly handles different query languages and different interface functionalities. This paper describes in detail (a) the distributed mediator architecture of {\\sc Disco}, (b) its query processing semantics, (c) the data model and its modeling of data source connections, and (d) the interface to underlying data sources. We describe several advantages of our system and describe the internal architecture of our planned prototype.", "num_citations": "57\n", "authors": ["2106"]}
{"title": "Experiences in federated databases: From iro-db to miro-web\n", "abstract": " From beginning of 1994 to the end of 1996, the IRO-DB ESPRIT project has developed tools for accessing relational and object-oriented databases in an integrated way. The system is based on the ODMG standard as pivot model and language. It consists of three layers. The local layer provides for an ODMG interface to heterogeneous DBMSs, the communication layer implements object-oriented remote data access, and the interoperable layer supports design and querying of integrated views. This paper describes the architecture and main design choices of IRO-DB, and reviews them against the experiences gained with implementation and application. It concludes with analyzing the revisions and extensions needed for applying the developed technology to inter-and intranet federations, which are tackled in the follow-up ESPRIT project MIRO-Web.", "num_citations": "54\n", "authors": ["2106"]}
{"title": "Partial answers for unavailable data sources\n", "abstract": " Many heterogeneous database system products and prototypes exist today; they will soon be deployed in a wide variety of environments. Most existing systems suffer from an Achilles' heel: they ungracefully fail in presence of unavailable data sources. If some data sources are unavailable when accessed, these systems either silently ignore them or generate an error. This behavior is improper in environments where there is a non-negligible probability that data sources cannot be accessed (e.g., Internet). In case some data sources cannot be accessed when processing a query, the complete answer to this query cannot be computed; some work can however be done with the data sources that are available. In this paper, we propose a novel approach where, in presence of unavailable data sources, the answer to a query is a partial answer. A partial answer is a representation of the work that has been done\u00a0\u2026", "num_citations": "54\n", "authors": ["2106"]}
{"title": "Understanding the space for co-design in riders' interactions with a transit service\n", "abstract": " The recent advances in web 2.0 technologies and the rapid adoption of smart phones raises many opportunities for public services to improve their services by engaging their users (who are also owners of the service) in co-design: a dialog where users help design the services they use. To investigate this opportunity, we began a service design project investigating how to create repeated information exchanges between riders and a transit agency in order to create a virtual\" place\" from which the dialog on services could take place. Through interviews with riders, a workshop with a transit agency, and speed dating of design concepts, we have developed a design direction. Specifically, we propose a service that combines vehicle location and\" fullness\" ratings provided by riders with dynamic route change information from the transit agency as a foundation for a dialog around riders conveying input for continuous\u00a0\u2026", "num_citations": "52\n", "authors": ["2106"]}
{"title": "Data structures for efficient broker implementation\n", "abstract": " With the profusion of text databases on the Internet, it is becoming increasingly hard to find the most useful databases for a given query. To attack this problem, several existing and proposed systems employ brokers to direct user queries, using a local database of summary information about the available databases. This summary information must effectively distinguish relevant databases and must be compact while allowing efficient access. We offer evidence that one broker, GlOSS, can be effective at locating databases of interest even in a system of hundreds of databased and can examine the performance of accessing the GlOSS summeries for two promising storage methods: the grid file and partitioned hashing. We show that both methods can be tuned to provide good performance for a  particular workload (within a broad range of workloads), and we discuss the tradeoffs between the two data structures. As a\u00a0\u2026", "num_citations": "52\n", "authors": ["2106"]}
{"title": "Precision and recall of GlOSS estimators for database discovery\n", "abstract": " Online information vendors and the Internet together offer thousands of text databases from which a user may choose for a given information need. This paper presents a framework for and analyses a solution to this problem, which we call the text-database discovery problem. Our solution is to build a service that can suggest potentially good databases to search. A user's query goes through two steps: first, the query is presented to the GlOSS server (Glossary-Of-Servers Server) to select a set of promising databases to search. Secondly, the query is actually evaluated in the chosen databases. GlOSS gives a hint of what databases might be useful for the user's query, based on word-frequency information for each database. This information indicates how many documents in each database actually contain a keyword, for each field designator. To evaluate the set of databases that GlOSS returns for a given query, we\u00a0\u2026", "num_citations": "50\n", "authors": ["2106"]}
{"title": "Crowd-Sourcing of Information for Shared Transportation Vehicles\n", "abstract": " Systems, apparatuses, methods, and software for collecting and disseminating crowd-sourced information relating to one or more shared vehicles, such as buses, passenger trains, subway vehicles, streetcars, etc. The crowd-sourced information is collected via mobile client devices carried by users, such a riders of the shared vehicle at issue. Information collected includes tracing data for tracing the route and timing of each shared vehicles. The tracing data is used to update a computer model that helps predict arrival/departure times. The predicted arrival times can be conveyed to users and to allow people to arrange rendezvous events. Other information collected includes user-report information on items such as condition of the shared vehicle, fullness of the vehicle, and the user's experience with the vehicle and/or corresponding infrastructure. Collected user-report information can be shared with other users and\u00a0\u2026", "num_citations": "49\n", "authors": ["2106"]}
{"title": "Synthetic workload performance analysis of incremental updates\n", "abstract": " Declining disk and CPU costs have kindled a renewed interest in efficient document indexing techniques. In this paper, the problem of incremental updates of inverted lists is addressed using a dual-structure index data structure that dynamically separates long and short inverted lists and optimizes the retrieval, update, and storage of each type of list. The behavior of this index is studied with the use of a synthetically-generated document collection and a simulation model of the algorithm. The index structure is shown to support rapid insertion of documents, fast queries, and to scale well to large document collections and many disks.", "num_citations": "48\n", "authors": ["2106"]}
{"title": "Citizen motivation on the go: The role of psychological empowerment\n", "abstract": " Although advances in technology now enable people to communicate \u2018anytime, anyplace\u2019, it is not clear how citizens can be motivated to actually do so. This paper evaluates the impact of three principles of psychological empowerment, namely perceived self-efficacy, sense of community and causal importance, on public transport passengers\u2019 motivation to report issues and complaints while on the move. A week-long study with 65 participants revealed that self-efficacy and causal importance increased participation in short bursts and increased perceptions of service quality over longer periods. Finally, we discuss the implications of these findings for citizen participation projects and reflect on design opportunities for mobile technologies that motivate citizen participation.", "num_citations": "47\n", "authors": ["2106"]}
{"title": "Word sense disambiguation via human computation\n", "abstract": " One formidable problem in language technology is the word sense disambiguation (WSD) problem: disambiguating the true sense of a word as it occurs in a sentence (eg, recognizing whether the word\" bank\" refers to a river bank or to a financial institution). This paper explores a strategy for harnessing the linguistic abilities of human beings to develop datasets that can be used to train machine learning algorithms for WSD. To create such datasets, we introduce a new interactive system: a fun game designed to produce valuable output by engaging human players in what they perceive to be a cooperative task of guessing the same word as another player. Our system makes a valuable contribution by tackling the knowledge acquisition bottleneck in the WSD problem domain. Rather than using conventional and costly techniques of paying lexicographers to generate training data for machine learning algorithms, we\u00a0\u2026", "num_citations": "45\n", "authors": ["2106"]}
{"title": "Retrieval-based neural code generation\n", "abstract": " In models to generate program source code from natural language, representing this code in a tree structure has been a common approach. However, existing methods often fail to generate complex code correctly due to a lack of ability to memorize large and complex structures. We introduce ReCode, a method based on subtree retrieval that makes it possible to explicitly reference existing code examples within a neural code generation model. First, we retrieve sentences that are similar to input sentences using a dynamic-programming-based sentence similarity scoring method. Next, we extract n-grams of action sequences that build the associated abstract syntax tree. Finally, we increase the probability of actions that cause the retrieved n-gram action subtree to be in the predicted code. We show that our approach improves the performance on two code generation tasks by up to +2.6 BLEU.", "num_citations": "44\n", "authors": ["2106"]}
{"title": "Improving responsiveness for wide-area data access\n", "abstract": " In a wide-area environment, the time required to obtain data from remote sources can vary unpredictably due to network congestion, link failure or other problems. Traditional techniques for query optimization and query execution do not cope well with such unpredictability. The static nature of those techniques prevents them from adapting to remote access delays that arise at runtime. In this paper we describe two separate, but related techniques aimed at tackling this problem. The first technique, called Query Scrambling, hides relatively short, intermittent delays by dynamically adjusting query execution plans on-the-fly. The second technique addresses the longer-term unavailability of data sources by allowing the return of partial query answers when some of the data needed to fully answer a query are missing. 1 Introduction The continued dramatic growth in global interconnectivity via the Internet has made around-the-clock, on-demand access to widely-distributed data a common expect...", "num_citations": "42\n", "authors": ["2106"]}
{"title": "A data model and query processing techniques for scaling access to distributed heterogeneous databases in disco\n", "abstract": " CiNii \u8ad6\u6587 - A Data Model and Query Processing Techniques for Scaling Access to Distributed Heterogeneous Databases in Disco CiNii \u56fd\u7acb\u60c5\u5831\u5b66\u7814\u7a76\u6240 \u5b66\u8853\u60c5\u5831\u30ca\u30d3\u30b2\u30fc\u30bf[\u30b5\u30a4\u30cb\u30a3] \u65e5\u672c\u306e \u8ad6\u6587\u3092\u3055\u304c\u3059 \u5927\u5b66\u56f3\u66f8\u9928\u306e\u672c\u3092\u3055\u304c\u3059 \u65e5\u672c\u306e\u535a\u58eb\u8ad6\u6587\u3092\u3055\u304c\u3059 \u65b0\u898f\u767b\u9332 \u30ed\u30b0\u30a4\u30f3 English \u691c\u7d22 \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u9589\u3058\u308b \u30bf\u30a4\u30c8\u30eb \u8457\u8005\u540d \u8457\u8005ID \u8457\u8005\u6240\u5c5e \u520a\u884c\u7269\u540d ISSN \u5dfb\u53f7\u30da\u30fc\u30b8 \u51fa\u7248\u8005 \u53c2\u8003\u6587\u732e \u51fa\u7248\u5e74 \u5e74\u304b\u3089 \u5e74\u307e\u3067 \u691c\u7d22 \u691c\u7d22 \u691c\u7d22 [\u6a5f\u95a2\u8a8d\u8a3c] \u5229\u7528\u7d99\u7d9a\u624b\u7d9a\u304d\u306e\u3054\u6848\u5185 A Data Model and Query Processing Techniques for Scaling Access to Distributed Heterogeneous Databases in Disco TOMASIC A. \u88ab\u5f15\u7528\u6587\u732e: 1\u4ef6 \u8457\u8005 TOMASIC A. \u53ce\u9332\u520a\u884c\u7269 Invited paper in the IEEE Transactions on Computers, special issue on Distributed Computing Systems Invited paper in the IEEE Transactions on Computers, special issue on Distributed Computing Systems, 1997 \u88ab\u5f15\u7528\u6587\u732e: 1\u4ef6\u4e2d 1-1\u4ef6\u3092 \u8868\u793a 1 \u30de\u30eb\u30c1\u30c7\u30fc\u30bf\u30d9\u30fc\u30b9\u74b0\u5883\u306b\u304a\u3051\u308b\u6642\u9593\u7684\u2026", "num_citations": "41\n", "authors": ["2106"]}
{"title": "Planning adaptive mobile experiences when wireframing\n", "abstract": " Machine learning improves mobile user experience. Interestingly, envisioning apps with adaptive interfaces that reduce navigation and selection effort is not standard UX practice. When implementing an adaptive UI for our mobile transit app, we encountered a number of problems. Our original design did not log necessary information nor did it induce users to provide good labels. On reflection, we realized UX designers should identify and refine UI adaptions when sketching wireframes. To advance on this insight, we reviewed the interfaces of popular apps and extracted six design patterns where UI adaptation can improve in-app navigation. Next, we designed an exemplar set of wireframes, illustrating how UX designers might annotate their interaction flows to communicate planned adaptation and note the information (logs and labels) needed to make the desired inferences.", "num_citations": "40\n", "authors": ["2106"]}
{"title": "Equal time for data on the internet with websemantics\n", "abstract": " Many collections of scientific data in particular disciplines are available today around the world. Much of this data conforms to some agreed upon standard for data exchange, i.e., a standard schema and its semantics. However, sharing this data among a global community of users is still difficult because of a lack of standards for the following necessary functions: (i) data providers need a standard for describing or publishing available sources of data; (ii) data administrators need a standard for discovering the published data and (iii) users need a standard for accessing this discovered data. This paper describes a prototype implementation of a system, WebSemantics, that accomplishes the above tasks. We describe an architecture and protocols for the publication, discovery and access to scientific data. We define a language for discovering sources and querying the data in these sources, and we provide a\u00a0\u2026", "num_citations": "35\n", "authors": ["2106"]}
{"title": "NER systems that suit user\u2019s preferences: adjusting the recall-precision trade-off for entity extraction\n", "abstract": " We describe a method based on \u201ctweaking\u201d an existing learned sequential classifier to change the recall-precision tradeoff, guided by a user-provided performance criterion. This method is evaluated on the task of recognizing personal names in email and newswire text, and proves to be both simple and effective.", "num_citations": "34\n", "authors": ["2106"]}
{"title": "The efficacy of GlOSS for the text database discovery problem\n", "abstract": " The popularity of information retrieval has led users to a new problem: finding which text databases (out of thousands of candidate choices) are the most relevant to a user. Answering a given query with a list of relevant databases is the text database discovery problem. The first part of this paper presents a practical method for attacking this problem based on estimating the result size of a query and a database. The method is termed GlOSS-Glossary of Servers Server. The second part of this paper evaluates GlOSS using four different semantics to answer a user's queries. Real users' queries were used in the experiments. We also describe several variations of GlOSS and compare their effcacy. In addition, we analyze the storage cost of our approach to the problem.", "num_citations": "34\n", "authors": ["2106"]}
{"title": "An introduction to the e-xml data integration suite\n", "abstract": " This paper describes the e-XML component suite, a modular product for integrating heterogeneous data sources under an XML schema and querying in real-time the integrated information using XQuery, the emerging W3C standard for XML query. We describe the two main components of the suite, i.e., the repository for warehousing XML and the mediator for distributed query processing. We also discuss some typical applications.", "num_citations": "31\n", "authors": ["2106"]}
{"title": "Dealing with discrepancies in wrapper functionality\n", "abstract": " Much of the world's information is stored electronically in data sources. The data sources can be full-fledged databases, simple files, HTML pages or specialized data sources that possess diverse query processing capabilities. The common architecture to integrate such sources consists of {\\it mediators} that give a global view over the content of all sources, and {\\it wrappers} that give a local view of each source. Answering queries in this architecture is a difficult problem due to the wide range of capabilities of data sources. This paper presents a solution to this problem in the context of the \\Duery processor. We provide a tool to the wrapper implementor to describe the capabilities of the wrapper in fine detail. When a wrapper is registered with the mediator, the mediator uploads the capabilities of the wrapper, and smoothly integrates these capabilities into query processing. Our solution is novel both in the level of detail permitted by the tool and its easy incorporation into existing query optimization strategies. In this paper we describe: the query processing of \\Dthe language for specifying wrapper capabilities, the algorithms that integrates these capabilities into query processing, and an implementation of these techniques in the \\Drototype.", "num_citations": "31\n", "authors": ["2106"]}
{"title": "VIO: a mixed-initiative approach to learning and automating procedural update tasks\n", "abstract": " Today many workers spend too much of their time translating their co-workers' requests into structures that information systems can understand. This paper presents the novel interaction design and evaluation of VIO, an agent that helps workers trans late request. VIO monitors requests and makes suggestions to speed up the translation. VIO allows users to quickly correct agent errors. These corrections are used to improve agent performance as it learns to automate work. Our evaluations demonstrate that this type of agent can significantly reduce task completion time, freeing workers from mundane tasks.", "num_citations": "30\n", "authors": ["2106"]}
{"title": "Improving access to environmental data using context information\n", "abstract": " A very large number of data sources on environment, energy, and natural resources are available worldwide. Unfortunately, users usually face several problems when they want to search and use environmental information. In this paper, we analyze these problems. We describe a conceptual analysis of the four major tasks in the production of environmental data, from the technology point of view, and describe the organization of the data that results from these tasks. We then discuss the notion of metainformation and outline an architecture for environmental data systems that formally models metadata and addresses some of the major problems faced by users.", "num_citations": "28\n", "authors": ["2106"]}
{"title": "Apparatuses, systems, and methods to automate a procedural task\n", "abstract": " Methods, apparatuses, and systems to automate a procedural task. In one embodiment, computer-readable memory including computer readable instructions which, when executed by a processor, cause the processor to perform steps comprising: identifying a set of data, wherein the data includes a plurality of items; prompting the user to process at least one item of the data in a predetermined manner, wherein the user interacts with a predetermined form system to process the data; monitoring input of the user, wherein the input of the user causes the at least one item of the data to be processed in the predetermined manner; producing computer-generated, computer-readable instructions in response to monitoring the input of the user, wherein the computer-generated, computer-readable instructions cause the processor to process data in the predetermined manner; and executing the computer-generated\u00a0\u2026", "num_citations": "27\n", "authors": ["2106"]}
{"title": "Holistic query transformations for dynamic web applications\n", "abstract": " A promising approach to scaling Web applications is to distribute the server infrastructure on which they run. This approach, unfortunately, can introduce latency between the application and database servers, which in turn increases the network latency of Web interactions for the clients (end users). In this paper we introduce the concept of source-to-source holistic transformations - transformations that seek to optimize both the application code and the database requests made by it, to reduce client latency. As examples of our concept, we propose and evaluate two source-to-source holistic transformations that focus on hiding the latencies of database queries. We argue that opportunities for applying these transformations will continue to exist in Web applications. We then present algorithms for automating these transformations in a source-to-source compiler. Finally, we evaluate the effect of these two transformations\u00a0\u2026", "num_citations": "26\n", "authors": ["2106"]}
{"title": "Simultaneous scalability and security for data-intensive web applications\n", "abstract": " For Web applications in which the database component is the bottleneck, scalability can be provided by a third-party Database Scalability Service Provider (DSSP) that caches application data and supplies query answers on behalf of the application. Cost-effective DSSPs will need to cache data from many applications, inevitably raising concerns about security. However, if all data passing through a DSSP is encrypted to enhance security, then data updates trigger invalidation of large regions of cache. Consequently, achieving good scalability becomes virtually impossible. There is a tradeoff between security and scalability, which requires careful consideration. In this paper we study the security-scalability tradeoff, both formally and empirically. We begin by providing a method for statically identifying segments of the database that can be encrypted without impacting scalability. Experiments over a prototype DSSP\u00a0\u2026", "num_citations": "21\n", "authors": ["2106"]}
{"title": "Learning to Understand Web Site Update Requests.\n", "abstract": " In many organizations, users submit requests to update the organizational website via email to a human webmaster. In this paper, we propose an intelligent system that can process certain website update requests semi-automatically. In particular, we describe a system that can analyze requests to update the factual content of individual tuples in a databasebacked website, using a particular scheme for decomposing request-understanding into a sequence of entity recognition and text classification tasks. Each of these tasks can be solved using existing learning methods. Using a corpus generated by human-subject experiments, we experimentally evaluate the components of this system, as well as various combinations of these components. We also present experimental results on the robustness of the system. In particular, we present results predicting how the system will perform on request types not seen in training; how it will perform on user-specific language usage not seen in training; and how it will perform in the absence of features specific to the database schema of the website.", "num_citations": "21\n", "authors": ["2106"]}
{"title": "Intent-based information processing and updates in association with a service agent\n", "abstract": " In various embodiments, a method for processing a request made by a requester is provided. The method may include receiving at least one request from at least one requester, the request being in the form of natural language; analyzing the request with an agent; selecting at least one form based on analyzing the request; permitting a service agent to perform at least one step in association with the request; executing at least one update based on at least one form; and, forwarding information regarding processing of the request to a learning module operatively associated with the agent.", "num_citations": "20\n", "authors": ["2106"]}
{"title": "Design of a Data Management and Data Visualization System for Coastal Zone Management of the Mediterranean Sea\n", "abstract": " The THETIS system is viewed as a digital library of data repositories and visualization tools. In addition to its index/search capacity, the digital library also provides data querying, data combining, and data visualization capabilities. This paper presents an overview of the design of THETIS, a system that addresses the frequent requirement of scientists, engineers and decision-makers to access, process and subsequently visualize data collected and stored in different formats and held at different locations. The need exists for tools that enable the integration of these data, together with their associated data models, data interpretation techniques, and visualization requirements. The objective is to build an advanced integrated interoperable system for transparent access and visualization of such data repositories, via the Internet and the WWW. Vast amounts of information exist, collected and processed over many years at different research institutions. The data collections are stored in various databases, files, spreadsheets, or are generated by sophisticated data simulation models of physical and biological processes, and by data processing techniques. Data collections comprise numeric, audio, and video data, data models, images, and text. Data models are implemented in program code, which usually needs visualization tools to represent results.", "num_citations": "20\n", "authors": ["2106"]}
{"title": "Performance issues in distributed shared-nothing information-retrieval systems\n", "abstract": " Many information-retrieval systems provides access to abstracts. For example, Stanford University, through its FOLIO system, provides access to the INSPEC database of abstracts of the literature on physics, computer science, electrical engineering, etc. In this article, this database is studied by using a trace-driven simulation. It focuses on a physical-index design that accommodates truncations, inverted-index caching, and database scaling in a distributed shared-nothing system. All three issues are shown to have a strong effect on response time and throughput. Database scaling is explored in two ways. One way assumes an \u201coptimal\u201d configuration for a single host and then linearly scales the database by duplicating the host architecture as needed. The second way determines the optimal number of hosts given a fixed database size.", "num_citations": "20\n", "authors": ["2106"]}
{"title": "Invalidation clues for database scalability services\n", "abstract": " For their scalability needs, data-intensive Web applications can use a database scalability service (DBSS), which caches applications' query results and answers queries on their behalf. One way for applications to address their security/privacy concerns when using a DBSS is to encrypt all data that passes through the DBSS. Doing so, however, causes the DBSS to invalidate large regions of its cache when data updates occur. To invalidate more precisely, the DBSS needs help in order to know which results to invalidate; such help inevitably reveals some properties about the data. In this paper, we present invalidation clues, a general technique that enables applications to reveal little data to the DBSS, yet limit the number of unnecessary invalidations. Compared with previous approaches, invalidation clues provide applications significantly improved tradeoffs between security/privacy and scalability. Our\u00a0\u2026", "num_citations": "16\n", "authors": ["2106"]}
{"title": "Towards a next generation of open scientific data repositories and services\n", "abstract": " Scientific repositories found in institutions and organizations consist of data and programs. Data consists principally of numeric data, images, and text documents. Programs consist principally of software methods for visualizing and processing data and simulators of natural processes. Data represents both measured physical behavior and the results of simulations. The integration and visualization of scientific repositories into an easily accessed interoperable networked environment is needed in many disciplines for both scientific and management purposes. To satisfy these needs we present an open hybrid architecture, which combines digital library technology, information integration mechanisms and workflow-based systems. Our experience is based on the THETIS1 [15] project, a distributed collection of scientific repositories focused on supporting Coastal Zone Management of the Mediterranean Region in Europe. It will demonstrate its ability to respond to users such as scientists and public administration authorities that use scientific information for decision making.", "num_citations": "16\n", "authors": ["2106"]}
{"title": "Energy efficient and accuracy aware (e2a2) location services via crowdsourcing\n", "abstract": " Many mobile applications rely on location information gained from location services on mobile devices. However, continuously tracking the device location with high accuracy drains the battery quickly. Furthermore, sensing the same location can be redundant when multiple devices are co-located. In this paper, we develop a crowdsourcing-based location service, E2A2 (energy efficient and accuracy aware), which places colo-cated devices into groups, and uses group location to represent individual device location. The E2A2 location service aims to reduce individual device battery consumption associated with location services while simultaneously maintaining high location accuracy for each device. Our experimental results from a prototype system show the effectiveness of our proposed solution with different mobility patterns. We also present results on the impact of different system parameters and the number of\u00a0\u2026", "num_citations": "15\n", "authors": ["2106"]}
{"title": "Mixer: mixed-initiative data retrieval and integration by example\n", "abstract": " Office administrators are frequently asked to create ad hoc reports based on web accessible data. The web contains the desired data but does not allow efficient access in the way the administrator needs, prompting a tedious and labor-intensive task of retrieving and integrating the required data. Mixer is a programming-by-demonstration (PBD) tool empowering administrators to construct ad hoc reports from diverse web sources without tedious piecemeal labor. Mixer\u2019s design builds on the exploration into end user conceptualization of data retrieval tasks from our previous Wizard-of-Oz study [39], and incorporates insights from mixed-initiative researchers into collaboration between end users and software agents. This paper justifies the design decisions that drive Mixer, focusing on general lessons for designers of programming-by-demonstration systems targeting nonprogrammers. We evaluate Mixer by\u00a0\u2026", "num_citations": "14\n", "authors": ["2106"]}
{"title": "Parachute queries in the presence of unavailable data sources\n", "abstract": " Mediator systems are used today in a wide variety of unreliable environments. When processing a query, a mediator may try to access a data source which is unavailable. In this situation, existing systems either silently ignore unavailable data sources or generate an error. In either case, to obtain the complete answer, the query is reprocessed from scratch. This behavior is inefficient in environments with a non-negligible probability that a data source is unavailable (e.g., the Internet). In the case that some data sources are unavailable, the complete answer to a query cannot be obtained; however useful work can be done with the available data sources. In this paper, we describe a novel approach to mediator query processing where, in the presence of unavailable data sources, the answer to a query is a {\\em partial answer}. The partial answer represents the state of the mediator at the end of query processing, i.e., materialized data. This state is used to construct an {\\em incremental query}. The answer to the incremental query is the same as the complete answer, but it is more efficient to evaluate than the original query. In addition, information can be extracted from the mediator state through the use of secondary queries, called {\\em parachute queries}. We describe an intuitive class of parachute queries and an algorithm which generates it.We define two new architectures for partial answers, incremental and parachute queries and analytically model for these architectures the probability of obtaining the answer to query in the presence of unavailable data sources. The analysis shows that complete answers are more likely in our two architectures\u00a0\u2026", "num_citations": "14\n", "authors": ["2106"]}
{"title": "Distributed queries and incremental updates in information retrieval systems\n", "abstract": " With the proliferation of the world's\" information highways\" has renewed interest in efficient document indexing techniques. This thesis considers the architecture of information retrieval systems. Distributed queries are studied with analytical and trace-driven simulations. We focus on physical index design, inverted index caching, and database scaling in a distributed shared-nothing system. All three issues are shown to have a strong effect on response time and throughput. Incremental updates of inverted lists are studied using a new dual-structure index data structure. The index dynamically separates long and short inverted lists and optimizes the retrieval, update, and storage of each type of list. To study the behavior of the index, a space of engineering trade-offs which range from optimizing update time to optimizing query performance is described. We quantitatively explore this space by using actual data and\u00a0\u2026", "num_citations": "14\n", "authors": ["2106"]}
{"title": "Systems and methods for implementing a machine-learning agent to retrieve information in response to a message\n", "abstract": " Mixed-initiative message-augmenting agent systems and methods that provide users with tools that allow them to respond to messages, such as email messages, containing requests for information or otherwise requiring responses that require information that needs to be retrieved from one or more data sources. The systems and methods allow users to train machine-learning agents how to retrieve and present information in responses to like messages so that the machine-learning agents can eventually automatedly generate responses with minimal involvement by the users. Embodiments of the systems and methods allow users to build message-augmenting forms containing the desired information for responding to messages and to demonstrate to the machine-learning agents where to retrieve pertinent information for populating the forms. Embodiments of the systems and methods allow users to modify and\u00a0\u2026", "num_citations": "13\n", "authors": ["2106"]}
{"title": "Scheduling OLTP transactions via learned abort prediction\n", "abstract": " Current main memory database system architectures are still challenged by high contention workloads and this challenge will continue to grow as the number of cores in processors continues to increase [23]. These systems schedule transactions randomly across cores to maximize concurrency and to produce a uniform load across cores. Scheduling never considers potential conflicts. Performance could be improved if scheduling balanced between concurrency to maximize throughput and scheduling transactions linearly to avoid conflicts. In this paper, we present the design of several intelligent transaction scheduling algorithms that consider both potential transaction conflicts and concurrency. To incorporate reasoning about transaction conflicts, we develop a supervised machine learning model that estimates the probability of conflict. This model is incorporated into several scheduling algorithms. In addition, we\u00a0\u2026", "num_citations": "12\n", "authors": ["2106"]}
{"title": "User-created forms as an effective method of human-agent communication\n", "abstract": " A key challenge for mixed-initiative systems is to create a shared understanding of the task between human and agent. To address this challenge, we created a mixed-initiative interface called Mixer to aid administrators with automating tedious information-retrieval tasks. Users initiate communication with the agent by constructing a form, creating a structure to hold the information they require and to show context in order to interpret this information. They then populate the form with the desired results, demonstrating to the agent the steps required to retrieve the information. This method of form creation explicitly defines the shared understanding between human and agent. An evaluation of the interface shows that administrators can effectively create forms to communicate with the agent, that they are likely to accept this technology in their work environment, and that the agent's help can significantly reduce the time\u00a0\u2026", "num_citations": "12\n", "authors": ["2106"]}
{"title": "Locating and accessing data repositories with WebSemantics\n", "abstract": " Many collections of scientific data in particular disciplines are available today on the World Wide Web. Most of these data sources are compliant with some standard for interoperable access. In addition, sources may support a common semantics, i.e., a shared meaning for the data types and their domains. However, sharing data among a global community of users is still difficult because of the following reasons: (i) data providers need a mechanism for describing and publishing available sources of data; (ii) data administrators need a mechanism for discovering the location of published sources and obtaining metadata from these sources; and (iii) users need a mechanism for browsing and selecting sources. This paper describes a system, WebSemantics, that accomplishes the above tasks. We describe an architecture for the publication and discovery of scientific data sources, which is an extension of the\u00a0\u2026", "num_citations": "12\n", "authors": ["2106"]}
{"title": "A framework for classifying scientific metadata\n", "abstract": " The scientific community, public organizations and administrations have generated a large amount of data concerning the environment. There is a need to allow sharing and exchange of this type of information by various kinds of users including scientists, decision-makers and public authorities. Metadata arises as the solution to support these requirements. We present a formal framework for classification of metadata that will give a uniform definition of what metadata is, how it can be used and where it must be used. This framework also provides a procedure for classifying elements of existing metadata standards.", "num_citations": "12\n", "authors": ["2106"]}
{"title": "Performance of OLTP via intelligent scheduling\n", "abstract": " Current architectures for main-memory online transaction processing (OLTP) database management systems (DBMS) typically use random scheduling to assign transactions to threads. This approach achieves uniform load across threads but it ignores the likelihood of conflicts between transactions. If the DBMS could estimate the potential for transaction conflict and then intelligently schedule transactions to avoid conflicts, then the system could improve its performance. Such estimation of transaction conflict, however, is non-trivial for several reasons. First, conflicts occur under complex conditions that are far removed in time from the scheduling decision. Second, transactions must be represented in a compact and efficient manner to allow for fast conflict detection. Third, given some evidence of potential conflict, the DBMS must schedule transactions in such a way that minimizes this conflict. In this paper, we\u00a0\u2026", "num_citations": "10\n", "authors": ["2106"]}
{"title": "SmartWrap: seeing datasets with the crowd's eyes\n", "abstract": " The web contains many datasets presented visually, whose lack of semantic markup renders them difficult to understand and navigate using a screen reader. In this work, we explore the possibility of understanding the semantics of web datasets by asking sighted web users to manually scrape web pages into spreadsheets. Web users constitute a huge population of potential workers, but most are not programmers and may have difficulty understanding and communicating the abstractions involved in labeling web datasets. We present the design of a tool we call SmartWrap that directs the manual scraping work of everyday end users, explicitly including nonprogrammers, towards the construction of reusable programs, called wrappers, that map the scraped website into a structured dataset. To engage with nontechnical end users, we designed the tool to use very simple interactions. We present a user study\u00a0\u2026", "num_citations": "10\n", "authors": ["2106"]}
{"title": "Energy Efficient and Accuracy Aware Mobile Services\n", "abstract": " Systems and methods that can reduce energy resources required by sensors in mobile devices and thereby mitigate premature battery depletion are disclosed. Mobile devices can be assigned groups by a sensor usage management (SUM) server based on the proximity of the mobile devices to one another. Once groups have been established, one or more of the mobile devices within each group can share sensor readings with other mobile devices in its group via the SUM server, such that each mobile device in each group can reduce the overall number of sensor readings that particular device must take. This co-locational \u201csharing\u201d of representative sensor data among members with groups of co-located mobile devices can reduce overall energy consumption and/or, for a given amount of energy consumption, increase accuracy compared to a situation in which each mobile device in each group exclusively relies\u00a0\u2026", "num_citations": "10\n", "authors": ["2106"]}
{"title": "Scalable consistency management for web database caches\n", "abstract": " We have built a prototype of a scalable dynamic web-content delivery system, which we call S3. Initial experiments with S3 led us to conclude that the key to achieving scalability lay in reducing the workload on back-end databases. S3 utilizes proxy servers to generate dynamic content and cache the results of queries forwarded to the back-end database. This approach introduces the challenge of maintaining cache consistency when the database is updated. In this paper we introduce a fully-distributed consistency management infrastructure that uses a scalable publish/subscribe substrate to propagate update notifications. We use static analysis of the database workload to introduce several design alternatives for how to map database requests to publish/subscribe groups. Finally, we develop a simulation framework and use both simulation and our S3 prototype implementation to evaluate these alternatives empirically to determine which design is best for typical dynamic web workloads.", "num_citations": "10\n", "authors": ["2106"]}
{"title": "Linking messages and form requests\n", "abstract": " Large organizations with sophisticated infrastructures have large form-based systems that manage the interaction between the user community and the infrastructure. In many cases, when a user needs to complete a form to accomplish a task, the user e-mails a description of the task to the appropriate form expert. In many cases this description is incomplete and the expert engages in a clarification dialog to determine the details of the task. Since many tasks and descriptions are routine, this e-mail dialog can be replaced with an intelligent user interface. The interface proactively reads e-mail (or IM) messages and assists the user in completing the associated task without involving the expert. To ground our vision in a specific application, we have built an agent that functions as a webmaster assistant. For example, a user emails the request:\" Change John Doe's home phone number to 800-555-1212\" to the agent. The\u00a0\u2026", "num_citations": "10\n", "authors": ["2106"]}
{"title": "The performance of a crowdsourced transportation information system\n", "abstract": " Crowdsourced mobile sensing systems provide a counterpoint to the idea of fully automated sensing systems by transferring some or all of the sensing duties to the end users. Humans can easily sense in some ways that are impossible for machines to sense, leading to hybrid crowdsourced-automated systems. However, this transfer of sensing to humans comes with design trade-offs in terms of the sparsity of the sensed information and human entry errors. To better understand these design trade-offs, the authors developed a real-time arrival information system for a local transit agency that crowdsources the location of transit vehicles by having riders share location traces from their smart phones. The authors then deployed the system and measured the public\u2019s use of it for 10 months, gathering data on 296,283 interactive sessions. Analysis shows that relying on users can produce very sparse information for the whole system but much better information relative to the times and places that users access the service. Reflecting on this deployment, the authors conclude that crowdsourced-sensing systems perform well when (i) the value of an observation persists over time,(ii) many people make observations, and (iii) observations would be difficult or expensive to sense with an automated sensing system.", "num_citations": "9\n", "authors": ["2106"]}
{"title": "Learning Information Intent via Observation\n", "abstract": " Users in an organization frequently request help by sending request messages to assistants that express information intent: an intention to update data in an information system. Human assistants spend a significant amount of time and effort processing these requests. For example, human resource assistants process requests to update personnel records, and executive assistants process requests to schedule conference rooms or to make travel reservations. To process the intent of a request, assistants read the request and then locate, complete, and submit a form that corresponds to the expressed intent. Automatically or semi-automatically processing the intent expressed in a request on behalf of an assistant would ease the mundane and repetitive nature of this kind of work. For a well-understood domain, a straightforward application of natural language processing techniques can be used to build an intelligent\u00a0\u2026", "num_citations": "9\n", "authors": ["2106"]}
{"title": "Symmetric publish/subscribe via constraint publication\n", "abstract": " Current publish subscribe systems offer a range of expressive subscription languages for constraints. However, classical systems restrict the publish operation to be a single published object that contains only constants and no constraints. We introduce symmetric publish subscribe, a novel generalization of publish subscribe where both publications and subscriptions contain constraints in addition to constants. Published objects are matched to subscriptions by computing the intersection of their constraints. This generalization improves the performance of classical publish subscribe systems and introduces a new class of applications for publish subscribe. This paper describes the core algorithms of our publish subscribe implementation, evaluates the performance of these algorithms both analytically and empirically, and documents cases where the additional expressive power of symmetric publish subscribe can be gained with minimal additional computational cost compared to the classical system.Descriptors:", "num_citations": "9\n", "authors": ["2106"]}
{"title": "Contingent responsivity in E-books modeled from quality adult-child interactions: Effects on children\u2019s learning and attention.\n", "abstract": " Experiences of contingent responsivity during shared book reading predict better learning outcomes. However, it is unclear whether contingent responsivity from a digital book could provide similar support for children. The effects on story recall and engagement interacting with a digital book that responded contingently on children\u2019s vocalizations (contingent book) were investigated, with a focus on the role of individual differences in attention. The study used a within-subject design with 3 experiments from 90 3-to 5-year-old children. Children were presented with a contingent book and 3 noncontingent control conditions: a board book (Experiment 1), a static digital book (Experiment 2), and an animated book (Experiment 3). The use of the contingent book significantly increased children\u2019s story recall and was found to be especially useful for children with less developed attention regulation.", "num_citations": "8\n", "authors": ["2106"]}
{"title": "Co-producing value through public transit information services\n", "abstract": " Public transit riders are often unable to obtain timely information about transit service. At the same time, the complexity and scale of public transit systems makes it difficult for service providers to capture and manage rider concerns. To this end, we designed a system to foster a greater sense of community between riders and transit service providers. This system, named Tiramisu (www. tiramisutransit. com), focuses on crowdsourcing information about bus location and bus load, predicting the arrival time of buses, and providing a convenient platform for reporting problems and positive experiences within the transit system. Tiramisu is also designed to support the needs of riders with disabilities through the application of universal design principles. This system allows co-production of value from all users, including people who frequently encounter accessibility barriers with social computing and interaction techniques\u00a0\u2026", "num_citations": "8\n", "authors": ["2106"]}
{"title": "Efficient processing of mapped boolean queries via generative indexing\n", "abstract": " A computer assisted method of searching at least one corpus of information based on at least one query. The method includes creating a generative index based on the corpus and a mapping of terms of the query to terms of the corpus. The method also includes searching the generative index and the corpus with the query to create a result comprising a portion of the corpus, wherein the result satisfies the query.", "num_citations": "8\n", "authors": ["2106"]}
{"title": "Issues in parallel information retrieval\n", "abstract": " The proliferation of the world's\\information highways\" has renewed interest in e cient document indexing techniques. In this article, we provide an overview of the issues in parallel information retrieval. Basic issues in information retrieval are described and various parallel processing approaches to the problem are discussed. To illustrate the issues involved, we discuss an example of physical index design issues for inverted indexes, a common form of document index. Advantages and disadvantages for query processing are discussed. Finally, to provide an overview of design issues for distributed architectures, we discuss the parameters involved in the design of a system and rank them in terms of their in uence on query response time.", "num_citations": "8\n", "authors": ["2106"]}
{"title": "Scheduling OLTP transactions via machine learning\n", "abstract": " Current main memory database system architectures are still challenged by high contention workloads and this challenge will continue to grow as the number of cores in processors continues to increase. These systems schedule transactions randomly across cores to maximize concurrency and to produce a uniform load across cores. Scheduling never considers potential conflicts. Performance could be improved if scheduling balanced between concurrency to maximize throughput and scheduling transactions linearly to avoid conflicts. In this paper, we present the design of several intelligent transaction scheduling algorithms that consider both potential transaction conflicts and concurrency. To incorporate reasoning about transaction conflicts, we develop a supervised machine learning model that estimates the probability of conflict. This model is incorporated into several scheduling algorithms. In addition, we integrate an unsupervised machine learning algorithm into an intelligent scheduling algorithm. We then empirically measure the performance impact of different scheduling algorithms on OLTP and social networking workloads. Our results show that, with appropriate settings, intelligent scheduling can increase throughput by 54% and reduce abort rate by 80% on a 20-core machine, relative to random scheduling. In summary, the paper provides preliminary evidence that intelligent scheduling significantly improves DBMS performance.", "num_citations": "7\n", "authors": ["2106"]}
{"title": "Learning to navigate web forms\n", "abstract": " Given a particular update request to a WWW system, users are faced with the navigation problem of finding the correct form to accomplish the update request. In a large system, such as SAP with about 10,000 relations for the standard installation, users are faced with a sea of thousands of forms to navigate. For familiar tasks, users have various aids, such as personal tool bars, but for more complex tasks, users are forced to search or navigate for the correct form, or forward the update request to a specialist with the expertise to handle the request. In this later case, the execution of the request may be delayed since the specialist may be unavailable, or have other priorities. Also, typically the user and specialist engaged in a time consuming clarification dialog to extract additional information required to complete the request. In this paper we study the problem of building an assistant for the navigation problem for web forms. This assistant can be deployed either directly to a user, or to specialist that receives a stream of requests from users. In the former case the assistant helps the user navigate to the right form. In the latter case, the assistant cuts ambiguous communication between the user and specialist. We present experimental results from behavioral experiments and machine learning that demonstrate the usefulness of our assistant.", "num_citations": "7\n", "authors": ["2106"]}
{"title": "Combining personalized agents to improve content-based recommendations\n", "abstract": " Ratings-based recommender systems typically predict user preferences for items based on the user's preference history, information about items, and the preferences of similar users. In content-based recommending, the similarities between items the user has previously expressed interest in form the basis for recommending new items. There are a number of practical reasons why users may not rate all of the items they have experience with, a fact that indicates ratings are not missing at random. We introduce a missing data model that takes this observation into account. By combining the personalized content models with missing data models, we build classifier agents for each user using the predicted ratings of the first two models. These stacked agents use collaborative filtering to construct a hybrid recommender system that improves upon the baseline scores produced by the content-based recommender on a popular movie ratings data set.", "num_citations": "6\n", "authors": ["2106"]}
{"title": "User Constructed Data Integration via Mixed-Initiative Design.\n", "abstract": " Administrators frequently perform data integration \u201cby hand\u201d on the desktop as part of the execution of administrative tasks. This position paper discusses the application of mixed-initiative design to this problem. This design style leverages the interaction between a user and an intelligent assistant, minimizing the effort required to execute a task.", "num_citations": "6\n", "authors": ["2106"]}
{"title": "Validating mediator cost models with DISCO\n", "abstract": " Disco is a mediator system developed at INRIA for accessing heterogeneous data sources over the Internet. In Disco, mediators accept queries from users, process them with respect to wrappers, and return answers. Wrapper provide access to underlying sources. To e ciently process queries, the mediator performs cost-based query optimization. In a heterogeneous distributed database, cost-estimate based query optimization is di cult to achieve because the underlying data sources do not export cost information. Disco's approach relies on combining a generic cost model with speci c cost information exported by wrappers. In this paper, we propose a validation of Disco's cost model based on experimentation with real Web data sources. This validation shows the e ciency of our generic cost model as well as the e ciency of more specialized cost functions.", "num_citations": "5\n", "authors": ["2106"]}
{"title": "A framework for classifying environmental metadata\n", "abstract": " The scientific community, public organizations and administrations have generated a large amount of data concerning the environment. There is a need to allow sharing and exchange of this type of information by various kinds of users including scientists, decision-makers and public authorities. Metadata arises as the solution to support these requirements. We present a formal framework for classification of metadata that will give a uniform definition of what metadata is, how it can be used and where it must be used. This framework also provides a procedure for classifying elements of existing metadata standards.", "num_citations": "5\n", "authors": ["2106"]}
{"title": "Leveraging mediator cost models with heterogeneous data sources\n", "abstract": " Distributed systems require declarative access to diverse data sources of information. One approach to solving this heterogeneous distributed database problem is based on mediator architectures. In these architectures, mediators accept queries from users, process them with respect to wrappers, and return answers. Wrapper provide access to underlying data sources. To efficiently process queries, the mediator must optimize the plan used for processing the query. In classical databases, cost-estimate based query optimization is an effective method for optimization. In a heterogeneous distributed databases, cost-estimate based query optimization is difficult to achieve because the underlying data sources do not export cost information. This paper describes a new method that permits the wrapper programmer to export cost estimates (cost estimate formulas and statistics). For the wrapper programmer to describe all cost estimates may be impossible due to lack of information or burdensome due to the amount of information. We ease this responsibility of the wrapper programmer by leveraging the generic cost model of the mediator with specific cost estimates from the wrappers. This paper describes the mediator architecture, the language for specifying cost estimates, the algorithm for the blending of cost estimates during query optimization, and experimental results based on a combination of analytical formulas and real measurements of an object database system.", "num_citations": "5\n", "authors": ["2106"]}
{"title": "Holistic application analysis for update-independence\n", "abstract": " Current database performance optimizations stop at the border between the database application and the database system, focusing either on improving the performance of just the database system or the application's execution in isolation of the other. We argue that typical database application design enables a more holistic analysis that maintains the relationship between the database and application data. We describe techniques to maintain this relationship and introduce several optimizations to improve the efficiency of Web application execution in a distributed environment. We show that our holistic analysis outperforms traditional non- holistic methods both statically and when used as part of a dynamic, distributed environment for executing Web applications using database caches.", "num_citations": "4\n", "authors": ["2106"]}
{"title": "Learning to extract gene-protein names from weaklylabeled text in preparation\n", "abstract": " Training a named entity recognizer (NER) has always been a difficult task due to the effort required to generate a significant amount of annotated training data. In this paper, we reduce or eliminate the effort required to create training data by automatically converting other sources of data into annotated training data. The performance of this approach is tested on a geneprotein name extractor by using the mouse and fly data obtained from the BioCreAtIvE challenge. Results show that our methods are effective and that our trained NER system outperforms all of our baseline results. 1", "num_citations": "4\n", "authors": ["2106"]}
{"title": "Find and Seek: Assessing the Impact of Table Navigation on Information Look-up with a Screen Reader\n", "abstract": " Web designers use visual cues such as layout and typography to make pages easier to navigate and understand. Yet, screen readers generally ignore these features and present page information in a linear audio stream. We investigate whether transcoding the visual semantics of grid-based layouts to tables supports better navigation. In a controlled experiment, participants navigated re-written pages significantly faster when doing data synthesis tasks and more accurately when looking up information meeting multiple criteria. Participants rated their table navigation experience better in terms of effort, memorization, ease of navigation, understanding of page information, and confidence in submitted answers. Participants attributed these gains to the table structure\u2019s support for (1) predictable audio presentation, (2) adopting an appropriate search strategy, and (3) making sense of page content. Contrary to the\u00a0\u2026", "num_citations": "3\n", "authors": ["2106"]}
{"title": "The utility of tables for screen reader users\n", "abstract": " The mere existence of digital documents on the web makes those documents more accessible to blind people than if they were not digitized. The web provides several opportunities, however, for web authors to describe what content looks like, rather than what it means, and this visual description inconveniences and confuses people using screen readers. We focus on the particular instance of this problem where information that is logically tabular is presented using repeated visual templates. We recruited a group of experienced screen reader users and observed them performing a series of tasks designed to elicit tabular behavior. We find that people who make use of tabular operations perform the tasks faster and more successfully than those who do not. We furthermore find that while many users discover the tables we introduced as part of the study, many others do not or do not choose to make use of the tables\u00a0\u2026", "num_citations": "3\n", "authors": ["2106"]}
{"title": "GlOSS: Text-Source Discovery over the Internet\n", "abstract": " CiteSeerX \u2014 GlOSS: Text-Source Discovery over the Internet Documents Authors Tables Log in Sign up MetaCart DMCA Donate CiteSeerX logo Documents: Advanced Search Include Citations Authors: Advanced Search Include Citations Tables: DMCA GlOSS: Text-Source Discovery over the Internet (1998) Cached Download as a PDF Download Links [www1.cs.columbia.edu] [www.cs.columbia.edu] [www.cs.virginia.edu] [wortschatz.uni-leipzig.de] [wortschatz.uni-leipzig.de] [www.cs.columbia.edu] Other Repositories/Bibliography DBLP Save to List Add to Collection Correct Errors Monitor Changes by Luis Gravano , Hector Garcia-Molina , Anthony Tomasic Citations: 214 - 17 self Summary Citations Active Bibliography Co-citation Clustered Documents Version History Share Facebook Twitter Reddit Bibsonomy OpenURL Abstract Keyphrases text-source discovery Powered by: Apache Solr About CiteSeerX Submit \u2026", "num_citations": "3\n", "authors": ["2106"]}
{"title": "Correct View Update Translations via Containment\n", "abstract": " One approach to the view update problem for deductive databases proves properties of translations that is, a language specifies the meaning of an update to the intensional database (IDB) in terms of updates to the extensional database We argue that the view update problem should be viewed as a question of the expressive power of the translation language and the computational cost of demonstrating properties of a translation. We use an active rule based database language as a means of specifying translations of updates on the IDB into updates on the EDB. This paper uses the containment of one datalog program (or conjunctive query) by another to demonstrate that a translation is semantically correct. We show that the complexity of correctness is lower for insertion than deletion. Finally, we discuss extension to the translation language.", "num_citations": "3\n", "authors": ["2106"]}
{"title": "Determining correct view update translations via query containment\n", "abstract": " Given an intensional database (IDB) and an extension database the view update problem translates updates on the IDB into updates on the EDB. One approach to the view update problem uses a translation langauge to specify the meaning of a view update. In this paper we prove properties of a translation language. This approach to the view update problem studies the expressive power of the translation language and the computational cost of demonstrating properties of a translation. We use an active rule based database language for specifying translations of view updates. This paper uses the containment of one datalog program (or conjunctive query) by another to demonstrate that a translation is semantically correct. We show that the complexity of correctness is lower for insertion than deletion. Finally, we discuss extensions to the translation language.", "num_citations": "3\n", "authors": ["2106"]}
{"title": "A Long-Term Evaluation of Adaptive Interface Design for Mobile Transit Information\n", "abstract": " Personalization of user experience has a long history of success in the HCI community. More recently the community has focused on adaptive user interfaces, supported by machine learning, that reduce interaction efforts and improves user experience by collapsing transactions and pre-filtering results. However, generally, these more recent results have only been demonstrated in the laboratory environment. In this paper, we share the case of a deployed mobile transit app that adapts based on users\u2019 previous usage. We examine the impact of adaptation, both good and bad, and user abandonment rates. We conducted an 18-month assessment where 2,616 participants (with and without vision impairments) were recruited and participated in an A/B study. Finally, we draw some insights on some unusual effects that appear over the long term.", "num_citations": "2\n", "authors": ["2106"]}
{"title": "Supervised Contextual Embeddings for Transfer Learning in Natural Language Processing Tasks\n", "abstract": " Pre-trained word embeddings are the primary method for transfer learning in several Natural Language Processing (NLP) tasks. Recent works have focused on using unsupervised techniques such as language modeling to obtain these embeddings. In contrast, this work focuses on extracting representations from multiple pre-trained supervised models, which enriches word embeddings with task and domain specific knowledge. Experiments performed in cross-task, cross-domain and cross-lingual settings indicate that such supervised embeddings are helpful, especially in the low-resource setting, but the extent of gains is dependent on the nature of the task and domain. We make our code publicly available.", "num_citations": "2\n", "authors": ["2106"]}
{"title": "Combining contribution interactions to increase coverage in mobile participatory sensing systems\n", "abstract": " Participatory sensing systems use people and their smartphones as a sensing infrastructure, and getting people to make contributions remains a critical challenge. Little work details how system designers should combine different interactions to increase coverage of service location. Tiramisu, a participatory sensing system, invites transit riders to crowdsource real-time arrival information by sharing location traces when they commute. We extended this system with a new feature that allows riders at stops to\" spot\" buses passing by. To better understand the impact of this new feature, we conducted an observational log analysis, examining changes in coverage and user behavior before and after the new feature. Following the addition of the spotting feature, participants' contributions increased coverage (the number of trips with real-time data) by 98%, and they used the app more than twice as much. The addition of\u00a0\u2026", "num_citations": "2\n", "authors": ["2106"]}
{"title": "EnTable: Rewriting web data sets as accessible tables\n", "abstract": " Today, many data-driven web pages present information in a way that is difficult for blind and low vision users to navigate and to understand. EnTable addresses this challenge. It re-writes confusing and complicated template-based data sets as accessible tables. EnTable allows blind and low vision users to submit requests for pages they wish to access. The system then employs sighted informants to markup the desired page with semantic information, allowing the page to be re-written using straightforward tags. Screen reader users who browse the web using the EnTable browser extension can report data sets that are confusing, and utilize data sets re-written with the tag based on their own requests or on the requests of other users.", "num_citations": "2\n", "authors": ["2106"]}
{"title": "Learning to extract gene-protein names from weakly-labeled text\n", "abstract": " Training a named entity recognizer NER has always been a difficult task due to the effort required to generate a significant amount of annotated training data. In this paper, we reduce or eliminate the effort required to create training data by automatically converting other sources of data into annotated training data. The performance of this approach is tested on a gene-protein name extractor by using the mouse and fly data obtained from the BioCreAtIvE challenge. Results show that our methods are effective and that our trained NER system outperforms all of our baseline results.Descriptors:", "num_citations": "2\n", "authors": ["2106"]}
{"title": "Query Scrambling for Bursty Data Arrival.\n", "abstract": " Distributed databases operating over wide-area networks, such as the Internet, must deal with the unpredictable nature of the performance of communication.  The response times of accessing remote sources may vary widely due to network congestion, link failure, and other problems.  In this paper we examine a new class of methods, called query scrambling, for dealing with unpredictable response times. Query scrambling dynamically modifies query execution plans on-the-fly in reaction to unexpected delays in data access. We explore various choices in the implementation of these methods and examine, through a detailed simulation, the effects of these choices.  Our experimental environment considers pipelined and non-pipelined join processing in a client with multiple remote data sources and it focuses on bursty arrivals of data. We identify and study a number of the basic trade-offs that arise when designing scrambling policies for the bursty environment.  Our performance results show that query scrambling is effective in hiding the impact of delays on query response time for a number of different delay scenarios. (Also cross-referenced as UMIACS-TR-96-84)", "num_citations": "2\n", "authors": ["2106"]}
{"title": "Unavailable Data Sources in Mediator Based Applications\n", "abstract": " We discuss the problem of unavailable data sources in the context of two mediator based applications. We discuss the limitations of existing system with respect to this problem and describe a novel evaluation model that overcomes these shortcomings.", "num_citations": "2\n", "authors": ["2106"]}
{"title": "Scrambling query plans to cope with unexpected delays\n", "abstract": " Accessing numerous widely-distributed data sources poses significant new challenges for query optimization and execution. Congestion or failure in the network introduce highly-variable response times for wide-area data access. This paper is an initial exploration of solutions to this variability. We investigate a class of dynamic, run-time query plan modification techniques that we call query plan scrambling. We present an algorithm which modifies execution plans on-the-fly in response to unexpected delays in data access. The algorithm both reschedules operators and introduces new operators into the plan. We present simulation results that show how our technique effectively hides delays in receiving the initial requested tuples from remote data sources. 1 Introduction Ongoing improvements in networking technology and infrastructure have resulted in a dramatic increase in the demand for accessing and collating data from disparate, remote data sources over wide-area networks su...", "num_citations": "2\n", "authors": ["2106"]}
{"title": "Scaling Heterogeneous Databases and the Design of D\n", "abstract": " Access to large numbers of data sources introduces new problems for users of heterogeneous distributed databases. End users and application programmers must deal with unavailable data sources. Database administrators must deal with incorporating each new data source into the model. Database implementors must deal with the translation of queries between query languages and schemas. The Distributed Information Search COmponent (Disco) addresses these problems. Query processing semantics are developed to process queries over data sources which do not return answers. Data modeling techniques manage connections to data sources. The component interface to data sources exibly handles di erent query languages and translates queries. This paper describes in detail (a) the distributed mediator architecture of Disco,(b) its query processing semantics,(c) the data model and its modeling of data source connections, and (d) the interface to underlying data sources. We describe several advantages of our system and describe the internal architecture of our planned prototype.", "num_citations": "2\n", "authors": ["2106"]}
{"title": "Real-Time Detection of Crowded Buses via Mobile Phones\n", "abstract": " Automated passenger counting (APC) technology is central to many aspects of the public transit experience. APC information informs public transit planners about utilization in a public transit system and operations about dynamic fluctuations in demand. Perhaps most importantly, APC information provides one metric to the rider experience - standing during a long ride because of a crowded vehicle is an unpleasant experience. Several technologies have been successfully used for APC including light beam sensing and video image analysis. However, these technologies are expensive and must be installed in buses. In this paper, we analyze a new source of data using statistical models: rider smartphone accelerometers. Smartphones are ubiquitous in society and accelerometers have been shown to accurately model user states such as walking and sitting. We extend these models to use accelerometers to detect if the rider is standing or sitting on a bus. Standing riders are a signal that the bus is crowded. This paper provides evidence that user smartphones are a valid source of participatory sensing and thus a new source of automated passenger counting data.", "num_citations": "1\n", "authors": ["2106"]}
{"title": "Universal Design and Adaptive Interfaces as a Strategy for Induced Disabilities\n", "abstract": " There is great promise in creating effective technology experiences during situationally-induced impairments and disabilities through the combination of universal design and adaptive interfaces. We believe this combination is a powerful approach for meeting the UX needs of people with disabilities, including those which are temporary in nature. Research in each of these areas, and the combination, illustrates this promise.", "num_citations": "1\n", "authors": ["2106"]}
{"title": "Tiramisu: Information from Live Data Streams.\n", "abstract": " The primary source of information for rider safety with respect to dynamic events such as cancelled buses, detours, traffic conditions and other factors is the transit system website. Although technological enhancements, such as real-time tracking, rider alert RSS feeds, and Twitter feeds, are available for transit users, such information sources do not always report updates reliably. For example, some real-time tracking systems stop transmitting updates about a bus if the bus takes a detour from its scheduled route. Rider alerts might temporarily suspend updates due to holidays or employee PTO. Notifications through social media frequently offer information of relevance, but are often difficult to understand, or for one to comprehend the impact of the message on their trip. Paradoxically, users are still faced with a lot of effort at navigating the appropriate information sources, finding and understanding messages and updates of relevance to their trip. Our project goal is to access transit service live update data feeds, identify the routes and stops on which their updates will have an impact, and provide an integrated display of that information in the user\u2019s Tiramisu smart phone app. The extracted information can be used in multiple ways to improve the interactive experience of the user and keep them better informed. For example, if bus stops will be discontinued between certain hours of a day for, say, water main repair, and other stops will be established during that period, a search of nearby stops will reflect such temporary changes. This, combined with vehicle fullness data from the Tiramisu system, will allow riders to identify alternate transit trip options\u00a0\u2026", "num_citations": "1\n", "authors": ["2106"]}