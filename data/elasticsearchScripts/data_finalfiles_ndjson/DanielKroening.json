{"title": "A survey of automated techniques for formal software verification\n", "abstract": " The quality and the correctness of software are often the greatest concern in electronic systems. Formal verification tools can provide a guarantee that a design is free of specific flaws. This paper surveys algorithms that perform automatic static analysis of software to detect programming errors or prove their absence. The three techniques considered are static analysis with abstract domains, model checking, and bounded model checking. A short tutorial on these techniques is provided, highlighting their differences when applied to practical problems. This paper also surveys tools implementing these techniques and describes their merits and shortcomings.", "num_citations": "486\n", "authors": ["1024"]}
{"title": "Model checking concurrent Linux device drivers\n", "abstract": " The S< scp> lam toolkit demonstrates that predicate abstraction enables automated verification of real world Windows device drivers. Our predicate abstraction-based tool DDV< scp> erify enables the automated verification of Linux device drivers and provides an accurate model of the relevant parts of the kernel. We report on benchmarks based on Linux device drivers, confirming the results that S< scp> lam established for the Windows world. Furthermore, we take predicate abstraction one step further and introduce a technique to verify concurrent software with shared memory", "num_citations": "145\n", "authors": ["1024"]}
{"title": "Dynamic cutoff detection in parameterized concurrent programs\n", "abstract": " We consider the class of finite-state programs executed by an unbounded number of replicated threads communicating via shared variables. The thread-state reachability problem for this class is essential in software verification using predicate abstraction. While this problem is decidable via Petri net coverability analysis, techniques solely based on coverability suffer from the problem\u2019s exponential-space complexity. In this paper, we present an alternative method based on a thread-state cutoff: a number n of threads that suffice to generate all reachable thread states. We give a condition, verifiable dynamically during reachability analysis for increasing n, that is sufficient to conclude that n is a cutoff. We then make the method complete, via a coverability query that is of low cost in practice. We demonstrate the efficiency of the approach on Petri net encodings of communication protocols, as well as on non\u00a0\u2026", "num_citations": "129\n", "authors": ["1024"]}
{"title": "Interpolant strength\n", "abstract": " Interpolant-based model checking is an approximate method for computing invariants of transition systems. The performance of the model checker is contingent on the approximation computed, which in turn depends on the logical strength of the interpolants. A good approximation is coarse enough to enable rapid convergence but strong enough to be contained within the weakest inductive invariant. We present a system for constructing propositional interpolants of different strength from a resolution refutation. This system subsumes existing methods and allows interpolation systems to be ordered by the logical strength of the obtained interpolants. Interpolants of different strength can also be obtained by transforming a resolution proof. We analyse an existing proof transformation, generalise it, and characterise the interpolants obtained.", "num_citations": "123\n", "authors": ["1024"]}
{"title": "Putting it all together\u2013Formal verification of the VAMP\n", "abstract": " In the verified architecture microprocessor (VAMP) project we have designed, functionally verified, and synthesized a processor with full DLX instruction set, delayed branch, Tomasulo scheduler, maskable nested precise interrupts, pipelined fully IEEE compatible dual precision floating point unit with variable latency, and separate instruction and data caches. The verification has been carried out in the theorem proving system PVS. The processor has been implemented on a Xilinx FPGA.", "num_citations": "123\n", "authors": ["1024"]}
{"title": "Software Verification Using k-Induction\n", "abstract": " We present combined-case k-induction, a novel technique for verifying software programs. This technique draws on the strengths of the classical inductive-invariant method and a recent application of k-induction to program verification. In previous work, correctness of programs was established by separately proving a base case and inductive step. We present a new k-induction rule that takes an unstructured, reducible control flow graph (CFG), a natural loop occurring in the CFG, and a positive integer k, and constructs a single CFG in which the given loop is eliminated via an unwinding proportional to k. Recursively applying the proof rule eventually yields a loop-free CFG, which can be checked using SAT-/SMT-based techniques. We state soundness of the rule, and investigate its theoretical properties. We then present two implementations of our technique: K-Inductor, a verifier for C programs built on top of\u00a0\u2026", "num_citations": "114\n", "authors": ["1024"]}
{"title": "Race analysis for SystemC using model checking\n", "abstract": " SystemC is a system-level modeling language that offers a wide range of features to describe concurrent systems at different levels of abstraction. The SystemC standard permits simulators to implement a deterministic scheduling policy, which often hides concurrency-related design flaws. We present a novel compiler for SystemC that integrates a very precise formal race analysis by means of model checking. Our compiler produces a simulator that uses the outcome of the analysis to perform partial order reduction. The key insight to make the model checking engine scale is to apply it only to tiny fractions of the SystemC model. We show that the outcome of the analysis is not only valuable to eliminate redundant context switches at runtime, but can also be used to diagnose race conditions statically. In particular, our analysis is able to reveal races that can remain undetected during simulation and is able to formally\u00a0\u2026", "num_citations": "92\n", "authors": ["1024"]}
{"title": "Mixed abstractions for floating-point arithmetic\n", "abstract": " Floating-point arithmetic is essential for many embedded and safety-critical systems, such as in the avionics industry. Inaccuracies in floating-point calculations can cause subtle changes of the control flow, potentially leading to disastrous errors. In this paper, we present a simple and general, yet powerful framework for building abstractions from formulas, and instantiate this framework to a bit-accurate, sound and complete decision procedure for IEEE-compliant binary floating-point arithmetic. Our procedure benefits in practice from its ability to flexibly harness both over- and underapproximations in the abstraction process. We demonstrate the potency of the procedure for the formal analysis of floating-point software.", "num_citations": "89\n", "authors": ["1024"]}
{"title": "Understanding counterexamples with explain\n", "abstract": " The counterexamples produced by model checkers are often lengthy and difficult to understand. In practical verification, showing the existence of a (potential) bug is not enough: the error must be understood, determined to not be a result of faulty specification or assumptions, and, finally, located and corrected. The explain tool uses distance metrics on program executions to provide automated assistance in understanding and localizing errors in ANSI-C programs. explain is integrated with CBMC, a bounded model checker for the C language, and features a GUI front-end that presents error explanations to the user.", "num_citations": "87\n", "authors": ["1024"]}
{"title": "Don\u2019t sit on the fence\n", "abstract": " Modern architectures rely on memory fences to prevent undesired weakenings of memory consistency. As the fences\u2019 semantics may be subtle, the automation of their placement is highly desirable. But precise methods for restoring consistency do not scale to deployed systems code. We choose to trade some precision for genuine scalability: our technique is suitable for large code bases. We implement it in our new musketeer tool, and detail experiments on more than 350 executables of packages found in Debian Linux 7.1, e.g. memcached (about 10000 LoC).", "num_citations": "83\n", "authors": ["1024"]}
{"title": "Mutation-based test case generation for Simulink models\n", "abstract": " The Matlab/Simulink language has become the standard formalism for modeling and implementing control software in areas like avionics, automotive, railway, and process automation. Such software is often safety critical, and bugs have potentially disastrous consequences for people and material involved. We define a verification methodology to assess the correctness of Simulink programs by means of automated test-case generation. In the style of fault- and mutation-based testing, the coverage of a Simulink program by a test suite is defined in terms of the detection of injected faults. Using bounded model checking techniques, we are able to effectively and automatically compute test suites for given fault models. Several optimisations are discussed to make the approach practical for realistic Simulink programs and fault models, and to obtain accurate coverage measures.", "num_citations": "83\n", "authors": ["1024"]}
{"title": "Making the most of BMC counterexamples\n", "abstract": " The value of model checking counterexamples for debugging programs (and specifications) is widely recognized. Unfortunately, bounded model checkers often produce counterexamples that are difficult to understand due to the values chosen by a SAT solver. This paper presents two approaches to making better use of BMC counterexamples. The first contribution is a new notion of counterexample minimization that minimizes values with respect to the type system of the language being model checked, rather than at the level of SAT variables. Greedy and optimal approaches to the minimization problem are presented and compared. The second contribution extends a BMC-based error explanation approach to automatically hypothesize causes for the error in a counterexample. These hypotheses (in terms of relationships between variables) can be automatically checked to determine if a causal dependence exists\u00a0\u2026", "num_citations": "78\n", "authors": ["1024"]}
{"title": "Test-case generation for embedded simulink via formal concept analysis\n", "abstract": " Mutation testing suffers from the high computational cost of automated test-vector generation, due to the large number of mutants that can be derived from programs and the cost of generating test-cases in a white-box manner. We propose a novel algorithm for mutation-based test-case generation for Simulink models that combines white-box testing with formal concept analysis. By exploiting similarity measures on mutants, we are able to effectively generate small sets of short test-cases that achieve high coverage on a collection of Simulink models from the automotive domain. Experiments show that our algorithm performs significantly better than random testing or simpler mutation-testing approaches.", "num_citations": "76\n", "authors": ["1024"]}
{"title": "Deciding floating-point logic with abstract conflict driven clause learning\n", "abstract": " We present a bit-precise decision procedure for the theory of floating-point arithmetic. The core of our approach is a non-trivial, lattice-theoretic generalisation of the conflict-driven clause learning algorithm in modern sat solvers to lattice-based abstractions. We use floating-point intervals to reason about the ranges of variables, which allows us to directly handle arithmetic and is more efficient than encoding a formula as a bit-vector as in current floating-point solvers. Interval reasoning alone is incomplete, and we obtain completeness by developing a conflict analysis algorithm that reasons natively about intervals. We have implemented this method in the mathsat5                 smt solver and evaluated it on assertion checking problems that bound the values of program variables. Our new technique is faster than a bit-vector encoding approach on 80\u00a0% of the benchmarks, and is faster by one order of magnitude\u00a0\u2026", "num_citations": "70\n", "authors": ["1024"]}
{"title": "Deciding floating-point logic with systematic abstraction\n", "abstract": " We present a bit-precise decision procedure for the theory of binary floating-point arithmetic. The core of our approach is a non-trivial generalisation of the conflict analysis algorithm used in modern SAT solvers to lattice-based abstractions. Existing complete solvers for floating-point arithmetic employ bit-vector encodings. Propositional solvers based on the Conflict Driven Clause Learning (CDCL) algorithm are then used as a backend. We present a natural-domain SMT approach that lifts the CDCL framework to operate directly over abstractions of floating-point values. We have instantiated our method inside MATHSAT5 with the floating-point interval abstraction. The result is a sound and complete procedure for floating-point arithmetic that outperforms the state-of-the-art significantly on problems that check ranges on numerical variables. Our technique is independent of the specific abstraction and can be applied to\u00a0\u2026", "num_citations": "69\n", "authors": ["1024"]}
{"title": "Precise predictive analysis for discovering communication deadlocks in MPI programs\n", "abstract": " The Message Passing Interface (MPI) is the standard API for high-performance and scientific computing. Communication deadlocks are a frequent problem in MPI programs, and this paper addresses the problem of discovering such deadlocks. We begin by showing that if an MPI program is single-path, the problem of discovering communication deadlocks is NP-complete. We then present a novel propositional encoding scheme which captures the existence of communication deadlocks. The encoding is based on modelling executions with partial orders, and implemented in a tool called MOPPER. The tool executes an MPI program, collects the trace, builds a formula from the trace using the propositional encoding scheme, and checks its satisfiability. Finally, we present experimental results that quantify the benefit of the approach in comparison to a dynamic analyser and demonstrate that it offers a scalable\u00a0\u2026", "num_citations": "64\n", "authors": ["1024"]}
{"title": "Symbolic counter abstraction for concurrent software\n", "abstract": " The trend towards multi-core computing has made concurrent software an important target of computer-aided verification. Unfortunately, Model Checkers for such software suffer tremendously from combinatorial state space explosion. We show how to apply counter abstraction to real-world concurrent programs to factor out redundancy due to thread replication. The traditional global state representation as a vector of local states is replaced by a vector of thread counters, one per local state. In practice, straightforward implementations of this idea are unfavorably sensitive to the number of local states. We present a novel symbolic exploration algorithm that avoids this problem by carefully scheduling which counters to track at any moment during the search. Our experiments are carried out on Boolean programs, an abstraction promoted by the Slam project. To our knowledge, this marks the first application of\u00a0\u2026", "num_citations": "64\n", "authors": ["1024"]}
{"title": "Interpolation-Based Software Verification with Wolverine\n", "abstract": " Wolverine is a software verification tool using Craig interpolation to compute invariants of ANSI-C and C++ programs. The tool is an implementation of the lazy abstraction approach, generating a reachability tree by unwinding the transition relation of the input program and annotating its nodes with interpolants representing safe states. Wolverine features a built-in interpolating decision procedure for equality logic with uninterpreted functions which provides limited support for bit-vector operations. In addition, it provides an API enabling the integration of other interpolating decision procedures, making it a valuable source of benchmarks and allowing it to take advantage of the continuous performance improvements of SMT solvers. We evaluate the performance of Wolverine by comparing it to the predicate abstraction-based verifier SatAbs on a number of verification conditions of Linux device drivers.", "num_citations": "62\n", "authors": ["1024"]}
{"title": "Unfolding-based partial order reduction\n", "abstract": " Partial order reduction (POR) and net unfoldings are two alternative methods to tackle state-space explosion caused by concurrency. In this paper, we propose the combination of both approaches in an effort to combine their strengths. We first define, for an abstract execution model, unfolding semantics parameterized over an arbitrary independence relation. Based on it, our main contribution is a novel stateless POR algorithm that explores at most one execution per Mazurkiewicz trace, and in general, can explore exponentially fewer, thus achieving a form of super-optimality. Furthermore, our unfolding-based POR copes with non-terminating executions and incorporates state-caching. Over benchmarks with busy-waits, among others, our experiments show a dramatic reduction in the number of executions when compared to a state-of-the-art DPOR.", "num_citations": "60\n", "authors": ["1024"]}
{"title": "Instantiating uninterpreted functional units and memory system: Functional verification of the VAMP\n", "abstract": " In the VAMP (verified architecture microprocessor) project we have designed, functionally verified, and synthesized a processor with full DLX instruction set, delayed branch, Tomasulo scheduler, maskable nested precise interrupts, pipelined fully IEEE compatible dual precision floating point unit with variable latency, and separate instruction and data caches. The verification has been carried out in the theorem proving system PVS. The processor has been implemented on a Xilinx FPGA.", "num_citations": "58\n", "authors": ["1024"]}
{"title": "Automated pipeline design\n", "abstract": " The interlock and forwarding logic is considered the tricky part of fully-featured piplined microprocessor and especially debugging these parts delays the hardware design process considerably. It is therefore desirable to automate the design of both interlock and forwarding logic. The hardware design engineer begins with a sequential implementation without any interlock and forwarding logic. A tool then adds the forwarding and interlock logic required for pipelining. This paper describes the algorithm for such a tool and the correctness is formally verified. We use a standard DLX RISC processor as an example.", "num_citations": "55\n", "authors": ["1024"]}
{"title": "Symmetry-aware predicate abstraction for shared-variable concurrent programs\n", "abstract": " Predicate abstraction is a key enabling technology for applying finite-state model checkers to programs written in mainstream languages. It has been used very successfully for debugging sequential system-level C code. Although model checking was originally designed for analyzing concurrent systems, there is little evidence of fruitful applications of predicate abstraction to shared-variable concurrent software. The goal of this paper is to close this gap. We have developed a symmetry-aware predicate abstraction strategy: it takes into account the replicated structure of C programs that consist of many threads executing the same procedure, and generates a Boolean program template whose multi-threaded execution soundly overapproximates the concurrent C program. State explosion during model checking parallel instantiations of this template can now be absorbed by exploiting symmetry. We\u00a0\u2026", "num_citations": "54\n", "authors": ["1024"]}
{"title": "Abstract conflict driven learning\n", "abstract": " Modern satisfiability solvers implement an algorithm, called Conflict Driven Clause Learning, which combines search for a model with analysis of conflicts. We show that this algorithm can be generalised to solve the lattice-theoretic problem of determining if an additive transformer on a Boolean lattice is always bottom. Our generalised procedure combines overapproximations of greatest fixed points with underapproximation of least fixed points to obtain more precise results than computing fixed points in isolation. We generalise implication graphs used in satisfiability solvers to derive underapproximate transformers from overapproximate ones. Our generalisation provides a new method for static analysers that operate over non-distributive lattices to reason about properties that require disjunction.", "num_citations": "52\n", "authors": ["1024"]}
{"title": "Safety Verification and Refutation by k-Invariants and k-Induction\n", "abstract": " Most software verification tools can be classified into one of a number of established families, each of which has their own focus and strengths. For example, concrete counterexample generation in model checking, invariant inference in abstract interpretation and completeness via annotation for deductive verification. This creates a significant and fundamental usability problem as users may have to learn and use one technique to find potential problems but then need an entirely different one to show that they have been fixed. This paper presents a single, unified algorithm II, which strictly generalises abstract interpretation, bounded model checking and k-induction. This not only combines the strengths of these techniques but allows them to interact and reinforce each other, giving a \u2018single-tool\u2019 approach to verification.", "num_citations": "49\n", "authors": ["1024"]}
{"title": "Under-approximating loops in C programs for fast counterexample detection\n", "abstract": " Many software model checkers only detect counterexamples with deep loops after exploring numerous spurious and increasingly longer counterexamples. We propose a technique that aims at eliminating this weakness by constructing auxiliary paths that represent the effect of a range of loop iterations. Unlike acceleration, which captures the exact effect of arbitrarily many loop iterations, these auxiliary paths may under-approximate the behaviour of the loops. In return, the approximation is sound with respect to the bit-vector semantics of programs. Our approach supports arbitrary conditions and assignments to arrays in the loop body, but may as a result introduce quantified conditionals. To reduce the resulting performance penalty, we present two quantifier elimination techniques specially geared towards our application. Loop under-approximation can be combined with a broad range of verification\u00a0\u2026", "num_citations": "49\n", "authors": ["1024"]}
{"title": "An interpolating sequent calculus for quantifier-free Presburger arithmetic\n", "abstract": " Craig interpolation has become a versatile tool in formal verification, for instance to generate intermediate assertions for safety analysis of programs. Interpolants are typically determined by annotating the steps of an unsatisfiability proof with partial interpolants. In this paper, we consider Craig interpolation for full quantifier-free Presburger arithmetic (QFPA), for which currently no efficient interpolation procedures are known. Closing this gap, we introduce an interpolating sequent calculus for QFPA and prove it to be sound and complete. We have extended the Princess theorem prover to generate interpolating proofs, and applied it to a large number of publicly available linear integer arithmetic benchmarks. The results indicate the robustness and efficiency of our proof-based interpolation procedure.", "num_citations": "49\n", "authors": ["1024"]}
{"title": "Efficient coverability analysis by proof minimization\n", "abstract": " We consider multi-threaded programs with an unbounded number of threads executing a finite-state, non-recursive procedure. Safety properties of such programs can be checked via reduction to the coverability problem for well-structured transition systems (WSTS). In this paper, we present a novel, sound and complete yet empirically much improved solution to this problem. The key idea to achieve a compact search structure is to track uncoverability only for minimal uncoverable elements, even if these elements are not part of the original coverability query. To this end, our algorithm examines elements in the downward closure of elements backward-reachable from the initial queries. A downside is that the algorithm may unnecessarily explore elements that turn out coverable and thus fail to contribute to the proof minimization. We counter this effect using a forward search engine that simultaneously generates\u00a0\u2026", "num_citations": "47\n", "authors": ["1024"]}
{"title": "Formal verification of pipelined microprocessors\n", "abstract": " Subject of this thesis is the formal verification of pipelined microprocessors. This includes processors with state of the art schedulers, such as the Tomasulo scheduler and speculation. In contrast to most of the literature, we verify synthesizable design at gate level. Furthermore, we prove both data consistency and liveness. We verify the proofs using the theorem proving system PVS. We verify both in-order and out-of-order machines. For verifying in-order machines, we extend the stall engine concept presented in [MP00]. We describe and implement an algorithm that does the transformation into a pipelined machine. We describe a generic machine that supports speculating on arbitraty values. We formally verify proofs for the Tomasulo scheduling algorithm with reorder buffer.", "num_citations": "46\n", "authors": ["1024"]}
{"title": "Automatic analysis of scratch-pad memory code for heterogeneous multicore processors\n", "abstract": " Modern multicore processors, such as the Cell Broadband Engine, achieve high performance by equipping accelerator cores with small \u201cscratch-pad\u201d memories. The price for increased performance is higher programming complexity \u2013 the programmer must manually orchestrate data movement using direct memory access (DMA) operations. Programming using asynchronous DMAs is error-prone, and DMA races can lead to nondeterministic bugs which are hard to reproduce and fix. We present a method for DMA race analysis which automatically instruments the program with assertions modelling the semantics of a memory flow controller. To enable automatic verification of instrumented programs, we present a new formulation of k-induction geared towards software, as a proof rule operating on loops. We present a tool, Scratch, which we apply to a large set of programs supplied with the IBM Cell SDK, in\u00a0\u2026", "num_citations": "45\n", "authors": ["1024"]}
{"title": "A widening approach to multithreaded program verification\n", "abstract": " Pthread-style multithreaded programs feature rich thread communication mechanisms, such as shared variables, signals, and broadcasts. In this article, we consider the automated verification of such programs where an unknown number of threads execute a given finite-data procedure in parallel. Such procedures are typically obtained as predicate abstractions of recursion-free source code written in C or Java. Many safety problems over finite-data replicated multithreaded programs are decidable via a reduction to the coverability problem in certain types of well-ordered infinite-state transition systems. On the other hand, in full generality, this problem is Ackermann-hard, which seems to rule out efficient algorithmic treatment. We present a novel, sound, and complete yet empirically efficient solution. Our approach is to judiciously widen the original set of coverability targets by configurations that involve fewer\u00a0\u2026", "num_citations": "40\n", "authors": ["1024"]}
{"title": "Component-based design and verification in X-MAN\n", "abstract": " Compositionality has the potential to enable a step-change in the scalability of formal design and verification methods for industrial-scale systems: by designing systems in a compositional manner, components can be modelled, specified, implemented, and verified independently and in parallel by different teams, leading to significant gains in terms of productivity and the ability to reuse components. We discuss why component-based frameworks have, up to now fallen short of meeting those expectations, and present the X-MAN framework; a component-based framework and development methodology that has been designed to overcome limitations of previous solutions. Walking through an industrial case study, we illustrate architecture, specification, detailed design, and implementation of systems in X-MAN. Correctness and reliability concerns are addressed uniformly within X- MAN through integration with existing static analysis tools for functional and extra-functional properties.", "num_citations": "40\n", "authors": ["1024"]}
{"title": "2LS for program analysis\n", "abstract": " 2LS is a program analysis tool for C programs built upon the CPROVER infrastructure. 2LS is bit-precise and it can verify and refute program assertions. 2LS implements invariant generation techniques, incremental bounded model checking and incremental k-induction. The competition submission uses an algorithm combining all three techniques, called kIkI (k-invariants and k-induction). As a back end, the competition submission of 2LS uses Glucose\u00a04.0.", "num_citations": "39\n", "authors": ["1024"]}
{"title": "Beyond quantifier-free interpolation in extensions of Presburger arithmetic\n", "abstract": " Craig interpolation has emerged as an effective means of generating candidate program invariants. We present interpolation procedures for the theories of Presburger arithmetic combined with (i)\u00a0uninterpreted predicates (QPA+UP), (ii)\u00a0uninterpreted functions (QPA+UF) and (iii)\u00a0extensional arrays (QPA+AR). We prove that none of these combinations can be effectively interpolated without the use of quantifiers, even if the input formulae are quantifier-free. We go on to identify fragments of QPA+UP and QPA+UF with restricted forms of guarded quantification that are closed under interpolation. Formulae in these fragments can easily be mapped to quantifier-free expressions with integer division. For QPA+AR, we formulate a sound interpolation procedure that potentially produces interpolants with unrestricted quantifiers.", "num_citations": "39\n", "authors": ["1024"]}
{"title": "A proposal for a theory of finite sets, lists, and maps for the SMT-LIB standard\n", "abstract": " Sets, lists, and maps are elementary data structures used in most programs. Program analysis tools therefore need to decide verification conditions containing variables of such types. We propose a new theory for the SMT-Lib standard as the standard format for such formulae.", "num_citations": "36\n", "authors": ["1024"]}
{"title": "An interpolating sequent calculus for quantifier-free Presburger arithmetic\n", "abstract": " Craig interpolation has become a versatile tool in formal verification, used for instance to generate program assertions that serve as candidates for loop invariants. In this paper, we consider Craig interpolation for quantifier-free Presburger arithmetic (QFPA). Until recently, quantifier elimination was the only available interpolation method for this theory, which is, however, known to be potentially costly and inflexible. We introduce an interpolation approach based on a sequent calculus for QFPA that determines interpolants by annotating the steps of an unsatisfiability proof with partial interpolants. We prove our calculus to be sound and complete. We have extended the Princess theorem prover to generate interpolating proofs, and applied it to a large number of publicly available Presburger arithmetic benchmarks. The results document the robustness and efficiency of our interpolation procedure. Finally, we\u00a0\u2026", "num_citations": "35\n", "authors": ["1024"]}
{"title": "Lifting propositional interpolants to the word-level\n", "abstract": " Craig interpolants are often used to approximate inductive invariants of transition systems. Arithmetic relationships between numeric variables require word-level interpolants, which are derived from word-level proofs of unsatisfiability. While word-level theorem provers have made significant progress in the past few years, competitive solvers for many logics are based on flattening the word-level structure to the bit-level. We propose an algorithm that lifts a resolution proof obtained from a bit-flattened formula up to the word-level, which enables the computation of word-level interpolants. Experimental results for equality logic suggest that the overhead of lifting the propositional proof is very low compared to the solving time of a state-of-the-art solver.", "num_citations": "35\n", "authors": ["1024"]}
{"title": "Satisfiability solvers are static analysers\n", "abstract": " This paper shows that several propositional satisfiability algorithms compute approximations of fixed points using lattice-based abstractions. The Boolean Constraint Propagation algorithm (bcp) is a greatest fixed point computation over a lattice of partial assignments. The original algorithm of Davis, Logemann and Loveland refines bcp by computing a set of greatest fixed points. The Conflict Driven Clause Learning algorithm alternates between overapproximate deduction with bcp, and underapproximate abduction, with conflict analysis. Thus, in a precise sense, satisfiability solvers are abstract interpreters. Our work is the first step towards a uniform framework for the design and implementation of satisfiability algorithms, static analysers and their combination.", "num_citations": "34\n", "authors": ["1024"]}
{"title": "Automatic analysis of DMA races using model checking and k-induction\n", "abstract": " Modern multicore processors, such as the Cell Broadband Engine, achieve high performance by equipping accelerator cores with small \u201cscratch-pad\u201d memories. The price for increased performance is higher programming complexity \u2013 the programmer must manually orchestrate data movement using direct memory access (DMA) operations. Programming using asynchronous DMA operations is error-prone, and DMA races can lead to nondeterministic bugs which are hard to reproduce and fix. We present a method for DMA race analysis in C programs. Our method works by automatically instrumenting a program with assertions modeling the semantics of a memory flow controller. The instrumented program can then be analyzed using state-of-the-art software model checkers. We show that bounded model checking is effective for detecting DMA races in buggy programs. To enable automatic verification of\u00a0\u2026", "num_citations": "34\n", "authors": ["1024"]}
{"title": "Counterexamples with loops for predicate abstraction\n", "abstract": " Predicate abstraction is a major abstraction technique for the verification of software. Data is abstracted by means of Boolean variables, which keep track of predicates over the data. In many cases, the technique suffers from the fact that it requires at least one predicate for each iteration of a loop construct in the program. We propose to extract looping counterexamples from the abstract model, and to parameterize the simulation instance in the number of loop iterations.", "num_citations": "34\n", "authors": ["1024"]}
{"title": "Verifying C++ with STL containers via predicate abstraction\n", "abstract": " This paper describes a flexible and easily extensible predicate abstraction-based approach to the verification of STLusage, and observes the advantages of verifying programsin terms of high-level data structures rather than low-level pointer manipulations. We formalize the semantics of theSTL by means of a Hoare-style axiomatization. The verification requires an operational model conservatively approximating the semantics given by the Standard. Our results show advantages (in terms of errors detected and false positives avoided) over previous attempts to analyze STL usage, due to the power of the abstraction engine and model checker", "num_citations": "33\n", "authors": ["1024"]}
{"title": "A SAT-based algorithm for reparameterization in symbolic simulation\n", "abstract": " Parametric representations used for symbolic simulation of circuits usually use BDDs. After a few steps of symbolic simulation, state set representation is converted from one parametric representation to another smaller representation, in a process called reparameterization. For large circuits, the reparametrization step often results in a blowup of BDDs and is expensive due to a large number of quantifications of input variables involved. Efficient SAT solvers have been applied successfully for many verification problems. This paper presents a novel SAT-based reparameterization algorithm that is largely immune to the large number of input variables that need to be quantified. We show experimental results on large industrial circuits and compare our new algorithm to both SAT-based Bounded Model Checking and BDD based symbolic simulation. We were able to achieve on average 3x improvement in time and\u00a0\u2026", "num_citations": "32\n", "authors": ["1024"]}
{"title": "Incremental bounded model checking for embedded software\n", "abstract": " Program analysis is on the brink of mainstream usage in embedded systems development. Formal verification of behavioural requirements, finding runtime errors and test case generation are some of the most common applications of automated verification tools based on bounded model checking (BMC). Existing industrial tools for embedded software use an off-the-shelf bounded model checker and apply it iteratively to verify the program with an increasing number of unwindings. This approach unnecessarily wastes time repeating work that has already been done and fails to exploit the power of incremental SAT solving. This article reports on the extension of the software model checker CBMC to support incremental BMC and its successful integration with the industrial embedded software verification tool BTC EMBEDDED                         TESTER. We present an extensive evaluation over large industrial\u00a0\u2026", "num_citations": "30\n", "authors": ["1024"]}
{"title": "Formal techniques for effective co-verification of hardware/software co-designs\n", "abstract": " Verification is indispensable for building reliable of hardware/software co-designs. However, the scope of formal methods in this domain is limited. This is attributed to the lack of unified property specification languages, the semantic gap between hardware and software components, and the lack of verifiers that support both C and Verilog/VHDL. To address these limitations, we present an approach that uses a bounded co-verification tool, HW-CBMC, for formally validating hardware/software co-designs written in Verilog and C. Properties are expressed in C enriched with special-purpose primitives that capture temporal correlation between hardware and software events. We present an industrial case-study, proving bounded safety properties as well as discovering critical co-design bugs on a large and complex text analytics FPGA accelerator from IBM \u00ae .", "num_citations": "29\n", "authors": ["1024"]}
{"title": "Evaluation of measures for statistical fault localisation and an optimising scheme\n", "abstract": " Statistical Fault Localisation (SFL) is a widely used method for localizing faults in software. SFL gathers coverage details of passed and failed executions over a faulty program and then uses a measure to assign a degree of suspiciousness to each of a chosen set of program entities (statements, predicates, etc.) in that program. The program entities are then inspected by the engineer in descending order of suspiciousness until the bug is found. The effectiveness of this process relies on the quality of the suspiciousness measure. In this paper, we compare 157 measures, 95 of which are new to SFL and borrowed from other branches of science and philosophy. We also present a new measure optimiser Lex                                        g                   , which optimises a given measure g according to a criterion of single bug optimality. An experimental comparison on benchmarks from the Software-artifact\u00a0\u2026", "num_citations": "27\n", "authors": ["1024"]}
{"title": "Computing binary combinatorial gray codes via exhaustive search with SAT solvers\n", "abstract": " The term binary combinatorial Gray code refers to a list of binary words such that the Hamming distance between two neighboring words is one and the list satisfies some additional properties that are of interest to a particular application, e.g., circuit testing, data compression, and computational biology. New distance-preserving and circuit codes are presented along with a complete list of equivalence classes of the coil-in-the-box codes for codeword length with respect to symmetry transformations of hypercubes. A Gray-ordered code composed of all necklaces of the length is presented, improving the known result with length .", "num_citations": "27\n", "authors": ["1024"]}
{"title": "Formal verification at higher levels of abstraction\n", "abstract": " Most formal verification tools on the market convert a high-level register transfer level (RTL) design into a bit-level model. Algorithms that operate at the bit-level are unable to exploit the structure provided by the higher abstraction levels, and thus, are less scalable. This tutorial surveys recent advances in formal verification using high-level models. We present word-level verification with predicate abstraction and satisfiability modulo theories (SMT) solvers. We then describe techniques for term-level modeling and ways to combine word-level and term-level approaches for scalable verification.", "num_citations": "26\n", "authors": ["1024"]}
{"title": "Specifying and verifying systems with multiple clocks\n", "abstract": " Multiple clock domains are a challenge for hardware specification and verification. We present a method for specifying the relations between multiple clocks, and for modeling the possible behaviors. We can then verify a hardware design assuming that the clocks meet these constraints. We implement our ideas in the context of SAT based bounded model checking (BMC), using ANSI-C programs to specify the functional behavior of the design.", "num_citations": "26\n", "authors": ["1024"]}
{"title": "Formal verification of a basic circuits library\n", "abstract": " We describe the results and status of a project aiming to provide a provably correct library of basic circuits. We use the theorem proving system PVS in order to prove circuits such as incrementers, adders, arithmetic units, multipliers, leading zero counters, shifters, and decoders. All specifications and proofs are available on the web.", "num_citations": "26\n", "authors": ["1024"]}
{"title": "Interpolation-based verification of floating-point programs with abstract CDCL\n", "abstract": " One approach for smt solvers to improve efficiency is to delegate reasoning to abstract domains. Solvers using abstract domains do not support interpolation and cannot be used for interpolation-based verification. We extend Abstract Conflict Driven Clause Learning (acdcl) solvers with proof generation and interpolation. Our results lead to the first interpolation procedure for floating-point logic and subsequently, the first interpolation-based verifiers for programs with floating-point variables. We demonstrate the potential of this approach by verifying a number of programs which are challenging for current verification tools.", "num_citations": "25\n", "authors": ["1024"]}
{"title": "Interpolating quantifier-free Presburger arithmetic\n", "abstract": " Craig interpolation has become a key ingredient in many symbolic model checkers, serving as an approximative replacement for expensive quantifier elimination. In this paper, we focus on an interpolating decision procedure for the full quantifier-free fragment of Presburger Arithmetic, i.e., linear arithmetic over the integers, a theory which is a good fit for the analysis of software systems. In contrast to earlier procedures based on quantifier elimination and the Omega test, our approach uses integer linear programming techniques: relaxation of interpolation problems to the rationals, and a complete branch-and-bound rule tailored to efficient interpolation. Equations are handled via a dedicated polynomial-time sub-procedure. We have fully implemented our procedure on top of the SMT-solver OpenSMT and present an extensive experimental evaluation.", "num_citations": "25\n", "authors": ["1024"]}
{"title": "Software verification\n", "abstract": " This chapter covers an application of propositional satisfiability to program analysis. We focus on the discovery of programming flaws in low-level programs, such as embedded software. The loops in the program are unwound together with a property to form a formula, which is then converted into CNF. The method supports low-level programming constructs such as bit-wise operators or pointer arithmetic.", "num_citations": "25\n", "authors": ["1024"]}
{"title": "Checking consistency of C and Verilog using predicate abstraction and induction\n", "abstract": " It is common practice to write C models of circuits due to the greater simulation efficiency. Once the C program satisfies the requirements, the circuit is designed in a hardware description language (HDL) such as Verilog. It is therefore highly desirable to automatically perform a correspondence check between the C model and a circuit given in HDL. We present an algorithm that checks consistency between an ANSI-C program and a circuit given in Verilog using predicate abstraction. The algorithm exploits the fact that the C program and the circuit share many basic predicates. In contrast to existing tools that perform predicate abstraction, our approach is SAT-based and allows all ANSI-C and Verilog operators in the predicates. We report experimental results on an out-of-order RISC processor. We compare the performance of the new technique to bounded model checking (BMC).", "num_citations": "24\n", "authors": ["1024"]}
{"title": "Sound static deadlock analysis for C/Pthreads\n", "abstract": " We present a static deadlock analysis approach for C/pthreads. The design of our method has been guided by the requirement to analyse real-world code. Our approach is sound (ie, misses no deadlocks) for programs that have defined behaviour according to the C standard and the pthreads specification, and is precise enough to prove deadlock-freedom for a large number of such programs. The method consists of a pipeline of several analyses that build on a new context-and thread-sensitive abstract interpretation framework. We further present a lightweight dependency analysis to identify statements relevant to deadlock analysis and thus speed up the overall analysis. In our experimental evaluation, we succeeded to prove deadlock-freedom for 292 programs from the Debian GNU/Linux distribution with in total 2.3 MLOC in 4 hours.", "num_citations": "23\n", "authors": ["1024"]}
{"title": "Successful use of incremental BMC in the automotive industry\n", "abstract": " Program analysis is on the brink of mainstream usage in embedded systems development. Formal verification of behavioural requirements, finding runtime errors and automated test case generation are some of the most common applications of automated verification tools based on Bounded Model Checking (BMC). Existing industrial tools for embedded software use an off-the-shelf Bounded Model Checker and apply it iteratively to verify the program with an increasing number of unwindings. This approach unnecessarily wastes time repeating work that has already been done and fails to exploit the power of incremental SAT solving. This paper reports on the extension of the software model checker Cbmc to support incremental BMC and its successful integration with the industrial embedded software verification tool BTC                   EmbeddedTester. We present an extensive evaluation over large\u00a0\u2026", "num_citations": "23\n", "authors": ["1024"]}
{"title": "Accelerated test execution using GPUs\n", "abstract": " As product life-cycles become shorter and the scale and complexity of systems increase, accelerating the execution of large test suites gains importance. Existing research has primarily focussed on techniques that reduce the size of the test suite. By contrast, we propose a technique that accelerates test execution, allowing test suites to run in a fraction of the original time, by parallel execution with a Graphics Processing Unit (GPU).", "num_citations": "22\n", "authors": ["1024"]}
{"title": "Counterexample-guided precondition inference\n", "abstract": " The precondition for an assertion inside a procedure is useful for understanding, verifying and debugging programs. As the procedure might be used in multiple calling-contexts within a program, the precondition should be sufficiently general to enable re-use. We present an extension of counterexample-guided abstraction refinement (CEGAR) for automated precondition inference. Starting with an over-approximation of both the set of safe and unsafe states, we iteratively refine them until they become disjoint. The resulting precondition is then necessary and sufficient for the validity of the assertion, which prevents false alarms. We have implemented our approach in a tool called P-Gen. We present experimental results on string and array-manipulating programs.", "num_citations": "22\n", "authors": ["1024"]}
{"title": "Verification of SpecC using predicate abstraction\n", "abstract": " Languages such as SystemC or SpecC offer modeling of hardware and whole system designs at a high level of abstraction. However, formal verification techniques are widely applied in the hardware design industry only for low level designs, such as a netlist or RTL. The higher abstraction levels offered by these new languages are not yet amenable to rigorous, formal verification. This paper describes how to apply predicate abstraction to SpecC system descriptions. The technique supports the concurrency constructs offered by SpecC. It models the bit-vector semantics of the language accurately, and can be used both for property checking and for checking refinement together with a traditional low-level design given in Verilog.", "num_citations": "22\n", "authors": ["1024"]}
{"title": "Program synthesis: challenges and opportunities\n", "abstract": " Program synthesis is the mechanized construction of software, dubbed \u2018self-writing code\u2019. Synthesis tools relieve the programmer from thinking about how the problem is to be solved; instead, the programmer only provides a description of what is to be achieved. Given a specification of what the program should do, the synthesizer generates an implementation that provably satisfies this specification. From a logical point of view, a program synthesizer is a solver for second-order existential logic. Owing to the expressiveness of second-order logic, program synthesis has an extremely broad range of applications. We survey some of these applications as well as recent trends in the algorithms that solve the program synthesis problem. In particular, we focus on an approach that has raised the profile of program synthesis and ushered in a generation of new synthesis tools, namely counter-example-guided inductive\u00a0\u2026", "num_citations": "21\n", "authors": ["1024"]}
{"title": "Computing over-approximations with bounded model checking\n", "abstract": " Abstract Bounded Model Checking (BMC) searches for counterexamples to a property \u03d5 with a bounded length k. If no such counterexample is found, k is increased. This process terminates when k exceeds the completeness threshold CT (ie, k is sufficiently large to ensure that no counterexample exists) or when the SAT procedure exceeds its time or memory bounds. However, the completeness threshold is too large for most practical instances or too hard to compute. Hardware designers often modify their designs for better verification and testing results. This paper presents an automated technique based on cut-point insertion to obtain an over-approximation of the model that 1) preserves safety properties and 2) has a CT which is small enough to actually prove \u03d5 using BMC. The algorithm uses proof-based abstraction refinement to remove spurious counterexamples.", "num_citations": "21\n", "authors": ["1024"]}
{"title": "Predicate abstraction and refinement techniques for verifying Verilog\n", "abstract": " Model checking techniques applied to large industrial circuits suffer from the state explosion problem. A major technique to address this problem is abstraction. Predicate abstraction has been applied successfully to large software programs. Applying this technique to hardware designs poses additional challenges. This paper evaluates three techniques to improve the performance of SAT-based predicate abstraction of circuits 1 We partition the abstraction problem by forming subsets of the predicates. The resulting abstractions are more coarse, but the computation of the abstract transition relation becomes easier. 2 We evaluate the performance effect of lazy abstraction, ie, the abstraction is only performed if required by a spurious counterexample. 3 We use weakest preconditions of circuit transitions in order to obtain new predicates during refinement. We provide experimental results on publicly available benchmarks from the Texas97 benchmark suite.Descriptors:", "num_citations": "21\n", "authors": ["1024"]}
{"title": "Using program synthesis for program analysis\n", "abstract": " In this paper, we propose a unified framework for designing static analysers based on program synthesis. For this purpose, we identify a fragment of second-order logic with restricted quantification that is expressive enough to capture numerous static analysis problems (e.g. safety proving, bug finding, termination and non-termination proving, superoptimisation). We call this fragment the synthesis fragment. We build a decision procedure for the synthesis fragment over finite domains in the form of a program synthesiser. Given our initial motivation to solve static analysis problems, this synthesiser is specialised for such analyses. Our experimental results show that, on benchmarks capturing static analysis problems, our program synthesiser compares positively with other general purpose synthesisers.", "num_citations": "20\n", "authors": ["1024"]}
{"title": "Faster linearizability checking via p-compositionality\n", "abstract": " Linearizability is a well-established consistency and correctness criterion for concurrent data types. An important feature of linearizability is Herlihy and Wing\u2019s locality principle, which says that a concurrent system is linearizable if and only if all of its constituent parts (so-called objects) are linearizable. This paper presents P-compositionality, which generalizes the idea behind the locality principle to operations on the same concurrent data type. We implement P-compositionality in a novel linearizability checker. Our experiments with over nine implementations of concurrent sets, including Intel\u2019s TBB library, show that our linearizability checker is one order of magnitude faster and/or more space efficient than the state-of-the-art algorithm.", "num_citations": "20\n", "authors": ["1024"]}
{"title": "Context-aware counter abstraction\n", "abstract": " The trend towards multi-core computing has made concurrent software an important target of computer-aided verification. Unfortunately, Model Checkers for such software suffer tremendously from combinatorial state space explosion. We show how to apply counter abstraction to real-world concurrent programs to factor out redundancy due to thread replication. The traditional global state representation as a vector of local states is replaced by a vector of thread counters, one per local state. In practice, straightforward implementations of this idea are unfavorably sensitive to the number of local states. We present a novel symbolic exploration algorithm that avoids this problem by carefully scheduling which counters to track at any moment during the search. We have carried out experiments on Boolean programs, an abstraction promoted by the success of the Slam project. The experiments give evidence of the\u00a0\u2026", "num_citations": "20\n", "authors": ["1024"]}
{"title": "Verification and falsification of programs with loops using predicate abstraction\n", "abstract": " Predicate abstraction is a major abstraction technique for the verification of software. Data is abstracted by means of Boolean variables, which keep track of predicates over the data. In many cases, predicate abstraction suffers from the need for at least one predicate for each iteration of a loop construct in the program. We propose to extract looping counterexamples from the abstract model, and to parametrise the simulation instance in the number of loop iterations. We present a novel technique that speeds up the detection of long counterexamples as well as the verification of programs with loops.", "num_citations": "20\n", "authors": ["1024"]}
{"title": "Approximation refinement for interpolation-based model checking\n", "abstract": " Model checking using Craig interpolants provides an effective method for computing an over-approximation of the set of reachable states using a SAT solver. This method requires proofs of unsatisfiability from the SAT solver to progress. If an over-approximation leads to a satisfiable formula, the computation restarts using more constraints and the previously computed approximation is not reused. Though the new formula eliminates spurious counterexamples of a certain length, there is no guarantee that the subsequent approximation is better than the one previously computed. We take an abstract, approximation-oriented view of interpolation based model checking. We study counterexample-free approximations, which are neither over- nor under-approximations of the set of reachable states but still contain enough information to conclude if counterexamples exist. Using such approximations, we devise a\u00a0\u2026", "num_citations": "20\n", "authors": ["1024"]}
{"title": "Don\u2019t sit on the fence: A static analysis approach to automatic fence insertion\n", "abstract": " Modern architectures rely on memory fences to prevent undesired weakenings of memory consistency. As the fences\u2019 semantics may be subtle, the automation of their placement is highly desirable. But precise methods for restoring consistency do not scale to deployed systems\u2019 code. We choose to trade some precision for genuine scalability: our technique is suitable for large code bases. We implement it in our new musketeer tool and report experiments on more than 700 executables from packages found in Debian GNU/Linux 7.1, including memcached with about 10,000 LoC.", "num_citations": "19\n", "authors": ["1024"]}
{"title": "Lost in abstraction: Monotonicity in multi-threaded programs\n", "abstract": " Monotonicity in concurrent systems stipulates that, in any global state, extant system actions remain executable when new processes are added to the state. This concept is not only natural and common in multi-threaded software, but also useful: if every thread\u2019s memory is finite, monotonicity often guarantees the decidability of safety property verification even when the number of running threads is unknown. In this paper, we show that the act of obtaining finite-data thread abstractions for model checking can be at odds with monotonicity: Predicate-abstracting certain widely used monotone software results in non-monotone multi-threaded Boolean programs \u2014 the monotonicity is lost in the abstraction. As a result, well-established sound and complete safety checking algorithms become inapplicable; in fact, safety checking turns out to be undecidable for the obtained class of unbounded-thread\u00a0\u2026", "num_citations": "19\n", "authors": ["1024"]}
{"title": "Abstract satisfaction\n", "abstract": " This article introduces an abstract interpretation framework that codifies the operations in SAT and SMT solvers in terms of lattices, transformers and fixed points. We develop the idea that a formula denotes a set of models in a universe of structures. This set of models has characterizations as fixed points of deduction, abduction and quantification transformers. A wide range of satisfiability procedures can be understood as computing and refining approximations of such fixed points. These include procedures in the DPLL family, those for preprocessing and inprocessing in SAT solvers, decision procedures for equality logics, weak arithmetics, and procedures for approximate quantification. Our framework provides a unified, mathematical basis for studying and combining program analysis and satisfiability procedures. A practical benefit of our work is a new, logic-agnostic architecture for implementing solvers.", "num_citations": "19\n", "authors": ["1024"]}
{"title": "Boom: Taking Boolean Program Model Checking One Step Further\n", "abstract": " We present Boom, a comprehensive analysis tool for Boolean programs. We focus in this paper on model-checking non-recursive concurrent programs. Boom implements a recent variant of counter abstraction, where thread counters are used in a program-context aware way. While designed for bounded counters, this method also integrates well with the Karp-Miller tree construction for vector addition systems, resulting in a reachability engine for programs with unbounded thread creation. The concurrent version of Boom is implemented using BDDs and includes partial order reduction methods. Boom is intended for model checking system-level code via predicate abstraction. We present experimental results for the verification of Boolean device driver models.", "num_citations": "19\n", "authors": ["1024"]}
{"title": "SAT-based summarization for boolean programs\n", "abstract": " Boolean programs are frequently used to model abstractions of software programs. They have the advantage that reachability properties are decidable, despite the fact that their stack is not bounded. The enabling technique is summarization of procedure calls. Most model checking tools for Boolean programs use BDDs to represent these summaries, allowing for efficient fix-point detection. However, BDDs are highly sensitive to the number of state variables. We present an approach to over-approximate summaries using Bounded Model Checking. Our technique is based on a SAT solver and requires only few calls to a QBF solver for fix-point detection. Our benchmarks show that our implementation is able handle a larger number of variables than BDD-based algorithms on some examples.", "num_citations": "19\n", "authors": ["1024"]}
{"title": "ANSI-C bounded model checker user manual\n", "abstract": " We describe a tool that formally verifies ANSI-C programs. The tool implements a technique called Bounded Model Checking (BMC). In BMC, the transition relation for a complex state machine and its specification are jointly unwound to obtain a Boolean formula, which is then checked for satisfiability by using a SAT procedure. The tool supports all ANSI-C integer operators and all pointer constructs allowed by the ANSIC standard, including dynamic memory allocation, pointer arithmetic, and pointer type casts.This research was sponsored by the Semiconductor Research Corporation (SRC) under contract no. 99-TJ-684, the National Science Foundation (NSF) under grant no. CCR-9803774, the Office of Naval Research (ONR), the Naval Research Laboratory (NRL) under contract no. N00014-01-1-0796, and by the Defense Advanced Research Projects Agency and the Army Research Office (ARO) under contract no. DAAD19-01-1-0485. The views and conclusions contained in this document are those of the author and should not be interpreted as representing the official policies, either expressed or implied, of SRC, NSF, ONR, NRL, DOD, ARO, or the US government.", "num_citations": "19\n", "authors": ["1024"]}
{"title": "Coverage in interpolation-based model checking\n", "abstract": " Coverage is a means to quantify the quality of a system specification, and is frequently applied to assess progress in system validation. Coverage is a standard measure in testing, but is very difficult to compute in the context of formal verification. We present efficient algorithms for identifying those parts of the system that are covered by a given property. Our algorithm is integrated into state-of-the-art SAT-based Model Checking using Craig interpolation. The key insight of our algorithm is to re-use previously computed inductive invariants and counterexamples. This re-use permits a quick conclusion of the vast majority of tests, and enables the computation of a coverage measure with 96% accuracy with only 5x the runtime of the Model Checker.", "num_citations": "18\n", "authors": ["1024"]}
{"title": "Correct hardware by synthesis from PVS\n", "abstract": " We present a methodology for obtaining provably correct hardware. We model and prove complex hardware in the theorem proving system PVS. The hardware is synthesized by translating the PVS designs into synthesizeable Verilog HDL by means of an automated tool. The paper describes how we model both combinatorial and clocked circuits in the theorem prover. We describe how the tool translates the hardware to Verilog. Finally, we give experimental results, which include the translation of a complete and provably correct IEEE floating point unit. The FPU has been implemented and tested on a Xilinx FPGA.", "num_citations": "18\n", "authors": ["1024"]}
{"title": "Proving the correctness of a complete microprocessor\n", "abstract": " This paper presents status results of a microprocessor verification project. The authors verify a complete 32-bit RISC microprocessor including the floating point unit and the control logic of the pipeline. The paper describes a formal definition of a \u201ccorrect\u201d microprocessor. This correctness criterion is proven for an implementation using formal methods. All proofs are verified mechanically by means of the theorem proving system PVS.", "num_citations": "18\n", "authors": ["1024"]}
{"title": "Digitaltechnik: Eine praxisnahe Einf\u00fchrung\n", "abstract": " Dieses Einf\u00fchrungswerk in die Digitaltechnik wurde speziell f\u00fcr Bachelorstudenten entwickelt. Es enth\u00e4lt viele auf den Anf\u00e4nger zugeschnittene praktische Anwendungen. Folgende Aspekte sind einmalig: Tool-orientierter Ansatz-Verwendung der Hardwarebeschreibungssprache Verilog-Einf\u00fchrung in systematische Methoden zur Fehlersuche-Geringe Anforderungen an die mathematischen Vorkenntnisse-Ein vereinfachter X86 IA32-Prozessor als Anwendungsbeispiel Die vorgestellten Beispiele werden mit Hilfe von Tools wie XILINX ISE und MentorGraphics ModelSim in echte Schaltungen umgesetzt. Diese Tools werden auch im industriellen Alltag eingesetzt. Im Internet werden weitere \u00dcbungen, realisierte Beispiele sowie Animationen angeboten. F\u00fcr Dozenten stehen Folien zum Abrufen bereit.", "num_citations": "17\n", "authors": ["1024"]}
{"title": "An abstract interpretation of DPLL (T)\n", "abstract": " dpll(t) is a central algorithm for Satisfiability Modulo Theories (smt) solvers. The algorithm combines results of reasoning about the Boolean structure of a formula with reasoning about conjunctions of theory facts to decide satisfiability. This architecture enables modern solvers to combine the performance benefits of propositional satisfiability solvers and conjunctive theory solvers. We characterise dpll(t) as an abstract interpretation algorithm that computes a product of two abstractions. Our characterisation allows a new understanding of dpll(t) as an instance of an abstract procedure to combine reasoning engines beyond propositional solvers and conjunctive theory solvers. In addition, we show theoretically that the split into Boolean and theory reasoning is sometimes unnecessary and demonstrate empirically that it can be detrimental to performance.", "num_citations": "16\n", "authors": ["1024"]}
{"title": "An interpolating decision procedure for transitive relations with uninterpreted functions\n", "abstract": " We present a proof-generating decision procedure for the quantifier-free fragment of first-order logic with the relations =, \u2260, \u2265, and > and argue that this logic, augmented with a set of theory-specific rewriting rules, is adequate for bit-level accurate verification. We describe our decision procedure from an algorithmic point of view and explain how it is possible to efficiently generate Craig interpolants for this logic.               Furthermore, we discuss the relevance of the logical fragment in software model checking and provide a preliminary evaluation of its applicability using an interpolation-based program analyser.", "num_citations": "16\n", "authors": ["1024"]}
{"title": "Program analysis with interpolants\n", "abstract": " This dissertation discusses novel techniques for interpolation-based software model checking, an approximate method which uses Craig interpolation to compute invariants of programs. Our work addresses two aspects of program analyses based on model checking: verification (the construction of correctness proofs for programs) and falsification (the detection of counterexamples that violate the specification).In Hoare\u2019s calculus, a proof of correctness comprises assertions which establish that a program adheres to its specification. The principal challenge is to derive appropriate assertions and loop invariants. Contemporary software verification tools use Craig interpolation (as opposed to traditional predicate transformers such as the weakest precondition) to derive approximate assertions. The performance of the model checker is contingent on the Craig interpolants computed. We present novel interpolation techniques which provide the following advantages over existing methods. Firstly, the resulting interpolants are sound with respect to the bit-level semantics of programs, which is an improvement over interpolation systems that use linear arithmetic over the reals to approximate bit-vector arithmetic and/or do not support bit-level operations. Secondly, our interpolation systems afford us a choice of interpolants and enable us to finetune their logical strength and structure. In contrast, existing procedures are limited to a single ad hoc choice of an interpolant. Interpolation-based verification tools are typically forced to refine an initial approximation repeatedly in order to achieve the accuracy required to establish or refute the correctness of a\u00a0\u2026", "num_citations": "16\n", "authors": ["1024"]}
{"title": "Property-driven fence insertion using reorder bounded model checking\n", "abstract": " Modern architectures provide weaker memory consistency guarantees than sequential consistency. These weaker guarantees allow programs to exhibit behaviours where the program statements appear to have executed out of program order. Fortunately, modern architectures provide memory barriers (fences) to enforce the program order between a pair of statements if needed. Due to the intricate semantics of weak memory models, the placement of fences is challenging even for experienced programmers. Too few fences lead to bugs whereas overuse of fences results in performance degradation. This motivates automated placement of fences. Tools that restore sequential consistency in the program may insert more fences than necessary for the program to be correct. Therefore, we propose a property-driven technique that introduces reorder-bounded exploration to identify the smallest number of\u00a0\u2026", "num_citations": "15\n", "authors": ["1024"]}
{"title": "SCRATCH: a tool for automatic analysis of DMA races\n", "abstract": " We present the SCRATCH tool, which uses bounded model checking and k-induction to automatically analyse software for multicore processors such as the Cell BE, in order to detect DMA races.", "num_citations": "15\n", "authors": ["1024"]}
{"title": "Automatic generation of propagation complete SAT encodings\n", "abstract": " Almost all applications of SAT solvers generate Boolean formulae from higher level expression graphs by encoding the semantics of each operation or relation into propositional logic. All non-trivial relations have many different possible encodings and the encoding used can have a major effect on the performance of the system. This paper gives an abstract satisfaction based formalisation of one aspect of encoding quality, the propagation strength, and shows that propagation complete SAT encodings can be modelled by our formalism and automatically computed for key operations. This allows a more rigorous approach to designing encodings as well as improved performance.", "num_citations": "14\n", "authors": ["1024"]}
{"title": "ExpliSat: Guiding SAT-based software verification with explicit states\n", "abstract": " We present a hybrid method for software model checking that combines explicit-state and symbolic techniques. Our method traverses the control flow graph of the program explicitly, and encodes the data values in a CNF formula, which we solve using a SAT solver. In order to avoid traversing control flow paths that do not correspond to a valid execution of the program we introduce the idea of a representative of a control path. We present favorable experimental results, which show that our method scales well both with regards to the non-deterministic data and the number of threads.", "num_citations": "14\n", "authors": ["1024"]}
{"title": "Efficient verification of multi-property designs (The benefit of wrong assumptions)\n", "abstract": " We consider the problem of efficiently checking a set of safety properties P 1 ,...,P k  of one design. We introduce a new approach called JA-verification, where JA stands for \u201cJustAssume\u201d (as opposed to \u201cassume-guarantee\u201d). In this approach, when proving a property P i , one assumes that every property P j  for j \u2260 i holds. The process of proving properties either results in showing that P 1 ,...,P k  hold without any assumptions or finding a \u201cdebugging set\u201d of properties. The latter identifies a subset of failed properties that are the first to break. The design behaviors that cause the properties in the debugging set to fail must be fixed first. Importantly, in our approach, there is no need to prove the assumptions used. We describe the theory behind our approach and report experimental results that demonstrate substantial gains in performance, especially in the cases where a small debugging set exists.", "num_citations": "13\n", "authors": ["1024"]}
{"title": "Danger invariants\n", "abstract": " Static analysers search for overapproximating proofs of safety commonly known as safety invariants. Conversely, static bug finders (e.g.\u00a0Bounded Model Checking) give evidence for the failure of an assertion in the form of a counterexample trace. As opposed to safety invariants, the size of a counterexample is dependent on the depth of the bug, i.e., the length of the execution trace prior to the error state, which also determines the computational effort required to find them. We propose a way of expressing danger proofs that is independent of the depth of bugs. Essentially, such danger proofs constitute a compact representation of a counterexample trace, which we call a danger invariant. Danger invariants summarise sets of traces that are guaranteed to be able to reach an error state. Our conjecture is that such danger proofs will enable the design of bug finding analyses for which the computational effort is\u00a0\u2026", "num_citations": "13\n", "authors": ["1024"]}
{"title": "On partial order semantics for SAT/SMT-based symbolic encodings of weak memory concurrency\n", "abstract": " Concurrent systems are notoriously difficult to analyze, and technological advances such as weak memory architectures greatly compound this problem. This has renewed interest in partial order semantics as a theoretical foundation for formal verification techniques. Among these, symbolic techniques have been shown to be particularly effective at finding concurrency-related bugs because they can leverage highly optimized decision procedures such as SAT/SMT solvers. This paper gives new fundamental results on partial order semantics for SAT/SMT-based symbolic encodings of weak memory concurrency. In particular, we give the theoretical basis for a decision procedure that can handle a fragment of concurrent programs endowed with least fixed point operators. In addition, we show that a certain partial order semantics of relaxed sequential consistency is equivalent to the conjunction of three\u00a0\u2026", "num_citations": "13\n", "authors": ["1024"]}
{"title": "Towards a classification of Hamiltonian cycles in the 6-cube\n", "abstract": " In this paper, we consider the problem of classifying Hamiltonian cycles in a binary hypercube. Previous work proposed a classification of these cycles using the edge representation, and presented it for dimension 4. We classify cycles in two further dimensions using a reduction to propositional SAT. Our proposed algorithm starts with an over-approximation of the set of equivalence classes, which is then refined using queries to a SAT-solver to remove spurious cycles. Our method performs up to three orders of magnitude faster than an enumeration with symmetry breaking in the 5-cube.", "num_citations": "13\n", "authors": ["1024"]}
{"title": "Accelerating invariant generation\n", "abstract": " Acceleration is a technique for summarising loops by computing a closed-form representation of the loop behaviour. The closed form can be turned into an accelerator, which is a code snippet that skips over intermediate states of the loop to the end of the loop in a single step. Program analysers rely on invariant generation techniques to reason about loops. The state-of-the-art invariant generation techniques, in practice, often struggle to find concise loop invariants, and, instead, degrade into unrolling loops, which is ineffective for non-trivial programs. In this paper, we evaluate experimentally whether loop accelerators enable existing program analysis algorithm to discover loop invariants more reliably and more efficiently. This paper is the first comprehensive study on the synergies between acceleration and invariant generation. We report our experience with a collection of safe and unsafe programs drawn from the\u00a0\u2026", "num_citations": "12\n", "authors": ["1024"]}
{"title": "Strengthening induction-based race checking with lightweight static analysis\n", "abstract": " Direct Memory Access (DMA) is key to achieving high performance in system-level software for multicore processors such as the Cell Broadband Engine. Incorrectly orchestrated DMAs cause DMA races, leading to subtle bugs that are hard to reproduce and fix. In previous work, we have shown that k-induction yields an effective method for proving absence of a restricted class of DMA races. We extend this work to handle a larger class of DMA races. We show that the applicability of k-induction can be significantly improved when combined with three inexpensive static analyses: 1)\u00a0abstract-interpretation-based static analysis; 2)\u00a0chunking, a domain-specific invariant generation technique; and 3)\u00a0code transformations based on statement independence. Our techniques are implemented in the SCRATCH tool. We evaluate our work on industrial benchmarks.", "num_citations": "12\n", "authors": ["1024"]}
{"title": "An efficient SAT encoding of circuit codes\n", "abstract": " Circuit codes in hypercubes are generalized snake-in-the-box codes and are used in analog-to-digital conversion devices. The construction of the longest known circuit codes is based on either an exhaustive search or an algorithm that restricts the search to the codes with periodic coordinate sequences. In this paper, we describe an efficient SAT encoding of circuit codes, which enabled us to obtain new circuit codes.", "num_citations": "12\n", "authors": ["1024"]}
{"title": "Method and system to check correspondence between different representations of a circuit\n", "abstract": " A method to check correspondence between different representations of a circuit may include abstracting a first computer language representation of the circuit to form a first abstract model of the circuit and abstracting a second computer language representation of the circuit to form a second abstract model of the circuit. The method may also include forming a product machine using the first and second abstract models. The method may further include checking correspondence between the first and second abstract models using the product machine.", "num_citations": "12\n", "authors": ["1024"]}
{"title": "Application specific higher order logic theorem proving\n", "abstract": " Theorem proving allows the formal verification of the correctness of very large systems. In order to increase the acceptance of theorem proving systems during the design process, we implemented higher order logic proof systems for ANSI-C and Verilog within a framework for application specific proof systems. Furthermore, we implement the language of the PVS theorem prover as well-established higher order specification language. The tool allows the verification of the design languages using a PVS specification and the verification of hardware designs using a C program as specification. We implement powerful decision procedures using Model Checkers and satisfiability checkers. We provide experimental results that compare the performance of our tool with PVS on large industrial scale hardware examples.", "num_citations": "12\n", "authors": ["1024"]}
{"title": "Proving the correctness of processors with delayed branch using delayed PC\n", "abstract": " We show that the programming model of delayed branch is equivalent to what we call delayed PC: all instruction fetches are delayed by one instruction, not just taken branches. This leads to a very simple new implementation of the delayed branch mechanism. We then prove the correctness of a pipelined machine with delayed PC.", "num_citations": "12\n", "authors": ["1024"]}
{"title": "Proving safety with trace automata and bounded model checking\n", "abstract": " Loop under-approximation enriches C programs with additional branches that represent the effect of a (limited) range of loop iterations. While this technique can speed up bug detection significantly, it introduces redundant execution traces which may complicate the verification of the program. This holds particularly true for tools based on Bounded Model Checking, which incorporate simplistic heuristics to determine whether all feasible iterations of a loop have been considered.                 We present a technique that uses trace automata to eliminate redundant executions after performing loop acceleration. The method reduces the diameter of the program under analysis, which is in certain cases sufficient to allow a safety proof using Bounded Model Checking. Our transformation is precise\u2013it does not introduce false positives, nor does it mask any errors. We have implemented the analysis as a source-to\u00a0\u2026", "num_citations": "11\n", "authors": ["1024"]}
{"title": "Periodic orbits and equilibria in glass models for gene regulatory networks\n", "abstract": " Glass models are frequently used to model gene regulatory networks. A distinct feature of the Glass model is that its dynamics can be formalized as paths through multi-dimensional binary hypercubes. In this paper, we report a broad range of results about Glass models that have been obtained by computing the binary codes that correspond to the hypercube paths. Specifically, we propose algorithmic methods for the synthesis of specific Glass networks based on these codes. In contrast to existing work, bi-periodic networks and networks possessing both stable equilibria and periodic trajectories are considered. The robustness of the attractor is also addressed, which gives rise to hypercube paths with nondominated nodes and double coils. These paths correspond to novel combinatorial problems, for which initial experimental results are presented. Finally, a classification of Glass networks with respect to their\u00a0\u2026", "num_citations": "11\n", "authors": ["1024"]}
{"title": "Formal verification of the VAMP microprocessor (project status)\n", "abstract": " Microprocessors are in use in many safety-critical environments, such as cars or planes. We therefore consider the correctness of such components as a matter of vital importance. Testing microprocessors is limited by the huge state space of modern microprocessors. We therefore think formal verification is the sole way to obtain a correctness guarantee.At Saarland University, we are currently working on a project aiming to formally verify the correctness of a complete microprocessor called VAMP. The VAMP (Verified Architecture Microprocessor) is a variant of the DLX processor [11]. It features a Tomasulo-scheduled 5-stage pipeline, precise interrupts, delayed branch, virtual memory management, cache memory, and a fully IEEE compliant dual-precision floating point unit that handles denormals and exceptions entirely in hardware. The specification and verification is performed on the gate level using the PVS theorem proving system [25]. Our group has developed a tool which automatically translates hardware specifications from the PVS language to Verilog HDL. This enables us to translate the VAMP to Verilog and synthesize it on a Xilinx FPGA [7]. This paper provides an overview of the VAMP project. We sketch the proof techniques used in the verification of the different VAMP components.", "num_citations": "11\n", "authors": ["1024"]}
{"title": "Abstract interpretation with unfoldings\n", "abstract": " We present and evaluate a technique for computing path-sensitive interference conditions during abstract interpretation of concurrent programs. In lieu of fixed point computation, we use prime event structures to compactly represent causal dependence and interference between sequences of transformers. Our main contribution is an unfolding algorithm that uses a new notion of independence to avoid redundant transformer application, thread-local fixed points to reduce the size of the unfolding, and a novel cutoff criterion based on subsumption to guarantee termination of the analysis. Our experiments show that the abstract unfolding produces an order of magnitude fewer false alarms than a mature abstract interpreter, while being several orders of magnitude faster than solver-based tools that have the same precision.", "num_citations": "10\n", "authors": ["1024"]}
{"title": "Bit-precise procedure-modular termination analysis\n", "abstract": " Non-termination is the root cause of a variety of program bugs, such as hanging programs and denial-of-service vulnerabilities. This makes an automated analysis that can prove the absence of such bugs highly desirable. To scale termination checks to large systems, an interprocedural termination analysis seems essential. This is a largely unexplored area of research in termination analysis, where most effort has focussed on small but difficult single-procedure problems. We present a modular termination analysis for C programs using template-based interprocedural summarisation. Our analysis combines a context-sensitive, over-approximating forward analysis with the inference of under-approximating preconditions for termination. Bit-precise termination arguments are synthesised over lexicographic linear ranking function templates. Our experimental results show the advantage of interprocedural reasoning\u00a0\u2026", "num_citations": "9\n", "authors": ["1024"]}
{"title": "Proving the correctness of pipelined micro-architectures\n", "abstract": " This paper presents how to generate the implementation of a pipelined microprocessor from an arbitrary sequential specification. All necessary forwarding and stalling logic is created automatically. The implementation is provided in the language of the theorem proving system (PVS). This implementation is translated to the Verilog hardware description language. Furthermore, a mathematical correctness proof for the machine is supplied. This proof is verified by the theorem proving system.", "num_citations": "9\n", "authors": ["1024"]}
{"title": "Towards verifiable and safe model-free reinforcement learning\n", "abstract": " Reinforcement Learning (RL) is a widely employed machine learning architecture that has been applied to a variety of decision-making problems, from resource management to robot locomotion, from recommendation systems to systems biology, and from traffic control to superhuman-level gaming. However, RL has experienced limited success beyond rigidly controlled or constrained applications, and successful employment of RL in safety-critical scenarios is yet to be achieved. A principal reason for this limitation is the lack of formal approaches to specify requirements as tasks and learning constraints, and to provide guarantees with respect to these requirements and constraints, during and after learning. This line of work addresses these issues by proposing a general framework that leverages the success of RL in learning high-performance controllers, while guaranteeing the satisfaction of given requirements and guiding the learning process within safe configurations.", "num_citations": "8\n", "authors": ["1024"]}
{"title": "Program synthesis for program analysis\n", "abstract": " In this article, we propose a unified framework for designing static analysers based on program synthesis. For this purpose, we identify a fragment of second-order logic with restricted quantification that is expressive enough to model numerous static analysis problems (e.g., safety proving, bug finding, termination and non-termination proving, refactoring). As our focus is on programs that use bit-vectors, we build a decision procedure for this fragment over finite domains in the form of a program synthesiser. We\u00a0provide instantiations of our framework for solving a diverse range of program verification tasks such as termination, non-termination, safety and bug finding, superoptimisation, and refactoring. Our experimental results show that our program synthesiser compares positively with specialised tools in each area as well as with general-purpose synthesisers.", "num_citations": "8\n", "authors": ["1024"]}
{"title": "Second-order SAT solving using program synthesis\n", "abstract": " Program synthesis is the automated construction of software from a specification. While program synthesis is undecidable in general, we show that synthesising finite-state programs is NEXPTIME-complete. We then present a fully automatic, sound and complete algorithm for synthesising C programs from a specification written in C. Our approach uses a combination of bounded model checking, explicit-state model checking and genetic programming to achieve surprisingly good performance for a problem with such high complexity. By identifying a correspondence between program synthesis and secondorder logic, we show how to use our program synthesiser as a decision procedure for existential second-order logic over finite domains. We illustrate the expressiveness of this logic by encoding several program analysis problems including superoptimisation, de-obfuscation, safety and termination. Finally, we present experimental results showing that our approach is tractable in practice.", "num_citations": "8\n", "authors": ["1024"]}
{"title": "Program Verification via Craig Interpolation for Presburger Arithmetic with Arrays.\n", "abstract": " Craig interpolation has become a versatile tool in formal verification, in particular for generating intermediate assertions in safety analysis and model checking. In this paper, we present a novel interpolation procedure for the theory of arrays, extending an interpolating calculus for the full theory of quantifier-free Presburger arithmetic, which will be presented at IJCAR this year. We investigate the use of this procedure in a software model checker for C programs. A distinguishing feature of the model checker is its ability to faithfully model machine arithmetic with an encoding into Presburger arithmetic with uninterpreted predicates. The interpolation procedure allows the synthesis of quantified invariants about arrays. This paper presents work in progress; we include initial experiments to demonstrate the potential of our method.", "num_citations": "8\n", "authors": ["1024"]}
{"title": "Strengthening properties using abstraction refinement\n", "abstract": " Model checking is an automated formal method for verifying whether a finite-state system satisfies a user-supplied specification. The usefulness of the verification result depends on how well the specification distinguishes intended from non-intended system behavior. Vacuity is a notion that helps formalize this distinction in order to improve the user's understanding of why a property is satisfied. The goal of this paper is to expose vacuity in a property in a way that increases our knowledge of the design. Our approach, based on abstraction refinement, computes a maximal set of atomic subformula occurrences that can be strengthened without compromising satisfaction. The result is a shorter and stronger and thus, generally, more valuable property. We quantify the benefits of our technique on a substantial set of circuit benchmarks.", "num_citations": "8\n", "authors": ["1024"]}
{"title": "A complete bounded model checking algorithm for pushdown systems\n", "abstract": " Pushdown systems (PDSs) consist of a stack and a finite state machine and are frequently used to model abstractions of software. They correspond to sequential recursive programs with finite-domain variables. This paper presents a novel algorithm for deciding reachability of particular locations of PDSs. We exploit the fact that most PDSs used in practice are shallow, and propose to use SAT-based Bounded Model Checking to search for counterexamples. Completeness is achieved by computing universal summaries of the procedures in the program.", "num_citations": "8\n", "authors": ["1024"]}
{"title": "The Impact of Hardware Scheduling Mechanismus on the Performance and Cost of Processor Designs.\n", "abstract": " Hardware schedulers supporting out-of-order execution are widespread nowadays. Nevertheless, studies quantifying there impact on the performance and cost of processors are rare. The paper tries to close this gap. It turns out that the hardware schedulers can double the performance at a moderate increase (20%) in a processor's gate count. Earlier re-ordering of instructions allows for better performance, but it does not guarantee it. The lack of features like forwarding and non-blocking resources can nullify this gain. Despite of its out-of-order dispatch capability, the original Scoreboard scheduler, for example, performs signi cantly worse than a standard in-order pipeline. The paper also identi es the aspects responsible for this poor performance and quanti es their impact. The single most important aspect is the lack of result forwarding.", "num_citations": "8\n", "authors": ["1024"]}
{"title": "Satisfiability checking and symbolic computation\n", "abstract": " Symbolic Computation and Satisfiability Checking are viewed as individual research areas, but they share common interests in the development, implementation and application of decision procedures for arithmetic theories. Despite these commonalities, the two communities are currently only weakly connected. We introduce a new project SC2 to build a joint community in this area, supported by a newly accepted EU (H2020-FETOPEN-CSA) project of the same name. We aim to strengthen the connection between these communities by creating common platforms, initiating interaction and exchange, identifying common challenges, and developing a common roadmap. This abstract and accompanying poster describes the motivation and aims for the project, and reports on the first activities.", "num_citations": "7\n", "authors": ["1024"]}
{"title": "Static program analysis for identifying energy bugs in graphics-intensive mobile apps\n", "abstract": " A major drawback of mobile devices is limited battery life. Apps that use graphics are especially energy greedy and developers must invest significant effort to make such apps energy efficient. We propose a novel static optimization technique for eliminating drawing commands to produce energy-efficient apps. The key insight we exploit is that the static analysis is able to predict future behavior of the app, and we give three exemplars that demonstrate the value of this approach. Firstly, loop invariant texture analysis identifies repetitive texture transfers in the render loop so that they can be moved out of the loop and performed just once. Secondly, packing identifies images that are drawn together and therefore can be combined into a larger image to eliminate overhead associated with multiple smaller images. Finally, identical frames detection uses a combination of static and dynamic analysis to identify frames that\u00a0\u2026", "num_citations": "7\n", "authors": ["1024"]}
{"title": "Computer Aided Verification: 27th International Conference, CAV 2015, San Francisco, CA, USA, July 18-24, 2015, Proceedings, Part I\n", "abstract": " The two-volume set LNCS 9206 and LNCS 9207 constitutes the refereed proceedings of the 27th International Conference on Computer Aided Verification, CAV 2015, held in San Francisco, CA, USA, in July 2015. The total of 58 full and 11 short papers presented in the proceedings was carefully reviewed and selected from 252 submissions. The papers were organized in topical sections named: model checking and refinements; quantitative reasoning; software analysis; lightning talks; interpolation, IC3/PDR, and Invariants; SMT techniques and applications; HW verification; synthesis; termination; and concurrency.", "num_citations": "7\n", "authors": ["1024"]}
{"title": "Tutorial: Software model checking\n", "abstract": " Model Checking is an automated technique for the systematic exploration of the state space of a state transition system. The first part of the tutorial provides an introduction to the basic concepts of model checking, including BDD- and SAT-based symbolic model checking, partial order reduction, abstraction, and compositional verification. Model Checking has been applied sucessfully to hardware in the past. However, software has become the most complex part of safety ciritcal systems. The second part of the tutorial covers tools that use Model Checking to formally verify computer software.", "num_citations": "7\n", "authors": ["1024"]}
{"title": "Optimising Spectrum Based Fault Localisation for Single Fault Programs Using Specifications.\n", "abstract": " Spectrum based fault localisation determines how suspicious a line of code is with respect to being faulty as a function of a given test suite. Outstanding problems include identifying properties that the test suite should satisfy in order to improve fault localisation effectiveness subject to a given measure, and developing methods that generate these test suites efficiently.We address these problems as follows. First, when single bug optimal measures are being used with a single-fault program, we identify a formal property that the test suite should satisfy in order to optimise fault localisation. Second, we introduce a new method which generates test data that satisfies this property. Finally, we empirically demonstrate the utility of our implementation at fault localisation on sv-comp benchmarks and the tcas program, demonstrating that test suites can be generated in almost a second with a fault identified after inspecting under 1% of the program.", "num_citations": "6\n", "authors": ["1024"]}
{"title": "Probabilistic fault localisation\n", "abstract": " Efficient fault localisation is becoming increasingly important as software grows in size and complexity. In this paper we present a new formal framework, denoted probabilistic fault localisation (pfl), and compare it to the established framework of spectrum based fault localisation (sbfl). We formally prove that pfl satisfies some desirable properties which sbfl does not, empirically demonstrate that pfl is significantly more effective at finding faults than all known sbfl measures in large scale experimentation, and show pfl has comparable efficiency. Results show that the user investigates 37\u00a0% more code (and finds a fault immediately in 27\u00a0% fewer cases) when using the best performing sbfl measures, compared to the pfl framework. Furthermore, we show that it is theoretically impossible to design strictly rational sbfl measures that outperform pfl techniques on a large set of benchmarks.", "num_citations": "6\n", "authors": ["1024"]}
{"title": "Model and proof generation for heap-manipulating programs\n", "abstract": " Existing heap analysis techniques lack the ability to supply counterexamples in case of property violations. This hinders diagnosis, prevents test-case generation and is a barrier to the use of these tools among non-experts. We present a verification technique for reasoning about aliasing and reachability in the heap which uses ACDCL (a\u00a0combination of the well-known CDCL SAT algorithm and abstract interpretation) to perform interleaved proof generation and model construction. Abstraction provides us with a tractable way of reasoning about heaps; ACDCL adds the ability to search for a model in an efficient way. We present a prototype tool and demonstrate a number of examples for which we are able to obtain useful concrete counterexamples.", "num_citations": "6\n", "authors": ["1024"]}
{"title": "WOLVERINE: Battling bugs with interpolants\n", "abstract": " Wolverine is a software verifier that checks safety properties of sequential ANSI-C and C++ programs, deploying Craig interpolation to derive program invariants. We describe the underlying approach and the architecture, and provide instructions for installation and usage.", "num_citations": "6\n", "authors": ["1024"]}
{"title": "An algebraic algorithm for the identification of Glass networks with periodic orbits along cyclic attractors\n", "abstract": " Glass piecewise linear ODE models are frequently used for simulation of neural and gene regulatory networks. Efficient computational tools for automatic synthesis of such models are highly desirable. However, the existing algorithms for the identification of desired models are limited to four-dimensional networks, and rely on numerical solutions of eigenvalue problems. We suggest a novel algebraic criterion to detect the type of the phase flow along network cyclic attractors that is based on a corollary of the Perron-Frobenius theorem. We show an application of the criterion to the analysis of bifurcations in the networks. We propose to encode the identification of models with periodic orbits along cyclic attractors as a propositional formula, and solving it using state-of-the-art SAT-based tools for real linear arithmetic. New lower bounds for the number of equivalence classes are calculated for cyclic attractors in\u00a0\u2026", "num_citations": "6\n", "authors": ["1024"]}
{"title": "Method and system to verify a circuit design by verifying consistency between two different language representations of a circuit design\n", "abstract": " A method to verify a circuit design may include applying a bounded model checking technique to a first computer language representation of the circuit design and to a second computer language representation of the circuit design. The method may also include determining a behavioral consistency between the first and second computer language representations.", "num_citations": "6\n", "authors": ["1024"]}
{"title": "The taint rabbit: Optimizing generic taint analysis with dynamic fast path generation\n", "abstract": " Generic taint analysis is a pivotal technique in software security. However, it suffers from staggeringly high overhead. In this paper, we explore the hypothesis whether just-in-time (JIT) generation of fast paths for tracking taint can enhance the performance. To this end, we present the Taint Rabbit, which supports highly customizable user-defined taint policies and combines a JIT with fast context switching. Our experimental results suggest that this combination outperforms notable existing implementations of generic taint analysis and bridges the performance gap to specialized trackers. For instance, Dytan incurs an average overhead of 237x, while the Taint Rabbit achieves 1.7 x on the same set of benchmarks. This compares favorably to the 1.5 x overhead delivered by the bitwise, non-generic, taint engine LibDFT.", "num_citations": "5\n", "authors": ["1024"]}
{"title": "Lost in abstraction: Monotonicity in multi-threaded programs\n", "abstract": " Monotonicity in concurrent systems stipulates that, in any global state, system actions remain executable when new processes are added to the state. This concept is both natural and useful: if every thread's memory is finite, monotonicity often guarantees the decidability of safety properties even when the number of running threads is unknown. In this paper, we show that finite-data thread abstractions for model checking can be at odds with monotonicity: predicate-abstracting monotone software can result in non-monotone Boolean programs\u2014the monotonicity is lost in the abstraction. As a result, pertinent well-established safety checking algorithms for infinite-state systems become inapplicable. We demonstrate how monotonicity in the abstraction can be restored, without affecting safety properties. This improves earlier approaches of enforcing monotonicity via overapproximations. We implemented our solution in\u00a0\u2026", "num_citations": "5\n", "authors": ["1024"]}
{"title": "Independence abstractions and models of concurrency\n", "abstract": " Mathematical representations of concurrent systems rely on two fundamental notions: an atomic unit of behaviour called an event, and a constraint called independence which asserts that the order in which certain events occur does not affect the final configuration of the system. We apply abstract interpretation to study models of concurrency by treating events and independence as abstractions. Events arise as Boolean abstractions of traces. Independence is a parameter to an abstraction that adds certain permutations to a set of sequences of events. Our main result is that several models of concurrent system are a composition of an event abstraction and an independence specification. These models include Mazurkiewicz traces, pomsets, prime event structures, and transition systems with independence. These results establish the first connections between abstraction interpretation and event-based\u00a0\u2026", "num_citations": "5\n", "authors": ["1024"]}
{"title": "Propositional reasoning about safety and termination of heap-manipulating programs\n", "abstract": " This paper shows that it is possible to reason about the safety and termination of programs handling potentially cyclic, singly-linked lists using propositional reasoning even when the safety invariants and termination arguments depend on constraints over the lengths of lists. For this purpose, we propose the theory SLH of singly-linked lists with length, which is able to capture non-trivial interactions between shape and arithmetic. When using the theory of bit-vector arithmetic as background theory, SLH is efficiently decidable via a reduction to SAT. We show the utility of SLH for software verification by using it to express safety invariants and termination arguments for programs manipulating potentially cyclic, singly-linked lists with unrestricted, unspecified sharing. We also provide an implementation of the decision procedure and apply it to check safety and termination proofs for several heap-manipulating\u00a0\u2026", "num_citations": "5\n", "authors": ["1024"]}
{"title": "Computing mutation coverage in interpolation-based model checking\n", "abstract": " Coverage is a means to quantify the quality of a system specification, and is frequently applied to assess progress in system validation. Coverage is a standard measure in testing, but is very difficult to compute in the context of formal verification. We present efficient algorithms for identifying those parts of the system that are covered by a given property. Our algorithm is integrated into state-of-the-art Boolean satisfiability problem-based model checking using Craig interpolation. The key insight into our algorithm is the re-use of previously computed inductive invariants and counterexamples. This re-use permits a a rapid completion of the vast majority of tests, and enables the computation of a coverage measure with 96% accuracy with only 5\u00d7 the runtime of the model checker.", "num_citations": "5\n", "authors": ["1024"]}
{"title": "Restructuring resolution refutations for interpolation\n", "abstract": " Interpolants are the cornerstone of several approximate verification techniques. Current interpolation techniques restrict the search heuristics of the underlying decision procedure to compute interpolants, incurring a negative impact on performance, and apply primarily to the lazy proof explication framework. We bridge the gap between fast decision procedures that aggressively use propositional reasoning and slower interpolating decision procedures by extending the scope of the latter to non-lazy approaches and relaxing the restrictions on search heuristics. Both are achieved by combining a simple set of transformations on resolution refutations. Our experiments show that this method leads to speedups when computing interpolants and to reductions in proof size.", "num_citations": "5\n", "authors": ["1024"]}
{"title": "Kayak: Safe semantic refactoring to java streams\n", "abstract": " Refactorings are structured changes to existing software that leave its externally observable behaviour unchanged. Their intent is to improve readability, performance or other non-behavioural properties. State-of-the-art automatic refactoring tools are syntax-driven and, therefore, overly conservative. In this paper we explore semantics-driven refactoring, which enables much more sophisticated refactoring schemata. As an exemplar of this broader idea, we present Kayak, an automatic refactoring tool that transforms Java with external iteration over collections into code that uses Streams, a new abstraction introduced by Java 8. Our refactoring procedure performs semantic reasoning and search in the space of possible refactorings using automated program synthesis. Our experimental results support the conjecture that semantics-driven refactorings are more precise and are able to rewrite more complex code scenarios when compared to syntax-driven refactorings.", "num_citations": "4\n", "authors": ["1024"]}
{"title": "Functional requirements-based automated testing for avionics\n", "abstract": " We propose and demonstrate a method for the reduction of testing effort in safety-critical software development using DO-178 guidance. We achieve this through the application of Bounded Model Checking (BMC) to formal low-level requirements, in order to generate tests automatically that are good enough to replace existing labor-intensive test writing procedures while maintaining independence from implementation artefacts. Given that manual processes are often empirical and subjective, we begin by formally defining a metric, which extends recognized best practice from code coverage analysis strategies to generate tests that adequately cover the requirements. We then implement it in an automated requirements testing procedure and apply it in a case study with industrial partners. In review, the toolchain developed here is demonstrated to significantly reduce the human effort for the qualification of software\u00a0\u2026", "num_citations": "4\n", "authors": ["1024"]}
{"title": "Sound numerical computations in abstract acceleration\n", "abstract": " Soundness is a major objective for verification tools. Methods that use exact arithmetic or symbolic representations are often prohibitively slow and do not scale past small examples. We propose the use of numerical floating-point computations to improve performance combined with an interval analysis to ensure soundness in reach-set computations for numerical dynamical models. Since the interval analysis cannot provide exact answers we reason about over-approximations of the reachable sets that are guaranteed to contain the true solution of the problem. Our theory is implemented in a numerical algorithm for Abstract Acceleration in a tool called Axelerator. Experimental results show a large increase in performance while maintaining soundness of reachability results.", "num_citations": "4\n", "authors": ["1024"]}
{"title": "Formalizing and checking thread refinement for data-race-free execution models\n", "abstract": " When optimizing a thread in a concurrent program (either done manually or by the compiler), it must be guaranteed that the resulting thread is a refinement of the original thread. Most definitions of refinement are formulated in terms of valid syntactic transformations on the program code, or in terms of valid transformations on thread execution traces. We present a new theory formulated instead in terms of state transitions between synchronization operations. Our new method shows refinement in more cases and leads to more efficient and simpler procedures for refinement checking. We develop the theory for the SC-for-DRF execution model (using locks for synchronization), and show that its application in compiler testing yields speedups of on average more than two orders of magnitude compared to a previous approach.", "num_citations": "4\n", "authors": ["1024"]}
{"title": "Unbounded-time analysis of guarded LTI systems with inputs by abstract acceleration (extended version)\n", "abstract": " Linear Time Invariant (LTI) systems are ubiquitous in control applications. Unbounded-time reachability analysis that can cope with industrial-scale models with thousands of variables is needed. To tackle this problem, we use abstract acceleration, a method for unbounded-time polyhedral reachability analysis for linear systems. Existing variants of the method are restricted to closed systems, i.e., dynamical models without inputs or non-determinism. In this paper, we present an extension of abstract acceleration to linear loops with inputs, which correspond to discrete-time LTI control systems under guard conditions. The new method relies on a relaxation of the solution of the linear dynamical equation that leads to a precise over-approximation of the set of reachable states, which are evaluated using support functions. In order to increase scalability, we use floating-point computations and ensure soundness by interval arithmetic. Our experiments show that performance increases by several orders of magnitude over alternative approaches in the literature. In turn, this tremendous gain allows us to improve on precision by computing more expensive abstractions. We outperform state-of-the-art tools for unbounded-time analysis of LTI system with inputs in speed as well as in precision.", "num_citations": "4\n", "authors": ["1024"]}
{"title": "Verifying synchronous reactive systems using lazy abstraction\n", "abstract": " Embedded software systems are frequently modeled as a set of synchronous reactive processes. The transitions performed by the processes are given as sequential, atomic code blocks. Most existing verifiers flatten such programs into a global transition system, to be able to apply off-the-shelf verification methods. However, this monolithic approach fails to exploit the lock-step execution of the processes, severely limiting scalability. We present a novel formal verification technique that analyses synchronous concurrency explicitly rather than encoding it. We present a variant of Lazy Abstraction with Interpolants (LAwI), a technique successfully used in software verification, and tailor it to synchronous reactive concurrency. We exploit the synchronous communication structure by fixing an execution schedule, circumventing the exponential blow-up of state space caused by simulating synchronous behaviour by means\u00a0\u2026", "num_citations": "4\n", "authors": ["1024"]}
{"title": "Automated verification of concurrent software\n", "abstract": " Effective use of concurrency is key to accelerating computations in a post frequency-scaling era. We review a research programme aimed at automated formal verification of a broad variety of concurrent systems. We briefly survey different forms of asynchronous concurrent computations, with a focus on multi-threaded, multi-core computation. We then highlight semantic and scalability challenges that arise when applying automated reasoning technology to this class of software.             We then discuss two very different techniques to address the challenges in this domain. The key insight behind the first technique is to exploit the symmetry that is inherent in many concurrent software programs: the programs execute a parametric number of identical threads, operating on different input data. Awareness of this design principle enables the application of symmetry reduction techniques such as counter abstraction\u00a0\u2026", "num_citations": "4\n", "authors": ["1024"]}
{"title": "Fixed points for multi-cycle path detection\n", "abstract": " Accurate timing analysis is crucial for obtaining the optimal clock frequency, and for other design stages such as power analysis. Most methods for estimating propagation delay identify multi-cycle paths (MCPs), which allow timing to be relaxed, but ignore the set of reachable states, achieving scalability at the cost of a severe lack of precision. Even simple circuits contain paths affecting timing that can only be detected if the set of reachable states is considered. We examine the theoretical foundations of MCP identification and characterise the MCPs in a circuit by a fixed point equation. The optimal solution to this equation can be computed iteratively and yields the largest set of MCPs in a circuit. Further, we define conservative approximations of this set, show how different MCP identification methods in the literature compare in terms of precision, and show one method to be unsound. The practical application of these\u00a0\u2026", "num_citations": "4\n", "authors": ["1024"]}
{"title": "Behavioral Consistency of C and Verilog Programs\n", "abstract": " We present an algorithm that checks behavioral consistency between an ANSI-C program and a circuit given in Verilog using Bounded Model Checking. Both the circuit and the program are unwound and translated into a formula that is satisfiable if and only if the circuit and the code disagree. The formula is then checked using a SAT solver. We are able to translate C programs that make use of side effects, pointers, dynamic memory allocation, and loops with conditions that cannot be evaluated statically. We describe experimental results on various reactive circuits and programs, including a small processor given in Verilog and its Instruction Set Architecture given in ANSI-C.", "num_citations": "4\n", "authors": ["1024"]}
{"title": "Counterexample guided neural synthesis\n", "abstract": " Program synthesis is the generation of a program from a specification. Correct synthesis is difficult, and methods that provide formal guarantees suffer from scalability issues. On the other hand, neural networks are able to generate programs from examples quickly but are unable to guarantee that the program they output actually meets the logical specification. In this work we combine neural networks with formal reasoning: using the latter to convert a logical specification into a sequence of examples that guides the neural network towards a correct solution, and to guarantee that any solution returned satisfies the formal specification. We apply our technique to synthesising loop invariants and compare the performance to existing solvers that use SMT and existing techniques that use neural networks. Our results show that the formal reasoning based guidance improves the performance of the neural network substantially, nearly doubling the number of benchmarks it can solve.", "num_citations": "3\n", "authors": ["1024"]}
{"title": "Evaluating manual intervention to address the challenges of bug finding with KLEE\n", "abstract": " Symbolic execution has shown its ability to find security-relevant flaws in software, but faces significant scalability challenges. There is a commonly held belief that manual intervention by an expert can help alleviate these limiting factors. However, there has been little formal investigation of this idea. In this paper, we present our experiences applying the KLEE symbolic execution engine to a new bug corpus, and of using manual intervention to alleviate the issues encountered. Our contributions are (1) Hemiptera, a novel corpus of over 130 bugs in real world software, (2) a comprehensive evaluation of the KLEE symbolic execution engine on Hemiptera with a categorisation of frequently occurring software patterns that are problematic for symbolic execution, and (3) an evaluation of manual mitigations aimed at addressing the underlying issues of symbolic execution. Our experience shows that manual intervention can increase both code coverage and bug detection in many situations. It is not a silver bullet however, and we discuss its limitations and the challenges encountered.", "num_citations": "3\n", "authors": ["1024"]}
{"title": "Methods and measures for statistical fault localisation\n", "abstract": " Fault localisation is the process of finding the causes of a given error,  and is one of the most costly elements of software development. One of  the most efficient approaches to fault localisation appeals to statistical  methods. These methods are characterised by their ability to estimate  how faulty a program artefact is as a function of statistical information  about a given program and test suite. However, the major problem facing  statistical approaches is their effectiveness -- particularly with respect to  finding single (or multiple) faults in large programs typical to the real  world. A solution to this problem hinges on discovering new formal properties  of faulty programs and developing scalable statistical techniques which  exploit them. In this thesis I address this by identifying new properties  of faulty programs, developing the formal frameworks and methods which  are formally proven to exploit them, and demonstrating that many of  our new techniques substantially and statistically significantly outperform  competing algorithms at given fault localisation tasks (using p = 0.01) on  what (to our knowledge) is one of the largest scale set of experiments in  fault localisation to date. This research is thus designed to corroborate the following thesis statement: That the new algorithms presented in this thesis are effective and  efficient at software fault localisation and outperform state of the art statistical techniques at a range of fault localisation tasks. In more detail,  the major thesis contributions are as follows: 1. We perform a thorough investigation into the existing framework of  (sbfl), which currently stands at the cutting edge of statistical fault  localisation. To\u00a0\u2026", "num_citations": "3\n", "authors": ["1024"]}
{"title": "Safety verification and refutation by k-invariants and k-induction (extended version)\n", "abstract": " Most software verification tools can be classified into one of a number of established families, each of which has their own focus and strengths. For example, concrete counterexample generation in model checking, invariant inference in abstract interpretation and completeness via annotation for deductive verification. This creates a significant and fundamental usability problem as users may have to learn and use one technique to find potential problems but then need an entirely different one to show that they have been fixed. This paper presents a single, unified algorithm kIkI, which strictly generalises abstract interpretation, bounded model checking and k-induction. This not only combines the strengths of these techniques but allows them to interact and reinforce each other, giving a `single-tool' approach to verification.", "num_citations": "3\n", "authors": ["1024"]}
{"title": "Second-order propositional satisfiability\n", "abstract": " Fundamentally, every static program analyser searches for a proof through a combination of heuristics providing candidate solutions and a candidate validation technique. Essentially, the heuristic reduces a second-order problem to a first-order/propositional one, while the validation is often just a call to a SAT/SMT solver. This results in a monolithic design of such analyses that conflates the formulation of the problem with the solving process. Consequently, any change to the latter causes changes to the whole analysis. This design is dictated by the state of the art in solver technology. While SAT/SMT solvers have experienced tremendous progress, there are barely any second-order solvers. This paper takes a step towards addressing this situation by proposing a decidable fragment of second-order logic that is still expressive enough to capture numerous program analysis problems (e.g. safety proving, bug finding, termination and non-termination proving, superoptimisation). We refer to the satisfiability problem for this fragment as Second-Order SAT and show it is NEXPTIME-complete. Finally, we build a decision procedure for Second-Order SAT based on program synthesis and present experimental evidence that our approach is tractable for program analysis problems.", "num_citations": "3\n", "authors": ["1024"]}
{"title": "Abstraction of syntax\n", "abstract": " The theory of abstract interpretation is a conceptual framework for reasoning about approximation of semantics. We ask if the creative process of designing an approximation can be studied mathematically. Semantic approximations, whether studied in a purely mathematical setting, or implemented in a static analyser, must have a representation. We apply abstract interpretation to syntactic representations and study abstraction of syntax. We show that semantic abstractions and syntactic abstractions are different, and identify criteria for deriving semantic abstractions by purely syntactic means. As a case study, we show that descriptions of numeric abstract domains can be derived by abstraction of syntax.", "num_citations": "3\n", "authors": ["1024"]}
{"title": "Speeding Up Simulation of SystemC Using Model Checking\n", "abstract": " SystemC is a system-level modeling language that offers a wide range of features to describe concurrent systems. The SystemC standard permits simulators to implement a deterministic thread scheduling policy, which often hides concurrency-related design flaws. We present a novel compiler for SystemC that integrates a formal race analysis based on Model Checking techniques. The key insight to make the formal analysis scalable is to apply the Model Checker only to small partitions of the model. Our compiler produces a simulator that uses the race analysis information at runtime to perform partial-order reduction, thereby eliminating context switches that do not affect the result of the simulation. Experimental results show simulation speedups of one order of magnitude and better.", "num_citations": "3\n", "authors": ["1024"]}
{"title": "Finding lean induced cycles in binary hypercubes\n", "abstract": " Induced (chord-free) cycles in binary hypercubes have many applications in computer science. The state of the art for computing such cycles relies on genetic algorithms, which are, however, unable to perform a complete search. In this paper, we propose an approach to finding a special class of induced cycles we call lean, based on an efficient propositional SAT encoding. Lean induced cycles dominate a minimum number of hypercube nodes. Such cycles have been identified in Systems Biology as candidates for stable trajectories of gene regulatory networks. The encoding enabled us to compute lean induced cycles for hypercubes up to dimension\u00a07. We also classify the induced cycles by the number of nodes they fail to dominate, using a custom-built All-SAT solver. We demonstrate how clause filtering can reduce the number of blocking clauses by two orders of magnitude.", "num_citations": "3\n", "authors": ["1024"]}
{"title": "Linear Arithmetic\n", "abstract": " This chapter introduces decision procedures for conjunctions of linear constraints. An extension of these decision procedures for solving a general linear arithmetic formula, ie, with an arbitrary Boolean structure, is given in Chap. 11.", "num_citations": "3\n", "authors": ["1024"]}
{"title": "Experiments with SAT-Based Symbolic Simulation Using Reparameterization in the Abstraction Refinement Framework\n", "abstract": " This paper presents experimental results on the performance effect of using symbolic simulation with SAT-based reparametrization within the Counter-example Guided Abstraction Refinement framework. Abstraction refinement has been applied successfully to prove safety properties of large industrial circuits. However, all existing abstraction refinement frameworks simply use SAT-based Bounded Model Checking BMC to refute the property. The model used for the BMC instance is not abstracted, and thus is susceptible to the state space explosion problem. We address this issue by using a symbolic simulator with a SAT-based reparametrization algorithm as a replacement for BMC within the abstraction refinement framework. The reparametrization is performed as soon as the equations maintained by the symbolic simulator become too large. We discuss the quality of the refinement information that is extracted from the symbolic simulator.Descriptors:", "num_citations": "3\n", "authors": ["1024"]}
{"title": "The Impact of Write Back on Cache Performance\n", "abstract": " This paper quantifies the impact of write policies and allocation policies on the cache performance. Adding write back to a cache design increases hardware cost, since additional storage for dirty bits is required. Furthermore, write back is not always faster. This paper derives a criterion which allows to check easily whether write back improves the performance of a given memory system. This is done with the help of extensive cache simulations on a MIPS RISC architecture with SPEC92 benchmarks as workload.", "num_citations": "3\n", "authors": ["1024"]}
{"title": "Unbounded-time safety verification of guarded lti models with inputs by abstract acceleration\n", "abstract": " Reachability analysis of dynamical models is a relevant problem that has seen much progress in the last decades, however with clear limitations pertaining to the nature of the dynamics and the soundness of the results. This article focuses on sound safety verification of unbounded-time (infinite-horizon) linear time-invariant (LTI) models with inputs using reachability analysis. We achieve this using counterexample-guided Abstract Acceleration: this approach over-approximates the reachability tube of the LTI model over an unbounded time horizon by using abstraction, possibly finding concrete counterexamples for refinement based on the given safety specification. The technique is applied to a number of LTI models and the results show robust performance when compared to state-of-the-art tools.", "num_citations": "2\n", "authors": ["1024"]}
{"title": "SC-square: when Satisfiability Checking and Symbolic Computation join forces\n", "abstract": " Symbolic Computation and Satisfiability Checking are two research areas, both having their individual scientific focus but with common interests, eg, in the development, implementation and application of decision procedures for arithmetic theories. Despite their commonalities, the two communities are rather weakly connected. The aim of the SC-Square initiative is to strengthen the connection between these communities by creating common platforms, initiating interaction and exchange, identifying common challenges, and developing a common roadmap from theory along the way to tools and (industrial) applications.", "num_citations": "2\n", "authors": ["1024"]}
{"title": "From AgentSpeak to C for safety considerations in unmanned aerial vehicles\n", "abstract": " Unmanned aerial vehicles (UAV) are becoming increasingly popular for both recreational and industrial applications, leading to growing concerns about safety. Autonomous systems, such as UAVs, are typically hybrid systems consisting of a low-level continuous control part and a high-level discrete decision making part. In this paper, we discuss using the agent programming language AgentSpeak to model the high-level decision making. We present a translation from AgentSpeak to C that bridges the gap between high-level decision making and low-level control code for safety-critical systems. This allows code to be written in a more natural high-level language, thereby reducing its overall complexity and making it easier to maintain, while still conforming to safety guidelines. As an exemplar, we present the code for a UAV autopilot. The generated code is evaluated on a simulator and a Parrot AR.Drone\u00a0\u2026", "num_citations": "2\n", "authors": ["1024"]}
{"title": "Synthesising interprocedural bit-precise termination proofs (extended version)\n", "abstract": " Proving program termination is key to guaranteeing absence of undesirable behaviour, such as hanging programs and even security vulnerabilities such as denial-of-service attacks. To make termination checks scale to large systems, interprocedural termination analysis seems essential, which is a largely unexplored area of research in termination analysis, where most effort has focussed on difficult single-procedure problems. We present a modular termination analysis for C programs using template-based interprocedural summarisation. Our analysis combines a context-sensitive, over-approximating forward analysis with the inference of under-approximating preconditions for termination. Bit-precise termination arguments are synthesised over lexicographic linear ranking function templates. Our experimental results show that our tool 2LS outperforms state-of-the-art alternatives, and demonstrate the clear advantage of interprocedural reasoning over monolithic analysis in terms of efficiency, while retaining comparable precision.", "num_citations": "2\n", "authors": ["1024"]}
{"title": "Verified Software: Theories, Tools and Experiments: 6th International Conference, VSTTE 2014, Vienna, Austria, July 17-18, 2014, Revised Selected Papers\n", "abstract": " This volume constitutes the thoroughly refereed post-conference proceedings of the 6th International Conference on Verified Software: Theories, Tools and Experiments, VSTTE 2014, held in July 2014 at the Vienna Summer of Logic in Vienna, Austria, as an associated event of CAV 2014, the International Conference on Computer-Aided Verification. The 17 revised full papers presented were carefully revised and selected from 34 submissions. The papers are organized in topical sections such as analysis: understanding and explanation; verification frameworks and applications; hypervisors and dynamic data structures; certification; real time and security.", "num_citations": "2\n", "authors": ["1024"]}
{"title": "A visual studio plug-in for CProver\n", "abstract": " In recent years, automatic software verification has emerged as a complementary approach to program testing for enhancing software quality. Finding bugs is the ultimate aim of software verification tools. How do we best support the programmer who has to diagnose and understand those bugs? Unfortunately, most of the existing tools do not offer enough support for error diagnosis. We have developed a plug-in which implements a graphical user interface for the CProver tools within the Visual Studio IDE. Our plug-in enables visual debugging and error trace simulating within C programs as well as co-debugging C programs in tandem with wave-form views of hardware designs. Another feature of our plug-in is background verification. Each time a program source is saved, the verification process is silently triggered in background. If an error is found, its location is highlighted in the program. The user interacts\u00a0\u2026", "num_citations": "2\n", "authors": ["1024"]}
{"title": "Hardware and Software: Verification and Testing\n", "abstract": " Hardware and Software: Verification and Testing - NASA/ADS Now on home page ads icon ads Enable full ADS view NASA/ADS Hardware and Software: Verification and Testing Barner, Sharon ; Harris, Ian ; Kroening, Daniel ; Raz, Orna Abstract Publication: Lecture Notes in Computer Science Pub Date: 2011 DOI: 10.1007/978-3-642-19583-9 Bibcode: 2011LNCS......B Keywords: Computer Science; Software Engineering; Logics and Meanings of Programs; Programming Languages; Compilers; Interpreters full text sources Publisher | \u00a9 The SAO/NASA Astrophysics Data System adshelp[at]cfa.harvard.edu The ADS is operated by the Smithsonian Astrophysical Observatory under NASA Cooperative Agreement NNX16AC86A NASA logo Smithsonian logo Resources About ADS ADS Help What's New Careers@ADS Social @adsabs ADS Blog Project Switch to full ADS Is ADS down? (or is it just me...) Smithsonian \u2026", "num_citations": "2\n", "authors": ["1024"]}
{"title": "Craig interpolation for quantifier-free Presburger arithmetic\n", "abstract": " Craig interpolation has become a versatile algorithmic tool for improving software verification. Interpolants can, for instance, accelerate the convergence of fixpoint computations for infinite-state systems. They also help improve the refinement of iteratively computed lazy abstractions. Efficient interpolation procedures have been presented only for a few theories. In this paper, we introduce a complete interpolation method for the full range of quantifier-free Presburger arithmetic formulas. We propose a novel convex variable projection for integer inequalities and a technique to combine them with equalities. The derivation of the interpolant has complexity low-degree polynomial in the size of the refutation proof and is typically fast in practice.", "num_citations": "2\n", "authors": ["1024"]}
{"title": "Neural Termination Analysis\n", "abstract": " We introduce a novel approach to the automated termination analysis of computer programs: we train neural networks to act as ranking functions. Ranking functions map program states to values that are bounded from below and decrease as the program runs. The existence of a valid ranking function proves that the program terminates. While in the past ranking functions were usually constructed using static analysis, our method learns them from sampled executions. We train a neural network so that its output decreases along execution traces as a ranking function would; then, we use formal reasoning to verify whether it generalises to all possible executions. We present a custom loss function for learning lexicographic ranking functions and use satisfiability modulo theories for verification. Thanks to the ability of neural networks to generalise well, our method succeeds over a wide variety of programs. This includes programs that use data structures from standard libraries. We built a prototype analyser for Java bytecode and show the efficacy of our method over a standard dataset of benchmarks.", "num_citations": "1\n", "authors": ["1024"]}
{"title": "Shielding Atari Games with Bounded Prescience\n", "abstract": " Deep reinforcement learning (DRL) is applied in safety-critical domains such as robotics and autonomous driving. It achieves superhuman abilities in many tasks, however whether DRL agents can be shown to act safely is an open problem. Atari games are a simple yet challenging exemplar for evaluating the safety of DRL agents and feature a diverse portfolio of game mechanics. The safety of neural agents has been studied before using methods that either require a model of the system dynamics or an abstraction; unfortunately, these are unsuitable to Atari games because their low-level dynamics are complex and hidden inside their emulator. We present the first exact method for analysing and ensuring the safety of DRL agents for Atari games. Our method only requires access to the emulator. First, we give a set of 43 properties that characterise \"safe behaviour\" for 30 games. Second, we develop a method for exploring all traces induced by an agent and a game and consider a variety of sources of game non-determinism. We observe that the best available DRL agents reliably satisfy only very few properties; several critical properties are violated by all agents. Finally, we propose a countermeasure that combines a bounded explicit-state exploration with shielding. We demonstrate that our method improves the safety of all agents over multiple properties.", "num_citations": "1\n", "authors": ["1024"]}
{"title": "Ranking Policy Decisions\n", "abstract": " Policies trained via Reinforcement Learning (RL) are often needlessly complex, making them more difficult to analyse and interpret. In a run with  time steps, a policy will decide  times on an action to take, even when only a tiny subset of these decisions deliver value over selecting a simple default action. Given a pre-trained policy, we propose a black-box method based on statistical fault localisation that ranks the states of the environment according to the importance of decisions made in those states. We evaluate our ranking method by creating new, simpler policies by pruning decisions identified as unimportant, and measure the impact on performance. Our experimental results on a diverse set of standard benchmarks (gridworld, CartPole, Atari games) show that in some cases less than half of the decisions made contribute to the expected reward. We furthermore show that the decisions made in the most frequently visited states are not the most important for the expected reward.", "num_citations": "1\n", "authors": ["1024"]}
{"title": "Precise abstract interpretation of hardware designs\n", "abstract": " This dissertation shows that the bounded property verification of hardware Register Transfer Level (RTL) designs can be efficiently performed by precise abstract interpretation of a software representation of the RTL.  The first part of this dissertation presents a novel framework for RTL verification using  native software analyzers. To this end, we first present a translation of the hardware circuit expressed in Verilog RTL into the software in C called the software netlist. We then present  the application of native software analyzers based on SAT/SMT-based decision procedures as well as abstraction-based techniques such as abstract interpretation for the formal verification of the software netlist design generated from the hardware RTL. In particular, we show that the path-based symbolic execution techniques, commonly used for automatic test case generation in system softwares, are also effective for proving bounded safety as well as detecting bugs in the software netlist designs. Furthermore, by means of experiments, we  show that abstract interpretation techniques, commonly used for static program analysis, can also be used for bounded as well as unbounded safety property verification of the software netlist designs. However, the analysis using abstract interpretation shows high degree of imprecision on our benchmarks which is handled by manually guiding the analysis with various trace partitioning directives. The second part of this dissertation presents a new theoretical framework and a practical instantiation for automatically refining the precision of abstract interpretation using Conflict  Driven Clause Learning (CDCL)-style analysis. The\u00a0\u2026", "num_citations": "1\n", "authors": ["1024"]}
{"title": "Algebraic Techniques in Software Verification: Challenges and Opportunities\n", "abstract": " One of the main application areas and driving forces behind the development of Satisfiability Modulo Theory (SMT) solvers is software verification. The requirements of software verification are somewhat different to other applications of automated reasoning, posing a number of challenges but also providing some interesting opportunities. This paper brings together and summarises the algebras and structures of interest, along with some of the problems that are characteristic of software verification. It is hoped that this will allow computer algebra researchers to assess the applicability of their techniques to this challenging, but rewarding domain.", "num_citations": "1\n", "authors": ["1024"]}
{"title": "Sound static deadlock analysis for C/Pthreads (extended version)\n", "abstract": " We present a static deadlock analysis approach for C/pthreads. The design of our method has been guided by the requirement to analyse real-world code. Our approach is sound (i.e., misses no deadlocks) for programs that have defined behaviour according to the C standard, and precise enough to prove deadlock-freedom for a large number of programs. The method consists of a pipeline of several analyses that build on a new context- and thread-sensitive abstract interpretation framework. We further present a lightweight dependency analysis to identify statements relevant to deadlock analysis and thus speed up the overall analysis. In our experimental evaluation, we succeeded to prove deadlock-freedom for 262 programs from the Debian GNU/Linux distribution with in total 2.6 MLOC in less than 11 hours.", "num_citations": "1\n", "authors": ["1024"]}
{"title": "Towards automated bounded model checking of api implementations\n", "abstract": " We introduce and demonstrate the viability of a novel technique for verifying that implementations of application program interfaces (APIs) are bug free. Our technique applies a new abstract interpretation to extract an underlying model of API usage, and then uses this to synthesise a set of verifiable program fragments. These fragments are evaluated using CBMC and any potentially spurious property violation is presented to a domain expert user. The user\u2019s response is then used to refine the underlying model of the API to eliminate false positives. The refinement-analysis process is repeated iteratively. We demonstrate the viability of the technique by showing how it can find an integer underflow within Google\u2019s Brotli, an underflow that has been shown to lead directly to allow remote attackers to execute arbitrary code in CVE 2016-1968.", "num_citations": "1\n", "authors": ["1024"]}
{"title": "Verification of Concurrent Software.\n", "abstract": " We provide a tutorial on the verification of concurrent software. We first discuss semantics of modern shared-variable concurrent software. We then provide an overview of algorithmic methods for analysing such software. We begin with Bounded Model Checking, a technique that performs an analysis of program paths up to a user-specified length. We then discuss methods that are, in principle, able to provide an unbounded analysis. We discusshow to apply predicate abstraction to concurrent software and then present an extension of lazy abstraction with interpolants to such software.", "num_citations": "1\n", "authors": ["1024"]}
{"title": "Camera-laser projector stereo system based anti-collision system for robotic wheelchair users with cognitive impairment\n", "abstract": " This paper presents a camera-laser projector based system for the real-time estimation of distance to obstacles designed to assist wheelchair users with cognitive impairment. Upon falling under the specified safe distance to an obstacle an alarm alerts that it can be used by the control system to act immediately to avert a possible collision even before the user stops the wheelchair. This system consists of a fisheye camera, which allows to cover a large field of view (FOV) to enable the pattern to be available at all times, and a laser circle projector mounted on a fixed baseline. The approach uses the geometrical information obtained by the projection of the laser circle onto the plane simultaneously perceived by the camera. We show a theoretical study of the system in which the camera is modelled as a sphere and show that the estimation of a conic on this sphere allows to estimate the distance between wheel chair\u00a0\u2026", "num_citations": "1\n", "authors": ["1024"]}
{"title": "Split-case k-induction for program verification\n", "abstract": " We present a novel proof rule for verifying software programs using k-induction. The rule takes an unstructured control flow graph (CFG) and a set of disjoint, natural loops occurring in the CFG, and decomposes the CFG into a base case and step case in which all the given loops are eliminated via unwinding. Correctness of the base and step cases guarantees correctness of the whole program. Recursively applying the proof rule yields loop-free base and step cases, which can be checked using SAT-/SMT-based techniques. We refer to this verification technique as split-case k-induction. The soundness of our new rule is shown by reduction to a more general notion which we term inductive decomposition. Because the new rule can be applied to many loops of a program simultaneously, it strictly generalises an existing single-loop proof rule for k-induction; compared with the existing rule, the number of loop-free programs that must be checked is drastically reduced, especially in the presence of nested loops. We describe an implementation of our techniques on top of the CBMC tool. Our experiments, using a range of benchmarks, show that the new rule leads to significantly faster verification than the existing rule, and that additional performance benefits can be obtained by parallelising k-induction analysis on a multicore machine.", "num_citations": "1\n", "authors": ["1024"]}
{"title": "Tightening test coverage metrics: a case study in equivalence checking using k-induction\n", "abstract": " We present a case study applying the k-induction method to equivalence checking of Simulink designs. In particular, we are interested in the problem of equivalence detection in mutation-based testing: given a design S, determining whether a \u201cmutant\u201d design S\u2032 derived from S by syntactic fault injection is behaviourally equivalent to S. In this situation, efficient equivalence checking techniques are needed to avoid redundant and expensive search for test cases that observe differences between S and S\u2032. We have integrated k-induction into our test case generation framework for Simulink. We show, using a selection of benchmarks, that k-induction can be effective in detecting equivalent mutants, sometimes as a stand-alone technique, and sometimes with some manual assistance. We further discuss how the level of automation of the method can be increased by using static analysis to derive\u00a0\u2026", "num_citations": "1\n", "authors": ["1024"]}
{"title": "Beyond quantifier-free interpolation in extensions of Presburger arithmetic (extended Technical Report)\n", "abstract": " Craig interpolation has emerged as an effective means of generating candidate program invariants. We present interpolation procedures for the theories of Presburger arithmetic combined with (i) uninterpreted predicates (QPA+UP), (ii) uninterpreted functions (QPA+UF) and (iii) extensional arrays (QPA+AR). We prove that none of these combinations can be effectively interpolated without the use of quantifiers, even if the input formulae are quantifier-free. We go on to identify fragments of QPA+UP and QPA+UF with restricted forms of guarded quantification that are closed under interpolation. Formulae in these fragments can easily be mapped to quantifier-free expressions with integer division. For QPA+AR, we formulate a sound interpolation procedure that potentially produces interpolants with unrestricted quantifiers.", "num_citations": "1\n", "authors": ["1024"]}
{"title": "Checking consistency of C and Verilog using predicate abstraction and induction\n", "abstract": " It is common practice to write C models of circuits due to the greater simulation efficiency. Once the C program satisfies the requirements, the circuit is designed in a hardware description language HDL such as Verilog. It is therefore highly desirable to automatically perform a correspondence check between the C model and a circuit given in HDL. We present an algorithm that checks consistency between an ANSI-C program and a circuit given in Verilog using Predicate Abstraction. The algorithm exploits the fact that the C program and the circuit share many basic predicates. In contrast to existing tools that perform predicate abstraction, our approach is SAT-based and allows all ANSI-C and Verilog operators in the predicates. We report experimental results on an out-of-order RISC processor. We compare the performance of the new technique to Bounded Model Checking BMC.Descriptors:", "num_citations": "1\n", "authors": ["1024"]}