{"title": "Empirical evaluation of the tarantula automatic fault-localization technique\n", "abstract": " The high cost of locating faults in programs has motivated the development of techniques that assist in fault localization by automating part of the process of searching for faults. Empirical studies that compare these techniques have reported the relative effectiveness of four existing techniques on a set of subjects. These studies compare the rankings that the techniques compute for statements in the subject programs and the effectiveness of these rankings in locating the faults. However, it is unknown how these four techniques compare with Tarantula, another existing fault-localization technique, although this technique also provides a way to rank statements in terms of their suspiciousness. Thus, we performed a study to compare the Tarantula technique with the four techniques previously compared. This paper presents our study---it overviews the Tarantula technique along with the four other techniques studied\u00a0\u2026", "num_citations": "1374\n", "authors": ["923"]}
{"title": "Visualization of test information to assist fault localization\n", "abstract": " One of the most expensive and time-consuming components of the debugging process is locating the errors or faults. To locate faults, developers must identify statements involved in failures and select suspicious statements that might contain faults. The paper presents a technique that uses visualization to assist with these tasks. The technique uses color to visually map the participation of each program statement in the outcome of the execution of the program with a test suite, consisting of both passed and failed test cases. Based on this visual mapping, a user can inspect the statements in the program, identify statements involved in failures, and locate potentially faulty statements. The paper also describes a prototype tool that implements our technique along with a set of empirical studies that use the tool for evaluation of the technique. The empirical studies show that, for the subject we studied, the technique can\u00a0\u2026", "num_citations": "1329\n", "authors": ["923"]}
{"title": "Test-suite reduction and prioritization for modified condition/decision coverage\n", "abstract": " Software testing is particularly expensive for developers of high-assurance software, such as software that is produced for commercial airborne systems. One reason for this expense is the Federal Aviation Administration's requirement that test suites be modified condition/decision coverage (MC/DC) adequate. Despite its cost, there is evidence that MC/DC is an effective verification technique and can help to uncover safety faults. As the software is modified and new test cases are added to the test suite, the test suite grows and the cost of regression testing increases. To address the test-suite size problem, researchers have investigated the use of test-suite reduction algorithms, which identify a reduced test suite that provides the same coverage of the software according to some criterion as the original test suite, and test-suite prioritization algorithms, which identify an ordering of the test cases in the test suite\u00a0\u2026", "num_citations": "552\n", "authors": ["923"]}
{"title": "Lightweight fault-localization using multiple coverage types\n", "abstract": " Lightweight fault-localization techniques use program coverage to isolate the parts of the code that are most suspicious of being faulty. In this paper, we present the results of a study of three types of program coverage-statements, branches, and data dependencies-to compare their effectiveness in localizing faults. The study shows that no single coverage type performs best for all faults-different kinds of faults are best localized by different coverage types. Based on these results, we present a new coverage-based approach to fault localization that leverages the unique qualities of each coverage type by combining them. Because data dependencies are noticeably more expensive to monitor than branches, we also investigate the effects of replacing data-dependence coverage with an approximation inferred from branch coverage. Our empirical results show that (1) the cost of fault localization using combinations of\u00a0\u2026", "num_citations": "347\n", "authors": ["923"]}
{"title": "An empirical study of the effects of test-suite reduction on fault localization\n", "abstract": " Fault-localization techniques that utilize information about all test cases in a test suite have been presented. These techniques use various approaches to identify the likely faulty part(s) of a program, based on information about the execution of the program with the test suite. Researchers have begun to investigate the impact that the composition of the test suite has on the effectiveness of these fault-localization techniques. In this paper, we present the first experiment on one aspect of test-suite composition--test-suite reduction. Our experiment studies the impact of the test-suite reduction on the effectiveness of fault-localization techniques. In our experiment, we apply 10 test-suite reduction strategies to test suites for eight subject programs. We then measure the differences between the effectiveness of four existing fault-localization techniques on the unreduced and reduced test suites. We also measure the reduction\u00a0\u2026", "num_citations": "304\n", "authors": ["923"]}
{"title": "Debugging in parallel\n", "abstract": " The presence of multiple faults in a program can inhibit the ability of fault-localization techniques to locate the faults. This problem occurs for two reasons: when a program fails, the number of faults is, in general, unknown; and certain faults may mask or obfuscate other faults. This paper presents our approach to solving this problem that leverages the well-known advantages of parallel work flows to reduce the time-to-release of a program. Our approach consists of a technique that enables more effective debugging in the presence of multiple faults and a methodology that enables multiple developers to simultaneously debug multiple faults. The paper also presents an empirical study that demonstrates that our parallel-debugging technique and methodology can yield a dramatic decrease in total debugging time compared to a one-fault-at-a-time, or conventionally sequential, approach.", "num_citations": "282\n", "authors": ["923"]}
{"title": "Visualization for fault localization\n", "abstract": " Software errors significantly impact software productivity and quality. Attempts to reduce the number of delivered faults are estimated to consume between 50% and 80% of the development and maintenance effort [4]. Debugging is one of the most time-consuming, and thus expensive, tasks required to reduce the number of delivered faults in a program. Because software debugging is so expensive, researchers have investigated techniques and tools to assist developers with these tasks (eg,[1, 3, 7]). However, these tools often do not scale to large programs or they require extensive manual intervention. This lack of effective tools hinders the development and maintenance process.Studies show that locating the errors1 is the most difficult and time-consuming component of the debugging process (eg,[8]). Pan and Spafford observed that developers consistently perform four tasks when attempting to locate the errors in a program:(1) identify statements involved in failures;(2) select suspicious statements that might contain faults;(3) hypothesize about suspicious faults; and (4) restore program variables to a specific state [6]. A source-code debugger can help with the first task: a developer runs the program, one line at a time, with a test case that caused it to fail, and during this execution, the developer can inspect the results produced by the execution of each statement in the program. Information about incorrect results at a statement can help a developer locate the source of the problem. Stepping through large programs one statement at a time, however, and inspecting the results of the execution can be very time consuming. Thus, developers often try\u00a0\u2026", "num_citations": "121\n", "authors": ["923"]}
{"title": "On the influence of multiple faults on coverage-based fault localization\n", "abstract": " This paper presents an empirical study on the effects of the quantity of faults on statistical, coverage-based fault localization techniques. The former belief was that the effectiveness of fault-localization techniques was inversely proportional to the quantity of faults. In an attempt to verify these beliefs, we conducted a study on three programs varying in size on more than 13,000 multiple-fault versions. We found that the influence of multiple faults (1) was not as great as expected,(2) created a negligible effect on the effectiveness of the fault localization, and (3) was often even complimentary to the fault-localization effectiveness. In general, even in the presence of many faults, at least one fault was found by the fault-localization technique with high effectiveness. We also found that some faults were localizable regardless of the presence of other faults, whereas other faults' ability to be found by these techniques varied\u00a0\u2026", "num_citations": "111\n", "authors": ["923"]}
{"title": "Rapid: Identifying bug signatures to support debugging activities\n", "abstract": " Most existing fault-localization techniques focus on identifying and reporting single statements that may contain a fault. Even in cases where a fault involves a single statement, it is generally hard to understand the fault by looking at that statement in isolation. Faults typically manifest themselves in a specific context, and knowing that context is necessary to diagnose and correct the fault. In this paper, we present a novel fault-localization technique that identifies sequences of statements that lead to a failure. The technique works by analyzing partial execution traces corresponding to failing executions and identifying common segments in these traces, incrementally. Our approach provides developers a context that is likely to result in a more directed approach to fault understanding and a lower overall cost for debugging.", "num_citations": "83\n", "authors": ["923"]}
{"title": "Visualization of program-execution data for deployed software\n", "abstract": " Software products are often released with missing functionality, errors, or incompatibilities that may result in failures in the field, inferior performances, or, more generally, user dissatisfaction. In previous work, we presented the GAMMA technology, which facilitates remote analysis and measurement of deployed software and allows for gathering programexecution data from the field. When monitoring a high number of deployed instances of a software product, however, a large amount of data is collected. Such raw data are useless in the absence of a suitable data-mining and visualization technique that supports exploration and understanding of the data. In this paper, we present a new technique for collecting, storing, and visualizing program-execution data gathered from deployed instances of a software product. We also present a prototype toolset, GAMMATELLA, that implements the technique. We show how the\u00a0\u2026", "num_citations": "81\n", "authors": ["923"]}
{"title": "Fault localization using visualization of test information\n", "abstract": " Attempts to reduce the number of delivered faults in software are estimated to consume 50% to 80% of the development and maintenance effort according to J.S. Collofello ans S.N. Woodfield (1989). Among the tasks required to reduce the number of delivered faults, debugging is one of the most time-consuming according to T. Ball and S.G. Eick and Telcordia Technologies, and locating the errors is the most difficult component of this debugging task according to I. Vessey (1985). Clearly, techniques that can reduce the time required to locate faults can have a significant impact on the cost and quality of software development and maintenance.", "num_citations": "61\n", "authors": ["923"]}
{"title": "Fault density, fault types, and spectra-based fault localization\n", "abstract": " This paper presents multiple empirical experiments that investigate the impact of fault quantity and fault type on statistical, coverage-based fault localization techniques and fault-localization interference. Fault-localization interference is a phenomenon revealed in earlier studies of coverage-based fault localization that causes faults to obstruct, or interfere, with other faults\u2019 ability to be localized. Previously, it had been asserted that a fault-localization technique\u2019s effectiveness was negatively correlated to the quantity of faults in the program. To investigate these beliefs, we conducted an experiment on six programs consisting of more than 72,000 multiple-fault versions. Our data suggests that the impact of multiple faults exerts a significant, but slight influence on fault-localization effectiveness. In addition, faults were categorized according to four existing fault-taxonomies and found no correlation between fault\u00a0\u2026", "num_citations": "51\n", "authors": ["923"]}
{"title": "Semi-automatic fault localization\n", "abstract": " One of the most expensive and time-consuming components of the debugging process is locating the errors or faults. To locate faults, developers must identify statements involved in failures and select suspicious statements that might contain faults. In practice, this localization is done by developers in a tedious and manual way, using only a single execution, targeting only one fault, and having a limited perspective into a large search space.", "num_citations": "48\n", "authors": ["923"]}
{"title": "Gammatella: Visualizing program-execution data for deployed software\n", "abstract": " Software systems are often released with missing functionality, errors, or incompatibilities that may result in failures in the field, inferior performances, or, more generally, user dissatisfaction. In previous work, some of the authors presented the gamma approach, whose goal is to improve software quality by augmenting software-engineering tasks with dynamic information collected from deployed software. The gamma approach enables analyses that (1) rely on actual field data instead of synthetic in-house data and (2) leverage the vast and heterogeneous resources of an entire user community instead of limited, and often homogeneous, in-house resources. When monitoring a large number of deployed instances of a software product, however, a significant amount of data is collected. Such raw data are useless in the absence of suitable datamining and visualization techniques that support exploration and\u00a0\u2026", "num_citations": "43\n", "authors": ["923"]}
{"title": "Fault interaction and its repercussions\n", "abstract": " Multiple faults in a program can interact to form new behaviors in a program that would not be realized if the program were to contain the individual faults. This paper presents an in-depth study of the effects of the interaction of faults within a program. Many researchers attempt to ameliorate the effects of faulty programs. Unfortunately, such researchers are left to rely upon intuition about fault behavior due to the paucity of formalized studies of faults and their behavior. In an attempt to advance the understanding of faults and their behavior, we conducted a study of fault interaction across six subjects with more than 65,000 multiple-fault versions. The results of our study show four significant types of interaction, with one type - faults obscuring the effects of other faults - as the most prevalent type. The prevalence of obscuring faults' effects has an adverse effect on many automated software-engineering techniques, such\u00a0\u2026", "num_citations": "41\n", "authors": ["923"]}
{"title": "Visually encoding program test information to find faults in software\n", "abstract": " Large test suites are frequently used to evaluate the correctness of software systems and to locate errors. Unfortunately, this process can generate a huge amount of data that is difficult to interpret manually. We have created a system called Tarantula that visually encodes test data to help find program errors. The system uses a principled color mapping to represent how particular source lines act in passed and failed tests. It also provides a flexible user interface for examining different perspectives that show the effects on source regions of test suites ranging from individual tests, to important subsets such as the set of failed tests, to the entire test suite.", "num_citations": "40\n", "authors": ["923"]}
{"title": "Auditory feedback control of vocal pitch during sustained vocalization: a cross-sectional study of adult aging\n", "abstract": " Background Auditory feedback has been demonstrated to play an important role in the control of voice fundamental frequency (F0), but the mechanisms underlying the processing of auditory feedback remain poorly understood. It has been well documented that young adults can use auditory feedback to stabilize their voice F0 by making compensatory responses to perturbations they hear in their vocal pitch feedback. However, little is known about the effects of aging on the processing of audio-vocal feedback during vocalization.   Methodology/Principal Findings In the present study, we recruited adults who were between 19 and 75 years of age and divided them into five age groups. Using a pitch-shift paradigm, the pitch of their vocal feedback was unexpectedly shifted \u00b150 or \u00b1100 cents during sustained vocalization of the vowel sound/u/. Compensatory vocal F0 response magnitudes and latencies to pitch feedback perturbations were examined. A significant effect of age was found such that response magnitudes increased with increasing age until maximal values were reached for adults 51\u201360 years of age and then decreased for adults 61\u201375 years of age. Adults 51\u201360 years of age were also more sensitive to the direction and magnitude of the pitch feedback perturbations compared to younger adults.   Conclusion These findings demonstrate that the pitch-shift reflex systematically changes across the adult lifespan. Understanding aging-related changes to the role of auditory feedback is critically important for our theoretical understanding of speech production and the clinical applications of that knowledge.", "num_citations": "39\n", "authors": ["923"]}
{"title": "Gammatella: Visualization of program-execution data for deployed software\n", "abstract": " To investigate the program-execution data efficiently, we must be able to view the data at different levels of detail. In our visualization approach, we represent software systems at three different levels: statement level, file level, and system level. At the statement level, we represent the actual code. The representation at the file level provides a miniaturized view of the source code similar to the one used in the SeeSoft system (Eick et al., 1992). The system level uses treemaps (Shneiderman, 1992 and Bruls et al., 2000) to represent the software and is the most abstracted level in our visualization. At each level, coloring is used to represent one- or two-dimensional information about the code, using the colors' hue and brightness components. The coloring technique that we apply is a generalization of the coloring technique defined for fault-localization by Jones and colleagues (2001). GAMMATELLA is a toolset that\u00a0\u2026", "num_citations": "37\n", "authors": ["923"]}
{"title": "Concept-based failure clustering\n", "abstract": " When attempting to determine the number and set of execution failures that are caused by particular faults, developers must perform an arduous task of investigating and diagnosing each individual failure. Researchers proposed failure-clustering techniques to automatically categorize failures, with the intention of isolating each culpable fault. The current techniques utilize dynamic control flow to characterize each failure to then cluster them. These existing techniques, however, are blind to the intent or purpose of each execution, other than what can be inferred by the control-flow profile. We hypothesize that semantically rich execution information can aid clustering effectiveness by categorizing failures according to which functionality they exhibit in the software. This paper presents a novel clustering method that utilizes latent-semantic-analysis techniques to categorize each failure by the semantic concepts that are\u00a0\u2026", "num_citations": "28\n", "authors": ["923"]}
{"title": "Bridging gaps between developers and testers in globally-distributed software development\n", "abstract": " One of the main challenges in distributed development is ensuring effective communication and coordination among the distributed teams. In this context, little attention has been paid so far to coordination in software testing. In distributed development environments, testing is often performed by specialized teams that operate as independent quality assurance centers. The use of these centers can be advantageous from both an economic and a software quality perspective. These benefits, however, are offset by severe difficulties in coordination between testing and software development centers. Test centers operate as isolated silos and have little to no interactions with developers, which can result in multiple problems that lead to poor quality of software. Based on our preliminary investigation, we claim that we need to rethink the way testing is performed in distributed development environments. We then present a\u00a0\u2026", "num_citations": "28\n", "authors": ["923"]}
{"title": "Software behavior and failure clustering: An empirical study of fault causality\n", "abstract": " To cluster executions that exhibit faulty behavior by the faults that cause them, researchers have proposed using internal execution events, such as statement profiles, to (1) measure execution similarities, (2) categorize executions based on those similarity results, and (3) suggest the resulting categories as sets of executions exhibiting uniform fault behavior. However, due to a paucity of evidence correlating profiles and output behavior, researchers employ multiple simplifying assumptions in order to justify such approaches. In this paper we present an empirical study of profile correlation with output behavior, and we reexamine the suitability of such simplifying assumptions. We examine over 4 billion test-case outputs and execution profiles from multiple programs with over 9000 versions. Our data provides evidence that with current techniques many executions should be omitted from the clustering analysis to\u00a0\u2026", "num_citations": "26\n", "authors": ["923"]}
{"title": "Weighted system dependence graph\n", "abstract": " In this paper, we present a weighted, hybrid program-dependence model that represents the relevance of highly related, dependent code to assist developer comprehension of the program for multiple software-engineering tasks. Programmers often need to understand the dependencies among program elements, which may exist across multiple modules. Although such dependencies can be gathered from traditional models, such as slices, the scalability of these approaches is often prohibitive for direct, practical use. To address this scalability issue, as well as to assist developer comprehension, we introduce a program model that includes static dependencies as well as information about any number of executions, which inform the weight and relevance of the dependencies. Additionally, classes of executions can be differentiated in such a way as to support multiple software-engineering tasks. We evaluate this\u00a0\u2026", "num_citations": "21\n", "authors": ["923"]}
{"title": "Constellation visualization: Augmenting program dependence with dynamic information\n", "abstract": " This paper presents a scalable, statement-level visualization that shows related code in a way that supports human interpretation of clustering and context. The visualization is applicable to many software-engineering tasks through the utilization and visualization of problem-specific meta-data. The visualization models statement-level code relations from a system-dependence-graph model of the program being visualized. Dynamic, run-time information is used to augment the static program model to further enable visual cluster identification and interpretation. In addition, we performed a user study of our visualization on an example program domain. The results of the study show that our new visualization successfully revealed relevant context to the programmer participants.", "num_citations": "20\n", "authors": ["923"]}
{"title": "Visualizing constituent behaviors within executions\n", "abstract": " In this New Ideas and Emerging Results paper, we present a novel visualization, THE BRAIN, that reveals clusters of source code that co-execute to produce behavioral features of the program throughout and within executions. We created a clustered visualization of source-code that is informed by dynamic control flow of multiple executions; each cluster represents commonly interacting logic that composes software features. In addition, we render individual executions atop the clustered multiple-execution visualization as user-controlled animations to reveal characteristics of specific executions-these animations may provide exemplars for the clustered features and provide chronology for those behavioral features, or they may reveal anomalous behaviors that do not fit with the overall operational profile of most executions. Both the clustered multiple-execution view and the animated individual-execution view\u00a0\u2026", "num_citations": "19\n", "authors": ["923"]}
{"title": "Semantic fault diagnosis: automatic natural-language fault descriptions\n", "abstract": " Before a fault can be fixed, it first must be understood. However, understanding why a system fails is often a difficult and time consuming process. While current automated-debugging techniques provide assistance in knowing where a fault is, developers are left unaided in understanding what a fault is, and why the system is failing. We present Semantic Fault Diagnosis (SFD), a technique that leverages lexicographic and dynamic information to automatically capture natural-language fault descriptors. SFD utilizes class names, method names, variable expressions, developer comments, and keywords from the source code to describe a fault. SFD can be used immediately after observing a failing execution and requires no input from developers or bug reports. In addition we present motivating examples and results from a SFD prototype to serve as a proof of concept.", "num_citations": "17\n", "authors": ["923"]}
{"title": "Empirical studies of control dependence graph size for c programs\n", "abstract": " Many tools and techniques for performing software engineering tasks require control-dependence information, represented in the form of control-dependence graphs. Worst-case analysis of these graphs has shown that their size may be quadratic in the number of statements in the procedure that they represent. Despite this result, two empirical studies suggest that in practice, the relationship between control-dependence graph size and program size is linear. These studies, however, were performed on a relatively small number of Fortran procedures, all of which were derived from numerical methods programs. To further investigate control-dependence size, we implemented tools for constructing the two most popular types of control-dependence graphs, and ran our tools on over 3000 C functions extracted from a wide range of source programs. Our results support the earlier conclusions about control\u00a0\u2026", "num_citations": "16\n", "authors": ["923"]}
{"title": "Enabling and enhancing collaborations between software development organizations and independent test agencies\n", "abstract": " While the use of independent test agencies is on the rise - currently estimated to be a $25B marketplace - there are a number of challenges to successful collaboration between these agencies and their client software development organizations. These agencies offer independent verification of software, skilled testing experts, and economic advantages that arise from differential global labor rates. However, these benefits are often offset by difficulties in effectively integrating the outsourced testing into software development practice. We conducted extensive discussions with test managers and engineers at software development organizations. This position paper presents the findings of these discussions that identify key difficulties of integrating independent test agencies into software development practice, and it describes our position on how these findings can be addressed.", "num_citations": "15\n", "authors": ["923"]}
{"title": "Improving efficiency of dynamic analysis with dynamic dependence summaries\n", "abstract": " Modern applications make heavy use of third-party libraries and components, which poses new challenges for efficient dynamic analysis. To perform such analyses, transitive dependent components at all layers of the call stack must be monitored and analyzed, and as such may be prohibitively expensive for systems with large libraries and components. As an approach to address such expenses, we record, summarize, and reuse dynamic dataflows between inputs and outputs of components, based on dynamic control and data traces. These summarized dataflows are computed at a fine-grained instruction level; the result of which, we call \u201cdynamic dependence summaries.\u201d Although static summaries have been proposed, to the best of our knowledge, this work presents the first technique for dynamic dependence summaries. The benefits to efficiency of such summarization may be afforded with losses of accuracy\u00a0\u2026", "num_citations": "14\n", "authors": ["923"]}
{"title": "Revealing runtime features and constituent behaviors within software\n", "abstract": " Software engineers organize source code into a dominant hierarchy of components and modules that may emphasize various characteristics over runtime behavior. In this way, runtime features may involve cross-cutting aspects of code from multiple components, and some of these features may be emergent in nature, rather than designed. Although source-code modularization assists software engineers to organize and find components, identifying such cross-cutting feature sets can be more difficult. This work presents a visualization that includes a static (i.e., compile-time) representation of source code that gives prominence to clusters of cooperating source-code instructions to identify dynamic (i.e., runtime) features and constituent behaviors within executions of the software. In addition, the visualization animates software executions to reveal which feature clusters are executed and in what order. The result has\u00a0\u2026", "num_citations": "10\n", "authors": ["923"]}
{"title": "SPIDER SENSE: Software-engineering, networked, system evaluation\n", "abstract": " Today, many of the research innovations in software visualization and comprehension are evaluated on small-scale programs in a way that avoids actual human evaluation, despite the fact that these techniques are designed to help programmers develop and understand large and complex software. The investments required to perform such human studies often outweigh the need to publish. As such, the goal of this work (and toolkit) is to enable the evaluation of software visualizations of real-life software systems by its actual developers, as well as to understand the factors that influence adoption. The approach is to directly assist practicing software developers with visualizations through open and online collaboration tools. The mechanism by which we accomplish this goal is an online service that is linked through the projects' revision-control and build systems. We are calling this system SPIDER SENSE, and it\u00a0\u2026", "num_citations": "6\n", "authors": ["923"]}
{"title": "Inferred dependence coverage to support fault contextualization\n", "abstract": " This paper provides techniques for aiding developers' task of familiarizing themselves with the context of a fault. Many fault-localization techniques present the software developer with a subset of the program to inspect in order to aid in the search for faults that cause failures. However, typically, these techniques do not describe how the components of the subset relate to each other in a way that enables the developer to understand how these components interact to cause failures. These techniques also do not describe how the subset relates to the rest of the program in a way that enables the developer to understand the context of the subset. In this paper, we present techniques for providing static and dynamic relations among program elements that can be used as the basis for the exploration of a program when attempting to understand the nature of faults.", "num_citations": "6\n", "authors": ["923"]}
{"title": "New housing development through partnerships with private developers\n", "abstract": " A common and often chronic challenge facing most universities today is the over supply of older, outdated residential facilities that are often expensive to operate and maintain and sometimes difficult to lease and the under supply of modern residential living-learning facilities with the singleoccupancy bedrooms and amenities desired by today's students. To address this challenge, many universities are turning to private developers for assistance with all aspects of campus housing development, including design, construction, and financing. Synergistic partnerships between universities and private developers, often referred to as\" public-private partnerships,\" have yielded impressive results for many institutions and are transforming the way campus housing needs are addressed.", "num_citations": "3\n", "authors": ["923"]}
{"title": "Dynamic Dependence Summaries\n", "abstract": " Software engineers construct modern-day software applications by building on existing software libraries and components that they necessarily do not author themselves. Thus, contemporary software applications rely heavily on existing standard and third-party libraries for their execution and behavior. As such, effective runtime analysis of such a software application\u2019s behavior is met with new challenges. To perform dynamic analysis of a software application, all transitively dependent external libraries must also be monitored and analyzed at each layer of the software application\u2019s call stack. However, monitoring and analyzing large and often numerous external libraries may prove to be prohibitively expensive. Moreover, an overabundance of library-level analyses may obfuscate the details of the actual software application\u2019s dynamic behavior. In other words, the extensive use of existing libraries by a software\u00a0\u2026", "num_citations": "2\n", "authors": ["923"]}
{"title": "Society\n", "abstract": " JOURNAL MOTION PICTURE ENGINEERS Page 1 JOURNAL MOTION PICTURE ENGINEERS OF THE SOCIETY OF LOYD A. JONES, EDITOR $ro fm. Associate Editors 0. L. CHANIRR P. Mom S. E. S~PPARD A. C. HARDY P. E. Smmg LT TROLAND 0. M. GLUNT W. B. RAYTON J. L. SPRNCR Published monthly at Easton, Pa., by the Society of Motion Picture Engineers Publication Office, 20th & Northampton Sts., Easton, Pa. Editorial Office, 343 State St., Rochester, N. Y. Copyrighted, 1930. by the Society of Motion Picture Engineers, Inc. Subscription to non-members $12.00 per year; to members $9.00 per year; single Order from the Society of Motion Picture Engineers, Inc., 20th and Papers or abstracts may be reprinted if credit is given to the Journal of the The Society is not responsible for statements made by authors. Entered as second class matter January 15,1930, at the Post Office at Easton, copies $1.60. \u2026", "num_citations": "2\n", "authors": ["923"]}
{"title": "Discriminating influences among instructions in a dynamic slice\n", "abstract": " Dynamic slicing is an analysis that operates on program execution models (eg, dynamic dependence graphs) to support the interpreation of program-execution traces. Given an execution event of interest (ie, the slicing criterion), it solves for all instruction-execution events that either affect or are affected by that slicing criterion, and thereby reduces the search space to find influences within execution traces. Unfortunately, the resulting dynamic slices are still often prohibitively large for many uses. Despite this reduction search space, the dynamic slices are often still prohibitively large for many uses, and moreover, are provided without guidance of which and to what degree those influences are exerted. In this work, we present a novel approach to quantify the relevance of each instruction-execution event within a dynamic slice by its degree of relative influence on the slicing criterion. As such, we augment the dynamic\u00a0\u2026", "num_citations": "1\n", "authors": ["923"]}