{"title": "The international exascale software project roadmap\n", "abstract": " Over the last 20 years, the open-source community has provided more and more software on which the world\u2019s high-performance computing systems depend for performance and productivity. The community has invested millions of dollars and years of effort to build key components. However, although the investments in these separate software elements have been tremendously valuable, a great deal of productivity has also been lost because of the lack of planning, coordination, and key integration of technologies necessary to make them work together smoothly and efficiently, both within individual petascale systems and between different systems. It seems clear that this completely uncoordinated development model will not provide the software needed to support the unprecedented parallelism required for peta/ exascale computation on millions of cores, or the flexibility required to exploit new hardware models\u00a0\u2026", "num_citations": "878\n", "authors": ["498"]}
{"title": "Grid'5000: a large scale and highly reconfigurable experimental grid testbed\n", "abstract": " Large scale distributed systems such as Grids are difficult to study from theoretical                 models and simulators only. Most Grids deployed at large scale are production                 platforms that are inappropriate research tools because of their limited                 reconfiguration, control and monitoring capabilities. In this paper, we present                 Grid'5000, a 5000 CPU nation-wide infrastructure for research in Grid                 computing. Grid'5000 is designed to provide a scientific tool for computer                 scientists similar to the large-scale instruments used by physicists, astronomers,                 and biologists. We describe the motivations, design considerations, architecture,                 control, and monitoring infrastructure of this experimental platform. We present                 configuration examples and performance results for the reconfiguration subsystem.", "num_citations": "643\n", "authors": ["498"]}
{"title": "Xtremweb: A generic global computing system\n", "abstract": " Global computing achieves high throughput computing by harvesting a very large number of unused computing resources connected to the Internet. This parallel computing model targets a parallel architecture defined by a very high number of nodes, poor communication performance and continuously varying resources. The unprecedented scale of the global computing architecture paradigm requires us to revisit many basic issues related to parallel architecture programming models, performance models, and class of applications or algorithms suitable for this architecture. XtremWeb is an experimental global computing platform dedicated to provide a tool for such studies. The paper presents the design of XtremWeb. Two essential features of this design are multi-applications and high-performance. Accepting multiple applications allows institutions or enterprises to set up their own global computing applications or\u00a0\u2026", "num_citations": "569\n", "authors": ["498"]}
{"title": "Cost-benefit analysis of cloud computing versus desktop grids\n", "abstract": " Cloud Computing has taken commercial computing by storm. However, adoption of cloud computing platforms and services by the scientific community is in its infancy as the performance and monetary cost-benefits for scientific applications are not perfectly clear. This is especially true for desktop grids (aka volunteer computing) applications. We compare and contrast the performance and monetary cost-benefits of clouds for desktop grid applications, ranging in computational size and storage. We address the following questions: (i) What are the performance tradeoffs in using one platform over the other? (ii) What are the specific resource requirements and monetary costs of creating and deploying applications on each platform? (iii) In light of those monetary and performance cost-benefits, how do these platforms compare? (iv) Can cloud computing platforms be used in combination with desktop grids to improve\u00a0\u2026", "num_citations": "512\n", "authors": ["498"]}
{"title": "Toward exascale resilience\n", "abstract": " Over the past few years resilience has became a major issue for high-performance computing (HPC) systems, in particular in the perspective of large petascale systems and future exascale systems. These systems will typically gather from half a million to several millions of central processing unit (CPU) cores running up to a billion threads. From the current knowledge and observations of existing large systems, it is anticipated that exascale systems will experience various kind of faults many times per day. It is also anticipated that the current approach for resilience, which relies on automatic or application level checkpoint/ restart, will not work because the time for checkpointing and restarting will exceed the mean time to failure of a full system. This set of projections leaves the community of fault tolerance for HPC systems with a difficult challenge: finding new approaches, which are possibly radically disruptive, to run\u00a0\u2026", "num_citations": "454\n", "authors": ["498"]}
{"title": "FTI: High performance fault tolerance interface for hybrid systems\n", "abstract": " Large scientific applications deployed on current petascale systems expend a significant amount of their execution time dumping checkpoint files to remote storage. New fault tolerant techniques will be critical to efficiently exploit post-petascale systems. In this work, we propose a low-overhead high-frequency multi-level checkpoint technique in which we integrate a highly-reliable topology-aware Reed-Solomon encoding in a three-level checkpoint scheme. We efficiently hide the encoding time using one Fault-Tolerance dedicated thread per node. We implement our technique in the Fault Tolerance Interface FTI. We evaluate the correctness of our performance model and conduct a study of the reliability of our library. To demonstrate the performance of FTI, we present a case study of the Mw9. 0 Tohoku Japan earthquake simulation with SPECFEM3D on TSUBAME2. 0. We demonstrate a checkpoint overhead as\u00a0\u2026", "num_citations": "390\n", "authors": ["498"]}
{"title": "Toward exascale resilience: 2014 update\n", "abstract": " Resilience is a major roadblock for HPC executions on future exascale systems. These systems will typically gather millions of CPU cores running up to a billion threads. Projections from current large systems and technology evolution predict errors will happen in exascale systems many times per day. These errors will propagate and generate various kinds of malfunctions, from simple process crashes to result corruptions.The past five years have seen extraordinary technical progress in many domains related to exascale resilience. Several technical options, initially considered inapplicable or unrealistic in the HPC context, have demonstrated surprising successes. Despite this progress, the exascale resilience problem is not solved, and the community is still facing the difficult challenge of ensuring that exascale applications complete and generate correct results while running on unstable systems. Since 2009\u00a0\u2026", "num_citations": "378\n", "authors": ["498"]}
{"title": "MPI versus MPI+ OpenMP on the IBM SP for the NAS Benchmarks\n", "abstract": " The hybrid memory model of clusters of multiprocessors raises two issues: programming model and performance. Many parallel programs have been written by using the MPI standard. To evaluate the pertinence of hybrid models for existing MPI codes, we compare a unified model (MPI) and a hybrid one (OpenMP fine grain parallelization after profiling) for the NAS 2.3 benchmarks on two IBM SP systems. The superiority of one model depends on 1) the level of shared memory model parallelization, 2) the communication patterns and 3) the memory access patterns. The relative speeds of the main architecture components (CPU, memory, and network) are of tremendous importance for selecting one model. With the used hybrid model, our results show that a unified MPI approach is better for most of the benchmarks. The hybrid approach becomes better only when fast processors make the communication\u00a0\u2026", "num_citations": "373\n", "authors": ["498"]}
{"title": "Fault tolerance in petascale/exascale systems: Current knowledge, challenges and research opportunities\n", "abstract": " The emergence of petascale systems and the promise of future exascale systems have reinvigorated the community interest in how to manage failures in such systems and ensure that large applications, lasting several hours or tens of hours, are completed successfully. Most of the existing results for several key mechanisms associated with fault tolerance in high-performance computing (HPC) platforms follow the rollback\u2014recovery approach. Over the last decade, these mechanisms have received a lot of attention from the community with different levels of success. Unfortunately, despite their high degree of optimization, existing approaches do not fit well with the challenging evolutions of large-scale systems. There is room and even a need for new approaches. Opportunities may come from different origins: diskless checkpointing, algorithmic-based fault tolerance, proactive operation, speculative execution\u00a0\u2026", "num_citations": "280\n", "authors": ["498"]}
{"title": "Fast error-bounded lossy HPC data compression with SZ\n", "abstract": " Today's HPC applications are producing extremely large amounts of data, thus it is necessary to use an efficient compression before storing them to parallel file systems. In this paper, we optimize the error-bounded HPC data compression, by proposing a novel HPC data compression method that works very effectively on compressing large-scale HPC data sets. The compression method starts by linearizing multi-dimensional snapshot data. The key idea is to fit/predict the successive data points with the bestfit selection of curve fitting models. The data that can be predicted precisely will be replaced by the code of the corresponding curve-fitting model. As for the unpredictable data that cannot be approximated by curve-fitting models, we perform an optimized lossy compression via a binary representation analysis. We evaluate our proposed solution using 13 real-world HPC applications across different scientific\u00a0\u2026", "num_citations": "279\n", "authors": ["498"]}
{"title": "Grid'5000: A large scale and highly reconfigurable grid experimental testbed\n", "abstract": " Large scale distributed systems like Grids are difficult to study only from theoretical models and simulators. Most Grids deployed at large scale are production platforms that are inappropriate research tools because of their limited reconfiguration, control and monitoring capabilities. In this paper, we present Grid'5000, a 5000 CPUs nation-wide infrastructure for research in Grid computing. Grid'5000 is designed to provide a scientific tool for computer scientists similar to the large-scale instruments used by physicists, astronomers and biologists. We describe the motivations, design, architecture, configuration examples of Grid'5000 and performance results for the reconfiguration subsystem.", "num_citations": "275\n", "authors": ["498"]}
{"title": "Grid'5000: a large scale, reconfigurable, controlable and monitorable Grid platform\n", "abstract": " INRIA| IRISA| LABRI| CNRS| UNIV-RENNES1| LIP| UNIV-BORDEAUX| LIFL| ENS-LYON| UNIV-TLSE2| UNIV-LILLE3| UMR8623| UGA| IMAG| DIEUDONNE| SMS| ENS-CACHAN| LORIA| UNIV-LORRAINE| UR1-UFR-ISTIC| UNIV-LYON1| UNIV-RENNES| ENPC| PARISTECH| INSA-GROUPE| UNIV-TLSE3| UT1-CAPITOLE| UDL| IRIT", "num_citations": "259\n", "authors": ["498"]}
{"title": "Uncoordinated checkpointing without domino effect for send-deterministic MPI applications\n", "abstract": " As reported by many recent studies, the mean time between failures of future post-petascale supercomputers is likely to reduce, compared to the current situation. The most popular fault tolerance approach for MPI applications on HPC Platforms relies on coordinated check pointing which raises two major issues: a) global restart wastes energy since all processes are forced to rollback even in the case of a single failure, b) checkpoint coordination may slow down the application execution because of congestions on I/O resources. Alternative approaches based on uncoordinated check pointing and message logging require logging all messages, imposing a high memory/storage occupation and a significant overhead on communications. It has recently been observed that many MPI HPC applications are send-deterministic, allowing to design new fault tolerance protocols. In this paper, we propose an uncoordinated\u00a0\u2026", "num_citations": "160\n", "authors": ["498"]}
{"title": "Significantly improving lossy compression for scientific data sets based on multidimensional prediction and error-controlled quantization\n", "abstract": " Today's HPC applications are producing extremely large amounts of data, such that data storage and analysis are becoming more challenging for scientific research. In this work, we design a new error-controlled lossy compression algorithm for large-scale scientific data. Our key contribution is significantly improving the prediction hitting rate (or prediction accuracy) for each data point based on its nearby data values along multiple dimensions. We derive a series of multilayer prediction formulas and their unified formula in the context of data compression. One serious challenge is that the data prediction has to be performed based on the preceding decompressed values during the compression in order to guarantee the error bounds, which may degrade the prediction accuracy in turn. We explore the best layer for the prediction by considering the impact of compression errors on the prediction accuracy. Moreover\u00a0\u2026", "num_citations": "156\n", "authors": ["498"]}
{"title": "Fault prediction under the microscope: A closer look into hpc systems\n", "abstract": " A large percentage of computing capacity in today's large high-performance computing systems is wasted because of failures. Consequently current research is focusing on providing fault tolerance strategies that aim to minimize fault's effects on applications. By far the most popular technique is the checkpointrestart strategy. A complement to this classical approach is failure avoidance, by which the occurrence of a fault is predicted and preventive measures are taken. This requires a reliable prediction system to anticipate failures and their locations. Thus far, research in this field has used ideal predictors that were not implemented in real HPC systems. In this paper, we merge signal analysis concepts with data mining techniques to extend the ELSA (Event Log Signal Analyzer) toolkit and offer an adaptive and more efficient prediction module. Our goal is to provide models that characterize the normal behavior of a\u00a0\u2026", "num_citations": "155\n", "authors": ["498"]}
{"title": "Coordinated Checkpoint versus Message Log for Fault Tolerant MPI.\n", "abstract": " MPI is one of the most adopted programming models for large clusters and grid deployments. However, these systems often suffer from network or node failures. This raises the issue of selecting a fault tolerance approach for MPI. Automatic and transparent ones are based on either coordinated checkpointing or message logging associated with uncoordinated checkpoint. There are many protocols, implementations and optimizations for these approaches but few results about their comparison. Coordinated checkpoint has the advantage of a very low overhead on fault free executions. In contrary a message logging protocol systematically adds a significant message transfer penalty. The drawbacks of coordinated checkpoint come from its synchronization cost at checkpoint and restart times. In this paper we implement, evaluate and compare the two kinds of protocols with a special emphasis on their respective\u00a0\u2026", "num_citations": "137\n", "authors": ["498"]}
{"title": "Damaris: How to efficiently leverage multicore parallelism to achieve scalable, jitter-free I/O\n", "abstract": " With exascale computing on the horizon, the performance variability of I/O systems represents a key challenge in sustaining high performance. In many HPC applications, I/O is concurrently performed by all processes, which leads to I/O bursts. This causes resource contention and substantial variability of I/O performance, which significantly impacts the overall application performance and, most importantly, its predictability over time. In this paper, we propose a new approach to I/O, called Damaris, which leverages dedicated I/O cores on each multicore SMP node, along with the use of shared-memory, to efficiently perform asynchronous data processing and I/O in order to hide this variability. We evaluate our approach on three different platforms including the Kraken Cray XT5 supercomputer (ranked 11th in Top500), with the CM1 atmospheric model, one of the target HPC applications for the Blue Waters\u00a0\u2026", "num_citations": "134\n", "authors": ["498"]}
{"title": "Characterizing resource availability in enterprise desktop grids\n", "abstract": " Desktop grids, which use the idle cycles of many desktop PCs, are one of the largest distributed systems in the world. Despite the popularity and success of many desktop grid projects, the heterogeneity and volatility of hosts within desktop grids have been poorly understood. Yet, resource characterization is essential for accurate simulation and modelling of such platforms. In this paper, we present application-level traces of four real desktop grids that can be used for simulation and modelling purposes. In addition, we describe aggregate and per host statistics that reflect the heterogeneity and volatility of desktop grid resources. Finally, we apply our characterization to develop a performance model for desktop grid applications for various task granularities, and then use a cluster equivalence metric to quantify the utility of the desktop grid relative to that of a dedicated cluster for task-parallel applications.", "num_citations": "130\n", "authors": ["498"]}
{"title": "Characterizing cloud applications on a Google data center\n", "abstract": " In this paper, we characterize Google applications, based on a one-month Google trace with over 650k jobs running across over 12000 heterogeneous hosts from a Google data center. On one hand, we carefully compute the valuable statistics about task events and resource utilization for Google applications, based on various types of resources (such as CPU, memory) and execution types (e.g., whether they can run batch tasks or not). Resource utilization per application is observed with an extremely typical Pareto principle. On the other hand, we classify applications via a K-means clustering algorithm with optimized number of sets, based on task events and resource usage. The number of applications in the K-means clustering sets follows a Pareto-similar distribution. We believe our work is very interesting and valuable for the further investigation of Cloud environment.", "num_citations": "117\n", "authors": ["498"]}
{"title": "Modeling and tolerating heterogeneous failures in large parallel systems\n", "abstract": " As supercomputers and clusters increase in size and complexity, system failures are inevitable. Different hardware components (such as memory, disk, or network) of such systems can have different failure rates. Prior works assume failures equally affect an application, whereas our goal is to provide failure models for applications that reflect their specific component usage. This is challenging because component failure dynamics are heterogeneous in space and time.", "num_citations": "115\n", "authors": ["498"]}
{"title": "BlobCR: Efficient checkpoint-restart for HPC applications on IaaS clouds using virtual disk image snapshots\n", "abstract": " Infrastructure-as-a-Service (IaaS) cloud computing is gaining significant interest in industry and academia as an alternative platform for running scientific applications. Given the dynamic nature of IaaS clouds and the long runtime and resource utilization of such applications, an efficient checkpoint-restart mechanism becomes paramount in this context. This paper proposes a solution to the aforementioned challenge that aims at minimizing the storage space and performance overhead of checkpoint-restart. We introduce an approach that leverages virtual machine (VM) disk-image multi-snapshotting and multi-deployment inside checkpoint-restart protocols running at guest level in order to efficiently capture and potentially roll back the complete state of the application, including file system modifications. Experiments on the G5K testbed show substantial improvement for MPI applications over existing approaches\u00a0\u2026", "num_citations": "114\n", "authors": ["498"]}
{"title": "The international exascale software project: a call to cooperative action by the global high-performance community\n", "abstract": " Over the last 20 years, the open-source community has provided more and more software on which the world\u2019s high-performance computing systems depend for performance and productivity. The community has invested millions of dollars and years of effort to build key components. Although the investments in these separate software elements have been tremendously valuable, a great deal of productivity has also been lost because of the lack of planning, coordination, and key integration of technologies necessary to make them work together smoothly and efficiently, both within individual petascale systems and between different systems. A repository gatekeeper and an email discussion list can coordinate open-source development within a single project, but there is no global mechanism working across the community to identify critical holes in the overall software environment, spot opportunities for beneficial\u00a0\u2026", "num_citations": "109\n", "authors": ["498"]}
{"title": "XtremWeb: building an experimental platform for Global Computing\n", "abstract": " Global Computing achieves highly distributed computations by harvesting a very large number of unused computing resources connected to the Internet. Although the basic techniques for Global Computing are well understood, several issues remain unadressed, such as the ability to run a large variety of applications, economical models for resource management, performance models accounting for WAN and machine components, and finally new parallel algorithms based on true massive parallelism, with very limited, if any, communication capability. The main purpose of XtremWeb is to build a platform to explore the potential of Global Computing. This paper presents the design decisions of the first implementation of XtremWeb. We also present some early performance measurement, mostly to highlight that even some basic performance features are not well understood yet.", "num_citations": "106\n", "authors": ["498"]}
{"title": "Optimization of multi-level checkpoint model for large scale HPC applications\n", "abstract": " HPC community projects that future extreme scale systems will be much less stable than current Petascale systems, thus requiring sophisticated fault tolerance to guarantee the completion of large scale numerical computations. Execution failures may occur due to multiple factors with different scales, from transient uncorrectable memory errors localized in processes to massive system outages. Multi-level checkpoint/restart is a promising model that provides an elastic response to tolerate different types of failures. It stores checkpoints at different levels: e.g., local memory, remote memory, using a software RAID, local SSD, remote file system. In this paper, we respond to two open questions: 1) how to optimize the selection of checkpoint levels based on failure distributions observed in a system, 2) how to compute the optimal checkpoint intervals for each of these levels. The contribution is three-fold. (1) We build a\u00a0\u2026", "num_citations": "97\n", "authors": ["498"]}
{"title": "Taming of the shrew: Modeling the normal and faulty behaviour of large-scale hpc systems\n", "abstract": " HPC systems are complex machines that generate a huge volume of system state data called \"events\". Events are generated without following a general consistent rule and different hardware and software components of such systems have different failure rates. Distinguishing between normal system behaviour and faulty situation relies on event analysis. Being able to detect quickly deviations from normality is essential for system administration and is the foundation of fault prediction. As HPC systems continue to grow in size and complexity, mining event flows become more challenging and with the upcoming 10 Pet flop systems, there is a lot of interest in this topic. Current event mining approaches do not take into consideration the specific behaviour of each type of events and as a consequence, fail to analyze them according to their characteristics. In this paper we propose a novel way of characterizing the\u00a0\u2026", "num_citations": "96\n", "authors": ["498"]}
{"title": "Characterizing result errors in internet desktop grids\n", "abstract": " Desktop grids use the free resources in Intranet and Internet environments for large-scale computation and storage. While desktop grids offer a high return on investment, one critical issue is the validation of results returned by participating hosts. Several mechanisms for result validation have been previously proposed. However, the characterization of errors is poorly understood. To study error rates, we implemented and deployed a desktop grid application across several thousand hosts distributed over the Internet. We then analyzed the results to give quantitative and empirical characterization of errors stemming from input or output (I/O) failures. We find that in practice, error rates are widespread across hosts but occur relatively infrequently. Moreover, we find that error rates tend to not be stationary over time nor correlated between hosts. In light of these characterization results, we evaluated state-of-the\u00a0\u2026", "num_citations": "93\n", "authors": ["498"]}
{"title": "Lightweight silent data corruption detection based on runtime data analysis for HPC applications\n", "abstract": " Next-generation supercomputers are expected to have more components and, at the same time, consume several times less energy per operation. Consequently, the number of soft errors is expected to increase dramatically in the coming years. In this respect, techniques that leverage certain properties of iterative HPC applications (such as the smoothness of the evolution of a particular dataset) can be used to detect silent errors at the application level. In this paper, we present a pointwise detection model with two phases: one involving the prediction of the next expected value in the time series for each data point, and another determining a range (ie, normal value interval) surrounding the predicted next-step value. We show that dataset correlation can be used to detect corruptions indirectly and limit the size of the data set to monitor, taking advantage of the underlying physics of the simulation. Our results show\u00a0\u2026", "num_citations": "92\n", "authors": ["498"]}
{"title": "Big data and extreme-scale computing: Pathways to convergence-toward a shaping strategy for a future software and data ecosystem for scientific inquiry\n", "abstract": " Over the past four years, the Big Data and Exascale Computing (BDEC) project organized a series of five international workshops that aimed to explore the ways in which the new forms of data-centric discovery introduced by the ongoing revolution in high-end data analysis (HDA) might be integrated with the established, simulation-centric paradigm of the high-performance computing (HPC) community. Based on those meetings, we argue that the rapid proliferation of digital data generators, the unprecedented growth in the volume and diversity of the data they generate, and the intense evolution of the methods for analyzing and using that data are radically reshaping the landscape of scientific computing. The most critical problems involve the logistics of wide-area, multistage workflows that will move back and forth across the computing continuum, between the multitude of distributed sensors, instruments and other\u00a0\u2026", "num_citations": "91\n", "authors": ["498"]}
{"title": "Optimization of cloud task processing with checkpoint-restart mechanism\n", "abstract": " In this paper, we aim at optimizing fault-tolerance techniques based on a checkpointing/restart mechanism, in the context of cloud computing. Our contribution is three-fold.(1) We derive a fresh formula to compute the optimal number of checkpoints for cloud jobs with varied distributions of failure events. Our analysis is not only generic with no assumption on failure probability distribution, but also attractively simple to apply in practice.(2) We design an adaptive algorithm to optimize the impact of checkpointing regarding various costs like checkpointing/restart overhead.(3) We evaluate our optimized solution in a real cluster environment with hundreds of virtual machines and Berkeley Lab Checkpoint/Restart tool. Task failure events are emulated via a production trace produced on a large-scale Google data center. Experiments confirm that our solution is fairly suitable for Google systems. Our optimized formula\u00a0\u2026", "num_citations": "91\n", "authors": ["498"]}
{"title": "Error-controlled lossy compression optimized for high compression ratios of scientific datasets\n", "abstract": " Today's scientific simulations require a significant reduction of the data size because of extremely large volumes of data they produce and the limitation of storage bandwidth and space. If the compression is set to reach a high compression ratio, however, the reconstructed data are often distorted too much to tolerate. In this paper, we explore a new compression strategy that can effectively control the data distortion when significantly reducing the data size. The contribution is threefold. (1) We propose an adaptive compression framework to select either our improved Lorenzo prediction method or our optimized linear regression method dynamically in different regions of the dataset. (2) We explore how to select them accurately based on the data features in each block to obtain the best compression quality. (3) We analyze the effectiveness of our solution in details using four real-world scientific datasets with 100\u00a0\u2026", "num_citations": "90\n", "authors": ["498"]}
{"title": "On communication determinism in parallel HPC applications\n", "abstract": " Current fault tolerant protocols for high performance computing parallel applications have two major drawbacks: either they require to restart all processes even in the case of only a single process failure or they have a high performance overhead in fault free situation. As a consequence none of existing generic fault tolerant protocols matches needs of HPC applications and surprisingly, there is no fault tolerant protocol dedicated to them. One way to design better fault tolerant protocols for HPC applications is to explore and take advantage of their specific characteristics. In particular we suspect that most of them present some form of determinism in communication patterns. Communication determinism can play an important role in the design of new fault tolerant protocols by reducing their complexity. In this paper, we explore the communication determinism in 27 HPC parallel applications that are representative of\u00a0\u2026", "num_citations": "83\n", "authors": ["498"]}
{"title": "Bitdew: a programmable environment for large-scale data management and distribution\n", "abstract": " Desktop Grids use the computing, network and storage resources from idle desktop PC's distributed over multiple-LAN's or the Internet to compute a large variety of resource-demanding distributed applications. While these applications need to access, compute, store and circulate large volumes of data, little attention has been paid to data management in such large-scale, dynamic, heterogeneous, volatile and highly distributed Grids. In most cases, data management relies on ad-hoc solutions, and providing a general approach is still a challenging issue. To address this problem, we propose the BitDew framework, a programmable environment for automatic and transparent data management on computational Desktop Grids. This paper describes the BitDew programming interface, its architecture, and the performance evaluation of its runtime components. BitDew relies on a specific set of meta-data to drive key\u00a0\u2026", "num_citations": "83\n", "authors": ["498"]}
{"title": "Scheduling the I/O of HPC applications under congestion\n", "abstract": " A significant percentage of the computing capacity of large-scale platforms is wasted because of interferences incurred by multiple applications that access a shared parallel file system concurrently. One solution to handling I/O bursts enlarge-scale HPC systems is to absorb them at an intermediate storage layer consisting of burst buffers. However, our analysis of the Argonne's Mira system shows that burst buffers cannot prevent congestion at all times. Consequently, I/O performances dramatically degraded, showing in some cases a decrease in I/O throughput of 67%. In this paper, we analyze the effects of interference on application I/O bandwidth and propose several scheduling techniques to mitigate congestion. We show through extensive experiments that our global I/O scheduler is able to reduce the effects of congestion, even on systems where burst buffers are used, and can increase the overall system\u00a0\u2026", "num_citations": "82\n", "authors": ["498"]}
{"title": "XtremWeb & Condor: sharing resources between Internet connected Condor pool\n", "abstract": " Grid computing presents two major challenges for deploying large scale applications across wide area networks gathering volunteers PC and clusters/parallel computers as computational resources: security and fault tolerance. This paper presents a lightweight Grid solution for the deployment of multi-parameters applications on a set of clusters protected by firewalls. The system uses a hierarchical design based on Condor for managing each cluster locally and XtremWeb for enabling resource sharing among the clusters. We discuss the security and fault tolerance mechanisms used for this design and demonstrate the usefulness of the approach measuring the performances of a multi-parameters bio-chemistry application deployed on two sites: University of Wisconsin/Madison and Paris South University. This experiment shows that we can efficiently and safely harness the computational power of about 200 PC\u00a0\u2026", "num_citations": "76\n", "authors": ["498"]}
{"title": "Scalability comparison of four host virtualization tools\n", "abstract": " Virtualization tools are becoming popular in the context of Grid Computing because they allow running multiple operating systems on a single host and provide a confined execution environment. In several Grid projects, virtualization tools are envisioned to run many virtual machines per host. This immediately raises the issue of virtualization scalability.               In this paper, we compare the scalability merits of Four virtualization tools. First, from a simple experiment, we motivate the need for simple microbenchmarks. Second, we present a set of metrics and related methodologies. We propose four microbenchmarks to measure the different scalability parameters for the different machine resources (CPU, memory disk and network) on three scalability metrics (overhead, linearity and isolation). Third, we compare four virtual machine technologies (Vserver, Xen, UML and VMware).                The results of this\u00a0\u2026", "num_citations": "74\n", "authors": ["498"]}
{"title": "Hydee: Failure containment without event logging for large scale send-deterministic mpi applications\n", "abstract": " High performance computing will probably reach exascale in this decade. At this scale, mean time between failures is expected to be a few hours. Existing fault tolerant protocols for message passing applications will not be efficient anymore since they either require a global restart after a failure (check pointing protocols) or result in huge memory occupation (message logging). Hybrid fault tolerant protocols overcome these limits by dividing applications processes into clusters and applying a different protocol within and between clusters. Combining coordinated check pointing inside the clusters and message logging for the inter-cluster messages allows confining the consequences of a failure to a single cluster, while logging only a subset of the messages. However, in existing hybrid protocols, event logging is required for all application messages to ensure a correct execution after a failure. This can significantly\u00a0\u2026", "num_citations": "72\n", "authors": ["498"]}
{"title": "BitDew: A data management and distribution service with multi-protocol file transfer and metadata abstraction\n", "abstract": " Desktop Grids use the computing, network and storage resources from idle desktop PCs distributed over multiple-LANs or the Internet to compute a large variety of resource-demanding distributed applications. While these applications need to access, compute, store and circulate large volumes of data, little attention has been paid to data management in such large-scale, dynamic, heterogeneous, volatile and highly distributed Grids. In most cases, data management relies on ad hoc solutions, and providing a general approach is still a challenging issue. A new class of data management service is desirable to deal with such a variety of file transfer protocols than client/server, P2P or the new and emerging Cloud storage service.To address this problem, we propose the BitDew framework, a programmable environment for automatic and transparent data management on computational Desktop Grids. This paper\u00a0\u2026", "num_citations": "72\n", "authors": ["498"]}
{"title": "Improving the computing efficiency of hpc systems using a combination of proactive and preventive checkpointing\n", "abstract": " As the failure frequency is increasing with the components count in modern and future supercomputers, resilience is becoming critical for extreme scale systems. The association of failure prediction with proactive checkpointing seeks to reduce the effect of failures in the execution time of parallel applications. Unfortunately, proactive checkpointing does not systematically avoid restarting from scratch. To mitigate this issue, failure prediction and proactive checkpointing can be coupled with periodic checkpointing. However, blind use of these techniques does not always improves system efficiency, because everyone of them comes with a mix of overheads and benefits. In order to study and understand the combination of these techniques and their improvement in the system's efficiency, we developed: (i) a prototype combining state of the art failure prediction, fast proactive checkpointing and preventive checkpointing\u00a0\u2026", "num_citations": "71\n", "authors": ["498"]}
{"title": "Computing just what you need: Online data analysis and reduction at extreme scales\n", "abstract": " A growing disparity between supercomputer computation speeds and I/O rates makes it increasingly infeasible for applications to save all results for offline analysis. Instead, applications must analyze and reduce data online so as to output only those results needed to answer target scientific question(s). This change in focus complicates application and experiment design and introduces algorithmic, implementation, and programming model challenges that are unfamiliar to many scientists and that have major implications for the design of various elements of supercomputer systems. We review these challenges and describe methods and tools that we are developing to enable experimental exploration of algorithmic, software, and system design alternatives.", "num_citations": "66\n", "authors": ["498"]}
{"title": "Distributed diskless checkpoint for large scale systems\n", "abstract": " In high performance computing (HPC), the applications are periodically check pointed to stable storage to increase the success rate of long executions. Nowadays, the overhead imposed by disk-based checkpoint is about 20% of execution time and in the next years it will be more than 50% if the checkpoint frequency increases as the fault frequency increases. Diskless checkpoint has been introduced as a solution to avoid the IO bottleneck of disk-based checkpoint. However, the encoding time, the dedicated resources (the spares) and the memory overhead imposed by diskless checkpoint are significant obstacles against its adoption. In this work, we address these three limitations: 1) we propose a fault tolerant model able to tolerate up to 50% of process failures with a low check pointing overhead 2) our fault tolerance model works without spare node, while still guarantying high reliability, 3) we use solid state\u00a0\u2026", "num_citations": "66\n", "authors": ["498"]}
{"title": "Energy considerations in checkpointing and fault tolerance protocols\n", "abstract": " Exascale supercomputers will gather hundreds millions cores. The first problem that we address is resiliency and fault tolerance to reach application termination on such platforms. The second problem is energy consumption since such systems will consume enormous amount of energy. In this paper, we evaluate checkpointing and existing fault tolerance protocols from an energy point of view. We measure on a real testbed the power consumption of the main atomic operations found in these protocols. The first results show that process coordination and RAM consume more power than checkpointing and HDD logging. However, the results we presented in Joules per Bytes for I/O operations, emphasize that checkpointing and HDD logging consume more energy than RAM logging. Finally, we propose to consider energy consumption as a criterion for the choice of fault tolerance protocols. In terms of energy\u00a0\u2026", "num_citations": "63\n", "authors": ["498"]}
{"title": "On the use of cluster-based partial message logging to improve fault tolerance for mpi hpc applications\n", "abstract": " Fault tolerance is becoming a major concern in HPC systems. The two traditional approaches for message passing applications, coordinated checkpointing and message logging, have severe scalability issues. Coordinated checkpointing protocols make all processes roll back after a failure. Message logging protocols log a huge amount of data and can induce an overhead on communication performance. Hierarchical rollback-recovery protocols based on the combination of coordinated checkpointing and message logging are an alternative. These partial message logging protocols are based on process clustering: only messages between clusters are logged to limit the consequence of a failure to one cluster. These protocols would work efficiently only if one can find clusters of processes in the applications such that the ratio of logged messages is very low. We study the communication patterns of\u00a0\u2026", "num_citations": "60\n", "authors": ["498"]}
{"title": "Toward an optimal online checkpoint solution under a two-level HPC checkpoint model\n", "abstract": " The traditional single-level checkpointing method suffers from significant overhead on large-scale platforms. Hence, multilevel checkpointing protocols have been studied extensively in recent years. The multilevel checkpoint approach allows different levels of checkpoints to be set (each with different checkpoint overheads and recovery abilities), in order to further improve the fault tolerance performance of extreme-scale HPC applications. How to optimize the checkpoint intervals for each level, however, is an extremely difficult problem. In this paper, we construct an easy-to-use two-level checkpoint model. Checkpoint level 1 deals with errors with low checkpoint/recovery overheads such as transient memory errors, while checkpoint level 2 deals with hardware crashes such as node failures. Compared with previous optimization work, our new optimal checkpoint solution offers two improvements: (1) it is an online\u00a0\u2026", "num_citations": "56\n", "authors": ["498"]}
{"title": "Event log mining tool for large scale HPC systems\n", "abstract": " Event log files are the most common source of information for the characterization of events in large scale systems. However the large size of these files makes the task of manual analysing log messages to be difficult and error prone. This is the reason why recent research has been focusing on creating algorithms for automatically analysing these log files. In this paper we present a novel methodology for extracting templates that describe event formats from large datasets presenting an intuitive and user-friendly output to system administrators. Our algorithm is able to keep up with the rapidly changing environments by adapting the clusters to the incoming stream of events. For testing our tool, we have chosen 5 log files that have different formats and that challenge different aspects in the clustering task. The experiments show that our tool outperforms all other algorithms in all tested scenarios achieving an\u00a0\u2026", "num_citations": "56\n", "authors": ["498"]}
{"title": "Detecting silent data corruption through data dynamic monitoring for scientific applications\n", "abstract": " Parallel programming has become one of the best ways to express scientific models that simulate a wide range of natural phenomena. These complex parallel codes are deployed and executed on large-scale parallel computers, making them important tools for scientific discovery. As supercomputers get faster and larger, the increasing number of components is leading to higher failure rates. In particular, the miniaturization of electronic components is expected to lead to a dramatic rise in soft errors and data corruption. Moreover, soft errors can corrupt data silently and generate large inaccuracies or wrong results at the end of the computation. In this paper we propose a novel technique to detect silent data corruption based on data monitoring. Using this technique, an application can learn the normal dynamics of its datasets, allowing it to quickly spot anomalies. We evaluate our technique with synthetic\u00a0\u2026", "num_citations": "55\n", "authors": ["498"]}
{"title": "Adaptive impact-driven detection of silent data corruption for HPC applications\n", "abstract": " For exascale HPC applications, silent data corruption (SDC) is one of the most dangerous problems because there is no indication that there are errors during the execution. We propose an adaptive impact-driven method that can detect SDCs dynamically. The key contributions are threefold. (1) We carefully characterize 18 HPC applications/benchmarks and discuss the runtime data features, as well as the impact of the SDCs on their execution results. (2) We propose an impact-driven detection model that does not blindly improve the prediction accuracy, but instead detects only influential SDCs to guarantee user-acceptable execution results. (3) Our solution can adapt to dynamic prediction errors based on local runtime data and can automatically tune detection ranges for guaranteeing low false alarms. Experiments show that our detector can detect 80-99.99 percent of SDCs with a false alarm rate less that 1\u00a0\u2026", "num_citations": "54\n", "authors": ["498"]}
{"title": "Scheduling independent tasks sharing large data distributed with bittorrent\n", "abstract": " Data-centric applications are still a challenging issue for large scale distributed computing systems. The emergence of new protocols and software for collaborative content distribution over Internet offers a new opportunity for efficient and fast delivery of high volume of data. In a previous paper, we have investigated BitTorrent as a protocol for data diffusion in the context of computational desktop grid. We showed that BitTorrent is efficient for large file transfers, scalable when the number of nodes increases but suffers from a high overhead when transmitting small files. This paper investigates two approach to overcome these limitations. First, we propose a performance model to select the best of FTP and BitTorrent protocols according to the size of the file to distribute and the number of receiver nodes. Next we propose enhancement of the BitTorrent protocol which provides more predictable communication patterns\u00a0\u2026", "num_citations": "53\n", "authors": ["498"]}
{"title": "Failure prediction for HPC systems and applications: Current situation and open issues\n", "abstract": " As large-scale systems evolve towards post-petascale computing, it is crucial to focus on providing fault-tolerance strategies that aim to minimize fault\u2019s effects on applications. By far the most popular technique is the checkpoint\u2013restart strategy. A complement to this classical approach is failure avoidance, by which the occurrence of a fault is predicted and proactive measures are taken. This requires a reliable prediction system to anticipate failures and their locations. One way of offering prediction is by the analysis of system logs generated during production by large-scale systems. Current research in this field presents a number of limitations that make them unusable for running on real production high-performance computing (HPC) systems. Based on our observations that different failures have different distributions and behaviours, we propose a novel hybrid approach that combines signal analysis with data\u00a0\u2026", "num_citations": "48\n", "authors": ["498"]}
{"title": "Adaptive event prediction strategy with dynamic time window for large-scale hpc systems\n", "abstract": " In this paper, we analyse messages generated by different HPC large-scale systems in order to extract sequences of correlated events which we lately use to predict the normal and faulty behaviour of the system. Our method uses a dynamic window strategy that is able to find frequent sequences of events regardless on the time delay between them. Most of the current related research narrows the correlation extraction to fixed and relatively small time windows that do not reflect the whole behaviour of the system. The generated events are in constant change during the lifetime of the machine. We consider that it is important to update the sequences at runtime by applying modifications after each prediction phase according to the forecast's accuracy and the difference between what was expected and what really happened. Our experiments show that our analysing system is able to predict around 60% of events with\u00a0\u2026", "num_citations": "48\n", "authors": ["498"]}
{"title": "Investigating the performance of two programming models for clusters of SMP PCs\n", "abstract": " Multiprocessors and high performance networks allow building CLUsters of MultiProcessors (CLUMPs). One distinctive feature over traditional parallel computers is their hybrid memory model (message passing between the nodes and shared memory inside the nodes). We evaluate the performance of a cluster of dual processor PCs connected by a Myrinet network for NAS benchmarks using two programming models: a Single Memory Model based on the MPICH-PM/CLUMP library of the RWCP and a Hybrid Memory Model using MPICH-PM and OpenMP. MPI programs are used as the reference in all experiments involving programming models. We compare dual processor node configurations speedup versus uniprocessor node configurations for each model. We demonstrate that the superiority of one model over the other depends on the features of the applications. In particular, we detail the speedup results\u00a0\u2026", "num_citations": "48\n", "authors": ["498"]}
{"title": "Detecting and correcting data corruption in stencil applications through multivariate interpolation\n", "abstract": " High-performance computing is a powerful tool that allows scientists to study complex natural phenomena. Extreme-scale supercomputers promise orders of magnitude higher performance compared with that of current systems. However, power constrains in future exascale systems might limit the level of resilience of those machines. In particular, data could get corrupted silently, that is, without the hardware detecting the corruption. This situation is clearly unacceptable: simulation results must be within the error margin specified by the user. In this paper, we exploit multivariate interpolation in order to detect and correct data corruption in stencil applications. We evaluate this technique with a turbulent fluid application, and we demonstrate that the prediction error using multivariate interpolation is on the order of 0.01. Our results show that this mechanism can detect and correct most important corruptions and keep the\u00a0\u2026", "num_citations": "46\n", "authors": ["498"]}
{"title": "Characterizing and modeling cloud applications/jobs on a Google data center\n", "abstract": " In this paper, we characterize and model Google applications and jobs, based on a 1-month Google trace from a large-scale Google data center. We address four contributions: (1) we compute the valuable statistics about task events and resource utilization for Google applications, based on various types of resources and execution types; (2) we analyze the classification of applications via a K-means clustering algorithm with optimized number of sets, based on task events and resource usage; (3) we study the correlation of Google application properties and running features (e.g., job priority and scheduling class); (4) we finally build a model that can simulate Google jobs/tasks and dynamic events, in accordance with Google trace. Experiments show that the tasks simulated based on our model exhibit fairly analogous features with those in Google trace. 95+\u00a0% of tasks\u2019 simulation errors are 20\u00a0\u2026", "num_citations": "46\n", "authors": ["498"]}
{"title": "Hihcohp-toward a realistic communication model for hierarchical hyperclusters of heterogeneous processors\n", "abstract": " A parameterized model of hyperclusters of processors-clusters of clusters of... of clusters of processors-is formulated under which a hypercluster enjoys generality along three orthogonal axes: (1) Its processors are heterogeneous: they may have different computational powers (speed of computation and memory access). (2) Its constituent clusters are interconnected via a hierarchy of networks of possibly differing bandwidths and speeds. (3) Its clusters at each level of the hierarchy are heterogeneous: they may differ in size. The model accounts for architectural details such as the bandwidths and transit costs of both networks and their ports. The algorithmic tractability of the model is demonstrated via broadcast and reduction algorithms, which are predictably efficient in general and actually optimal in special circumstances.", "num_citations": "46\n", "authors": ["498"]}
{"title": "An algorithmic model for heterogeneous hyper-clusters: Rationale and experience\n", "abstract": " A formal model of hyperclusters of processors\u2014that  is, clusters of clusters of \u2026 of clusters of processors\u2014is  formulated. The model characterizes a hypercluster \u210b via a  suite of parameters that expose the computational and communicational  powers of \u210b's constituent processors and networks. The  hyperclusters studied enjoy heterogeneity along three orthogonal axes.  (1) The processors that populate a hypercluster may differ in  computational powers (speed of computation and memory access). (2)  The clusters comprising a hypercluster are organized hierarchically  and are interconnected via a hierarchy of networks of possibly  differing bandwidths and speeds. (3) The clusters at each level of  the hierarchy may differ in sizes. The resulting HiHCoHP model  is rather detailed, exposing architectural features such as the  bandwidth and transit costs of both networks and their ports. The  algorithmic tractability of the\u00a0\u2026", "num_citations": "44\n", "authors": ["498"]}
{"title": "Optimizing lossy compression rate-distortion from automatic online selection between SZ and ZFP\n", "abstract": " With ever-increasing volumes of scientific data produced by high-performance computing applications, significantly reducing data size is critical because of limited capacity of storage space and potential bottlenecks on I/O or networks in writing/reading or transferring data. SZ and ZFP are two leading BSD licensed open source C/C++ libraries for compressed floating-point arrays that support high throughput read and write random access. However, their performance is not consistent across different data sets and across different fields of some data sets, which raises the need for an automatic online (during compression) selection between SZ and ZFP, with minimal overhead. In this paper, the automatic selection optimizes the rate-distortion, an important statistical quality metric based on the signal-to-noise ratio. To optimize for rate-distortion, we investigate the principles of SZ and ZFP. We then propose an efficient\u00a0\u2026", "num_citations": "43\n", "authors": ["498"]}
{"title": "BlobCR: Virtual disk based checkpoint-restart for HPC applications on IaaS clouds\n", "abstract": " Infrastructure-as-a-Service (IaaS) cloud computing is gaining significant interest in industry and academia as an alternative platform for running HPC applications. Given the need to provide fault tolerance, support for suspend\u2013resume and offline migration, an efficient Checkpoint-Restart mechanism becomes paramount in this context. We propose BlobCR, a dedicated checkpoint repository that is able to take live incremental snapshots of the whole disk attached to the virtual machine (VM) instances. BlobCR aims to minimize the performance overhead of checkpointing by persisting VM disk snapshots asynchronously in the background using a low overhead technique we call selective copy-on-write. It includes support for both application-level and process-level checkpointing, as well as support to roll back filesystem changes. Experiments at large scale demonstrate the benefits of our proposal both in synthetic\u00a0\u2026", "num_citations": "43\n", "authors": ["498"]}
{"title": "A hybrid local storage transfer scheme for live migration of i/o intensive workloads\n", "abstract": " Live migration of virtual machines (VMs) is key feature of virtualization that is extensively leveraged in IaaS cloud environments: it is the basic building block of several important features, such as load balancing, pro-active fault tolerance, power management, online maintenance, etc. While most live migration efforts concentrate on how to transfer the memory from source to destination during the migration process, comparatively little attention has been devoted to the transfer of storage. This problem is gaining increasing importance: due to performance reasons, virtual machines that run large-scale, data-intensive applications tend to rely on local storage, which poses a difficult challenge on live migration: it needs to handle storage transfer in addition to memory transfer. This paper proposes a memory migration independent approach that addresses this challenge. It relies on a hybrid active push/prioritized prefetch\u00a0\u2026", "num_citations": "43\n", "authors": ["498"]}
{"title": "Use cases of lossy compression for floating-point data in scientific data sets\n", "abstract": " Architectural and technological trends of systems used for scientific computing call for a significant reduction of scientific data sets that are composed mainly of floating-point data. This article surveys and presents experimental results of currently identified use cases of generic lossy compression to address the different limitations of scientific computing systems. The article shows from a collection of experiments run on parallel systems of a leadership facility that lossy data compression not only can reduce the footprint of scientific data sets on storage but also can reduce I/O and checkpoint/restart times, accelerate computation, and even allow significantly larger problems to be run than without lossy compression. These results suggest that lossy compression will become an important technology in many aspects of high performance scientific computing. Because the constraints for each use case are different and often\u00a0\u2026", "num_citations": "42\n", "authors": ["498"]}
{"title": "An efficient transformation scheme for lossy data compression with point-wise relative error bound\n", "abstract": " Because of the ever-increasing execution scale of scientific applications, how to store the extremely large volume of data efficiently is becoming a serious issue. A significant reduction of the scientific data size can effectively mitigate the I/O burden and save considerable storage space. Since lossless compressors suffer from limited compression ratios, error-controlled lossy compressors have been studied for years. Existing error-controlled lossy compressors, however, focus mainly on absolute error bounds, which cannot meet users' diverse demands such as pointwise relative error bounds. Although some of the state-of-the-art lossy compressors support pointwise relative error bound, the compression ratios are generally low because of the limitation in their designs and possible spiky data changes in local data regions. In this work, we propose a novel, efficient approach to perform compression based on the\u00a0\u2026", "num_citations": "42\n", "authors": ["498"]}
{"title": "Selecting a virtualization system for grid/p2p large scale emulation\n", "abstract": " Virtualization is becoming a key feature of Grids where it essentially provides the feature of abstracting some specific characteristics of the Grid infrastructure. The concept of a Virtual Machine (VM) is not recent and was proposed very early in the history of computers. There are several approaches at the hardware and the OS levels. As presented in [1], an example of pioneer works was the VM of the IBM 370. VM were identical copies of the hardware where every copy runs its own OS. Currently, there is a trend toward the adoption of the VM concept. Microprocessor vendors (AMD, Intel, IBM, etc.) are proposing new hardware mechanisms to improve virtualization performance in their last generation of products. The main motivations of machine virtualization in the context of Grid and large scale distributed system experimental platforms (PlanetLab, Grid eXplorer) are the following: a) each VM provides a confined environment where non-trusted applications can be run, b) a VM can limit hardware resource access and usage, through isolation techniques, c) VM allows adapting the runtime environment to the application instead of porting the application to the runtime environment, d) VM allows using dedicated or optimized OS mechanisms (scheduler, virtual memory management, network protocol) for each application, e) Applications and processes running within a VM can be managed as a whole. For example, time allocation can be different for every VM and under the control of a VM manager (also called scheduler, hypervisor or orchestrator).", "num_citations": "42\n", "authors": ["498"]}
{"title": "Logaider: A tool for mining potential correlations of hpc log events\n", "abstract": " Today's large-scale supercomputers are producing a huge amount of log data. Exploring various potential correlations of fatal events is crucial for understanding their causality and improving the working efficiency for system administrators. To this end, we developed a toolkit, named LogAider, that can reveal three types of potential correlations: across-field, spatial, and temporal. Across-field correlation refers to the statistical correlation across fields within a log or across multiple logs based on probabilistic analysis. For analyzing the spatial correlation of events, we developed a generic, easy-to-use visualizer that can view any events queried by userson a system machine graph. LogAider can also mine spatial correlations by an optimized K-meaning clustering algorithm over a Torus network topology. It is also able to disclose the temporal correlations (or error propagations) over a certain period inside a log or\u00a0\u2026", "num_citations": "40\n", "authors": ["498"]}
{"title": "GloudSim: Google trace based cloud simulator with virtual machines\n", "abstract": " In 2011, Google released a 1\u2010month production trace with hundreds of thousands of jobs running across over 12,000 heterogeneous hosts. In order to perform in\u2010depth research based on the trace, it is necessary to construct a close\u2010to\u2010practice simulation system. In this paper, we devise a distributed cloud simulator (or toolkit) based on virtual machines, with three important features. (1) The dynamic changing resource amounts (such as CPU rate and memory size) consumed by the reproduced jobs can be emulated as closely as possible to the real values in the trace. (2) Various types of events (e.g., kill/evict event) can be emulated precisely based on the trace. (3) Our simulation toolkit is able to emulate more complex and useful cases beyond the original trace to adapt to various research demands. We evaluate the system on a real cluster environment with 16\u00d78=128 cores and 112 virtual machines constructed\u00a0\u2026", "num_citations": "40\n", "authors": ["498"]}
{"title": "Toward high communication performance through compiled communications on a circuit switched interconnection network\n", "abstract": " This paper discusses a new principle of interconnection network for massively parallel architectures in the field of numerical computation. The principle is motivated by an analysis of the application features and the need to design new kind of communication networks combining very high bandwidth, very low latency, performance independence to communication pattern or network load and a performance improvement proportional to the hardware performance improvement. Our approach is to associate compiled communications and a circuit switched interconnection network. This paper presents the motivations for this principle, the hardware and software issues and the design of a first prototype. The expected performance are a sustained aggregate bandwidth of more than 500 GBytes/s and an overall latency less than 270 ns, for a large implementation (4K inputs) with the current available technology.< >", "num_citations": "39\n", "authors": ["498"]}
{"title": "Transferring a petabyte in a day\n", "abstract": " Extreme-scale simulations and experiments can generate large amounts of data, whose volume can exceed the compute and/or storage capacity at the simulation or experimental facility. With the emergence of ultra-high-speed networks, researchers are considering pipelined approaches in which data are passed to a remote facility for analysis. Here we examine an extreme-scale cosmology simulation that, when run on a large fraction of a leadership computer, generates data at a rate of one petabyte per elapsed day. Writing those data to disk is inefficient and impractical, and in situ analysis poses its own difficulties. Thus we implement a pipeline in which data are generated on one supercomputer and then transferred, as they are generated, to a remote supercomputer for analysis. We use the Swift scripting language to instantiate this pipeline across Argonne National Laboratory and the National Center for\u00a0\u2026", "num_citations": "38\n", "authors": ["498"]}
{"title": "Full-state quantum circuit simulation by using data compression\n", "abstract": " Quantum circuit simulations are critical for evaluating quantum algorithms and machines. However, the number of state amplitudes required for full simulation increases exponentially with the number of qubits. In this study, we leverage data compression to reduce memory requirements, trading computation time and fidelity for memory space. Specifically, we develop a hybrid solution by combining the lossless compression and our tailored lossy compression method with adaptive error bounds at each timestep of the simulation. Our approach optimizes for compression speed and makes sure that errors due to lossy compression are uncorrelated, an important property for comparing simulation output with physical machines. Experiments show that our approach reduces the memory requirement of simulating the 61-qubit Grover's search algorithm from 32 exabytes to 768 terabytes of memory on Argonne's Theta\u00a0\u2026", "num_citations": "37\n", "authors": ["498"]}
{"title": "An efficient silent data corruption detection method with error-feedback control and even sampling for HPC applications\n", "abstract": " The silent data corruption (SDC) problem is attracting more and more attentions because it is expected to have a great impact on exascale HPC applications. SDC faults are hazardous in that they pass unnoticed by hardware and can lead to wrong computation results. In this work, we formulate SDC detection as a runtime one-step-ahead prediction method, leveraging multiple linear prediction methods in order to improve the detection results. The contributions are twofold: (1) we propose an error feedback control model that can reduce the prediction errors for different linear prediction methods, and (2) we propose a spatial-data-based even-sampling method to minimize the detection overheads (including memory and computation cost). We implement our algorithms in the fault tolerance interface, a fault tolerance library with multiple checkpoint levels, such that users can conveniently protect their HPC applications\u00a0\u2026", "num_citations": "37\n", "authors": ["498"]}
{"title": "Checkpointing vs. migration for post-petascale supercomputers\n", "abstract": " An alternative to classical fault-tolerant approaches for large-scale clusters is failure avoidance, by which the occurrence of a fault is predicted and a preventive measure is taken. We develop analytical performance models for two types of preventive measures: preventive checkpointing and preventive migration. We also develop an analytical model of the performance of a standard periodic checkpoint fault-tolerant approach. We instantiate these models for platform scenarios representative of current and future technology trends. We find that preventive migration is the better approach in the short term by orders of magnitude. However, in the longer term, both approaches have comparable merit with a marginal advantage for preventive checkpointing. We also find that standard non-prediction-based fault tolerance achieves poor scaling when compared to prediction-based failure avoidance, thereby demonstrating\u00a0\u2026", "num_citations": "37\n", "authors": ["498"]}
{"title": "Collaborative data distribution with bittorrent for computational desktop grids\n", "abstract": " Data-centric applications are still a challenging issue for large scale distributed computing systems. The emergence of new protocols and software for collaborative content distribution over Internet offers a new opportunity for efficient and fast delivery of high volume of data. This paper presents an evaluation of the BitTorrent protocol for computational desktop grids. We first present a prototype of a generic subsystem dedicated to data management and designed to serve as a building block for any desktop grid system. Based on this prototype we conduct experimentations to evaluate the potential of BitTorrent compared to a classical approach based on FTP data server. The preliminary results obtained with a 65-nodes cluster measure the basic characteristics of BitTorrent in terms of latency and bandwidth and evaluate the scalability of BitTorrent for the delivery of large input files. Moreover, we show that BitTorrent\u00a0\u2026", "num_citations": "37\n", "authors": ["498"]}
{"title": "Extreme heterogeneity 2018-productive computational science in the era of extreme heterogeneity: Report for DOE ASCR workshop on extreme heterogeneity\n", "abstract": " Fundamental trends in computer architecture predict that nearly all aspects of future high-performance computing (HPC) architectures will have many more diverse components than past systems, leading toward a period of extreme heterogeneity (EH). The HPC community has already seen evidence of this trend [1, 2, 7, 9, 12, 15, 70]. In 2009, the drive to deploy more energy-efficient computers led the Oak Ridge Leadership Computing Facility (OLCF) to propose a system upgrade, composed of CPUs (central processing units) coupled with GPUs (graphics processing units), that firmly established an era of heterogeneous HPC across the Department of Energy (DOE). Continuing this trend, both recently deployed CORAL systems, Summit and Sierra\u2014selected by OLCF and Lawrence Livermore National Laboratory, respectively\u2014are composed of CPUs coupled with multiple GPUs and three types of memory. This trend is also manifested in the most recent TOP500 list: heterogeneous accelerators are used in 110 TOP500 systems and the majority of the TOP10 systems. Furthermore, a recent analysis of vendors\u2019 current architectural roadmaps is consistent with the extreme heterogeneity that the Advanced Scientific Computing Research (ASCR) program is seeing in its computing upgrades. It indicates that future computers will be more complex and will be composed of a variety of processing units and specialized accelerators supported by open interconnects and deep memory hierarchies. Looking forward, we expect even more diverse accelerators for paradigms like machine learning, neuromorphic computing, and quantum computing.Several\u00a0\u2026", "num_citations": "36\n", "authors": ["498"]}
{"title": "Adaptive algorithm for minimizing cloud task length with prediction errors\n", "abstract": " Compared to traditional distributed computing like grid system, it is non-trivial to optimize cloud task's execution performance due to its more constraints like user payment budget and divisible resource demand. In this paper, we analyze in-depth our proposed optimal algorithm minimizing task execution length with divisible resources and payment budget: 1) We derive the upper bound of cloud task length, by taking into account both workload prediction errors and hostload prediction errors. With such state-of-the-art bounds, the worst-case task execution performance is predictable, which can improve the quality of service in turn. 2) We design a dynamic version for the algorithm to adapt to the load dynamics over task execution progress, further improving the resource utilization. 3) We rigorously build a cloud prototype over a real cluster environment with 56 virtual machines, and evaluate our algorithm with different\u00a0\u2026", "num_citations": "36\n", "authors": ["498"]}
{"title": "Veloc: Towards high performance adaptive asynchronous checkpointing at large scale\n", "abstract": " Global checkpointing to external storage (e.g., a parallel file system) is a common I/O pattern of many HPC applications. However, given the limited I/O throughput of external storage, global checkpointing can often lead to I/O bottlenecks. To address this issue, a shift from synchronous checkpointing (i.e., blocking until writes have finished) to asynchronous checkpointing (i.e., writing to faster local storage and flushing to external storage in the background) is increasingly being adopted. However, with rising core count per node and heterogeneity of both local and external storage, it is non-trivial to design efficient asynchronous checkpointing mechanisms due to the complex interplay between high concurrency and I/O performance variability at both the node-local and global levels. This problem is not well understood but highly important for modern supercomputing infrastructures. This paper proposes a versatile\u00a0\u2026", "num_citations": "35\n", "authors": ["498"]}
{"title": "Preventive migration vs. preventive checkpointing for extreme scale supercomputers\n", "abstract": " An alternative to classical fault-tolerant approaches for large-scale clusters is failure avoidance, by which the occurrence of a fault is predicted and a preventive measure is taken. We develop analytical performance models for two types of preventive measures: preventive checkpointing and preventive migration. We instantiate these models for platform scenarios representative of current and future technology trends. We find that preventive migration is the better approach in the short term by orders of magnitude. However, in the longer term, both approaches have comparable merit with a marginal advantage for preventive checkpointing. We also develop an analytical model of the performance for fault tolerance based on periodic checkpointing and compare this approach to both failure avoidance techniques. We find that this comparison is sensitive to the nature of the stochastic distribution of the time between\u00a0\u2026", "num_citations": "33\n", "authors": ["498"]}
{"title": "Private virtual cluster: Infrastructure and protocol for instant grids\n", "abstract": " Given current complexity of Grid technologies, the lack of security of P2P systems and the rigidity of VPN technologies make sharing resources belonging to different institutions still technically difficult. We propose a new approach called \u201dInstant Grid\u201d (IG), which combines various Grid, P2P and VPN approaches, allowing simple deployment of applications over different administration domains. Three main requirements should be fulfilled to make Instant Grids realistic: 1) simple networking configuration (Firewall and NAT), 2) no degradation of resource security and 3) no need to re-implement existing distributed applications. In this paper, we present Private Virtual Cluster, a low-level middleware that meets these three requirements. To demonstrate its properties, we have connected with PVC a set of firewall-protected PCs and conducted experiments to evaluate the networking performance and the capability\u00a0\u2026", "num_citations": "33\n", "authors": ["498"]}
{"title": "Z-checker: A framework for assessing lossy compression of scientific data\n", "abstract": " Because of the vast volume of data being produced by today\u2019s scientific simulations and experiments, lossy data compressor allowing user-controlled loss of accuracy during the compression is a relevant solution for significantly reducing the data size. However, lossy compressor developers and users are missing a tool to explore the features of scientific data sets and understand the data alteration after compression in a systematic and reliable way. To address this gap, we have designed and implemented a generic framework called Z-checker. On the one hand, Z-checker combines a battery of data analysis components for data compression. On the other hand, Z-checker is implemented as an open-source community tool to which users and developers can contribute and add new analysis components based on their additional analysis demands. In this article, we present a survey of existing lossy compressors\u00a0\u2026", "num_citations": "32\n", "authors": ["498"]}
{"title": "Damaris: Addressing performance variability in data management for post-petascale simulations\n", "abstract": " With exascale computing on the horizon, reducing performance variability in data management tasks (storage, visualization, analysis, etc.) is becoming a key challenge in sustaining high performance. This variability significantly impacts the overall application performance at scale and its predictability over time. In this article, we present Damaris, a system that leverages dedicated cores in multicore nodes to offload data management tasks, including I/O, data compression, scheduling of data movements, in situ analysis, and visualization. We evaluate Damaris with the CM1 atmospheric simulation and the Nek5000 computational fluid dynamic simulation on four platforms, including NICS\u2019s Kraken and NCSA\u2019s Blue Waters. Our results show that (1) Damaris fully hides the I/O variability as well as all I/O-related costs, thus making simulation performance predictable; (2) it increases the sustained write throughput by a\u00a0\u2026", "num_citations": "32\n", "authors": ["498"]}
{"title": "Exploring the feasibility of lossy compression for PDE simulations\n", "abstract": " Checkpoint restart plays an important role in high-performance computing (HPC) applications, allowing simulation runtime to extend beyond a single job allocation and facilitating recovery from hardware failure. Yet, as machines grow in size and in complexity, traditional approaches to checkpoint restart are becoming prohibitive. Current methods store a subset of the application\u2019s state and exploit the memory hierarchy in the machine. However, as the energy cost of data movement continues to dominate, further reductions in checkpoint size are needed. Lossy compression, which can significantly reduce checkpoint sizes, offers a potential to reduce computational cost in checkpoint restart. This article investigates the use of numerical properties of partial differential equation (PDE) simulations, such as bounds on the truncation error, to evaluate the feasibility of using lossy compression in checkpointing PDE\u00a0\u2026", "num_citations": "30\n", "authors": ["498"]}
{"title": "Low-overhead diskless checkpoint for hybrid computing systems\n", "abstract": " As the size of new supercomputers scales to tens of thousands of sockets, the mean time between failures (MTBF) is decreasing to just several hours and long executions need some kind of fault tolerance method to survive failures. Checkpoint\\Restart is a popular technique used for this purpose; but writing the state of a big scientific application to remote storage will become prohibitively expensive in the near future. Diskless checkpoint was proposed as a solution to avoid the I/O bottleneck of disk-based checkpoint. However, the complex time-consuming encoding techniques hinder its scalability. At the same time, heterogeneous computing is becoming more and more popular in high performance computing (HPC), with new clusters combining CPUs and graphic processing units (GPUs). However, hybrid applications cannot always use all the resources available on the nodes, leaving some idle resources suc h us\u00a0\u2026", "num_citations": "30\n", "authors": ["498"]}
{"title": "Performance comparison of MPI and OpenMP on shared memory multiprocessors\n", "abstract": " When using a shared memory multiprocessor, the programmer faces the issue of selecting the portable programming model which will provide the best performance. Even if they restricts their choice to the standard programming environments (MPI and OpenMP), they have to select a programming approach among MPI and the variety of OpenMP programming styles. To help the programmer in their decision, we compare MPI with three OpenMP programming styles (loop level, loop level with large parallel sections, SPMD) using a subset of the NAS benchmark (CG, MG, FT, LU), two dataset sizes (A and B), and two shared memory multiprocessors (IBM SP3 NightHawk II, SGI Origin 3800). We have developed the first SPMD OpenMP version of the NAS benchmark and gathered other OpenMP versions from independent sources (PBN, SDSC and RWCP). Experimental results demonstrate that OpenMP provides\u00a0\u2026", "num_citations": "30\n", "authors": ["498"]}
{"title": "Optimization of a multilevel checkpoint model with uncertain execution scales\n", "abstract": " Future extreme-scale systems are expected to experience different types of failures affecting applications with different failure scales, from transient uncorrectable memory errors in processes to massive system outages. In this paper, we propose a multilevel checkpoint model by taking into account uncertain execution scales (different numbers of processes/cores). The contribution is threefold: (1) we provide an in-depth analysis on why it is difficult to derive the optimal checkpoint intervals for different checkpoint levels and optimize the number of cores simultaneously, (2) we devise a novel method that can quickly obtain an optimized solution -- the first successful attempt in multilevel checkpoint models with uncertain scales, and (3) we perform both large scale real experiments and extreme-scale numerical simulation to validate the effectiveness of our design. The experiments confirm that our optimized solution\u00a0\u2026", "num_citations": "29\n", "authors": ["498"]}
{"title": "DeepSZ: A novel framework to compress deep neural networks by using error-bounded lossy compression\n", "abstract": " Today's deep neural networks (DNNs) are becoming deeper and wider because of increasing demand on the analysis quality and more and more complex applications to resolve. The wide and deep DNNs, however, require large amounts of resources (such as memory, storage, and I/O), significantly restricting their utilization on resource-constrained platforms. Although some DNN simplification methods (such as weight quantization) have been proposed to address this issue, they suffer from either low compression ratios or high compression errors, which may introduce an expensive fine-tuning overhead (ie, a costly retraining process for the target inference accuracy). In this paper, we propose DeepSZ: an accuracy-loss expected neural network compression framework, which involves four key steps: network pruning, error bound assessment, optimization for error bound configuration, and compressed model\u00a0\u2026", "num_citations": "28\n", "authors": ["498"]}
{"title": "Coupling exascale multiphysics applications: Methods and lessons learned\n", "abstract": " With the growing computational complexity of science and the complexity of new and emerging hardware, it is time to re-evaluate the traditional monolithic design of computational codes. One new paradigm is constructing larger scientific computational experiments from the coupling of multiple individual scientific applications, each targeting their own physics, characteristic lengths, and/or scales. We present a framework constructed by leveraging capabilities such as in-memory communications, workflow scheduling on HPC resources, and continuous performance monitoring. This code coupling capability is demonstrated by a fusion science scenario, where differences between the plasma at the edges and at the core of a device have different physical descriptions. This infrastructure not only enables the coupling of the physics components, but it also connects in situ or online analysis, compression, and\u00a0\u2026", "num_citations": "28\n", "authors": ["498"]}
{"title": "Improving floating point compression through binary masks\n", "abstract": " Modern scientific technology such as particle accelerators, telescopes, and supercomputers are producing extremely large amounts of data. That scientific data needs to be processed by using systems with high computational capabilities such as supercomputers. Given that the scientific data is increasing in size at an exponential rate, storing and accessing the data are becoming expensive in both time and space. Most of this scientific data is stored by using floating point representation. Scientific applications executed on supercomputers spend a large amount of CPU cycles reading and writing floating point values, making data compression techniques an interesting way to increase computing efficiency. Given the accuracy requirements of scientific computing, we only focus on lossless data compression. In this paper we propose a masking technique that partially decreases the entropy of scientific datasets\u00a0\u2026", "num_citations": "28\n", "authors": ["498"]}
{"title": "Towards efficient data distribution on computational desktop grids with BitTorrent\n", "abstract": " Datacentric applications are still a challenging issue for large-scale distributed computing systems. The emergence of new protocols and software for collaborative content distribution over the Internet offers a new opportunity for efficient and fast delivery of a high volume of data. This paper presents an evaluation of the BitTorrent protocol for computational desktop grids. We first present a prototype of a generic subsystem dedicated to data management and designed to serve as a building block for any desktop grid system. Based on this prototype we conduct experiments to evaluate the potential of BitTorrent compared to a classical approach based on FTP data server. The preliminary results obtained with a 65-node cluster measure the basic characteristics of BitTorrent in terms of latency and bandwidth and evaluate the scalability of BitTorrent for the delivery of large input files. Moreover, we show that BitTorrent\u00a0\u2026", "num_citations": "28\n", "authors": ["498"]}
{"title": "Understanding performance of SMP clusters running MPI programs\n", "abstract": " Clusters of multiprocessors (CLUMPs) have an hybrid memory model, with message passing between nodes and shared memory inside nodes. We examine the performance of Myrinet clusters of SMP PCs when using a single memory model (SMM) based on the MPICH-PM/CLUMP library of the RWCP, which can directly use the MPI programs written for a cluster of uniprocessors. The specificities of the communication patterns with the SMM approach are detailed. PC clusters with 2-way and 4-way nodes are considered and compared.", "num_citations": "28\n", "authors": ["498"]}
{"title": "Improving performance of iterative methods by lossy checkponting\n", "abstract": " Iterative methods are commonly used approaches to solve large, sparse linear systems, which are fundamental operations for many modern scientific simulations. When the large-scale iterative methods are running with a large number of ranks in parallel, they have to checkpoint the dynamic variables periodically in case of unavoidable fail-stop errors, requiring fast I/O systems and large storage space. To this end, significantly reducing the checkpointing overhead is critical to improving the overall performance of iterative methods. Our contribution is fourfold.(1) We propose a novel lossy checkpointing scheme that can significantly improve the checkpointing performance of iterative methods by leveraging lossy compressors.(2) We formulate a lossy checkpointing performance model and derive theoretically an upper bound for the extra number of iterations caused by the distortion of data in lossy checkpoints, in\u00a0\u2026", "num_citations": "27\n", "authors": ["498"]}
{"title": "Ai-ckpt: leveraging memory access patterns for adaptive asynchronous incremental checkpointing\n", "abstract": " With increasing scale and complexity of supercomputing and cloud computing architectures, faults are becoming a frequent occurrence, which makes reliability a difficult challenge. Although for some applications it is enough to restart failed tasks, there is a large class of applications where tasks run for a long time or are tightly coupled, thus making a restart from scratch unfeasible. Checkpoint-Restart (CR), the main method to survive failures for such applications faces additional challenges in this context: not only does it need to minimize the performance overhead on the application due to checkpointing, but it also needs to operate with scarce resources. Given the iterative nature of the targeted applications, we launch the assumption that first-time writes to memory during asynchronous checkpointing generate the same kind of interference as they did in past iterations. Based on this assumption, we propose novel\u00a0\u2026", "num_citations": "27\n", "authors": ["498"]}
{"title": "Distributed monitoring and management of exascale systems in the argo project\n", "abstract": " New computing technologies are expected to change the highperformance computing landscape dramatically. Future exascale systems will comprise hundreds of thousands of compute nodes linked by complex networks-resources that need to be actively monitored and controlled, at a scale difficult to manage from a central point as in previous systems.               In this context, we describe here on-going work in the Argo exascale software stack project to develop a distributed collection of services working together to track scientific applications across nodes, control the power budget of the system, and respond to eventual failures. Our solution leverages the idea of enclaves: a hierarchy of logical partitions of the system, representing groups of nodes sharing a common configuration, created to encapsulate user jobs as well as by the user inside its own job. These enclaves provide a second (and greater) level\u00a0\u2026", "num_citations": "26\n", "authors": ["498"]}
{"title": "Optimizing multi-deployment on clouds by means of self-adaptive prefetching\n", "abstract": " With Infrastructure-as-a-Service (IaaS) cloud economics getting increasingly complex and dynamic, resource costs can vary greatly over short periods of time. Therefore, a critical issue is the ability to deploy, boot and terminate VMs very quickly, which enables cloud users to exploit elasticity to find the optimal trade-off between the computational needs (number of resources, usage time) and budget constraints. This paper proposes an adaptive prefetching mechanism aiming to reduce the time required to simultaneously boot a large number of VM instances on clouds from the same initial VM image (multi-deployment). Our proposal does not require any foreknowledge of the exact access pattern. It dynamically adapts to it at run time, enabling the slower instances to learn from the experience of the faster ones. Since all booting instances typically access only a small part of the virtual image along almost the\u00a0\u2026", "num_citations": "26\n", "authors": ["498"]}
{"title": "Spatial support vector regression to detect silent errors in the exascale era\n", "abstract": " As the exascale era approaches, the increasing capacity of high-performance computing (HPC) systems with targeted power and energy budget goals introduces significant challenges in reliability. Silent data corruptions (SDCs) or silent errors are one of the major sources that corrupt the executionresults of HPC applications without being detected. In this work, we explore a low-memory-overhead SDC detector, by leveraging epsilon-insensitive support vector machine regression, to detect SDCs that occur in HPC applications that can be characterized by an impact error bound. The key contributions are three fold. (1) Our design takes spatialfeatures (i.e., neighbouring data values for each data point in a snapshot) into training data, such that little memory overhead (less than 1%) is introduced. (2) We provide an in-depth study on the detection ability and performance with different parameters, and we optimize the\u00a0\u2026", "num_citations": "25\n", "authors": ["498"]}
{"title": "Coordinated checkpoint versus message log for fault tolerant MPI\n", "abstract": " Large clusters, high availability clusters and grid deployments often suffer from network, node or operating system faults and thus require the use of fault tolerant programming models. MPI is one of the most widely adopted programming models for high performance computing. There are several approaches for fault tolerance in an MPI environment. The automatic and transparent ones are based on either coordinated or uncoordinated checkpoint associated with a message log strategy. There are many protocols and optimisations for these approaches and several implementations have been made. However, few results of comparison between them exist. Coordinated checkpoint has the advantage of a very low overhead as long as the execution stays fault free. In contrast, uncoordinated checkpoint must be complemented by a message log protocol which adds a significant penalty for all message transfers even for\u00a0\u2026", "num_citations": "25\n", "authors": ["498"]}
{"title": "Fault-tolerant protocol for hybrid task-parallel message-passing applications\n", "abstract": " We present a fault-tolerant protocol for task-parallel message-passing applications to mitigate transient errors. The protocol requires the restart only of the task that experienced the error and transparently handles any MPI calls inside the task. The protocol is implemented in Nanos -- a dataflow runtime for task-based OmpSs programming model -- and the PMPI profiling layer to fully support hybrid OmpSs+MPI applications. In our experiments we demonstrate that our fault-tolerant solution has a reasonable overhead, with a maximum observed overhead of 4.5%. We also show that fine-grained parallelization is important for hiding the overheads related to the protocol as well as the recovery of tasks.", "num_citations": "24\n", "authors": ["498"]}
{"title": "Intra node parallelization of MPI programs with OpenMP\n", "abstract": " The availability of multiprocessors and high performance networks o er the opportunity to construct CLUMPs (Cluster of Multiprocessors) and use them as parallel computing platforms. The main distinctive feature of the CLUMP architecture over the usual parallel computers is its hybrid memory model (message passing between the nodes and shared memory inside the nodes). Some of the primary issues to address for the CLUMP are: 1) to be able to execute the existing programs with few modi cations 2) to provide some programming models coherent with the performance hierarchy of the data movements inside the CLUMP 3) to limit the e ort of the programmer while ensuring the portability of the codes on a wide variety of CLUMP con gurations. We investigate an approach based on the MPI and OpenMP standards. The approach consists in the intra-node parallelization of the MPI programs with an OpenMP directive based parallel compiler. The paper presents a detailed study of the approach in the context of the biprocessor PC CLUMPs. It provides three contributions. First, it evaluates the ability of biprocessor PCs to e ectively provide a speed up over single processor PCs in the context of shared memory parallel programs. Second, it investigates the method to transform MPI parallel programs in order to execute them on a CLUMP. Third, it presents the performance evaluation of this method applied on the NAS parallel benchmarks executed on a cluster of biprocessor PCs.", "num_citations": "24\n", "authors": ["498"]}
{"title": "FT-CNN: Algorithm-based fault tolerance for convolutional neural networks\n", "abstract": " Convolutional neural networks (CNNs) are becoming more and more important for solving challenging and critical problems in many fields. CNN inference applications have been deployed in safety-critical systems, which may suffer from soft errors caused by high-energy particles, high temperature, or abnormal voltage. Of critical importance is ensuring the stability of the CNN inference process against soft errors. Traditional fault tolerance methods are not suitable for CNN inference because error-correcting code is unable to protect computational components, instruction duplication techniques incur high overhead, and existing algorithm-based fault tolerance (ABFT) techniques cannot protect all convolution implementations. In this article, we focus on how to protect the CNN inference process against soft errors as efficiently as possible, with the following three contributions. (1) We propose several systematic ABFT\u00a0\u2026", "num_citations": "23\n", "authors": ["498"]}
{"title": "Exploring properties and correlations of fatal events in a large-scale hpc system\n", "abstract": " In this paper, we explore potential correlations of fatal system events for one of the most powerful supercomputers-IBM Blue Gene/Q Mira, which is deployed at Argonne National Laboratory, based on its 5-year reliability, availability, and serviceability (RAS) log. Our contribution is two-fold. (1) We design an efficient log analysis tool, namely LogAider, with a novel filtering method to effectively extract fatal events from masses of system messages that are heavily duplicated in the log. LogAider exhibits a very precise detection of temporal-correlation with a high similarity (up to 95 percent) to the ground-truth (i.e., compared to the failure records reported by the administrators). The total number of fatal events can be reduced to about 1,255 compared with originally 2.6 million duplicated fatal messages. (2) We analyze the 5-year RAS log of the MIRA system using LogAider, and summarize six important \u201ctakeaways\u201d which\u00a0\u2026", "num_citations": "23\n", "authors": ["498"]}
{"title": "In-depth exploration of single-snapshot lossy compression techniques for N-body simulations\n", "abstract": " In situ lossy compression allowing user-controlled data loss can significantly reduce the I/O burden. For large-scale N-body simulations where only one snapshot can be compressed at a time, the lossy compression ratio is very limited because of the fairly low spatial coherence of the particle data. In this work, we assess the state-of-the-art single-snapshot lossy compression techniques of two common N-body simulation models: cosmology and molecular dynamics. We design a series of novel optimization techniques based on the two representative real-world N-body simulation codes. For molecular dynamics simulation, we propose three compression modes (i.e., best speed, best tradeoff, best compression mode) that can refine the tradeoff between the compression rate (a.k.a., speed/throughput) and ratio. For cosmology simulation, we identify that our improved SZ is the best lossy compressor with respect to both\u00a0\u2026", "num_citations": "22\n", "authors": ["498"]}
{"title": "Exploiting spatial smoothness in HPC applications to detect silent data corruption\n", "abstract": " Next-generation supercomputers are expected to have more components and, at the same time, consume several times less energy per operation. This situation is pushing supercomputer constructors to the limits of miniaturization and energy-saving strategies. Consequently, the number of soft errors is expected to increase dramatically in the coming years. While mechanisms are in place to correct or at least detect soft errors, a percentage of those errors pass unnoticed by the system. Such silent errors are extremely damaging because they can make applications produce wrong results. In this paper we propose a technique that leverages certain properties of HPC applications in order to detect silent errors at the application level. Our technique detects corruption solely based on the data behavior and is algorithm-agnostic. We show that this strategy can detect up to 90% of injected errors in some regions while\u00a0\u2026", "num_citations": "22\n", "authors": ["498"]}
{"title": "Toward an International\" Computer Science Grid\"\n", "abstract": " The computer science discipline, especially in large scale distributed systems like grids and P2P systems and in high performance computing areas, tends to address issues related to increasingly complex systems, gathering thousands to millions of non trivial components. Theoretical analysis, simulation and even emulation are reaching their limits. Like in other scientific disciplines such as physics, chemistry and life science, there is a need to develop, run and maintain generations of scientific instruments for the observation of complex distributed systems running at real scale and under reproducible experimental conditions. Grid'5000 and DAS3 are two large scale systems designed as scientific instruments for researchers in the domains of grid, P2P and networking. More than testbeds, Grid'5000 and DAS3 have been designed as \"computer science grids\", where researchers share experimental resources\u00a0\u2026", "num_citations": "21\n", "authors": ["498"]}
{"title": "Resource availability in enterprise desktop grids\n", "abstract": " Desktop grids, which use the idle cycles of many desktop PC's, are currently the largest distributed systems in the world. Despite the popularity and success of many desktop grid projects, the heterogeneity and volatility of hosts within desktop grids has been poorly understood. Yet, host characterization is essential for accurate simulation and modelling of such platforms. In this paper, we present application-level traces of four real desktop grids that can be used for simulation and modelling purposes. In addition, we describe aggregate and per host statistics that reflect the heterogeneity and volatility of desktop grid resources.", "num_citations": "21\n", "authors": ["498"]}
{"title": "BLAST application with data-aware desktop grid middleware\n", "abstract": " There exists numerous Grid middleware to develop and execute programs on the computational Grid, but they still require intensive work from their users. BitDew is made to facilitate the usage of large scale Grid with dynamic, heterogeneous, volatile and highly distributed computing resources for applications that require a huge amount of data processing. Data-intensive applications form an important class of applications for the e-Science community which require secure and coordinated access to large datasets, wide-area transfers and broad distribution of TeraBytes of data while keeping track of multiple data replicas. In genetic biology, gene sequences comparison and analysis are the most basic routines. With the considerable increase of sequences to analyze, we need more and more computing power as well as efficient solution to manage data. In this work, we investigate the advantages of using a new\u00a0\u2026", "num_citations": "20\n", "authors": ["498"]}
{"title": "Global computing systems\n", "abstract": " Global Computing harvest the idle time ofIn ternet connected computers to run very large distributed applications. The unprecedented scale ofthe GCS paradigm requires to revisit the basic issues of distributed systems: performance models, security, fault-tolerance and scalability. The first parts ofthis paper review recent work in Global Computing, with particular interest in Peer-to-Peer systems. In the last section, we present XtremWeb, the Global Computing System we are currently developing.", "num_citations": "20\n", "authors": ["498"]}
{"title": "Performance characteristics of a network of commodity multiprocessors for the NAS benchmarks using a hybrid memory model\n", "abstract": " Multiprocessors and high performance networks offer the opportunity to construct CLUster of MultiProcessors (CLUMPs) and use them as parallel computing platforms. The distinctive feature of the CLUMPs over traditional parallel computers is their hybrid memory model (message passing between the nodes and shared memory inside the nodes). We investigate the performance characteristics of a cluster of biprocessor PCs for the NAS 2.3 parallel benchmark using a programming model based on MPI for message passing between biprocessor nodes and OpenMP for shared memory inside biprocessor nodes. The paper provides several contributions. These include: speed-up measurements of a cluster of biprocessor PCs over a cluster of uniprocessor PCs using the hybrid memory model; a detailed analysis of the speed-up results from a breakdown of the benchmarks execution time; and a performance\u00a0\u2026", "num_citations": "20\n", "authors": ["498"]}
{"title": "Optimization of error-bounded lossy compression for hard-to-compress HPC data\n", "abstract": " Since today's scientific applications are producing vast amounts of data, compressing them before storage/transmission is critical. Results of existing compressors show two types of HPC data sets: highly compressible and hard to compress. In this work, we carefully design and optimize the error-bounded lossy compression for hard-to-compress scientific data. We propose an optimized algorithm that can adaptively partition the HPC data into best-fit consecutive segments each having mutually close data values, such that the compression condition can be optimized. Another significant contribution is the optimization of shifting offset such that the XOR-leading-zero length between two consecutive unpredictable data points can be maximized. We finally devise an adaptive method to select the best-fit compressor at runtime for maximizing the compression factor. We evaluate our solution using 13 benchmarks based on\u00a0\u2026", "num_citations": "19\n", "authors": ["498"]}
{"title": "Evaluating irregular memory access on opencl fpga platforms: A case study with xsbench\n", "abstract": " FPGAs are becoming an attractive choice as a heterogeneous computing unit for scientific computing because FPGA vendors are adding floating-point-optimized architectures to their product lines. Additionally, high-level synthesis (HLS) tools such as Altera OpenCL SDK are emerging, which could potentially break the FPGA programming wall and provide a streamlined flow for domain experts in scientific computing. On the other hand, providing high performance in the presence of irregular memory access patterns to off-chip memory remains a challenge for the automated synthesis flows. In this paper, we study the performance/energy characteristics of OpenCL-generated FPGA designs on irregular memory access patterns, targeting XSBench, a memory-intensive Monte Carlo simulation code, as a case study. To complete our study, we implement XSBench in OpenCL and study optimization strategies for FPGAs\u00a0\u2026", "num_citations": "19\n", "authors": ["498"]}
{"title": "Ecofit: A framework to estimate energy consumption of fault tolerance protocols for hpc applications\n", "abstract": " Energy consumption and fault tolerance are two interrelated issues to address for designing future exascale systems. Fault tolerance protocols used for checkpointing have different energy consumption depending on parameters like application features, number of processes in the execution and platform characteristics. Currently, the only way to select a protocol for a given execution is to run the application and monitor the energy consumption of different fault tolerance protocols. This is needed for any variation of the execution setting. To avoid this time and energy consuming process, we propose an energy estimation framework. It relies on an energy calibration of the considered platform and a user description of the execution setting. We evaluate the accuracy of our estimations with real applications running on a real platform with energy consumption monitoring. Results show that our estimations are highly\u00a0\u2026", "num_citations": "19\n", "authors": ["498"]}
{"title": "Scalable data management for map-reduce-based data-intensive applications: a view for cloud and hybrid infrastructures\n", "abstract": " As map-reduce emerges as a leading programming paradigm for data-intensive computing, today\u2019s frameworks which support it still have substantial shortcomings that limit its potential scalability. In this paper, we discuss several directions where there is room for such progress: they concern storage efficiency under massive data access concurrency, scheduling, volatility and fault-tolerance. We place our discussion in the perspective of the current evolution towards an increasing integration of large-scale distributed platforms (clouds, cloud federations, enterprise desktop grids, etc.). We propose an approach which aims to overcome the current limitations of existing map-reduce frameworks, in order to achieve scalable, concurrency-optimised, fault-tolerant map-reduce data processing on hybrid infrastructures. This approach will be evaluated with real-life bio-informatics applications on existing Nimbus-powered\u00a0\u2026", "num_citations": "19\n", "authors": ["498"]}
{"title": "Scalable Reed-Solomon-based reliable local storage for HPC applications on IaaS clouds\n", "abstract": " With increasing interest among mainstream users to run HPC applications, Infrastructure-as-a-Service (IaaS) cloud computing platforms represent a viable alternative to the acquisition and maintenance of expensive hardware, often out of the financial capabilities of such users. Also, one of the critical needs of HPC applications is an efficient, scalable and persistent storage. Unfortunately, storage options proposed by cloud providers are not standardized and typically use a different access model. In this context, the local disks on the compute nodes can be used to save large data sets such as the data generated by Checkpoint-Restart (CR). This local storage offers high throughput and scalability but it needs to be combined with persistency techniques, such as block replication or erasure codes. One of the main challenges that such techniques face is to minimize the overhead of performance and I/O\u00a0\u2026", "num_citations": "18\n", "authors": ["498"]}
{"title": "Characterizing and understanding hpc job failures over the 2k-day life of ibm bluegene/q system\n", "abstract": " An in-depth understanding of the failure features of HPC jobs in a supercomputer is critical to the large-scale system maintenance and improvement of the service quality for users. In this paper, we investigate the features of hundreds of thousands of jobs in one of the most powerful supercomputers, the IBM Blue Gene/Q Mira, based on 2001 days of observations with a total of over 32.44 billion core-hours. We study the impact of the system's events on the jobs' execution in order to understand the system's reliability from the perspective of jobs and users. The characterization involves a joint analysis based on multiple data sources, including the reliability, availability, and serviceability (RAS) log; job scheduling log; the log regarding each job's physical execution tasks; and the I/O behavior log. We present 22 valuable takeaways based on our in-depth analysis. For instance, 99,245 job failures are reported in the job\u00a0\u2026", "num_citations": "17\n", "authors": ["498"]}
{"title": "Self-adaptive density estimation of particle data\n", "abstract": " We present a study of density estimation, the conversion of discrete particle positions to a continuous field of particle density defined over a three-dimensional Cartesian grid.  The study features a methodology for evaluating the accuracy and performance of various density estimation methods, results of that evaluation for four density estimators, and a large-scale parallel algorithm for a self-adaptive method that computes a Voronoi tessellation as an intermediate step.  We demonstrate the performance and scalability of our parallel algorithm on a supercomputer when estimating the density of 100 million particles over 500 billion grid points.", "num_citations": "17\n", "authors": ["498"]}
{"title": "Analysis of the tradeoffs between energy and run time for multilevel checkpointing\n", "abstract": " In high-performance computing, there is a perpetual hunt for performance and scalability. Supercomputers grow larger offering improved computational science throughput. Nevertheless, with an increase in the number of systems\u2019 components and their interactions, the number of failures and the power consumption will increase rapidly. Energy and reliability are among the most challenging issues that need to be addressed for extreme scale computing. We develop analytical models for run time and energy usage for multilevel fault-tolerance schemes. We use these models to study the tradeoff between run time and energy in FTI, a recently developed multilevel checkpoint library, on an IBM Blue Gene/Q. Our results show that energy consumed by FTI is low and the tradeoff between the run time and energy is small. Using the analytical models, we explore the impact of various system-level parameters on\u00a0\u2026", "num_citations": "17\n", "authors": ["498"]}
{"title": "Multi-criteria checkpointing strategies: Response-time versus resource utilization\n", "abstract": " Failures are increasingly threatening the efficiency of HPC systems, and current projections of Exascale platforms indicate that rollback recovery, the most convenient method for providing fault tolerance to general-purpose applications, reaches its own limits at such scales. One of the reasons explaining this unnerving situation comes from the focus that has been given to per-application completion time, rather than to platform efficiency. In this paper, we discuss the case of uncoordinated rollback recovery where the idle time spent waiting recovering processors is used to progress a different, independent application from the system batch queue. We then propose an extended model of uncoordinated checkpointing that can discriminate between idle time and wasted computation. We instantiate this model in a simulator to demonstrate that, with this strategy, uncoordinated checkpointing per application\u00a0\u2026", "num_citations": "16\n", "authors": ["498"]}
{"title": "Towards soft real-time applications on enterprise desktop grids\n", "abstract": " Desktop grids use the idle cycles of desktop PC\u2019s to provide huge computational power at low cost. However, because the underlying desktop computing resources are volatile, achieving performance guarantees such as task completion rate is difficult. We investigate the use of buffering to ensure task completion rates, which is essential for soft real-time applications. In particular, we develop a model of task completion rate as a function of buffer size. We instantiate this model using parameters derived from two enterprise desktop grid data sets, evaluate the model via trace-driven simulation, and show how this model can be used to ensure application task completion rates on enterprise desktop grid systems.", "num_citations": "16\n", "authors": ["498"]}
{"title": "Auger & XtremWeb: Monte Carlo computation on a global computing platform\n", "abstract": " XtremWeb main goal, as a Global Computing platform, is to compute distributed applications using idle time of widely interconnected machines. It is especially dedicated to-but not limited tomulti-parameters applications such as monte carlos computations; its security mechanisms ensuring not only hosts integrity but also results certification and its fault tolerant features, encouraged us to test it and, finally, to deploy it as to support our CPU needs to simulate showers. We first introduce Auger computing needs and how Global Computing could help. We then detail XtremWeb architecture and goals. The fourth and last part presents the profits we have gained to choose this platform. We conclude on what could be done next.", "num_citations": "16\n", "authors": ["498"]}
{"title": "Significantly improving lossy compression quality based on an optimized hybrid prediction model\n", "abstract": " With the ever-increasing volumes of data produced by today's large-scale scientific simulations, error-bounded lossy compression techniques have become critical: not only can they significantly reduce the data size but they also can retain high data fidelity for postanalysis. In this paper, we design a strategy to improve the compression quality significantly based on an optimized, hybrid prediction model. Our contribution is fourfold.(1) We propose a novel, transform-based predictor and optimize its compression quality.(2) We significantly improve the coefficient-encoding efficiency for the data-fitting predictor.(3) We propose an adaptive framework that can select the best-fit predictor accurately for different datasets.(4) We evaluate our solution and several existing state-of-the-art lossy compressors by running real-world applications on a supercomputer with 8,192 cores. Experiments show that our adaptive compressor\u00a0\u2026", "num_citations": "15\n", "authors": ["498"]}
{"title": "Improving performance of data dumping with lossy compression for scientific simulation\n", "abstract": " Because of the ever-increasing data being produced by today's high performance computing (HPC) scientific simulations, I/O performance is becoming a significant bottleneck for their executions. An efficient error-controlled lossy compressor is a promising solution to significantly reduce data writing time for scientific simulations running on supercomputers. In this paper, we explore how to optimize the data dumping performance for scientific simulation by leveraging error-bounded lossy compression techniques. The contributions of the paper are threefold. (1) We propose a novel I/O performance profiling model that can effectively represent the I/O performance with different execution scales and data sizes, and optimize the estimation accuracy of data dumping performance using least square method. (2) We develop an adaptive lossy compression framework that can select the bestfit compressor (between two\u00a0\u2026", "num_citations": "15\n", "authors": ["498"]}
{"title": "Efficient lossy compression for scientific data based on pointwise relative error bound\n", "abstract": " An effective data compressor is becoming increasingly critical to today's scientific research, and many lossy compressors are developed in the context of absolute error bounds. Based on physical/chemical definitions of simulation fields or multiresolution demand, however, many scientific applications need to compress the data with a pointwise relative error bound (i.e., the smaller the data value, the smaller the compression error to tolerate). To this end, we propose two optimized lossy compression strategies under a state-of-the-art three-staged compression framework (prediction + quantization + entropy-encoding). The first strategy (called block-based strategy) splits the data set into many small blocks and computes an absolute error bound for each block, so it is particularly suitable for the data with relatively high consecutiveness in space. The second strategy (called multi-threshold-based strategy) splits the\u00a0\u2026", "num_citations": "15\n", "authors": ["498"]}
{"title": "Toward general software level silent data corruption detection for parallel applications\n", "abstract": " Silent data corruption (SDC) poses a great challenge for high-performance computing (HPC) applications as we move to extreme-scale systems. Mechanisms have been proposed that are able to detect SDC in HPC applications by using the peculiarities of the data (more specifically, its \u201csmoothness\u201d in time and space) to make predictions. However, these data-analytic solutions are still far from fully protecting applications to a level comparable with more expensive solutions such as full replication. In this work, we propose partial replication to overcome this limitation. More specifically, we have observed that not all processes of an MPI application experience the same level of data variability at exactly the same time. Thus, we can smartly choose and replicate only those processes for which the lightweight data-analytic detectors would perform poorly. In addition, we propose a new evaluation method based on the\u00a0\u2026", "num_citations": "15\n", "authors": ["498"]}
{"title": "Detecting silent data corruption for extreme-scale MPI applications\n", "abstract": " Next-generation supercomputers are expected to have more components and, at the same time, consume several times less energy per operation. These trends are pushing supercomputer construction to the limits of miniaturization and energy-saving strategies. Consequently, the number of soft errors is expected to increase dramatically in the coming years. While mechanisms are in place to correct or at least detect some soft errors, a significant percentage of those errors pass unnoticed by the hardware. Such silent errors are extremely damaging because they can make applications silently produce wrong results. In this work we propose a technique that leverages certain properties of high-performance computing applications in order to detect silent errors at the application level. Our technique detects corruption based solely on the behavior of the application datasets and is application-agnostic. We propose\u00a0\u2026", "num_citations": "15\n", "authors": ["498"]}
{"title": "Toward effective detection of silent data corruptions for hpc applications\n", "abstract": " Because of the large number of components, future extreme-scale systems are expected to suffer a lot of silent data corruptions. Changes caused by silent errors flipping low-order bit positions are very small, making them difficult to detect by software. In this work, we convert the detection problem to a one-step look-ahead prediction issue and explore the most effective prediction methods for different HPC applications. We exploit the Auto Regressive (AR) model, Auto Regressive Moving Average (ARMA) Model, Linear Curve Fitting (LCF), and Quadratic Curve Fitting (QCF). We evaluate them using real HPC application traces. Experiments show that the error feed-back control plays an important role in improving detection. AR and QCF perform the best among all evaluated methods, where F-measure can be kept around 80% for silent bit-flip errors occurring around the bit position 20 for double-precision data or around bit 8 for single-precision data.", "num_citations": "15\n", "authors": ["498"]}
{"title": "Hierarchical clustering strategies for fault tolerance in large scale HPC systems\n", "abstract": " Future high performance computing systems will need to use novel techniques to allow scientific applications to progress despite frequent failures. Checkpoint-Restart is currently the most popular way to mitigate the impact of failures during long-running executions. Different techniques try to reduce the cost of Checkpoint-Restart, some of them such as local check pointing and erasure codes aim to reduce the time to checkpoint while others such as uncoordinated checkpoint and message-logging aim to decrease the cost of recovery. In this paper, we study how to combine all these techniques together in order to optimize both: check pointing and recovery. We present several clustering and topology challenges that lead us to an optimization problem in a four-dimensional space: reliability level, recovery cost, encoding time and message logging overhead. We propose a novel clustering method inspired from brain\u00a0\u2026", "num_citations": "15\n", "authors": ["498"]}
{"title": "Cusz: An efficient gpu-based error-bounded lossy compression framework for scientific data\n", "abstract": " Error-bounded lossy compression is a state-of-the-art data reduction technique for HPC applications because it not only significantly reduces storage overhead but also can retain high fidelity for postanalysis. Because supercomputers and HPC applications are becoming heterogeneous using accelerator-based architectures, in particular GPUs, several development teams have recently released GPU versions of their lossy compressors. However, existing state-of-the-art GPU-based lossy compressors suffer from either low compression and decompression throughput or low compression quality. In this paper, we present an optimized GPU version, cuSZ, for one of the best error-bounded lossy compressors-SZ. To the best of our knowledge, cuSZ is the first error-bounded lossy compressor on GPUs for scientific data. Our contributions are fourfold. (1) We propose a dual-quantization scheme to entirely remove the data dependency in the prediction step of SZ such that this step can be performed very efficiently on GPUs. (2) We develop an efficient customized Huffman coding for the SZ compressor on GPUs. (3) We implement cuSZ using CUDA and optimize its performance by improving the utilization of GPU memory bandwidth. (4) We evaluate our cuSZ on five real-world HPC application datasets from the Scientific Data Reduction Benchmarks and compare it with other state-of-the-art methods on both CPUs and GPUs. Experiments show that our cuSZ improves SZ's compression throughput by up to 370.1x and 13.1x, respectively, over the production version running on single and multiple CPU cores, respectively, while getting the same quality of\u00a0\u2026", "num_citations": "13\n", "authors": ["498"]}
{"title": "Significantly improving lossy compression for HPC datasets with second-order prediction and parameter optimization\n", "abstract": " Today's extreme-scale high-performance computing (HPC) applications are producing volumes of data too large to save or transfer because of limited storage space and I/O bandwidth. Error-bounded lossy compression has been commonly known as one of the best solutions to the big science data issue, because it can significantly reduce the data volume with strictly controlled data distortion based on user requirements. In this work, we develop an adaptive parameter optimization algorithm integrated with a series of optimization strategies for SZ, a state-of-the-art prediction-based compression model. Our contribution is threefold.(1) We exploit effective strategies by using 2nd-order regression and 2nd-order Lorenzo predictors to improve the prediction accuracy significantly for SZ, thus substantially improving the overall compression quality.(2) We design an efficient approach selecting the best-fit parameter setting\u00a0\u2026", "num_citations": "13\n", "authors": ["498"]}
{"title": "Optimizing lossy compression with adjacent snapshots for n-body simulation data\n", "abstract": " Today's N-body simulations are producing extremely large amounts of data. The Hardware/Hybrid Accelerated Cosmology Code (HACC), for example, may simulate trillions of particles, producing tens of petabytes of data to store in a parallel file system, according to the HACC users. In this paper, we design and implement an efficient, in situ error-bounded lossy compressor to significantly reduce the data size for N-body simulations. Not only can our compressor save significant storage space for N-body simulation researchers, but it can also improve the I/O performance considerably with limited memory and computation overhead. Our contribution is threefold. (1) We propose an efficient data compression model by leveraging the consecutiveness of the cosmological data in both space and time dimensions as well as the physical correlation across different fields. (2) We propose a lightweight, efficient alignment\u00a0\u2026", "num_citations": "13\n", "authors": ["498"]}
{"title": "Coping with silent and fail-stop errors at scale by combining replication and checkpointing\n", "abstract": " This paper provides a model and an analytical study of replication as a technique to cope with silent errors, as well as a mixture of both silent and fail-stop errors on large-scale platforms. Compared with fail-stop errors that are immediately detected when they occur, silent errors require a detection mechanism. To detect silent errors, many application-specific techniques are available, either based on algorithms (e.g., ABFT), invariant preservation or data analytics, but replication remains the most transparent and least intrusive technique. We explore the right level (duplication, triplication or more) of replication for two frameworks: (i) when the platform is subject to only silent errors, and (ii) when the platform is subject to both silent and fail-stop errors. A higher level of replication is more expensive in terms of resource usage but enables to tolerate more errors and to even correct some errors, hence there is a trade-off to\u00a0\u2026", "num_citations": "13\n", "authors": ["498"]}
{"title": "Unified fault-tolerance framework for hybrid task-parallel message-passing applications\n", "abstract": " We present a unified fault-tolerance framework for task-parallel message-passing applications to mitigate transient errors. First, we propose a fault-tolerant message-logging protocol that only requires the restart of the task that experienced the error and transparently handles any message passing interface calls inside the task. In our experiments we demonstrate that our fault-tolerant solution has a reasonable overhead, with a maximum observed overhead of 4.5%. We also show that fine-grained parallelization is important for hiding the overheads related to the protocol as well as the recovery of tasks. Secondly, we develop a mathematical model to unify task-level checkpointing and our protocol with system-wide checkpointing in order to provide complete failure coverage. We provide closed formulas for the optimal checkpointing interval and the performance score of the unified scheme. Experimental results show\u00a0\u2026", "num_citations": "13\n", "authors": ["498"]}
{"title": "Evaluation of a floating-point intensive kernel on FPGA\n", "abstract": " Heterogeneous platforms provide a promising solution for high-performance and energy-efficient computing applications. This paper presents our research on usage of heterogeneous platform for a floating-point intensive kernel. We first introduce the floating-point intensive kernel from the geographical information system. Then we analyze the FPGA designs generated by the Intel FPGA SDK for OpenCL, and evaluate the kernel performance and the floating-point error rate of the FPGA designs. Finally, we compare the performance and energy efficiency of the kernel implementations on the Arria 10 FPGA, Intel\u2019s Xeon Phi Knights Landing CPU, and NVIDIA\u2019s Kepler GPU. Our evaluation shows the energy efficiency of the single-precision kernel on the FPGA is 1.35X better than on the CPU and the GPU, while the energy efficiency of the double-precision kernel on the FPGA is 1.36X and 1.72X less than the\u00a0\u2026", "num_citations": "13\n", "authors": ["498"]}
{"title": "Exploration of pattern-matching techniques for lossy compression on cosmology simulation data sets\n", "abstract": " Because of the vast volume of data being produced by today\u2019s scientific simulations, lossy compression allowing user-controlled information loss can significantly reduce the data size and the I/O burden. However, for large-scale cosmology simulation, such as the Hardware/Hybrid Accelerated Cosmology Code (HACC), where memory overhead constraints restrict compression to only one snapshot at a time, the lossy compression ratio is extremely limited because of the fairly low spatial coherence and high irregularity of the data. In this work, we propose a pattern-matching (similarity searching) technique to optimize the prediction accuracy and compression ratio of SZ lossy compressor on the HACC data sets. We evaluate our proposed method with different configurations and compare it with state-of-the-art lossy compressors. Experiments show that our proposed optimization approach can improve the\u00a0\u2026", "num_citations": "13\n", "authors": ["498"]}
{"title": "Lightweight and accurate silent data corruption detection in ordinary differential equation solvers\n", "abstract": " Silent data corruptions (SDCs) are errors that corrupt the system or falsify results while remaining unnoticed by firmware or operating systems. In numerical integration solvers, SDCs that impact the accuracy of the solver are considered significant. Detecting SDCs in high-performance computing is necessary because results need to be trustworthy and the increase of the number and complexity of components in emerging large-scale architectures makes SDCs more likely to occur. Until recently, SDC detection methods consisted in replicating the processes of the execution or in using checksums (for example algorithm-based fault tolerance). Recently, new detection methods have been proposed relying on mathematical properties of numerical kernels or performing data analysis of the results modified by the application. None of those methods, however, provide a lightweight solution guaranteeing that all significant\u00a0\u2026", "num_citations": "13\n", "authors": ["498"]}
{"title": "Grid5000\n", "abstract": " 1) Building a nation wide experimental platform for Grid & P2P researches (like a particle accelerator for the computerscientists)\u2022 9 geographically distributed sites\u2022 every site hosts a cluster (from 256 CPUs to 1K CPUs)\u2022 All sites are connected by RENATER (French Res. and Edu. Net.)\u2022 RENATER hosts probes to trace network load conditions\u2022 Design and develop a system/middleware environment for safely test and repeat experiments", "num_citations": "13\n", "authors": ["498"]}
{"title": "Fraz: A generic high-fidelity fixed-ratio lossy compression framework for scientific floating-point data\n", "abstract": " With ever-increasing volumes of scientific floating-point data being produced by high-performance computing applications, significantly reducing scientific floating-point data size is critical, and error-controlled lossy compressors have been developed for years. None of the existing scientific floating-point lossy data compressors, however, support effective fixed-ratio lossy compression. Yet fixed-ratio lossy compression for scientific floating-point data not only compresses to the requested ratio but also respects a user-specified error bound with higher fidelity. In this paper, we present FRaZ: a generic fixed-ratio lossy compression framework respecting user-specified error constraints. The contribution is twofold. (1) We develop an efficient iterative approach to accurately determine the appropriate error settings for different lossy compressors based on target compression ratios. (2) We perform a thorough performance\u00a0\u2026", "num_citations": "12\n", "authors": ["498"]}
{"title": "Pastri: Error-bounded lossy compression for two-electron integrals in quantum chemistry\n", "abstract": " Computation of two-electron repulsion integrals is the critical and the most time-consuming step in a typical parallel quantum chemistry simulation. Such calculations have massive computing and storage requirements, which scale as O(N^4) with the size of a chemical system. Compressing the integral's data and storing it on disk can avoid costly recalculation, significantly speeding the overall quantum chemistry calculations; but it requires a fast compression algorithm. To this end, we developed PaSTRI (Pattern Scaling for Two-electron Repulsion Integrals) and implemented the algorithm in the data compression package SZ. PaSTRI leverages the latent pattern features in the integral dataset and optimizes the calculation of the appropriate number of bits required for the storage of the integral. We have evaluated PaSTRI using integral datasets generated by the quantum chemistry program GAMESS. The results\u00a0\u2026", "num_citations": "12\n", "authors": ["498"]}
{"title": "Identifying the right replication level to detect and correct silent errors at scale\n", "abstract": " This paper provides a model and an analytical study of replication as a technique to detect and correct silent errors. Although other detection techniques exist for HPC applications, based on algorithms (ABFT), invariant preservation or data analytics, replication remains the most transparent and least intrusive technique. We explore the right level (duplication, triplication or more) of replication needed to efficiently detect and correct silent errors. Replication is combined with checkpointing and comes with two flavors: process replication and group replication. Process replication applies to message-passing applications with communicating processes. Each process is replicated, and the platform is composed of process pairs, or triplets. Group replication applies to black-box applications, whose parallel execution is replicated several times. The platform is partitioned into two halves (or three thirds). In both scenarios\u00a0\u2026", "num_citations": "12\n", "authors": ["498"]}
{"title": "Exploring partial replication to improve lightweight silent data corruption detection for HPC applications\n", "abstract": " Silent data corruption (SDC) poses a great challenge for high-performance computing (HPC) applications as we move to extreme-scale systems. If not dealt with properly, SDC has the potential to influence important scientific results, leading scientists to wrong conclusions. In previous work, our detector was able to detect SDC in HPC applications to a certain level by using the peculiarities of the data (more specifically, its \u201csmoothness\u201d in time and space) to make predictions. Accurate predictions allow us to detect corruptions when data values are far \u201cenough\u201d from them. However, these data-analytic solutions are still far from fully protecting applications to a level comparable with more expensive solutions such as full replication. In this work, we propose partial replication to overcome this limitation. More specifically, we have observed that not all processes of an MPI application experience the same level of\u00a0\u2026", "num_citations": "12\n", "authors": ["498"]}
{"title": "Errors and faults\n", "abstract": " Understanding the behavior of failures in large-scale systems is important in order to design techniques to tolerate them. Reliability knowledge of resources can be used in numerous ways by scientist of systems administrators: (1) it can be used to improve the quality of service of the machine; (2) to reduce performance loss due to unexpected failures either by reliability-aware scheduling or by reliability-aware checkpointing; (3) to design more resilient applications, programming models or machines in the future. This chapter focuses on offering an overview of failures observed in real large-scale systems and their characteristics, with an emphasis on modeling, detection, and prediction.", "num_citations": "12\n", "authors": ["498"]}
{"title": "Scheduling the I/O of HPC applications under congestion\n", "abstract": " A significant percentage of the computing capacity of large-scale platforms is wasted due to interferences incurred by multiple applications that access a shared parallel file system concurrently. One solution to handling I/O bursts in large-scale HPC systems is to absorb them at an intermediate storage layer consisting of burst buffers. However, our analysis of the Argonne's Mira system shows that burst buffers cannot prevent congestion at all times. As a consequence, I/O performance is dramatically degraded, showing in some cases a decrease in I/O throughput of 67%. In this paper, we analyze the effects of interference on application I/O bandwidth, and propose several scheduling techniques to mitigate congestion. We show through extensive experiments that our global I/O scheduler is able to reduce the effects of congestion, even on systems where burst buffers are used, and can increase the overall system throughput up to 56%. We also show that it outperforms current Mira I/O schedulers.", "num_citations": "12\n", "authors": ["498"]}
{"title": "Performance of the NAS benchmarks on a cluster of SMP PCs using a parallelization of the MPI programs with OpenMP\n", "abstract": " The availability of multiprocessors and high performance networks offer the opportunity to build CLUMPs (Cluster of Multi- processors) and use them as parallel computing platforms. The main distinctive feature of the CLUMP architecture over the usual parallel computers is its hybrid memory model (message passing between the nodes and shared memory inside the nodes). To be largely used, the CLUMPs must be able to execute the existing programs with few mod- ifications. We investigate the performance of a programming approach based on the MPI for inter-multiprocessor communications and OpenMP standards for intra-multiprocessor exchanges. The approach consists in the intra-node parallelization of the MPI programs with an OpenMP directive based parallel compiler. The paper details the approach in the context of the biprocessor PC CLUMPs and presents a performance eval- uation for the\u00a0\u2026", "num_citations": "12\n", "authors": ["498"]}
{"title": "PTAH Introduction to a new parallel architecture for highly numeric processing\n", "abstract": " This paper proposes a new architectural design for high performance parallel computers: the one-cycle machine. In such a computer the memory access, network access, instruction sequencing, data computation take the same duration: one clock cycle. We first consider the communication network efficiency as the main critical resource. We show that the adaptation of the network performance to the processing element power is more important than the CPU power in itself with respect to the global processing effectiveness. Two guidelines are derived from our analysis and conduct to the design of PTAH. Two simple examples are used to illustrate the interest of PTAH for the execution of numeric applications. Finally, some hardware features are proposed for a PTAH implementation being able to reach the TeraFLOPS.", "num_citations": "12\n", "authors": ["498"]}
{"title": "One step further in large-scale evaluations: the V-DS environment\n", "abstract": " Validating current and next generation of distributed systems targeting large-scale infrastructures is a complex task. Several methodologies are possible. However, experimental evaluations on real testbeds are unavoidable in the life-cycle of a distributed middleware prototype. In particular, performing such real experiments in a rigorous way requires to benchmark developed prototypes at larger and larger scales. Fulfilling this requirement is an open challenge but is mandatory in order to fully observe and understand the behaviors of distributed systems. In this work, we present our Virtualization environment for large-scale Distributed Systems, called V-DS, as an answer for reaching scales steps further than current experimental evaluation scales. Then, through the use of V-DS, we experimentally benchmark the scalability of three P2P libraries by reaching scales commonly used for evaluations through simulations. Notably, our results on the Grid'5000 research testbed show that only one of the P2P library can optimally build overlays, whatever the size of the overlay is", "num_citations": "11\n", "authors": ["498"]}
{"title": "On resource volatility in enterprise desktop grids\n", "abstract": " Desktop grids, which use the idle cycles of many desktop PC's, are currently one of the largest distributed systems in the world. Despite the popularity and success of many desk-top grid projects, the volatility of hosts within desktop grids has been poorly understood. Yet, such host characterization is essential for accurate simulation and modelling of such platforms. In this paper, we present application-level traces of four enterprise desktop grids with a wide range of user bases. We then describe aggregate and per host statistics that reflect the volatility of desktop grid resources. Further, we determine the correlation of volatility between resources, and investigate the correlation of volatility and other host characteristics. Finally, we detail a number of implications of these findings with respect to application performance.", "num_citations": "11\n", "authors": ["498"]}
{"title": "Fixed-psnr lossy compression for scientific data\n", "abstract": " Error-controlled lossy compression has been studied for years because of extremely large volumes of data being produced by today's scientific simulations. None of existing lossy compressors, however, allow users to fix the peak signal-to-noise ratio (PSNR) during compression, although PSNR has been considered as one of the most significant indicators to assess compression quality. In this paper, we propose a novel technique providing a fixed-PSNR lossy compression for scientific data sets. We implement our proposed method based on the SZ lossy compression framework and release the code as an open-source toolkit. We evaluate our fixed-PSNR compressor on three realworld high-performance computing data sets. Experiments show that our solution has a high accuracy in controlling PSNR, with an average deviation of 0.1 ~ 5.0 dB on the tested data sets.", "num_citations": "10\n", "authors": ["498"]}
{"title": "Transparent low-overhead checkpoint for GPU-accelerated clusters\n", "abstract": " * Node: High BW Compute Nodes x 1408* CPU: Intel Westmere-EP 2.93 GHz 12Cores/node* Mem: 55.8 GB (= 52GiB) 103GB (= 96GiB)* GPU: NVIDIA M2050 515GFlops, 3GPUs/node* SSD 60GB x 2 120GB\u203b 55.8 GB node 120GB x 2 240GB\u203b 103GB node* OS: Suse Linux Enterprise+ Windows HPC", "num_citations": "10\n", "authors": ["498"]}
{"title": "Balanced distributed memory parallel computers\n", "abstract": " Mismatches between on-chip high performance CPU and data access times is the basic reason for the increasing gap between peak and sustained performance in distributed memory parallel computers. We propose the concept of balanced architectures, based on a network with a dynamic topology and communication patterns determined at compile time. The corresponding processing element is a cacheless CPU, which can achieve a 1 FLOP/clock cycle rate. Network and PE features are presented. An example shows that balanced architectures keep efficiency when scaling.", "num_citations": "10\n", "authors": ["498"]}
{"title": "La VALSE: Scalable Log Visualization for Fault Characterization in Supercomputers.\n", "abstract": " We design and implement La VALSE\u2014a scalable visualization tool to explore tens of millions of records of reliability, availability, and serviceability (RAS) logs\u2014for IBM Blue Gene/Q systems. Our tool is designed to meet various analysis requirements, including tracing causes of failure events and investigating correlations from the redundant and noisy RAS messages. La VALSE consists of multiple linked views to visualize RAS logs; each log message has a time stamp, physical location, network address, and multiple categorical dimensions such as severity and category. The timeline view features the scalable ThemeRiver and arc diagrams that enables interactive exploration of tens of millions of log messages. The spatial view visualizes the occurrences of RAS messages on hundreds of thousands of elements of Mira\u2014compute cards, node boards, midplanes, and racks\u2014with viewdependent level-of-detail rendering. The multidimensional view enables interactive filtering of different categorical dimensions of RAS messages. To achieve interactivity, we develop an efficient and scalable online data cube engine that can query 55 million RAS logs in less than one second. We present several case studies on Mira, a top supercomputer at Argonne National Laboratory. The case studies demonstrate that La VALSE can help users quickly identify the sources of failure events and analyze spatiotemporal correlations of RAS messages in different scales.", "num_citations": "9\n", "authors": ["498"]}
{"title": "Wavesz: A hardware-algorithm co-design of efficient lossy compression for scientific data\n", "abstract": " Error-bounded lossy compression is critical to the success of extreme-scale scientific research because of ever-increasing volumes of data produced by today's high-performance computing (HPC) applications. Not only can error-controlled lossy compressors significantly reduce the I/O and storage burden but they can retain high data fidelity for post analysis. Existing state-of-the-art lossy compressors, however, generally suffer from relatively low compression and decompression throughput (up to hundreds of megabytes per second on a single CPU core), which considerably restrict the adoption of lossy compression by many HPC applications especially those with a fairly high data production rate. In this paper, we propose a highly efficient lossy compression approach based on field programmable gate arrays (FPGAs) under the state-of-the-art lossy compression model SZ. Our contributions are fourfold.(1) We\u00a0\u2026", "num_citations": "8\n", "authors": ["498"]}
{"title": "Online data analysis and reduction: An important co-design motif for extreme-scale computers\n", "abstract": " A growing disparity between supercomputer computation speeds and I/O rates means that it is rapidly becoming infeasible to analyze supercomputer application output only after that output has been written to a file system. Instead, data-generating applications must run concurrently with data reduction and/or analysis operations, with which they exchange information via high-speed methods such as interprocess communications. The resulting parallel computing motif, online data analysis and reduction (ODAR), has important implications for both application and HPC systems design. Here we introduce the ODAR motif and its co-design concerns, describe a co-design process for identifying and addressing those concerns, present tools that assist in the co-design process, and present case studies to illustrate the use of the process and tools in practical settings.", "num_citations": "8\n", "authors": ["498"]}
{"title": "PaSTRI: A novel data compression algorithm for two-electron integrals in quantum chemistry\n", "abstract": " Integral computations for two-electron repulsion energies are very frequently used applications in quantum chemistry. Computational complexity, energy consumption and the size of the output data generated by these computations scales with O (N4), where N is the number of atoms simulated in the system. In many applications, the same integrals are required to be calculated multiple times. Storing these values and reusing them requires impractical amounts of storage space; whereas recalculating them requires a lot of computations. On the other hand, generated data typically requires much less precision than the built-in floating point data types. We propose PaSTRI (Pattern Scaling for Two-electron Repulsion Integrals), a fast novel compression algorithm which makes it possible to calculate these integrals only once, store them, and reuse them at much smaller computational cost then recalculation. PaSTRI is lossy compared to floating point numbers, but still maintains the precision level required by the integral computations. PaSTRI is an extension to SZ [1] compressor package as a part of ECP-EZ. We have evaluated our compressor using GAMESS [4] dataset, and achieved 17.5: 1 compression ratio whereas compression ratios for original SZ was 8.0: 1 and ZFP [3] was 7.1: 1.", "num_citations": "8\n", "authors": ["498"]}
{"title": "Integrating computing resources on multiple Grid-enabled job scheduling systems through a Grid RPC system\n", "abstract": " We present a framework for a parallel programming model by remote procedure calls, which bridge large-scale computing resource pools managed by multiple Grid-enabled job scheduling systems. With this system, the user can exploit not only remote servers and clusters, but also the computing resources provided by Grid-enabled job scheduling systems located on different sites. This framework requires a Grid remote procedure call (RPC) system to decouple the computation in a remote node from the Grid RPC mechanism and uses document-based communication rather than connection-based communication. We implemented the proposed framework as an extension of the OmniRPC system, which is a Grid RPC system for parallel programming. We designed a general interface to easily adapt the OmniRPC system to various Grid-enabled job scheduling systems, including XtremWeb, CyberGRIP\u00a0\u2026", "num_citations": "8\n", "authors": ["498"]}
{"title": "SPMD OpenMP versus MPI on a IBM SMP for 3 Kernels of the NAS Benchmarks\n", "abstract": " Shared Memory Multiprocessors are becoming more popular since they are used to deploy large parallel computers. The current trend is to enlarge the number of processors inside such multiprocessor nodes. However a lot of existing applications are using the message passing paradigm even when running on shared memory machines. This is due to three main factors: 1) the legacy of previous versions written for distributed memory computers, 2) the difficulty to obtain high performances with OpenMP when using loop level parallelization and 3) the complexity of writing multithreaded programs using a low level thread library. In this paper we demonstrate that OpenMP can provide better performance than MPI on SMP machines. We use a coarse grain parallelization approach, also known as the SPMD programming style with OpenMP. The performance evaluation considers the IBM SP3 NH2 and three\u00a0\u2026", "num_citations": "8\n", "authors": ["498"]}
{"title": "Calcul global paira pair: extension des systemes paira pair au calcul\n", "abstract": " La disponibilit\u00e9 simultan\u00e9e de nombreuses ressources de calcul/stockage performantes, d\u2019un r\u00e9seau de communication mondial suffisamment efficace et stable pour v\u00e9hiculer rapidement des grands volumes d\u2019informations et d\u2019applications n\u00e9cessitant des tr\u00e8s grandes capacit\u00e9s de calcul et de stockage a donn\u00e9 naissance au concept de Syst\u00e8mes Distribu\u00e9s \u00e0 Grande \u00c9chelle (SDGE). Les projets \u00e9tudi\u00e9s et les infrastructures propos\u00e9es \u00e9manent de diff\u00e9rentes communaut\u00e9s et peuvent \u00eatre class\u00e9s en trois cat\u00e9gories: les syst\u00e8mes de GRID, les syst\u00e8mes de Calcul Global et les syst\u00e8mes de partage de ressources entre Pairs", "num_citations": "8\n", "authors": ["498"]}
{"title": "Revisiting huffman coding: Toward extreme performance on modern gpu architectures\n", "abstract": " Today\u2019s high-performance computing (HPC) applications are producing vast volumes of data, which are challenging to store and transfer efficiently during the execution, such that data compression is becoming a critical technique to mitigate the storage burden and data movement cost. Huffman coding is arguably the most efficient Entropy coding algorithm in information theory, such that it could be found as a fundamental step in many modern compression algorithms such as DEFLATE. On the other hand, today\u2019s HPC applications are more and more relying on the accelerators such as GPU on supercomputers, while Huffman encoding suffers from low throughput on GPUs, resulting in a significant bottleneck in the entire data processing. In this paper, we propose and implement an efficient Huffman encoding approach based on modern GPU architectures, which addresses two key challenges: (1) how to parallelize\u00a0\u2026", "num_citations": "7\n", "authors": ["498"]}
{"title": "FT-iSort: efficient fault tolerance for introsort\n", "abstract": " Introspective sorting is a ubiquitous sorting algorithm which underlies many large scale distributed systems. Hardware-mediated soft errors can result in comparison and memory errors, and thus cause introsort to generate incorrect output, which in turn disrupts systems built upon introsort; hence, it is critical to incorporate fault tolerance capability within introsort. This paper proposes the first theoretically-sound, practical fault tolerant introsort with negligible overhead: FT-iSort. To tolerate comparison errors, we use minimal TMR protection via exploiting the properties of the effects of soft errors on introsort. This algorithm-based selective protection incurs far less overhead than na\u00efve TMR protection designed to protect an entire application. To tolerate memory errors that escape DRAM error correcting code, we propose XOR-based re-execution. We incorporate our fault tolerance method into the well-known parallel\u00a0\u2026", "num_citations": "7\n", "authors": ["498"]}
{"title": "Towards an energy estimator for fault tolerance protocols\n", "abstract": " Checkpointing protocols have different energy consumption depending on parameters like application features and platform characteristics. To select a protocol for a given execution, we propose an energy estimator that relies on an energy calibration of the considered platform and a user description of the execution settings.", "num_citations": "7\n", "authors": ["498"]}
{"title": "Damaris: Leveraging multicore parallelism to mask I/O Jitter\n", "abstract": " With exascale computing on the horizon, the performance variability of I/O systems represents a key challenge in sustaining high performance. In many HPC applications, I/O is concurrently performed by all processes, which leads to I/O bursts. This causes resource contention and substantial variability of I/O performance, which significantly impacts the overall application performance together with the predictability of its run time. In this paper, we first illustrate the presence of I/O jitter on different platforms, and show the impact of different user-configurable parameters and I/O approaches on write performance variability. We then propose a new approach to I/O, called Damaris, which leverages dedicated I/O cores on each multicore SMP node, along with the use of shared-memory, to efficiently perform asynchronous data processing and I/O. We evaluate our approach on three different platforms including the Kraken Cray XT5 supercomputer, with the CM1 atmospheric model, which is one of the target HPC applications for the Blue Waters project. By overlapping I/O with computation and by gathering data into large files while avoiding synchronization between cores, our solution brings several benefits: 1) it fully hides the jitter as well as all I/O-related costs, which makes simulation performance predictable; 2) it increases the sustained write throughput by a factor of 15 compared to standard approaches; 3) it allows almost perfect scalability of the simulation where other I/O approaches fail to scale; 4) it enables a 600% compression ratio without any additional overhead, leading to a major reduction of storage requirements.", "num_citations": "7\n", "authors": ["498"]}
{"title": "Towards efficient live migration of I/O intensive workloads: A transparent storage transfer proposal\n", "abstract": " Live migration of virtual machines (VMs) is key feature of virtualization that is extensively leveraged in IaaS cloud environments: it is the basic building block of several important features, such as load balancing, pro-active fault tolerance, power management, online maintenance, etc. While most live migration efforts concentrate on how to transfer the memory from source to destination during the migration process, comparatively little attention has been devoted to the transfer of storage. This problem is gaining increasing importance: due to performance reasons, virtual machines that run I/O intensive workloads tend to rely on local storage, which poses a difficult challenge on live migration: it needs to handle storage transfer in addition to memory transfer. This paper proposes a completely hypervisor-transparent approach that addresses this challenge. It relies on a hybrid active push-prioritized prefetch strategy, which makes it highly resilient to rapid changes of disk state exhibited by I/O intensive workloads. At the same time, transparency ensures a maximum of portability with a wide range of hypervisors. Large scale experiments that involve multiple simultaneous migrations of both synthetic benchmarks and a real scientific application show improvements of up to 10x faster migration time, 5x less bandwidth consumption and 62% less performance degradation over state-of-art.", "num_citations": "7\n", "authors": ["498"]}
{"title": "Towards end-to-end SDC detection for HPC applications equipped with lossy compression\n", "abstract": " Data reduction techniques have been widely demanded and used by large-scale high performance computing (HPC) applications because of vast volumes of data to be produced and stored for post-analysis. Due to very limited compression ratios of lossless compressors, error-bounded lossy compression has become an indispensable part in many HPC applications nowadays, because it can significantly reduce science data volume with user-acceptable data distortion. Since the large-scale HPC applications equipped with lossy compression techniques always need to deal with vast volume of data, soft errors or silent data corruptions (SDC) are non-negligible. Although SDC detection techniques have been studied for years, no studies were performed toward the HPC applications with lossy compression, leaving a significant gap between these applications and confidence of execution results. To fill this gap, this\u00a0\u2026", "num_citations": "6\n", "authors": ["498"]}
{"title": "Toward Feature-Preserving 2D and 3D Vector Field Compression.\n", "abstract": " The objective of this work is to develop error-bounded lossy compression methods to preserve topological features in 2D and 3D vector fields. Specifically, we explore the preservation of critical points in piecewise linear vector fields. We define the preservation of critical points as, without any false positive, false negative, or false type change in the decompressed data,(1) keeping each critical point in its original cell and (2) retaining the type of each critical point (eg, saddle and attracting node). The key to our method is to adapt a vertex-wise error bound for each grid point and to compress input data together with the error bound field using a modified lossy compressor. Our compression algorithm can be also embarrassingly parallelized for large data handling and in situ processing. We benchmark our method by comparing it with existing lossy compressors in terms of false positive/negative/type rates, compression ratio, and various vector field visualizations with several scientific applications.", "num_citations": "6\n", "authors": ["498"]}
{"title": "Navigating the blue waters: Online failure prediction in the petascale era\n", "abstract": " At the scale of todays large scale systems, fault tolerance is no longer an option, but a necessity. As the size of supercomputers increases, so does the probability of a single component failure within a time frame. With a system MTBF of less than one day and predictions that future systems will experience delays of couple of hours between failures, current fault tolerance strategies face serious limitations. Checkpointing is currently an area of significant research, however, even when implemented in a satisfactory manner, there is still a considerable loss of computation time due to frequent application roll-backs. With the growing operation cost of extreme scale supercomputers like Blue Waters, the act of predicting failures to prevent the loss of computation hours becomes cumbersome and presents a couple of challenges not encountered for smaller systems.In this paper, we present a novel methodology for truly online failure prediction for the Blue Water system. We analyze its results and show that some failures types can be predicted with over 60% recall and a precision of over 85%. The failures wich represent the main bottlenecks are discussed in detail and possible solutions are proposed by investigating different prediction methods. We show to what extent online failure prediction is a possibility at petascale and what are the challenges in achieving an effective fault prediction mechanism for Blue Waters.", "num_citations": "6\n", "authors": ["498"]}
{"title": "Towards Scalable Data Management for Map-Reduce-based Data-Intensive Applications on Cloud and Hybrid Infrastructures\n", "abstract": " As Map-Reduce emerges as a leading programming paradigm for data-intensive computing, today's frameworks which support it still have substantial shortcomings that limit its potential scalability. In this paper we discuss several directions where there is room for such progress: they concern storage efficiency under massive data access concurrency, scheduling, volatility and fault-tolerance. We place our discussion in the perspective of the current evolution towards an increasing integration of large-scale distributed platforms (clouds, cloud federations, enterprise desktop grids, etc.). We propose an approach which aims to overcome the current limitations of existing Map-Reduce frameworks, in order to achieve scalable, concurrency-optimized, fault-tolerant Map-Reduce data processing on hybrid infrastructures. This approach will be evaluated with real-life bio-informatics applications on existing Nimbus-powered cloud testbeds interconnected with desktop grids.", "num_citations": "6\n", "authors": ["498"]}
{"title": "Toward third generation internet desktop grids\n", "abstract": " Projects like SETI@home and Folding@home have popularized Internet Desktop Grid (IDG) computing. The first generation of IDG projects scalled to millions of participatings but was dedicated to a specific application. BOINC, United Device and XtremWeb belong to a second generation of IDG platforms. Their architecture was designed to accommodate many applications but has drawbacks like limited security and a centralized architecture. In this paper we present a new design for Internet Desktop Grid, following a layered approach. The new architecture establishes an overlay network, giving the participating nodes direct communication capabilities. From that basis many key mechanisms of IDG can be implemented using existing cluster tools and extra IDG specificic software. As a proof of concept, we run a bioinformatic application on a third generation IDG, based on a connectivity service (PVC), an existing job scheduler (Condor), a high performance data transport service (Bittorent) and a custom result certification mechanism.", "num_citations": "6\n", "authors": ["498"]}
{"title": "Optimizing error-bounded lossy compression for scientific data by dynamic spline interpolation\n", "abstract": " Today\u2019s scientific simulations are producing vast volumes of data that cannot be stored and transferred efficiently because of limited storage capacity, parallel I/O bandwidth, and network bandwidth. The situation is getting worse over time because of the ever-increasing gap between relatively slow data transfer speed and fast-growing computation power in modern supercomputers. Error-bounded lossy compression is becoming one of the most critical techniques for resolving the big scientific data issue, in that it can significantly reduce the scientific data volume while guaranteeing that the reconstructed data is valid for users because of its compression-error-bounding feature. In this paper, we present a novel error-bounded lossy compressor based on a state-of-the-art prediction-based compression framework. Our solution exhibits substantially better compression quality than all of the existing error-bounded lossy\u00a0\u2026", "num_citations": "5\n", "authors": ["498"]}
{"title": "SDC resilient error-bounded lossy compressor\n", "abstract": " Lossy compression is one of the most important strategies to resolve the big science data issue, however, little work was done to make it resilient against silent data corruptions (SDC). In fact, SDC is becoming non-negligible because of exa-scale computing demand on complex scientific simulations with vast volume of data being produced or in some particular instruments/devices (such as interplanetary space probe) that need to transfer large amount of data in an error-prone environment. In this paper, we propose an SDC resilient error-bounded lossy compressor upon the SZ compression framework. Specifically, we adopt a new independent-block-wise model that decomposes the entire dataset into many independent sub-blocks to compress. Then, we design and implement a series of error detection/correction strategies based on SZ. We are the first to extend algorithm-based fault tolerance (ABFT) to lossy compression. Our proposed solution incurs negligible execution overhead without soft errors. It keeps the correctness of decompressed data still bounded within user's requirement with a very limited degradation of compression ratios upon soft errors.", "num_citations": "5\n", "authors": ["498"]}
{"title": "Accelerating relative-error bounded lossy compression for hpc datasets with precomputation-based mechanisms\n", "abstract": " Scientific simulations in high-performance computing (HPC) environments are producing vast volume of data, which may cause a severe I/O bottleneck at runtime and a huge burden on storage space for post-analysis. Unlike the traditional data reduction schemes (such as deduplication or lossless compression), not only can error-controlled lossy compression significantly reduce the data size but it can also hold the promise to satisfy user demand on error control. Point-wise relative error bounds (i.e., compression errors depends on the data values) are widely used by many scientific applications in the lossy compression, since error control can adapt to the precision in the dataset automatically. Point-wise relative error bounded compression is complicated and time consuming. In this work, we develop efficient precomputation-based mechanisms in the SZ lossy compression framework. Our mechanisms can avoid\u00a0\u2026", "num_citations": "5\n", "authors": ["498"]}
{"title": "Neural network based silent error detector\n", "abstract": " As we move toward exascale platforms, silent data corruptions (SDC) are likely to occur more frequently. Such errors can lead to incorrect results. Attempts have been made to use generic algorithms to detect such errors. Such detectors have demonstrated high precision and recall for detecting errors, but only if they run immediately after an error has been injected. In this paper, we propose a neural network detector that can detect SDCs even multiple iterations after they were injected. We have evaluated our detector with 6 FLASH applications and 2 Mantevo mini-apps. Experiments show that our detector can detect more than 89% of SDCs with a false positive rate of less than 2%.", "num_citations": "5\n", "authors": ["498"]}
{"title": "Failure prediction: what to do with unpredicted failures\n", "abstract": " As large parallel systems increase in size and complexity, failures are inevitable and exhibit complex space and time dynamics. Several key results have demonstrated that recent advances in event log analysis can provide precise failure prediction. The state of the art in failure prediction provides a ratio of correctly identified failures to the number of all predicted failures of over 90% and able to discover around 50% of all failures in a system. However, large parts of failures are not predicted and are considered as false negative alerts. Therefore, developing efficient fault tolerance strategies to tolerate failures requires a good perception and understanding of failure prediction characteristics. To understand the properties of false negative alerts, we conducted a statistical analysis of the probability distribution of such alerts and their impact on fault tolerance techniques. specifically we studied failures logs from different HPC production systems. We show that (i) the false negative distribution has the same nature as the failure distribution (ii) After adding failure prediction, we were able to infer statistical models that describe the inter-arrival time between false negative alerts and hence current fault tolerance can be applied to these systems. Moreover, we show that the current failures traces have a high correlation between the failure inter-arrival time that can be used to improve the failure prediction mechanism. Another important result is that checkpoint intervals can still be computed from an existing first-order formula.", "num_citations": "5\n", "authors": ["498"]}
{"title": "P2P: D\u00e9veloppements r\u00e9cents et perspectives\n", "abstract": " \u2022 Caract\u00e9ristiques statiques et dynamiques(SETI, ADSL, etc.)\u2022 Techniques de connectivit\u00e9(firewalls!)\u2022 Protocoles de communication(Distribution de contenu)\u2022 Recherche de ressources(DHT)\u2022 Approches g\u00e9n\u00e9riques(Jxta, PVC)\u2022 Applications(Archivage r\u00e9parti)\u2022 Aspects juridiques(T\u00e9l\u00e9chargement)\u2022 Perspectives(Externalisation)", "num_citations": "5\n", "authors": ["498"]}
{"title": "XtremWeb: une plate-forme de recherche sur le calcul global et pair \u00e0 pair\n", "abstract": " La connection \u00e0 grande \u00e9chelle des ordinateurs aux r\u00e9seaux de communication et les m\u00e9canismes qui permettent leur interop\u00e9rabilit\u00e9 (standardisation des protocoles, homog\u00e9n\u00e9isation des formats d'\u00e9change, syst\u00e8mes de nomage ef\ufb01cace, s\u00e9curisation des sites) motivent des tentatives de globalisation des ressources et des donn\u00e9es \u00e0 partir de syst\u00e8mes de Calcul Global, Pair \u00e0 Pair ou de GRID. Ces syst\u00e8mes sont les pr\u00e9misses de l'\u00e9mergence de syst\u00e8mes d'exploitation \u00e0 grande \u00e9chelle. Dans cet article nous proposons d'abord de situer les syst\u00e9mes Pair \u00e0 Pair par rapport aux autres structures de GRID en soulignant leur diff\u00e9rences qui ont principalement une origine historique. Dans une deuxi\u00e8me partie nous pr\u00e9sentons une ratio- nalisation des syst\u00e9mes de Calcul Global et Pair \u00e0 Pair. Nous d\u00e9taillons ensuite la plate-forme XtremWeb qui constitue l'une des premi\u00e8res tentatives de g\u00e9n\u00e9ralisation du concept de parit\u00e9 entre les ressources appliqu\u00e9 \u00e0 la globalisation du processeur, de la m\u00e9moire et du stockage. Nous terminons par la pr\u00e9sentation de quelques points durs en recherche : la s\u00e9curisation des participants et la communication inter-noeud.", "num_citations": "5\n", "authors": ["498"]}
{"title": "A risc central processing unit for a massivelly parallel architecture\n", "abstract": " We present the CPU of a message-passing Massively Parallel Architecture. It has an efficient reduced instruction set to execute a simple lexical LISP language. With a 1.5 \u03bcm CMOS technology, the chip area is approximatively 10 mm2, to be compatible with a 1 cm2 single chip including local memory and a hardwared Communication Unit.", "num_citations": "5\n", "authors": ["498"]}
{"title": "SDRBench: Scientific data reduction benchmark for lossy compressors\n", "abstract": " Efficient error-controlled lossy compressors are becoming critical to the success of today\u2019s large-scale scientific applications because of the ever-increasing volume of data produced by the applications. In the past decade, many lossless and lossy compressors have been developed with distinct design principles for different scientific datasets in largely diverse scientific domains. In order to support researchers and users assessing and comparing compressors in a fair and convenient way, we establish a standard compression assessment benchmark \u2013 Scientific Data Reduction Benchmark (SDRBench) 1 . SDRBench contains a vast variety of real-world scientific datasets across different domains, summarizes several critical compression quality evaluation metrics, and integrates many state-of-the-art lossy and lossless compressors. We demonstrate evaluation results using SDRBench and summarize six valuable\u00a0\u2026", "num_citations": "4\n", "authors": ["498"]}
{"title": "Exploring variable accuracy storage through lossy compression techniques in numerical linear algebra: a first application to flexible GMRES\n", "abstract": " Large scale applications running on HPC systems often require a substantial amount of memory and can have a large computational overhead. Lossy data compression techniques can reduce the size of the data and associated communication cost, but the effect of the loss ofaccuracy on the numerical algorithm can be hard to predict. In this paper we examine the FGMRES algorithm, which requires the storage of a basis for the Krylov subspace and for the search space spanned by the solutions of the preconditioning systems. We show that the vectors spanning this search space can be compressed by looking at the combination of FGMRES and compression in the context of inexact Krylov subspace methods. This allows us to derive a bound on the normwise relative compression error in each iteration. We use this bound to formulate a number of different practical compression strategies, and validate and compare them through numerical experiments.", "num_citations": "4\n", "authors": ["498"]}
{"title": "Analyzing the performance and accuracy of lossy checkpointing on sub-iteration of nwchem\n", "abstract": " Future exascale systems are expected to be characterized by more frequent failures than current petascale systems. This places increased importance on the application to minimize the amount of time wasted due to recompution when recovering from a checkpoint. Typically HPC application checkpoint at iteration boundaries. However, for applications that have a high per-iteration cost, checkpointing inside the iteration limits the amount of re-computation. This paper analyzes the performance and accuracy of using lossy compressed check-pointing in the computational chemistry application NWChem. Our results indicate that lossy compression is an effective tool for reducing the sub-iteration checkpoint size. Moreover, compression error tolerances that yield acceptable deviation in accuracy and iteration count are quantified.", "num_citations": "4\n", "authors": ["498"]}
{"title": "Memory-efficient quantum circuit simulation by using lossy data compression\n", "abstract": " In order to evaluate, validate, and refine the design of new quantum algorithms or quantum computers, researchers and developers need methods to assess their correctness and fidelity. This requires the capabilities of quantum circuit simulations. However, the number of quantum state amplitudes increases exponentially with the number of qubits, leading to the exponential growth of the memory requirement for the simulations. In this work, we present our memory-efficient quantum circuit simulation by using lossy data compression. Our empirical data shows that we reduce the memory requirement to 16.5% and 2.24E-06 of the original requirement for QFT and Grover's search, respectively. This finding further suggests that we can simulate deep quantum circuits up to 63 qubits with 0.8 petabytes memory.", "num_citations": "4\n", "authors": ["498"]}
{"title": "Detection of silent data corruption in adaptive numerical integration solvers\n", "abstract": " Scientific computing requires trust in results. In high-performance computing, trust is impeded by silent data corruption (SDC), in other words corruption that remains unnoticed. Numerical integration solvers are especially sensitive to SDCs because an SDC introduced in a certain step affects all the following steps. SDCs can even cause the solver to become unstable. Adaptive solvers can change the step size, by comparing an estimation of the approximation error with an user-defined tolerance. If the estimation exceeds the tolerance, the step is rejected and recomputed. Adaptive solvers have an inherent resilience, because some SDCs might have no consequences on the accuracy of the results, and some SDCs might push the approximation error beyond the tolerance. Our first contribution shows that the rejection mechanism is not reliable enough to reject all SDCs that affect the results' accuracy, because the\u00a0\u2026", "num_citations": "4\n", "authors": ["498"]}
{"title": "Poster: Energy-performance tradeoffs in multilevel checkpoint strategies\n", "abstract": " Increased complexity of computer architectures, consideration of power constraints, and expected failure rates of hardware components make the design and analysis of energy-efficient fault-tolerance schemes an increasingly challenging and important task. We develop run-time and study FTI, a multilevel checkpoint library, on an IBM Blue Gene/Q. We show that FTI has a low energy footprint and that, consequently optimal checkpoint-interval values with respect to time and energy are similar.", "num_citations": "4\n", "authors": ["498"]}
{"title": "Comparaison de MPI, OpenMP et MPI+ OpenMP sur un n\u0153ud multiprocesseur multic\u0153urs AMD \u00e0 m\u00e9moire partag\u00e9e\n", "abstract": " La majorit\u00e9 des architectures parall\u00e8les sont organis\u00e9es comme un cluster de n\u0153uds de multi-c\u0153urs \u00e0 m\u00e9moire partag\u00e9e (mcSMN). Les statistiques montrent que majorit\u00e9 des t\u00e2ches ex\u00e9cut\u00e9es sur ces plateformes utilisent un seul n\u0153ud. Si on restreint l\u2019investigation aux environnements et mod\u00e8les de programmation parall\u00e8les les plus connus, il y a 4 mod\u00e8les \u00e0 comparer: MPI, OpenMP (grain fin et SPMD) et MPI+ OpenMP. Dans ce papier, nous avons compar\u00e9 les temps d\u2019ex\u00e9cution, de calcul et de communication, les d\u00e9fauts de caches L1 et L2 pour ces mod\u00e8les en utilisant 4 benchmarks NAS sur un quadriprocesseur quadri-c\u0153urs \u00e0 m\u00e9moire partag\u00e9e d\u2019AMD. Contrairement aux r\u00e9sultats ant\u00e9rieurs sur des multi-processeurs mono-c\u0153ur, nous montrons que pour les benchmarks de classe A et B: 1) OpenMP est plus performant que MPI sur toutes les variantes et 2) le mod\u00e8le hybride MPI+ OpenMP est plus efficace que le mod\u00e8le pur MPI. Les analyses des temps de calcul et de communication permettent d\u2019expliquer ces r\u00e9sultats pour le noud AMD utilis\u00e9.", "num_citations": "4\n", "authors": ["498"]}
{"title": "Availability traces of enterprise desktop grids\n", "abstract": " Desktop grids use the idle cycles of mostly desktop PC's to support large-scale computation and data storage. Today, these types of computing platforms are the largest distributed computing systems in the world. The most popular project, SETI@home, uses over 20 TeraFlop/sec provided by hundreds of thousands of desktops. Numerous other projects, which span a wide range of scientific domains, also use the cumulative computing power offered by desktop grids, and there have also been commercial endeavors for harnessing the computing power within an enterprise, i.e., an organization's local area network. Despite the popularity and success of many desktop grid projects, the volatility of the hosts within desktop grids has been poorly understood. Yet, this characterization is essential for accurate simulation and modelling of such platforms. In an effort to support such activities, we gathered availability traces\u00a0\u2026", "num_citations": "4\n", "authors": ["498"]}
{"title": "Augernome & XtremWeb: Monte Carlos computation on a global computing platform\n", "abstract": " In this paper, we present XtremWeb, a Global Computing platform used to generate monte carlos showers in Auger, an HEP experiment to study the highest energy cosmic rays at Mallargue-Mendoza, Argentina. XtremWeb main goal, as a Global Computing platform, is to compute distributed applications using idle time of widely interconnected machines. It is especially dedicated to -but not limited to- multi-parameters applications such as monte carlos computations; its security mechanisms ensuring not only hosts integrity but also results certification and its fault tolerant features, encouraged us to test it and, finally, to deploy it as to support our CPU needs to simulate showers. We first introduce Auger computing needs and how Global Computing could help. We then detail XtremWeb architecture and goals. The fourth and last part presents the profits we have gained to choose this platform. We conclude on what could be done next.", "num_citations": "4\n", "authors": ["498"]}
{"title": "High performance computing with rpc programming style\n", "abstract": " This paper investigates the performance of the RPC model of an original parallel environment called OVM for Out-of-order execution parallel Virtual Machine. OVM system is designed to reach high performance for parallel applications. OVM is based on a client-server architecture. Its design follows two main principles: RPC programming style and dynamic load balancing. This paper mainly address the performance issues related to RPC programming style. We test the performance of an OVM implementation on top of Myrinet for regular as well as irregular parallel applications. The contribution is three fold. First, we provide some key performance results of OVM. Second we show that RPC programming style can approach the performance of traditional SPMD programming style at least for limited number of nodes. To demonstrate this, we compare OVM implementation and original MPI implementation for 3 kernels\u00a0\u2026", "num_citations": "4\n", "authors": ["498"]}
{"title": "Introduction au parall\u00e9lisme et aux architectures parall\u00e8les\n", "abstract": " Sauf mention contraire ci-dessus, le contenu de cette notice bibliographique peut \u00eatre utilis\u00e9 dans le cadre d\u2019une licence CC BY 4.0 Inist-CNRS/Unless otherwise stated above, the content of this bibliographic record may be used under a CC BY 4.0 licence by Inist-CNRS/A menos que se haya se\u00f1alado antes, el contenido de este registro bibliogr\u00e1fico puede ser utilizado al amparo de una licencia CC BY 4.0 Inist-CNRS", "num_citations": "4\n", "authors": ["498"]}
{"title": "Sur la nature auto-similaire de l'activit\u00e9 de stations de travail et de serveurs HTTP\n", "abstract": " Sauf mention contraire ci-dessus, le contenu de cette notice bibliographique peut \u00eatre utilis\u00e9 dans le cadre d\u2019une licence CC BY 4.0 Inist-CNRS/Unless otherwise stated above, the content of this bibliographic record may be used under a CC BY 4.0 licence by Inist-CNRS/A menos que se haya se\u00f1alado antes, el contenido de este registro bibliogr\u00e1fico puede ser utilizado al amparo de una licencia CC BY 4.0 Inist-CNRS", "num_citations": "4\n", "authors": ["498"]}
{"title": "Communications in parallel architectures and networks of workstations: From standardisation to new standards\n", "abstract": " Standardisation, which is the rule for PCs and workstation, is quickly expending for parallel machines and networks of workstations. The use of commodities is the key issue to reduce the cost /performance ratio: standard microprocessors, OS, libraries... are used. Standardisation of communications is more difficult if both high performance and protection in multi-user context are wanted. We examine the present trends toward standardisation of communication for parallel machines and networks of workstations. We discuss some software and hardware features to improve performance compared to the usual PVM-Unix-TCP/IP-Ethernet stack of protocols.", "num_citations": "4\n", "authors": ["498"]}
{"title": "DeepClone: Lightweight state replication of deep learning models for data parallel training\n", "abstract": " Training modern deep neural network (DNN) models involves complex workflows triggered by model exploration, sensitivity analysis, explainability, etc. A key primitive in this context is the ability to clone a model training instance, i.e. \u201cfork\u201d the training process in a potentially different direction, which enables comparisons of different evolution paths using variations of training data and model parameters. However, in a quest improve the training throughput, a mix of data parallel, model parallel, pipeline parallel and layer-wise parallel approaches are making the problem of cloning highly complex. In this paper, we explore the problem of efficient cloning under such circumstances. To this end, we leverage several properties of data-parallel training and layer-wise parallelism to design DeepClone, a cloning approach based on augmenting the execution graph to gain direct access to tensors, which are then sharded\u00a0\u2026", "num_citations": "3\n", "authors": ["498"]}
{"title": "Fulfilling the promises of lossy compression for scientific applications\n", "abstract": " Many scientific simulations, machine/deep learning applications and instruments are in need of significant data reduction. Error-bounded lossy compression has been identified as one solution and has been tested for many use-cases: reducing streaming intensity (instruments), reducing storage and memory footprints, accelerating computation and accelerating data access and transfer. Ultimately, users\u2019 trust in lossy compression relies on the preservation of science: same conclusions should be drawn from computations or analysis done from lossy compressed data. Experience from scientific simulations, Artificial Intelligence (AI) and instruments reveals several points: (i) there are important gaps in the understanding of the effects of lossy compressed data on computations, AI and analysis, (ii) each use-case, application and user has its own requirements in terms of compression ratio, speed and accuracy\u00a0\u2026", "num_citations": "3\n", "authors": ["498"]}
{"title": "Performance optimization for relative-error-bounded lossy compression on scientific data\n", "abstract": " Scientific simulations in high-performance computing (HPC) environments generate vast volume of data, which may cause a severe I/O bottleneck at runtime and a huge burden on storage space for postanalysis. Unlike traditional data reduction schemes such as deduplication or lossless compression, not only can error-controlled lossy compression significantly reduce the data size but it also holds the promise to satisfy user demand on error control. Pointwise relative error bounds (i.e., compression errors depends on the data values) are widely used by many scientific applications with lossy compression since error control can adapt to the error bound in the dataset automatically. Pointwise relative-error-bounded compression is complicated and time consuming. We develop efficient precomputation-based mechanisms based on the SZ lossy compression framework. Our mechanisms can avoid costly logarithmic\u00a0\u2026", "num_citations": "3\n", "authors": ["498"]}
{"title": "5G enabled energy innovation: advanced wireless networks for science\n", "abstract": " 5G Award List.xlsx Page 1 Principal Investigator Title Institution City State 9-digit zip code Kagawa, Ai 5G-enabled Reliable and Decentralized IoT Framework with Blockchain Brookhaven National Laboratory Upton NY 11973-5000 Beckman, Pete Wildebeest: Migratory Computation for the Wireless 5G Digital Continuum Argonne National Laboratory Lemont IL 60439-4803 Kiran, Mariam Large-scale Self-driving 5G Network for Science Lawrence Berkeley National Laboratory Berkeley CA 94720-8099 Fan, Xiaoyuan Energy FRAME: 5G Fabricated Resource and Asset Management Encompassment for Energy Infrastructure Pacific Northwest National Laboratory Richland WA 99352-1793 Reisner, Jon 5G Drones: Real Time Data Assimilation to Transform Wildfire Predictability Los Alamos National Laboratory Los Alamos NM 87544-0600 5G Enabled Energy Innovation Advanced Wireless Networks for Science \u2026", "num_citations": "3\n", "authors": ["498"]}
{"title": "Amplitude-aware lossy compression for quantum circuit simulation\n", "abstract": " Classical simulation of quantum circuits is crucial for evaluating and validating the design of new quantum algorithms. However, the number of quantum state amplitudes increases exponentially with the number of qubits, leading to the exponential growth of the memory requirement for the simulations. In this paper, we present a new data reduction technique to reduce the memory requirement of quantum circuit simulations. We apply our amplitude-aware lossy compression technique to the quantum state amplitude vector to trade the computation time and fidelity for memory space. The experimental results show that our simulator only needs 1/16 of the original memory requirement to simulate Quantum Fourier Transform circuits with 99.95% fidelity. The reduction amount of memory requirement suggests that we could increase 4 qubits in the quantum circuit simulation comparing to the simulation without our technique. Additionally, for some specific circuits, like Grover's search, we could increase the simulation size by 18 qubits.", "num_citations": "3\n", "authors": ["498"]}
{"title": "Evaluation of the Single-precision Floatingpoint Vector Add Kernel Using the Intel FPGA SDK for OpenCL\n", "abstract": " Open Computing Language (OpenCL) is a high-level language that enables software programmers to explore Field Programmable Gate Arrays (FPGAs) for application acceleration. The Intel FPGA software development kit (SDK) for OpenCL allows a user to specify applications at a high level and explore the performance of low-level hardware acceleration. In this report, we present the FPGA performance and power consumption results of the single-precision floating-point vector add OpenCL kernel using the Intel FPGA SDK for OpenCL on the Nallatech 385A FPGA board. The board features an Arria 10 FPGA. We evaluate the FPGA implementations using the compute unit duplication and kernel vectorization optimization techniques. On the Nallatech 385A FPGA board, the maximum compute kernel bandwidth we achieve is 25.8 GB/s, approximately 76% of the peak memory bandwidth. The power consumption of the FPGA device when running the kernels ranges from 29W to 42W.", "num_citations": "3\n", "authors": ["498"]}
{"title": "Evaluation of the OpenCL AES Kernel using the Intel FPGA SDK for OpenCL\n", "abstract": " The OpenCL standard is an open programming model for accelerating algorithms on heterogeneous computing system. OpenCL extends the C-based programming language for developing portable codes on different platforms such as CPU, Graphics processing units (GPUs), Digital Signal Processors (DSPs) and Field Programmable Gate Arrays (FPGAs). The Intel FPGA SDK for OpenCL is a suite of tools that allows developers to abstract away the complex FPGA-based development flow for a high-level software development flow. Users can focus on the design of hardware-accelerated kernel functions in OpenCL and then direct the tools to generate the low-level FPGA implementations. The approach makes the FPGA-based development more accessible to software users as the needs for hybrid computing using CPUs and FPGAs are increasing. It can also significantly reduce the hardware development time as users can evaluate different ideas with high-level language without deep FPGA domain knowledge. In this report, we evaluate the performance of the kernel using the Intel FPGA SDK for OpenCL and Nallatech 385A FPGA board. Compared to the M506 module, the board provides more hardware resources for a larger design exploration space. The kernel performance is measured with the compute kernel throughput, an upper bound to the FPGA throughput. The report more\u00bb", "num_citations": "3\n", "authors": ["498"]}
{"title": "OpenWP: Combining annotation language and workflow environments for porting existing applications on grids\n", "abstract": " Many Industrial companies are looking for programming environments to port their existing applications onto the grid. Workflow environments are an appealing solution for them as they match the architectural features of grids (hierarchy, heterogeneity, dynamism). However,current workflow environments require from programmers significant efforts to adapt existing applications. In this paper, we propose the OpenWP programming and runtime environment, in order to ease the adaptation and execution of existing applications onto grids. OpenWP essentially allows the programmer:1) to express the parallelism and distribution in existing codes using directives and 2) to execute the applications on grids using existing workflow engines. This paper presents the OpenWP environment in details and evaluates its performance with a non trivial industrial mesher application used by an aerospace company.", "num_citations": "3\n", "authors": ["498"]}
{"title": "Fault Tolerance for PetaScale Systems: Current Knowledge, Challenges and Opportunities\n", "abstract": " Ckpt-rest on local disc does not help to reduce the ckpt time rest on local disc does not help to reduce the ckpt time--> need to save (async.) checkpoint on stable storage.> need to save (async.) checkpoint on stable storage. Restarting a failed node from local storage does help since", "num_citations": "3\n", "authors": ["498"]}
{"title": "A file transfer service with client/server, P2P and wide area storage protocols\n", "abstract": " The last years have seen the emergence of new P2P file distribution protocols such as BitTorrent as well as new Wide Area Storage based on Web Service technology. In this paper, we propose a multi-protocol file transfer service which supports client/server and P2P protocols, as well as Wide Area Storage such as Amazon S3. We describe the mechanisms used to ensure file transfer monitoring and reliability. We explain how to plug-in new or existing protocols and we give evidence of the versatility of the framework by implementing the HTTP, FTP and BitTorrent protocols and access to the Amazon S3 and IBP Wide Area Storage. Finally, we report on basic performance evaluation of our framework, both in a Grid context and on the Internet.", "num_citations": "3\n", "authors": ["498"]}
{"title": "Recent Advances in Parallel Virtual Machine and Message Passing Interface: 14th European PVM/MPI User's Group Meeting, Paris France, September 30-October 3, 2007, Proceedings\n", "abstract": " This book constitutes the refereed proceedings of the 14th European PVM/MPI Users' Group Meeting held in Paris, France, September 30-October 3, 2007. The 40 revised full papers presented together with abstracts of six invited contributions, three tutorial papers and six poster papers were carefully reviewed and selected from 68 submissions. The papers are organized in topical sections.", "num_citations": "3\n", "authors": ["498"]}
{"title": "Parallel machines with a three-dimensional processor mesh(Machines paralleles a grille de processeurs tridimensionnelle)\n", "abstract": " The communication system of a massively parallel architecture called MEGA is presented. A new routing strategy called forced routing is presented which minimizes contention by using all the shortest paths in the network. Its performance has been simulated, and the results are presented including an IC implementation study showing its feasibility. Finally, the paper presents a new IC package permitting Full 30 interconnection designed to build massively parallel machines with 3D-mesh interconnection technology. (L.M.)", "num_citations": "3\n", "authors": ["498"]}
{"title": "VELOC: VEry Low Overhead Checkpointing in the Age of Exascale\n", "abstract": " Checkpointing large amounts of related data concurrently to stable storage is a common I/O pattern of many HPC applications. However, such a pattern frequently leads to I/O bottlenecks that lead to poor scalability and performance. As modern HPC infrastructures continue to evolve, there is a growing gap between compute capacity vs. I/O capabilities. Furthermore, the storage hierarchy is becoming increasingly heterogeneous: in addition to parallel file systems, it comprises burst buffers, key-value stores, deep memory hierarchies at node level, etc. In this context, state of art is insufficient to deal with the diversity of vendor APIs, performance and persistency characteristics. This extended abstract presents an overview of VeloC (Very Low Overhead Checkpointing System), a checkpointing runtime specifically design to address these challenges for the next generation Exascale HPC applications and systems. VeloC offers a simple API at user level, while employing an advanced multi-level resilience strategy that transparently optimizes the performance and scalability of checkpointing by leveraging heterogeneous storage.", "num_citations": "2\n", "authors": ["498"]}
{"title": "Parallel Partial Reduction for Large-Scale Data Analysis and Visualization\n", "abstract": " We present a novel partial reduction algorithm to aggregate sparsely distributed intermediate results that are generated by data-parallel analysis and visualization algorithms. Applications of partial reduction include flow trajectory analysis, big data online analytical processing, and volume rendering. Unlike traditional full parallel reduction that exchanges dense data across all processes, the purpose of partial reduction is to exchange only intermediate results that correspond to the same query, such as line segments of the same flow trajectory. To this end, we design a three-stage algorithm that minimizes the communication cost: (1) partitioning the result space into groups; (2) constructing and optimizing the reduction partners for each group; and (3) initiating collective reduction operations for all groups concurrently. Both theoretical and empirical analyses show that our algorithm outperforms the traditional methods\u00a0\u2026", "num_citations": "2\n", "authors": ["498"]}
{"title": "An efficient approach to lossy compression with pointwise relative error bound\n", "abstract": " BackgroundExtreme large volumes of scientific data Climate Simulation: 260TB every 16s HACC: 60PB for simulating 3.5 trillion particles Error control", "num_citations": "2\n", "authors": ["498"]}
{"title": "Addressing the last roadblock for message logging in HPC: Alleviating the memory requirement using dedicated resources\n", "abstract": " Currently used global application checkpoint-restart will not be a suitable solution for HPC applications running on large scale as, given the predicted fault rates, it will impose a high load on the I/O subsystem and lead to inefficient resource usage. Combining application checkpointing with message logging is appealing as it allows restarting only the processes that actually failed. One major issue with message logging protocols is the high amount of memory required to store logs. In this work we propose to use additional dedicated resources to save the part of the logs that would not fit in the memory of a compute node. We show that, combined with a cluster-based hierarchical logging technique, only few dedicated nodes would be required to accommodate the memory requirement of message logging protocols. We additionally show that the proposed technique achieves a reasonable performance\u00a0\u2026", "num_citations": "2\n", "authors": ["498"]}
{"title": "Improving the trust in results of numerical simulations and scientific data analytics\n", "abstract": " This white paper investigates several key aspects of the trust that a user can give to the results of numerical simulations and scientific data analytics. In this document, the notion of trust is related to the integrity of numerical simulations and data analytics applications. This white paper complements the DOE ASCR report on Cybersecurity for Scientific Computing Integrity by (1) exploring the sources of trust loss;(2) reviewing the definitions of trust in several areas;(3) providing numerous cases of result alteration, some of them leading to catastrophic failures;(4) examining the current notion of trust in numerical simulation and scientific data analytics;(5) providing a gap analysis; and (6) suggesting two important research directions and their respective research topics. To simplify the presentation without loss of generality, we consider that trust in results can be lost (or the results\u2019 integrity impaired) because of any form of corruption happening during the execution of the numerical simulation or the data analytics application. In general, the sources of such corruption are threefold: errors, bugs, and attacks. Current applications are already using techniques to deal with different types of corruption. However, not all potential corruptions are covered by these techniques. We firmly believe more\u00bb", "num_citations": "2\n", "authors": ["498"]}
{"title": "HPCS 2013 panel: The era of exascale sciences: Challenges, needs and requirements\n", "abstract": " Computer simulation, or simulation-based engineering and science, is now widely accepted by many companies as a way of testing the product before manufacturing physical prototypes. Using simulation-based engineering design, companies are expected to deliver more business value to their end customers in the dynamic business changing environment by reducing product development cost and time while increasing the performance. There are many similar gains in other fields, such as healthcare, biomedical and biosciences, climate and environmental changes, design and manufacturing of advanced materials, geology, chemistry and physics, and even financial systems. Recent directions in research and technological studies have shown that Exascale computing may provide solutions for some of these issues by leading to advances in several areas of science and technology while allowing more\u00a0\u2026", "num_citations": "2\n", "authors": ["498"]}
{"title": "An implementation of parallel 3-D FFT with 1.5-d decomposition\n", "abstract": " \u2022 Background\u2022 Related works\u2022 Approach\u2022 3-D FFT with 4-D formulation\u2022 Parallel 3-D FFT algorithm with 1.5-D decomposition", "num_citations": "2\n", "authors": ["498"]}
{"title": "Comparing archival policies for Blue Waters\n", "abstract": " This paper introduces two new tape archival policies that can improve tape archive performance in certain regimes, compared to the classical RAIT (Redundant Array of Independent Tapes) policy. The first policy, PARALLEL, still requires as many parallel tape drives as RAIT but pre-computes large data stripes that are written contiguously on tapes to increase write/read performance. The second policy, VERTICAL, writes contiguous data into a single tape, while updating error correcting information on the fly and delaying its archival until enough data has been archived. This second approach reduces the number of tape drives used for every user request to one. The performance of the three RAIT, PARALLEL and VE RTICAL policies is assessed through extensive simulations, using a hardware configuration and a distribution of I/O requests similar to these expected on the Blue Waters system. These simulations\u00a0\u2026", "num_citations": "2\n", "authors": ["498"]}
{"title": "Peer-to-peer grid technologies\n", "abstract": " The February 2010 Special Issue of Future Generation Computer Systems includes seven papers from researchers who have demonstrated the effectiveness and efficiency of a variety of theories, optimizations, architecture, applications and services models in different areas of P2P, grid and scalable computing. Marc Sanchez-Artigas and Pedro Garcia-Lopez proposed a P2P-based e-science grid, eSciGrid, towards scalable and efficient data sharing for data-intensive e-science applications. Yunhong Gu and Robert L. Grossman in their paper entitled'Sector: A High Performance Wide Area Community Data Storage and Sharing System'introduce a high performance distributed and Sharing System introduce a high performance distributed storage system that provides support for persistent data storage, data sharing, and distributed data analysis for communities connected by wide area high speed networks. Mustafizur Rahman, Rajiv Ranjan and Rajkumar Buyya in their paper entitled'Cooperative and Decentralized Workflow Scheduling in Global Grids' presented a decentralized and cooperative scheduling technique for workflow applications in Peer-to-Peer Grids.", "num_citations": "2\n", "authors": ["498"]}
{"title": "Desktop grids: From volunteer distributed computing to high throughput computing production platforms\n", "abstract": " Desktop Grids, literally Grids made of Desktop Computers, are very popular in the context of \u201cVolunteer Computing\u201d for large scale \u201cDistributed Computing\u201d projects like SETI@ home and Folding@ home. They are very appealing, as \u201cInternet Computing\u201d platforms for scientific projects seeking a huge amount of computational resources for massive high throughput computing, like the EGEE project in Europe. Companies are also interested of using cheap computing solutions that does not add extra hardware and cost of ownership. A very recent argument for Desktop Grids is their ecological impact: by scavenging unused CPU cycles without increasing excessively the power consumption, they reduce the waste of electricity. This book chapter presents the background of Desktop Grid, their principles and essential mechanisms, the evolution of their architectures, their applications and the research tools associated\u00a0\u2026", "num_citations": "2\n", "authors": ["498"]}
{"title": "Checkpointing vs. migration for post-petascale machines\n", "abstract": " We craft a few scenarios for the execution of sequential and parallel jobs on future generation machines. Checkpointing or migration, which technique to choose?", "num_citations": "2\n", "authors": ["498"]}
{"title": "Distributing and managing data on desktop grids with BitDew\n", "abstract": " Desktop Grids use the computing, network and storage resources from idle desktop PC's distributed over multiple-LAN's or the Internet to compute a large variety of resource-demanding distributed applications. While these applications need to access, compute, store and circulate large volumes of data, little attention has been paid to data management in such large-scale, dynamic, heterogeneous, volatile and highly distributed Grids.", "num_citations": "2\n", "authors": ["498"]}
{"title": "Veloc: Very low overhead checkpointing system\n", "abstract": " High performance computing (HPC) applications produce massive amounts of checkpointing data during their runtime, which is often used for defensive purposes, ie, to employ a checkpoint-restart resilience strategy in case of failures. In addition, productive uses of checkpointing are becoming increasingly popular to facilitate analytics (offline or in-situ) or computational models that need to revisit previous states (eg, adjoint computations). Checkpointing is a challenging I/O pattern, because the data coming from a large number of distributed processes typically needs to be coordinated to form a globally consistent checkpoint. This generates high write concurrency that overwhelms the I/O bandwidth of the system, leading to large performance overheads and poor scalability.In the quest to reach Exascale, many architectural trade-offs are necessary, including a high degree of parallelism and a growing gap between the compute capacity and I/O capabilities, which means less I/O bandwidth will be available per compute unit [1]. Therefore, checkpointing will be even more challenging: it needs to be performed more frequently (increasing failure rates) and it puts more pressure on the I/O bandwidth. To avoid this issue, many applications have switched from writing checkpoints directly to a parallel file system to more advanced approaches, such as multi-level checkpointing. The idea is to use the faster and less reliable local storage of the compute nodes (or that of neighboring nodes) to implement \u201clighter\u201d resilience levels that hold the checkpoints. This enables applications to survive a majority of failures, thereby decreasing the need to write the\u00a0\u2026", "num_citations": "2\n", "authors": ["498"]}
{"title": "Accelerating DNN architecture search at scale using selective weight transfer\n", "abstract": " Deep learning applications are rapidly gaining traction both in industry and scientific computing. Unsurprisingly, there has been significant interest in adopting deep learning at a very large scale on supercomputing infrastructures for a variety of scientific applications. A key issue in this context is how to find an appropriate model architecture that is suitable to solve the problem. We call this the neural architecture search (NAS) problem. Over time, many automated approaches have been proposed that can explore a large number of candidate models. However, this remains a time-consuming and resource expensive process: the candidates are often trained from scratch for a small number of epochs in order to obtain a set of top-K best performers, which are fully trained in a second phase. To address this problem, we propose a novel method that leverages checkpoints of previously discovered candidates to\u00a0\u2026", "num_citations": "1\n", "authors": ["498"]}
{"title": "Exploring Autoencoder-Based Error-Bounded Compression for Scientific Data\n", "abstract": " Error-bounded lossy compression is becoming an indispensable technique for the success of today's scientific projects with vast volumes of data produced during the simulations or instrument data acquisitions. Not only can it significantly reduce data size, but it also can control the compression errors based on user-specified error bounds. Autoencoder (AE) models have been widely used in image compression, but few AE-based compression approaches support error-bounding features, which are highly required by scientific applications. To address this issue, we explore using convolutional autoencoders to improve error-bounded lossy compression for scientific data, with the following three key contributions. (1) We provide an in-depth investigation of the characteristics of various autoencoder models and develop an error-bounded autoencoder-based framework in terms of the SZ model. (2) We optimize the compression quality for main stages in our designed AE-based error-bounded compression framework, fine-tuning the block sizes and latent sizes and also optimizing the compression efficiency of latent vectors. (3) We evaluate our proposed solution using five real-world scientific datasets and comparing them with six other related works. Experiments show that our solution exhibits a very competitive compression quality from among all the compressors in our tests. In absolute terms, it can obtain a much better compression quality (100% ~ 800% improvement in compression ratio with the same data distortion) compared with SZ2.1 and ZFP in cases with a high compression ratio.", "num_citations": "1\n", "authors": ["498"]}
{"title": "Demystifying asynchronous I/O Interference in HPC applications\n", "abstract": " With increasing complexity of HPC workflows, data management services need to perform expensive I/O operations asynchronously in the background, aiming to overlap the I/O with the application runtime. However, this may cause interference due to competition for resources: CPU, memory/network bandwidth. The advent of multi-core architectures has exacerbated this problem, as many I/O operations are issued concurrently, thereby competing not only with the application but also among themselves. Furthermore, the interference patterns can dynamically change as a response to variations in application behavior and I/O subsystems (e.g. multiple users sharing a parallel file system). Without a thorough understanding, I/O operations may perform suboptimally, potentially even worse than in the blocking case. To fill this gap, this paper investigates the causes and consequences of interference due to asynchronous\u00a0\u2026", "num_citations": "1\n", "authors": ["498"]}
{"title": "Intermediate-Scale Full State Quantum Circuit Simulation by Using Lossy Data Compression\n", "abstract": " ConclusionWe propose a lossy data compression strategy that could be used for quantum circuit simulation.", "num_citations": "1\n", "authors": ["498"]}
{"title": "Exploring best lossy compression strategy by combining SZ with spatiotemporal decimation\n", "abstract": " In today\u2019s extreme-scale scientific simulations, vast volumes of data are being produced such that the data cannot be accommodated by the parallel file system or the data writing/reading performance will be fairly low because of limited I/O bandwidth. In the past decade, many snapshot-based (or spacebased) lossy compressors have been developed, most of which rely on the smoothness of the data in space. However, the simulation data may get more and more complicated in space over time steps, such that the compression ratios decrease significantly. In this paper, we propose a novel, hybrid lossy compression method by leveraging spatiotemporal decimation under the SZ compression model. The contribution is twofold.(1) We explore several strategies of combining the decimation method with the SZ lossy compression model in both the space dimension and time dimension during the simulation.(2) We investigate the best-fit combined strategy upon different demands based on a couple of typical real-world simulations with multiple fields. Experiments show that either the space-based SZ or time-based SZ leads to the best rate distortion. Decimation methods have very high compression rate with low rate distortion though, and SZ combined with temporal decimation is a good tradeoff.", "num_citations": "1\n", "authors": ["498"]}
{"title": "Understanding and improving the trust in results of numerical simulations and scientific data analytics\n", "abstract": " With ever-increasing execution scale of parallel scientific simulations, potential unnoticed corruptions to scientific data during simulation make users more suspicious about the correctness of floating-point calculations than ever before. In this paper, we analyze the issue of the trust in results of numerical simulations and scientific data analytics. We first classify the corruptions into two categories, nonsystematic corruption and systematic corruption, and also discuss their origins. Then, we provide a formal definition of the trust in simulation and analytical results across multiple areas. We also discuss what kind of result accuracy would be expected from user\u2019s perspective and how to build trust by existing techniques. We finally identify the current gap and discuss two potential research directions based on existing techniques. We believe that this paper will be interesting to the researchers who are working on\u00a0\u2026", "num_citations": "1\n", "authors": ["498"]}
{"title": "Evaluation of the fir example using xilinx vivado high-level synthesis compiler\n", "abstract": " Compared to central processing units (CPUs) and graphics processing units (GPUs), field programmable gate arrays (FPGAs) have major advantages in reconfigurability and performance achieved per watt. This development flow has been augmented with high-level synthesis (HLS) flow that can convert programs written in a high-level programming language to Hardware Description Language (HDL). Using high-level programming languages such as C, C++, and OpenCL for FPGA-based development could allow software developers, who have little FPGA knowledge, to take advantage of the FPGA-based application acceleration. This improves developer productivity and makes the FPGA-based acceleration accessible to hardware and software developers. Xilinx Vivado HLS compiler is a high-level synthesis tool that enables C, C++ and System C specification to be directly targeted into Xilinx FPGAs without the need to create RTL manually. The white paper [1] published recently by Xilinx uses a finite impulse response (FIR) example to demonstrate the variable-precision features in the Vivado HLS compiler and the resource and power benefits of converting floating point to fixed point for a design. To get a better understanding of variable-precision features in terms of resource usage and performance, this report presents the experimental results of evaluating the FIR example using Vivado HLS 2017.1 and more\u00bb", "num_citations": "1\n", "authors": ["498"]}
{"title": "Evaluation of CHO Benchmarks on the Arria 10 FPGA using Intel FPGA SDK for OpenCL\n", "abstract": " The OpenCL standard is an open programming model for accelerating algorithms on heterogeneous computing system. OpenCL extends the C-based programming language for developing portable codes on different platforms such as CPU, Graphics processing units (GPUs), Digital Signal Processors (DSPs) and Field Programmable Gate Arrays (FPGAs). The Intel FPGA SDK for OpenCL is a suite of tools that allows developers to abstract away the complex FPGA-based development flow for a high-level software development flow. Users can focus on the design of hardware-accelerated kernel functions in OpenCL and then direct the tools to generate the low-level FPGA implementations. The approach makes the FPGA-based development more accessible to software users as the needs for hybrid computing using CPUs and FPGAs are increasing. It can also significantly reduce the hardware development time as users can evaluate different ideas with high-level language without deep FPGA domain knowledge. Benchmarking of OpenCL-based framework is an effective way for analyzing the performance of system by studying the execution of the benchmark applications. CHO is a suite of benchmark applications that provides support for OpenCL [1]. The authors presented CHO as an OpenCL port of the CHStone benchmark. Using Altera OpenCL (AOCL) compiler to synthesize the benchmark applications, they more\u00bb", "num_citations": "1\n", "authors": ["498"]}
{"title": "Re-form: FPGA-powered true codesign flow for high-performance computing in the post-Moore era\n", "abstract": " Multicore scaling will end soon because of practical power limits. Dark silicon is becoming a major issue even more than the end of Moore\u2019s law. In the post-Moore era, the energy efficiency of computing will be a major concern. FPGAs could be a key to maximizing the energy efficiency. In this paper we address severe challenges in the adoption of FPGA in HPC and describe \u201cRe-form,\u201d an FPGA-powered codesign flow.", "num_citations": "1\n", "authors": ["498"]}
{"title": "An information brokering service provider (ibsp) for virtual clusters\n", "abstract": " Virtual clusters spanning over resources belonging to different administration domains have to face with the computational resources brokering problem in order to allow the direct interaction among them. Typically, it requires some component collecting, elaborating and providing resources information. Currently, every virtual cluster project provides its own solution with a low degree of portability and isolation from the rest of the platform components. We propose the Information Brokering Service Provider (IBSP), a general approach which wants to represent an uniform solution adaptable to a wide set of existing platforms. Its pluggable and flexible design is based on a decentralized architecture where each of the cooperating nodes is composed by a resource interface exposing the brokering service, and a portion of a distributed information system. In this paper, we present the motivations, principles and\u00a0\u2026", "num_citations": "1\n", "authors": ["498"]}
{"title": "Handbook of Research on Scalable Computing Technologies\n", "abstract": " The past decade has witnessed a fruitful proliferation of increasingly high performance scalable computing systems mainly due to the availability of enabling technologies in hardware, software, and networks. The Handbook of Research on Scalable Computing Technologies presents ideas, results, and experiences in significant advancements and future challenges of enabling technologies. A defining body of research on topics such as service-oriented computing, data-intensive computing, and cluster and grid computing, this Handbook of Research contains valuable findings for those involved with developing programming tools and environments in computing as well as those in related upper-level undergraduate and graduate courses.", "num_citations": "1\n", "authors": ["498"]}
{"title": "Special issue on global and peer-to-peer computing\n", "abstract": " Grid and P2P systems are different instances of distributed systems: while Grids are often motivated by intensive computations and required access to huge amounts of distributed data in a secure and trusted environment with quality of service guarantees, P2P systems are driven by large-scale dynamic collaborations in highly untrusted and unreliable environments. Since 2001, the Global and Peer to Peer Computing Workshop has been held annually along with the IEEE Cluster Computing and the Grid (CCGRID) Conference to capture the fuzzy border between these areas and blur it even more, showing a much welcome exchange and enrichment of research ideas. The workshop popularity has increased continuously, making it one of the most successful workshops held at the CCGRID conference. This special issue includes extensions of the best papers of the 2005 workshop issue along with papers\u00a0\u2026", "num_citations": "1\n", "authors": ["498"]}
{"title": "Des systemes client-serveur aux systemes pair \u00e0 pair\n", "abstract": " Inria - Des systemes client-serveur aux systemes pair \u00e0 pair Acc\u00e9der directement au contenu Acc\u00e9der directement \u00e0 la navigation Toggle navigation CCSD HAL HAL HALSHS TEL M\u00e9diHAL Liste des portails AUR\u00e9HAL API Data Documentation Episciences.org Episciences.org Revues Documentation Sciencesconf.org Support HAL-Inria Les publications, logiciels... des scientifiques Inria Accueil D\u00e9poser Consulter tout HAL par date de publication/r\u00e9daction par domaine par type de publication par collection arXiv les derniers d\u00e9p\u00f4ts Publications Inria Recherche Services HalTools : cr\u00e9er sa page web Haltools : export RAWEB X2Hal : import par lot Consulter les structures de recherche connues de HAL Documentation Aide en ligne de HAL V3 Derni\u00e8res \u00e9volutions de HAL V3 Documentation API HAL Ajouter des vignettes Aide en ligne Haltools Aide en ligne de X2hal OpenAccess Inria soutient la science ouverte -, 1 d\u2026", "num_citations": "1\n", "authors": ["498"]}
{"title": "A Case for Efficient Execution of Data-Intense Applications with BitTorrent on Computational Desktop Grid\n", "abstract": " Data-centric applications are still a challenging issue for large scale distributed computing systems. The emergence of new protocols and software for collaborative content distribution over Internet offers a new opportunity for efficient and fast delivery of high volume of data. In this paper, we investigate BitTorrent as a protocol for data diffusion in the context of Computational Desktop Grid. We show that BitTorrent is efficient for large file transfers, scalable when the number of nodes increases but suffers from a high overhead when transmitting small files. This paper also investigates two approach to overcome these limitations. First, we propose a performance model to select the best of FTP and BitTorrent protocols according to the size of the file to distribute and the number of receiver nodes. Next we propose enhancement of the BitTorrent protocol which provides more predictable communication patterns. We design\u00a0\u2026", "num_citations": "1\n", "authors": ["498"]}
{"title": "Calcul global, desktop grids et xtremweb\n", "abstract": " Calcul Global, Desktop Grids et XtremWeb Outline Page 1 Ecole GridUse 1 23 June 2004 \u03b3\u03bbGrand Large Calcul Global, Desktop Grids et XtremWeb Franck Cappello PCRI / INRIA Grand-Large LRI, Universit\u00e9 Paris sud. fci@lri.fr www.lri.fr/~fci Grand Large PCRI / INRIA \u03b3\u03bb Ecole GridUse 2 23 June 2004 \u03b3\u03bbGrand Large Outline \u2022 Introduction : Desktop Grid, foundations for a Large Scale Operating System. \u2022 Some architecture issues \u2022 User interfaces (XtremWeb) \u2022 Fault tolerance (XtremWeb) \u2022 Security (Trust) \u2022 Final Remarks (what we have learned so far) Page 2 Ecole GridUse 3 23 June 2004 \u03b3\u03bbGrand Large Several types of GRID 2 kinds of distributed systems Computing \u00ab GRID \u00bb \u00ab Desktop GRID \u00bb or \u00ab Internet Computing \u00bb Peer-to-Peer systems Large scale distributed systems Large sites Computing centers, Clusters PC Windows, Linux \u2022<100 \u2022Stables \u2022Individual credential \u2022Confidence \u2022~100 000 \u2022Volatiles \u2022No \u2022No : \u2022 \u2026", "num_citations": "1\n", "authors": ["498"]}
{"title": "Metacomputing: vers une nouvelle dimension pour le calcul haute performance\n", "abstract": " UMR8623| CNRS| UNIV-RENNES1| LIP| ENS-LYON| IRISA| SMS| UNIV-TLSE2| INRIA| ENS-CACHAN| UR1-UFR-ISTIC| UNIV-LYON1| UNIV-RENNES| UNIV-LILLE| INSA-GROUPE| LIG| UNIV-TLSE3| POLYTECH-GRENOBLE| UT1-CAPITOLE| UDL| IRIT| ARTS-ET-METIERS-SCIENCES-ET-TECHNOLOGIES| L2EP| IRIT-SEPIA| INSA-RENNES| UGA", "num_citations": "1\n", "authors": ["498"]}
{"title": "Performance Evaluation of Two Programming Models for a Cluster of PC Biprocessors.\n", "abstract": " We evaluate performance on NAS benchmarks of a cluster of 2-way SMP PCs connected by a Myrinet network. Two programming models are considered. A Single Memory Model uses the MPI-BIP/CLUMP library and gives the programmer a uni ed memory view. A Hybrid Memory Model combines shared memory model within nodes and message passing between nodes. We compare 2-way SMP con gurations speed-up versus single CPU con gurations for each model. Better model depends on the features of the applications. We then show that HMM model gives performance close to performance of scalable high-end multicomputers for 8-node con gurations. The overall performance evaluation shows that 2-way SMP nodes are a cost-e ective solution for clusters of PCs.", "num_citations": "1\n", "authors": ["498"]}
{"title": "The static network: a high performance reconfigurable communication network\n", "abstract": " This paper presents a new interconnection network for a massively parallel architecture based on the static model of communication. The principle of the model is to schedule at run time, on a circuit switching hardware, the off-line routed circuits computed at compile time. Designing a network especially for the compiled model can take full benefit of the current interconnection technology. The static network is intended to a synchronised MIMD distributed memory machine, based on high performance processing nodes including microprocessors. The application scope is numerical computation, where all communication patterns may occur. The hardware issues are the various synchronisations inside the network, which need a sophisticated switch design to keep pace with 1.33 Gbit/s links. The network is designed to reach a sustained aggregate bandwidth of more than 500GBytes/s, and provide an overall latency\u00a0\u2026", "num_citations": "1\n", "authors": ["498"]}
{"title": "PTAH: etude d'une architecture massivement parallele a ressources equilibrees et communications compilees\n", "abstract": " Les etudes realisees dans cette these ont pour objet de definir l'architecture d'un calculateur massivement parallele original, de verifier sa faisabilite et d'etudier ses performances. L'objectif est d'obtenir une architecture extensible comme les architectures massivement paralleles et dont l'efficacite reste proche de celle des calculateurs vectoriels. Apres avoir etudie les parametres de l'extensibilite et de l'efficacite, nous proposons une architecture massivement parallele a memoire distribuee dont les ressources sont equilibrees en performance. L'equilibrage des performances du cpu, de la memoire et du reseau consiste a augmenter les performances du reseau de 2 ordres de grandeurs. Il est possible d'obtenir un tel gain en combinant l'utilisation de liaisons series a haut debit et celle d'un nouveau modele de communication: la compilation des communications. La faisabilite de cette approche repose sur plusieurs points. D'abord, la majorite des references aux donnees dans les applications numeriques doivent etre compilables. Ceci est verifie par une analyse statistique qui rapporte que plus de 80% des references d'un ensemble representatif d'applications sont connues des la compilation. Ensuite, il faut determiner les contraintes architecturales. Notre etude indique que l'architecture doit etre fortement synchrone, que la memoire ne peut etre hierarchisee et que le processeur elementaire est de type vliw. Enfin, il faut verifier que la technologie actuelle permet d'obtenir les performances requises. Nous etudions donc en detail la structure des elements determinants du reseau. Les performances obtenues par simulation montrent que l\u00a0\u2026", "num_citations": "1\n", "authors": ["498"]}
{"title": "Hardware features of the static communication network of a parallel architecture\n", "abstract": " We present the hardware features of a communication network which has been designed for compiled communication schemes. It is based on a high speed serial link crossbar switch and a Clos-like interconnection network.", "num_citations": "1\n", "authors": ["498"]}
{"title": "Design of the processing node of the PTAH 64 parallel computer\n", "abstract": " This paper presents the design of the Processing Node (PN) for an implementation of the PTAH architectural model with 64 PNs. The PTAH architecture is presented. The requirements of a 64 PNs implementation are derived. The design tradeoff for the global Processing Node architecture, the control system, the Floating Point and Integer Unit and the data memory is discussed. Finaly the instruction formats are presented.", "num_citations": "1\n", "authors": ["498"]}