{"title": "A QoS broker based architecture for efficient web services selection\n", "abstract": " Quality of service (QoS) support in Web services plays a great role for the success of this emerging technology. In this paper, we present a QoS broker-based architecture for Web services. The main goal of the architecture is to support the client in selecting Web services based on his/her required QoS. To achieve this goal, we propose a two-phase verification technique that is performed by a third party broker. The first phase consists of syntactic and semantic verification of the service interface description including the QoS parameters description. The second phase consists of applying a measurement technique to compute the QoS metrics stated in the service interface and compares their values with the claimed one. This is used to verify the conformity of a Web service from the QoS point of view (QoS testing). A methodological approach to generate QoS test cases, as input to QoS verification is used. We have\u00a0\u2026", "num_citations": "263\n", "authors": ["545"]}
{"title": "Visualization-based analysis of quality for large-scale software systems\n", "abstract": " We propose an approach for complex software analysis based on visualization. Our work is motivated by the fact that in spite of years of research and practice, software development and maintenance are still time and resource consuming, and high-risk activities. The most important reason in our opinion is the complexity of many phenomena related to software, such as its evolution and its reliability. In fact, there is very little theory explaining them. Today, we have a unique opportunity to empirically study these phenomena, thanks to large sets of software data available through open-source programs and open repositories. Automatic analysis techniques, such as statistics and machine learning, are usually limited when studying phenomena with unknown or poorly-understood influence factors. We claim that hybrid techniques that combine automatic analysis with human expertise through visualization are excellent\u00a0\u2026", "num_citations": "226\n", "authors": ["545"]}
{"title": "Can metrics help bridging the gap between the improvement of OO design quality and its automation\n", "abstract": " During the evolution of object-oriented systems, the preservation of correct design should be a permanent quest. However, for systems involving a large number of classes and subject to frequent modifications, detection and correction of design flaws may be a complex and resource-consuming task. The use of automatic detection and correction tools can be helpful for this task. Various work propose transformations that improve the quality of an object-oriented system while preserving its behavior. In this paper we propose to investigate whether some objectoriented metrics can be used as indicators for automatically detecting situations where a particular transformation can be applied to improve the quality of a system. The detection process is based on analyzing the impact of various transformations on these object-oriented metrics using quality estimation models.", "num_citations": "138\n", "authors": ["545"]}
{"title": "Visual detection of design anomalies\n", "abstract": " Design anomalies, introduced during software evolution, are frequent causes of low maintainability and low flexibility to future changes. Because of the required knowledge, an important subset of design anomalies is difficult to detect automatically, and therefore, the code of anomaly candidates must be inspected manually to validate them. However, this task is time- and resource-consuming. We propose a visualization-based approach to detect design anomalies for cases where the detection effort already includes the validation of candidates. We introduce a general detection strategy that we apply to three types of design anomaly. These strategies are illustrated on concrete examples. Finally we evaluate our approach through a case study. It shows that performance variability against manual detection is reduced and that our semi-automatic detection has good recall for some anomaly types.", "num_citations": "112\n", "authors": ["545"]}
{"title": "Formal verification of web applications modeled by communicating automata\n", "abstract": " In this paper, we present an approach for modeling an existing web application using communicating finite automata model based on the user-defined properties to be validated. We elaborate a method for automatic generation of such a model from a recorded browsing session. The obtained model could then be used to verify properties with a model checker, as well as for regression testing and documentation. Unlike previous attempts, our approach is oriented towards complex multi-window/frame applications. We present an implementation of the approach that uses the model checker Spin and provide an example.", "num_citations": "79\n", "authors": ["545"]}
{"title": "Applying concept formation methods to object identification in procedural code\n", "abstract": " Legacy software systems present a high level of entropy combined with imprecise documentation. This makes their maintenance more difficult, more time consuming, and costlier. In order to address these issues, many organizations have been migrating their legacy systems to new technologies. In this paper, we describe a computer-supported approach aimed at supporting the migration of procedural software systems to the object-oriented (OO) technology, which supposedly fosters reusability, expandability, flexibility, encapsulation, information hiding, modularity, and maintainability. Our approach relies heavily on the automatic formation of concepts based on information extracted directly from code to identify objects. The approach tends, thus, to minimize the need for domain application experts. We also propose rules for the identification of OO methods from routines. A well known and self-contained example is\u00a0\u2026", "num_citations": "72\n", "authors": ["545"]}
{"title": "Simulated annealing for improving software quality prediction\n", "abstract": " In this paper, we propose an approach for the combination and adaptation of software quality predictive models. Quality models are decomposed into sets of expertise. The approach can be seen as a search for a valuable set of expertise that when combined form a model with an optimal predictive accuracy. Since, in general, there will be several experts available and each expert will provide his expertise, the problem can be reformulated as an optimization and search problem in a large space of solutions. We present how the general problem of combining quality experts, modeled as Bayesian classifiers, can be tackled via a simulated annealing algorithm customization. The general approach was applied to build an expert predicting object-oriented software stability, a facet of software quality. Our findings demonstrate that, on available data, composed expert predictive accuracy outperforms the best available\u00a0\u2026", "num_citations": "60\n", "authors": ["545"]}
{"title": "Reusability hypothesis verification using machine learning techniques: a case study\n", "abstract": " Since the emergence of object technology, organizations have accumulated a tremendous amount of object-oriented (OO) code. Instead of continuing to recreate components that are similar to existing artifacts, and considering the rising costs of development, many organizations would like to decrease software development costs and cycle time by reusing existing OO components. This paper proposes an experiment to verify three hypotheses about the impact of three internal characteristics (inheritance, coupling and complexity) of OO applications on reusability. This verification is done through a machine learning approach (the C4.5 algorithm and a windowing technique). Two kinds of results are produced: (1) for each hypothesis (characteristic), a predictive model is built using a set of metrics derived from this characteristic; and (2) for each predictive model, we measure its completeness, correctness and global\u00a0\u2026", "num_citations": "59\n", "authors": ["545"]}
{"title": "An analogy-based approach for predicting design stability of Java classes\n", "abstract": " Predicting stability in object-oriented (OO) software, i.e., the ease with which a software item evolves while preserving its design, is a key feature for software maintenance. In fact, a well designed OO software must be able to evolve without violating the compatibility among versions, provided that no major requirement reshuffling occurs. Stability, like most quality factors, is a complex phenomenon and its prediction is a real challenge. We present an approach, which relies on the case-based reasoning (CBR) paradigm and thus overcomes the handicap of insufficient theoretical knowledge on stability. The approach explores structural similarities between classes, expressed as software metrics, to guess their chances of becoming unstable. In addition, our stability model binds its value to the impact of changing requirements, i.e., the degree of class responsibilities increase between versions, quantified as the stress\u00a0\u2026", "num_citations": "55\n", "authors": ["545"]}
{"title": "Exploring the evolution of software quality with animated visualization\n", "abstract": " Assessing software quality and understanding how events in its evolution have lead to anomalies are two important steps toward reducing costs in software maintenance. Unfortunately, evaluation of large quantities of code over several versions is a task too time-consuming, if not overwhelming, to be applicable in general. To address this problem, we designed a visualization framework as a semi-automatic approach to quickly investigate programs composed of thousands of classes, over dozens of versions. Programs and their associated quality characteristics for each version are graphically represented and displayed independently. Real-time navigation and animation between these representations recreate visual coherences often associated with coherences intrinsic to subsequent software versions. Exploiting such coherences can reduce cognitive gaps between the different views of software, and allows\u00a0\u2026", "num_citations": "53\n", "authors": ["545"]}
{"title": "Analyzing change impact in object-oriented systems\n", "abstract": " The development of software products consumes a lot of time and resources. On the other hand, these development costs are lower than maintenance costs, which represent a major concern, specially, for systems designed with recent technologies. Systems modification should be taken rigorously, and change effects must be considered. In this paper, we propose an approach, both analytical and experimental; its objective is to analyze and predict changes impacts in object-oriented (OO) systems. The method we follow consists first, to choose an existing impact model, and adapt it afterward. An impact calculation technique based on a meta-model is developed. To evaluate our approach, an empirical study was led on a real system in which a correlation hypothesis between coupling and change impact was advanced. A concrete change was done in the target system and coupling metrics were extracted from it\u00a0\u2026", "num_citations": "47\n", "authors": ["545"]}
{"title": "A Metamodeling technique: The METAGEN system\n", "abstract": " We propose a technique for bridging the gap between the user's and the implementor's points of view in designing software. This technique relies heavily on OO technology and on rule based programming. It borrows part of its inspiration from the AI subfield of Knowledge Acquisition. This technique is implemented as a system now called M\u00c9TAGEN, written in Smalltalk-80. Several applications of M\u00c9TAGEN have been used to model information systems in public administration, to validate conceptual models by simulation for insurance companies, and to specify a risk management system. We are now customizing M\u00c9TAGEN in two directions: as a tool for implementing semantic interfaces for OO DBMS, and as a user support tool for the proper use of OO Application Frameworks. The present paper has been written on the basis of a position paper presented at the OOPSLA\u201994 workshop on AI and OO Software Engineering.", "num_citations": "43\n", "authors": ["545"]}
{"title": "Predicting software stability using case-based reasoning\n", "abstract": " Predicting stability in object-oriented (OO) software, i.e., the ease with which a software item can evolve while preserving its design, is a key feature for software maintenance. We present a novel approach which relies on the case-based reasoning (CBR) paradigm. Thus, to predict the chances of an OO software item breaking downward compatibility, our method uses knowledge of past evolution extracted from different software versions. A comparison of our similarity-based approach to a classical inductive method such as decision trees, is presented which includes various tests on large datasets from existing software.", "num_citations": "42\n", "authors": ["545"]}
{"title": "A concept formation based approach to object identification in procedural code\n", "abstract": " Legacy software systems present a high level of entropy combined with imprecise documentation. This makes their maintenance more difficult, more time consuming, and costlier. In order to address these issues, many organizations have been migrating their legacy systems to emerging technologies. In this paper, we describe a computer-supported approach aimed at supporting the migration of procedural software systems to the object-oriented (OO) technology. Our approach is based on the automatic formation of concepts, and uses information extracted directly from code to identify objects. The approach tends, thus, to minimize the need for domain application experts.", "num_citations": "42\n", "authors": ["545"]}
{"title": "Genetic-programming approach to learn model transformation rules from examples\n", "abstract": " We propose a genetic programming-based approach to automatically learn model transformation rules from prior transformation pairs of source-target models used as examples. Unlike current approaches, ours does not need fine-grained transformation traces to produce many-to-many rules. This makes it applicable to a wider spectrum of transformation problems. Since the learned rules are produced directly in an actual transformation language, they can be easily tested, improved and reused. The proposed approach was successfully evaluated on well-known transformation problems that highlight three modeling aspects: structure, time constraints, and nesting.", "num_citations": "40\n", "authors": ["545"]}
{"title": "Systematic mapping study of template-based code generation\n", "abstract": " Context: Template-based code generation (TBCG) is a synthesis technique that produces code from high-level specifications, called templates. TBCG is a popular technique in model-driven engineering (MDE) given that they both emphasize abstraction and automation. Given the diversity of tools and approaches, it is necessary to classify existing TBCG techniques to better guide developers in their choices.Objective: The goal of this article is to better understand the characteristics of TBCG techniques and associated tools, identify research trends, and assess the importance of the role of MDE in this code synthesis approach.Method: We survey the literature to paint an interesting picture about the trends and uses of TBCG in research. To this end, we follow a systematic mapping study process.Results: Our study shows, among other observations, that the research community has been diversely using TBCG over the\u00a0\u2026", "num_citations": "38\n", "authors": ["545"]}
{"title": "How good is your comment? a study of comments in java programs\n", "abstract": " Comments are very useful to developers during maintenance tasks and are useful as well to help structuring a code at development time. They convey useful information about the system functionalities as well as the state of mind of a developer. Comments in code have been the focus of several studies, but none of them was targeted at analyzing commenting habits precisely. In this paper, we present an empirical study which analyzes existing comments in different open source Java projects. We study comments from both a quantitative and a qualitative point of view. We propose a taxonomy of comments that we used for conducting our analysis.", "num_citations": "37\n", "authors": ["545"]}
{"title": "From object-oriented applications to component-oriented applications via component-oriented architecture\n", "abstract": " Object-oriented applications of significant size are often complex and therefore, costly to maintain. Indeed, they rely on the concept of class which has low granularity with many implicit dependencies not always explicit. The component paradigm provides a projection space well-structured and of highest level for a better understanding through abstract architectural views. But it is possible to go further. It may also be the ultimate target of a complete process of re engineering. The end-to-end automation of this process is a subject on which literature has made very little attention. In this paper, we propose such a method to automatically transform an object-oriented application in an operational component-oriented application. We illustrate this method on a real Java application which is transformed in an operational OSGi application.", "num_citations": "37\n", "authors": ["545"]}
{"title": "An observational study on api usage constraints and their documentation\n", "abstract": " Nowadays, APIs represent the most common reuse form when developing software. However, the reuse benefits depend greatly on the ability of client application developers to use correctly the APIs. In this paper, we present an observational study on the API usage constraints and their documentation. To conduct the study on a large number of APIs, we implemented and validated strategies to automatically detect four types of usage constraints in existing APIs. We observed that some of the constraint types are frequent and that for three types, they are not documented in general. Surprisingly, the absence of documentation is, in general, specific to the constraints and not due to the non documenting habits of developers.", "num_citations": "36\n", "authors": ["545"]}
{"title": "Validation of software visualization tools: A systematic mapping study\n", "abstract": " Software visualization as a research field focuses on the visualization of the structure, behavior, and evolution of software. It studies techniques and methods for graphically representing these different aspects of software. Interest in software visualization has grown in recent years, producing rapid advances in the diversity of research and in the scope of proposed techniques, and aiding the application experts who use these techniques to advance their own research. Despite the importance of evaluating software visualization research, there is little work studying validation methods. As a consequence, it is usually difficult producing compelling evidence about the effectiveness of software visualization contributions. The goal of this paper is to study the validation techniques performed in the software visualization literature. We conducted a systematic mapping study of validation methods in software visualization. We\u00a0\u2026", "num_citations": "36\n", "authors": ["545"]}
{"title": "Coping with legacy system migration complexity\n", "abstract": " During the last three decades, a considerable amount of software has been developed based on obsolete technologies (such as using procedural languages). This type of systems has undergone severe code revisions during a long time period. As a consequence, the high level of entropy combined with imprecise documentation about the design and architecture make the maintenance more difficult, time consuming, and costly. On the other hand, these systems have important economical value; many of them are crucial to their owners (Bennett, 1995). For the high cost of lost former investment and business knowledge that embedded in those systems, in many cases, simply abandon legacy systems and re-develop new systems based on new technology is not the choice. Migrating legacy system toward new emerging technology is an appropriate solution. However, migrating legacy system towards new\u00a0\u2026", "num_citations": "33\n", "authors": ["545"]}
{"title": "Object-relational database metrics\n", "abstract": " Les bases de donn\u00e9es objet-relationnelles substitueront les syst\u00e8mes apparent\u00e9s pour devenir la prochaine grande vague des bases de donn\u00e9es parce qu'elles combinent des caract\u00e9ristiques traditionnelles de base de donn\u00e9es avec le principe orient\u00e9 a l\u2019objet. Ainsi, il est fondamental proposer des m\u00e9triques pour controller la qualit\u00e9 de ce genre de bases de donn\u00e9es. Mais la d\u00e9finition de m\u00e9triques doit \u00eatre faite d'une voie m\u00e9thodologique, il est n\u00e9cessaire suivre un certain nombre d'\u00e9tapes pour assurer la fiabilit\u00e9 de les m\u00e9triques propos\u00e9es. Dans ce travail, nous pr\u00e9sentons la m\u00e9thode que nous utilisons pour la proposition des m\u00e9triques (qui se compose par d\u00e9finition de m\u00e9trique, validation formelle et validation empirique) et comment nous l'avons utilis\u00e9e pour obtenir des m\u00e9triques pour les bases de donn\u00e9es objet-relationnelles.", "num_citations": "31\n", "authors": ["545"]}
{"title": "Modeling web quality using a probabilistic approach: An empirical validation\n", "abstract": " Web-based applications are software systems that continuously evolve to meet users' needs and to adapt to new technologies. Assuring their quality is then a difficult, but essential task. In fact, a large number of factors can affect their quality. Considering these factors and their interaction involves managing uncertainty and subjectivity inherent to this kind of applications. In this article, we present a probabilistic approach for building Web quality models and the associated assessment method. The proposed approach is based on Bayesian Networks. A model is built following a four-step process consisting in collecting quality characteristics, refining them, building a model structure, and deriving the model parameters. The feasibility of the approach is illustrated on the important quality characteristic of Navigability design. To validate the produced model, we conducted an experimental study with 20 subjects and 40\u00a0\u2026", "num_citations": "30\n", "authors": ["545"]}
{"title": "Towards a multidimensional model for web-based applications quality assessment\n", "abstract": " Several approaches for quantitative evaluation of Web-based applications have proposed test-benches, checklists and tools. In this paper, we propose an evaluation approach that integrates and extends the various approaches proposed in this field. We have developed a list of over than 300 criteria and sub-criteria, which we classified using the ISO/IEC 9126 standard as a guide. These criteria represent a first basis for the development of a hierarchical quality model specific to Web-Based Applications. In order to refine our quality model, we followed a systematic approach using the GQM paradigm. A tool supporting our assessment approach has been developed. With this tool, we conducted an experience on several Web systems. The obtained results were rather encouraging. However, they have shown that more dimensions have to be considered to increase the reliability of our model. As a next step\u00a0\u2026", "num_citations": "30\n", "authors": ["545"]}
{"title": "Improving rule set based software quality prediction: A genetic algorithm-based approach\n", "abstract": " The object-oriented (OO) paradigm has now reached maturity. OO software products are becoming more complex which makes their evolution effort and time consuming. In this respect, it has become important to develop tools that allow assessing the stability of OO software (ie, the ease with which a software item can evolve while preserving its design). In general, predicting the quality of OO software is a complex task. Although many predictive models are proposed in the literature, we remain far from having reliable tools that can be applied to real industrial systems. The main obstacle for building reliable predictive tools for real industrial systems is the lackof representative samples. Unlike other domains where such samples can be drawn from available large repositories of data, in OO software the lack of such repositories makes it hard to generalize, to validate and to reuse existing models. Since universal models do not exist, selecting an appropriate quality model is a difficult, non-trivial decision for a company. In this paper, we propose two general approaches to solve this problem. They consist of combining/adapting a set of existing models. The process is driven by the context of the target company. These approaches are applied to OO software stability prediction.", "num_citations": "30\n", "authors": ["545"]}
{"title": "Predicting change impact in object-oriented applications with bayesian networks\n", "abstract": " This study has to be considered as another step towards the proposal of assessment/predictive models in software quality. We consider in this work, that a probabilistic model using Bayesian nets constitutes an interesting alternative to non-probabilistic models suggested in the literature. Thus, we propose in this paper a probabilistic approach using Bayesian networks to analyze and predict change impact in object-oriented systems. An impact model is built and probabilities are assigned to network nodes. Data obtained from a real system are exploited to empirically study causality hypotheses between some software internal attributes and change impact. Several scenarios are executed on the network, and the obtained results confirm that coupling is a good indicator of change impact.", "num_citations": "29\n", "authors": ["545"]}
{"title": "Combining and adapting software quality predictive models by genetic algorithms\n", "abstract": " The goal of quality models is to predict a quality factor starting from a set of direct measures. Selecting an appropriate quality model for a particular software is a difficult, non-trivial decision. In this paper, we propose an approach to combine and/or adapt existing models (experts) in such way that the combined/adapted model works well on the particular system. Test results indicate that the models perform significantly better than individual experts in the pool.", "num_citations": "29\n", "authors": ["545"]}
{"title": "Multi-step learning and adaptive search for learning complex model transformations from examples\n", "abstract": " Model-driven engineering promotes models as main development artifacts. As several models may be manipulated during the software-development life cycle, model transformations ensure their consistency by automating model generation and update tasks. However, writing model transformations requires much knowledge and effort that detract from their benefits. To address this issue, Model Transformation by Example (MTBE) aims to learn transformation programs from source and target model pairs supplied as examples. In this article, we tackle the fundamental issues that prevent the existing MTBE approaches from efficiently solving the problem of learning model transformations. We show that, when considering complex transformations, the search space is too large to be explored by naive search techniques. We propose an MTBE process to learn complex model transformations by considering three\u00a0\u2026", "num_citations": "28\n", "authors": ["545"]}
{"title": "Extracting sequence diagrams from execution traces using interactive visualization\n", "abstract": " We present a semi-automated approach for the reverse engineering of UML sequence diagrams. Our approach starts with a set of execution traces that are automatically aligned in order to determine the common behavior. Sequence diagrams are then extracted with an interactive visualization, which allows navigating into execution traces and performing extraction operations. We provide a concrete illustration of our approach with a case study, and show in particular that the resulting diagrams are more meaningful and more compact than those extracted by automated approaches.", "num_citations": "27\n", "authors": ["545"]}
{"title": "Restructuring object-oriented applications into component-oriented applications by using consistency with execution traces\n", "abstract": " Software systems should evolve in order to respond to changing client requirements and their evolving environments. But unfortunately, the evolution of legacy applications generates an exorbitant cost. In this paper, we propose an approach to restructure legacy object-oriented applications into component-based applications. Our approach is based on dynamic dependencies between classes to identify potential components. In this way, the composition is dictated by the context of the application to improve its evolvability. We validate our approach through the study of three legacy Java applications.", "num_citations": "27\n", "authors": ["545"]}
{"title": "A metric extraction framework based on a high-level description language\n", "abstract": " Nowadays, many tools are available for metric extraction. However, extending these tools with new metrics or modifying the calculation of existing ones is often difficult, sometimes impossible. Indeed, many of them are black box tools. Others can be extended only by modifying third-party code. Moreover, metric specifications often lack precision, which leads to implementations that do not correspond necessarily to userspsila expectations. In this paper, we propose a flexible approach for metric collection based on a metric description language that allows manipulating basic data extracted from the code. These data are mapped to a generic object-oriented meta-model that is language agnostic. This makes it easy to focus on the metric specification rather than language specific constructs. Metric specifications are interpreted automatically to extract their corresponding values for a target program.", "num_citations": "27\n", "authors": ["545"]}
{"title": "Combining software quality predictive models: An evolutionary approach\n", "abstract": " During the last ten years, a large number of quality models have been proposed in the literature. In general, the goal of these models is to predict a quality factor starting from a set of direct measures. The lack of data behind these models makes it hard to generalize, cross-validate, and reuse existing models. As a consequence, for a company, selecting an appropriate quality model is a difficult, non-trivial decision. In this paper, we propose a general approach and a particular solution to this problem. The main idea is to combine and adapt existing models (experts) in such a way that the combined model works well on the particular system or in the particular type of organization. In our particular solution, the experts are assumed to be decision tree or rule-based classifiers and the combination is done by a genetic algorithm. The result is a white-box model: for each software component, not only does the model give a\u00a0\u2026", "num_citations": "26\n", "authors": ["545"]}
{"title": "Recovering model transformation traces using multi-objective optimization\n", "abstract": " Model Driven Engineering (MDE) is based on a large set of models that are used and manipulated throughout the development cycle. These models are manually or automatically produced and/or exploited using model transformations. To allow engineers to maintain the models and track their changes, recovering transformation traces is essential. In this paper, we propose an automated approach, based on multi-objective optimization, to recover transformation traces between models. Our approach takes as input a source model in the form of a set of fragments (fragments are defined using the source meta-model cardinalities and OCL constraints), and a target model. The recovered transformation traces take the form of many-to-many mappings between the constructs of the two models.", "num_citations": "25\n", "authors": ["545"]}
{"title": "Visualizing software dynamicities with heat maps\n", "abstract": " Interactive software visualization offers a promising support for program comprehension, including program dynamicity. We present, the extension of an existing visualization tool with heat maps to explore the time and other dimensions of software. To this end, we first propose a framework to unify the two main software dynamicities, execution and evolution. Then, this unified framework is exploited to define a visualization environment based on heat maps. We illustrate our approach on two comprehension tasks: understanding the behavior of programmers during the evolution of an application and understanding class contributions in use cases. The case studies show that the heat-map metaphor contributes to answer, more easily, many of the questions important to program comprehension.", "num_citations": "25\n", "authors": ["545"]}
{"title": "Deriving coupling metrics from call graphs\n", "abstract": " Coupling metrics play an important role in empirical software engineering research as well as in industrial measurement programs. The existing coupling metrics have usually been defined in a way that they can be computed from a static analysis of the source code. However, modern programs extensively use dynamic language features such as polymorphism and dynamic class loading that are difficult to capture by static analysis. Consequently, the derived metric values might not accurately reflect the state of a program. In this paper, we express existing definitions of coupling metrics using call graphs. We then compare the results of four different call graph construction algorithms with standard tool implementations of these metrics in an empirical study. Our results show important variations in coupling between standard and call graph-based calculations due to the support of dynamic features.", "num_citations": "24\n", "authors": ["545"]}
{"title": "Building quality estimation models with fuzzy threshold values\n", "abstract": " Ce travail pr\u00e9sente une approche pour r\u00e9soudre un des principaux probl\u00e8mes reli\u00e9s aux techniques de construction et d\u2019application des mod\u00e8les d'estimation de la qualit\u00e9 de logiciel, \u00e0 savoir l'utilisation des valeurs-seuils pr\u00e9cises pour les m\u00e9triques. Nous avons utilis\u00e9 une approche bas\u00e9e sur la logique floue pour \u00e9tudier la stabilit\u00e9 de l\u2019interface des biblioth\u00e8ques de classes. Nous nous sommes bas\u00e9s sur les m\u00e9triques structurelles comme indicateurs de stabilit\u00e9. Pour \u00e9valuer cette nouvelle approche, nous avons entrepris une \u00e9tude sur trois versions d'une biblioth\u00e8que de classes commerciale \u00e9crite en C++. Les r\u00e9sultats obtenus sont tr\u00e8s prometteurs comparativement \u00e0 ceux de deux approches classiques d\u2019apprentissage, TDIDT et classificateurs bay\u00e9siens.", "num_citations": "23\n", "authors": ["545"]}
{"title": "Learning implicit and explicit control in model transformations by example\n", "abstract": " We propose an evolutionary approach that, in addition to learn model transformation rules from examples, allows to capture implicit and explicit control over the transformation rules. The derivation of both transformation and control knowledge is performed through a heuristic search, i.e., a genetic programming algorithm, guided by the conformance with examples of past transformations supplied as pairs of source and target models. Our approach is evaluated on four model transformation problems that require non-trivial control. The obtained results are convincing for three of the four studied problems.", "num_citations": "22\n", "authors": ["545"]}
{"title": "Discovering new change patterns in object-oriented systems\n", "abstract": " Modern software has to evolve to meet the needs of stakeholders; but the nature and scope of this evolution is difficult to anticipate and manage. In this paper, we examine techniques which can discover interesting patterns of evolution in large object-oriented systems. To locate patterns, we use clustering to group together classes which change in the same manner at the same time. Then, we use dynamic time warping to find if a group of classes is similar to another when we ignore the exact moment when changes occur. Groups that exhibit distinctive evolution properties are potential candidates for new evolution patterns. Finally, in a study of two industrial open-source libraries, we identified four new types of change patterns whose usefulness is determined by perusal of the release notes and the architecture.", "num_citations": "22\n", "authors": ["545"]}
{"title": "Automated metamodel/model co-evolution using a multi-objective optimization approach\n", "abstract": " We propose a generic automated approach for the metamodel/model co-evolution. The proposed technique refines an initial model to make it as conformant as possible to the new metamodel version by finding the best compromise between three objectives, namely minimizing (i) the non-conformities with new metamodel version, (ii) the changes to existing models, and (iii) the loss of information. Consequently, we view the co-evolution as a multi-objective optimization problem, and solve it using the NSGA-II algorithm. We successfully validated our approach on the evolution of the well-known UML state machine metamodel. The results confirm the effectiveness of our approach with average precision and recall respectively higher than 87\u00a0% and 89\u00a0%.", "num_citations": "21\n", "authors": ["545"]}
{"title": "Identifying components in object-oriented programs using dynamic analysis and clustering\n", "abstract": " We propose an approach for component candidate identification as a first step towards the extraction of component-based architectures from object-oriented programs. Our approach uses as input dynamic call graphs, built from execution traces corresponding to use cases. This allows to better capture the functional dependencies between classes. The component identification is treated as a clustering problem. To this end, we use formal concept analysis and design heuristics.", "num_citations": "20\n", "authors": ["545"]}
{"title": "Modeling web-based applications quality: a probabilistic approach\n", "abstract": " Quality assurance of Web-based applications is considered as a main concern. Many factors can affect their quality. Modeling and measuring these factors are by nature uncertain and subjective tasks. In addition, representing relationships between these factors is a complex task. In this paper, we propose an approach for modeling and supporting the assessment of Web-based applications quality. Our proposal is based on Bayesian Networks.", "num_citations": "20\n", "authors": ["545"]}
{"title": "Predicting Class Libraries Interface Evolution: an investigation into machine learning approaches\n", "abstract": " Managing the evolution of an OO system constitutes a complex and resource-consuming task. This is particularly true for reusable class libraries since the user interface must be preserved for version compatibility. Thus, the symptomatic detection of potential instabilities during the design phase of such libraries may help avoid later problems. This paper introduces a fuzzy logic-based approach for evaluating the stability of a reusable class library interface, using structural metrics as stability indicators. To evaluate this new approach, we conducted a preliminary study on a set of commercial C++ class libraries. The obtained results are very promising when compared to those of two classical machine learning approaches, top down induction of decision trees and Bayesian classifiers.", "num_citations": "20\n", "authors": ["545"]}
{"title": "A metric based technique for design flaws detection and correction\n", "abstract": " During the evolution of object-oriented (OO) systems, the preservation of correct design should be a permanent quest. However, for systems involving a large number of classes and which are subject to frequent modifications, the detection and correction of design flaws may be a complex and resource-consuming task. Automating the detection and correction of design flaws is a good solution to this problem. Various authors have proposed transformations that improve the quality of an OO system while preserving its behavior. In this paper, we propose a technique for automatically detecting situations where a particular transformation can be applied to improve the quality of a system. The detection process is based on analyzing the impact of various transformations on software metrics using quality estimation models.", "num_citations": "20\n", "authors": ["545"]}
{"title": "Migrating to an object-oriented database using semantic clustering and transformation rules\n", "abstract": " This paper presents a methodology for handling an important step of database migration. The methodology is based on a set of techniques: (i) semantic clustering, (ii) metamodeling, and (iii) knowledge-based schema transformation. Semantic clustering (i.e., grouping based on semantic cohesion) is mainly used to facilitate the process of translating an extended entity relationship schema into a schema of complex objects. Meta modeling is used to define the data models involved in the process of schema transformation. Finally, transformation rules are defined and used for mapping a schema from a source model into a schema expressed in a target model. In this paper, we limit ourselves to the mapping of an extended entity relationship diagram into an object-oriented database schema using the object model supported by the ODMG standard.", "num_citations": "20\n", "authors": ["545"]}
{"title": "Generating model transformation rules from examples using an evolutionary algorithm\n", "abstract": " We propose an evolutionary approach to automatically generate model transformation rules from a set of examples. To this end, genetic programming is adapted to the problem of model transformation in the presence of complex input/output relationships (i.e., models conforming to meta-models) by generating declarative programs (i.e., transformation rules in this case). Our approach does not rely on prior transformation traces for the model-example pairs, and directly generates executable, many-to-many rules with complex conditions. The applicability of the approach is illustrated with the well-known problem of transforming UML class diagrams into relational schemas, using examples collected from the literature.", "num_citations": "19\n", "authors": ["545"]}
{"title": "Object identification in legacy code as a grouping problem\n", "abstract": " Maintenance is undoubtedly the most effort-consuming activity in software production whereby the entropy of legacy systems is a major challenge. Migration of legacy systems to object-oriented technology is considered by many organizations as a suitable way out, however, the cost and the complexity of the task may dissuade the decision-makers. As a contribution to the automation, complete or partial, of the migration process, this paper presents two algorithms for identifying objects in procedural code, a task which is crucial within the entire process. The suggested algorithms are experimentally evaluated, using the examples of three existing systems.", "num_citations": "19\n", "authors": ["545"]}
{"title": "A generic framework for model-set selection for the unification of testing and learning MDE tasks\n", "abstract": " We propose a generic framework for model-set selection for learning or testing Model-Driven Engineering tasks. We target specifically tasks that apply to or manipulate models, such as model definition, model well-formedness checking, and model transformation. In our framework, we view the model-set selection as a multi-objective optimization problem. The framework can be tailored to the learning or testing of a specific task by firstly expressing the coverage criterion, which will be encoded as a first optimization objective. The coverage is expressed by tagging the subset of the input metamodel that is relevant to the considered task. Then, one or more minimality criteria are selected as additional optimization objectives. We illustrate the use of our framework with the testing of metamodels. This case study shows that the multi-objective approach gives better results than random and mono-objective selections.", "num_citations": "18\n", "authors": ["545"]}
{"title": "Detecting program execution phases using heuristic search\n", "abstract": " Understanding a program from its execution traces is extremely difficult because a trace consists of thousands to millions of events, such as method calls, object creation and destruction, etc. Nonetheless, execution traces can provide valuable information, once abstracted from their low-level events. We propose to identify feature-level phases based on events collected from traces of the program execution. We cast our approach in an optimization problem, searching through the dynamic information provided by the program\u2019s execution traces to form a set of phases that minimizes coupling while maximizing cohesion. We applied and evaluated our search algorithms on different execution scenarios of JHotDraw and Pooka.", "num_citations": "18\n", "authors": ["545"]}
{"title": "Multi-level evaluation of web site navigability\n", "abstract": " Much research in recent years has focused on the evaluation of web site quality. The majority of this research has focused on evaluating the quality of individual pages or that of a site as a whole. In this paper, we propose an evaluation approach that combines evaluations at the page level with the one of the web site by means of a page-importance weighing model. We illustrate our approach with the particular characteristic of navigability. Both navigability models at the page and site levels are implemented as Bayesian Belief Networks to manage explicitly the uncertainty of web site evaluation. The page importance weighting is implemented using a random walk strategy. We evaluated the resulting model using a mean-different hypothesis testing. The results showed that our model allows discriminating correctly between web sites having a good navigability and randomly-selected web sites.", "num_citations": "17\n", "authors": ["545"]}
{"title": "A Probabilistic Approach for Change Impact Prediction in Object-Oriented Systems.\n", "abstract": " Several non probabilistic approaches were proposed in the literature to analyze and predict change impact in Object-Oriented (OO) systems. Different aspects were considered in these studies and several experiments were conducted to check some hypotheses. However, causality relation between software internal attributes and change impact still misses convincing explanations. In this paper, we propose a probabilistic approach using Bayesian networks to answer to this problematic of change impact analysis and prediction in OO systems. The built probabilistic model is tested on data extracted from a real system. The running of different scenarios on the network, globally confirm results already found in previous studies.", "num_citations": "17\n", "authors": ["545"]}
{"title": "Specification patterns for formal web verification\n", "abstract": " Quality assurance of Web applications is usually an informal process. Meanwhile, formal methods have been proven to be reliable means for the specification, verification, and testing of systems. However, the use of these methods requires learning their mathematical foundations, including temporal logics. Specifying properties using temporal logic is often complicated even to experts, while it is a daunting and error prone task for non-expert users. To assist web developers and testers in formally specifying web related properties, we elaborate a library of web specification patterns. The current version of the library of 119 functional and non-functional patterns is a result of scrutinizing various resources in the field of quality assurance of Web Applications, which characterize successful web application using a set of standardized attributes.", "num_citations": "17\n", "authors": ["545"]}
{"title": "Properties and scopes in web model checking\n", "abstract": " We consider a formal framework for property verification of web applications using Spin model checker. Some of the web related properties concern all states of the model, while others-only a proper subset of them. To be able to discriminate states of interest in the state space, we solve the problem of property specification in LTL over a subset of states of a system under test while ignoring the valuation of the properties in the rest of them. We introduce specialized operators that facilitate specifying properties over propositional scopes, where each scope constitutes a subset of states that satisfy a propositional logic formula. Using the proposed operators, the user can specify web properties more concisely and intuitively. We illustrate the proposed solution in specifying properties of web applications and discuss other potential applications.", "num_citations": "17\n", "authors": ["545"]}
{"title": "A cooperative approach for combining client-based and library-based api usage pattern mining\n", "abstract": " Software developers need to cope with the complexity of Application Programming Interfaces (APIs) of external libraries or frameworks. Typical APIs provide thousands of methods to their client programs, and these methods are not used independently of each other. Much existing work has provided different techniques to mine API usage patterns based on client programs in order to help developers understanding and using existing libraries. Other techniques propose to overcome the strong constraint of clients' dependency and infer API usage patterns only using the library source code. In this paper, we propose a cooperative usage pattern mining technique (COUPminer) that combines client-based and library-based usage pattern mining. We evaluated our technique through four APIs and the obtained results show that the cooperative approach allows taking advantage at the same time from the precision of\u00a0\u2026", "num_citations": "16\n", "authors": ["545"]}
{"title": "Automated metamodel/model co-evolution: A search-based approach\n", "abstract": " Context: Metamodels evolve over time to accommodate new features, improve existing designs, and fix errors identified in previous releases. One of the obstacles that may limit the adaptation of new metamodels by developers is the extensive manual changes that have been applied to migrate existing models. Recent studies addressed the problem of automating the metamodel/model co-evolution based on manually defined migration rules. The definition of these rules requires the list of changes at the metamodel level which are difficult to fully identify. Furthermore, different possible alternatives may be available to translate a metamodel change to a model change. Thus, it is hard to generalize these co-evolution rules.Objective: We propose an alternative automated approach for the metamodel/model co-evolution. The proposed approach refines an initial model instantiated from the previous metamodel version to\u00a0\u2026", "num_citations": "15\n", "authors": ["545"]}
{"title": "Systematic mapping study of model transformations for concrete problems\n", "abstract": " As a contribution to the adoption of the Model-Driven Engineering (MDE) paradigm, the research community has proposed concrete model transformation solutions for the MDE infrastructure and for domain-specific problems. However, as the adoption increases and with the advent of the new initiatives for the creation of repositories, it is legitimate to question whether proposals for concrete transformation problems can be still considered as research contributions or if they respond to a practical/technical work. In this paper, we report on a systematic mapping study that aims at understanding the trends and characteristics of concrete model transformations published in the past decade. Our study shows that the number of papers with, as main contribution, a concrete transformation solution, is not as high as expected. This number increased to reach a peak in 2010 and is decreasing since then. Our results also include\u00a0\u2026", "num_citations": "15\n", "authors": ["545"]}
{"title": "A formal approach for run-time verification of web applications using scope-extended LTL\n", "abstract": " ContextIn the past decade, the World Wide Web has been subject to rapid changes. Web sites have evolved from static information pages to dynamic and service-oriented applications that are used for a broad range of activities on a daily basis. For this reason, thorough analysis and verification of Web Applications help assure the deployment of high quality applications.ObjectivesIn this paper, an approach is presented to the formal verification and validation of existing web applications. The approach consists of using execution traces of a web application to automatically generate a communicating automata model. The obtained model is used to model checking the application against predefined properties, to perform regression testing, and for documentation.MethodsTraces used in the proposed approach are collected by monitoring a web application while it is explored by a user or a program. An automata\u00a0\u2026", "num_citations": "15\n", "authors": ["545"]}
{"title": "Integrating the designer in-the-loop for metamodel/model co-evolution via interactive computational search\n", "abstract": " Metamodels evolve even more frequently than programming languages. This evolution process may result in a large number of instance models that are no longer conforming to the revised meta-model. On the one hand, the manual adaptation of models after the metamodels' evolution can be tedious, error-prone, and time-consuming. On the other hand, the automated co-evolution of metamodels/models is challenging especially when new semantics is introduced to the metamodels. In this paper, we propose an interactive multi-objective approach that dynamically adapts and interactively suggests edit operations to developers and takes their feedback into consideration. Our approach uses NSGA-II to find a set of good edit operation sequences that minimizes the number of conformance errors, maximizes the similarity with the initial model (reduce the loss of information) and minimizes the number of proposed edit\u00a0\u2026", "num_citations": "14\n", "authors": ["545"]}
{"title": "Enactment of components extracted from an object-oriented application\n", "abstract": " Software architecture plays an important role for the application understanding before its maintenance. Unfortunately, for legacy systems code often there is no corresponding (or up to date) architecture. So, several work tackle this problem by extracting components from the legacy system and define their links. Although these components allow to get an architectural view of the legacy system, they still can\u2019t be easily implemented in a concrete framework. In fact, restructuring completely the legacy system facilitates the mapping between the architectural elements and their corresponding ones in the code. This paves the way to the future maintenance of the system.               Our approach aims to reach this complete restructuring. Thus it goes beyond what exists in the state of the art by proposing a technique that makes components extracted from object-oriented applications implementable within a concrete\u00a0\u2026", "num_citations": "14\n", "authors": ["545"]}
{"title": "Concerned about separation\n", "abstract": " The separation of concerns, as a conceptual tool, enables us to manage the complexity of the software systems that we develop. There have been a number of approaches aimed at modularizing software around the natural boundaries of the various concerns, including subject-oriented programming, composition filters, aspect-oriented programming, and our own view-oriented programming. The growing body of experiences in using these approaches has identified a number of fundamental issues such as what is a concern, what is an aspect, which concerns are inherently separable, and which aspects are composable. To address these issues, we need to focus on the semantics of separation of concerns, as opposed to the mechanics (and semantics) of aspect-oriented software development methods. We propose a conceptual framework based on a transformational view of software development. Our\u00a0\u2026", "num_citations": "14\n", "authors": ["545"]}
{"title": "Representing and querying reusable object frameworks\n", "abstract": " It has kmgbeen recogm\u201d zedthat objects are too small units of reme to tmvide anv real ieverape IDeutsch, l 9891. When a set of obiects ar; oflen U. re; together to\u201d u; ornp [ish aj% q\u201d uently needed~ ask~ it is worthwhik packaging them as a unit; this is what we call object jiameworks. There has been a lot of interest recently in the &sign, description, and use of object fkmteworks. Wesel out to develop a representatwn ofjkzrneworks that satisfies thefollowing, goals,(i) a description of inter-object behavior that supports both formal verifrcatwn and understanding,(ii) abstraction and generalization,(iii) support for reure related tasks such as searchability and description of usage,(iv) language-independence, and (v) an easy attraction of the descriptions from actual code. We propose a rnodei that attempts to address these often convicting goals, and discuss the major trade-offs, including on issues related to the abstraction of\u00a0\u2026", "num_citations": "14\n", "authors": ["545"]}
{"title": "Heuristic-Based Recommendation for Metamodel\u2014OCL Coevolution\n", "abstract": " We propose a novel approach for solving the problem of coevolution between metamodels and OCL constraints. Unlike existing solutions, our approach does not rely on predefined update rules and explicit tracking of high level changes to the metamodel. Rather, we pose it as a multi-objective optimization problem, exploring the space of possible OCL modifications to identify solutions that (a) do not violate the structure of the new version of the metamodel, (b) minimize changes to existing constraints, and (c) minimize loss of information. Finally, we recommend an appropriate subset of solutions to the user. We evaluate our approach on three cases of metamodel and OCL coevolution. The results show that we recommend accurate solutions for updating OCL constraints, even for complex evolution changes.", "num_citations": "13\n", "authors": ["545"]}
{"title": "A fuzzy logic framework to improve the performance and interpretation of rule-based quality prediction models for oo software\n", "abstract": " Current object-oriented (OO) software systems must satisfy new requirements that include quality aspects. These, contrary to functional requirements, are difficult to determine during the test phase of a project. Predictive and estimation models offer an interesting solution to this problem. This paper describes an original approach to build rule-based predictive models that are based on fuzzy logic and that enhance the performance of classical decision trees. The approach also attempts to bridge the cognitive gap that may exist between the antecedent and the consequent of a rule by turning the latter into a chain of sub rules that account for domain knowledge. The whole framework is evaluated on a set of OO applications.", "num_citations": "13\n", "authors": ["545"]}
{"title": "Using coupling metrics for change impact analysis in object-oriented systems\n", "abstract": " The development of software products consumes a lot of time and resources. On the other hand, these development costs are lower than maintenance costs, which represent a major concern, specially, for systems designed with recent technologies. Systems modification should be taken rigorously, and change effects must be considered. In this paper, we propose an approach, both analytical and experimental; its objective is to analyze and predict changes impacts in Object-Oriented (OO) systems. The method we follow consists first, to choose an existing impact model, and adapt it afterward. An impact calculation technique based on a meta-model is developed. To evaluate our approach, an empirical study was led on a real system in which a correlation hypothesis between coupling and change impact was advanced. A concrete change was done in the target system and coupling metrics were extracted from it. The hypothesis was verified with machine-learning (ML) techniques and results are presented and commented.", "num_citations": "11\n", "authors": ["545"]}
{"title": "Application de la meta-modelisation a la generation des outils de conception et de mise en uvre des bases de donnees\n", "abstract": " La construction d'outils de conception et de mise en uvre des bases de donnees est une activite complexe, compte tenu de la masse de connaissances qu'elle met en jeu. La reduction de cette complexite necessite une structuration de ces connaissances. Dans cette optique, nous proposons un systeme d'aide a la construction de ces outils base sur les techniques de meta-modelisation. Le principe de la demarche sous-jacente a ce systeme est de voir un outil de conception et de mise en uvre des bases comme la somme de trois elements:(1) un editeur de schemas conceptuels base sur un formalisme de modelisation,(2) une representation logique correspondant a la famille du (ou des) sgbd cible (s), et (3) un mecanisme de transformation des schemas conceptuels en schemas logiques. A ces trois elements s' ajoutent des mecanismes de traductions des schemas logiques en codes dans les differents langages de description des sgbd. Notre systeme permet d'engendrer de maniere automatique et economique des editeurs de schemas conceptuels a partir des descriptions des formalismes de modelisation. Il permet en outre de decrire les representations logiques des sgbd. Il permet enfin de decrire les mecanismes de transformations des schemas conceptuels en schemas logiques par des systemes de regles. Nous avons utilise ce systeme pour reconstruire des outils existants. Les resultats de ces experiences ont permis d'envisager un travail de generalisation de cette demarche a la construction des ateliers de genie logiciel", "num_citations": "11\n", "authors": ["545"]}
{"title": "Automated co-evolution of metamodels and transformation rules: A search-based approach\n", "abstract": " Metamodels frequently change over time by adding new concepts or changing existing ones to keep track with the evolving problem domain they aim to capture. This evolution process impacts several depending artifacts such as model instances, constraints, as well as transformation rules. As a consequence, these artifacts have to be co-evolved to ensure their conformance with new metamodel versions. While several studies addressed the problem of metamodel/model co-evolution (Please note the potential name clash for the term co-evolution. In this paper, we refer to the problem of having to co-evolve different dependent artifacts in case one of them changes. We are not referring to the application or adaptation of co-evolutionary search algorithms.), the co-evolution of metamodels and transformation rules has been less studied. Currently, programmers have to manually change model transformations\u00a0\u2026", "num_citations": "10\n", "authors": ["545"]}
{"title": "Generating visualization-based analysis scenarios from maintenance task descriptions\n", "abstract": " Software visualization is an efficient and flexible tool to inspect and analyze software data at various levels of detail. However, software analysts typically do not have a sufficient background in visualization and cognitive science to select efficient representations and parameters without the help of visualization experts. To overcome this problem, we propose an approach to generate software analysis tasks that use visualization. To this end, we use taxonomies of low-level analytic tasks, high-level interactive tasks, and perceptual rules to design an assistant that proposes analysis scenarios.", "num_citations": "10\n", "authors": ["545"]}
{"title": "Gestion de mod\u00e8les: d\u00e9finitions, besoins et revue de litt\u00e9rature\n", "abstract": " La gestion de mod\u00e8les int\u00e9resse de nombreuses communaut\u00e9s de recherche. L\u2019Object Management Group) travaille sur MDA (Model Driven Architecture). Les communaut\u00e9s travaillant sur la gestion des connaissances, la gestion de m\u00e9ta-donn\u00e9es, les ontologies, la qualit\u00e9 de service et le g\u00e9nie logiciel s\u2019 int\u00e9resse \u00e0 la d\u00e9finition et l\u2019utilisation des mod\u00e8les. Nous faisons dans cet article, le point sur les d\u00e9finitions et besoins pour la gestion de mod\u00e8les et passons en revue divers formalismes de mod\u00e9lisation: sNets, graphes conceptuels, UML (Unified Modeling Language) et MOF (Meta Object Facility), XML (eXtensible Markup Language) et XML Schema, RDF (Resource description Framework) et RDF Schema, et OWL (Web Ontology Language).", "num_citations": "10\n", "authors": ["545"]}
{"title": "Understanding the use of inheritance with visual patterns\n", "abstract": " The goal of this work is to visualize inheritance in object-oriented programs to help its comprehension. We propose a single, compact view of all class hierarchies at once using a custom Sunburst layout. It enables to quickly discover interesting facts across classes while preserving the essential relationship between parent and children classes. We explain how standard inheritance metrics are mapped into our visualization. Additionally, we define a new metric characterizing similar children classes. Using these metrics and the proposed layout, a set of common visual patterns is derived. These patterns allow the programmer to quickly understand how inheritance is used and provide answers to some essential questions when performing program comprehension tasks. Our approach is evaluated through a case study that involves examples from large programs, demonstrating its scalability.", "num_citations": "9\n", "authors": ["545"]}
{"title": "VAQoS: architecture for end-to-end QoS management of value added Web services\n", "abstract": " During the last few years, Value Added Web Services (VAWS) are increasingly becoming a hot issue in both research and industry. With the abundance of VAWS providers, Quality of Service (QoS) is a key factor to allow potential clients to differentiate between providers. In this paper, we propose a new architecture, called VAQoS, for managing and assuring QoS provision for VAWS. This architecture performs management of QoS by:(1) allowing providers to extend the service description with QoS-centered annotations,(2) including a validation process that enables providers to test their service interfaces as well as the level of QoS they can provide prior to publishing the service,(3) allowing clients to express their required functionalities with QoS requirements;(4) providing support for QoS negotiation between clients and providers,(5) allowing monitoring of the agreed QoS between clients and providers, and\u00a0\u2026", "num_citations": "9\n", "authors": ["545"]}
{"title": "Merging conceptual hierarchies using concept lattices\n", "abstract": " In many situations, one faces the need for integrating a set of conceptual hierarchies that represent parts of the same domain. The target structure of the integration is a unique conceptual hierarchy that embeds at least the total of the knowledge encoded in the initial ones. The key issues to address here are the discovery of higher-level abstractions on top of the existing concepts and the resolution of naming conflicts among these. Formal concepts analysis (FCA) is a mathematical approach toward abstracting from attribute-based object descriptions which has been recently extended to fit relational descriptions thus giving rise to the relational concept analysis (RCA) framework. Building up on RCA, our integration approach amounts to encoding the initial hierarchies into set of binary tables, one for each component category, e.g., classes, associations, etc., and subsequently constructing, querying and reorganizing the corresponding abstraction hierarchies until a unique satisfactory hierarchy is obtained. This paper puts the emphasis on the mechanisms of discovering new abstractions and exporting them between the abstraction hierarchies of related component categories. The impact of naming conflicts within the RCA process is discussed as well. The paper uses UML as description language for conceptual hierarchies.", "num_citations": "9\n", "authors": ["545"]}
{"title": "Analogy-based software quality prediction\n", "abstract": " Predicting the stability of object-oriented systems is an important and challenging task. Classical approaches to quality prediction perform some form of inductive inference starting from datasets of software items with known quality factor values and looking for typical features that discriminate the items regarding the quality factor. However, most of the effective methods for predictive model construction are based on the implicit hypothesis that the available samples are representative, which is rather strong. The approach we propose implements a similarity-based comparison principle: the quality factor (stability) of a given software item is estimated from the recorded stability of a set of other items that have been recognized as the most similar to that item among a larger set of items stored in a database. This approach is evaluated using the successive versions of the JDK API.", "num_citations": "9\n", "authors": ["545"]}
{"title": "Towards assisting developers in API usage by automated recovery of complex temporal patterns\n", "abstract": " ContextDespite the many advantages, the use of external libraries through their APIs remains difficult because of the usage patterns and constraints that are hidden or not properly documented. Existing work provides different techniques to recover API usage patterns from client programs in order to help developers use those libraries. However, most of these techniques produce patterns that generally do not involve temporal properties.ObjectiveIn this paper, we discuss the problem of temporal usage patterns recovery and propose an algorithm to solve it. We also discuss how the obtained patterns can be used at different stages of client development.MethodWe address the recovery of temporal API usage patterns as an optimization problem and solve it using a genetic-programming algorithm.ResultsOur evaluation on different APIs shows that the proposed algorithm allows to derive non-trivial temporal usage that\u00a0\u2026", "num_citations": "8\n", "authors": ["545"]}
{"title": "Deriving component interfaces after a restructuring of a legacy system\n", "abstract": " Although there are contributions on component-oriented languages, components are mostly implemented using object-oriented (OO) languages. In this perspective, a component corresponds to a set of classes that work together to provide one or more services. Services are grouped together in interfaces that are each implemented by a class. Thus, dependencies between components are defined using the semantic of the enclosed classes, which is mostly structural. This makes it difficult to understand an architecture described with such links. Indeed, at an architectural level dependencies between components must represent functional aspects. This problem is worse, when the components are obtained by re-engineering of legacy OO systems. Indeed, in this case the obtained components are mainly based on the consistency of the grouping logic. So, in this paper we propose an approach to identify the interfaces\u00a0\u2026", "num_citations": "8\n", "authors": ["545"]}
{"title": "Analyse et pr\u00e9diction de l'impact de changements dans un syst\u00e8me \u00e0 objets: Approche probabiliste.\n", "abstract": " Nous proposons dans cet article une approche probabiliste utilisant les r\u00e9seaux bay\u00e9siens pour analyser et pr\u00e9dire les impacts des changements dans les syst\u00e8mes \u00e0 objets. Un mod\u00e8le d\u2019impact a \u00e9t\u00e9 construit et des probabilit\u00e9s ont \u00e9t\u00e9 affect\u00e9es aux diff\u00e9rents sommets du r\u00e9seau. Des donn\u00e9es r\u00e9colt\u00e9es sur un syst\u00e8me r\u00e9el sont utilis\u00e9es pour \u00e9tudier empiriquement des hypoth\u00e8ses (relations) de causalit\u00e9 entre d\u2019une part, des attributs internes de logiciel, et d\u2019autre part, l\u2019impact du changement. Pour ce faire, plusieurs sc\u00e9narios ont \u00e9t\u00e9 ex\u00e9cut\u00e9s sur le r\u00e9seau. Les r\u00e9sultats obtenus ont d\u2019une part, confirm\u00e9 certains r\u00e9sultats d\u00e9j\u00e0 trouv\u00e9s lors de travaux ant\u00e9rieurs, mais d\u2019autre part, remis en cause d\u2019autres conclusions. Cette \u00e9tude entre dans le cadre g\u00e9n\u00e9ral d\u2019une d\u00e9marche tendant \u00e0 proposer des mod\u00e8les d\u2019estimation de la qualit\u00e9 du produit logiciel; elle montre qu\u2019un mod\u00e8le probabiliste constitue une alternative int\u00e9ressante, aux mod\u00e8les non probabilistes propos\u00e9s dans la litt\u00e9rature.", "num_citations": "8\n", "authors": ["545"]}
{"title": "Animation coherence in representing software evolution\n", "abstract": " Software evolution study is crucial to the understanding of software in general and software quality in particular. However, the study of software evolution requires the analysis of large amounts of data, ie, source code information for every single version of a program. Considering the size, manual analysis of this information is virtually impossible. Automatic analysis is quick, but requires strong assumptions on data which is hard to establish in our domain. We propose a semi-automatic approach based on visualization to represent software versions. We use animation to represent the transitions between versions. We exploit the coherence between two successive versions and we transform it into visual coherence that can be perceived by the user. Our solution is interesting because the movement in the graphical representation can be aligned naturally with the code modifications. It also reduces the required space in 3D by using a fourth dimension which is time.", "num_citations": "8\n", "authors": ["545"]}
{"title": "Propositional scopes in linear temporal logic\n", "abstract": " In this paper, we address the problem of specifying a property in LTL over a subset of the states of a system under test, ignoring the rest of the states. A modern LTL semantics that applies for both finite and infinite traces is considered. We introduce specialized operators (syntax and semantic) that help specifying properties over propositional scopes, where each scope constitute a subset of states that satisfy a propositional logic formula. These operators assist the user in specifying the properties more easily and intuitively.", "num_citations": "8\n", "authors": ["545"]}
{"title": "Supporting web collaboration for cooperative software development\n", "abstract": " With the rapid growth of web technology, more and more software development projects use web collaborations to facilitate the development process. However, web collaboration activity is a complex orchestration. It involves many people work together without the barrier of time and space difference. Therefore, how to efficiently monitor and control web collaboration activity becomes a critical issue in a web-based collaborative software development project. In this paper, we present a novel approach to tackle this difficult problem by means of monitoring collaboration task progress. In addition, we also provide solutions to automate the dynamic control of cooperation, thus to improve the web collaboration performance.", "num_citations": "8\n", "authors": ["545"]}
{"title": "Estimating object-relational database understandability using structural metrics\n", "abstract": " New Object-Relational Database Management Systems (ORDBMSs) are replacing existing relational ones. In spite of the high expressiveness, application systems built upon ORDBMS are more complex and difficult to maintain due to the mixing of two paradigms, the relational and the objectoriented. This paper describes a suite of metrics for measuring different aspects of an object-relational database. An empirical validation of the usefulness of the proposed metrics in estimating the understandability of an object-relational schema is given. The analysis procedure comprises the use of two techniques: C4.5, a machine learning algorithm, and RoC, a robust Bayesian classifier. The results demonstrate that a subset of the proposed measures is relevant for the estimation of the understandability.", "num_citations": "8\n", "authors": ["545"]}
{"title": "Recommending model refactoring rules from refactoring examples\n", "abstract": " Models, like other first-class artifacts such as source code, are maintained and may be refactored to improve their quality and, consequently, one of the derived artifacts. Considering the size of the manipulated models, automatic support is necessary for refactoring tasks. When the refactoring rules are known, such a support is simply the implementation of these rules in editors. However, for less popular and proprietary modeling languages, refactoring rules are generally difficult to define. Nevertheless, their knowledge is often embedded in practical examples. In this paper, we propose an approach to recommend refactoring rules that we lean automatically from refactoring examples. The evaluation of our approach on three modeling languages shows that, in general, the learned rules are accurate.", "num_citations": "7\n", "authors": ["545"]}
{"title": "Learning Model Transformations from Examples using FCA: One for All or All for One?\n", "abstract": " In Model-Driven Engineering (MDE), model transformations are basic and primordial entities. An efficient way to assist the definition of these transformations consists in completely or partially learning them. MTBE (Model Transformation By-Example) is an approach that aims at learning a model transformation from a set of examples, i.e. pairs of transformation source and target models. To implement this approach, we use Formal Concept Analysis as a learning mechanism in order to extract executable rules. In this paper, we investigate two learning strategies. In the first strategy, transformation rules are learned independently from each example. Then we gather these rules into a single set of rules. In the second strategy, we learn the set of rules from all the examples. The comparison of the two strategies on the well-known transformation problem of class diagrams to relational schema showed that the rules obtained from the two strategies are interesting. Besides the first one produces rules which are more proper to their examples and apply well compared to the second one which builds more detailed rules but larger and more difficult to analyze and to apply.", "num_citations": "7\n", "authors": ["545"]}
{"title": "CompQoS: Towards an Architecture for QoS Composition, Monitoring, and Validation of Composed Web Services\n", "abstract": " COMPQOS: TOWARDS AN ARCHITECTURE FOR QOS COMPOSITION, MONITORING, AND VALIDATION OF COMPOSED WEB SERVICES *MA Serhani, *A. Benharref, *R. Dssouli, **H. Sahraoui *Concordia University Department of Electrical and Computer Engineering 1455 de Maisonneuve Blvd. West Montreal, Quebec H3G 1M8 Canada Correspondence Email: {m_serhan,abdel,dssouli}@ece.concordia.ca ** D\u00e9partement d'Informatique et recherche op\u00e9rationnelle Universit\u00e9 de Montr\u00e9al CP 6128 succ Centre-Ville, Montr\u00e9al QC H3C 3J7, Canada Correspondence Email: sahraouh@iro.umontreal.ca ABSTRACT Quality of Service (QoS) support in web services plays a great role in their success. Composing web services to provide new functionalities is gaining more and more momentum while the number of available web services proliferates and the need for composite web services increases. In this paper, we \u2026", "num_citations": "7\n", "authors": ["545"]}
{"title": "QoS integration in value added Web services\n", "abstract": " Nowadays, Value Added Web Services (VAWS) are becoming a hot spot for web services providers. Quality of Service (QoS) support in VAWS is an important issue. The need for providing and assuring an acceptable level of QoS is the objective of most of service providers. Service providers compete in satisfying requirements of potential clients by providing QoS support of VAWS. In this work, we propose an architecture for managing and assuring QoS provision of VAWS. The endeavor of this architecture is to support QoS management and enforcement for both clients and providers of web services. We have developed the components of the architecture that allow providers to introduce QoS-centered annotations in the service description and to guarantee the publication of these QoS carried in the service interface description. Moreover, a validation process is developed to test the service interface and the QoS for the providers before publishing its services. The validation practices will enforce the accurateness of available services and their QoS, and the clients\u2019 trustfulness. The architecture allows clients to introduce theirs QoS requirements in their requests to select web services. It also supports bidirectional QoS negotiation based algorithm for clients and providers. This will add more flexibility for the provision of QoS, for both clients and providers. The validation of the overall architecture is studied through an implementation.", "num_citations": "7\n", "authors": ["545"]}
{"title": "Extending software quality predictive models using domain knowledge\n", "abstract": " Current methods to build software quality estimation models suffer from two problems: The use of precise threshold values and their interpretation in the absence of formal models, and the crudeness of the derived rules which can only serve to build na\u00efve models. In this position paper, we describe a novel approach to alleviate both problems. We propose to build fuzzy decision processes that combine both software metrics and heuristic knowledge from the field. As a result, quality estimation models are obtained that are both efficient and that provide a more comprehensive explanation of the relationship that exists between the observed data and the predicted software quality characteristic.", "num_citations": "7\n", "authors": ["545"]}
{"title": "Towards rule-based detection of design patterns in model transformations\n", "abstract": " Model transformations are at the very heart of the Model-Driven Engineering paradigm. As modern programs, they are complex, difficult to write and test, and overall, difficult to understand, maintain, and reuse. In other paradigms, such as object-oriented programming, design patterns play an important role for understanding and reusing code. Many works have been proposed to detect complete design pattern instances for understanding and documentation purposes, but also partial design pattern instances for quality assessment and refactoring purposes. Recently, a catalog of design patterns has been proposed for model transformations. In this paper, we propose to detect these design patterns in declarative model transformation programs. Our approach first detects the rules that may play a role in a design pattern. Then, it ensures that the control flow over these rules corresponds to the scheduling\u00a0\u2026", "num_citations": "6\n", "authors": ["545"]}
{"title": "Automated evaluation of website navigability: an empirical validation of multilevel quality models\n", "abstract": " Websites are an important and efficient means of communication for companies wishing to interact with their clients. Therefore, research has focused on evaluating how websites should be structured to ensure their quality. The majority of this research has focused on evaluating the quality of individual pages or that of a site as a whole. In this article, we propose the use of two\u2010level models that combine evaluations at the page level with evaluations at the site level, and applied them to the problem of evaluating the navigability of websites. To test our models, we conducted a study with 21 subjects who had to complete navigation tasks on several websites, and compared their quality judgments to those produced by single and two\u2010level quality models. We found that two\u2010level models are better predictors of navigability. Finally, we show how two\u2010level models are able to suggest modifications to improve site\u00a0\u2026", "num_citations": "6\n", "authors": ["545"]}
{"title": "What you see is what you asked for: An effort-based transformation of code analysis tasks into interactive visualization scenarios\n", "abstract": " We propose an approach that derives interactive visualization scenarios from descriptions of code analysis tasks. The scenario derivation is treated as an optimization process. In this context, we evaluate different possibilities of using a given visualization tool to perform the analysis task, and select the scenario that requires the least effort from the analyst. Our approach was applied successfully to various analysis tasks such as design defect detection and feature location.", "num_citations": "6\n", "authors": ["545"]}
{"title": "Vers une approche d'analyse de l'impact du changement dans un syst\u00e8me \u00e0 objets\n", "abstract": " Sauf mention contraire ci-dessus, le contenu de cette notice bibliographique peut \u00eatre utilis\u00e9 dans le cadre d\u2019une licence CC BY 4.0 Inist-CNRS/Unless otherwise stated above, the content of this bibliographic record may be used under a CC BY 4.0 licence by Inist-CNRS/A menos que se haya se\u00f1alado antes, el contenido de este registro bibliogr\u00e1fico puede ser utilizado al amparo de una licencia CC BY 4.0 Inist-CNRS", "num_citations": "6\n", "authors": ["545"]}
{"title": "A Bayesian network to represent a data quality model\n", "abstract": " Web portals provide data to many people where data consumers need to assess Data Quality (DQ). In our previous work a Portal Data Quality Model (PDQM) was developed. PDQM is focused on data consumers' perspective and is composed by 33 attributes appropriate for DQ evaluation. Now, we have organised these attributes into a generic and operational structure. Considering the uncertainty inherent in perception of quality, we decided to use a probabilistic approach, using Bayesian Networks (BNs). This paper, explains the definition of the BN structure that supports PDQM.", "num_citations": "6\n", "authors": ["545"]}
{"title": "Program comprehension with dynamic recovery of code collaboration patterns and roles\n", "abstract": " Software functionalities and behavior are accomplished by the cooperation of code artifacts. The understanding of this type of source code collaboration provides an important aid to the maintenance and evolution of legacy systems. Normally, legacy systems were developed decades ago using early programming languages such as Cobol, Fortran or C etc., and a huge number of such systems are still in use. Coyle et. al estimate that only the systems written in Cobol have been account for more than 100 billion LOC. Moreover, a large amount of domain business knowledge has been coded in legacy software. After many years of maintenance, the quality of operation and maintainability has deteriorated dramatically due to many reasons, such as lack of up-to-date documents, lost of key personnel, shift of technology of inter-operating peripheral systems etc. The extraction of code artifact collaborations and their roles\u00a0\u2026", "num_citations": "6\n", "authors": ["545"]}
{"title": "An empirical study with object-relational databases metrics\n", "abstract": " It is important that the software products, and obviously databases, are evaluated for every relevant quality characteristic, using validated or widely accepted metrics. It is also important to validate the metrics from a formal point of view in order to ensure its usefulness. However, into the aspects of software measurement, the research is needed, from theoretical but also from a practical point of view. So, it is necessary to do experiments to validate the metrics. We have developed an experiment in order to validate some object-relational databases metrics. The experiment was done on CRIM (Centre de Recherche Informatique de Montr\u00e9al) from Canada and repeated on the University of Castilla-La Mancha from Spain. The results obtained on both experiments were very similar, so we can conclude that both, metric TS (Table Size) and DRT (Depth of Referential Tree) seem to be good indicators for the maintainability of a table.", "num_citations": "6\n", "authors": ["545"]}
{"title": "Using fuzzy threshold values for predicting class libraries interface evolution\n", "abstract": " This work presents a technique to circumvent one of the major problems associated with building and applying techniques to build software quality estimation models, namely the use of precise metric thresholds values; we used a fuzzy binary decision tree to investigate the stability of a reusable class library interface, using structural metrics as stability indicators. To evaluate this new approach, we conducted a study on different versions of a commercial C++ class library. The obtained results are very promising when compared to those of two classical machine learning approaches, Top Down Induction of Decision Trees and Bayesian classifiers.", "num_citations": "6\n", "authors": ["545"]}
{"title": "Towards the automated recovery of complex temporal api-usage patterns\n", "abstract": " Despite the many advantages, the use of external libraries through their APIs remains difficult because of the usage patterns and constraints that are hidden or not properly documented. Existing work provides different techniques to recover API usage patterns from client programs in order to help developers understand and use those libraries. However, most of these techniques produce basic patterns that generally do not involve temporal properties. In this paper, we discuss the problem of temporal usage patterns recovery and propose a genetic-programming algorithm to solve it. Our evaluation on different APIs shows that the proposed algorithm allows to derive non-trivial temporal usage patterns that are useful and generalizable to new API clients.", "num_citations": "5\n", "authors": ["545"]}
{"title": "Mining complex temporal api usage patterns: an evolutionary approach\n", "abstract": " Learning to use existing or new software libraries is a difficult task for software developers, which would impede their productivity. Much existing work has provided different techniques to mine API usage patterns from client programs inorder to help developers on understanding and using existinglibraries. However, these techniques produce incomplete patterns, i.e., without temporal properties, or simple ones. In this paper, we propose a new formulation of the problem of API temporal pattern mining and a new approach to solve it. Indeed, we learn complex temporal patterns using a genetic programming approach. Our preliminary results show that across a considerable variability of client programs, our approach has been able to infer non-trivial patterns that reflect informative temporal properties.", "num_citations": "5\n", "authors": ["545"]}
{"title": "Recommending improvements to web applications using quality-driven heuristic search\n", "abstract": " Planning out maintenance tasks to increase the quality of Web applications can be difficult for a manager. First, it is hard to evaluate the precise effect of a task on quality. Second, quality improvement will generally be the result of applying a combination of available tasks; identifying the best combination can be complicated. We present a general approach to recommend improvements to Web applications. The approach uses a meta-heuristic algorithm to find the best sequence of changes given a quality model responsible to evaluate the fitness of candidate sequences. This approach was tested using a navigability model on 15 different Web pages. The meta-heuristic recommended the best possible sequence for every tested configuration, while being much more efficient than an exhaustive search with respect to execution time.", "num_citations": "5\n", "authors": ["545"]}
{"title": "Un m\u00e9canisme de s\u00e9lection de composants logiciels\n", "abstract": " Une approche \u00e0 base de composants permet de construire des applications complexes \u00e0 partir de composants \u00ab sur \u00e9tag\u00e8re \u00bb provenant de diff\u00e9rents march\u00e9s. L'effort de d\u00e9veloppement r\u00e9side d\u00e9sormais dans la s\u00e9lection des composants qui r\u00e9pondent le mieux aux besoins apparaissant lors du d\u00e9veloppement d'une application. Dans cet article, nous proposons un m\u00e9canisme permettant la s\u00e9lection automatique d'un composant parmi un ensemble de candidats, en fonction de ses propri\u00e9t\u00e9s fonctionnelles et non fonctionnelles. Ce m\u00e9canisme a \u00e9t\u00e9 test\u00e9 sur un cas concret utilisant le march\u00e9 aux composants ComponentSource.", "num_citations": "5\n", "authors": ["545"]}
{"title": "WeSPaS: A specification pattern system for web verification\n", "abstract": " CiteSeerX \u2014 WeSPaS: A Specification Pattern System for Web Verification Documents Authors Tables Log in Sign up MetaCart DMCA Donate CiteSeerX logo Documents: Advanced Search Include Citations Authors: Advanced Search Include Citations Tables: DMCA WeSPaS: A Specification Pattern System for Web Verification (2007) Cached Download as a PDF Download Links [www.crim.ca] Save to List Add to Collection Correct Errors Monitor Changes by May Haydar , Houari Sahraoui Summary Citations Active Bibliography Co-citation Clustered Documents Version History Share Facebook Twitter Reddit Bibsonomy OpenURL Abstract Keyphrases specification pattern system web verification Powered by: Apache Solr About CiteSeerX Submit and Index Documents Privacy Policy Help Data Source Contact Us Developed at and hosted by The College of Information Sciences and Technology \u00a9 2007-2019 The \u2026", "num_citations": "5\n", "authors": ["545"]}
{"title": "Accommodating software development collaboration\n", "abstract": " With the rapid progress of Internet technology, more and more software projects adopt e-development to facilitate the software development process in a world wide context. However, collaborative software e-development activity itself is a complex orchestration. It involves many people working together without the barrier of time and space difference. Therefore, how to efficiently monitor and control software e-development in a global perspective becomes an important issue for any Internet-based software e-development project. In this paper, we present a novel approach to tackle this crucial issue by means of controlling e-development process, collaborative task progress and communication quality. Meanwhile, we also present our e-development supporting environment prototype: Caribou, to demonstrate the viability of our approach.", "num_citations": "5\n", "authors": ["545"]}
{"title": "Special section on Visual Analytics in Software Engineering.\n", "abstract": " In the recent years there has been a tremendous development in handling large quantities of data in di erent aspects of software product ff development. One of the areas is the ability to understand customers and collect the data from the product usage in-eld; another area is the ability fi to use large quantities of product development data to optimize software development ows. fl As a community we have also made great progress in providing more data from software development systems such as source code management systems (eg Git), defect management systems (eg Jira) and many more. We can see, that visual analytics in software engineering is gaining importance. The ability to quickly, and accurately, understand the software, and its context, is important for software engineers, business analys software product managers, and other roles involved in software development. Visual analytics is a new area\u00a0\u2026", "num_citations": "4\n", "authors": ["545"]}
{"title": "Visualization based api usage patterns refining\n", "abstract": " Learning to use existing or new software libraries is a difficult task for software developers, which would impede their productivity. Most of existing work provided different techniques to mine API usage patterns from client programs, in order to help developers to understand and use existing libraries. However, considering only client programs to identify API usage patterns, is a strong constraint as collecting several similar client programs for an API is not a trivial task. And even if these clients are available, all the usage scenarios of the API of interest may not be covered by those clients. In this paper, we propose a visualization based approach for the refinement of Client-based Usage Patterns. We first visualize the patterns structure. Then we enrich the patterns with API methods that are semantically related to them, and thus may contribute together to the implementation of a particular functionality for potential client\u00a0\u2026", "num_citations": "4\n", "authors": ["545"]}
{"title": "A Unified Framework for the Comprehension of Software's Time\n", "abstract": " The dimension of time in software appears in both program execution and software evolution. Much research has been devoted to the understanding of either program execution or software evolution, but these two research communities have developed tools and solutions exclusively in their respective context. In this paper, we claim that a common comprehension framework should apply to the time dimension of software. We formalize this as a meta-model that we instantiate and apply to the two different comprehension problems.", "num_citations": "4\n", "authors": ["545"]}
{"title": "Do software libraries evolve differently than applications? an empirical investigation\n", "abstract": " More and more, developers use reusable components like libraries to produce high quality software systems. These systems need to satisfy not only the initial demands of their stakeholders, but they need to also offer support for future, changing requirements. While several studies have looked at the cost of modifying systems, there exists no work verifying if libraries evolve differently than applications. This study attempts to do so quantitatively.", "num_citations": "4\n", "authors": ["545"]}
{"title": "An empirical study with metrics for object-relational databases\n", "abstract": " Object-relational databases are supposed to be the substitutes of relational ones because they are a good mixture between the relational model and object-oriented principles. In this paper we present the empirical work we have developed with four metrics for object-relational databases (Percentage of Complex Columns (PCC), Number of Shared Classes (NSC), Number of Involved Classes (NIC) and Table Size (TS)) defined at different granularity levels (attribute, class, table and schema). The empirical work presented is the validation made with the aim of proving the usefulness of the four metrics in estimating the complexity of an object-relational schema. This study can be considered to be a replica of another one we made with the same purpose but with two main differences: the dependent variable and the way we analyze the results. The results obtained from the empirical work seem to prove the\u00a0\u2026", "num_citations": "4\n", "authors": ["545"]}
{"title": "Quantitative approaches in object-oriented software engineering\n", "abstract": " This full-day workshop was organized in four sessions. The first three were thematic technical sessions dedicated to the presentation of the recent research results of participants. Seven, out of eleven accepted submissions were orally presented during these three sessions. The first session also included a metrics collection tool demonstration. The themes of the sessions were, respectively,\" Metrics Definition and Collection\",\" Quality Assessment\" and\" Metrics Validation\". The last session was dedicated to the discussion of a set of topics selected by the participants.", "num_citations": "4\n", "authors": ["545"]}
{"title": "Mod\u00e9lisation Conceptuelle des Bases de Donn\u00e9es: Techniques de M\u00e9ta-Mod\u00e9lisation\n", "abstract": " La complexit6 des nouveaux systkmes B automatiser, combide au changement de mentalid des utilisateurs, exige des outils de conception spkcifiques, simples zi comprendre et faciles B utiliser. Dans cet article, nous presentons une demarche et un m\u00e9ta-outil appliquk B la conception de schkmas de bases de donnh et des traitements s'y rapportant. Le type de conception proposCe est base sur le vocabulaire du domaine. Notre outil permet, de plus, de transformer automatiquement les modkles conceptuels etablis en programmes de description et de manipulation de donnees.", "num_citations": "4\n", "authors": ["545"]}
{"title": "Recommending Metamodel Concepts during Modeling Activities with Pre-Trained Language Models\n", "abstract": " The design of conceptually sound metamodels that embody proper semantics in relation to the application domain is particularly tedious in Model-Driven Engineering. As metamodels define complex relationships between domain concepts, it is crucial for a modeler to define these concepts thoroughly while being consistent with respect to the application domain. We propose an approach to assist a modeler in the design of a metamodel by recommending relevant domain concepts in several modeling scenarios. Our approach does not require to extract knowledge from the domain or to hand-design completion rules. Instead, we design a fully data-driven approach using a deep learning model that is able to abstract domain concepts by learning from both structural and lexical metamodel properties in a corpus of thousands of independent metamodels. We evaluate our approach on a test set containing 166 metamodels, unseen during the model training, with more than 5000 test samples. Our preliminary results show that the trained model is able to provide accurate top- lists of relevant recommendations for concept renaming scenarios. Although promising, the results are less compelling for the scenario of the iterative construction of the metamodel, in part because of the conservative strategy we use to evaluate the recommendations.", "num_citations": "3\n", "authors": ["545"]}
{"title": "Vasco: A visual approach to explore object churn in framework-intensive applications\n", "abstract": " Bloat, and particularly object churn, is a common performance problem in framework-intensive applications. Object churn consists of an excessive use of temporary objects. Identifying and understanding sources of churn is a difficult and labor-intensive task, despite recent advances in automated analysis techniques. We present an interactive visualization approach designed to help developers quickly and intuitively explore the behavior of their application with respect to object churn. We have implemented this technique in Vasco, a new flexible and scalable visualization platform. Vasco is designed to minimize the cognitive effort required for the visualization task. We demonstrate the effectiveness of our approach by applying it to three framework-intensive applications and identifying previously unreported churn in a commercial system.", "num_citations": "3\n", "authors": ["545"]}
{"title": "Visualisation et analyse de logiciels de grande taille.\n", "abstract": " La visualisation par la m\u00e9taphore permet la repr\u00e9sentation et l\u2019analyse de logiciels \u00e0 objets de grande taille. Elle exploite les ressources des syst\u00e8mes visuel et cognitif humains afin d\u2019extraire de ces repr\u00e9sentations les r\u00e9gularit\u00e9s et les discontinuit\u00e9s qui sont les \u00e9l\u00e9ments de base de toute analyse qualitative. Pour mettre en \u0153uvre ces principes, nous pr\u00e9sentons dans cet article un syst\u00e8me de visualisation bas\u00e9e sur la perception instantan\u00e9e. Ce syst\u00e8me sert de base \u00e0 la m\u00e9taphore de la ville que nous utilisons pour conduire divers types d\u2019analyses.ABSTRACT. Metaphor-based visualization helps in the representation and analysis of large object-oriented software. It uses resources from human visual and cognitive systems to extract from these representations regularities and discontinuities that are the basic elements of any qualitative study. Based on these principles, we propose a preattentive visualization system. Within our system, we use the city metaphor to conduct various powerful analyses.", "num_citations": "3\n", "authors": ["545"]}
{"title": "Adaptation dynamique des syst\u00e8mes multi-agents bas\u00e9e sur le concept de m\u00e9ta-CATN.\n", "abstract": " This paper presents a dynamically adaptive agent model. This model is built on top of two main ideas: the CATN formalism and the meta-modeling approach. Indeed, this agent model includes two levels. The base level uses the CATN formalism to specify explicitly agent behaviors and interactions with other agents. The meta-level allows to interpret and possibly modify these specifications(behaviors and interactions). It also allows to verify the consistency of the whole system; a modification in one agent will be broadcasted to all the related agents by the meta-level and will imply necessary modifications in these ones. All these modifications can take effect immediately, which makes the multiagents systems dynamically evolvable. Moreover, the proposed model is reflexive in the sense that the meta-level itself can be described by the CATN formalism and then can be dynamically adapted. The multiagent systems\u00a0\u2026", "num_citations": "3\n", "authors": ["545"]}
{"title": "Un outil pour la conception de bases de donn\u00e9es \u00e0 objets\n", "abstract": " Sauf mention contraire ci-dessus, le contenu de cette notice bibliographique peut \u00eatre utilis\u00e9 dans le cadre d\u2019une licence CC BY 4.0 Inist-CNRS/Unless otherwise stated above, the content of this bibliographic record may be used under a CC BY 4.0 licence by Inist-CNRS/A menos que se haya se\u00f1alado antes, el contenido de este registro bibliogr\u00e1fico puede ser utilizado al amparo de una licencia CC BY 4.0 Inist-CNRS", "num_citations": "3\n", "authors": ["545"]}
{"title": "Fixing Multiple Type Errors in Model Transformations with Alternative Oracles to Test Cases\n", "abstract": " This paper addresses the issue of correcting type errors in model transformations in realistic scenarios where neither predefined patches nor behavior-safe guards such as test suites are available. Instead of using predefined patches targeting isolated errors of specific categories, we propose to explore the space of possible patches by combining basic edit operations for model transformation programs. To guide the search, we define two families of objectives: one to limit the number of type errors and the other to preserve the transformation behavior. To approximate the latter, we study two objectives: minimizing the number of changes and keeping the changes local. Additionally, we define four heuristics to refine candidate patches to increase the likelihood of correcting type errors while preserving the transformation behavior. We implemented our approach for the ATL language using the evolutionary algorithm NSGA-II, and performed an evaluation based on three published case studies. The evaluation results show that our approach was able to automatically correct on average more than82% of type errors for two cases and more than 56% for the third case.", "num_citations": "2\n", "authors": ["545"]}
{"title": "A vision for helping developers use APIs by leveraging temporal patterns\n", "abstract": " To achieve any meaningful task of a certain complexity, developers need to use APIs. Learning to use them is both time consuming and cognitively demanding. We propose to leverage a formal description of API usage as temporal patterns to help developers make sense of the complexities of working with APIs. To achieve this, we propose to deploy recommender systems at various points of the development process that make these patterns useful when most needed. In this paper, we illustrate the approach on a non trivial, real world running example from Android development. The example allows us to articulate a research agenda for leveraging API usage patterns during: (a) testing and compilation times by recommending potential violations of the patterns; (b) coding time by recommending API method calls; and (c) API delivery by recommending improvements to documentation.", "num_citations": "2\n", "authors": ["545"]}
{"title": "Injecting Social Diversity in Multi-objective Genetic Programming: The Case of Model Well-Formedness Rule Learning\n", "abstract": " Software modelling activities typically involve a tedious and time-consuming effort by specially trained personnel. This lack of automation hampers the adoption of the Model Driven Engineering (MDE) paradigm. Nevertheless, in the recent years, much research work has been dedicated to learn MDE artifacts instead of writing them manually. In this context, mono- and multi-objective Genetic Programming (GP) has proven being an efficient and reliable method to derive automation knowledge by using, as training data, a set of examples representing the expected behavior of an artifact. Generally, the conformance to the training example set is the main objective to lead the search for a solution. Yet, single fitness peak, or local optima deadlock, one of the major drawbacks of GP, remains when adapted to MDE and hinders the results of the learning. We aim at showing in this paper that an improvement in\u00a0\u2026", "num_citations": "2\n", "authors": ["545"]}
{"title": "Model matching for Model Transformation a meta-heuristic approach\n", "abstract": " Model Transformation By Example (MTBE) is a recent approach that derives model transformation rules from a source model, a target model, and matching between models. Building a match between models may be a complex task especially when models have been created or edited manually. In this paper, we propose an automated approach to generate mappings between source and target models. The novetly of our approach consists in the production of many-to-many mappings between the elements of the two models.", "num_citations": "2\n", "authors": ["545"]}
{"title": "Anomaly Detection and Quality Evaluation of Web Applications\n", "abstract": " This chapter addresses the problem of Web application quality assessment from two perspectives. First, it shows the use of model checking of properties formulated in LTL to detect anomalies in Web applications. Anomalies can be derived from standard quality principles or defined for a specific organization or application. The detection is performed on communicating automata models inferred from execution traces. Second, the chapter explains how probabilistic models (Bayesian networks) can be built and used to evaluate quality characteristics. The structure of the networks is defined by refinement of existing models, where the parameters (probabilities and probability tables) are set using expert judgment and fuzzy clustering of empirical data. The two proposed approaches are evaluated and a discussion on how they complement each other is presented.", "num_citations": "2\n", "authors": ["545"]}
{"title": "Automatic generation of strategies for visual anomaly detection\n", "abstract": " An important subset of design anomalies is difficult to detect automatically in the code because of the required knowledge. Fortunately, software visualization offers an efficient and flexible tool to inspect software data searching for such anomalies. However, as maintainers typically do not have a background in visualization, they often must seek assistance from visualization expert. We propose an approach based on taxonomies of low-level analytic tasks, interactive tasks, and perceptual rules to design an assistant that helps analysts to effectively use a visualization tool to accomplish detection tasks.", "num_citations": "2\n", "authors": ["545"]}
{"title": "A bayesian network to structure a data quality model for web portals.\n", "abstract": " The technological advances and the use of the internet have favoured the appearance of a great diversity of web applications, among them Web portals. Through them, organizations develop their businesses in a highly competitive environment. One decisive factor for this competitiveness is the assurance of its data quality. In previous works, a data quality model for Web portals has been developed. The model is represented as a matrix that links the user expectations of data web quality to the portal functionalities. Into this matrix a set of 34 attributes where classified. However, the quality attributes on this model have not an operational structure, necessary to be used actual assessment. In this paper we present how we have structured these attributes by means of a probabilistic approach, using Bayesian Networks. The final objective is to use the Bayesian network obtained for evaluating the quality of a data portal (or a subset of its characteristics).", "num_citations": "2\n", "authors": ["545"]}
{"title": "Visualization based Analysis of Quality for Largescale Software Systems\n", "abstract": " We propose an approach for complex software analysis based on visualization. Our work is motivated by the fact that in spite of years of research and practice, software development and maintenance are still time and resource consuming, and high-risk activities. The most important reason in our opinion is the complexity of many phenomena related to software, such as its evolution and its reliability. In fact, there is very little theory explaining them. Today, we have a unique opportunity to empirically study these phenomena, thanks to large sets of software data available through open-source programs and open repositories. Automatic analysis techniques, such as statistics and machine learning, are usually limited when studying phenomena with unknown or poorlyunderstood influence factors. We claim that hybrid techniques that combine automatic analysis with human expertise through visualization are excellent alternatives to them. In this paper, we propose a visualization framework that supports quality analysis of large-scale software systems. We circumvent the problem of size by exploiting perception capabilities of the human visual system.", "num_citations": "2\n", "authors": ["545"]}
{"title": "Visualisation and analysis of software quantitative data\n", "abstract": " We propose an approach for complex software analysis based on visualization. Our work is motivated by the fact that in spite of years of research and practice, software development and maintenance are still time and resource-consuming and high-risk activities [1, 5].Object-oriented and related technologies have improved significantly the ease of development and maintenance. Indeed, OO has considerably reduced the gap between user requirements and their implementation in software. In practice however, the cost and the risk remain high compared to the development and the maintenance of other manufactured products. Many reasons can explain these facts. The most important one in our opinion is that many phenomena related to software such as its evolution and its reliability are complex. There is very little theory explaining them. Having a good understanding and modeling of these phenomena are essential conditions to increase our control on the development and maintenance activities. In other scientific fields, similar situations are addressed using empirical research based on the classical cycle \u201cobservations, laws, validation, theory\u201d. Today, we have a unique opportunity to empirically study these phenomena. Indeed, large sets of software data are available through open source programs and open repositories [4]. In fact, the idea of empirically studying phenomena related to software in not new. Many studies were conducted especially by the community of software measurement and quality. However, in spite of the many quality estimation/prediction models published in the literature, concrete applications in industrial contexts are\u00a0\u2026", "num_citations": "2\n", "authors": ["545"]}
{"title": "Des signatures num\u00e9riques pour am\u00e9liorer la recherche structurelle de patrons.\n", "abstract": " Les patrons de conception orient\u00e9s-objets d\u00e9crivent de bonnes solutions \u00e0 des probl\u00e8mes r\u00e9currents de conception des programmes. Les solutions propos\u00e9es sont des motifs de conception que les concepteurs introduisent dans l\u2019architecture de leurs programmes. Il est important d\u2019identifier, pendant la maintenance, les motifs de conception utilis\u00e9s dans l\u2019architecture d\u2019un programme pour comprendre les probl\u00e8mes de conception r\u00e9solus et faire des modifications pertinentes au programme. L\u2019identification de micro-architectures similaires \u00e0 des motifs de conception est difficile \u00e0 cause du large espace de recherche, ie, les nombreuses combinaisons de classes possibles. Nous proposons une \u00e9tude exp\u00e9rimentale des classes jouant un r\u00f4le dans des motifs de conception avec des m\u00e9triques et un algorithme d\u2019apprentissage pour associer des signatures num\u00e9riques aux r\u00f4les dans les motifs de conception. Une signature num\u00e9rique est un ensemble de valeurs de m\u00e9triques qui caract\u00e9rise les classes jouant un r\u00f4le dans un motif de conception. Nous montrons que les signatures num\u00e9riques permettent de r\u00e9duire efficacement l\u2019espace de recherche des micro-architectures similaires \u00e0 des motifs de conception sur l\u2019exemple du patron de conception Composite et du programme JHOTDRAW.ABSTRACT. Design patterns describe good solutions to common and recurring problems in program design. The solutions are design motifs, which software engineers introduce in the architecture of their programs. It is important to identify the design motifs used in a program architecture to understand solved design problems and to make informed changes\u00a0\u2026", "num_citations": "2\n", "authors": ["545"]}
{"title": "Automatic detecting code cooperation\n", "abstract": " Software functionalities and behavior are accomplished by the cooperation of code artifacts. The understanding of this type of source code collaboration provides an important aid to the maintenance and evolution of legacy systems. However, the original collaboration design information is dispersed at the implementation level. The extraction of code artifacts' collaborations and the roles is therefore an important support in legacy software comprehension and design recovery. In this paper, we present a novel approach to automatically detect and analyze code collaborations and roles based on dynamic program analysis technique. We also demonstrate the tools that we have developed to support our approach and illustrate the viability of our approach in a case study.", "num_citations": "2\n", "authors": ["545"]}
{"title": "Combining and Adapting Software Quality Predictive Models\n", "abstract": " Object oriented (OO) design and programming have reached the maturity stage. OO software products are becoming more and more complex. Quality requirements are increasingly becoming determining factors in selecting from design alternatives during software development. Therefore, it is important that the quality of the software be evaluated during the different stages of the development.During the past ten years, a large number of quality models have been proposed in the literature. In general, the goal of these models is to predict a quality factor starting from a set of direct measures. The role of real software systems is crucial for building and/or validating these models. In most of the domains where predictive models are built (such as sociology, medicine, finance, and speech recognition) researchers are free to use large data repositories from which representative samples can be drawn. In the area of software engineering, however, such repositories are rare. The lack of data makes it hard to generalize, to cross-validate, and to reuse existing models. Since universal models do not exist, for a company, selecting an appropriate quality model is a difficult, non-trivial decision.\u00a1", "num_citations": "2\n", "authors": ["545"]}
{"title": "V. A comparison between using a neural network and a fuzzy regression system to predict the values of hydro power system variables\n", "abstract": " A COMPARISON BETWEEN USING A NEURAL NETWORK AND A FUZZY REGRESSION SYSTEM TO PREDICT THE VALUES OF HYDRO POWER SYSTEM VARIABLES Mounir Boukadoum1 , Hakim Lounis1 , Gang Mai2 , Houari Sahraoui2 and Vincent Siveton3 , 1 Department of Computer Science, University of Quebec at Montreal, Canada 2 Departement of Computer Science, University of Montreal, Canada 3 Centre de Recherche Informatique de Montr\u00e9al Abstract We compared the performance of an extended Elman neural network vs. that of a tree-based fuzzy regression system when using a database of historical hydrological data to predict the natural contributions flow in a hydroelectric power generation network. The neural network was trained with the Resilient Backpropagation (RPROP) algorithm and the fuzzy regression tree consisted of a new design where input fuzzification is accomplished by using \u2026", "num_citations": "2\n", "authors": ["545"]}
{"title": "Software Quality\n", "abstract": " The sections in this article are", "num_citations": "2\n", "authors": ["545"]}
{"title": "Vers un mod\u00e8le de pr\u00e9diction de la qualit\u00e9 du logiciel pour les syst\u00e8mes \u00e0 objets\n", "abstract": " La modularit\u00e9 est consid\u00e9r\u00e9e comme un crit\u00e8re important de la qualit\u00e9 du logiciel. Un produit logiciel est dit modulaire si ses composants pr\u00e9sentent un haut degr\u00e9 de coh\u00e9sion et un faible degr\u00e9 de couplage. Dans le cadre des applications orient\u00e9es objets (OO), il existe diff\u00e9rents types de couplages entre classes. Mesurer ces types de relations peut nous permettre de mieux comprendre le lien qui existe entre le couplage des classes et les attributs de qualit\u00e9. Dans cet article, notre but est de valider empiriquement l'hypoth\u00e8se selon laquelle certaines formes de couplage sont pr\u00e9judiciables \u00e0 la qualit\u00e9 du logiciel, en utilisant une m\u00e9thode statistique et une technique d'apprentissage symbolique.ABSTRACT: Modularity has been considered an important software product quality criterion. A software product is considered modular if its components exhibit a high cohesion and its components are weakly coupled. A module has high cohesion if all of its elements are related strongly. In the particular context of object oriented applications, there are different types of coupling between classes. Measuring this types of coupling can helps better understanding the relationship between class coupling and software quality attributes. In this paper, our goal is to validate empirically the hypothesis that some types of coupling have an impact on class error-proneness, using a statistical method and a machine learning technique.", "num_citations": "2\n", "authors": ["545"]}
{"title": "A metamodeling technique\n", "abstract": " We propose a technique for bridging the gap between the user's and the implementor's points of view in designing software. This technique relies heavily on OO technology and on rule based programming. It borrows part of its inspiration from the AI subfield of Knowledge Acquisition. This technique is implemented as a system now called M\u00c9TAGEN [Sahraoui94], written in Smalltalk-80. Several applications of M\u00c9TAGEN have been used to modelize information systems in public administration, to validate conceptual models by simulation for insurance companies, and to specify a risk management system. We are now customizing M\u00c9TAGEN in two directions: as a tool for implementing the INTERSEM project [Prud'homme92], which aims at building semantic interfaces for OO DBMS, and as a user support tool for the proper use of OO Application Frameworks [Johnson92]. The paper gives a summary of the technique, illustrated by a simple example, and offers a brief discussion of our approach. An extended version has been accepted for presentation at TOOLS-Europe'95 [Revault&al. 95].", "num_citations": "2\n", "authors": ["545"]}
{"title": "Program analysis using interactive and visual querying\n", "abstract": " We propose an interactive querying approach for program analysis and comprehension tasks. In our approach, an analyst uses a set of basic filters (information retrieval, structural, quantitative, and user selection) to define complex queries. These queries are built following an interactive and iterative process where basic filters are selected and executed, and their results displayed, changed, and combined using predefined operators.", "num_citations": "1\n", "authors": ["545"]}
{"title": "\u00c9tude de la changeabilit\u00e9 des syst\u00e8mes orient\u00e9s objet.\n", "abstract": " Plusieurs \u00e9tudes montrent qu\u2019avec le temps, la plupart des syst\u00e8mes deviennent difficiles \u00e0 maintenir et que leur croissance ralentit. Il existe cependant certains syst\u00e8mes qui utilisent les m\u00e9canismes fournis par le paradigme des objets pour soutenir un rythme de d\u00e9veloppement \u00e9lev\u00e9. Dans cet article, nous \u00e9tudions les facteurs qui affectent la changeabilit\u00e9 de quatre logiciels libres populaires. Deux applications et deux librairies ont \u00e9t\u00e9 s\u00e9lectionn\u00e9es, puis caract\u00e9ris\u00e9es avec des m\u00e9triques orient\u00e9es objet classiques. Ces informations ont \u00e9t\u00e9 utilis\u00e9es pour b\u00e2tir des mod\u00e8les de pr\u00e9diction de changement avec des techniques d\u2019apprentissage automatique. Dans le cas de deux librairies avec des mod\u00e8les de domaine suffisamment pr\u00e9cis, les mod\u00e8les pr\u00e9dictifs ont \u00e9t\u00e9 capables d\u2019estimer correctement le taux de changement dans le code. Dans le cas de deux applications, ces mod\u00e8les \u00e9taient beaucoup moins pr\u00e9cis, mais il a \u00e9t\u00e9 toutefois possible de pr\u00e9dire les changements dans les classes responsables des interfaces graphiques.", "num_citations": "1\n", "authors": ["545"]}
{"title": "D\u00e9tection visuelle d'anomalies de conception.\n", "abstract": " D\u00e9tection visuelle d\u2019anomalies de conception Page 1 D\u00e9tection visuelle d\u2019anomalies de conception Karim Dhambri, Salima Hassaine, Houari Sahraoui, Pierre Poulin 13 d\u00e9cembre 2007 1 Figures Fig. 1 \u2013 Une vue de Xerces avec le filtre de distribution appliqu\u00e9 pour la m\u00e9trique CBO. Note : les images de cet article doivent id\u00e9alement \u00eatre vues en couleur. Vous pouvez les trouver `a l\u2019adresse www.iro.umontreal.ca/~sahraouh/papers/LMO08 detection 1 Page 2 Fig. 2 \u2013 (Gauche) Repr\u00e9sentation de Xalan (1194 classes). (Droite) Un exemple de filtre de relation. Les classes ayant conserv\u00e9 leur couleur originale sont reli\u00e9es `a la classe verte par rapport `a la relation s\u00e9lectionn\u00e9e. Fig. 3 \u2013 Un exemple de classe mal plac\u00e9e d\u00e9couverte dans Art of Illusion. (Gauche) Mapping initial pour d\u00e9tecter la classe de r\u00f4le principal. (Droite) Filtre d\u2019association sur la classe encercl\u00e9e. Fig. 4 \u2013 Quelques exemples de d\u00e9composition \u2026", "num_citations": "1\n", "authors": ["545"]}
{"title": "Do software libraries evolve differently than applications?\n", "abstract": " More and more, developers use reusable components like libraries to produce high quality software systems. These systems need to satisfy not only the initial demands of their stakeholders, but they need to also offer support for future, changing requirements. While several studies have looked at the cost of modifying systems, there exists no work comparing if libraries evolve differently than applications. This study attempts to verify this assumption quantitatively.In this paper, we define design changes metrics to estimate the amount of high-level change required of individual classes and use metrics to describe their structure. These measures are then used as inputs in models capable of predicting code change. We used machine learning techniques to build these models and tested them on the evolution of industrial open-source systems. Two of the systems were libraries, and two were standalone applications. We found that while design changes are systematically correlated with code changes, structure metrics are better predictors of code change in libraries with well developed class hierarchies. With the two applications without this characteristic, structure alone was a poor predictor.", "num_citations": "1\n", "authors": ["545"]}
{"title": "A Proposal of a Probabilistic Framework for Web-Based Applications Quality\n", "abstract": " Many studies on quantitative evaluation of Web-based applications quality have proposed metrics, tools and models. Most of these studies do not address some key issues inherent to this field such as causality, uncertainty and subjectivity. In this paper, we propose a framework for assessing Web-based applications quality by using a probabilistic approach. The approach uses a model including most factors related to the evaluation of Web-based applications quality. A methodology regrouping these factors, integrating and extending various existing works in this field is proposed. A tool supporting our assessment methodology is developed. Some preliminary results are reported to demonstrate the effectiveness of our model.", "num_citations": "1\n", "authors": ["545"]}
{"title": "Caribou: a supporting environment for software e-development\n", "abstract": " With the rapid progress of Internet technology, more and more software projects adopt e-development to facilitate the software development process in a world wide context. However, collaborative software e-development activity itself is a complex orchestration. It involves many people working together without the barrier of time and space difference. Therefore, how to efficiently monitor and control software e-development in a global perspective becomes an important issue for any Internet-based software e-development project. In this paper, we present a novel approach to tackle this crucial issue by means of controlling e-development process and communication quality. Meanwhile, we also present our e-development supporting environment prototype: Caribou, to demonstrate the viability of our approach.", "num_citations": "1\n", "authors": ["545"]}
{"title": "Reengineering an Industrial Legacy Software Towards an Object-Oriented Knowledge-Based System\n", "abstract": " A software product is expected to fulfill some need and meet some acceptance standards that dictate the qualities it must have. This paper presents a reengineering work tending to increase to a significant degree some software qualities relevant in the management of production of a hydroelectric network. An object-oriented knowledge-based architecture is proposed to ensure an intelligent and automatic management of the knowledge in use in the daily decisional process of a major Canadian company.", "num_citations": "1\n", "authors": ["545"]}
{"title": "Quantitative approaches in object-oriented software engineering\n", "abstract": " The QAOOSE'2003 workshop brought together, for a full day, researchers and practitioners working on several aspects related to quantitative evaluation of software artifacts developed with the object-oriented paradigm. Ideas and experiences where shared and discussed. This report includes a summary of the technical presentations and subsequent discussions raised by them. Eleven out of twelve submitted position papers were presented, covering different aspects such as metrics formalization, new metrics (for coupling, cohesion, constraints or dynamic behavior) or metrics validation, to name a few. In the closing session the participants were able to discuss open issues and challenges arising from researching in this area, as well as they tried to forecast which will be the hot topics for research in the short to medium term.", "num_citations": "1\n", "authors": ["545"]}
{"title": "Toward the automatic assessment of evolvability for reusable class libraries\n", "abstract": " Many sources agree that managing the evolution of an OO system constitutes a complex and resource-consuming task. This is particularly true for reusable class libraries, as the user interface must be preserved to allow for version compatibility. Thus, the symptomatic detection of potential instabilities during the design phase of such libraries may serve to avoid later problems. This paper presents a fuzzy logic-based approach for evaluating the interface stability of a reusable class library, by using structural metrics as stability indicators.", "num_citations": "1\n", "authors": ["545"]}
{"title": "Impact of Complexity on Reusability in OO Systems.\n", "abstract": " It is widely recognized today that reuse reduces the costs of software development [1]. This reduction is the result of two factors:(1) developing new components is expensive, and (2) the reusable components are supposed to have been tested and thus are not expensive to maintain. Our concern is to automate the detection of potentially reusable components in existing systems. Our position is that some internal attributes like complexity can be good indicators of the possibility of reuse of a component. We present an experiment for verifying a hypothesis on the relationship between volume and complexity, and the reusability of existing components. We derived a set of related metrics to measure components\u2019 volume and complexity. This verification is done through a machine-learning approach (C4. 5 algorithm, windowing and cross-validation technique). Two kinds of results are produced:(1) a predictive model is\u00a0\u2026", "num_citations": "1\n", "authors": ["545"]}
{"title": "The METAGEN system\n", "abstract": " With the advent of object oriented database systems, there is an urgent need to define methodologies and tools for mapping conceptual schemes into object oriented ones. The evolution and the diversity of OO models on the one hand, and the increase of user needs on the other hand, are the two factors that make these tools require important flexibility properties, that is to say easiness in maintenance and extension.In this context, we present a technique for rapid prototyping of such tools in order to experiment there associated methodologies. Our technique is based on metamodeling principles. It relies heavily on OO technology and on rule based programming. It borrows part of its inspiration from the AI subfield of Knowledge Acquisition.", "num_citations": "1\n", "authors": ["545"]}