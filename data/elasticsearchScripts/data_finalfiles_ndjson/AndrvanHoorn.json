{"title": "Kieker: A framework for application performance monitoring and dynamic software analysis\n", "abstract": " Kieker is an extensible framework for monitoring and analyzing the runtime behavior of concurrent or distributed software systems. It provides measurement probes for application performance monitoring and control-flow tracing. Analysis plugins extract and visualize architectural models, augmented by quantitative observations. Configurable readers and writers allow Kieker to be used for online and offline analysis. This paper reviews the Kieker framework focusing on its features, its provided extension points for custom components, as well the imposed monitoring overhead.", "num_citations": "410\n", "authors": ["1907"]}
{"title": "Self-adaptive software system monitoring for performance anomaly localization\n", "abstract": " Autonomic computing components and services require continuous monitoring capabilities for collecting and analyzing data of runtime behavior. Particularly for software systems, a trade-off between monitoring coverage and performance overhead is necessary.", "num_citations": "90\n", "authors": ["1907"]}
{"title": "WESSBAS: Extraction of probabilistic workload specifications for load testing and performance prediction\u2014A model-driven approach for session-based application systems\n", "abstract": " The specification of workloads is required in order to evaluate performance characteristics of application systems using load testing and model-based performance prediction. Defining workload specifications that represent the real workload as accurately as possible is one of the biggest challenges in both areas. To overcome this challenge, this paper presents an approach that aims to automate the extraction and transformation of workload specifications for load testing and model-based performance prediction of session-based application systems. The approach (WESSBAS) comprises three main components. First, a system- and tool-agnostic domain-specific language (DSL) allows the layered modeling of workload specifications of session-based systems. Second, instances of this DSL are automatically extracted from recorded session logs of production systems. Third, these instances are transformed\u00a0\u2026", "num_citations": "75\n", "authors": ["1907"]}
{"title": "How is Performance Addressed in DevOps? A Survey on Industrial Practices\n", "abstract": " DevOps is a modern software engineering paradigm that is gaining widespread adoption in industry. The goal of DevOps is to bring software changes into production with a high frequency and fast feedback cycles. This conflicts with software quality assurance activities, particularly with respect to performance. For instance, performance evaluation activities---such as load testing---require a considerable amount of time to get statistically significant results.", "num_citations": "42\n", "authors": ["1907"]}
{"title": "Automatic Extraction of Probabilistic Workload Specifications for Load Testing Session-Based Application Systems\n", "abstract": " Workload generation is essential to systematically evaluate performance properties of application systems under controlled conditions, e.g., in load tests or benchmarks. The definition of workload specifications that represent the real workload as accurately as possible is one of the biggest challenges in this area. This paper presents our approach for the modeling and automatic extraction of probabilistic workload specifications for load testing session-based application systems. The approach, called WESSBAS, comprises (i.) a domain specific language (DSL) enabling layered modeling of workload specifications as well as support for (ii.) automatically extracting instances of the DSL from recorded sessions logs and (iii.) transforming instances of the DSL to workload specifications of existing load testing tools. During the extraction process, different groups of customers with similar navigational patterns are identified using clustering techniques. We developed corresponding tool support including a transformation to probabilistic test scripts for the Apache JMeter load testing tool. The evaluation of the proposed approach using the industry standard benchmark SPECjEnterprise2010 demonstrates its applicability and representativeness of the extracted workloads.", "num_citations": "42\n", "authors": ["1907"]}
{"title": "Performance simulation of runtime reconfigurable component-based software architectures\n", "abstract": " Architectural runtime reconfiguration is a promising means for controlling the quality of service (QoS) of distributed software systems. Particularly self-adaptation approaches rely on runtime reconfiguration capabilities provided by the systems under control. For example, our online capacity management approach SLAstic employs changing component deployments and server allocations to control the performance and resource efficiency of component-based (C-B) software systems at runtime.               In this context, we developed a performance simulator for runtime configurable\u00a0C-B software systems, called SLAstic.SIM. The system architectures to be simulated are specified as instances of the Palladio Component Model\u00a0(PCM). The simulation is driven by external workload traces and reconfiguration plans which can be requested during simulation, based on continuously accessible monitoring data of the\u00a0\u2026", "num_citations": "42\n", "authors": ["1907"]}
{"title": "Model-Driven Online Capacity Management for Component-Based Software Systems\n", "abstract": " Capacity management is a core activity when designing and operating distributed software systems. Particularly, enterprise application systems are exposed to highly varying workloads. Employing static capacity management, this leads to unnecessarily high total cost of ownership due to poor resource usage efficiency. This thesis introduces a model-driven online capacity management approach for distributed component-based software systems, called SLAstic. The core contributions of this approach are a) modeling languages to capture relevant architectural information about a controlled software system, b) an architecture-based online capacity management framework based on the common MAPE-K control loop architecture, c) model-driven techniques supporting the automation of the approach, d) architectural runtime reconfiguration operations for controlling a system\u2019s capacity, as well as e) an integration of the Palladio Component Model. A qualitative and quantitative evaluation of the approach is performed by case studies, lab experiments, and simulation.", "num_citations": "37\n", "authors": ["1907"]}
{"title": "Model-driven Generation of Microservice Architectures for Benchmarking Performance and Resilience Engineering Approaches\n", "abstract": " Microservice architectures are steadily gaining adoption in industrial practice. At the same time, performance and resilience are important properties that need to be ensured. Even though approaches for performance and resilience have been developed (eg, for anomaly detection and fault tolerance), there are no benchmarking environments for their evaluation under controlled conditions. In this paper, we propose a generative platform for benchmarking performance and resilience engineering approaches in microservice architectures, comprising an underlying metamodel, a generation platform, and supporting services for workload generation, problem injection, and monitoring.", "num_citations": "36\n", "authors": ["1907"]}
{"title": "DynaMod project: Dynamic analysis for model-driven software modernization\n", "abstract": " Our project DynaMod addresses model-driven modernization of software systems. Key characteristics of the envisioned approach are: (1) combining static and dynamic analysis for extracting models of a legacy system's architecture and usage profile; (2) augmenting these models with information that is relevant to the subsequent architecture-based modernization steps; and (3) automatically generating implementation artifacts and test cases based on the information captured in the models. This paper provides an overview of the DynaMod project.", "num_citations": "35\n", "authors": ["1907"]}
{"title": "Asking \"What\", Automating the \"How?\": The Vision of Declarative Performance Engineering\n", "abstract": " Over the past decades, various methods, techniques, and tools for modeling and evaluating performance properties of software systems have been proposed covering the entire software life cycle. However, the application of performance engineering approaches to solve a given user concern is still rather challenging and requires expert knowledge and experience. There are no recipes on how to select, configure, and execute suitable methods, tools, and techniques allowing to address the user concerns. In this paper, we describe our vision of Declarative Performance Engineering (DPE), which aims to decouple the description of the user concerns to be solved (performance questions and goals) from the task of selecting and applying a specific solution approach. The strict separation of\" what\" versus\" how\" enables the development of different techniques and algorithms to automatically select and apply a suitable\u00a0\u2026", "num_citations": "29\n", "authors": ["1907"]}
{"title": "Towards Automating Representative Load Testing in Continuous Software Engineering\n", "abstract": " As an application's performance can significantly impact the user satisfaction and, consequently, the business success, companies need to test performance before delivery. Though load testing allows for testing the performance under representative load by simulating user behavior, it typically entails high maintenance and execution overhead, hindering application in practice. With regard to the trend of continuous software engineering with its parallel and frequently executed delivery pipelines, load testing is even harder to be applied. In this paper, we present our vision of automated, context-specific and low-overhead load testing in continuous software engineering. First, we strive for reducing the maintenance overhead by evolving manual adjustments to generated workload models over a changing environment. Early evaluation results show a seamless evolution over changing user behavior. Building on this\u00a0\u2026", "num_citations": "25\n", "authors": ["1907"]}
{"title": "Exploiting DevOps practices for dependable and secure continuous delivery pipelines\n", "abstract": " Continuous delivery (CD) pipelines recently gained wide adoption. They provide means for short and high-frequent development cycles in DevOps by automating many steps after a commit has been issued and bringing it into production. CD pipelines have become essential for development and delivery. Hence, they are crucial and business-critical assets that need to be protected from harm in terms of dependability and security. DevOps practices like canary releasing and A/B testing aim to improve the quality of the software that is built by CD pipelines while keeping a high pace of development. Although CD is a part of DevOps, the DevOps practices have primarily been applied to the artifacts that are processed but not on the pipelines themselves. We outline our vision of using these DevOps practices to improve the dependability and security of CD pipelines. The goal is to detect, diagnose, and resolve\u00a0\u2026", "num_citations": "22\n", "authors": ["1907"]}
{"title": "Microservices: A Performance Tester\u2019s Dream or Nightmare?\n", "abstract": " In recent years, there has been a shift in software development towards microservice-based architectures, which consist of small services that focus on one particular functionality. Many companies are migrating their applications to such architectures to reap the benefits of microservices, such as increased flexibility, scalability and a smaller granularity of the offered functionality by a service. On the one hand, the benefits of microservices for functional testing are often praised, as the focus on one functionality and their smaller granularity allow for more targeted and more convenient testing. On the other hand, using microservices has their consequences (both positive and negative) on other types of testing, such as performance testing. Performance testing is traditionally done by establishing the baseline performance of a software version, which is then used to compare the performance testing results of later software\u00a0\u2026", "num_citations": "20\n", "authors": ["1907"]}
{"title": "A Framework for System Event Classification and Prediction by Means of Machine Learning\n", "abstract": " During operation, software systems produce large amounts of log events, comprising notifications of different severity from various hardware and software components. These data include important information that helps to diagnose problems in the system, eg, post-mortem root cause analysis. Manual processing of system logs after a problem occurred is a common practice. However, it is time-consuming and error-prone. Moreover, this way, problems are diagnosed after they occurred---even though the data may already include symptoms of upcoming problems.", "num_citations": "20\n", "authors": ["1907"]}
{"title": "Automated Source-Level Instrumentation for Dynamic Dependency Analysis of COBOL Systems\n", "abstract": " Dynamic analysis requires the instrumentation of application code with monitoring probes. This paper presents an approach to generate instrumentation artifacts from models augmented with analysis directives. Special emphasis is put on how to add monitoring instrumentation by means of aspect-oriented programming (AOP) to programs written in legacy languages.", "num_citations": "17\n", "authors": ["1907"]}
{"title": "MAMBA: A Measurement Architecture for Model-Based Analysis\n", "abstract": " Model-based measurement techniques are relevant in the field of software analysis. Several meta models for the specification of quantitative measures have been proposed. However, they often focus either on static or dynamic aspects of a software system. Nevertheless, considering reengineering activities often both dimensions reveal valuable complementary insights. Existing meta-models are also frequently bound to specific modeling languages, redefine underlying concepts for any new meta model, or provide only limited tool support for the automated computation of measurements from modeled measures.     We present MAMBA, an integrated measurement architecture for model-based analysis---both static and dynamic---of software systems, that can be specified by arbitrary Ecore-based modeling languages. MAMBA extends the Structured Metrics Meta-Model (SMM) by additional modeling features, such as arbitrary statistical aggregate functions and periodic aggregate functions, e.g., for dynamic analysis at runtime. To consider measurements for querying system models, we outline the MAMBA Query Language (MQL) that employs SMM measures. Furthermore, we provide tool support that applies the measures specified in an (extended) SMM model and can integrate raw measurements provided by arbitrary static and dynamic analysis tools to produce the desired measurement model.   We demonstrate the applicability of the approach based on three evaluation scenarios from different contexts: migration of software systems into the cloud, model-based engineering of railway control systems, and dynamic analysis for model-driven\u00a0\u2026", "num_citations": "17\n", "authors": ["1907"]}
{"title": "Automatic Extraction of Session-Based Workload Specifications for Architecture-Level Performance Models\n", "abstract": " Workload specifications are required in order to accurately evaluate performance properties of session-based application systems. These properties can be evaluated using measurement-based approaches such as load tests and model-based approaches, eg, based on architecture-level performance models. Workload specifications for both approaches are created separately from each other which may result in different workload characteristics. To overcome this challenge, this paper extends our existing WESSBAS approach which defines a domain-specific language (WESSBAS-DSL) enabling the layered modeling and automatic extraction of workload specifications, as well as the transformation into load test scripts. In this paper, we extend WESSBAS by the capability of transforming WESSBAS-DSL instances into workload specifications of architecture-level performance models. The transformation\u00a0\u2026", "num_citations": "16\n", "authors": ["1907"]}
{"title": "MAMBA: Model-Based Software Analysis Utilizing OMG\u2019s SMM\n", "abstract": " Most software system properties can be quantified through applying measurement processes. OMG's Structured Metrics Meta-Model (SMM) supports the meta-model agnostic definition of those measurement processes with an emphasis on architecture-driven modernization scenarios. We present the MAMBA framework that addresses major obstacles software engineers currently face when using SMM in practice. Among those are (1) the lack of appropriate tool support, (2) the cumbersome integration of precomputed measurement data, and (3) the complexity of specifying SMM models and queries.", "num_citations": "16\n", "authors": ["1907"]}
{"title": "Model-driven instrumentation for dynamic analysis of legacy software systems\n", "abstract": " Dynamic analysis requires the instrumentation of application code with monitoring probes. This paper presents an approach to generate instrumentation artifacts from models augmented with analysis directives. Special emphasis is put on how to add monitoring instrumentation by means of aspect-oriented programming (AOP) to programs written in legacy languages.", "num_citations": "16\n", "authors": ["1907"]}
{"title": "Co-Evolution of Software Architecture and Fault Tree Models: An Explorative Case Study on a Pick and Place Factory Automation System\n", "abstract": " Safety-critical systems are subject to rigorous safety analyses, eg, hazard analyses. Fault trees are a deductive technique to derive the combination of faults which cause a hazard. There is a tight relationship between fault trees and system architecture as the components contain the faults and the component structure influences the fault combinations. In this paper, we describe an explorative case study on multiple evolution scenarios of a factory automation system. We report on the evolution steps on the system architecture models and fault trees and how the evolution steps in the different models relate to each other.", "num_citations": "15\n", "authors": ["1907"]}
{"title": "Kieker: A monitoring framework for software engineering research\n", "abstract": " Application-level monitoring and dynamic analysis of software systems are a basis for various tasks in software engineering research, such as performance evaluation and reverse engineering. The Kieker framework provides monitoring, analysis, and visualization support for these purposes. It commenced in 2006, and grew toward a high-quality open-source software that has been employed in a variety of software engineering research projects over the last decade. Several research groups constitute the open-source community to advance the Kieker framework. In this paper, we review Kieker\u2019s history, development, and impact both in research and technology transfer with industry.", "num_citations": "13\n", "authors": ["1907"]}
{"title": "Vulnerabilities in Continuous Delivery Pipelines? A Case Study\n", "abstract": " More and more companies are in the process of adopting modern continuous software development practices and approaches like continuous integration (CI), continuous delivery (CD), or DevOps. These approaches can support companies in order to increase the development speed, the frequency of product increments, and the time to market. To be able to get these advantages, especially the tooling and infrastructure need to be reliable and secure. In case CI/CD is compromised or even unavailable, all mentioned advantages are at stake. Potentially, this could also even hinder the forthcoming of the software development. Therefore, our goal was to identify which vulnerabilities are present in industry CD pipelines and how they can be detected. In this paper, we present our results of an industry case study which includes a qualitative survey of agile project teams regarding the awareness of security in CI/CD, the\u00a0\u2026", "num_citations": "12\n", "authors": ["1907"]}
{"title": "Reducing the maintenance effort for parameterization of representative load tests using annotations\n", "abstract": " Directly affecting the user experience, performance is a crucial aspect of today's software applications. Representative load testing allows to effectively test and preserve the performance before delivery by mimicking the actually expected workload. In the literature, various approaches have been proposed for extracting representative load tests from recorded user sessions. However, these approaches require manual parameterization for specifying input data and adjusting static properties such as a request's domain name. This manual effort accumulates when load tests need to be updated due to changing production workloads and APIs. In this paper, we address the reduction of the maintenance effort for representative load testing. We introduce input data and properties annotations (IDPAs) that store manual parameterizations and can be evolved automatically. Experts only have to parameterize extracted load\u00a0\u2026", "num_citations": "10\n", "authors": ["1907"]}
{"title": "Utility-Based Decision Making for Migrating Cloud-Based Applications\n", "abstract": " Nowadays, cloud providers offer a broad catalog of services for migrating and distributing applications in the cloud. However, the existence of a wide spectrum of cloud services has become a challenge for deciding where to host applications, as these vary in performance and cost. This work addresses such a challenge, and provides a utility-based decision support model and method that evaluates and ranks during design time potential application distributions spanned among heterogeneous cloud services. The utility model is evaluated using the MediaWiki (Wikipedia) application, and shows an improved efficiency for selecting cloud services in comparison to other decision making approaches.", "num_citations": "10\n", "authors": ["1907"]}
{"title": "ORCAS: Efficient Resilience Benchmarking of Microservice Architectures\n", "abstract": " Resilience benchmarking aims to assess a software system's and an organization's ability to cope with failures, e.g., by injecting faults and observing their effects in both, testing and production environments. However, existing resilience benchmarks are ad-hoc and based on randomly injected faults. In this paper, we give an overview of the vision and the current state of our ORCAS approach for a more efficient resilience benchmarking for microservice architectures. ORCAS leverages the following characteristics: i) relationship between resilience patterns, antipatterns, and fault injections; ii) automatically extracted architectural knowledge to generate and refine resilience benchmarks; iii) use of simulations to further reduce the number of benchmarks to execute in testing and production systems.", "num_citations": "10\n", "authors": ["1907"]}
{"title": "Towards Adaptive Monitoring of Java EE Applications\n", "abstract": " Continuous monitoring of software systems under production workload provides valuable data about application runtime behavior and usage. An adaptive monitoring infrastructure allows to control, for instance, the overhead as well as the granularity and quality of collected data at runtime. Focusing on application-level monitoring, this paper presents how we extended the monitoring framework Kieker by reconfiguration capabilities based on JMX technology. The extension allows to change the instrumentation of software operations in monitored distributed Java EE applications. As a proof-of-concept, we demonstrate the adaptive monitoring of a distributed sample Java EE application deployed to a JBoss application server.", "num_citations": "10\n", "authors": ["1907"]}
{"title": "Modeling complex user behavior with the palladio component model\n", "abstract": " The specification of workloads is required in order to evaluate performance characteristics of application systems using performance prediction approaches like the Palladio Component Model (PCM). One of the biggest challenges in workload modeling is to ensure that the modeled user behavior adequately resembles the real user behavior. However, PCM offers limited support to model such complex user behavior. Furthermore, reusing modeled activities is not possible. To overcome these limitations, workarounds are required. In order to avoid these workarounds, we extend the meta-model of the PCM Usage Model. We evaluate the extended PCM Usage Model by integrating it into our previous work on automatic extraction of workload specifications. Based on HTTP web logs, recorded from the standard industry benchmark SPECjEnterprise2010, instances of a domain-specific language (DSL) for modeling workload specifications are extracted. Afterwards, these instances are transformed to the extended PCM Usage Model. The evaluation shows that workload characteristics of the simulated workload match the measured workload with high accuracy.", "num_citations": "9\n", "authors": ["1907"]}
{"title": "Model-Driven Load and Performance Test Engineering in DynaMod\n", "abstract": " Defining representative workloads, involving workload intensity and service calls within user sessions, is a core requirement for meaningful performance testing. This paper presents the approach for obtaining representative workload models from production systems that has been developed in the DynaMod project for model-driven software modernization.", "num_citations": "8\n", "authors": ["1907"]}
{"title": "Agile Scalability Engineering: The ScrumScale Method\n", "abstract": " Scalability is a property that must be carefully designed into a system. A case study in the largest Norwegian public portal, Altinn, illustrates how developers and scalability experts improved scalability and spent less time during scalability testing. With minor adjustments to an agile development process, stakeholders spend more up-front time together.", "num_citations": "7\n", "authors": ["1907"]}
{"title": "DynaMod: Dynamische Analyse f\u00fcr modellgetriebene Software-Modernisierung\n", "abstract": " Erfolgreiche Softwaresysteme leben lange. Gleichzeitig sind diese jedoch der enormen Geschwindigkeit der Fortentwicklung der technischen Komponenten und Plattformen unterworfen, so dass die Anwendungen technisch sehr schnell altern. Von dieser Alterung sind jedoch nicht nur Programmiertechniken betroffen, sondern auch die Softwarearchitekturen erodieren sehr schnell. Um dieser Alterung entgegenzuwirken, neue technologische Potentiale zu nutzen und auch auf zuk\u00fcnftige Anforderungen flexibel reagieren zu k\u00f6nnen, ist eine kontinuierliche Modernisierung von Softwaresystemen erforderlich.Bei der Neuentwicklung von Softwaresystemen hat sich mit der Modellgetriebenen Softwareentwicklung (Model-Driven Software Development, MDSD) ein Konzept etabliert, das eine elegante L\u00f6sung dieser Problematik bietet: Anstatt das System vollst\u00e4ndig in einer technischen Programmiersprache zu entwickeln, werden fachliche Aspekte mittels geeigneter, abstrakter Modellierungssprachen dargestellt. Hierbei handelt es sich oftmals um sogenannte dom\u00e4nenspezifische Sprachen (Domain Specific Languages, DSLs), die speziell auf die betreffende Anwendungsdom\u00e4ne zugeschnitten sind und dadurch eine knappe und pr\u00e4zise Formulierung der relevanten Sachverhalte erm\u00f6glichen. Die Uberf\u00fchrung dieser abstrakten Modelle in technische Artefakte, beispielsweise Quellcode in einer Programmiersprache, wird automatisiert durch Codegeneratoren vorgenommen. Auf diese Weise ist es m\u00f6glich, durch Anpassung der Generatoren die Implementierung der Modelle zu ver\u00e4ndern, ohne Modifikationen an den zugrundeliegenden Modellen\u00a0\u2026", "num_citations": "5\n", "authors": ["1907"]}
{"title": "Context-tailored Workload Model Generation for Continuous Representative Load Testing\n", "abstract": " Load tests evaluate software quality attributes, such as performance and reliability, by eg, emulating user behavior that is representative of the production workload. Existing approaches extract workload models from recorded user requests. However, a single workload model cannot reflect the complex and evolving workload of today's applications, or take into account workload-influencing contexts, such as special offers, incidents, or weather conditions. In this paper, we propose an integrated framework for generating load tests tailored to the context of interest, which a user can describe in a language we provide. The framework applies multivariate time series forecasting for extracting a context-tailored load test from an initial workload model, which is incrementally learned by clustering user sessions recorded in production and enriched with relevant context information.", "num_citations": "4\n", "authors": ["1907"]}
{"title": "Identifying and prioritizing chaos experiments by using established risk analysis techniques\n", "abstract": " The prevalence of microservice architectures and container orchestration technologies increases the complexity of assessing such systems\u2019 resilience. Chaos engineering is an emerging approach for resilience assessment by testing hypotheses after intentionally injecting faults into a distributed system and observing customer- and business-affecting metrics. As the number of potential risks within a complex system is high, the identification and prioritization of effective and efficient chaos experiments are non-trivial. In the scope of an industrial case study, this work investigates means to identify and prioritize chaos experiments by using established risk analysis techniques known from engineering safety-critical systems, namely i) Fault Tree Analysis, ii) Failure Mode and Effects Analysis, iii) and Computer Hazard and Operability Study. We conducted semi-structured interviews to elicit architectural information and\u00a0\u2026", "num_citations": "4\n", "authors": ["1907"]}
{"title": "Performance Engineering for Microservices and Serverless Applications: The RADON Approach\n", "abstract": " Microservices and serverless functions are becoming integral parts of modern cloud-based applications. Tailored performance engineering is needed for assuring that the applications meet their requirements for quality attributes such as timeliness, resource efficiency, and elasticity. A novel DevOps-based framework for developing microservices and serverless applications is being developed in the RADON project. RADON contributes to performance engineering by including novel approaches for modeling, deployment optimization, testing, and runtime management. This paper summarizes the contents of our tutorial presented at the 11th ACM/SPEC International Conference on Performance Engineering (ICPE).", "num_citations": "3\n", "authors": ["1907"]}
{"title": "The Back End is Only One Part of the Picture: Mobile-Aware Application Performance Monitoring and Problem Diagnosis\n", "abstract": " The success of modern businesses relies on the quality of their supporting application systems. Continuous application performance management is mandatory to enable efficient problem detection, diagnosis, and resolution during production. In today's age of ubiquitous computing, large fractions of users access application systems from mobile devices, such as phones and tablets. For detecting, diagnosing, and resolving performance and availability problems, an end-to-end view, ie, traceability of requests starting on the (mobile) clients' devices, is becoming increasingly important. In this paper, we propose an approach for end-to-end monitoring of applications from the users' mobile devices to the back end, and diagnosing root-causes of detected performance problems. We extend our previous work on diagnosing performance anti-patterns from execution traces by new metrics and rules. The evaluation of this\u00a0\u2026", "num_citations": "3\n", "authors": ["1907"]}
{"title": "Adaptive Instrumentation of Java Applications for Experiment-Based Performance Analysis\n", "abstract": " Running load tests, instrumentation of selected application parts is a common practice in order to measure performance metrics. Instrumentation means to extend the target application by measurement probes while executing measurements. For instance, in Java bytecode instrumentation, additional commands are inserted into the bytecode of the target application. Since data generation is time-consuming, it may affect the target application and thus the measurement. Countering this problem, stepwise approaches execute several measurements while using only few measurement probes per measurement. Utilizing existing approaches, the target application has to be restarted in order to change the instrumentation. The resulting measurement overhead can cause the execution of stepwise measurements to be impracticable or bound to high manual effort. In this presentation, we introduce the Adaptable Instrumentation and Monitoring (AIM) framework enabling stepwise measurements without system restarts. Furthermore, we show the advantages of selective instrumentation with AIM over excessive instrumentations. For instance, we introduce an approach to highly precise and fully automated performance-model calibration. Thereby, the relative error is smaller than 4%, whereas excessive instrumentations introduce an error of up to 50%. Last not least, we present the embedding of AIM in the Kieker framework, merging the adaptability of AIM with the comprehensive monitoring and analysis capabilities of Kieker.", "num_citations": "3\n", "authors": ["1907"]}
{"title": "Adaptive Capacity Management for the Resource-Efficient Operation of Component-Based Software Systems\n", "abstract": " Overprovisioning capacity management for application service provision causes underutilized computing resources during low or medium workload periods. This paper gives an overview of our work in progress aiming for improving the resource efficiency in operating large component-based software systems that are exposed to highly varying workloads. Based on continuously updated architectural runtime models of the application and its deployment environment, the number of allocated computing resources as well as the deployment of the software components are automatically adapted with respect to current demands and specified performance requirements.", "num_citations": "3\n", "authors": ["1907"]}
{"title": "Application performance management: measuring and optimizing the digital customer experience\n", "abstract": " Nowadays, the success of most companies is determined by the quality of their IT services and application systems. To make sure that application systems provide the expected quality of service, it is crucial to have up-to-date information about the system and the user experience to detect problems and to be able to solve them effectively. Application performance management (APM) is a core IT operations discipline that aims to achieve an adequate level of performance during operations. APM comprises methods, techniques, and tools for i) continuously monitoring the state of an applications system and its usage, as well as for ii) detecting, diagnosing, and resolving performance-related problems using the monitored data. This book provides an introduction by covering a common conceptual foundation for APM. On top of the common foundation, we introduce today's tooling landscape and highlight current challenges and directions of this discipline.", "num_citations": "2\n", "authors": ["1907"]}
{"title": "diagnoseIT: Expertengest\u00fctzte automatische Diagnose von Performance-Problemen in Enterprise-Anwendungen (Abschlussbericht)\n", "abstract": " This is the final report of the collaborative research project diagnoseIT on expert-guided automatic diagnosis of performance problems in enterprise applications.", "num_citations": "2\n", "authors": ["1907"]}
{"title": "Open-source software as catalyzer for technology transfer: Kieker's development and lessons learned\n", "abstract": " The monitoring framework Kieker commenced as a joint diploma thesis of the University of Oldenburg and a telecommunication provider in 2006, and grew toward a high-quality open-source project during the last years. Meanwhile, Kieker has been and is employed in various projects. Several research groups constitute the open-source community to advance the Kieker framework. In this paper, we review Kieker's history, development, and impact as catalyzer for technology transfer.", "num_citations": "2\n", "authors": ["1907"]}
{"title": "OPAD: Online Performance Anomaly Detection with Kieker\n", "abstract": " OPAD: Online Performance Anomaly Detection with Kieker Page 1 \u0398PAD: Online Performance Anomaly Detection with Tillmann Bielefeld1 and Andr\u00e9 van Hoorn2 1 empuxa GmbH, Kiel 2 Software Engineering Group, Kiel University 7th Hamburg Web Performance Meetup October 24, 2012 @ Microsoft, Hamburg T. Bielefeld (empuxa) and A. van Hoorn (CAU) \u0398PAD w/ Kieker Oct. 24, 2012 @ Hamburg 1 / 27 Page 2 Motivation: Monitoring/Dynamic Analysis Kieker: Framework Overview T. Bielefeld (empuxa) and A. van Hoorn (CAU) \u0398PAD w/ Kieker Oct. 24, 2012 @ Hamburg 2 / 27 Page 3 Motivation: Monitoring/Dynamic Analysis Kieker: Framework Overview ? Does the searchBook service respond in <= 0.5 seconds in 95% of all cases? T. Bielefeld (empuxa) and A. van Hoorn (CAU) \u0398PAD w/ Kieker Oct. 24, 2012 @ Hamburg 2 / 27 Page 4 Motivation: Monitoring/Dynamic Analysis Kieker: Framework Overview ? \u2026", "num_citations": "2\n", "authors": ["1907"]}
{"title": "Workload-sensitive Timing Behavior Anomaly Detection in Large Software Systems\n", "abstract": " Anomaly detection is used for failure detection and diagnosis in large software systems to reduce repair time, thus increasing availability. A common approach is building a model of a system's normal behavior in terms of monitored parameters, and comparing this model with a dynamically generated model of the respective current behavior. Deviations are considered anomalies, indicating failures. Most anomaly detection approaches do not explicitly consider varying workload. Assuming that varying workload leads to varying response times of services provided by internal software components, our hypothesis is as follows: a novel workload-sensitive anomaly detection is realizable, using statistics of workload-dependent service response times as a model of the normal behavior. The goals of this work are divided into three parts. First, an application-generic technique will be developed to model and generate realistic workload based on an analytical workload model notation to be specified. Applying this technique to a sample application in a case study, the relation between workload intensity and response times will be statistically analyzed. Based on this, a workload-sensitive anomaly detection prototype will be implemented.", "num_citations": "2\n", "authors": ["1907"]}
{"title": "Scenario-based resilience evaluation and improvement of microservice architectures: An experience report\n", "abstract": " Context. Microservice-based architectures are expected to be resilient. However, various systems still suffer severe quality degradation from changes, eg, service failures or workload variations. Problem. In practice, the elicitation of resilience requirements and the quantitative evaluation of whether the system meets these requirements is not systematic or not even conducted. Objective. We explore (1) the scenario-based Architecture Trade-Off Analysis Method (ATAM) for resilience requirement elicitation and (2) resilience testing through chaos experiments for architecture assessment and improvement.Method. In an industrial case study, we design a structured ATAM-based workshop, including the system\u2019s stakeholders, to elicit resilience requirements. We specify these requirements into the ATAM scenario template. We transform those scenarios into resilience experiments to quantitatively evaluate and improve system resilience. Result. We identified 12 resilience scenarios. We use and extend ChaosToolkit to automate and execute two scenarios. We quantitatively evaluate resilience requirements and suggest resilience improvements in the scope of both scenarios. We share lessons learned from the case study. In particular, our work provides evidence that an ATAM-based workshop is intuitive to stakeholders in an industrial setting. Conclusion. Our approach helps requirement and quality engineering teams in the process of resilience requirements elicitation.", "num_citations": "1\n", "authors": ["1907"]}
{"title": "SQuAT-Vis: visualization and interaction in software architecture optimization\n", "abstract": " Optimization of software architectures is a complex task that can not be fully automated. For this reason, software architecture optimization approaches often require human architects to participate in the optimization process, e.g., by selecting architectural candidates. Nevertheless, most of these approaches fail to support architects in solving their tasks as they provide no or insufficient visualization and interaction techniques. Thus, architects usually have to invest time and effort to find a (not ideal) solution themselves. In this paper, we present SQuAT-Vis \u2014 a tool that can be plugged into software architecture optimization approaches and allows architects to investigate (intermediate) results visually. SQuAT-Vis has been developed based on four common use cases in the domain and to be compatible with the technologies used by SQuAT, a state-of-the-art software architecture optimization approach. Nevertheless\u00a0\u2026", "num_citations": "1\n", "authors": ["1907"]}
{"title": "Representative Load Testing in Continuous Software Engineering: Automation and Maintenance Support\n", "abstract": " This extended abstract summarizes our work on reducing the maintenance effort for the parameterization of representative load tests using annotations, which we have published in the Journal of Software Testing, Verification & Reliability in 2019.", "num_citations": "1\n", "authors": ["1907"]}
{"title": "Many Flies in One Swat: Automated Categorization of Performance Problem Diagnosis Results\n", "abstract": " As the importance of application performance grows in modern enterprise systems, many organizations employ application performance management (APM) tools to help them deal with potential performance problems during production. In addition to monitoring capabilities, these tools provide problem detection and alerting. In large enterprise systems these tools can report a very large number of performance problems. They have to be dealt with individually, in a time-consuming and error-prone manual process, even though many of them have a common root cause. In this vision paper, we propose using automatic categorization for dealing with large numbers of performance problems reported by APM tools. This leads to the aggregation of reported problems, reducing the work required for resolving them. Additionally, our approach opens the possibility of extending the analysis approaches to use this information\u00a0\u2026", "num_citations": "1\n", "authors": ["1907"]}
{"title": "Online Capacity Management for Increased Resource Efficiency of Software Systems\n", "abstract": " Resource efficiency is an increasingly important internal quality attribute of software systems. While the related attribute performance is mainly concerned with metrics quantifying timing behavior and resource usage characteristics, resource efficiency is a measure of a system\u2019s resource usage economy. Many software systems are exposed to varying workload conditions significantly influencing its timing behavior. However, the capacity of those systems is typically managed in a static and pessimistic way, causing temporarily underutilized resources, eg, application servers, during medium or low workload periods.SLAstic, the self-adaptive approach for online capacity management developed in this work, aims to increase the resource efficiency of distributed component-based software systems employing architectural runtime reconfiguration. A software system is equipped with reconfiguration capabilities that allow to control a system\u2019s performance and efficiency properties at runtime in an elastic way, eg, by migrating and (de-) replicating software components, and (de-) allocating server nodes. Architectural models play an important role in the approach since they are used to specify the system assembly, deployment, instrumentation, reconfiguration capabilities, performance properties etc. At runtime, these models are continuously updated and used for online quality-of-service evaluation, eg, workload forecasting and performance prediction, in order to determine required adaptations and to select appropriate reconfiguration plans. A prototype implementation of our adaptation framework [1] is used to quantitatively evaluate the approach by\u00a0\u2026", "num_citations": "1\n", "authors": ["1907"]}