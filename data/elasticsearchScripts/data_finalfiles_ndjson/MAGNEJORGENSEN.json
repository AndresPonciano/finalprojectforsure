{"title": "A review of studies on expert estimation of software development effort\n", "abstract": " This paper provides an extensive review of studies related to expert estimation of software development effort. The main goal and contribution of the review is to support the research on expert estimation, e.g., to ease other researcher\u2019s search for relevant expert estimation studies. In addition, we provide software practitioners with useful estimation guidelines, based on the research-based knowledge of expert estimation processes. The review results suggest that expert estimation is the most frequently applied estimation strategy for software projects, that there is no substantial evidence in favour of use of estimation models, and that there are situations where we can expect expert estimates to be more accurate than formal estimation models. The following 12 expert estimation \u201cbest practice\u201d guidelines are evaluated through the review: (1) evaluate estimation accuracy, but avoid high evaluation pressure; (2) avoid\u00a0\u2026", "num_citations": "757\n", "authors": ["155"]}
{"title": "A review of software surveys on software effort estimation\n", "abstract": " This paper summarizes estimation knowledge through a review of surveys on software effort estimation. Main findings were that: (1) most projects (60-80%) encounter effort and/or schedule overruns. The overruns, however, seem to be lower than the overruns reported by some consultancy companies. For example, Standish Group's \"Chaos Report\" describes an average cost overrun of 89%, which is much higher than the average overruns found in other surveys, i.e. 3040%. (2) The estimation methods in most frequent use of expert judgment is that there is no evidence that formal estimation models lead to more accurate estimates. (3) There is a lack of surveys including extensive analyses of the reasons for effort and schedule overruns.", "num_citations": "548\n", "authors": ["155"]}
{"title": "Experience with the accuracy of software maintenance task effort prediction models\n", "abstract": " The paper reports experience from the development and use of eleven different software maintenance effort prediction models. The models were developed applying regression analysis, neural networks and pattern recognition and the prediction accuracy was measured and compared for each model type. The most accurate predictions were achieved applying models based on multiple regression and on pattern recognition. We suggest the use of prediction models as instruments to support the expert estimates and to analyse the impact of the maintenance variables on the maintenance process and product. We believe that the pattern recognition based models evaluated, i.e., the prediction models based on the Optimized Set Reduction method, show potential for such use.< >", "num_citations": "366\n", "authors": ["155"]}
{"title": "How large are software cost overruns? A review of the 1994 CHAOS report\n", "abstract": " The Standish Group reported in their 1994 CHAOS report that the average cost overrun of software projects was as high as 189%. This figure for cost overrun is referred to frequently by scientific researchers, software process improvement consultants, and government advisors. In this paper, we review the validity of the Standish Group's 1994 cost overrun results. Our review is based on a comparison of the 189% cost overrun figure with the cost overrun figures reported in other cost estimation surveys, and an examination of the Standish Group's survey design and analysis methods. We find that the figure reported by the Standish Group is much higher than those reported in similar estimation surveys and that there may be severe problems with the survey design and methods of analysis, e.g. the population sampling method may be strongly biased towards \u2018failure projects\u2019. We conclude that the figure of 189% for\u00a0\u2026", "num_citations": "332\n", "authors": ["155"]}
{"title": "Forecasting of software development work effort: Evidence on expert judgement and formal models\n", "abstract": " The review presented in this paper examines the evidence on the use of expert judgement, formal models, and a combination of these two approaches when estimating (forecasting) software development work effort. Sixteen relevant studies were identified and reviewed. The review found that the average accuracy of expert judgement-based effort estimates was higher than the average accuracy of the models in ten of the sixteen studies. Two indicators of higher accuracy of judgement-based effort estimates were estimation models not calibrated to the organization using the model, and important contextual information possessed by the experts not included in the formal estimation models. Four of the reviewed studies evaluated effort estimates based on a combination of expert judgement and models. The mean estimation accuracy of the combination-based methods was similar to the best of that of the other\u00a0\u2026", "num_citations": "238\n", "authors": ["155"]}
{"title": "Practical guidelines for expert-judgment-based software effort estimation\n", "abstract": " This article presents seven guidelines for producing realistic software development effort estimates. The guidelines derive from industrial experience and empirical studies. While many other guidelines exist for software effort estimation, these guidelines differ from them in three ways: 1) They base estimates on expert judgments rather than models. 2) They are easy to implement. 3) They use the most recent findings regarding judgment-based effort estimation. Estimating effort on the basis of expert judgment is the most common approach today, and the decision to use such processes instead of formal estimation models shouldn't be surprising. Simple process changes such as reframing questions can lead to more realistic estimates of software development efforts.", "num_citations": "188\n", "authors": ["155"]}
{"title": "When 90% confidence intervals are 50% certain: On the credibility of credible intervals\n", "abstract": " Estimated confidence intervals for general knowledge items are usually too narrow. We report five experiments showing that people have much less confidence in these intervals than dictated by the assigned level of confidence. For instance, 90% intervals can be associated with an estimated confidence of 50% or less (and still lower hit rates). Moreover, interval width appears to remain stable over a wide range of instructions (high and low numeric and verbal confidence levels). This leads to a high degree of overconfidence for 90% intervals, but less for 50% intervals or for free choice intervals (without an assigned degree of confidence). To increase interval width one may have to ask exclusion rather than inclusion questions, for instance by soliciting \u2018improbable\u2019 upper and lower values (Experiment 4), or by asking separate \u2018more than\u2019 and \u2018less than\u2019 questions (Experiment 5). We conclude that interval width and\u00a0\u2026", "num_citations": "158\n", "authors": ["155"]}
{"title": "A comparison of software project overruns-flexible versus sequential development models\n", "abstract": " Flexible software development models, e.g., evolutionary and incremental models, have become increasingly popular. Advocates claim that among the benefits of using these models is reduced overruns, which is one of the main challenges of software project management. This paper describes an in-depth survey of software development projects. The results support the claim that projects which employ a flexible development model experience less effort overruns than do those which employ a sequential model. The reason for the difference is not obvious. We found, for example, no variation in project size, estimation process, or delivered proportion of planned functionality between projects applying different types of development model. When the managers were asked to provide reasons for software overruns and/or estimation accuracy, the largest difference was that more of flexible projects than sequential\u00a0\u2026", "num_citations": "151\n", "authors": ["155"]}
{"title": "Reasons for software effort estimation error: impact of respondent role, information collection approach, and data analysis method\n", "abstract": " This study aims to improve analyses of why errors occur in software effort estimation. Within one software development company, we collected information about estimation errors through: 1) interviews with employees in different roles who are responsible for estimation, 2) estimation experience reports from 68 completed projects, and 3) statistical analysis of relations between characteristics of the 68 completed projects and estimation error. We found that the role of the respondents, the data collection approach, and the type of analysis had an important impact on the reasons given for estimation error. We found, for example, a strong tendency to perceive factors outside the respondents' own control as important reasons for inaccurate estimates. Reasons given for accurate estimates, on the other hand, typically cited factors that were within the respondents' own control and were determined by the estimators' skill or\u00a0\u2026", "num_citations": "149\n", "authors": ["155"]}
{"title": "Better sure than safe? Over-confidence in judgement based software development effort prediction intervals\n", "abstract": " The uncertainty of a software development effort estimate can be indicated through a prediction interval (PI), i.e., the estimated minimum and maximum effort corresponding to a specific confidence level. For example, a project manager may be \u201c90% confident\u201d or believe that is it \u201cvery likely\u201d that the effort required to complete a project will be between 8000 and 12,000 work-hours. This paper describes results from four studies (Studies A\u2013D) on human judgement (expert) based PIs of software development effort. Study A examines the accuracy of the PIs in real software projects. The results suggest that the PIs were generally much too narrow to reflect the chosen level of confidence, i.e., that there was a strong over-confidence. Studies B\u2013D try to understand the reasons for the observed over-confidence. Study B examines the possibility that the over-confidence is related to type of experience or estimation process\u00a0\u2026", "num_citations": "149\n", "authors": ["155"]}
{"title": "Top-down and bottom-up expert estimation of software development effort\n", "abstract": " Expert estimation of software development effort may follow top-down or bottom-up strategies, i.e. the total effort estimate may be based on properties of the project as a whole and distributed over project activities (top-down) or calculated as the sum of the project activity estimates (bottom-up). The explorative study reported in this paper examines differences between these two strategies based on measurement and video recording of the discussions of seven estimation teams. Each estimation team applied a top-down estimation strategy on one project and a bottom-up estimation strategy on another. The main contribution of the study is the observation that the recall of very similar previously completed projects seemed to be a pre-condition for accurate top-down strategy based estimates, i.e. the abilities of the software estimators to transfer estimation experience from less similar projects was poor. This suggests\u00a0\u2026", "num_citations": "145\n", "authors": ["155"]}
{"title": "A survey on software estimation in the Norwegian industry\n", "abstract": " We provide an overview of the estimation methods that software companies apply to estimate their projects, why those methods are chosen, and how accurate they are. In order to improve estimation accuracy, such knowledge is essential. We conducted an in-depth survey, where information was collected through structured interviews with senior managers from 18 different companies and project managers of 52 different projects. We analyzed information about estimation approach, effort estimation accuracy and bias, schedule estimation accuracy and bias, delivered functionality and other estimation related information. Our results suggest, for example, that average effort overruns are 41%, that the estimation performance has not changed much the last 10-20 years, that expert estimation is the dominating estimation method, that estimation accuracy is not much impacted by use of formal estimation models, and\u00a0\u2026", "num_citations": "144\n", "authors": ["155"]}
{"title": "Software effort estimation terminology: The tower of Babel\n", "abstract": " It is well documented that the software industry suffers from frequent cost overruns. A contributing factor is, we believe, the imprecise estimation terminology in use. A lack of clarity and precision in the use of estimation terms reduces the interpretability of estimation accuracy results, makes the communication of estimates difficult, and lowers the learning possibilities. This paper reports on a structured review of typical software effort estimation terminology in software engineering textbooks and software estimation research papers. The review provides evidence that the term \u2018effort estimate\u2019 is frequently used without sufficient clarification of its meaning, and that estimation accuracy is often evaluated without ensuring that the estimated and the actual effort are comparable. Guidelines are suggested on how to reduce this lack of clarity and precision in terminology.", "num_citations": "126\n", "authors": ["155"]}
{"title": "From origami to software development: a review of studies on judgment-based predictions of performance time.\n", "abstract": " This article provides an integrative review of the literature on judgment-based predictions of performance time, often described as task duration predictions in psychology and as expert-based effort estimation in engineering and management science. We summarize results on the characteristics of performance time predictions, processes and strategies, the influence of task characteristics and contextual factors, and the relations between estimates and characteristics of the estimator. Although dependent on the type of study and the level of analysis, underestimation was more frequently reported than overestimation in studies from the engineering and management literature. However, this was not the case in studies from the psychology literature. Our summaries challenge earlier results regarding the effects of factors such as complexity/difficulty and experience. We also question the recurrent finding that small tasks\u00a0\u2026", "num_citations": "109\n", "authors": ["155"]}
{"title": "The impact of lessons-learned sessions on effort estimation and uncertainty assessments\n", "abstract": " Inaccurate estimates of software development effort is a frequently reported cause of IT-project failures. We report results from a study that investigated the effect of introducing lessons-learned sessions on estimation accuracy and the assessment of uncertainty. Twenty software professionals were randomly allocated to a Learning group or a Control group and instructed to estimate and complete the same five development tasks. Those in the Learning group but not those in the Control group were instructed to spend at least 30 minutes on identifying, analyzing, and summarizing their effort estimation and uncertainty assessment experience after completing each task. We found that the estimation accuracy and the realism of the uncertainty assessment were not better in the Learning group than in the Control group. A follow-up study with 83 software professionals was completed to better understand this lack of\u00a0\u2026", "num_citations": "105\n", "authors": ["155"]}
{"title": "Group processes in software effort estimation\n", "abstract": " The effort required to complete software projects is often estimated, completely or partially, using the judgment of experts, whose assessment may be biased. In general, such bias as there is seems to be towards estimates that are overly optimistic. The degree of bias varies from expert to expert, and seems to depend on both conscious and unconscious processes. One possible approach to reduce this bias towards over-optimism is to combine the judgments of several experts. This paper describes an experiment in which experts with different backgrounds combined their estimates in group discussion. First, 20 software professionals were asked to provide individual estimates of the effort required for a software development project. Subsequently, they formed five estimation groups, each consisting of four experts. Each of these groups agreed on a project effort estimate via the pooling of knowledge in\u00a0\u2026", "num_citations": "94\n", "authors": ["155"]}
{"title": "Software quality measurement\n", "abstract": " This article analyses our ability to measure software quality. The analysis is based on the representational theory of measurement. We analyse three assumptions on software quality measurement and recommend an increased measurement focus on establishing empirical relational systems (models of what we measure) in software quality related work. Applying measurement theory on the measurement of software quality related properties means that, we have a powerful tool to understand what we measure, what the meaningful operations on the measured values are, and how to interpret the results.", "num_citations": "91\n", "authors": ["155"]}
{"title": "What we do and don't know about software development effort estimation\n", "abstract": " Unrealistic project plans and cost overruns create problems for software professionals and their clients. In spite of much research and numerous attempts to learn from experience, the problem of inaccurate, frequently overoptimistic cost estimates hasn't been solved. This article summarizes some knowledge that could improve the estimation accuracy, but we still lack some knowledge that hinders us from being able to produce accurate estimates.", "num_citations": "84\n", "authors": ["155"]}
{"title": "Realism in assessment of effort estimation uncertainty: It matters how you ask\n", "abstract": " Traditionally, software professionals are requested to provide minimum-maximum intervals to indicate the uncertainty of their effort estimates. We claim that the traditional request is not optimal and leads to overoptimistic views about the level of estimation uncertainty. Instead, we propose that it is better to frame the request for uncertainty assessment: \"How likely is it that the actual effort will be more than/less than X?\" Our claim is based on the results of a previously reported-experiment and field studies in two companies. The two software companies were instructed to apply the traditional and our alternative framing on random samples of their projects. In total, we collected information about 47 projects applying the traditional-framing and 23 projects applying the alternative framing.", "num_citations": "81\n", "authors": ["155"]}
{"title": "An empirical study of software maintenance tasks\n", "abstract": " An empirical study of software maintenance tasks was carried out in a large Norwegian organization in the period 1992\u201393. More than one hundred randomly selected maintenance tasks were studied through interviews with the maintainers performing the tasks immediately before they started the tasks and immediately after they had completed the tasks. The collected data is used to develop distributions and to test 33 hypotheses about software maintenance. The findings from the study indicate, for example, that   Application system documentation plays only a minor role as maintenance information source.   The maintenance productivity, measured in LOC/effort, is predicted by the size of the task and type of change in the source code, but seems rather independent of language level, maintainer experience, application age and application size.   There is an economy of scale for software maintenance tasks.   The\u00a0\u2026", "num_citations": "81\n", "authors": ["155"]}
{"title": "Inconsistency of expert judgment-based estimates of software development effort\n", "abstract": " Expert judgment-based effort estimation of software development work is partly based on non-mechanical and unconscious processes. For this reason, a certain degree of intra-person inconsistency is expected, i.e., the same information presented to the same individual at different occasions sometimes lead to different effort estimates. In this paper, we report from an experiment where seven experienced software professionals estimated the same sixty software development tasks over a period of three months. Six of the sixty tasks were estimated twice. We found a high degree of inconsistency in the software professionals\u2019 effort estimates. The mean difference of the effort estimates of the same task by the same estimator was as much as 71%. The correlation between the corresponding estimates was 0.7. Highly inconsistent effort estimates will, on average, be inaccurate and difficult to learn from. It is consequently\u00a0\u2026", "num_citations": "77\n", "authors": ["155"]}
{"title": "Can you trust a single data source exploratory software engineering case study?\n", "abstract": " As the demand for empirical evidence for claims of improvements in software development and evolution has increased, the use of empirical methods such as case studies has grown. In case study methodology various types of triangulation is a commonly recommended technique for increasing validity. This study investigates a multiple data source case study with the objective of identifying whether more findings, trustworthier findings and other findings are made using multiple data source triangulation, than had a single data source been used. The case study investigated analyses key lead-time success factors for a software evolution project in a large organization developing eBusiness systems with high-availability high throughput transaction characteristics. By tracing each finding in that study to the individual evidences motivating the finding, it is suggested that a multiple data source explorative case\u00a0\u2026", "num_citations": "77\n", "authors": ["155"]}
{"title": "The impact of irrelevant and misleading information on software development effort estimates: A randomized controlled field experiment\n", "abstract": " Studies in laboratory settings report that software development effort estimates can be strongly affected by effort-irrelevant and misleading information. To increase our knowledge about the importance of these effects in field settings, we paid 46 outsourcing companies from various countries to estimate the required effort of the same five software development projects. The companies were allocated randomly to either the original requirement specification or a manipulated version of the original requirement specification. The manipulations were as follows: 1) reduced length of requirement specification with no change of content, 2) information about the low effort spent on the development of the old system to be replaced, 3) information about the client's unrealistic expectations about low cost, and 4) a restriction of a short development period with start up a few months ahead. We found that the effect sizes in the field\u00a0\u2026", "num_citations": "71\n", "authors": ["155"]}
{"title": "A framework for the analysis of software cost estimation accuracy\n", "abstract": " Many software companies track and analyze project performance by measuring cost estimation accuracy. A high estimation error is frequently interpreted as poor estimation skills. This is not necessarily a correct interpretation. High estimation error can also be a result of other factors, such as high estimation complexity and insufficient cost control of the project. Through a real-life example we illustrate how the lack of proper estimation error analysis technique can bias analyses of cost estimation accuracy and lead to wrong conclusions. Further, we examine a selection of cost estimation studies, and show that they frequently do not take the necessary actions to ensure meaningful interpretations of estimation error data. Motivated by these results, we propose a general framework that, we believe, will improve analyses of software cost estimation error.", "num_citations": "69\n", "authors": ["155"]}
{"title": "Avoiding irrelevant and misleading information when estimating development effort\n", "abstract": " Software projects average about 30 percent accuracy in effort estimation.1 Expecting highly accurate effort estimates might be unrealistic because software development projects are inherently uncertain. Nevertheless, software professionals' tendency toward overly optimistic estimates and their high level of estimation inconsistency suggest potential for improving effort estimation processes.", "num_citations": "68\n", "authors": ["155"]}
{"title": "Evidence-based guidelines for assessment of software development cost uncertainty\n", "abstract": " Several studies suggest that uncertainty assessments of software development costs are strongly biased toward overconfidence, i.e., that software cost estimates typically are believed to be more accurate than they really are. This overconfidence may lead to poor project planning. As a means of improving cost uncertainty assessments, we provide evidence-based guidelines for how to assess software development cost uncertainty, based on results from relevant empirical studies. The general guidelines provided are: 1) Do not rely solely on unaided, intuition-based uncertainty assessment processes, 2) do not replace expert judgment with formal uncertainty assessment models, 3) apply structured and explicit judgment-based processes, 4) apply strategies based on an outside view of the project, 5) combine uncertainty assessments from different sources through group work, not through mechanical combination, 6\u00a0\u2026", "num_citations": "68\n", "authors": ["155"]}
{"title": "Regression models of software development effort estimation accuracy and bias\n", "abstract": " This paper describes models whose purpose is to explain the accuracy and bias variation of an organization\u2019s estimates of software development effort through regression analysis. We collected information about variables that we believed would affect the accuracy or bias of estimates of the performance of tasks completed by the organization. In total, information about 49 software development tasks was collected. We found that the following conditions led to inaccuracies in estimates: (1) Estimates were provided by a person in the role of \u201csoftware developer\u201d instead of \u201cproject leader\u201d, (2) The project had as its highest priority time-to-delivery instead of quality or cost, and (3) The estimator did not participate in the completion of the task. The following conditions led to an increased bias towards under-estimation: (1) Estimates were provided by a person with the role of \u201cproject leader\u201d instead of \u201csoftware\u00a0\u2026", "num_citations": "66\n", "authors": ["155"]}
{"title": "A survey on the characteristics of projects with success in delivering client benefits\n", "abstract": " ContextA large waste of resources in software development projects currently results from being unable to produce client benefits.ObjectiveThe main objective is to better understand the characteristics of successful software projects and contribute to software projects that are more likely to produce the planned client benefits.MethodWe asked 63 Norwegian software professionals, representing both the client and the provider role, to report information about their last completed project. In a follow-up survey with 64 Norwegian software professionals, we addressed selected findings from the first survey.ResultsThe analysis of the project information showed the following: i) The project management triangle criteria of being on time, on budget, and having the specified functionality are poor correlates of the essential success dimension client benefits. ii) Benefit management planning before the project started and benefit\u00a0\u2026", "num_citations": "64\n", "authors": ["155"]}
{"title": "The role of deliberate artificial design elements in software engineering experiments\n", "abstract": " Increased realism in software engineering experiments is often promoted as an important means of increasing generalizability and industrial relevance. In this context, artificiality, e.g., the use of constructed tasks in place of realistic tasks, is seen as a threat. In this paper, we examine the opposite view that deliberately introduced artificial design elements may increase knowledge gain and enhance both generalizability and relevance. In the first part of this paper, we identify and evaluate arguments and examples in favor of and against deliberately introducing artificiality into software engineering experiments. We find that there are good arguments in favor of deliberately introducing artificial design elements to 1) isolate basic mechanisms, 2) establish the existence of phenomena, 3) enable generalization from particularly unfavorable to more favorable conditions (persistence of phenomena), and 4) relate\u00a0\u2026", "num_citations": "59\n", "authors": ["155"]}
{"title": "An empirical study of software project bidding\n", "abstract": " The study described in this paper reports from a real-life bidding process in which 35 companies were bidding for the same contract. The bidding process consisted of two separate phases: a prestudy phase and a bidding phase. In the prestudy phase, 17 of the 35 bidding companies provided rough price indications based on a brief, incomplete description of user requirements. In the bidding phase, all 35 companies provided bids based on a more complete requirement specification that described a software system with substantially more functionality than the system indicated in the prestudy phase. The main result of the study is that the 17 companies involved in the prestudy phase presented bids that were, on average, about 70 percent higher than the bids of the other companies, although all companies based their bids on the same requirement specification. We propose an explanation for this difference that is\u00a0\u2026", "num_citations": "59\n", "authors": ["155"]}
{"title": "Over-optimism in software development projects:\" the winner's curse\"\n", "abstract": " It is well known that software development projects tend to be based on over-optimistic cost estimates. Better knowledge of the sources of this over-optimism is necessary to improve realism in software development project bids and budgets. This paper analyses the effect of \"the winner's curse\". The winner's curse is a result of the selection of software providers among those with the lowest bid, i.e., those with a tendency towards the highest level of overoptimism. The winner's curse has not been extensively analyzed in software cost estimation studies, but is a well known phenomenon in domains such as auctioning. We exemplify the effect of the winner's curse with data from a real software development bidding round and simulate how increase in number of bidders and cost uncertainty impact the expected profit. We argue that the winners' curse is a problem for both clients and providers, and that it may lead to\u00a0\u2026", "num_citations": "58\n", "authors": ["155"]}
{"title": "Failure factors of small software projects at a global outsourcing marketplace\n", "abstract": " The presented study aims at a better understanding of when and why small-scale software projects at a global outsourcing marketplace fail. The analysis is based on a data set of 785,325\u00a0projects/tasks completed at vWorker.com. A binary logistic regression model relying solely on information known at the time of a project's start-up correctly predicted 74% of the project failures and 67% of the non-failures. The model-predicted failure probability corresponded well with the actual frequencies of failures for most levels of failure risk. The model suggests that the factors connected to the strongest reduction in the risk of failure are related to previous collaboration between the client and the provider and a low failure rate of previous projects completed by the provider. We found the characteristics of the client to be almost as important as those of the provider in explaining project failures and that the risk of project failure\u00a0\u2026", "num_citations": "53\n", "authors": ["155"]}
{"title": "How does project size affect cost estimation error? Statistical artifacts and methodological challenges\n", "abstract": " Empirical studies differ in what they report as the underlying relation between project size and percent cost overrun. As a consequence, the studies also differ in their project management recommendations. We show that studies with a project size measure based on the actual cost systematically report an increase in percent cost overrun with increased project size, whereas studies with a project size measure based on the estimated cost report a decrease or no change in percent cost overrun with increased project size. The observed pattern is, we argue, to some extent a statistical artifact caused by imperfect correlation between the estimated and the actual cost. We conclude that the previous observational studies cannot be considered to provide reliable evidence in favor of an underlying project size related cost estimation bias. We discuss the limited, statistically robust evidence and the need for other types of\u00a0\u2026", "num_citations": "52\n", "authors": ["155"]}
{"title": "The effects of request formats on judgment-based effort estimation\n", "abstract": " In this paper we study the effects of a change from the traditional request \u201cHow much effort is required to complete X?\u201d to the alternative \u201cHow much can be completed in Y work-hours?\u201d. Studies 1 and 2 report that software professionals receiving the alternative format provided much lower, and presumably more optimistic, effort estimates of the same software development work than those receiving the traditional format. Studies 3 and 4 suggest that the effect belongs to the family of anchoring effects. An implication of our results is that project managers and clients should avoid the alternative estimation request format.", "num_citations": "51\n", "authors": ["155"]}
{"title": "A critique of how we measure and interpret the accuracy of software development effort estimation\n", "abstract": " This paper criticizes current practice regarding the measurement and interpretation of the accuracy of software development effort estimation. The shortcomings we discuss are related to: 1) the meaning of \u2018effort estimate\u2019, 2) the meaning of \u2018estimation accuracy\u2019, 3) estimation of moving targets, and 4) assessment of the estimation process, and not only the discrepancy between the estimated and the actual effort, to evaluate estimation skill. It is possible to correct several of the discussed shortcomings by better practice. However, there are also inherent problems related to both laboratory and field analyses of the accuracy of software development effort estimation. It is essential that both software researchers and professionals are aware of these problems and their implications for the analysis of the measurement of effort estimation accuracy.", "num_citations": "47\n", "authors": ["155"]}
{"title": "Software development estimation biases: The role of interdependence\n", "abstract": " Software development effort estimates are frequently too low, which may lead to poor project plans and project failures. One reason for this bias seems to be that the effort estimates produced by software developers are affected by information that has no relevance for the actual use of effort. We attempted to acquire a better understanding of the underlying mechanisms and the robustness of this type of estimation bias. For this purpose, we hired 374 software developers working in outsourcing companies to participate in a set of three experiments. The experiments examined the connection between estimation bias and developer dimensions: self-construal (how one sees oneself), thinking style, nationality, experience, skill, education, sex, and organizational role. We found that estimation bias was present along most of the studied dimensions. The most interesting finding may be that the estimation bias increased\u00a0\u2026", "num_citations": "46\n", "authors": ["155"]}
{"title": "Expert estimation of web-development projects: are software professionals in technical roles more optimistic than those in non-technical roles?\n", "abstract": " Estimating the effort required to complete web-development projects involves input from people in both technical (e.g., programming), and non-technical (e.g., user interaction design) roles. This paper examines how the employees' role and type of competence may affect their estimation strategy and performance. An analysis of actual web-development project data and results from an experiment suggest that people with technical competence provided less realistic project effort estimates than those with less technical competence. This means that more knowledge about how to implement a requirement specification does not always lead to better estimation performance. We discuss, amongst others, two possible reasons for this observation: (1) Technical competence induces a bottom-up, construction-based estimation strategy, while lack of this competence induces a more \u201coutside\u201d view of the project\u00a0\u2026", "num_citations": "45\n", "authors": ["155"]}
{"title": "The role of outcome feedback in improving the uncertainty assessment of software development effort estimates\n", "abstract": " Previous studies report that software developers are over-confident in the accuracy of their effort estimates. Aim: This study investigates the role of outcome feedback, that is, feedback about the discrepancy between the estimated and the actual effort, in improving the uncertainty assessments. Method: We conducted two in-depth empirical studies on uncertainty assessment learning. Study 1 included five student developers and Study 2, 10 software professionals. In each study the developers repeatedly assessed the uncertainty of their effort estimates of a programming task, solved the task, and received estimation accuracy outcome feedback. Results: We found that most, but not all, developers were initially over-confident in the accuracy of their effort estimates and remained over-confident in spite of repeated and timely outcome feedback. One important, but not sufficient, condition for improvement based on\u00a0\u2026", "num_citations": "41\n", "authors": ["155"]}
{"title": "Identification of more risks can lead to increased over-optimism of and over-confidence in software development effort estimates\n", "abstract": " Software professionals are, on average, over-optimistic about the required effort usage and over-confident about the accuracy of their effort estimates. A better understanding of the mechanisms leading to the over-optimism and over-confidence may enable better estimation processes and, as a consequence, better managed software development projects. We hypothesize that there are situations where more work on risk identification leads to increased over-optimism and over-confidence in software development effort estimates, instead of the intended improvement of realism. Four experiments with software professionals are conducted to test the hypothesis. All four experiments provide results in support of the hypothesis. Possible explanations of the counter-intuitive finding relate to results from cognitive science on \u201cillusion-of-control\u201d, \u201ccognitive accessibility\u201d, \u201cthe peak-end rule\u201d and \u201crisk as feeling.\u201d Thorough\u00a0\u2026", "num_citations": "40\n", "authors": ["155"]}
{"title": "Combination of software development effort prediction intervals: why, when and how?\n", "abstract": " The uncertainty of a software development effort estimate may be described through a prediction interval, eg, that the most likely use of effort is 1.500 work-hours and that it is 90% probable (90% confidence level) that the actual use of effort will be between 1.000 (minimum) and 2.000 (maximum) work-hours. Previous studies suggest that software development effort prediction intervals are, on average, much too narrow to reflect high confidence levels, ie, the uncertainty is under-estimated. This paper analyses when and how a combination of several individual prediction intervals of the same task improves the correspondence between hit rate and confidence level of effort prediction intervals. We analyse three combination strategies:(1) Average of the individual minimum and maximum values,(2) Maximum and minimum of the individual maximum and minimum values, and (3) Group process (discussion) based\u00a0\u2026", "num_citations": "35\n", "authors": ["155"]}
{"title": "A preliminary checklist for software cost management\n", "abstract": " This paper presents a process framework and a preliminary checklist for software cost management. While most textbooks and research papers on cost estimation look mainly at the \"estimation\" phase, our framework and checklist includes the phases relevant to estimation: \"preparation\", \"estimation\", \"application\", and \"learning\". We believe that cost estimation processes and checklists should support these phases to enable high estimation accuracy. The checklist we suggest is based on checklists from a number of sources, e.g., a handbook in forecasting and checklists present in several Norwegian software companies, it needs, however, to be extended through feedback from other researchers and software practitioners. There is also a need for a provision of conditions for meaningful use of the checklist issues and descriptions of the strength and sources of evidence in favor of the checklist issues. The present\u00a0\u2026", "num_citations": "33\n", "authors": ["155"]}
{"title": "The influence of selection bias on effort overruns in software development projects\n", "abstract": " ContextA potentially important, but neglected, reason for effort overruns in software projects is related to selection bias. Selection bias\u2013induced effort overruns occur when proposals are more likely to be accepted and lead to actual projects when based on effort estimates that are too low rather than on realistic estimates or estimates that are too high. The effect of this bias may be particularly important in bidding rounds, but is potentially relevant in all situations where there is effort or cost-based selection between alternatives.ObjectiveTo better understand the relevance and management of selection bias effects in software development contexts.MethodFirst, we present a statistical model illustrating the relation between selection bias in bidding and other contexts and effort overruns. Then, we examine this relation in an experiment with software professionals who estimated and completed a set of development tasks\u00a0\u2026", "num_citations": "31\n", "authors": ["155"]}
{"title": "Does the use of Fibonacci numbers in Planning Poker affect effort estimates?\n", "abstract": " Background: The estimation technique Planning Poker is common in agile software development. The cards used to propose an estimate in Planning Poker do not include all numbers, but for example only the numbers 0, \u00bd, 1, 2, 3, 5, 8, 13, 20, 40 and 100. We denote this, somewhat inaccurately, a Fibonacci scale in this paper. In spite of the widespread use of the Fibonacci scale in agile estimation, we do not know much about how this scale influences the estimation process. Aim: Better understanding of the effect of going from a linear scale to a Fibonacci scale in effort estimation. Method: We conducted two empirical studies. In the first study, we gave computer science students the same estimation task. Half of the students estimated the task using the Fibonacci scale and the other half a linear scale. The second study included four estimation teams, each composed of four software professionals, estimating the\u00a0\u2026", "num_citations": "31\n", "authors": ["155"]}
{"title": "Relationships between project size, agile practices, and successful software development: results and analysis\n", "abstract": " Large-scale software development succeeds more often when using agile methods. Flexible scope, frequent deliveries to production, a high degree of requirement changes and more competent providers are possible reasons.", "num_citations": "28\n", "authors": ["155"]}
{"title": "What contributes to the success of IT projects? Success factors, challenges and lessons learned from an empirical study of software projects in the Norwegian public sector\n", "abstract": " Context. Each year the public sector invests large amounts of money in the development and modifications of their software systems. These investments are not always successful and many public sector software projects fail to deliver the expected benefits. Goal. This study aims at reducing the waste of resources on failed software projects through better understanding of the success factors and challenges. Method. Thirty-five completed software projects in 11 organizations in the public sector of Norway were analyzed. For each project, representatives from the project owners, project management and the user organization were interviewed. Results. Small and large software projects reported different challenges, especially related to project priority. Taking advantage of agile practices such as flexible scope and frequent delivery increased the success rate of the projects. Projects with time and material contracts\u00a0\u2026", "num_citations": "28\n", "authors": ["155"]}
{"title": "Uncertainty intervals versus interval uncertainty: An alternative method for eliciting effort prediction intervals in software development projects\n", "abstract": " Frequently, there is a poor correspondence between the judged and the actual uncertainty of effort usage in software development projects. This may to some extent be a consequence of the uncertainty elicitation process. Traditionally, software developers are asked to provide the minimum and maximum effort of development work for a given confidence level, eg, minimum and maximum effort that includes the actual effort usage with a 90% probability. An alternative uncertainty elicitation process is to instruct the software developers to provide the uncertainty of a given effort interval, eg, the probability that the actual effort is between 50% and 200% of the estimated most likely effort. In an empirical investigation, this alternative process led to significant improvement of prediction interval accuracy. The observed improvement using this alternative elicitation process can, we believe, be explained through a simplified interpretation of historical prediction accuracy data, less \u201cconflicting estimation goal\u201d, and less influence from the \u201canchoring effect\u201d.", "num_citations": "28\n", "authors": ["155"]}
{"title": "Do agile methods work for large software projects?\n", "abstract": " Is it true that agile methods do not scale well and are mainly useful for smaller software projects? Or is it rather the case that it is particularly in the context of larger, typically more complex software projects that the use of agile methods is likely to make the difference between success and failure? To find out more about this, we conducted a questionnaire-based survey analyzing information about 101 Norwegian software projects. Project success was measured as the combined performance of the project regarding delivered client benefits, cost control, and time control. We found that that projects using agile methods performed on average much better than those using non-agile methods for medium and large software projects, but not so much for smaller projects. This result gives support for the claim that agile methods are more rather than less successful compared to traditional methods when project size\u00a0\u2026", "num_citations": "25\n", "authors": ["155"]}
{"title": "Direct and indirect connections between type of contract and software project outcome\n", "abstract": " This paper reports two empirical studies on how the use of different contract types affects, directly and indirectly, the outcomes of software projects. The first study evaluates the effect of contract type on project failure using information from a large international dataset of small-scale, outsourced software projects and tasks. The second study proposes and tests how the use of contracts is connected with project outcome using information about Norwegian software projects with a public client. Both studies find that the use of fixed price contracts is connected with a higher risk of project failure compared to time and materials types of contracts. The results from the second study suggest that different project outcomes with different contract types is explained by differences in how the provider is selected, how the client is involved in the project, the use of agile practices and the use of benefit management during project\u00a0\u2026", "num_citations": "24\n", "authors": ["155"]}
{"title": "Contrasting ideal and realistic conditions as a means to improve judgment-based software development effort estimation\n", "abstract": " ContextThe effort estimates of software development work are on average too low. A possible reason for this tendency is that software developers, perhaps unconsciously, assume ideal conditions when they estimate the most likely use of effort. In this article, we propose and evaluate a two-step estimation process that may induce more awareness of the difference between idealistic and realistic conditions and as a consequence more realistic effort estimates. The proposed process differs from traditional judgment-based estimation processes in that it starts with an effort estimation that assumes ideal conditions before the most likely use of effort is estimated.ObjectiveThe objective of the paper is to examine the potential of the proposed method to induce more realism in the judgment-based estimates of work effort.MethodThree experiments with software professionals as participants were completed. In all three\u00a0\u2026", "num_citations": "24\n", "authors": ["155"]}
{"title": "Characteristics of software engineers with optimistic predictions\n", "abstract": " This paper examines the degree to which level of optimism in software engineers\u2019 predictions is related to optimism on previous predictions, general level of optimism (explanatory style, life orientation and self-assessed optimism), development skill, confidence in the accuracy of their own predictions, and ability to recall effort used on previous tasks. Results from four experiments suggest that more optimistic software engineers are characterized by more optimistic previous predictions, higher confidence in the accuracy of their own predictions, lower development skills, poorer ability or willingness to recall effort on previous tasks, and higher optimism scores. However, a substantial part of the variation in the level of optimism seems to be random.", "num_citations": "24\n", "authors": ["155"]}
{"title": "The effects of the format of software project bidding processes\n", "abstract": " This study investigates how differences in format of the bidding process affect companies\u2019 bids for software projects. Thirty outsourcing companies from different Asian and European countries participated in the bidding for a software project. A participating company either started with the provision of a bid based on a reduced version of the specification and then continued with a bid based on the full specification (the Increase situation), or started with the full specification and then continued with the reduced one (the Decrease situation). Our results constitute evidence that in situations similar to the one we studied, the client will typically select a provider with about a 40% lower price in the Decrease situation than in the Increase situation. We discuss implications of this finding for clients and providers.", "num_citations": "24\n", "authors": ["155"]}
{"title": "How much does a vacation cost? Or what is a software cost estimate?\n", "abstract": " What is a software cost estimate? Is it the most likely cost, the planned cost, the budget, the price, or, something else? Through comparison with vacation cost estimation and a real-life case we illustrate that it is not meaningful to compare and analyze cost estimates unless it is clear which interpretation is applied. Unfortunately, the software industry, software engineering textbooks and scientific estimation studies do frequently not clarify how they apply the term 'cost estimate'. We argue that this lack of clarity may lead to conflicting estimation goals, communication problems, and, learning problems, and provide recommendations on how to deal with these problems.", "num_citations": "24\n", "authors": ["155"]}
{"title": "Unit effects in software project effort estimation: Work-hours gives lower effort estimates than workdays\n", "abstract": " Software development effort estimates are typically expert judgment-based and too low to reflect the actual use of effort. Our goal is to understand how the choice of effort unit affects expert judgement-based effort estimates, and to use this knowledge to increase the realism of effort estimates. We conducted two experiments where the software professionals were randomly instructed to estimate the effort of the same projects in work-hours or in workdays. In both experiment, the software professionals estimating in work-hours had much lower estimates (on average 33%\u201359% lower) than those estimating in workdays. We argue that the unitosity effect\u2014i.e., that we tend to infer information about the quantity from the choice of unit\u2014is the main explanation for the large difference in effort estimates. A practical implication of the unit effect is that, in contexts where there is a tendency toward effort under estimation, the\u00a0\u2026", "num_citations": "23\n", "authors": ["155"]}
{"title": "Numerical anchors and their strong effects on software development effort estimates\n", "abstract": " The anchoring effect may be described as the tendency for an initial piece of information to influence people's subsequent judgement, even when the information is irrelevant. Previous studies suggest that anchoring is an important source of inaccurate software development effort estimates. This article examines how the preciseness and credibility of anchoring information affects effort estimates. Our hypotheses were that anchors with lower numerical precision and anchor sources with lower credibility would have less impact on effort estimates. The results from three software project effort estimation experiments, with 381 software professionals, support previous findings about the relevance of anchoring effects to software effort estimation. However, we found no decrease in the anchoring effect with decreasing anchor precision or source credibility. This suggests that even implausible anchors from low-credibility\u00a0\u2026", "num_citations": "22\n", "authors": ["155"]}
{"title": "Software development effort estimation\u2014Demystifying and improving expert estimation\n", "abstract": " The main determinant of many types of software-related investments is the amount of development effort required. The ability of software clients to make investment decisions based on cost estimates is consequently strongly tied to the software providers\u2019 ability to estimate the effort accurately. Similarly, the ability of project managers to plan a project and ensure efficient development frequently depends on accurate effort estimates. The importance of accurate effort estimates is illustrated by the findings of a 2007 survey of more than 1,000 IT professionals. The survey reports that two out of the three-most-important causes of IT project failure were related to poor resource estimation, that is, inaccurate effort estimates.", "num_citations": "22\n", "authors": ["155"]}
{"title": "How to avoid selecting bids based on overoptimistic cost estimates\n", "abstract": " This article documents important connections between cost overruns and bidding. It also recommends how clients should design their bidding process to avoid selecting bids based on overoptimistic cost estimates. The recommendations provided reduce the risk of initiating projects that have unrealistic plans, time overruns, low software quality, high maintenance costs, and inflexible providers. Although this article aims to improve the clients' bidding processes, the authos believes that it also has important implications for software providers. They could use the reported results to identify bidding rounds that they're likely to win only if they provide a bid based on strongly overoptimistic cost estimates. Software providers should consider carefully whether it's worthwhile to participate in bidding rounds in which this is the case.", "num_citations": "22\n", "authors": ["155"]}
{"title": "The clients' impact on effort estimation accuracy in software development projects\n", "abstract": " This paper focuses on the clients' impact on estimation accuracy in software development projects. Client related factors contributing to effort overruns as well as factors preventing overruns are investigated. Based on a literature review and a survey of 300 software professionals we find that: 1) software professionals perceive that clients impact estimation accuracy. Changed and new requirements are perceived as the clients' most frequent contribution to overruns, while overruns are prevented by the availability of competent clients and capable decision makers. 2) Survey results should not be used in estimation accuracy improvement initiatives without further analysis. Surveys typically identify directly observable and project specific causes for overruns, while substantial improvement is only possible when the underlying causes are understood", "num_citations": "22\n", "authors": ["155"]}
{"title": "Relative estimation of software development effort: it matters with what and how you compare\n", "abstract": " Estimating software development effort is frequently based on assessing the effort of one task relative to that of another. The author presents empirical results that show how relative estimation can result in biased assessments of similarity and overly optimistic effort estimates. Specifically, tasks tend to be assessed as more similar than they actually are. Furthermore, the similarity of two tasks depends on the direction of the comparison, and a comparison based on difference in work hours is distinct from one based on a ratio. From these results and other evidence, the author suggests ways to improve the accuracy of relative estimation.", "num_citations": "21\n", "authors": ["155"]}
{"title": "The impact of irrelevant information on estimates of software development effort\n", "abstract": " Software professionals typically estimate software development effort based on a requirement specification. Parts of this specification frequently contain information that is irrelevant to the estimation of the actual effort involved in the development of software. We hypothesize that effort-irrelevant information sometimes has a strong impact on effort estimates. To test this hypothesis, we conducted two controlled experiments with software professionals. In each of the experiments, the software professionals received specifications describing the same requirements. However, we gave one group of the software professionals a version of the requirement specification where we had included additional, effort-irrelevant, information. In both experiments we observed that the estimates of most likely effort increased when the estimates were based on requirement specifications that contained the information irrelevant to\u00a0\u2026", "num_citations": "20\n", "authors": ["155"]}
{"title": "Assessing uncertainty of software development effort estimates: The learning from outcome feedback\n", "abstract": " To enable properly sized software projects budgets and plans it is important to be able to assess the uncertainty of the estimates of most likely effort required to complete the projects. Previous studies show that software professionals tend to be too optimistic about the uncertainty of their effort estimates. This paper reports the results from a preliminary study on the role of outcome feedback in the learning process on effort estimation uncertainty assessment. Software developers were given repeated and immediate outcome feedback, i.e., feedback about the discrepancy between the estimated most likely effort and the actual effort, for the purpose of investigating how much, and how, they improve (learn). We found that a necessary condition for improvement of uncertainty assessments of effort estimates may be the use of explicitly formulated uncertainty assessment strategies. By contrast, intuition-based uncertainty\u00a0\u2026", "num_citations": "20\n", "authors": ["155"]}
{"title": "Eliminating over-confidence in software development effort estimates\n", "abstract": " Previous studies show that software development projects strongly underestimate the uncertainty of their effort estimates. This overconfidence in estimation accuracy may lead to poor project planning and execution. In this paper, we investigate whether the use of estimation error information from previous projects improves the realism of uncertainty assessments. As far as we know, there have been no empirical software studies on this topic before. Nineteen realistically composed estimation teams provided minimum-maximum effort intervals for the same software project. Ten of the teams (Group A) received no instructions about how to complete the uncertainty assessment process. The remaining nine teams (Group B) were instructed to apply a history-based uncertainty assessment process. The main results is that software professionals seem to willing to consider the error of previous effort estimates as\u00a0\u2026", "num_citations": "20\n", "authors": ["155"]}
{"title": "What Contributes to the Success of IT Projects? An Empirical Study of IT Projects in the Norwegian Public Sector\n", "abstract": " Each year the public sector invests large amounts of money in the development and modification of their software systems. These investments are not always successful and many public sector software projects fail to deliver the expected benefits. Goal. This study aims at reducing the waste of resources on failed software projects through better understanding of the success factors and challenges. Method. We collected information about project characteristics, project outcome, perceived success factors, challenges and lessons learned from 35 software projects in 11 organizations in the public sector of Norway. Results. The respondents experienced that extensive involvement and competence of the client, high priority of the project, good dialogue between the clients and the external supplier and application of agile practices were main success factors. The main challenges were related to the involvement and competence of the client, project planning and management, software architecture and integration issues, transition of the product to the user organization and benefit management. Small and large software projects reported different challenges, especially related to project priority and access to skilled personnel. Projects with time and materials contracts with suppliers and that involved clients during project execution were more successful than other projects. Conclusions. Success factors are usually human factors, eg, involvement, competence and collaboration. Challenges tend to be due to human factors as well as issues of a technical nature. Both aspects need to be addressed to enable successful software projects and avoid failures.", "num_citations": "19\n", "authors": ["155"]}
{"title": "To read two pages, I need 5 minutes, but give me 5 minutes and I will read four: How to change productivity estimates by inverting the question\n", "abstract": " Past research has shown that people underestimate the time they need to complete large tasks, whereas completion times for smaller tasks are often overestimated, suggesting higher productivity estimates for larger than for smaller tasks. By replacing the traditional question about how much time a given work will take with a question about how much work can be completed within a given amount of time, we also found the opposite pattern. Both trends could reflect a general tendency to underestimate large amounts (of work as well as time) relatively to small ones. This \u2018magnitude bias\u2019 was explored in two studies where students estimated reading tasks, a third where IT\u2010professionals estimated software projects, and a fourth where participants imagined a familiar walk, divided into time segments or part distances of varying lengths. Copyright \u00a9 2010 John Wiley & Sons, Ltd.", "num_citations": "19\n", "authors": ["155"]}
{"title": "The quality of questionnaire based software maintenance studies\n", "abstract": " Questionnaires sent to maintenance managers is a frequently used approach to collect data on software maintenance. This paper reports findings from two studies, carried out at a large Norwegian maintenance organisation, investigating the quality of questionnaire based software maintenance studies.Interesting findings were, among others, that:- The definition of essential terms, for example of 'software maintenance', at the beginning of a questionnaire did not assure a consistent use of the terms by the questionnaire respondents.- Manager estimates of the proportion of effort spent on corrective maintenance were biased when based on best guesses instead of good data. For this reason, the frequently referred studies of Lientz and Swanson (1980) and Nosek and Palvia (1990) may have reported a too high proportion of effort spent on corrective maintenance.", "num_citations": "19\n", "authors": ["155"]}
{"title": "Preliminary study of sequence effects in judgment-based software development work-effort estimation\n", "abstract": " Software development effort estimates are often inaccurate. This study investigates to what degree and why the sequence in which we estimate software work affect the effort estimates. The results may be used to improve judgment-based software development effort estimation processes. Two controlled experiments were conducted. In the first experiment, software professionals were randomly allocated to the groups SMALL and LARGE. First, those in group SMALL estimated the most likely effort required to complete a small software development task and those in group LARGE the effort of a larger task. Then both groups estimated the effort of the same medium-sized task. The first estimate had a large impact on the subsequent. The second experiment aimed at a better understanding of the nature of sequence effects in effort estimation. This experiment suggests that it is the experience and knowledge activated in\u00a0\u2026", "num_citations": "18\n", "authors": ["155"]}
{"title": "A strong focus on low price when selecting software providers increases the likelihood of failure in software outsourcing projects\n", "abstract": " Context: Bidding rounds are frequently used to select competent and cost-efficient providers for software projects. Objective: We hypothesize that emphasizing low price when selecting software providers in such bidding rounds substantially increases the likelihood the project will fail. Method: The hypothesis is tested by analyzing a dataset of 4,791,067 bids for 785,326 small-scale projects registered at a web-based marketplace connecting software clients and providers. Results: We find evidence supporting our hypothesis. For example, selecting providers with bids 25% lower than the average bid is connected to a 9% increase in the frequency of project failures for the same level of provider skill. In addition, we found that clients emphasizing a low price, on average, selected providers with lower skill levels. This decrease in provider skill level further strengthened the negative effect of a strong focus on low price on\u00a0\u2026", "num_citations": "17\n", "authors": ["155"]}
{"title": "Industrial use of formal software cost estimation models: Expert estimation in disguise?\n", "abstract": " The goal of this paper is to propose and evaluate the hypothesis that software cost estimates based on formal estimation models are frequently expert estimation in disguise, ie, that the cost estimates are not as mechanically derived as prescribed and assumed. We test implications of the hypothesis through discussion of related work and an empirical study of function point-based effort estimation of software projects. The actual effort estimates of the projects were compared with the effort estimates one would expect if the formal function point model was applied as prescribed. We observed several large deviations between the actual and the mechanically derived effort estimates, which we interpret as indications of a strong impact from expert judgment. Important limitations of our study are that the hypothesis is formulated vaguely, that there is not much evidence available, and, that we may have had a tendency to bias our search towards supporting evidence. More studies are therefore needed, preferably from independent researchers. If our hypothesis is correct, implementation of formal software cost estimation models should include means to avoid unwanted effects of initial beliefs and irrelevant information.", "num_citations": "16\n", "authors": ["155"]}
{"title": "Software Effort Estimation: unstructured group discussion as a method to reduce individual biasis.\n", "abstract": " The effort of software projects is often estimated, completely or partially, using expert judgement. This estimation process is subject to biases of the expert responsible. Generally, this bias seems to be towards too optimistic estimates regarding the effort needed to complete the project. The degree of bias varies depending on the expert involved, and seems to be connected to both conscious and unconscious decisions. One possible way to reduce this bias towards over-optimism is to combine the judgments of several experts. This paper describes an experiment where experts with different backgrounds combined their estimates through group discussion. Twenty software professionals were asked to provide individual effort estimates of a software development project. Subsequently, they formed five estimation groups, each consisting of four experts. Each of these groups agreed on a project effort estimate through discussion and combination of knowledge. We found that the groups were less optimistic in their estimates than the individual experts. Interestingly, the group discussion-based estimates were closer to the effort used by the actual project than the average individual expert, ie, the group discussions led to better estimates than a mechanical combination of the individual estimates. The groups\u2019 ability to identify more project activities is among the possible explanations for this reduction of bias.", "num_citations": "16\n", "authors": ["155"]}
{"title": "Selection of strategies in judgment-based effort estimation\n", "abstract": " We currently know little about the factors that motivate the selection and change of strategy in judgment-based effort estimation. A better understanding of these issues may lead to more accurate judgment-based effort estimates and motivates the four experiments reported in this paper. The experiments\u2019 two main results are the identification of the importance of \u201cestimation surprises\u201d (large estimation errors) to motivate estimation strategy change and the large individual variation in the initial choice of estimation strategy. The individual variation seems not only to be a result of differences in previous experiences, but also a result of differences in the mental \u201caccessibility\u201d of the strategies. We found, for example, that the use of a type of strategy was increased when we instructed a developer to use the same type of strategy on unrelated tasks immediately before.", "num_citations": "14\n", "authors": ["155"]}
{"title": "The\" magic step\" of judgment-based software effort estimation\n", "abstract": " Estimates of effort are important input to software development project planning and bidding. In the worst case, one single over-optimistic effort estimate can ruin a software development company. Currently, most software development effort estimates are based on expert judgments. In this paper we claim that the estimation process step from understanding the problem to quantifying the required effort to solve the problem is particularly poorly understood and difficult to analyze. Our claim is supported by data from two studies of software professionals' effort estimates. The software professionals seem to be unable to describe this quantification step in detail. A search for models that describe the mental processes involved has not, so far, led to identification of models or theories useful for the purpose of improving judgment-based software development effort estimation.", "num_citations": "13\n", "authors": ["155"]}
{"title": "Does use of development model affect estimation accuracy and bias?\n", "abstract": " Objective. To investigate how the use of incremental and evolutionary development models affects the accuracy and bias of effort and schedule estimates of software projects. Rationale. Advocates of incremental and evolutionary development models often claim that use of these models results in improved estimation accuracy. Design of study. We conducted an in-depth survey, where information was collected through structured interviews with 22 software project managers in 10 different companies. We collected and analyzed information about estimation approach, effort estimation accuracy and bias, schedule estimation accuracy and bias, completeness of delivered functionality and other estimation related information. Results. We found no impact from the development model on the estimation approach. However, we found that incremental and evolutionary projects were less prone to\u00a0\u2026", "num_citations": "13\n", "authors": ["155"]}
{"title": "Project Estimation in the Norwegian Software Industry-A Summary\n", "abstract": " This report provides an overview of the results obtained from a survey on project estimation in Norwegian software companies. The survey was conducted between February and November 2003. The main results are:", "num_citations": "13\n", "authors": ["155"]}
{"title": "Understanding use case models\n", "abstract": " Use Case Modeling is a technique for handling the functional requirements in a software development project. The Use Case Model can serve as a means of communication between the different stakeholders in a project. It is used in planning the project and is updated and used during the project. In order to reduce the possibilities for misunderstandings and differences in understanding, it would be useful to be able to evaluate to what extent the different stakeholders have understood the model and also to detect differences in interpretation. Low comprehension or differences in interpretation may indicate a need for more effort on specifying the requirements. If this is not feasible, it may be necessary to assume a higher risk when planning and estimating the project. We propose using knowledge on how humans understand text from cognitive psychology in the design of an experiment with a twofold goal\u00a0\u2026", "num_citations": "13\n", "authors": ["155"]}
{"title": "The use of precision of software development effort estimates to communicate uncertainty\n", "abstract": " The precision of estimates may be applied to communicate the uncertainty of required software development effort. The effort estimates 1000 and 975 work-hours, for example, communicate different levels of expected estimation accuracy. Through observational and experimental studies we found that software professionals (i) sometimes, but not in the majority of the examined projects, used estimate precision to convey effort uncertainty, (ii) tended to interpret overly precise, inaccurate effort estimates as indicating low developer competence and low trustworthiness of the estimates, while too narrow effort prediction intervals had the opposite effect. This difference remained even when the actual effort was known to be outside the narrow effort prediction interval. We identified several challenges related to the use of the precision of single value estimates to communicate effort uncertainty and recommend that\u00a0\u2026", "num_citations": "12\n", "authors": ["155"]}
{"title": "Program understanding behavior during estimation of enhancement effort on small Java programs\n", "abstract": " Good effort estimation is considered a key success factor for competitive software creation services. In this study, task level effort estimation by project leaders and software designers have been investigated in two Internet software service companies through an experiment. Protocol analysis of 27 think-aloud estimations of effort required for consecutive change tasks on a small Java program have been analysed, using the AFECS coding scheme. Results indicate that a) effort estimation at the task level is very different depending on the individual, even when small problems are addressed; b) AFECS seems be appropriate to use as a coding scheme when assessing program comprehension behaviour for the purpose of effort estimation; c) protocol analysis of comprehension during effort estimation does not necessarily capture all process elements. These results can be used to further guide detailed analysis\u00a0\u2026", "num_citations": "11\n", "authors": ["155"]}
{"title": "Better selection of software providers through trialsourcing\n", "abstract": " Software providers differ widely in productivity and quality, yet traditional performance evaluations fail to separate the competent from the incompetent. Trialsourcing- having multiple providers create sample pieces of software for evaluation-can help software clients select providers.", "num_citations": "10\n", "authors": ["155"]}
{"title": "Communication of software cost estimates\n", "abstract": " The meaning of an effort or cost estimate should be understood and communicated consistently and clearly to avoid planning and budgeting mistakes. Results from two studies, one of 42 software companies and one of 423 individual software developers, suggest that this is far from being the case. In both studies we found a large variety in what was meant by an effort estimate and that the meaning was frequently not communicated. To improve the planning and budgeting of software projects we recommend that the meaning of effort estimates is understood and communicated using a probability-based terminology.", "num_citations": "10\n", "authors": ["155"]}
{"title": "Time Predictions: Understanding and Avoiding Unrealism in Project Planning and Everyday Life\n", "abstract": " This book is published open access under a CC BY 4.0 license. Predicting the time needed to complete a project, task or daily activity can be difficult and people frequently underestimate how long an activity will take. This book sheds light on why and when this happens, what we should do to avoid it and how to give more realistic time predictions. It describes methods for predicting time usage in situations with high uncertainty, explains why two plus two is usually more than four in time prediction contexts, reports on research on time prediction biases, and summarizes the evidence in support of different time prediction methods and principles. Based on a comprehensive review of the research, it is the first book summarizing what we know about judgment-based time predictions. Large parts of the book are directed toward people wishing to achieve better time predictions in their professional life, such as project managers, graphic designers, architects, engineers, film producers, consultants, software developers, or anyone else in need of realistic time usage predictions. It is also of benefit to those with a general interest in judgment and decision-making or those who want to improve their ability to predict and plan ahead in daily life.", "num_citations": "8\n", "authors": ["155"]}
{"title": "Believing is seeing: Confirmation bias studies in software engineering\n", "abstract": " Confirmation bias is the human tendency to search for, collect, interpret, analyse, or recall information in a way that confirms one's prior beliefs or preferences. In this paper, we review previous research and demonstrate confirmation bias and its effect in two software engineering contexts. The first study documents that managers bias their interpretation of randomly generated project data towards confirmation of their preferred contract type. The second study reports from an analysis of the results of 35 published comparisons of regression and analogy-based cost estimation models. Twenty of these comparisons evaluate the performance of a self-developed analogy-based estimation model relative to a regression-based model and may be biased towards finding evidence confirming a better accuracy of their own model. A statistical meta-analysis of all 35 comparisons showed that the analogy-based models were\u00a0\u2026", "num_citations": "8\n", "authors": ["155"]}
{"title": "The Ignorance of Confidence Levels in Minimum-Maximum Software Development Effort Intervals\n", "abstract": " Software professionals are frequently asked to provide minimum-maximum effort intervals for a given confidence level. They may for example be asked to provide minimum-maximum intervals where it is 90% likely to include the actual use of effort. If their response is the interval from 800 to 1200 work-hours this should correspond to that it is 90% likely that the actual effort will be more than 800 and less than 1200 work-hours. This effort interval information is, amongst others, used in the planning and budgeting of software projects. In this paper we show that software professionals tend to ignore the confidence levels connected with the minimum-maximum effort intervals. As a consequence, the meaning of minimum-maximum effort interval is unclear and the use of such intervals questionable. The experiment used to document the ignorance of the confidence level is based on requesting one group of software developers to be 98% confident, and another group to be 80% confident when providing their effort intervals. In spite of a difference in confidence levels that should generate quite difference effort intervals, the actual intervals were almost the same. This finding challenge commonly recommended effort uncertainty assessment practices, eg, those implemented in the PERT method, which are based on the assumption that software professionals are able to provide minimum-maximum effort that reflect the stated confidence levels. The finding does also challenge the explanation typically given for too narrow confidence intervals, ie, that people are over-confident. Instead, we propose that a more likely explanation is that people ignore the\u00a0\u2026", "num_citations": "7\n", "authors": ["155"]}
{"title": "First impressions in software development effort estimation: Easy to create and difficult to neutralize\n", "abstract": " Background: Effort estimates of software development work are frequently very inaccurate, which contributes to project management problems and project failures. The most common estimation approach, expert judgment-based estimation, is currently not well understood and more knowledge about how experts create effort estimates may enable better estimation processes and eventually more accurate effort estimates. Aim: Software professionals are affected by their early judgment (first impression) of the effort required for software development work. They are, however, also affected by information presented late (more recently) in the information collection process. This study explores which of these effects, i.e., \u201cfirst impression\u201d or \u201crecency,\u201d that is likely to be the stronger. This is studied in a context where the information leading to the first impression and the update with more recent information are biased in\u00a0\u2026", "num_citations": "7\n", "authors": ["155"]}
{"title": "Individual differences in how much people are affected by irrelevant and misleading information\n", "abstract": " People differ in how much they update their beliefs based on new information. According to a recently proposed theory, individual differences in belief updating are, to some extent, determined by neurological differences, ie, differences in the organization of the brain. The same neurological differences that affect belief updating may also affect handedness. In particular, more mixed-handed people may have a lower threshold for updating beliefs than strongly right-handed people. On the basis of the proposed theory, we hypothesize that mixed-handed software engineers will be more affected by irrelevant and misleading information when providing expert judgments. This hypothesis is tested in five experiments conducted in software engineering contexts. All five experiments supported the hypothesis and suggest that a low threshold for updating beliefs, as measured by degree of mixed-handedness correlates with inaccurate judgment in situations that contain irrelevant or misleading information. On the basis of the results, we argue that software engineering decisions, problem solving and estimation processes should take into account differences in individuals\u2019 threshold for updating belief and not be based on the assumption that one \u201cprocess fits all\u201d.", "num_citations": "7\n", "authors": ["155"]}
{"title": "Interpretation problems related to the use of regression models to decide on economy of scale in software development\n", "abstract": " Many research studies report an economy of scale in software development, i.e., an increase in productivity with increasing project size. Several software practitioners seem, on the other hand, to believe in a diseconomy of scale, i.e., a decrease in productivity with increasing project size. In this paper we argue that violations of essential regression model assumptions in the research studies to a large extent may explain this disagreement. Particularly illustrating is the finding that the use of the production function (Size\u00a0=\u00a0a\u00b7Effortb), instead of the factor input model (Effort\u00a0=\u00a0a\u00b7Sizeb), would most likely have led to the opposite result, i.e., a tendency towards reporting diseconomy of scale in the research studies. We conclude that there are good reasons to warn against the use of regression analysis parameters to investigate economies of scale and to look for other analysis methods when studying economy of scale in\u00a0\u2026", "num_citations": "6\n", "authors": ["155"]}
{"title": "Understanding reasons for errors in software effort estimates\n", "abstract": " This study is a first step towards better processes of understanding why errors occur in software effort estimation. Within one software development company, we collected information about estimation errors through:(1) Interviews with estimation responsible employees in different roles,(2) Estimation experience reports from 68 completed projects, and,(3) Statistical analysis of relations between characteristics of the 68 completed projects and estimation error. We found that the role of the respondents, the data collection approach, and the type of analysis had an important impact on the reasons for estimation error that were given. We found, for example, a strong tendency to perceive factors outside the respondents\u2019 own control as important reasons for inaccurate estimates. Reasons given for accurate estimates, on the other hand, typically cited factors that were within the respondents\u2019 own control, and were determined by the estimators\u2019 skill or experience. This bias in types of reason means that the collection only of project managers\u2019 viewpoints will not yield balanced models of reasons for estimation error. Unfortunately, previous studies on reasons for estimation error have tended to collect information from project managers only. We recommend that software companies combine estimation error information from in-depth interviews with stakeholders in all relevant roles, estimation experience reports, and results from statistical analyses of project characteristics.", "num_citations": "6\n", "authors": ["155"]}
{"title": "An attempt to model software development effort estimation accuracy and bias\n", "abstract": " This paper describes models aiming at explaining the accuracy and bias variation of an organisation\u2019s software development effort estimates. We collected information about variables we believed would impact the estimation accuracy or bias of tasks completed by the organisation. In total, information about 49 software development tasks was collected. We found that the following conditions led to decrease in estimation accuracy: 1) Estimates were provided by a person in the role \u2018software developer\u2019instead of \u2018project leader\u2019, 2) The project had as its highest priority time-todelivery instead of quality or cost, and, 3) The estimator did not participate in the completion of the task. The following conditions led to increased bias towards under-estimation: 1) Estimates were provided by a person with the role \u2018software developer\u2019instead of \u2018project leader\u2019, and, 2) The estimator assessed the accuracy of own estimates of similar, previously completed tasks to be low (more than 20% deviation). Although all variables included in the models were significant (p< 0.1), the explanatory and predictive power of both models was poor, ie, most of the variance in estimation accuracy and bias was not explained or predicted by our models. An analysis of the estimators\u2019 own descriptions of the reasons for achieved estimation accuracy on each task suggests that it will be difficult to include all important estimation accuracy and bias factors in regression-based models. It is, for this reason, not realistic to expect such models to replace human judgment in risk analyses and as input to estimation improvement plans. We believe, nevertheless, that the type of formal analysis\u00a0\u2026", "num_citations": "6\n", "authors": ["155"]}
{"title": "Benefits management and agile practices in software projects: how perceived benefits are impacted\n", "abstract": " Considerable resources are wasted on projects that deliver few or no benefits. The main objective is to better understand the characteristics of projects that are successful in delivering good client benefits. We asked 71 Norwegian software professionals to report information about projects completed between 2016 and 2018. We found that both benefits management and agile practices have a significant relationship with perceived realisation of client benefits. This includes the benefits management practices of having a plan for benefits realisation, individuals with assigned responsibility for benefits realisation, benefits management during project execution, quantification of realised benefits, evaluation of realised benefits, re-estimation of benefits during project execution, and the agile practices of a flexible scope and frequent deliveries to production. The software projects that were successful in delivering client\u00a0\u2026", "num_citations": "5\n", "authors": ["155"]}
{"title": "Cultural characteristics and their connection to increased risk of software project failure\n", "abstract": " Offshoring software projects have been documented to increase the risk of project failure. In particular, the cultural differences between software client and provider countries are believed to increase the risk of project failure. We analyze a large data set of small software projects with providers and clients from various countries. For each provider and client country, and each combination of provider and client country, we calculate the project failure rate. We use Hofstede\u2019s culture dimensions power distance, individualism, masculinity and uncertainty avoidance together with Hall\u2019s communication style to measure cultural characteristics and differences. We found a statistically significant increase in failure rate with increased difference in the provider and client country\u2019s communication style, but no connection between cultural differences and project failure rate using Hofstede\u2019s cultural characteristics. Provider countries, and in particular, client countries with a low-context communication culture had lower proportions of project failures than countries with high-context cultures. A decrease in power distance and increase in individualism were both connected with a decrease in project failure rate. In total, the cultural differences between countries seem to matter less than cultural characteristics of the countries. In particular, some of the cultural characteristics of the client country were strongly connected with project failure rates. Software offshoring clients may benefit from an awareness of the importance of their role in avoiding project failures and consider adopting the cultural characteristics of the most successful client offshoring countries. They may\u00a0\u2026", "num_citations": "5\n", "authors": ["155"]}
{"title": "A preliminary theory of judgment-based project software effort predictions\n", "abstract": " An improvement of the software industry\u2019s software development effort estimation processes may benefit from a better understanding of the mental, partly unconscious, processes involved in estimating effort. This paper proposes and tests a theory potentially explaining essential parts of typical judgment-based effort estimation processes (\u201cexpert estimation\u201d). The theory is based on findings from the human judgment research literature and proposes that judgment-based effort estimation is based on: i) an early categorization of the project to be estimated, ii) a resistance towards a change of the chosen category, and, iii) a \u201cregression\u201d of the effort estimate towards a reference value of the chosen category, where the amount of regression depends on the level of uncertainty of the project work. Implications of the theory are tested with results from three software effort estimation experiments. All examined studies confirmed the theory. There is, however, a strong need for more work, independent evidence and clearer description of scope and concepts part of the theory. Finally, we outline a study planned for further testing of essential parts of the theory.", "num_citations": "5\n", "authors": ["155"]}
{"title": "Comments on'A Simulation Tool for Efficient Analogy Based Cost Estimation', by L. Angelis and I. Stamelos, published in Empirical Software Engineering, 5, 35-68 (2000)\n", "abstract": " The paper \u2018A Simulation Tool for Efficient Analogy Based Cost Estimation\u2019by L. Angelis and I. Stamelos describes a very interesting study on software development effort estimation. As far as we know, this is the very first paper empirically evaluating software development effort prediction interval models! Unfortunately, the regression model based software development effort minimum\u2013maximum intervals seem to be identical to the confidence intervals of the mean effort, instead of the more appropriate effort prediction intervals (of new observations). This error leads to problems with the comparison of \u2018hit rates\u2019, ie, proportion of actual effort inside the minimum\u2013maximum intervals, of the regression model and the analogy \u00fe bootstrap model suggested by the authors. For example, the authors report that the 95% prediction interval, ie, the interval that includes the actual effort with a probability of 95%, for the third smallest\u00a0\u2026", "num_citations": "5\n", "authors": ["155"]}
{"title": "Sequence effects in the estimation of software development effort\n", "abstract": " Currently, little is known about how much the sequence in which software development tasks or projects are estimated affects judgment-based effort estimates. To gain more knowledge, we examined estimation sequence effects in two experiments. In the first experiment, 362 software professionals estimated the effort of three large tasks of similar sizes, whereas in the second experiment 104 software professionals estimated the effort of four large and five small tasks. The sequence of the tasks was randomised in both experiments. The first experiment, with tasks of similar size, showed a mean increase of 10% from the first to the second and a 3% increase from the second to the third estimate. The second experiment showed that estimating a larger task after a smaller one led to a mean decrease in the estimate of 24%, and that estimating a smaller task after a larger one led to a mean increase of 25%. There was no\u00a0\u2026", "num_citations": "4\n", "authors": ["155"]}
{"title": "Software development contracts: the impact of the provider's risk of financial loss on project success\n", "abstract": " Contracts differ in the extent to which software providers risk financial loss in the case of cost overruns. In this paper, we hypothesise that an increase in the provider's risk of financial loss is related to an increased rate of problematic software projects. The hypothesis is tested by comparing software projects using target-price contracts with and without an upper limit on risk sharing. These two contract variants differ mainly in the provider's risk of financial loss. If the provider's risk of a financial loss makes a difference, we would expect that projects using target-price contracts with an upper limit on risk sharing would perform worse than projects using target-price contracts without an upper limit on risk sharing. Data were collected from 24 software professionals who had project experience with both contract variants. The software professionals were asked to assess the success of their last projects when using each of the\u00a0\u2026", "num_citations": "4\n", "authors": ["155"]}
{"title": "Myths and over-simplifications in software engineering\n", "abstract": " The software engineering discipline contains numerous myths and over-simplifications. Some of them may be harmless, but others may hamper evidence-based practices and contribute to a fashion-and myth-based software engineering discipline. In this article we give examples of software engineering myths and over-simplifications and discuss how they are created and spread. One essential mechanism of the creation and spread of myths and over-simplifications are, we argue, people\u2019s tendency towards searching for confirming and neglecting disconfirming evidence. We report from a study examining this tendency. The study demonstrated that the developers who believed in a positive effect of agile methods tended to interpret randomly generated (neutral) project data as evidence confirming the benefit of agile methods. For the purpose of supporting evidence-based practice and avoiding unwanted influence from myths and over-simplifications, we provide a checklist to be used to evaluate the validity of software engineering claims.", "num_citations": "4\n", "authors": ["155"]}
{"title": "Selection of effort estimation strategies\n", "abstract": " We currently know little about the factors that motivate the selection and change of estimation strategy in judgment-based effort estimation context. A better understanding of these issues may lead to more accurate judgment-based effort estimates and motivates the four experiments reported in this paper. The experiments\u2019 two main results are the identification of the importance of \u201cestimation surprises\u201d(large estimation errors) to motivate estimation strategy change and the large individual variation in the initial choice of estimation strategy. The individual variation seems not only to be a result of differences in previous experiences, but also a result of differences in the mental \u201caccessibility\u201d of the strategies. We found, for example, that the use of a strategy was increased when we instructed a developer to use the same type of strategy on unrelated tasks immediately before. The laboratory contexts of the studies means that the results should be interpreted as a first step towards more knowledge about expert estimation strategies and that there is a strong need for more studies, preferably in field situations, before recommending actions on the basis of the findings.", "num_citations": "4\n", "authors": ["155"]}
{"title": "Prediction of overoptimistic predictions\n", "abstract": " Overoptimistic predictions are common in software engineering projects, e.g., the average software project cost overrun is about 30%. This paper examines the use of two popular general tests of optimism (the ASQ and the LOT-R test) to select software engineers that are less likely to provide overoptimistic predictions. A necessary, but not sufficient, condition for this use is that there is a strong relationship between optimism score, as measured by the ASQ and LOT-R tests, and predictions. We report from two experiments on this topic. The experiments suggest that the relation between optimism score as measured by ASQ or LOT-R and predictions is too weak to enable a use of these optimism measurement instruments to select more realistic estimators in software organizations. Our results also suggest that a person\u2019s general level of optimism and over-optimistic predictions of performance are, to a large extent, unrelated.", "num_citations": "4\n", "authors": ["155"]}
{"title": "Evaluating probabilistic software development effort estimates: Maximizing informativeness subject to calibration\n", "abstract": " ContextProbabilistic effort estimates inform about the uncertainty and may give useful input to plans, budgets and investment analyses.Objective & methodThis paper introduces, motivates and illustrates two principles on how to evaluate the accuracy and other performance criteria of probabilistic effort estimates in software development contexts.ResultsThe first principle emphasizes a consistency between the estimation error measure and the loss function of the chosen type of probabilistic single point effort estimates. The second principle points at the importance of not just measuring calibration, but also informativeness of estimated prediction intervals and distributions. The relevance of the evaluation principles is illustrated by a performance evaluation of estimates from twenty-eight software professionals using two different uncertainty assessment methods to estimate the effort of the same thirty software\u00a0\u2026", "num_citations": "3\n", "authors": ["155"]}
{"title": "Looking back on previous estimation error as a method to improve the uncertainty assessment of benefits and costs of software development projects\n", "abstract": " Knowing the uncertainty of estimates of benefits and costs is useful when planning, budgeting and pricing projects. The traditional method for assessing such uncertainty is based on prediction intervals, e.g., asking for minimum and maximum values believed to be 90% likely to include the actual outcome. Studies report that the traditional method typically results in too narrow intervals and intervals that are too symmetric around the estimated most likely outcome when compared with the actual uncertainty of outcomes. We examine whether an uncertainty assessment method based on looking back on the previous estimation error of similar projects leads to wider and less symmetric prediction intervals. Sixty software professionals, with experience from estimating software project costs and benefits, were randomly divided into a group with a traditional or a group with a looking back-based uncertainty assessment\u00a0\u2026", "num_citations": "3\n", "authors": ["155"]}
{"title": "Judgment-updating among software professionals\n", "abstract": " Initial judgments related to key decisions in software projects are often based on one-sided or misleading information. The initial assessment of the benefits of introducing a new development tool may for example be based a vendor\u2019s sales demonstration or a reference client\u2019s favorable description. In this paper we study software professionals\u2019 abilities to adjust their early, biased judgments when receiving contradicting or less biased information. The first study, involving 160 software professionals, found a strong under-adjustment for the impact of misleading information and one-sided argument. A follow-up two weeks later found that this under-adjustment was not removed over time. The second study, involving 65 software professionals, found that the ability to update biased judgments may sometimes be quite good, but that it is hard to predict when. A practical consequence of our results is that software professionals should strongly emphasize the avoidance of biased and potentially misleading information and not trust that they are able to adjust their judgments and beliefs when more reliable and unbiased information are available.", "num_citations": "3\n", "authors": ["155"]}
{"title": "An empirical study of the correlation between development efficiency and software development tools\n", "abstract": " \u2022 MkII Function Points measure the size of the applications in terms of its user required functionality. In spite of the similarities between Albrecht FP and Mk II FP, there is no simple relationship between these two functionality measures. This is, among others, caused by the different views on what Function Points are supposed to measure. While Symons [4] relates his MkII FP to the effort necessary to develop the functionality, Albrecth [8] relates his FP to the value of the functionality. The necessary steps to calculate the Unadjusted MkII Function Points (UFP) are (very briefly):", "num_citations": "3\n", "authors": ["155"]}
{"title": "Overoptimistic Predictions\n", "abstract": " We usually say that a time prediction is overoptimistic when the actual time usage is greater than the predicted time usage. This does not mean that an optimistic or overoptimistic view on time usage was the cause of the too low time prediction. Lack of knowledge, miscalculation during the prediction process, and bad luck in the execution of the project are examples of alternative reasons for too low time predictions. Describing too low, or overoptimistic, time predictions as caused by overoptimism, in the rose-coloured glasses sense, not only is incorrect but may also stop us from seeking other explanations of time overrun besides overoptimism (Roy in Front Psychol 5:624, 2014, [1]).", "num_citations": "2\n", "authors": ["155"]}
{"title": "Working with industry: stories of successful and failed research-industry collaborations on empirical software engineering\n", "abstract": " The software engineering industry should be the laboratory of much, perhaps most, of the empirical software engineering research. Not only would this create a more realistic context and higher external validity of the empirical research, it would also ease the result transfer and make the results more convincing for the industry. Unfortunately, this is currently not the case. About 90% of software engineering experiments are, for example, conducted with students instead of software professionals as subjects. One reason for the lack of industry studies may be that an efficient and sustainable give-and-take-based collaboration between research and industry can be difficult to establish. The collaborations are frequently fragile, end before the research is completed, and lead to a waste of resources for both the researchers and the industrial partners. This paper presents stories and lessons learned from failed and\u00a0\u2026", "num_citations": "2\n", "authors": ["155"]}
{"title": "Software development methods and life cycle models\n", "abstract": " Different software development methods and life cycle models are based on different views on software development. This paper discusses some of these views and describes several software development methods and life cycle models in context of these views. The methods and models described are Structured Systems Analysis and Design Method (SSADM), the ESA Waterfall model, Coad and Yourdon\u2019s Object OrientedAnalysis (OOA), verification oriented software development and two evolutionary life cycle models. In addition software development by Syntax Description, Operational Software development and the spiral model are briefly described. More research on the effects of using different software development methods and life cycle models is needed. The advantages and disadvantages of two different research strategies are therefore briefly discussed.", "num_citations": "2\n", "authors": ["155"]}
{"title": "genus Amaranthus in the Canary Islands, mainly collected by Norwegian botanists\n", "abstract": " genus Amaranthus in the Canary Islands, mainly collected by Norwegian botanists FAO_logo home-icon English Espa\u00f1ol Fran\u00e7ais \u0627\u0644\u0639\u0631\u0628\u064a\u0629 \u4e2d\u6587 \u0420\u0443\u0441\u0441\u043a\u0438\u0439 home-icon Translate with Google Access the full text NOT AVAILABLE Lookup at Google Scholar google-logo Bibliographic information Language : English Type : Journal Article In AGRIS since : 2013 Volume : 1970, 10 Start Page : 5 End Page : 10 All titles : \" genus Amaranthus in the Canary Islands, mainly collected by Norwegian botanists \" \" Genus Amaranthus in the Canary Islands, mainly collected by Norwegian botanists. [Plant taxonomy] \" Save as: AGRIS_AP RIS EndNote(XML) genus Amaranthus in the Canary Islands, mainly collected by Norwegian botanists Loading... Paper Written Paper genus Amaranthus in the Canary Islands, mainly collected by Norwegian botanists [1970] Jorgensen, M. Access the full text NOT AVAILABLE Lookup at Google Scholar -\u2026", "num_citations": "2\n", "authors": ["155"]}
{"title": "Practices connected to perceived client benefits of software projects\n", "abstract": " It is well-documented that many software projects deliver fewer benefits than planned. However prior research has had a stronger focus on the ability to deliver within budget, on time and with the specified functionality, than on what to do to successfully deliver client benefits. The authors have conducted a survey collecting information about benefits management practices, agile practices, use of contracts, and the perceived success in delivery of client benefits. The authors received responses from 83 software professionals with information about 73 recent and 74 older software projects. There was no statistically significant improvement of the delivered client benefits from the older to the recent projects. Statistically significant findings, applying a general linear model-based analysis, include that the degree of success in delivering client benefits is connected to a project having: (i) a plan for how to realise the benefits\u00a0\u2026", "num_citations": "1\n", "authors": ["155"]}
{"title": "Relations between effort estimates, skill indicators, and measured programming skill\n", "abstract": " There are large skill differences among software developers, and clients and managers will benefit from being able to identify those with better skill. This study examines the relations between low effort estimates, and other commonly used skill indicators, and measured programming skill. One hundred and four professional software developers were recruited. After skill-related information was collected, they were asked to estimate the effort for four larger and five smaller programming tasks. Finally, they completed a programming skill test. The lowest and most over-optimistic effort estimates for the larger tasks were given by those with the lowest programming skill, which is in accordance with the well-known Dunning-Kruger effect. For the smaller tasks, however, those with the lowest programming skill had the highest and most over-pessimistic estimates. The other programming skill indicators, such as length of\u00a0\u2026", "num_citations": "1\n", "authors": ["155"]}
{"title": "Estimering av kostnader i store statlige prosjekter: Hvor gode er estimatene og usikkerhetsanalysene i KS2-rapportene?\n", "abstract": " Ordningen med ekstern kvalitetssikring av store statlige investeringsprosjekter (KS-ordningen/statens prosjektmodell) har blant annet som form\u00e5l \u00e5 sikre at budsjettrammer er realistiske og at usikkerhetsvurderingene av kostnadsestimatene gjenspeiler faktisk kostnadsusikkerhet. I hvilken grad rammer, estimater og usikkerhetsvurderinger er realistiske, og hvor det ligger forbedringsmuligheter er hovedtema for denne studien.Kapittel 1 beskriver bakgrunnen og motivasjonen for denne studien. Utgangspunktet er at forskningsprogrammet Concept fortl\u00f8pende samler inn sluttkostnader i prosjekter som har v\u00e6rt gjennom KS2. Det gir grunnlag for studier av utvikling i kostnadskontroll. Etter hvert som utvalget av prosjekter \u00f8ker, kan det ogs\u00e5 gi grunnlag for dypere unders\u00f8kelser av realismen i estimatene som l\u00e5 til grunn for Stortingets fastsettelse av styrings-og kostnadsramme.", "num_citations": "1\n", "authors": ["155"]}
{"title": "Time Prediction Biases\n", "abstract": " To a larger extent than we like to think, our judgements and decisions are affected by irrelevant factors. Fortunately, there are patterns to our irrationality. We are, in a sense, predictably irrational [1]. These patterns of irrationality are what we call judgement and decision biases. This chapter describes some of the biases relevant to understanding when and why we make systematic time prediction errors. Better knowledge about the biases and fallacies may help us become better at designing time prediction processes and avoiding situations and information that mislead us.", "num_citations": "1\n", "authors": ["155"]}
{"title": "Predictions and the Uncertainty of the Future\n", "abstract": " A project manager states that a project will require 432\u00a0hours. Your friend sends you a text message saying that he will be at your place in 12\u00a0minutes. The precision of these time predictions is most likely misleading when interpreted according to the rules of significant digits, where the number of trailing non-zero values indicates the intended accuracy. For example, 432\u00a0hours and 12\u00a0minutes should indicate that the time prediction error is \u00b11\u00a0hour and \u00b11\u00a0minute\u2014not very likely for most types of project or arrival time predictions.", "num_citations": "1\n", "authors": ["155"]}
{"title": "The effect of the time unit on software development effort estimates.\n", "abstract": " Estimates of software development effort are frequently inaccurate and over-optimistic. In this paper we describe how changes in the granularity of the unit of estimation, e.g., work-days instead of work-hours, affects the effort estimates. We describe four psychological mechanisms, how they interact and discuss the expected total effect of higher granularity units on effort estimates. We argue that the mechanisms in general imply that higher granularity effort units will result in higher effort estimates, e.g., that estimating software development work in work-days or weeks will lead to higher estimates than when estimating in work-hours. A possible implication of this predicted effect is that, in contexts where there is a tendency towards under-estimation, estimation in work-days or weeks instead of work-hours leads to more realistic estimates.", "num_citations": "1\n", "authors": ["155"]}
{"title": "Fallacies and biases when adding effort estimates\n", "abstract": " Software professionals do not always clarify what they mean by their effort estimates. Knowing what is meant by an estimate is, however, essential when adding individual effort estimates from a work breakdown structure to find the estimated total effort. Adding the most likely instead of the mean effort of a set of cost elements may result in substantial underestimation of the total effort. In a survey of forty-four software companies we found only two companies that clarified the meaning of their estimates and had a proper method for adding these estimates. The other companies typically added single point estimates without clarifying what they added or with types of estimates likely to give too low estimates of the total effort. We examine the effect of improper addition of estimates and find, for the studied contexts, that summing the most likely effort estimates would lead to a substantial under-estimation of the most likely\u00a0\u2026", "num_citations": "1\n", "authors": ["155"]}
{"title": "Failure Factors of Software Projects at a Global Outsourcing Marketplace\n", "abstract": " The presented study aims at a better understanding of when and why software projects fail. The analysis is based on a data set of 785,325 small-scale software projects at a global outsourcing marketplace. A binary logistic regression model relying solely on information known at the time of a project\u2019s start-up correctly predicted 74% of the project failures and 67% of the non-failures. The model-predicted failure probability corresponded well with the actual frequencies of failures for most levels of failure risk. The model suggests that the factors connected to the strongest reduction in the risk of failure are related to previous collaboration between the client and the provider and a low failure rate of previous projects completed by the provider. We found the characteristics of the client to be almost as important as those of the provider in explaining project failures and that the risk of project failure increased with an increased client emphasis on low price and with an increased project size. The identified relationships seem to be reasonable stable across the studied project size categories, which indicate that the results may potentially be applicable to larger projects than the small-scale outsourcing projects dominating this data set.", "num_citations": "1\n", "authors": ["155"]}
{"title": "How should we compare forecasting models with expert judgement?\n", "abstract": " EconPapers: How should we compare forecasting models with expert judgement? EconPapers Economics at your fingertips EconPapers Home About EconPapers Working Papers Journal Articles Books and Chapters Software Components Authors JEL codes New Economics Papers Advanced Search EconPapers FAQ Archive maintainers FAQ Cookies at EconPapers Format for printing The RePEc blog The RePEc plagiarism page How should we compare forecasting models with expert judgement? Magne Jorgensen International Journal of Forecasting, 2007, vol. 23, issue 3, 473-474 Date: 2007 References: Add references at CitEc Citations: Track citations by RSS feed Downloads: (external link) http://www.sciencedirect.com/science/article/pii/S0169-2070(07)00078-7 Full text for ScienceDirect subscribers only Related works: This item may be available elsewhere in EconPapers: Search for items with the same . : \u2026", "num_citations": "1\n", "authors": ["155"]}
{"title": "How much does a vacation cost?\n", "abstract": " What is a software cost estimate? Is it the most likely cost, the planned cost, the budget, the price, or, something else? Through comparison with vacation cost estimation and a real-life case we illustrate that it is not meaningful to compare and analyze cost estimates unless it is clear which interpretation is applied. Unfortunately, the software industry, software engineering textbooks and scientific estimation studies do frequently not clarify how they apply the term 'cost estimate'. We argue that this lack of clarity may lead to conflicting estimation goals, communication problems, and, learning problems, and provide recommendations on how to deal with these problems.", "num_citations": "1\n", "authors": ["155"]}