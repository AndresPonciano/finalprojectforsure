{"title": "Robustfill: Neural program learning under noisy i/o\n", "abstract": " The problem of automatically generating a computer program from some specification has been studied since the early days of AI. Recently, two competing approaches forautomatic program learning\u2019have received significant attention:(1)neural program synthesis\u2019, where a neural network is conditioned on input/output (I/O) examples and learns to generate a program, and (2)neural program induction\u2019, where a neural network generates new outputs directly using a latent program representation. Here, for the first time, we directly compare both approaches on a large-scale, real-world learning task and we additionally contrast to rule-based program synthesis, which uses hand-crafted semantics to guide the program generation. Our neural models use a modified attention RNN to allow encoding of variable-sized sets of I/O pairs, which achieve 92\\% accuracy on a real-world test set, compared to the 34\\% accuracy of the previous best neural synthesis approach. The synthesis model also outperforms a comparable induction model on this task, but we more importantly demonstrate that the strength of each approach is highly dependent on the evaluation metric and end-user application. Finally, we show that we can train our neural models to remain very robust to the type of noise expected in real-world data (eg, typos), while a highly-engineered rule-based system fails entirely.", "num_citations": "271\n", "authors": ["1699"]}
{"title": "Neuro-symbolic program synthesis\n", "abstract": " Recent years have seen the proposal of a number of neural architectures for the problem of Program Induction. Given a set of input-output examples, these architectures are able to learn mappings that generalize to new test inputs. While achieving impressive results, these approaches have a number of important limitations: (a) they are computationally expensive and hard to train, (b) a model has to be trained for each task (program) separately, and (c) it is hard to interpret or verify the correctness of the learnt mapping (as it is defined by a neural network). In this paper, we propose a novel technique, Neuro-Symbolic Program Synthesis, to overcome the above-mentioned problems. Once trained, our approach can automatically construct computer programs in a domain-specific language that are consistent with a set of input-output examples provided at test time. Our method is based on two novel neural modules. The first module, called the cross correlation I/O network, given a set of input-output examples, produces a continuous representation of the set of I/O examples. The second module, the Recursive-Reverse-Recursive Neural Network (R3NN), given the continuous representation of the examples, synthesizes a program by incrementally expanding partial programs. We demonstrate the effectiveness of our approach by applying it to the rich and complex domain of regular expression based string transformations. Experiments show that the R3NN model is not only able to construct programs from new input-output examples, but it is also able to construct new programs for tasks that it had never observed before during training.", "num_citations": "254\n", "authors": ["1699"]}
{"title": "Programmatically interpretable reinforcement learning\n", "abstract": " We present a reinforcement learning framework, called Programmatically Interpretable Reinforcement Learning (PIRL), that is designed to generate interpretable and verifiable agent policies. Unlike the popular Deep Reinforcement Learning (DRL) paradigm, which represents policies by neural networks, PIRL represents policies using a high-level, domain-specific programming language. Such programmatic policies have the benefits of being more easily interpreted than neural networks, and being amenable to verification by symbolic methods. We propose a new method, called Neurally Directed Program Search (NDPS), for solving the challenging nonsmooth optimization problem of finding a programmatic policy with maximal reward. NDPS works by first learning a neural policy network using DRL, and then performing a local search over programmatic policies that seeks to minimize a distance from this neural \u201coracle\u201d. We evaluate NDPS on the task of learning to drive a simulated car in the TORCS car-racing environment. We demonstrate that NDPS is able to discover human-readable policies that pass some significant performance bars. We also show that PIRL policies can have smoother trajectories, and can be more easily transferred to environments not encountered during training, than corresponding policies discovered by DRL.", "num_citations": "174\n", "authors": ["1699"]}
{"title": "OverCode: Visualizing variation in student solutions to programming problems at scale\n", "abstract": " In MOOCs, a single programming exercise may produce thousands of solutions from learners. Understanding solution variation is important for providing appropriate feedback to students at scale. The wide variation among these solutions can be a source of pedagogically valuable examples and can be used to refine the autograder for the exercise by exposing corner cases. We present OverCode, a system for visualizing and exploring thousands of programming solutions. OverCode uses both static and dynamic analysis to cluster similar solutions, and lets teachers further filter and cluster solutions based on different criteria. We evaluated OverCode against a nonclustering baseline in a within-subjects study with 24 teaching assistants and found that the OverCode interface allows teachers to more quickly develop a high-level view of students' understanding and misconceptions, and to provide feedback that is\u00a0\u2026", "num_citations": "134\n", "authors": ["1699"]}
{"title": "Leveraging grammar and reinforcement learning for neural program synthesis\n", "abstract": " Program synthesis is the task of automatically generating a program consistent with a specification. Recent years have seen proposal of a number of neural approaches for program synthesis, many of which adopt a sequence generation paradigm similar to neural machine translation, in which sequence-to-sequence models are trained to maximize the likelihood of known reference programs. While achieving impressive results, this strategy has two key limitations. First, it ignores Program Aliasing: the fact that many different programs may satisfy a given specification (especially with incomplete specifications such as a few input-output examples). By maximizing the likelihood of only a single reference program, it penalizes many semantically correct programs, which can adversely affect the synthesizer performance. Second, this strategy overlooks the fact that programs have a strict syntax that can be efficiently checked. To address the first limitation, we perform reinforcement learning on top of a supervised model with an objective that explicitly maximizes the likelihood of generating semantically correct programs. For addressing the second limitation, we introduce a training procedure that directly maximizes the probability of generating syntactically correct programs that fulfill the specification. We show that our contributions lead to improved accuracy of the models, especially in cases where the training data is limited.", "num_citations": "123\n", "authors": ["1699"]}
{"title": "Automated correction for syntax errors in programming assignments using recurrent neural networks\n", "abstract": " We present a method for automatically generating repair feedback for syntax errors for introductory programming problems. Syntax errors constitute one of the largest classes of errors (34%) in our dataset of student submissions obtained from a MOOC course on edX. The previous techniques for generating automated feed- back on programming assignments have focused on functional correctness and style considerations of student programs. These techniques analyze the program AST of the program and then perform some dynamic and symbolic analyses to compute repair feedback. Unfortunately, it is not possible to generate ASTs for student pro- grams with syntax errors and therefore the previous feedback techniques are not applicable in repairing syntax errors. We present a technique for providing feedback on syntax errors that uses Recurrent neural networks (RNNs) to model syntactically valid token sequences. Our approach is inspired from the recent work on learning language models from Big Code (large code corpus). For a given programming assignment, we first learn an RNN to model all valid token sequences using the set of syntactically correct student submissions. Then, for a student submission with syntax errors, we query the learnt RNN model with the prefix to- ken sequence to predict token sequences that can fix the error by either replacing or inserting the predicted token sequence at the error location. We evaluate our technique on over 14, 000 student submissions with syntax errors. Our technique can completely re- pair 31.69% (4501/14203) of submissions with syntax errors and in addition partially correct 6.39% (908\u00a0\u2026", "num_citations": "101\n", "authors": ["1699"]}
{"title": "Synthesizing data structure manipulations from storyboards\n", "abstract": " We present the Storyboard Programming framework, a new synthesis system designed to help programmers write imperative low-level data-structure manipulations. The goal of this system is to bridge the gap between the\" boxes-and-arrows\" diagrams that programmers often use to think about data-structure manipulation algorithms and the low-level imperative code that implements them. The system takes as input a set of partial input-output examples, as well as a description of the high-level structure of the desired solution. From this information, it is able to synthesize low-level imperative implementations in a matter of minutes.", "num_citations": "97\n", "authors": ["1699"]}
{"title": "Blinkfill: Semi-supervised programming by example for syntactic string transformations\n", "abstract": " The recent Programming By Example (PBE) techniques such as FlashFill have shown great promise for enabling end-users to perform data transformation tasks using input-output examples. Since examples are inherently an under-specification, there are typically a large number of hypotheses conforming to the examples, and the PBE techniques suffer from scalability issues for finding the intended program amongst the large space. We present a semi-supervised learning technique to significantly reduce this ambiguity by using the logical information present in the input data to guide the synthesis algorithm. We develop a data structure InputDataGraph to succinctly represent a large set of logical patterns that are shared across the input data, and use this graph to efficiently learn substring expressions in a new PBE system BlinkFill. We evaluate BlinkFill on 207 real-world benchmarks and show that BlinkFill is\u00a0\u2026", "num_citations": "96\n", "authors": ["1699"]}
{"title": "Natural language to structured query generation via meta-learning\n", "abstract": " In conventional supervised training, a model is trained to fit all the training examples. However, having a monolithic model may not always be the best strategy, as examples could vary widely. In this work, we explore a different learning protocol that treats each example as a unique pseudo-task, by reducing the original learning problem to a few-shot meta-learning scenario with the help of a domain-dependent relevance function. When evaluated on the WikiSQL dataset, our approach leads to faster convergence and achieves 1.1%-5.4% absolute accuracy gains over the non-meta-learning counterparts.", "num_citations": "92\n", "authors": ["1699"]}
{"title": "Dynamic neural program embedding for program repair\n", "abstract": " Neural program embeddings have shown much promise recently for a variety of program analysis tasks, including program synthesis, program repair, fault localization, etc. However, most existing program embeddings are based on syntactic features of programs, such as raw token sequences or abstract syntax trees. Unlike images and text, a program has an unambiguous semantic meaning that can be difficult to capture by only considering its syntax (i.e. syntactically similar pro- grams can exhibit vastly different run-time behavior), which makes syntax-based program embeddings fundamentally limited. This paper proposes a novel semantic program embedding that is learned from program execution traces. Our key insight is that program states expressed as sequential tuples of live variable values not only captures program semantics more precisely, but also offer a more natural fit for Recurrent Neural Networks to model. We evaluate different syntactic and semantic program embeddings on predicting the types of errors that students make in their submissions to an introductory programming class and two exercises on the CodeHunt education platform. Evaluation results show that our new semantic program embedding significantly outperforms the syntactic program embeddings based on token sequences and abstract syntax trees. In addition, we augment a search-based program repair system with the predictions obtained from our se- mantic embedding, and show that search efficiency is also significantly improved.", "num_citations": "89\n", "authors": ["1699"]}
{"title": "Not all bytes are equal: Neural byte sieve for fuzzing\n", "abstract": " Fuzzing is a popular dynamic program analysis technique used to find vulnerabilities in complex software. Fuzzing involves presenting a target program with crafted malicious input designed to cause crashes, buffer overflows, memory errors, and exceptions. Crafting malicious inputs in an efficient manner is a difficult open problem and often the best approach to generating such inputs is through applying uniform random mutations to pre-existing valid inputs (seed files). We present a learning technique that uses neural networks to learn patterns in the input files from past fuzzing explorations to guide future fuzzing explorations. In particular, the neural models learn a function to predict good (and bad) locations in input files to perform fuzzing mutations based on the past mutations and corresponding code coverage information. We implement several neural models including LSTMs and sequence-to-sequence models that can encode variable length input files. We incorporate our models in the state-of-the-art AFL (American Fuzzy Lop) fuzzer and show significant improvements in terms of code coverage, unique code paths, and crashes for various input formats including ELF, PNG, PDF, and XML.", "num_citations": "71\n", "authors": ["1699"]}
{"title": "Neural program meta-induction\n", "abstract": " Most recently proposed methods for Neural Program Induction work under the assumption of having a large set of input/output (I/O) examples for learning any underlying input-output mapping. This paper aims to address the problem of data and computation efficiency of program induction by leveraging information from related tasks. Specifically, we propose two approaches for cross-task knowledge transfer to improve program induction in limited-data scenarios. In our first proposal, portfolio adaptation, a set of induction models is pretrained on a set of related tasks, and the best model is adapted towards the new task using transfer learning. In our second approach, meta program induction, a -shot learning approach is used to make a model generalize to new tasks without additional training. To test the efficacy of our methods, we constructed a new benchmark of programs written in the Karel programming language. Using an extensive experimental evaluation on the Karel benchmark, we demonstrate that our proposals dramatically outperform the baseline induction method that does not use knowledge transfer. We also analyze the relative performance of the two approaches and study conditions in which they perform best. In particular, meta induction outperforms all existing approaches under extreme data sparsity (when a very small number of examples are available), i.e., fewer than ten. As the number of available I/O examples increase (i.e. a thousand or more), portfolio adapted program induction becomes the best approach. For intermediate data sizes, we demonstrate that the combined method of adapted meta program induction has the\u00a0\u2026", "num_citations": "62\n", "authors": ["1699"]}
{"title": "Program synthesis using abstraction refinement\n", "abstract": " We present a new approach to example-guided program synthesis based on counterexample-guided abstraction refinement. Our method uses the abstract semantics of the underlying DSL to find a program P whose abstract behavior satisfies the examples. However, since program P may be spurious with respect to the concrete semantics, our approach iteratively refines the abstraction until we either find a program that satisfies the examples or prove that no such DSL program exists. Because many programs have the same input-output behavior in terms of their abstract semantics, this synthesis methodology significantly reduces the search space compared to existing techniques that use purely concrete semantics.  While synthesis using abstraction refinement (SYNGAR) could be implemented in different settings, we propose a refinement-based synthesis algorithm that uses abstract finite tree automata (AFTA\u00a0\u2026", "num_citations": "58\n", "authors": ["1699"]}
{"title": "Search, align, and repair: data-driven feedback generation for introductory programming exercises\n", "abstract": " This paper introduces the \u201cSearch, Align, and Repair\u201d data-driven program repair framework to automate feedback generation for introductory programming exercises. Distinct from existing techniques, our goal is to develop an efficient, fully automated, and problem-agnostic technique for large or MOOC-scale introductory programming courses. We leverage the large amount of available student submissions in such settings and develop new algorithms for identifying similar programs, aligning correct and incorrect programs, and repairing incorrect programs by finding minimal fixes. We have implemented our technique in the Sarfgen system and evaluated it on thousands of real student attempts from the Microsoft-DEV204. 1x edX course and the Microsoft CodeHunt platform. Our results show that Sarfgen can, within two seconds on average, generate concise, useful feedback for 89.7% of the incorrect student\u00a0\u2026", "num_citations": "55\n", "authors": ["1699"]}
{"title": "Neuro-symbolic program corrector for introductory programming assignments\n", "abstract": " Automatic correction of programs is a challenging problem with numerous real world applications in security, verification, and education. One application that is becoming increasingly important is the correction of student submissions in online courses for providing feedback. Most existing program repair techniques analyze Abstract Syntax Trees (ASTs) of programs, which are unfortunately unavailable for programs with syntax errors. In this paper, we propose a novel Neuro-symbolic approach that combines neural networks with constraint-based reasoning. Specifically, our method first uses a Recurrent Neural Network (RNN) to perform syntax repairs for the buggy programs; subsequently, the resulting syntactically-fixed programs are repaired using constraint-based techniques to ensure functional correctness. The RNNs are trained using a corpus of syntactically correct submissions for a given programming\u00a0\u2026", "num_citations": "53\n", "authors": ["1699"]}
{"title": "Interpreting neural network judgments via minimal, stable, and symbolic corrections\n", "abstract": " We present a new algorithm to generate minimal, stable, and symbolic corrections to an input that will cause a neural network with ReLU activations to change its output. We argue that such a correction is a useful way to provide feedback to a user when the network's output is different from a desired output. Our algorithm generates such a correction by solving a series of linear constraint satisfaction problems. The technique is evaluated on three neural network models: one predicting whether an applicant will pay a mortgage, one predicting whether a first-order theorem can be proved efficiently by a solver using certain heuristics, and the final one judging whether a drawing is an accurate rendition of a canonical drawing of a cat.", "num_citations": "39\n", "authors": ["1699"]}
{"title": "Synthesis of data completion scripts using finite tree automata\n", "abstract": " In application domains that store data in a tabular format, a common task is to fill the values of some cells using values stored in other cells. For instance, such data completion tasks arise in the context of missing value imputation in data science and derived data computation in spreadsheets and relational databases. Unfortunately, end-users and data scientists typically struggle with many data completion tasks that require non-trivial programming expertise. This paper presents a synthesis technique for automating data completion tasks using programming-by-example (PBE) and a very lightweight sketching approach. Given a formula sketch (e.g., AVG(?1, ?2)) and a few input-output examples for each hole, our technique synthesizes a program to automate the desired data completion task. Towards this goal, we propose a domain-specific language (DSL) that combines spatial and relational reasoning over tabular\u00a0\u2026", "num_citations": "35\n", "authors": ["1699"]}
{"title": "Understanding conversational programmers: A perspective from the software industry\n", "abstract": " Recent research suggests that some students learn to program with the goal of becoming conversational programmers: they want to develop programming literacy skills not to write code in the future but mainly to develop conversational skills and communicate better with developers and to improve their marketability. To investigate the existence of such a population of conversational programmers in practice, we surveyed professionals at a large multinational technology company who were not in software development roles. Based on 3151 survey responses from professionals who never or rarely wrote code, we found that a significant number of them (42.6%) had invested in learning programming on the job. While many of these respondents wanted to perform traditional end-user programming tasks (eg, data analysis), we discovered that two top motivations for learning programming were to improve the efficacy of\u00a0\u2026", "num_citations": "33\n", "authors": ["1699"]}
{"title": "Learning component interfaces with may and must abstractions\n", "abstract": " Component interfaces are the essence of modular program analysis. In this work, a component interface documents correct sequences of invocations to the component\u2019s public methods. We present an automated framework that extracts finite safe, permissive, and minimal interfaces, from potentially infinite software components. Our proposed framework uses the L* automata-learning algorithm to learn finite interfaces for an infinite-state component. It is based on the observation that an interface permissive with respect to the component\u2019s must abstraction and safe with respect to its may abstraction provides a precise characterization of the legal invocations to the methods of the concrete component. The abstractions are refined automatically from counterexamples obtained during the reachability checks performed by our framework. The use of must abstractions enables us to avoid an exponentially\u00a0\u2026", "num_citations": "27\n", "authors": ["1699"]}
{"title": "Semantic code repair using neuro-symbolic transformation networks\n", "abstract": " We study the problem of semantic code repair, which can be broadly defined as automatically fixing non-syntactic bugs in source code. The majority of past work in semantic code repair assumed access to unit tests against which candidate repairs could be validated. In contrast, the goal here is to develop a strong statistical model to accurately predict both bug locations and exact fixes without access to information about the intended correct behavior of the program. Achieving such a goal requires a robust contextual repair model, which we train on a large corpus of real-world source code that has been augmented with synthetically injected bugs. Our framework adopts a two-stage approach where first a large set of repair candidates are generated by rule-based processors, and then these candidates are scored by a statistical model using a novel neural network architecture which we refer to as Share, Specialize, and Compete. Specifically, the architecture (1) generates a shared encoding of the source code using an RNN over the abstract syntax tree, (2) scores each candidate repair using specialized network modules, and (3) then normalizes these scores together so they can compete against one another in comparable probability space. We evaluate our model on a real-world test set gathered from GitHub containing four common categories of bugs. Our model is able to predict the exact correct repair 41\\% of the time with a single guess, compared to 13\\% accuracy for an attentional sequence-to-sequence model.", "num_citations": "25\n", "authors": ["1699"]}
{"title": "Modular synthesis of sketches using models\n", "abstract": " One problem with the constraint-based approaches to synthesis that have become popular over the last few years is that they only scale to relatively small routines, on the order of a few dozen lines of code. This paper presents a mechanism for modular reasoning that allows us to break larger synthesis problems into small manageable pieces. The approach builds on previous work in the verification community of using high-level specifications and partially interpreted functions (we call them models) in place of more complex pieces of code in order to make the analysis modular.             The main contribution of this paper is to show how to combine these techniques with the counterexample guided synthesis approaches used to efficiently solve synthesis problems. Specifically, we show two new algorithms; one to efficiently synthesize functions that use models, and another one to synthesize functions while\u00a0\u2026", "num_citations": "22\n", "authors": ["1699"]}
{"title": "Melford: Using neural networks to find spreadsheet errors\n", "abstract": " Spreadsheets are widely used for financial and other types of important numerical computations. Spreadsheet errors have accounted for hundreds of millions of dollars of financial losses, but tools for finding errors in spreadsheets are still quite primitive. At the same time, deep learning techniques have led to great advances in complex tasks such as speech and image recognition. In this paper, we show that applying neural networks to spreadsheets allows us to find an important class of error with high precision. The specific errors we detect are cases where an author has placed a number where there should be a formula, such as in the row totaling the numbers in a column. We use a spatial abstraction of the cells around a particular cell to build a classifier that predicts whether a cell should contain a formula whenever it contains a number. Our approach requires no labeled data and allows us to rapidly explore potential new classifiers to improve the effectiveness of the technique. Our classifier has a low false positive rate and finds more than 150 real errors in a collection of 70 benchmark workbooks. We also applied Melford to almost all of the financial spreadsheets in the EUSES corpus and within hours confirmed real errors that were previously unknown to us in 26 of the 696 workbooks. We believe that applying neural networks to helping individuals reason about the structure and content of spreadsheets has great potential.", "num_citations": "21\n", "authors": ["1699"]}
{"title": "Synthetic datasets for neural program synthesis\n", "abstract": " The goal of program synthesis is to automatically generate programs in a particular language from corresponding specifications, e.g. input-output behavior. Many current approaches achieve impressive results after training on randomly generated I/O examples in limited domain-specific languages (DSLs), as with string transformations in RobustFill. However, we empirically discover that applying test input generation techniques for languages with control flow and rich input space causes deep networks to generalize poorly to certain data distributions; to correct this, we propose a new methodology for controlling and evaluating the bias of synthetic data distributions over both programs and specifications. We demonstrate, using the Karel DSL and a small Calculator DSL, that training deep networks on these distributions leads to improved cross-distribution generalization performance.", "num_citations": "19\n", "authors": ["1699"]}
{"title": "AP: artificial programming\n", "abstract": " The ability to automatically discover a program consistent with a given user intent (specification) is the holy grail of Computer Science. While significant progress has been made on the so-called problem of Program Synthesis, a number of challenges remain; particularly for the case of synthesizing richer and larger programs. This is in large part due to the difficulty of search over the space of programs. In this paper, we argue that the above-mentioned challenge can be tackled by learning synthesizers automatically from a large amount of training data. We present a first step in this direction by describing our novel synthesis approach based on two neural architectures for tackling the two key challenges of Learning to understand partial input-output specifications and Learning to search programs. The first neural architecture called the Spec Encoder computes a continuous representation of the specification, whereas the second neural architecture called the Program Generator incrementally constructs programs in a hypothesis space that is conditioned by the specification vector. The key idea of the approach is to train these architectures using a large set of (spec, P) pairs, where P denotes a program sampled from the DSL L and spec denotes the corresponding specification satisfied by P. We demonstrate the effectiveness of our approach on two preliminary instantiations. The first instantiation, called Neural FlashFill, corresponds to the domain of string manipulation programs similar to that of FlashFill. The second domain considers string transformation programs consisting of composition of API functions. We show that a neural system is able to\u00a0\u2026", "num_citations": "17\n", "authors": ["1699"]}
{"title": "Deep API programmer: Learning to program with APIs\n", "abstract": " We present DAPIP, a Programming-By-Example system that learns to program with APIs to perform data transformation tasks. We design a domain-specific language (DSL) that allows for arbitrary concatenations of API outputs and constant strings. The DSL consists of three family of APIs: regular expression-based APIs, lookup APIs, and transformation APIs. We then present a novel neural synthesis algorithm to search for programs in the DSL that are consistent with a given set of examples. The search algorithm uses recently introduced neural architectures to encode input-output examples and to model the program search in the DSL. We show that synthesis algorithm outperforms baseline methods for synthesizing programs on both synthetic and real-world benchmarks.", "num_citations": "16\n", "authors": ["1699"]}
{"title": "SPT: storyboard programming tool\n", "abstract": " We present Spt, a tool that helps programmers write low-level data-structure manipulations by combining various forms of insights such as abstract and concrete input-output examples as well as implementation skeletons. When programmers write such manipulations, they typically have a clear high-level intuition about how the manipulation should work, but implementing efficient low-level pointer manipulating code is error-prone. Our tool aims to bridge the gap between the intuition and the corresponding implementation by automatically synthesizing the implementation. The tool frames the synthesis problem as a generalization of an abstract-interpretation based shape analysis, and represents the problem as a set of constraints which are solved efficiently by the Sketch solver. We report the successful evaluation of our tool on synthesizing several linked list and binary search tree manipulations.", "num_citations": "16\n", "authors": ["1699"]}
{"title": "Learning transferable graph exploration\n", "abstract": " This paper considers the problem of efficient exploration of unseen environments, a key challenge in AI. We propose a learning to explore'framework where we learn a policy from a distribution of environments. At test time, presented with an unseen environment from the same distribution, the policy aims to generalize the exploration strategy to visit the maximum number of unique states in a limited number of steps. We particularly focus on environments with graph-structured state-spaces that are encountered in many important real-world applications like software testing and map building.", "num_citations": "13\n", "authors": ["1699"]}
{"title": "WebRelate: integrating web data with spreadsheets using examples\n", "abstract": " Data integration between web sources and relational data is a key challenge faced by data scientists and spreadsheet users. There are two main challenges in programmatically joining web data with relational data. First, most websites do not expose a direct interface to obtain tabular data, so the user needs to formulate a logic to get to different webpages for each input row in the relational table. Second, after reaching the desired webpage, the user needs to write complex scripts to extract the relevant data, which is often conditioned on the input data. Since many data scientists and end-users come from diverse backgrounds, writing such complex regular-expression based logical scripts to perform data integration tasks is unfortunately often beyond their programming expertise.   We present WebRelate, a system that allows users to join semi-structured web data with relational data in spreadsheets using input\u00a0\u2026", "num_citations": "13\n", "authors": ["1699"]}
{"title": "Data-driven feedback generator for online programing courses\n", "abstract": " Manually providing feedback for programming assignments is a tedious task in traditional classroom education. The challenge increases drastically in Massive open online courses (MOOCs), where the student-teacher ratio can reach thousands to one or even millions to one. Despite the necessity, the current automated feedback approaches suffer from significant weaknesses: inability to scale to larger programs, manual involvement of teacher effort, and lack of precision for pin-pointing errors. We present a technique to tackle these challenges by developing a data-driven automated grader, iGrader, capable of generating instant and precise feedback for programming assignments.", "num_citations": "13\n", "authors": ["1699"]}
{"title": "Neural network for program synthesis\n", "abstract": " Described are systems, methods, and computer-readable media for program generation in a domain-specific language based on input-output examples. In accordance with various embodiments, a neural-network-based program generation model conditioned on an encoded set of input-output examples is used to generate a program tree by iteratively expanding a partial program tree, beginning with a root node and ending when all leaf nodes are terminal.", "num_citations": "10\n", "authors": ["1699"]}
{"title": "BUSTLE: Bottom-Up program synthesis through learning-guided exploration\n", "abstract": " Program synthesis is challenging largely because of the difficulty of search in a large space of programs. Human programmers routinely tackle the task of writing complex programs by writing sub-programs and then analyzing their intermediate results to compose them in appropriate ways. Motivated by this intuition, we present a new synthesis approach that leverages learning to guide a bottom-up search over programs. In particular, we train a model to prioritize compositions of intermediate values during search conditioned on a given set of input-output examples. This is a powerful combination because of several emergent properties. First, in bottom-up search, intermediate programs can be executed, providing semantic information to the neural network. Second, given the concrete values from those executions, we can exploit rich features based on recent work on property signatures. Finally, bottom-up search allows the system substantial flexibility in what order to generate the solution, allowing the synthesizer to build up a program from multiple smaller sub-programs. Overall, our empirical evaluation finds that the combination of learning and bottom-up search is remarkably effective, even with simple supervised learning approaches. We demonstrate the effectiveness of our technique on two datasets, one from the SyGuS competition and one of our own creation.", "num_citations": "10\n", "authors": ["1699"]}
{"title": "Augmented example-based synthesis using relational perturbation properties\n", "abstract": " Example-based specifications for program synthesis are inherently ambiguous and may cause synthesizers to generate programs that do not exhibit intended behavior on unseen inputs. Existing synthesis techniques attempt to address this problem by either placing a domain-specific syntactic bias on the hypothesis space or heavily relying on user feedback to help resolve ambiguity.   We present a new framework to address the ambiguity/generalizability problem in example-based synthesis. The key feature of our framework is that it places a semantic bias on the hypothesis space using \"relational perturbation properties\" that relate the perturbation/change in a program output to the perturbation/change in a program input. An example of such a property is permutation invariance: the program output does not change when the elements of the program input (array) are permuted. The framework is portable across\u00a0\u2026", "num_citations": "9\n", "authors": ["1699"]}
{"title": "Machine learning for constrained mutation-based fuzz testing\n", "abstract": " Techniques for constrained mutation-based fuzzing are described. Machine accesses an input file of code for testing. Machine performs multiple runs of a fuzzing algorithm using the input file and the code. Each run includes: performing a mutation of one or more bytes of the input file and determining which parts of the code were executed when the code was run with the mutated input file. Machine stores, for each run, an indication of whether the mutation caused execution of a portion of the code which was not executed prior to the mutation, Machine generates heatmap of the input file based on the stored indications. The heatmap maps each of the bytes in the input file to a value indicating whether the mutation of the byte caused execution of the portion of the code for testing which was not executed prior to the mutation. Machine tailors fuzzing algorithm based on heatmap.", "num_citations": "7\n", "authors": ["1699"]}
{"title": "Input-output example encoding\n", "abstract": " Generally discussed herein are devices, systems, and methods for encoding input-output examples. A method of generating a program using an encoding of input-output examples, may include processing an input example of the input-output examples, using a first long short term memory (LSTM) neural network, one character at a time to produce an input feature vector, processing an output example associated with the input example in the input-output examples, using the LSTM neural network, one character at a time to produce an output feature vector, determining (a) a cross-correlation between the input feature vector and the output feature vector or (b) previously computed feature vectors for a different input-output example that are sufficiently close to the input feature vector and the output feature vector, respectively, and using the determined cross-correlation or previously computed vector, generating a\u00a0\u2026", "num_citations": "6\n", "authors": ["1699"]}
{"title": "TF-Coder: Program Synthesis for Tensor Manipulations\n", "abstract": " The success and popularity of deep learning is on the rise, partially due to powerful deep learning frameworks such as TensorFlow and PyTorch that make it easier to develop deep learning models. However, these libraries also come with steep learning curves, since programming in these frameworks is quite different from traditional imperative programming with explicit loops and conditionals. In this work, we present a tool called TF-Coder for programming by example in TensorFlow. TF-Coder uses a bottom-up weighted enumerative search, with value-based pruning of equivalent expressions and flexible type- and value-based filtering to ensure that expressions adhere to various requirements imposed by the TensorFlow library. We also train models that predict TensorFlow operations from features of the input and output tensors and natural language descriptions of tasks, and use the models to prioritize relevant operations during the search. TF-Coder solves 63 of 70 real-world tasks within 5 minutes, often achieving superhuman performance -- finding solutions that are simpler than those written by TensorFlow experts, in less time as well.", "num_citations": "6\n", "authors": ["1699"]}
{"title": "Neural-guided symbolic regression with asymptotic constraints\n", "abstract": " Symbolic regression is a type of discrete optimization problem that involves searching expressions that fit given data points. In many cases, other mathematical constraints about the unknown expression not only provide more information beyond just values at some inputs, but also effectively constrain the search space. We identify the asymptotic constraints of leading polynomial powers as the function approaches zero and infinity as useful constraints and create a system to use them for symbolic regression. The first part of the system is a conditional production rule generating neural network which preferentially generates production rules to construct expressions with the desired leading powers, producing novel expressions outside the training domain. The second part, which we call Neural-Guided Monte Carlo Tree Search, uses the network during a search to find an expression that conforms to a set of data points and desired leading powers. Lastly, we provide an extensive experimental validation on thousands of target expressions showing the efficacy of our system compared to exiting methods for finding unknown functions outside of the training set.", "num_citations": "6\n", "authors": ["1699"]}
{"title": "Accessible Programming using Program Synthesis\n", "abstract": " New computing platforms have greatly increased the demand for programmers, but learning to program remains a big challenge. Program synthesis techniques have the potential to revolutionize programming by making it more accessible. In this dissertation, I present three systems, AutoProf, FlashFill, and Storyboard Programming Tool (Spt), that work towards making programming more accessible to a large class of people, namely students and end-users. The AutoProf (Automated Program Feedback) system provides automated feedback to students on introductory programming assignments. It has been successfully piloted on thousands of student submissions from an edX course and is currently being integrated on the MITx and edX platforms. The FlashFill system helps spreadsheet end-users perform semantic string transformations, number transformations, and table lookup transformations using few input-output examples. A part of the FlashFill system is shipping in Microsoft Excel 2013 and was quoted as one of the top features in Excel by many press articles. Finally, the Storyboard Programming Tool helps students write data structure manipulations using visual examples similar to the ones used in textbooks and classrooms. It has been used to synthesize many textbook manipulations over linked list, binary search trees, and graphs.These systems are enabled by new Program Synthesis techniques. Unlike traditional program synthesis approaches where the primary goal was to derive provably correct programs from a complete specification, these synthesis techniques are designed around natural specification mechanisms and intuitive\u00a0\u2026", "num_citations": "6\n", "authors": ["1699"]}
{"title": "Predicting spreadsheet properties\n", "abstract": " A device includes a logic machine and a data-holding machine having instructions executable by the logic machine to receive a spreadsheet including a plurality of cells, apply an abstraction to the spreadsheet that defines one or more features of a cell set including one or more cells of the plurality of cells to form an abstracted representation of the spreadsheet, form, for the cell set, an input vector for a machine-learning prediction function from the abstracted representation of the spreadsheet, the machine-learning prediction function configured to output a prediction of one or more properties of the cell set based on the input vector, wherein the machine-learning prediction function is previously trained based on a plurality of previously-created spreadsheets, provide the input vector to the machine-learning prediction function; and output the prediction from the machine-learning prediction function.", "num_citations": "5\n", "authors": ["1699"]}
{"title": "Approaches and Applications of Inductive Programming: Third International Workshop, AAIP 2009, Edinburgh, UK, September 4, 2009, Revised Papers\n", "abstract": " This book constitutes revised papers of the Third International Workshop on approaches and Applications of Inductive Programming, AAIP 2009, held in Edinburgh, UK, in September 2009. The 7 full papers included in this volume were carefully reviewed and selected. The book also contains two invited papers.", "num_citations": "5\n", "authors": ["1699"]}
{"title": "AvatarSAT: An auto-tuning boolean SAT solver\n", "abstract": " We present AVATARSAT, a SAT solver that uses machine-learning classifiers to automatically tune the heuristics of an off-the-shelf SAT solver on a per-instance basis. The classifiers use features of both the input and conflict clauses to select parameter settings for the solver\u2019s tunable heuristics. On a randomly selected set of SAT problems chosen from the 2007 and 2008 SAT competitions, AVATARSAT is, on average, over two times faster than MINISAT based on the geometric mean speedup measure and 50% faster based on the arithmetic mean speedup measure. Moreover, AVATARSAT is hundreds to thousands of times faster than MINISAT on many hard SAT instances and is never more than twenty times slower than MINISAT on any SAT instance.", "num_citations": "5\n", "authors": ["1699"]}
{"title": "Towards mixed optimization for reinforcement learning with program synthesis\n", "abstract": " Deep reinforcement learning has led to several recent breakthroughs, though the learned policies are often based on black-box neural networks. This makes them difficult to interpret and to impose desired specification constraints during learning. We present an iterative framework, MORL, for improving the learned policies using program synthesis. Concretely, we propose to use synthesis techniques to obtain a symbolic representation of the learned policy, which can then be debugged manually or automatically using program repair. After the repair step, we use behavior cloning to obtain the policy corresponding to the repaired program, which is then further improved using gradient descent. This process continues until the learned policy satisfies desired constraints. We instantiate MORL for the simple CartPole problem and show that the programmatic representation allows for high-level modifications that in turn lead to improved learning of the policies.", "num_citations": "4\n", "authors": ["1699"]}
{"title": "Latent programmer: Discrete latent codes for program synthesis\n", "abstract": " A key problem in program synthesis is searching over the large space of possible programs. Human programmers might decide the high-level structure of the desired program before thinking about the details; motivated by this intuition, we consider two-level search for program synthesis, in which the synthesizer first generates a plan, a sequence of symbols that describes the desired program at a high level, before generating the program. We propose to learn representations of programs that can act as plans to organize such a two-level search. Discrete latent codes are appealing for this purpose, and can be learned by applying recent work on discrete autoencoders. Based on these insights, we introduce the Latent Programmer (LP), a program synthesis method that first predicts a discrete latent code from input/output examples, and then generates the program in the target language. We evaluate the LP on two domains, demonstrating that it yields an improvement in accuracy, especially on longer programs for which search is most difficult.", "num_citations": "3\n", "authors": ["1699"]}
{"title": "Learning from input patterns in Programing-By-Example\n", "abstract": " Embodiments disclosed herein are related to systems and methods for using input logical patterns to generate one or more programs by an underlying Program-By-Example (PBE) system based on user input examples. A system includes a processor and a system memory. The system access a set of input data. The system receives one or more user input examples for the set of input data. The user input examples are indicative of an output that should be achieved to comply with a user determined result. The system analyzes the set of input data to identify one or more logical patterns that are common to the set of input data. The system generates one or more programs which will output the user determined result, based on a set of the one or more logical patterns that are consistent with the one or more user input examples.", "num_citations": "3\n", "authors": ["1699"]}
{"title": "Learning and analyzing vector encoding of symbolic representations\n", "abstract": " We present a formal language with expressions denoting general symbol structures and queries which access information in those structures. A sequence-to-sequence network processing this language learns to encode symbol structures and query them. The learned representation (approximately) shares a simple linearity property with theoretical techniques for performing this task.", "num_citations": "3\n", "authors": ["1699"]}
{"title": "Scaling Symbolic Methods using Gradients for Neural Model Explanation\n", "abstract": " Symbolic techniques based on Satisfiability Modulo Theory (SMT) solvers have been proposed for analyzing and verifying neural network properties, but their usage has been fairly limited owing to their poor scalability with larger networks. In this work, we propose a technique for combining gradient-based methods with symbolic techniques to scale such analyses and demonstrate its application for model explanation. In particular, we apply this technique to identify minimal regions in an input that are most relevant for a neural network's prediction. Our approach uses gradient information (based on Integrated Gradients) to focus on a subset of neurons in the first layer, which allows our technique to scale to large networks. The corresponding SMT constraints encode the minimal input mask discovery problem such that after masking the input, the activations of the selected neurons are still above a threshold. After solving for the minimal masks, our approach scores the mask regions to generate a relative ordering of the features within the mask. This produces a saliency map which explains \"where a model is looking\" when making a prediction. We evaluate our technique on three datasets - MNIST, ImageNet, and Beer Reviews, and demonstrate both quantitatively and qualitatively that the regions generated by our approach are sparser and achieve higher saliency scores compared to the gradient-based methods alone. Code and examples are at - https://github.com/google-research/google-research/tree/master/smug_saliency", "num_citations": "2\n", "authors": ["1699"]}
{"title": "Augmented Example-based Synthesis\n", "abstract": " Authors\u2019 addresses: Shengwei An, Purdue University, USA, an93@ purdue. edu; Rishabh Singh, Google Brain, USA, rising@ google. com; Sasa Misailovic, UIUC, USA, misailo@ illinois. edu; Roopsha Samanta, Purdue University, USA, roopsha@ purdue. edu.", "num_citations": "2\n", "authors": ["1699"]}
{"title": "Approaches and applications of inductive programming (Dagstuhl Seminar 17382)\n", "abstract": " This report documents the program and the outcomes of Dagstuhl Seminar 17382\" Approaches and Applications of Inductive Programming\". After a short introduction to the state of the art to inductive programming research, an overview of the introductory tutorials, the talks, program demonstrations, and the outcomes of discussion groups is given.", "num_citations": "2\n", "authors": ["1699"]}
{"title": "Data-driven feedback generation for introductory programming exercises\n", "abstract": " This paper introduces the \"Search, Align, and Repair\" data-driven program repair framework to automate feedback generation for introductory programming exercises. Distinct from existing techniques, our goal is to develop an efficient, fully automated, and problem-agnostic technique for large or MOOC-scale introductory programming courses. We leverage the large amount of available student submissions in such settings and develop new algorithms for identifying similar programs, aligning correct and incorrect programs, and repairing incorrect programs by finding minimal fixes. We have implemented our technique in the SARFGEN system and evaluated it on thousands of real student attempts from the Microsoft-DEV204.1X edX course and the Microsoft CodeHunt platform. Our results show that SARFGEN can, within two seconds on average, generate concise, useful feedback for 89.7% of the incorrect student submissions. It has been integrated with the Microsoft-DEV204.1X edX class and deployed for production use.", "num_citations": "2\n", "authors": ["1699"]}
{"title": "Learning Discrete Energy-based Models via Auxiliary-variable Local Exploration\n", "abstract": " Discrete structures play an important role in applications like program language modeling and software engineering. Current approaches to predicting complex structures typically consider autoregressive models for their tractability, with some sacrifice in flexibility. Energy-based models (EBMs) on the other hand offer a more flexible and thus more powerful approach to modeling such distributions, but require partition function estimation. In this paper we propose ALOE, a new algorithm for learning conditional and unconditional EBMs for discrete structured data, where parameter gradients are estimated using a learned sampler that mimics local search. We show that the energy function and sampler can be trained efficiently via a new variational form of power iteration, achieving a better trade-off between flexibility and tractability. Experimentally, we show that learning local search leads to significant improvements in challenging application domains. Most notably, we present an energy model guided fuzzer for software testing that achieves comparable performance to well engineered fuzzing engines like libfuzzer.", "num_citations": "1\n", "authors": ["1699"]}
{"title": "Towards modular algorithm induction\n", "abstract": " We present a modular neural network architecture Main that learns algorithms given a set of input-output examples. Main consists of a neural controller that interacts with a variable-length input tape and learns to compose modules together with their corresponding argument choices. Unlike previous approaches, Main uses a general domain-agnostic mechanism for selection of modules and their arguments. It uses a general input tape layout together with a parallel history tape to indicate most recently used locations. Finally, it uses a memoryless controller with a length-invariant self-attention based input tape encoding to allow for random access to tape locations. The Main architecture is trained end-to-end using reinforcement learning from a set of input-output examples. We evaluate Main on five algorithmic tasks and show that it can learn policies that generalizes perfectly to inputs of much longer lengths than the ones used for training.", "num_citations": "1\n", "authors": ["1699"]}
{"title": "Effect of Centrally Obscured Aperture on the Mrtd and Range Performance of Thermal Imaging systems\n", "abstract": " The effect of central obscuration of entrance aperture on the Minimum Resolvable Temperature Difference (Mrtd) and the recognition range performance of Thermal Imaging System has been investigated. Two representative systems have been considered, first a medium range system (Recognition Range ~ 1.8 Km) and the other a long range system (Recognition Range ~ 3.5 Km). It has been shown that central obscuration of the collecting optics aperture invariably deteriorates the performance of the system. It is concluded that all refractive optical system should always be preferred, unless the system design requirement specifically demands a centrally obscured aperture, such as Cassegranian optical system, frequently used in Thermal Systems.", "num_citations": "1\n", "authors": ["1699"]}