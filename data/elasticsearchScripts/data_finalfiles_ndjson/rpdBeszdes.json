{"title": "An efficient relevant slicing method for debugging\n", "abstract": " Dynamic program slicing methods are widely used for debugging, because many statements can be ignored in the process of localizing a bug. A dynamic program slice with respect to a variable contains only those statements that actually had an influence on this variable. How- ever, during debugging we also need to identify those statements that actually did not affect the variable but could have affected it had they been evaluated differently. A relevant slice includes these potentially af- fecting statements as well, therefore it is appropriate for debugging. In this paper a forward algorithm is introduced for the computation of relevant slices of programs. The space requirement of this method does not depend on the number of different dynamic slices nor on the size of the execution history, hence it can be applied for real size applications.", "num_citations": "209\n", "authors": ["986"]}
{"title": "Survey of code-size reduction methods\n", "abstract": " Program code compression is an emerging research activity that is having an impact in several production areas such as networking and embedded systems. This is because the reduced-sized code can have a positive impact on network traffic and embedded system costs such as memory requirements and power consumption. Although code-size reduction is a relatively new research area, numerous publications already exist on it. The methods published usually have different motivations and a variety of application contexts. They may use different principles and their publications often use diverse notations. To our knowledge, there are no publications that present a good overview of this broad range of methods and give a useful assessment. This article surveys twelve methods and several related works appearing in some 50 papers published up to now. We provide extensive assessment criteria for evaluating\u00a0\u2026", "num_citations": "124\n", "authors": ["986"]}
{"title": "Predictive complex event processing: a conceptual framework for combining complex event processing and predictive analytics\n", "abstract": " Complex Event Processing deals with the detection of complex events based on rules and patterns defined by domain experts. Many complex events require real-time detection in order to have enough time for appropriate reactions. However, there are several events (eg credit card fraud) that should be prevented proactively before they occur, not just responded after they happened. In this paper, we briefly describe Complex Event Processing (CEP) and Predictive Analytics (PA). Afterwards, we focus on a major future direction of CEP, namely the inclusion of PA technologies into CEP tools and applications. Involving PA opens a wide range of possibilities in several application fields. However, we have observed that only few solutions apply PA techniques. In this paper, we define a conceptual framework which combines CEP and PA and which can be the basis of generic design pattern in the future. The conceptual\u00a0\u2026", "num_citations": "119\n", "authors": ["986"]}
{"title": "Design pattern mining enhanced by machine learning\n", "abstract": " Design patterns present good solutions to frequently occurring problems in object-oriented software design. Thus their correct application in a system's design may significantly improve its internal quality attributes such as reusability and maintainability. In software maintenance the existence of up-to-date documentation is crucial, so the discovery of as yet unknown design pattern instances can help improve the documentation. Hence a reliable design pattern recognition system is very desirable. However, simpler methods (based on pattern matching) may give imprecise results due to the vague nature of the patterns' structural description. In previous work we presented a pattern matching-based system using the Columbus framework with which we were able to find pattern instances from the source code by considering the patterns' structural descriptions only, and therefore we could not identify false hits and\u00a0\u2026", "num_citations": "117\n", "authors": ["986"]}
{"title": "Dynamic slicing method for maintenance of large C programs\n", "abstract": " Different program slicing methods are used for maintenance, reverse engineering, testing and debugging. Slicing algorithms can be classified as static slicing and dynamic slicing methods. In several applications the computation of dynamic slices is preferable, since it can produce more precise results. In this paper, we introduce a new forward global method for computing backward dynamic slices of C programs. In parallel to the program execution, the algorithm determines the dynamic slices for any program instruction. We also propose a solution for some problems specific to the C language (such as pointers and function calls). The main advantage of our algorithm is that it can be applied to real-size C programs, because its memory requirements are proportional to the number of different memory locations used by the program (which is in most cases far smaller than the size of the execution history which is, in\u00a0\u2026", "num_citations": "109\n", "authors": ["986"]}
{"title": "Union slices for program maintenance\n", "abstract": " Owing to their relative simplicity and wide range of applications, static slices are specifically proposed for software maintenance and program understanding. Unfortunately, in many cases static slices are overly conservative and therefore too large to supply useful information to the software maintainer. Dynamic slicing methods can produce more precise results, but only for one test case. In this paper we introduce the concept of union slices (the union of dynamic slices for many test cases) and suggest using a combination of static and union slices. This way the size of program parts that need to be investigated can be reduced by concentrating on the most important parts first. We performed a series of experiments with our experimental implementation on three medium size C programs. Our initial results suggest that union slices are in most cases far smaller than static slices, and that the growth rate of union slices\u00a0\u2026", "num_citations": "58\n", "authors": ["986"]}
{"title": "Static execute after/before as a replacement of traditional software dependencies\n", "abstract": " The paper explores Static Execute After (SEA) dependencies in the program and their dual Static Execute Before (SEB) dependencies. It empirically compares the SEA/SEB dependencies with the traditional dependencies that are computed by System Dependence Graph (SDG) and program slicers. In our case study we use about 30 subject programs that were previously used by other authors in empirical studies of program analysis. We report two main results. The computation of SEA/SEB is much less expensive and much more scalable than the computation of the SDG. At the same time, the precision declines only very slightly, by some 4% on average. In other words, the precision is comparable to that of the leading traditional algorithms, while intuitively a much larger difference would be expected. The paper then discusses whether based on these results the computation of the SDG should be replaced in\u00a0\u2026", "num_citations": "56\n", "authors": ["986"]}
{"title": "The dynamic function coupling metric and its use in software evolution\n", "abstract": " Many of the existing techniques for impact set computation in change propagation and regression testing are approximate for the sake of efficiency. A way to improve precision is to apply dynamic analyses instead of static ones. The state-of-the-art dynamic impact analysis method is simple and efficient, but overly conservative and hence imprecise. In this paper we introduce the measure of dynamic function coupling (DFC) between two functions or methods, which we use to define a more precise way of computing impact sets on function level with a scalable rate of recall. The intuition behind our approach is that the 'closer' the execution of a function is to the execution of another function in some of the runs of the program, the more likely they are really dependent on each other. So, impact sets may be computed based on this kind of coupling. We provide experimental data to support the validity of the concept, which\u00a0\u2026", "num_citations": "50\n", "authors": ["986"]}
{"title": "Code coverage-based regression test selection and prioritization in WebKit\n", "abstract": " Automated regression testing is often crucial in order to maintain the quality of a continuously evolving software system. However, in many cases regression test suites tend to grow too large to be suitable for full re-execution at each change of the software. In this case selective retesting can be applied to reduce the testing cost while maintaining similar defect detection capability. One of the basic test selection methods is the one based on code coverage information, where only those tests are included that cover some parts of the changes. We experimentally applied this method to the open source web browser engine project WebKit to find out the technical difficulties and the expected benefits if this method is to be introduced into the actual build process. Although the principle is simple, we had to solve a number of technical issues, so we report how this method was adapted to be used in the official build\u00a0\u2026", "num_citations": "47\n", "authors": ["986"]}
{"title": "Columbus-tool for reverse engineering large object oriented software systems\n", "abstract": " One of the most critical issues in large-scale software development and maintenance is the rapidly growing size and complexity of the software systems. As a result of this rapid growth there is a need to understand the relationships between the different parts of a large system. In this paper we present a reverse engineering framework called Columbus that is able to analyze large C/C++ projects. Columbus supports project handling, data extraction,-representation,-storage and-export. Ecient ltering methods can be used to produce comprehensible diagrams from the extracted information. The flexible architecture of the Columbus system (based on plug-ins) makes it a really versatile and an easily extendible tool for reverse engineering.", "num_citations": "36\n", "authors": ["986"]}
{"title": "Comparison of different impact analysis methods and programmer's opinion: an empirical study\n", "abstract": " In change impact analysis, obtaining guidance from automatic tools would be highly desirable since this activity is generally seen as a very difficult program comprehension problem. However, since the notion of an'impact set'(or dependency set) of a specific change is usually very inexact and context dependent, the approaches and algorithms for computing these sets are also very diverse producing quite different results. The question'which algorithm finds program dependencies in the most efficient way?'has been preoccupying researchers for a long time, but there are still very few results published on the comparison of the different algorithms to what programmers think are real dependencies. In this work, we report on our experiment conducted with this goal in mind using a compact, easily comprehensible Java experimental software system, simulated program changes, and a group of programmers who were\u00a0\u2026", "num_citations": "34\n", "authors": ["986"]}
{"title": "Extracting facts with columbus from C++ code\n", "abstract": " Fact extraction from software systems is the fundamental building block in the process of understanding the relationships among the system\u2019s elements. It is evident that in real life situations manual fact extraction must be supported by software tools which are able to analyze the subject system and provide useful information about it in various forms. These forms are most useful if they adhere to prescribed schemas and this way promote tool interoperability. In this work we outline our solution to tool supported fact extraction, which is built upon the reverse engineering framework Columbus and is supported by schemas for the C++ language. We describe the extraction process in detail and show how the extracted facts can be used in practice by processing the schema instances. We also introduce new features of the Columbus system not published previously, which among others include compiler wrapping and source code auditing.", "num_citations": "32\n", "authors": ["986"]}
{"title": "Graph-less dynamic dependence-based dynamic slicing algorithms\n", "abstract": " Using Dynamic Dependence Graphs is a well understood method for computing dynamic program slices. However, in its basic form, the DDG is inappropriate for practical implementation, so several alternative approaches have been proposed by researchers. In this paper, we elaborate on different methods in which the execution trace is processed and, using local definition-use information, the dependence chains are followed \"on the fly\" to construct the slices without actually building any graphs. Naturally, various additional data structures still need to be maintained, but these vary on the slicing scenario. Firstly, one may want to perform the slicing in a demand-driven fashion, or to compute many slices globally. Next, one may be interested either in backward or forward slices. And finally, the slices can be produced by traversing the trace either in a forward or in a backward direction. This totals eight possibilities, of\u00a0\u2026", "num_citations": "30\n", "authors": ["986"]}
{"title": "CodeMetropolis-code visualisation in MineCraft\n", "abstract": " Data visualisation with high expressive power plays an important role in code comprehension. Recent visualization tools try to fulfil the expectations of the users and use various analogies. For example, in an architectural metaphor, each class is represented by a building. Buildings are grouped into districts according to the structure of the namespaces. We think that these unique ways of code representation have great potential, but in our opinion they use very simple graphical techniques (shapes, figures, low resolution) to visualize the structure of the source code.On the other hand, computer games use high quality graphic and good expressive power. A good example is Minecraft, a popular role playing game with great extensibility and interactivity from another (third party) software. It supports both high definition, photo-realistic textures and long range 3D scene displaying. Our main contribution is to connect data\u00a0\u2026", "num_citations": "23\n", "authors": ["986"]}
{"title": "Continuous software quality supervision using SourceInventory and Columbus\n", "abstract": " Several tools and methods for source code quality assurance based on static analysis finally reached a state when they are applicable in practice and recognized by the industry. However, most of these tools are used in an isolated manner and very rarely as organic parts of the quality assurance process. Furthermore, little or no help is provided in interpreting the outputs of these tools. This paper presents SourceInventory, a system for source code-based software quality assessment and monitoring, which is able to collect, store and present measurement data including metrics, coding problems and other kinds of data like bug numbers and test coverage information. It helps software developers, architects and managers to take control over their software's quality by performing continuous code scans, fault detection, coding style verification, architecture violation detection, and automatic report generation\u00a0\u2026", "num_citations": "22\n", "authors": ["986"]}
{"title": "CodeMetrpolis\u2014A minecraft based collaboration tool for developers\n", "abstract": " Data visualisation with high expressive power plays an important role in code comprehension. Recent visualisation tools try to fulfill the expectations of the users and use various analogies. For example, in an architectural metaphor, each class is represented by a building. Buildings are grouped into districts according to the structure of the namespaces. We think that these unique ways of code representation have great potential, but in our opinion they use very simple graphical techniques (shapes, figures, low resolution) to visualise the structure of the source code. On the other hand, computer games use high quality graphic and have high expressive power. A good example is Minecraft, a popular role playing game that supports both high definition, photorealistic textures and long range 3D scene displaying. Additionally, it provides great extensibility and interactivity for third party software. In this paper, we\u00a0\u2026", "num_citations": "18\n", "authors": ["986"]}
{"title": "Complex event processing synergies with predictive analytics\n", "abstract": " For Complex Event Processing (CEP), the synergy with Predictive Analytics (PA) is a promising research direction. In this paper we focus on the inclusion of PA technologies into CEP applications. Involving PA opens a wide range of possibilities in several application fields. However, we have observed that only a few CEP solutions apply PA techniques. We extended a CEP solution with predictive capabilities, defined the key aspects of the combination of these techniques, and summarized how CEP and PA could gain from the joint solution. Our approach is demonstrated in a proof-of-concept experiment and simulation results are provided.", "num_citations": "18\n", "authors": ["986"]}
{"title": "CodeMetropolis: Eclipse over the City of Source Code\n", "abstract": " The graphical representations of software (code visualization in particular) may provide both professional programmers and students learning only the basics with support in program comprehension. Among the numerous proposed approaches, our research applies the city metaphor for the visualisation of such code elements as classes, functions, or attributes by the tool CodeMetropolis. It uses the game engine of Minecraft for the graphics, and is able to visualize various properties of the code based on structural metrics. In this work, we present our approach to integrate our visualization tool into the Eclipse IDE environment. Previously, only standalone usage was possible, but with this new version the users can invoke the visualization directly from the IDE, and all the analysis is performed in the background. The new version of the tool now includes an Eclipse plug-in and a Minecraft modification in addition to the\u00a0\u2026", "num_citations": "17\n", "authors": ["986"]}
{"title": "Opening up the C/C++ preprocessor black box\n", "abstract": " File inclusion, conditional compilation and macro processing has made the preprocessor a powerful tool for programmers. Preprocessor directives are extensively used in C/C++ programs and have various purposes. However, program code with lots of directives often causes problems in program understanding and maintenance. The main source of the problem is the difference between the code that the programmer sees and the preprocessed code that the compiler is given. We designed a Preprocessor Schema and implemented a preprocessor which produces both preprocessed (. i) files and schema instances. The Schema is general purpose and its instances model both the whole preprocessed compilation unit and the transformations made by the preprocessor. Therefore it facilitates program comprehension and tool interoperability.", "num_citations": "17\n", "authors": ["986"]}
{"title": "Using the City Metaphor for Visualizing Test-Related Metrics\n", "abstract": " Software visualization techniques and tools play an important role in system comprehension efforts of software developers in the era of increasing code size and complexity. They enable the developer to have a global perception on various software attributes with the aid of different visualization metaphors and tools. One such tool is CodeMetropolis which is built on top of the game engine Minecraft and which uses the city metaphor to show the structure of the source code as a virtual city. In it, different physical properties of the city and the buildings are related to various code metrics. Up to now, it was limited to represent only code related artifacts. In this work, we extend the metaphor to include properties of the tests related to the program code using a novel concept. The test suite and the test cases are also associated with a set of metrics that characterize their quality (such as coverage and specialization), but also\u00a0\u2026", "num_citations": "16\n", "authors": ["986"]}
{"title": "Impact analysis in the presence of dependence clusters using Static Execute After in WebKit\n", "abstract": " Impact analysis based on code dependence can provide opportunities to identify parts of the software affected by a change. Because changes usually have far reaching effects in programs, effective and efficient impact analysis is vital. Static Execute After (SEA) is a relation on procedures that is efficiently computable and accurate enough to be a candidate for the use in impact analysis in practice. To assess the applicability of SEA in terms of capturing real defects, we present results on integrating it into the build system of WebKit, a large, open source software system, and on related experiments. We show that a large number of real defects can be captured by impact sets computed by SEA, albeit many of them are large. We demonstrate that this is not an issue in applying it to regression test prioritization, but generally it can be an obstacle in the path to efficient use of impact analysis. We believe that the main\u00a0\u2026", "num_citations": "16\n", "authors": ["986"]}
{"title": "Macro impact analysis using macro slicing\n", "abstract": " The expressiveness of the C/C++ preprocessing facility enables the development of highly configurable source code. However, the usage of language constructs like macros also bears the potential of resulting in highly incomprehensible and unmaintainable code, which is due to the flexibility and the \u201ccryptic\u201d nature of the preprocessor language. This could be overcome if suitable analysis tools were available for preprocessor-related issues, however, this is not the case (for instance, none of the modern Integrated Development Environments provides features to efficiently analyze and browse macro usage). A conspicuous problem in software maintenance is the correct (safe and efficient) management of change. In particular, due to the aforementioned reasons, determining efficiently the impact of a change in a specific macro definition is not yet possible. In this paper, we describe a method for the impact analysis of macro definitions, which significantly differs from the previous approaches. We reveal and analyze the dependencies among macro-related program points using the so-called macro slices.", "num_citations": "16\n", "authors": ["986"]}
{"title": "Combining preprocessor slicing with C/C++ language slicing\n", "abstract": " Of the very few practical implementations of program slicing algorithms, the majority deal with C/C++ programs. Yet, preprocessor-related issues have been marginally addressed by these slicers, despite the fact that ignoring (or only partially handling) these constructs may lead to serious inaccuracies in the slicing results and hence in the program analysis task being performed. Recently, an accurate slicing method for preprocessor-related constructs has been proposed, which\u2013when combined with existing C/C++ language slicers\u2013can provide more complete slices and hence a more successful analysis of programs written in one of these languages. In this paper, we present our approach which combines the two slicing methods and, via practical experiments, describe its benefits in terms of the completeness of the resulting slices.", "num_citations": "15\n", "authors": ["986"]}
{"title": "Code Factoring in GCC\n", "abstract": " Though compilers usually focus on optimizing for performance, the size of the generated code has only received attention recently. On general desktop systems the code size is not the biggest concern, but on devices with a limited storage capacity compilers should strive for as small a code as possible. GCC already contains some very useful algorithms for optimizing code size, but code factoring\u2013a very powerful approach to reducing code size\u2013has not been implemented yet in GCC. In this paper we will provide an overview of the possibilities of using code factoring in GCC. Two code factoring algorithms have been implemented so far. These algorithms, using CSiBE as a benchmark, produced a maximum of 27% in code size reduction and an average of 3%.", "num_citations": "13\n", "authors": ["986"]}
{"title": "CSiBE Benchmark: One Year Perspective and Plans\n", "abstract": " In this paper we summarize our experiences in designing and running CSiBE, the new code size benchmark for GCC. Since its introduction in 2003, it has been widely used by GCC developers in their daily work to help them keep the size of the generated code as small as possible. We have been making continuous observations on the latest results and informing GCC developers of any problem when necessary. We overview some concrete \u201csuccess stories\u201d of where GCC benefited from the benchmark. This paper overviews the measurement methodology, providing some information about the test bed, the measuring method, and the hardware/software infrastructure. The new version of CSiBE, launched in May 2004, has been extended with new features such as code performance measurements and a test bed\u2014four times larger\u2014with even more versatile programs.", "num_citations": "13\n", "authors": ["986"]}
{"title": "Software Quality Model and Framework with Applications in Industrial Context\n", "abstract": " Software Quality Assurance involves all stages of the software life cycle including development, operation and evolution as well. Low level measurements (product and process metrics) are used to predict and control higher level quality attributes. There exists a large body of proposed metrics, but their interpretation and the way of connecting them to actual quality management goals is still a challenge. In this work, we present our approach for modelling, collecting, storing and evaluating such software measurements, which can deal with all types of metrics collected at any stage of the life cycle. The approach is based on the Goal Question Metric paradigm, and its novelty lies in a unified representation of the metrics and the questions that evaluate them. It allows the definition of various complex questions involving different types of metrics, while the supporting framework enables the automatic collection of the\u00a0\u2026", "num_citations": "12\n", "authors": ["986"]}
{"title": "Verifying the concept of union slices on Java programs\n", "abstract": " Static program slicing is often proposed for software maintenance-related tasks. Due to different causes static slices are in many cases overly conservative and hence too large to reduce the program-part of interest meaningfully. In this paper we further investigate the concept of union slices, which are defined as the unions of dynamic slices computed for the same (static) slicing criteria, but for different executions of the program. We verify on real-world Java programs their usefulness as a replacement to static slices. For this we investigate the sizes of a number of backward and forward dynamic and union slices, also by comparing them to the corresponding static slices. Our results show that the union slices are precise enough (backward slices are 5-20% of the program and forward slices are 5-10%, the corresponding static slices being 25-45%), and that with the saturation of the overall coverage given many\u00a0\u2026", "num_citations": "11\n", "authors": ["986"]}
{"title": "Information retrieval based feature analysis for product line adoption in 4GL systems\n", "abstract": " New customers often require custom features of a successfully marketed product. As the number of variants grow, new challenges arise in the maintenance and evolution activities. Software product line (SPL) architecture is a timely answer to these challenges. The SPL adoption however is a large one time investment that affects both technical and organizational issues. From the program code point of view, the extractive approach is appropriate when there are already several product variants. Analyzing the feature structure, the differences and commonalities of the variants lead to the new common architecture. In this work in progress paper we report initial experiments of feature extraction from a set of product variants written in the Magic fourth generation language (4GL). Since existing approaches are mostly designed for mainstream languages, we adapted and reused reverse engineering approaches to the\u00a0\u2026", "num_citations": "9\n", "authors": ["986"]}
{"title": "Semi-Automatic Test Case Generation from Business Process Models\n", "abstract": " In this work, we describe our method for designing test cases based on high level functional specifications\u2013business process models. Category Partition Method (CPM) is used to automatically create test frames based on possible paths, which are determined by business rules. The test frames can then be used in the process of test case design, together with filtering and prioritization also given as CPM rules. We present the details of the adaptation of CPM, together with first experiences from applying the method in an industrial context.", "num_citations": "9\n", "authors": ["986"]}
{"title": "Optimizing for space: Measurements and possibilities for improvement\n", "abstract": " GCC\u2019s optimization for space seems to have been often neglected, in favor of performance tuning. With this work we aim at determining the weakpoints of GCC concerning its optimization capability for space. We compare (1) GCC with two non-free ARM cross-compiler toolchains,(2) how GCC evolved from release 3.2. 2 to version 3.3, and (3) two runtime libraries for the Linux kernel. All tests were performed using the C front end and for the ARM target both as standalone and as Linux executables. The test suite is comprised of applications from well-known benchmark suites such as SPEC and Mediabench. An optimal combination of compiler (and linker) options with respect to minimal code size is elaborated as well. We conclude that GCC 3.3 steadily improves with respect to version 3.2. 2 and that it is only about 11% behind a high-performance non-free compiler. At the same time, we were able to document a number of issues that deserve further investigation in order to improve code generation for space.", "num_citations": "9\n", "authors": ["986"]}
{"title": "Academic and Industrial Software Testing Conferences: Survey and Synergies\n", "abstract": " Just as with any other profession, an efficient way to exchange ideas and networking in software testing are conferences, workshops and similar events. This is true for both professional testers and researchers working in the testing area. However, these two groups usually look for different kinds of events: a tester likes to attend 'industrial' (sometimes called practitioner's or user) conferences, whereas a researcher is more likely interested in 'academic' (in other words scientific or research) conferences. Although there are notable exceptions, this separation is substantial, which hinders a successful academy-industry collaboration, and communication about the demand and supply of research in software testing. This paper reviews 101 conferences: two thirds are academic ones, the rest being industrial. Besides providing this reasonably comprehensive list, we analyze any visible synergies such as events that have a\u00a0\u2026", "num_citations": "8\n", "authors": ["986"]}
{"title": "Towards a Safe Method for Computing Dependencies in Database-Intensive Systems\n", "abstract": " Determining dependencies between different components of an application is useful in lots of applications (e.g., architecture reconstruction, reverse engineering, regression test case selection, change impact analysis). However, implementing automated methods to recover dependencies has many challenges, particularly in systems using databases, where dependencies may arise via database access. Furthermore, it is especially hard to find safe techniques (which do not omit any important dependency) that are applicable to large and complex systems at the same time. We propose two techniques that can cope with these problems in most situations. These methods compute dependencies between procedures or database tables, and they are based on the simultaneous static analysis of the source code, the database schema and the SQL instructions. In this paper, we quantitatively and qualitatively evaluate the\u00a0\u2026", "num_citations": "8\n", "authors": ["986"]}
{"title": "Supporting Product Line Adoption by Combining Syntactic and Textual Feature Extraction\n", "abstract": " Software product line (SPL) architecture facilitates systematic reuse to serve specific feature requests of new customers. Our work deals with the adoption of SPL architecture in an existing legacy system. In this case, the extractive approach of SPL adoption turned out to be the most viable method, where the system is redesigned keeping variants within the same code base. The analysis of the feature structure is a crucial point in this process as it involves both domain experts working at a higher level of abstraction and developers working directly on the program code. In this work, we propose an automatic method to extract feature-to-program connections starting from a very high level set of features provided by domain experts and existing program code. The extraction is performed by combining and further processing call graph information on the code with textual similarity between code and high level\u00a0\u2026", "num_citations": "7\n", "authors": ["986"]}
{"title": "Are my unit tests in the right package?\n", "abstract": " The software development industry has adopted written and de facto standards for creating effective and maintainable unit tests. Unfortunately, like any other source code artifact, they are often written without conforming to these guidelines, or they may evolve into such a state. In this work, we address a specific type of issues related to unit tests. We seek to automatically uncover violations of two fundamental rules: 1) unit tests should exercise only the unit they were designed for, and 2) they should follow a clear packaging convention. Our approach is to use code coverage to investigate the dynamic behaviour of the tests with respect to the code elements of the program, and use this information to identify highly correlated groups of tests and code elements (using community detection algorithm). This grouping is then compared to the trivial grouping determined by package structure, and any discrepancies found are\u00a0\u2026", "num_citations": "6\n", "authors": ["986"]}
{"title": "Feature analysis using information retrieval, community detection and structural analysis methods in product line adoption\n", "abstract": " In industrial practice the clone-and-own strategy is often applied when in the pressure of high demand of customized features. The adoption of software product line (SPL) architecture is a large one time investment that affects both technical and organizational issues. The analysis of the feature structure is a crucial point in the SPL adoption process involving domain experts working at a higher level of abstraction and developers working directly on the program code. We propose automatic methods to extract feature-to-program links starting from very high level set of features provided by domain experts. For this purpose we combine call graph information with textual similarity between code and high level features. In addition, in depth understanding of the feature structure is supported by finding communities between programs and relating them to features. As features are originated from domain experts, community\u00a0\u2026", "num_citations": "5\n", "authors": ["986"]}
{"title": "Identifying Wasted Effort in the Field via Developer Interaction Data\n", "abstract": " During software projects, several parts of the source code are usually re-written due to imperfect solutions before the code is released. This wasted effort is of central interest to the project management to assure on-time delivery. Although the amount of thrown-away code can be measured from version control systems, stakeholders are more interested in productivity dynamics that reflect the constant change in a software project. In this paper we present a field study of measuring the productivity of a medium-sized J2EE project. We propose a productivity analysis method where productivity is expressed through dynamic profiles - the so-called Micro-Productivity Profiles (MPPs). They can be used to characterize various constituents of software projects such as components, phases and teams. We collected detailed traces of developers' actions using an Eclipse IDE plug-in for seven months of software development\u00a0\u2026", "num_citations": "5\n", "authors": ["986"]}
{"title": "Empirical investigation of SEA-based dependence cluster properties\n", "abstract": " Dependence clusters are (maximal) groups of source code entities that each depend on the other according to some dependence relation. Such clusters are generally seen as detrimental to many software engineering activities, but their formation and overall structure are not well understood yet. In a set of subject programs from moderate to large sizes, we observed frequent occurrence of dependence clusters using Static Execute After (SEA) dependences (SEA is a conservative yet efficiently computable dependence relation on program procedures). We identified potential linchpins; these are procedures that can primarily be made responsible for keeping the cluster together. Furthermore, we found that as the size of the system increases, it is more likely that multiple procedures are jointly responsible as sets of linchpins. We also give a heuristic method based on structural metrics for locating possible linchpins as\u00a0\u2026", "num_citations": "5\n", "authors": ["986"]}
{"title": "Code coverage measurement framework for android devices\n", "abstract": " Software testing is a very important activity in the software development life cycle. Numerous general black-and white-box techniques exist to achieve different goals and there are a lot of practices for different kinds of software. The testing of embedded systems, however, raises some very special constraints and requirements in software testing. Special solutions exist in this field, but there is no general testing methodology for embedded systems. One of the goals of the CIRENE project was to fill this gap and define a general testing methodology for embedded systems that could be specialized to different environments. The project included a pilot implementation of this methodology in a specific environment: an Android-based Digital TV receiver (Set-Top-Box). In this pilot, we implemented method level code coverage measurement of Android applications. This was done by instrumenting the applications and creating a framework for the Android device that collected basic information from the instrumented applications and communicated it through the network towards a server where the data was finally processed. The resulting code coverage information was used for many purposes according to the methodology: test case selection and prioritization, traceability computation, dead code detection, etc. The resulting methodology and toolset were reused in another project where we investigated whether the coverage information can be used to determine locations to be instrumented in order to collect relevant information about software usability. In this paper, we introduce the pilot implementation and, as a proof-of-concept, present how the\u00a0\u2026", "num_citations": "5\n", "authors": ["986"]}
{"title": "Adding Process Metrics to Enhance Modification Complexity Prediction\n", "abstract": " Software estimation is used in various contexts including cost, maintainability or defect prediction. To make the estimate, different models are usually applied based on attributes of the development process and the product itself. However, often only one type of attributes is used, like historical process data or product metrics, and rarely their combination is employed. In this report, we present a project in which we started to develop a framework for such complex measurement of software projects, which can be used to build combined models for different estimations related to software maintenance and comprehension. First, we performed an experiment to predict modification complexity (cost of a unity change) based on a combination of process and product metrics. We observed promising results that confirm the hypothesis that a combined model performs significantly better than any of the individual measurements.", "num_citations": "5\n", "authors": ["986"]}
{"title": "Global dynamic slicing for the C language\n", "abstract": " In dynamic program slicing, program subsets are computed that represent the set of dependences that occur for specific program executions and can be associated with a program point of interest called the slicing criterion. Traditionally, dynamic dependence graphs are used as a preprocessing step before the actual slices are computed, but this approach is not scalable. We follow the approach of processing the execution trace and, using local definition-use information, follow the dependence chains \u201con the fly\u201d without actually building the dynamic dependence graph, but we retain specialized data structures. Here, we present in detail the practical modifications of our global dynamic slicing algorithm, which are needed to apply it to programs written in the C language.", "num_citations": "4\n", "authors": ["986"]}
{"title": "Relating clusterization measures and software quality\n", "abstract": " Empirical studies have shown that dependence clusters are both prevalent in source code and detrimental to many activities related to software, including maintenance, testing and comprehension. Based on such observations, it would be worthwhile to try to give a more precise characterization of the connection between dependence clusters and software quality. Such attempts are hindered by a number of difficulties: there are problems in assessing the quality of software, measuring the degree of clusterization of software and finding the means to exhibit the connection (or lack of it) between the two. In this paper we present our approach to establish a connection between software quality and clusterization. Software quality models comprise of low- and high-level quality attributes, in addition we defined new clusterization metrics that give a concise characterization of the clusters contained in programs. Apart from\u00a0\u2026", "num_citations": "4\n", "authors": ["986"]}
{"title": "Capturing Expert Knowledge to Guide Data Flow and Structure Analysis of Large Corporate Databases\n", "abstract": " Maintaining and improving existing large-scale systems that are based on relational databases is proven to be a challenging task. Among many other aspects, it is crucial to develop actionable methods for estimating costs and durations in the process of assessing new feature requirements. This is a very frequent activity during the evolution of large database systems and data warehouses. This goal requires the analysis of program code, data structures and business level objectives at the same time, which is a daunting task if made manually by experts. Our industrial partner started to develop a static database analysis software package that would automate and ease this process in order to make more accurate estimations. The goal of this work was to create a quality assessment model that can effectively help developers to assess the data flow (lineage) quality and the database structure quality of data warehouse (DWH) and online transaction processing (OLTP) database systems. Based on related literature, we created different models for these two interconnected topics, which were then evaluated by independent developers. The evaluation showed that the models are suitable for implementation, which are now included in a commercial product developed by our industrial partner, Clarity.", "num_citations": "3\n", "authors": ["986"]}
{"title": "CIASYS--Change Impact Analysis at System Level\n", "abstract": " The research field of change impact analysis plays an important role in software engineering theory and practice nowadays. Not only because it has many scientific challenges, but it has many industrial applications too (e.g., cost estimation, test optimization), and the current techniques are still not ready to fulfill the requirements of industry. Typically, the current solutions lack a whole-system view and give either precise results with high computation costs or less precise results with fast algorithms. For these reasons, they are not applicable to large industrial systems where both scalability and precision are very important. In this paper, we present a project whose main goal is to develop an innovative change impact analysis software-suit based on recent scientific results and modern technologies. The suite will use hybrid analysis techniques to benefit from all the advantages of static and dynamic analyses. In addition\u00a0\u2026", "num_citations": "3\n", "authors": ["986"]}
{"title": "Using Backward Dynamic Program Slicing to Isolate Influencing Statements in GDB\n", "abstract": " Program slicing is a program analysis technique initially introduced to assist debugging, based on the observation that programmers mentally form program slices when they debug and understand programs. Namely, only those statements need to be investigated that actually influenced the erroneous value, and eventually, these statements constitute the backward dynamic program slice. An efficient algorithm to compute such slices has been implemented in the GCC/GDB environment, which adds a new slice command to retrieve the slice for a given program entity. In this paper, a background on program slicing is given, followed by the details of implementation. The dependences are computed after \u2018gimplification\u2019in GCC, while STABS format is used to transfer them to GDB. The initial experimental results are presented as well.", "num_citations": "3\n", "authors": ["986"]}
{"title": "Poster: Aiding Java Developers with Interactive Fault Localization in Eclipse IDE\n", "abstract": " Spectrum-Based ones are a popular class of Fault Localization (FL) methods among researchers due to their relative simplicity. However, recent studies highlighted some barriers to the wider adoption of the technique in practical settings. One possibility to increase the practical usefulness of related tools is to involve interactivity between the user and the core FL algorithm. In this setting, the developer interacts with the fault localization algorithm by giving feedback on the elements proposed by the algorithm. This way, the proposed elements can be influenced in the hope to reach the faulty element earlier (we call the proposed approach Interactive Fault Localization, or iFL). With this work, we present our recent achievements in this topic. In particular, we overview the basic approach, our preliminary experimentation with user simulation, and the supporting tool for the actual usage of the method, iFL for Eclipse. Our\u00a0\u2026", "num_citations": "2\n", "authors": ["986"]}
{"title": "A New Interactive Fault Localization Method with Context Aware User Feedback\n", "abstract": " State-of-the-art fault localization tools provide a ranked list of suspicious code elements to aid the user in this debugging activity. Statistical (or Spectrum-Based) Fault Localization (SFL/SBFL) uses code coverage information of test cases and their execution outcomes to calculate the ranks. We propose an approach (called iFL) in which the developer interacts with the fault localization algorithm by giving feedback on the elements of the prioritized list. Contextual knowledge of the user about the current item (e. g., a statement) is exploited in the ranked list, and with this feedback larger code entities (e. g., a whole function) can be repositioned in the list. In our initial set of experiments, we evaluated the approach on the SIR benchmark using simulated users. Results showed significant improvements in fault localization accuracy: the ranking position of the buggy element was reduced by 72% on average, and iFL was\u00a0\u2026", "num_citations": "2\n", "authors": ["986"]}
{"title": "iFL for Eclipse\u2014A Tool to Support Interactive Fault Localization in Eclipse IDE\n", "abstract": " We present a tool to aid Spectrum-Based Fault Localization (SBFL)[1]. SBFL provides a ranked list of suspicious code elements to the user based on statistical analysis of test execution outcomes and code coverage. Recent studies highlighted some barriers to the wide adoption of SBFL, including a high number of suggested elements to investigate [2], and other issues [3]. A possibility to increase the practical usefulness of SBFL tools is to involve interactivity. In our approach, called iFL, we involve the user\u2019s previous knowledge about the system: the developer interacts with the fault localization algorithm by giving feedback on the elements of the prioritized list. Contextual knowledge of the user about the ranked items, classes of methods in our case, is used to reposition larger code entities in their suspiciousness, thus aiding the FL process. The benefits of developer\u2019s additional knowledge have already been explored. For example, Li et al.[4] reuse the knowledge about passing parameter values, while Gong et al.[5] ask only for a simple yes/no feedback for a given statement. To our knowledge, contextual information about higher level entities has not yet been leveraged for interactive SBFL.", "num_citations": "2\n", "authors": ["986"]}
{"title": "Interdisciplinary survey of fault localization techniques to aid software engineering\n", "abstract": " Fault localization (narrowing down the cause of a failure to a small number of suspicious components of the system) is an important concern in many different engineering fields and there have been a large number of algorithmic solutions proposed to aid this activity. In this work, we performed a systematic analysis of related literature, not limiting the search to any specific engineering field, with the aim to find solutions in nonsoftware areas that could be successfully adapted to software fault localization. We found out that few areas have significant literature, in this topic, that are good candidates for adaptation (computer networks, for instance), and that although some classes of methods are less suitable, there are useful ideas in almost all fields that could potentially be reused for software fault localization.", "num_citations": "2\n", "authors": ["986"]}
{"title": "Adjusting effort estimation using micro-productivity profiles.\n", "abstract": " We investigate a phenomenon we call micro-productivity decrease, which is expected to be found in most development or maintenance projects and has a specific profile that depends on the project, the development model, and the team. Microproductivity decrease refers to the observation that the cumulative effort to implement a series of changes is larger than the effort that would be needed if we made the same modification in only one step. The reason for the difference is that the same sections of code are usually modified more than once in the series of (sometimes imperfect) atomic changes. Hence, we suggest that effort estimation methods based on atomic change estimations should incorporate these profiles when being applied to larger modification tasks. We verify the concept on industrial development projects with our metrics-based machine learning models extended with statistical data. We show that the calculated Micro-Productivity Profile for these projects could be used for effort estimation of larger tasks with more accuracy than a naive atomic change-oriented estimation.", "num_citations": "2\n", "authors": ["986"]}
{"title": "Adjusting Effort Esti mation Using Micro-Productivity Profiles\n", "abstract": " We investigate a phenomenon we call micro-productivity decrease, which is expected to be found in most development or maintenance projects and has a specific profile that depends on the project, the development model, and the team. Microproductivity decrease refers to the observation that the cumulative effort to implement a series of changes is larger than the effort that would be needed if we made the same modification in only one step. The reason for the difference is that the same sections of code are usually modified more than once in the series of (sometimes imperfect) atomic changes. Hence, we suggest that effort estimation methods based on atomic change estimations should incorporate these profiles when being applied to larger modification tasks. We verify the concept on industrial development projects with our metrics-based machine learning models extended with statistical data. We show that the calculated Micro-Productivity Profile for these projects could be used for effort estimation of larger tasks with more accuracy than a naive atomic change-oriented estimation.", "num_citations": "2\n", "authors": ["986"]}
{"title": "Effect of test completeness and redundancy measurement on post release failures\u2014An industrial experience report\n", "abstract": " In risk-based testing, compromises are often made to release a system in spite of knowing that it has outstanding defects. In an industrial setting, time and cost are often the \u201cexit criteria\u201d and - unfortunately - not the technical aspects like coverage or defect ratio. In such situations, the stakeholders accept that the remaining defects will be found after release, so sufficient resources are allocated to the \u201cstabilization\u201d phases following the release. It is hard for many organizations to see that such an approach is significantly costlier than trying to locate the defects earlier. We performed an empirical investigation of this for one of our industrial partners (a financial company). In this project, significant perfective maintenance was performed on the large information system. Based on changes made to the system, we carried out procedure level code coverage measurements with code level change impact analysis, and a\u00a0\u2026", "num_citations": "2\n", "authors": ["986"]}
{"title": "Experiences in Adapting a Source Code-Based Quality Assessment Technology\n", "abstract": " Testing-based software quality assurance often does not provide an appropriate level of efficiency and reliability. To aid this problem, different kinds of static verification techniques can be applied, like code metrics and code inspection. Many quality assessment methods that are based on static source code analysis has already been proposed, yet these can be used is particular industrial environment - in which often proprietary programming languages are used - only after appropriate adaptation. This paper presents experiences in adapting an existing technology and tools suitable for quality assessment based on source code analysis. The technology has demonstrated its success and usability in industrial environment; being capable of comprehensive and continuous quality monitoring of large and complex software systems involving proprietary technologies.", "num_citations": "2\n", "authors": ["986"]}
{"title": "Forr\u00e1sk\u00f3d anal\u00edzis \u00e9s szeletel\u00e9s a programmeg\u00e9rt\u00e9s t\u00e1mogat\u00e1s\u00e1hoz\n", "abstract": " Forr\u00e1sk\u00f3d anal\u00edzis \u00e9s szeletel\u00e9s a programmeg\u00e9rt\u00e9s t\u00e1mogat\u00e1s\u00e1hoz - SZTE Doktori Repozit\u00f3rium SZTE Contenta Nyit\u00f3lap A repozit\u00f3riumr\u00f3l B\u00f6ng\u00e9sz\u00e9s, \u00c9v B\u00f6ng\u00e9sz\u00e9s, Tudom\u00e1nyter\u00fcletek B\u00f6ng\u00e9sz\u00e9s, Doktori iskola B\u00f6ng\u00e9sz\u00e9s, Szerz\u0151 Bejelentkez\u00e9s adatfelt\u00f6lt\u00e9shez Regisztr\u00e1ci\u00f3 adatfelt\u00f6lt\u00e9shez English Forr\u00e1sk\u00f3d anal\u00edzis \u00e9s szeletel\u00e9s a programmeg\u00e9rt\u00e9s t\u00e1mogat\u00e1s\u00e1hoz Besz\u00e9des, \u00c1rp\u00e1d Forr\u00e1sk\u00f3d anal\u00edzis \u00e9s szeletel\u00e9s a programmeg\u00e9rt\u00e9s t\u00e1mogat\u00e1s\u00e1hoz. [Disszert\u00e1ci\u00f3] (K\u00e9ziratban) [img] El\u0151n\u00e9zet PDF (disszert\u00e1ci\u00f3) Download (2MB) [img] El\u0151n\u00e9zet PDF (t\u00e9zis) Download (830kB) [img] El\u0151n\u00e9zet PDF (t\u00e9zis) Download (817kB) [img] PDF (b\u00edr\u00e1lati lap) Restricted to Csak az arch\u00edvum karbantart\u00f3ja Download (4MB) M\u0171 t\u00edpusa: Disszert\u00e1ci\u00f3 (Doktori \u00e9rtekez\u00e9s) Publik\u00e1ci\u00f3ban haszn\u00e1lt n\u00e9v: Besz\u00e9des, \u00c1rp\u00e1d Idegen nyelv\u0171 c\u00edm: Source Code Analysis and Slicing for Program Comprehension Doktori iskola: Matematika- \u00e9s \u2026", "num_citations": "2\n", "authors": ["986"]}
{"title": "CharmFL: A Fault Localization Tool for Python\n", "abstract": " Fault localization is one of the most time-consuming and error-prone parts of software debugging. There are several tools for helping developers in the fault localization process, however, they mostly target programs written in Java and C/C++ programming languages. While these tools are splendid on their own, we must not look over the fact that Python is a popular programming language, and still there are a lack of easy-to- use and handy fault localization tools for Python developers. In this paper, we present a tool called \u201cCharmFL\u201d for software fault localization as a plug-in for PyCharm IDE. The tool employs Spectrum-based fault localization (SBFL) to help Python developers automatically analyze their programs and generate useful data at run-time to be used, then to produce a ranked list of potentially faulty program elements (i.e., statements, functions, and classes). Thus, our proposed tool supports different\u00a0\u2026", "num_citations": "1\n", "authors": ["986"]}
{"title": "Software testing conferences and women\n", "abstract": " The question of gender equality is an increasing concern in all aspects of life these days. ICT has its peculiarities in this respect, as it is often regarded as a\" male\" discipline. Among the many different subfields of ICT, in this work we concentrate on software testing, an area in which a significant portion of all ICT professionals is engaged. Testing is an interesting field because according to certain views more women work in this area compared to other ICT fields. Since testing itself still covers a large topic involving education, research and industry, we further limit our analysis to software testing conferences and the rate of women participation in important roles at these venues. We looked at keynote speakers and chairs in different roles and program committees, but not the participants themselves as reliable data was available only for the former. We investigated if gender distribution was similar to or different from the\u00a0\u2026", "num_citations": "1\n", "authors": ["986"]}
{"title": "Feature Level Complexity and Coupling Analysis in 4GL Systems\n", "abstract": " Product metrics are widely used in the maintenance and evolution phase of software development to advise the development team about software quality. Although most of these metrics are defined for mainstream languages, several of them were adapted to fourth generation languages (4GL) as well. Usual concepts like size, complexity and coupling need to be re-interpreted and adapted to program elements defined by these languages. In this paper we take a further step in this process to address product line development in 4GL. Adopting product line architecture is a necessary step to handle challenges of a growing number of similar product variants. The product line adoption process itself is a tedious task where features of the product variants play crucial role. Features represent a higher level of abstraction that are cross-cutting to program elements of 4GL applications. We propose a set of metrics\u00a0\u2026", "num_citations": "1\n", "authors": ["986"]}
{"title": "Development of a Unified Software Quality Platform in the Szeged InfoP\u00f3lus Cluster\n", "abstract": " In Software Quality Assurance, system status reports are key to the stakeholders for controlling software quality. Although we may rely on a large number of low level measurements to this end, their interpretation and the way of connecting them to high level quality attributes is always a challenge. In this paper we report on a complex project involving industrial partners whose aim is the development of a unified software quality platform that deals with and bridges these low and high level quality aspects, and provides a basis for the industrial applications of the approach. The project is implemented by a consortium of software industry members of the Szeged Software Innovation Pole Cluster and associated researchers with the support from the EU co-financed national grant promoting innovation clusters of small and middle-sized enterprises. The approach to the unified quality platform is based on the Goal-Question\u00a0\u2026", "num_citations": "1\n", "authors": ["986"]}
{"title": "Model based code compression\n", "abstract": " The invention relates to a code compression method that has a phase for treatment of model, the phase comprising a pruning phase and a growing phase combined into one phase. According to the method, test data can be used in estimation of the cost for the treatment of model when evaluating whether or not to apply a treatment of model in the phase for a certain code to be compressed according to the model.", "num_citations": "1\n", "authors": ["986"]}