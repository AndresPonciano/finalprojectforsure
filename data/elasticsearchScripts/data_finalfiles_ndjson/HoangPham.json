{"title": "Software reliability\n", "abstract": " Providing a general introduction to software reliability engineering, this book presents detailed analytical models, state-of-the-art techniques, methodologies, and tools used to assess the reliability of software systems. It also explores new directions of research in the field of software reliability engineering, including fault tolerant software and a new software reliability model that includes environmental factors.", "num_citations": "738\n", "authors": ["449"]}
{"title": "System software reliability\n", "abstract": " Computer software reliability has never been more important. Today, computers are employed in areas as diverse as air traffic control, nuclear reactors, aircraft, real-time military, industrial process control, security system control, biometric scan-systems, automotive, mechanical and safety control, and hospital patient monitoring systems. Many of these applications require critical functionality as software applications increase in size and complexity. System Software Reliability is an introduction to software reliability engineering as well as a detailed survey of the state-of-the-art techniques, methodologies and tools used to assess the reliability of software and combined software-hardware systems. The most recent research results in the field are reported and future directions in the subject are signposted. This comprehensive text will be of interest to: graduate students as an introduction to software reliability engineering and a textbook for graduate courses; software and reliability engineers as a comprehensive and up-to-date survey of the field; and, researchers and lecturers in universities and research institutions as a one-volume reference.", "num_citations": "653\n", "authors": ["449"]}
{"title": "Handbook of reliability engineering\n", "abstract": " An effective reliability programme is an essential component of every product's design, testing and efficient production. From the failure analysis of a microelectronic device to software fault tolerance and from the accelerated life testing of mechanical components to hardware verification, a common underlying philosophy of reliability applies. Defining both fundamental and applied work across the entire systems reliability arena, this state-of-the-art reference presents methodologies for quality, maintainability and dependability. Featuring: Contributions from 60 leading reliability experts in academia and industry giving comprehensive and authoritative coverage. A distinguished international Editorial Board ensuring clarity and precision throughout. Extensive references to the theoretical foundations, recent research and future directions described in each chapter. Comprehensive subject index providing maximum\u00a0\u2026", "num_citations": "558\n", "authors": ["449"]}
{"title": "Springer handbook of engineering statistics\n", "abstract": " Engineers and practitioners contribute to society through their ability to apply basic scientific principles to real problems in an effective and efficient manner. They must collect data to test their products every day as part of the design and testing process and also after the product or process has been rolled out to monitor its effectiveness. Model building and validation, data collection, data analysis and data interpretation form the core of sound engineering practice. After the data has been gathered the engineers, statisticians, designers, and practitioners must be able to sift them and interpret them correctly so that meaning can be exposed from a mass of undifferentiated numbers or facts. To do this he must be familiar with the fundamental concepts of correlation, uncertainty, variability and risk in the face of uncertainty. In today\u2019s global and highly competitive environment, continuous improvement in the processes and products of any field of engineering is essential for survival. Many organizations have shown that the first step to continuous improvement is to integrate the widespread use of statistics and basic data analysis into the manufacturing development process as well as into the day-to-day business decisions taken in regard to engineering and technological information processes. The Springer Handbook of Engineering Statistics gathers together the full range of statistical techniques required by readers from all fields to gain sensible statistical feedback on how their processes or products are functioning and to give them realistic predictions of how these could be improved. Key Topics Fundamental Statistics Process Monitoring and\u00a0\u2026", "num_citations": "384\n", "authors": ["449"]}
{"title": "A general imperfect-software-debugging model with S-shaped fault-detection rate\n", "abstract": " A general software reliability model based on the nonhomogeneous Poisson process (NHPP) is used to derive a model that integrates imperfect debugging with the learning phenomenon. Learning occurs if testing appears to improve dynamically in efficiency as one progresses through a testing phase. Learning usually manifests itself as a changing fault-detection rate. Published models and empirical data suggest that efficiency growth due to learning can follow many growth-curves, from linear to that described by the logistic function. On the other hand, some recent work indicates that in a real industrial resource-constrained environment, very little actual learning might occur because nonoperational profiles used to generate test and business models can prevent the learning. When that happens, the testing efficiency can still change when an explicit change in testing strategy occurs, or it can change as a result of\u00a0\u2026", "num_citations": "333\n", "authors": ["449"]}
{"title": "Reliability modeling of multi-state degraded systems with multi-competing failures and random shocks\n", "abstract": " In this paper, we develop a generalized multi-state degraded system reliability model subject to multiple competing failure processes, including two degradation processes, and random shocks. The operating condition of the multi-state systems is characterized by a finite number of states. We also present a methodology to generate the system states when there are multi-failure processes. The model can be used not only to determine the reliability of the degraded systems in the context of multi-state functions, but also to obtain the states of the systems by calculating the system state probabilities. Several numerical examples are given to illustrate the concepts.", "num_citations": "302\n", "authors": ["449"]}
{"title": "NHPP software reliability and cost models with testing coverage\n", "abstract": " This paper proposes a software reliability model that incorporates testing coverage information. Testing coverage is very important for both software developers and customers of software products. For developers, testing coverage information helps them to evaluate how much effort has been spent and how much more is needed. For customers, this information estimates the confidence of using the software product. Although research has been conducted and software reliability models have been developed, some practical issues have not been addressed. Testing coverage is one of these issues. The model is developed based on a nonhomogeneous Poisson process (NHPP) and can be used to estimate and predict the reliability of software products quantitatively. We examine the goodness-of-fit of this proposed model and present the results using several sets of software testing data. Comparisons of this model\u00a0\u2026", "num_citations": "298\n", "authors": ["449"]}
{"title": "On recent generalizations of the Weibull distribution\n", "abstract": " This short communication first offers a clarification to a claim by Nadarajah & Kotz. We then present a short summary (by no means exhaustive) of some well-known, recent generations of Weibull-related lifetime models for quick information. A brief discussion on the properties of this general class is also given. Some future research directions on this topic are also discussed.", "num_citations": "268\n", "authors": ["449"]}
{"title": "Maintenance for industrial systems\n", "abstract": " New, global and extended markets are forcing companies to process and manage increasingly different products with shorter life cycles, low volumes and reduced customer delivery times. In today\u2019s global marketplace production systems need to deliver products on time, maintain market credibility, and introduce new products and services faster than competitors. As a result, a new production system paradigm has been developed and a supporting management decision-making approach, which simultaneously incorporates design, management, and control of the production system, is necessary in order for this challenge to be met effectively and efficiency.Maintenance for Industrial Systems addresses this need by introducing an original and integrated idea of maintenance: maintenance for productivity. The volume starts with the introduction and discussion of a new conceptual framework based on productivity\u00a0\u2026", "num_citations": "247\n", "authors": ["449"]}
{"title": "Considering fault removal efficiency in software reliability assessment\n", "abstract": " Software reliability growth models (SRGMs) have been developed to estimate software reliability measures such as the number of remaining faults, software failure rate, and software reliability. Issues such as imperfect debugging and the learning phenomenon of developers have been considered in these models. However, most SRGMs assume that faults detected during tests will eventually be removed. Consideration of fault removal efficiency in the existing models is limited. In practice, fault removal efficiency is usually imperfect. This paper aims to incorporate fault removal efficiency into software reliability assessment. Fault removal efficiency is a useful metric in software development practice and it helps developers to evaluate the debugging effectiveness and estimate the additional workload. In this paper, imperfect debugging is considered in the sense that new faults can be introduced into the software during\u00a0\u2026", "num_citations": "247\n", "authors": ["449"]}
{"title": "Modeling the dependent competing risks with multiple degradation processes and random shock using time-varying copulas\n", "abstract": " We develop s-dependent competing risk model for systems subject to multiple degradation processes and random shocks using time-varying copulas. The proposed model allows for a more flexible dependence structure between risks in which (a) the dependent relationship between random shocks and degradation processes is modulated by a time-scaled covariate factor, and (b) the dependent relationship among various degradation processes is fitted using the copula method. Two types of random shocks are considered in the model: fatal shocks, which fails the system immediately; and nonfatal shocks, which does not. In a nonfatal shock situation there are two impacts towards the degradation processes: sudden increment jumps, and degradation rate accelerations. The comparison results of the system reliability estimation from both constant and time-varying copulas are illustrated in the numerical examples\u00a0\u2026", "num_citations": "230\n", "authors": ["449"]}
{"title": "An inspection-maintenance model for systems with multiple competing processes\n", "abstract": " In some applications, the failure rate of the system depends not only on the time, but also upon the status of the system, such as vibration level, efficiency, number of random shocks on the system, etc., which causes degradation. In this paper, we develop a generalized condition-based maintenance model subject to multiple competing failure processes including two degradation processes, and random shocks. An average long-run maintenance cost rate function is derived based on the expressions for the degradation paths & cumulative shock damage, which are measurable. A geometric sequence is employed to develop the inter-inspection sequence. Upon inspection, one needs to decide whether to perform a maintenance, such as preventive or corrective, or to do nothing. The preventive maintenance thresholds for degradation processes & inspection sequences are the decision variables of the proposed model\u00a0\u2026", "num_citations": "227\n", "authors": ["449"]}
{"title": "An analysis of factors affecting software reliability\n", "abstract": " This paper presents the findings of empirical research from 13 companies participating in software development to identify the factors that may impact software reliability. Thirty-two potential factors involved in every stage of the software development process are defined. The study uses a survey instrument to analyze these factors and identify factors that have significant impact on software reliability. The survey focuses on the perspective of the primary participants, managers, system engineers, programmers, testers and other people involved in software research or development teams. Two techniques such as the relative weight method and analysis of variance technique (ANOVA) have been used to analyze all factors and rank them in terms of their impact on software reliability. The research findings have important implications for further research and the practice of software development. For researchers, it points\u00a0\u2026", "num_citations": "225\n", "authors": ["449"]}
{"title": "A software cost model with warranty and risk costs\n", "abstract": " In this paper, a cost model with warranty cost, time to remove each error detected in the software system, and risk cost due to software failure is developed. A software reliability model based on non-homogeneous Poisson process is used. The optimal release policies to minimize the expected total software cost are discussed. A software tool is also developed using Excel and Visual Basic to facilitate the task of determining the optimal software release time. Numerical examples are provided to illustrate the results.", "num_citations": "225\n", "authors": ["449"]}
{"title": "A unified approach for developing software reliability growth models in the presence of imperfect debugging and error generation\n", "abstract": " In this paper, we propose two general frameworks for deriving several software reliability growth models based on a non-homogeneous Poisson process (NHPP) in the presence of imperfect debugging and error generation. The proposed models are initially formulated for the case when there is no differentiation between failure observation and fault removal testing processes, and then extended for the case when there is a clear differentiation between failure observation and fault removal testing processes. During the last three decades, many software reliability growth models (SRGM) have been developed to describe software failures as a random process, and can be used to evaluate development status during testing. With SRGM, software engineers can easily measure (or forecast) the software reliability (or quality), and plot software reliability growth charts. It is not easy to select the best model from a plethora\u00a0\u2026", "num_citations": "197\n", "authors": ["449"]}
{"title": "An NHPP software reliability model and its comparison\n", "abstract": " In this paper, software reliability models based on a nonhomogeneous  Poisson process (NHPP) are summarized. A new model based on NHPP is  presented. All models are applied to two widely used data sets. It can  be shown that for the failure data used here, the new model fits and  predicts much better than the existing models. A software program is  written, using Excel & Visual Basic, which can be used to facilitate  the task of obtaining the estimators of model parameters.", "num_citations": "159\n", "authors": ["449"]}
{"title": "Software reliability and cost models: Perspectives, comparison, and practice\n", "abstract": " Computer software has gradually become an indispensable element in many aspects of our daily life and an important factor in numerous critical applications such as nuclear plants, medical monitoring control, real-time military and air traffic control. In this paper, we present recent studies in software reliability engineering that include nonhomogeneous Poisson process (NHPP) software reliability models, NHPP models with environmental factors, and cost models. Several applications are presented to illustrate the models. More applications are needed to fully validate the descriptive and predictive software reliability models in a general industrial setting. Future research directions in software reliability are also discussed.", "num_citations": "157\n", "authors": ["449"]}
{"title": "A software cost model with imperfect debugging, random life cycle and penalty cost\n", "abstract": " The paper develops a cost model with an imperfect debugging and random life cycle as well as a penalty cost that is used to determine the optimal release policies for a software system. The software reliability model, based on the nonhomogeneous Poisson process, allows for three different error types: critical, major and minor errors. The model also allows for the introduction of any of these errors during the removal of an error. Using the software reliability model presented, the cost model with multiple error types and imperfect debugging is developed. This cost also considers the penalty cost due to delay for a scheduled delivery time and the length of the software life cycle is random with a known distribution. The optimal software release policies that minimize the expected software system costs (subject to the various constraints) or maximize the software reliability subject to a cost constraint, are then determined\u00a0\u2026", "num_citations": "152\n", "authors": ["449"]}
{"title": "A multi-objective optimization of imperfect preventive maintenance policy for dependent competing risk systems with hidden failure\n", "abstract": " This paper studies a multi-objective maintenance optimization embedded within the imperfect preventive maintenance (PM) for one single-unit system subject to the dependent competing risks of degradation wear and random shocks. We consider two kinds of random shocks in the system: 1) fatal shocks that will cause the system to fail immediately, and 2) nonfatal shocks that will increase the system degradation level by a certain cumulative shock amount. Also, an improvement factor in the form of quasi-renewal sequences is introduced to modulate the imperfect maintenance by raising the degradation critical threshold proportionally. Finally, the two decision variables for maintenance scheduling, the number of PMs to replacement, and the initial PM interval, are determined by simultaneously maximizing the system asymptotic availability, and minimizing the system cost rate using the fast elitist non-dominated\u00a0\u2026", "num_citations": "146\n", "authors": ["449"]}
{"title": "A methodology for priority setting with application to software development process\n", "abstract": " In this paper we briefly review the basic ideas behind the Analytic Hierarchy Process (AHP). Based on these ideas, we introduce the concept of comparison interval and propose a methodology based on stochastic optimization to achieve global consistency and to accommodate the fuzzy nature of the comparison process. We also discuss systems with feedback. For systems with inter-component dependence, we review the super-matrix technique. For systems with intra-component dependence, we propose a systematic approximation scheme to compute the weight vector for priority setting. Applications of software development process and a numerical example are given to illustrate our methodology and results.", "num_citations": "132\n", "authors": ["449"]}
{"title": "Cost-effective condition-based maintenance using Markov decision processes\n", "abstract": " Investigations conducted in several industries indicate that there is no direct relationship between equipment failure and equipment age in the majority of cases. Most failures are caused by events or conditions that occur during component operation and manufacturing processes. Therefore, optimal maintenance decisions should be based on the actual deterioration conditions of the components. Condition-Based Maintenance (CBM) is a methodology that strives to identify incipient faults before they become critical to enable more accurate planning of preventive actions. For the ultimate success of CBM methodology, we must have sound methods for modeling deterioration (the propagation of faulty conditions), the conditions and their effects, and the optimal selection and scheduling of inspections and preventive maintenance actions (the right action at the right time). In this paper, we present a generalized CBM\u00a0\u2026", "num_citations": "126\n", "authors": ["449"]}
{"title": "Two dimensional multi-release software reliability modeling and optimal release planning\n", "abstract": " Long-lived software systems evolve through new product releases, which involve up-gradation of previous released versions of the software in the market. But, upgrades in software lead to an increase in the fault content. Thus, for modeling the reliability growth of software with multiple releases, we must consider the failures of the upcoming upgraded release, and the failures that were not debugged in the previous release. Based on this idea, this paper proposes a mathematical modeling framework for multiple releases of software products. The proposed model takes into consideration the combined effect of schedule pressure and resource limitations using a Cobb Douglas production function in modeling the failure process using a software reliability growth model. The model developed is validated on a four release failure data set. Another major concern for the software development firms is to plan the release of\u00a0\u2026", "num_citations": "119\n", "authors": ["449"]}
{"title": "Springer Series in Reliability Engineering\n", "abstract": " The failure of structures or (parts of) systems is a common problem in practice. In all sectors of industry, from transport (air, rail, water, and road) to process industry, energy generation and high tech manufacturing industry, complex capital assets are used and must be maintained to assure their failure-free operation. In many cases the failed parts can be repaired or replaced, after which the system can fulfil its intended function again. In other cases, failures must be prevented against all costs, as the consequences for the system or its surroundings can be significant or even disastrous. Examples of the latter are the failure of critical aero engine parts leading to aircraft crashes or failing safety systems in a nuclear plant. For many other failures, a deliberation must be made between the costs of preventive replacements and the costs and collateral damage of a failure. However, to be able to make a solid consideration, it\u00a0\u2026", "num_citations": "108\n", "authors": ["449"]}
{"title": "Tampered failure rate load-sharing systems: Status and perspectives\n", "abstract": " Load-sharing systems have several practical applications. In load-sharing systems, the event of a component failure will result in a higher load, therefore inducing a higher failure rate, in each of the surviving components. This introduces failure dependency among the load-sharing components, which in turn increases the complexity in analyzing these systems. In this chapter, we first discuss modeling approaches and existing solution methods for analyzing the reliability of load-sharing systems. We then describe tampered failure rate (TFR) load-sharing systems and their properties. Using these properties, we provide efficient solution methods for solving TFR load-sharing models. Because load-sharing k-out-of-n systems have several practical applications in reliability engineering, we provide a detailed analysis for various cases of these systems. The solution methods proposed in this chapter are applicable\u00a0\u2026", "num_citations": "107\n", "authors": ["449"]}
{"title": "Reliability Characteristics of -out-of- Warm Standby Systems\n", "abstract": " We study reliability characteristics of the  k -out-of-  n  warm standby system with identical components subject to exponential lifetime distributions. We derive state probabilities of the warm standby system in a form that is similar to the state probabilities of the active redundancy system. Subsequently, the system reliability is expressed in several forms that can provide new insights into the system reliability characteristics. We also show that all properties and computational procedures that are applicable for active redundancy are also applicable for the warm standby redundancy. As a result, it is shown that the system reliability can be evaluated using robust algorithms within  O ( n - k +1) computational time. In addition, we provide closed-form expressions for the hazard rate, probability density function, and mean residual life function. We show that the time-to-failure distribution of the  k -out-of- n  warm standby system\u00a0\u2026", "num_citations": "97\n", "authors": ["449"]}
{"title": "A new methodology for predicting software reliability in the random field environments\n", "abstract": " This paper presents a new methodology for predicting software reliability in the field environment. Our work differs from some existing models that assume a constant failure detection rate for software testing and field operation environments, as this new methodology considers the random environmental effects on software reliability. Assuming that all the random effects of the field environments can be captured by a unit-free environmental factor, eta, which is modeled as a random-distributed variable, we establish a generalized random field environment (RFE) software reliability model that covers both the testing phase and the operating phase in the software development cycle. Based on the generalized RFE model, two specific random field environmental reliability models are proposed for predicting software reliability in the field environment: the gamma-RFE model, and the beta-RFE model. A set of software\u00a0\u2026", "num_citations": "93\n", "authors": ["449"]}
{"title": "Reliability modeling of hardware and software interactions, and its applications\n", "abstract": " We classify system failures into three categories: hardware failures, software failures, and hardware-software interaction failures. We develop a unified reliability model that accounts for failures in all three categories. Hardware, and software failures are accounted for with well-known modeling approaches. In this paper, we propose a modeling methodology using Markov processes to capture hardware-software interaction failures. We illustrate the combined hardware & software modeling approach by applying it to a real telecommunication system", "num_citations": "88\n", "authors": ["449"]}
{"title": "Optimal design of k-out-of-n: G subsystems subjected to imperfect fault-coverage\n", "abstract": " Systems subjected to imperfect fault-coverage may fail even prior to the exhaustion of spares due to uncovered component failures. This paper presents optimal cost-effective design policies for k-out-of-n:G subsystems subjected to imperfect fault-coverage. It is assumed that there exists a k-out-of-n:G subsystem in a nonseries-parallel system and, except for this subsystem, the redundancy configurations of all other subsystems are fixed. This paper also presents optimal design polices which maximize overall system reliability. As a special case, results are presented for k-out-of-n:G systems subjected to imperfect fault-coverage. Examples then demonstrate how to apply the main results of this paper to find the optimal configurations of all subsystems simultaneously. In this paper, we show that the optimal n which maximizes system reliability is always less than or equal to the n which maximizes the reliability of the\u00a0\u2026", "num_citations": "87\n", "authors": ["449"]}
{"title": "Weighted voting systems\n", "abstract": " This paper deals with reliability and cost evaluation of weighted dynamic-threshold voting-systems. The particular voting system studied consists of n units that each provide a binary decision (0 or 1) or abstain from voting (x). The system output is 1 if the cumulative weight of all 1-opting units is at least a pre-specified fraction /spl tau/ of the cumulative weight of all nonabstaining units. Otherwise the system output is 0. Systems of this type are encountered in many areas ranging from target to pattern recognition, safety monitoring and human organization systems. These results for a voting system with a particular structure can easily be modified for other voting systems as well. A general mathematical model of this system is presented. However, its immense combinatorial complexity (even for small-scale and trivial systems) makes the model virtually inapplicable for real systems. By placing two restrictions on the\u00a0\u2026", "num_citations": "86\n", "authors": ["449"]}
{"title": "Software field failure rate prediction before software deployment\n", "abstract": " For both in-house development and outsourcing development environments, knowing the field failure rate of an integrated software system prior to field deployment provides guidance for better decision-makings in balancing reliability, time-to-market and development cost. This paper demonstrates a field failure rate prediction methodology that starts with analyzing system test data and field data (of previous releases or products) using software reliability growth models (SRGMs). A typical issue associated with predicting field failure rate based on test data is that potentially the test environment might not match exactly up the field environment. We discuss how to address the mismatch of the operational profiles of the test and filed environments. Two other practical issues in predicting field failure rates include that fault removals in the field are usually non-instantaneous and fixes of certain faults reported in the field can\u00a0\u2026", "num_citations": "85\n", "authors": ["449"]}
{"title": "Software reliability models with time-dependent hazard function based on Bayesian approach\n", "abstract": " In this paper, two models predicting mean time until next failure based on Bayesian approach are presented. Times between failures follow Weibull distributions with stochastically decreasing ordering on the hazard functions of successive failure time intervals, reflecting the tester's intent to improve the software quality with each corrective action. We apply the proposed models to actual software failure data and show they give better results under sum of square errors criteria as compared to previous Bayesian models and other existing times between failures models. Finally, we utilize likelihood ratios criterion to compare new model's predictive performance.", "num_citations": "82\n", "authors": ["449"]}
{"title": "NHPP software reliability model considering the uncertainty of operating environments with imperfect debugging and testing coverage\n", "abstract": " In this paper, we propose a testing-coverage software reliability model that considers not only the imperfect debugging (ID) but also the uncertainty of operating environments based on a non-homogeneous Poisson process (NHPP). Software is usually tested in a given control environment, but it may be used in different operating environments by different users, which are unknown to the developers. Many NHPP software reliability growth models (SRGMs) have been developed to estimate the software reliability measures, but most of the underlying common assumptions of these models are that the operating environment is the same as the developing environment. But in fact, due to the unpredictability of the uncertainty in the operating environments for the software, environments may considerably influence the reliability and software's performance in an unpredictable way. So when a software system works in a\u00a0\u2026", "num_citations": "78\n", "authors": ["449"]}
{"title": "A software reliability model with a Weibull fault detection rate function subject to operating environments\n", "abstract": " When software systems are introduced, these systems are used in field environments that are the same as or close to those used in the development-testing environments; however, they may also be used in many different locations that may differ from the environment in which they were developed and tested. As such, it is difficult to improve software reliability for a variety of reasons, such as a given environment, or a bug location in code. In this paper, we propose a new software reliability model that takes into account the uncertainty of operating environments. The explicit mean value function solution for the proposed model is presented. Examples are presented to illustrate the goodness of fit of the proposed model and several existing non-homogeneous Poisson process (NHPP) models and confidence intervals of all models based on two sets of failure data collected from software applications. The results show that the proposed model fits the data more closely than other existing NHPP models to a significant extent. View Full-Text", "num_citations": "76\n", "authors": ["449"]}
{"title": "Remote control and maintenance outsourcing networks and its applications in supply chain management\n", "abstract": " The paper analyzes the impact of e-business technologies on maintenance management and supply chain operations. The aim of this work is to investigate the network organization level of supply chains in case of remote maintenance application and to understand how maintenance policies are coupled with information technology (IT) solutions. To this purpose two literature reviews are presented: firstly, on the supply chain and network integration, and then on the evolution of maintenance using information technology. Following this, the paper present four specific industrial case-studies of eMRO network organisation. They have been chosen as reference models from a set of practical applications and pilot tests performed by the authors in different production sectors in the last 5 years. Technology complexity environments, maintenance outsourcing level, and supply chain integration context are discussed for\u00a0\u2026", "num_citations": "75\n", "authors": ["449"]}
{"title": "Availability and mean life time prediction of multistage degraded system with partial repairs\n", "abstract": " In some environments, components might not always fail fully, but can degrade, and there can be multiple stages of degradation. In such cases, the efficiency of the system may decrease. After a certain stage of degradation the efficiency of the system may decrease to an unacceptable limit and can be considered as a total failure. However, the system can fail randomly from any stage. and can be repaired. Further, the repair action cannot bring the system to the good stage, but can make it operational and the failure rate of the system will, therefore, remain the same as before the failure. In this study, we present a model for predicting the reliability, availability, mean life time, and mean time to first failure of multistage degraded systems with partial repairs. In the analysis, state dependent transition rates for the degradation process, as well as repair processes, are considered. A numerical example is provided to\u00a0\u2026", "num_citations": "75\n", "authors": ["449"]}
{"title": "Cost analysis on renewable full-service warranties for multi-component systems\n", "abstract": " This paper presents a novel warranty policy named \u2018full-service warranty\u2019 (FSW) for repairable multi-component systems under which the failed component(s) or subsystem(s) will be replaced, in addition, a (perfect) maintenance action will be performed to reduce the chance of future system failure, both free of charge to consumers. Such a policy is desirable for both consumers and manufacturers since consumers receive better warranty service compared to the traditional free repair policy, at the same time, manufacturers may enjoy increase in sale as well as cost-saving due to improved product reliability by the maintenance action. Under the renewable FSW policy, from manufacturers\u2019 point of view, cost models for complex systems with series, parallel, series\u2013parallel (s\u2013p) and parallel\u2013series (p\u2013s) structure are developed. Exact expressions for the first and second centered moments of warranty cost per product\u00a0\u2026", "num_citations": "72\n", "authors": ["449"]}
{"title": "Quasi-renewal time-delay fault-removal consideration in software reliability modeling\n", "abstract": " Software reliability growth models based on a nonhomogeneous Poisson process (NHPP) have been considered as one of the most effective among various models since they integrate the information regarding testing and debugging activities observed in the testing phase into the software reliability model. Although most of the existing NHPP models have progressed successfully in their estimation/prediction accuracies by modifying the assumptions with regard to the testing process, these models were developed based on the instantaneous fault-removal assumption. In this paper, we develop a generalized NHPP software reliability model considering quasi-renewal time-delay fault removal. The quasi-renewal process is employed to estimate the time delay due to identifying and prioritizing the detected faults before actual code change in the software reliability assessment. Model formulation based on the quasi\u00a0\u2026", "num_citations": "70\n", "authors": ["449"]}
{"title": "A software-reliability growth model for N-version programming systems\n", "abstract": " This paper presents a NHPP-based SRGM (software reliability growth model) for NVP (N-version programming) systems (NVP-SRGM) based on the NHPP (nonhomogeneous Poisson process). Although many papers have been devoted to modeling NVP-system reliability, most of them consider only the stable reliability, i.e., they do not consider the reliability growth in NVP systems due to continuous removal of faults from software versions. The model in this paper is the first reliability-growth model for NVP systems which considers the error-introduction rate and the error-removal efficiency. During testing and debugging, when a software fault is found, a debugging effort is devoted to remove this fault. Due to the high complexity of the software, this fault might not be successfully removed, and new faults might be introduced into the software. By applying a generalized NHPP model into the NVP system, a new NVP\u00a0\u2026", "num_citations": "66\n", "authors": ["449"]}
{"title": "An imperfect-debugging fault-detection dependent-parameter software\n", "abstract": " Software reliability growth models (SRGMs) incorporating the imperfect debugging and learning phenomenon of developers have recently been developed by many researchers to estimate software reliability measures such as the number of remaining faults and software reliability. However, the model parameters of both the fault content rate function and fault detection rate function of the SRGMs are often considered to be independent from each other. In practice, this assumption may not be the case and it is worth to investigate what if it is not. In this paper, we aim for such study and propose a software reliability model connecting the imperfect debugging and learning phenomenon by a common parameter among the two functions, called the imperfect-debugging fault-detection dependent-parameter model. Software testing data collected from real applications are utilized to illustrate the proposed model for\u00a0\u2026", "num_citations": "64\n", "authors": ["449"]}
{"title": "1-N-arylpyrazole derivatives in prevention of arthropod-borne and mosquito-borne diseases\n", "abstract": " This invention provides for a method for preventing or interrupting the transmission of anthropod and mosquito borne diseases from a first actual or putative amplifying or incipient host to a second actual or putative amplifying or incipient host, which comprises applying or administering a formulation comprising an effective amount of at least one 1-N-arylpyrazole to said first actual or putative amplifying host and/or actual or putative amplifying or incipient host. This invention also provides for a method of using a bait placed in a bird station, the bait comprises an affective amount of at least one 1-N-arylpyrazole either alone or in combination with an antiparasitic, antihelmintic, or insecti cidal agents. This invention also provides for an apparatus for delivering an effective amount of a formulation accord ing to the present invention to a bird or animal in an environment where it resides.", "num_citations": "62\n", "authors": ["449"]}
{"title": "Software cost model for quantifying the gain with considerations of random field environments\n", "abstract": " In this paper, we present a software gain model under random field environment with consideration of not only time to remove faults during in-house testing, cost of removing faults during beta testing, risk cost due to software failure, but also the benefits from reliable executions of the software during the beta testing and field operation. To our knowledge, this is the first study that incorporates the random field environmental factor into the cost model. We also provide an optimal release policy in which the net gain of the software development process is maximized. This gain model can help managers and developers to determine when to stop testing the software and release it to beta testing users and to end-users.", "num_citations": "62\n", "authors": ["449"]}
{"title": "On the maximum likelihood estimates for the Goel\u2013Okumoto software reliability model\n", "abstract": " We show that the maximum likelihood (ML) estimates of the parameters of a well-known software reliability model are not consistent as the observation period for observed software failures extends to infinity. Properties of the ML estimators as the observation period gets long are particularly important when the observation period corresponds to the test interval, since extending the test interval is the most natural way to improve the reliability of the software prior to its release. In addition to providing insight on how to interpret the ML estimators in actual applications, our result also has pedagogical value as an illustration that asymptotic properties of ML estimators cannot be taken for granted.", "num_citations": "61\n", "authors": ["449"]}
{"title": "Discounted warranty cost of minimally repaired series systems\n", "abstract": " Many factors should be considered in modeling DWC (discounted warranty cost) of repairable systems or products including system structure, components' failure processes, methods of discounting as well as the warranty policy itself. In this paper, we present DWC models for repairable series systems. In particular, a free repair warranty policy and a pro-rata warranty policy are studied. The impact of repair actions on components' failure times is assumed to be minimal, hence NHPPs are used to describe the failure processes. Two types of discounting methods are considered in this paper: a continuous discount function and a discrete discount function. Expressions for both the expected value and variance of DWC are derived. The applications of our findings can be seen in warranty design, warranty reserve determination and risk analysis. Our approach incorporates the information of system structure, the value of\u00a0\u2026", "num_citations": "60\n", "authors": ["449"]}
{"title": "Reliability analysis of a high voltage system with dependent failures and imperfect coverage\n", "abstract": " Nearly all models concerned with the reliability of redundant systems assume that a failed component does not affect the other working components. This assumption does not apply to many nuclear power systems. In this paper, a high voltage (HV) system consisting of a power supply and two transmitters is considered. We treat the more realistic case of stochastic dependence in the sense that, caused by the failure of a component (transmitter), the failure rate of the working component increases. A model of the HV system as well as a detailed development of the reliability function of this system are presented. The mean time to failure of the HV system is also obtained. A numerical example is provided to illustrate the results.", "num_citations": "58\n", "authors": ["449"]}
{"title": "Cost models for age replacement policies and block replacement policies under warranty\n", "abstract": " In this paper, we elaborate on cost models by examining the renewable and non-renewable warranty policies subject to minimal repair within the warranty period and the post-warranty period. Among various maintenance policies, the block replacement policy and the age replacement policy have been investigated and compared under the broader warranty perspective. The cost model is developed from the perspective of the customer. This analytical model should provide manufacturers with a better understanding of customer behavior. We consider failure time and repair time simultaneously instead of the traditional two dimensions such as age and usage because it may be difficult to obtain usage information. For the customer's satisfaction, the repair time threshold, which has several types based on the properties of a product, is fixed. However, if the repair time exceeds the repair time threshold, the decision is to\u00a0\u2026", "num_citations": "57\n", "authors": ["449"]}
{"title": "H\u00f6lder continuous solutions to Monge\u2013Amp\u00e8re equations\n", "abstract": " Let (X, \u03c9) be a compact K\u00e4hler manifold. We obtain uniform H\u00f6lder regularity for solutions to the complex Monge\u2013Amp\u00e8re equation on X with Lp right hand side, p> 1. The same regularity is furthermore proved on the ample locus in any big cohomology class. We also study the range MAH (X, \u03c9) of the complex Monge\u2013Amp\u00e8re operator acting on \u03c9-plurisubharmonic H\u00f6lder continuous functions. We show that this set is convex, by sharpening Ko\u0142odziej\u2019s result that measures with Lp-density belong to MAH (X, \u03c9) and proving that MAH (X, \u03c9) has the \u201cLp-property\u201d, p> 1. We also describe accurately the symmetric measures it contains.", "num_citations": "56\n", "authors": ["449"]}
{"title": "Calibrating software reliability models when the test environment does not match the user environment\n", "abstract": " Software failures have become the major factor that brings the system down or causes a degradation in the quality of service. For many applications, estimating the software failure rate from a user's perspective helps the development team evaluate the reliability of the software and determine the release time properly. Traditionally, software reliability growth models are applied to system test data with the hope of estimating the software failure rate in the field. Given the aggressive nature by which the software is exercised during system test, as well as unavoidable differences between the test environment and the field environment, the resulting estimate of the failure rate will not typically reflect the user\u2010perceived failure rate in the field. The goal of this work is to quantify the mismatch between the system test environment and the field environment. A calibration factor is proposed to map the failure rate estimated from\u00a0\u2026", "num_citations": "56\n", "authors": ["449"]}
{"title": "Warranty cost analyses using quasi-renewal processes for multicomponent systems\n", "abstract": " In this paper, warranty cost models are presented based on the quasi-renewal processes and exponential distribution. Cost analyses are conducted for various systems under the basic assumption that a repair service is imperfect. We develop warranty cost models, reliability, and other measures for several systems, including multicomponent systems. This paper focuses on warranty cost analysis, including repairable products with a given warranty period considering conditional probabilities and renewal theory. The exponential distribution is used to analyze and obtain the warranty cost. Numerical examples are discussed to demonstrate the applicability of the methodology derived in this paper.", "num_citations": "55\n", "authors": ["449"]}
{"title": "A new software reliability model with Vtub-shaped fault-detection rate and the uncertainty of operating environments\n", "abstract": " Many software reliability growth models (SRGMs) have developed in the past three decades to estimate software reliability measures such as the number of remaining faults and software reliability. The underlying common assumption of many existing models is that the operating environment and the developing environment are the same. This is often not the case in practice because the operating environments are usually unknown due to the uncertainty of environments in the field. In this paper, we develop a new software reliability model incorporating the uncertainty of system fault-detection rate per unit of time subject to operating environments. Examples are included to illustrate the goodness-of-fit of proposed model and several existing non-homogeneous Poisson process (NHPP) models based on a set of failure data collected from software applications. Three goodness-of-fit criteria, such as mean square\u00a0\u2026", "num_citations": "54\n", "authors": ["449"]}
{"title": "A software cost model with error removal times and risk costs\n", "abstract": " A cost model subject to times to remove errors in the software system and risk cost due to software failure is developed. A software reliability model based on the non-homogeneous Poisson process is used. The optimal release policies to minimize the expected total software cost are discussed. A software is also developed using Excel and Visual Basic tools to facilitate the task of determining the optimal software release time. Numerical examples are provided to illustrate the results.", "num_citations": "54\n", "authors": ["449"]}
{"title": "Repair-limit risk-free warranty policies with imperfect repair\n", "abstract": " The current competitive market environment requires manufacturers to continuously provide better service and support. As a result, warranty considerations emerge as a significant instrument for increasing product marketability. In this paper, we propose a new warranty policy, the repair-limit risk-free warranty with a threshold point on the number of repairs, where replacement is deemed to be more cost effective thereafter. Consumers are better off with this than with a traditional free-repair policy since they could be compensated with a new product in case of premature failures. As for the manufacturers, it not only offers extra marketing incentives, but also reduces the possibility of high-cost lawsuits due to the products with \"proven\" bad quality. Some useful results of the warranty cost of imperfectly repaired products are derived through a censored quasi-renewal process.", "num_citations": "53\n", "authors": ["449"]}
{"title": "Software release policies with gain in reliability justifying the costs\n", "abstract": " An important question in the developing process of software products is when to stop testing to make the gain in reliability justify the costs. In this paper, a software reliability-cost model is presented to determine the optimal release policies that maximize the expected net gain in reliability. Several software cost factors are considered in this study, such as the cost of removing detected errors and the risk cost due to software failure. The error removal cost is modeled based on a stochastic process. A software reliability model based on the non-homogeneous Poisson process is used. Numerical examples are provided to illustrate the results.", "num_citations": "53\n", "authors": ["449"]}
{"title": "A reliability decision framework for multiple repairable units\n", "abstract": " In practice, the analyst is often dealing with multiple repairable units, installed in different positions or functioning under different operating conditions, and maintained by different disciplines. This paper presents a decision framework to identify an appropriate reliability model for massive multiple repairable units. It splits non-homogeneous failure data into homogeneous groups and classifies them based on their failure trends using statistical tests. The framework discusses different scenarios for analysing multiple repairable units, according to trend, intensity, and dependency of the units\u05f3 failure data. The proposed framework has been verified in a fleet of aircraft and in two simulated data sets. The results show a reliability model of multiple repairable units may contain a mixture of different stochastic models. Considering single reliability models for such populations may cause erroneous calculation of the time to\u00a0\u2026", "num_citations": "47\n", "authors": ["449"]}
{"title": "A sharp lower bound for the log canonical threshold\n", "abstract": " In this note, we prove a sharp lower bound for the log canonical threshold of a plurisubharmonic function \u03c6 with an isolated singularity at 0 in an open subset of Cn. This threshold is defined as the supremum of constants c\u00a0>\u00a00 such that e-2c\u03c6 is integrable on a neighborhood of 0. We relate c(\u03c6) to the intermediate multiplicity numbers ej(\u03c6), defined as the Lelong numbers of (ddc\u03c6)j at 0 (so that in particular e0(\u03c6)=1). Our main result is that c(\u03c6)\u2a7e\u2211j=0n-1ej(\u03c6)/ej+1(\u03c6). This inequality is shown to be sharp; it simultaneously improves the classical result c(\u03c6)\u2a7e1/e1(\u03c6) due to Skoda, as well as the lower estimate c(\u03c6)\u2a7en/en(\u03c6)1/n which has received crucial applications to birational geometry in recent years. The proof consists in a reduction to the toric case, i.e. singularities arising from monomial ideals.", "num_citations": "47\n", "authors": ["449"]}
{"title": "Software reliability and testing\n", "abstract": " From the Publisher: This book presents 14 recent papers that provide an overview of the latest software reliability models, testing techniques, and applications. The book is targeted toward design engineers, software engineers, researchers, computer scientists, technical managers, and students wishing to conduct research or update their knowledge in the field of software reliability and testing. The papers on software reliability describe applications of software reliability growth modeling to software systems, investigate the relationship between program complexity measures and program errors, and explore the difficulty in accurately modeling software reliability. The remaining papers present new models that are useful in estimating the reliability of software systems and deal with the cost-reliability-optimal software release policies in software systems. The papers on software testing investigate the relationship\u00a0\u2026", "num_citations": "46\n", "authors": ["449"]}
{"title": "Optimal allocation of testing effort during testing and debugging phases: a control theoretic approach\n", "abstract": " Allocation of efforts to a software development project during the testing phase is a multifaceted task for software managers. The challenges become stiffer when the nature of the development process is considered in the dynamic environment. Many software reliability growth models have been proposed in last decade to minimise the total testing-effort expenditures, but mostly under static assumption. The main purpose of this article is to investigate an optimal resource allocation plan to minimise the cost of software during the testing and operational phase under dynamic condition. An elaborate optimisation policy based on the optimal control theory is proposed and numerical examples are illustrated. This article also studies the optimal resource allocation problems for various conditions by examining the behaviour of the model parameters and also suggests policy for the optimal release time of the software. The\u00a0\u2026", "num_citations": "45\n", "authors": ["449"]}
{"title": "Comparisons of nonhomogeneous Poisson process software reliability models and its applications\n", "abstract": " In this paper, existing software reliability models based on an nonhomogeneous Poisson process (NHPP) are summarized. A recently developed NHPP model with imperfect debugging and time-dependent fault detection rate is examined. The results show that this proposed model provides improved descriptive and predictive power when compared with other existing NHPP models. The model can also be incorporated into a software cost model for reliability assessment. The optimal release policies which minimize the expected total software cost is also presented.", "num_citations": "45\n", "authors": ["449"]}
{"title": "Optimal cost-effective design of triple-modular-redundancy-with-spares systems\n", "abstract": " The design issue for the optimal number of spare units in a triple-modular-redundancy system with spare units, including fault coverage and common-cause failure, is addressed. Two aspects of the problem are shown: (1) how to minimize the average total system cost, and how to minimize the average total system cost subject to an acceptance designed reliability level; and (2) how to maximize the system reliability with imperfect coverage, including common-cause failures. Application and numerical examples illustrate the results.< >", "num_citations": "45\n", "authors": ["449"]}
{"title": "A Generalized Block Replacement Policy for a-Out-of-System With Respect to Threshold Number of Failed Components and Risk Costs\n", "abstract": " We develop a generalized block replacement model for a  k -out-of- n  system and determine the optimum policies of both the threshold level for the number of failed components to prevent system failures and the maintenance cycle that minimizes the expected total system cost. To overcome the existing block replacement policies' drawbacks, i.e., it is rather wasteful if a preventive replacement happens just after a failure replacement, in our developed policy, a replacement service for a failure is provided when there are a threshold number of failed components occurring. We also take into consideration the downtime period of each failed component using the order statistics for lifetime and age distributions for  k -out-of- n  systems. Several numerical examples are discussed to demonstrate the applicability of the methodology derived in this paper.", "num_citations": "41\n", "authors": ["449"]}
{"title": "A software cost model with warranty cost, error removal times and risk costs\n", "abstract": " In this paper, a software cost model with a warranty period and cost, a cost to remove each error detected in the software and a risk cost due to software failure is developed. A software reliability model based on the nonhomogeneous Poisson process is used. The optimal release policies which minimize the expected total software costs are presented. A software tool is also developed using Excel and Visual Basic that facilitates the task of determining the optimal software release time. Numerical examples are provided to illustrate the results.", "num_citations": "40\n", "authors": ["449"]}
{"title": "Reliability evaluation of systems with degradation and random shocks\n", "abstract": " This paper introduces a proposed model to evaluate the reliability of multi-component degradation systems suffering two kinds of competing failure causes: internal degradation process and damage from external random shocks. The internal degradation is expressed as a random process with respect to working time, and a geometric process is employed to describe cumulative damage caused by external random shocks. In our proposed model, the system is assumed to be failed when internal degradation or cumulative damage from random shocks exceed random life thresholds. The reliability expression is derived when the random life threshold and degradation process are considered to follow a Weibull distribution. A studied case of series-parallel system is presented to illustrate the proposed model, and a numerical algorithm is provided to simplify the calculating process based on normal approximation and\u00a0\u2026", "num_citations": "39\n", "authors": ["449"]}
{"title": "Reliability and MTTF prediction of K-out-of-n complex systems with components subjected to multiple stages of degradation\n", "abstract": " In some environments the components may not fail fully but can degrade, and there may be multiple stages of degradation. In such cases, the efficiency of the system may decrease. In this study we present a model for predicting the reliability of k-out-of-n: G systems assuming that components are subjected to several stages of degradation as well as catastrophic failures. In the analysis we consider the state-dependent transition rates for the catastrophic failures and degradation processes. We also present the expressions to determine the reliability and mean time to failure (MTTF) of the k-out-of-n systems. Reliability and M TTF expressions for a special case of the model without catastrophic failures are also presented. Several numerical examples are given to illustrate the results.", "num_citations": "39\n", "authors": ["449"]}
{"title": "On the optimal design of k-out-of-n: G subsystems\n", "abstract": " For a k-out-of-n:G subsystem, the mathematical determination of the most economical number of components in the subsystem is sought. Optimal values of k (for fixed n) and n (for fixed k), which minimize the mean total cost of k-out-of-n:G subsystems, are given. A numerical example illustrates the results.< >", "num_citations": "39\n", "authors": ["449"]}
{"title": "Software reliability models for critical applications\n", "abstract": " This report presents the results of the first phase of the ongoing EG G Idaho, Inc. Software Reliability Research Program. The program is studying the existing software reliability models and proposes a state-of-the-art software reliability model that is relevant to the nuclear reactor control environment. This report consists of three parts:(1) summaries of the literature review of existing software reliability and fault tolerant software reliability models and their related issues,(2) proposed technique for software reliability enhancement, and (3) general discussion and future research. The development of this proposed state-of-the-art software reliability model will be performed in the second place. 407 refs., 4 figs., 2 tabs.", "num_citations": "39\n", "authors": ["449"]}
{"title": "A multi-release software reliability modeling for open source software incorporating dependent fault detection process\n", "abstract": " The increasing dependence of our modern society on software systems has driven the development of software products become even more competitive and time-consuming. Single release software product no longer meets the increasing market requirements. Thereby it is important to release multiple version software products in order to add new features in the next release and fix remaining faults from previous release. In this paper, we develop a multi-release software reliability model with consideration of the remaining software faults from previous release and the new introduced-faults (from newly added features). Additionally, dependent fault detection process is taken into account in this research. In particular, the detection of a new fault for developing the next release depends on the detection of the remaining faults from previous release and the detection of the new introduced-faults. The proposed\u00a0\u2026", "num_citations": "38\n", "authors": ["449"]}
{"title": "Exploratory analysis of environmental factors for enhancing the software reliability assessment\n", "abstract": " Today software development is no longer an isolated work of a single programmer. Large systems are usually developed in a multi-language environment and run simultaneously on various platforms. Software development is a very complex process involving various factors. In this paper 32 environmental factors are defined and a survey was launched to investigate the impact of these factors on software reliability assessment. We extend our study by combining the original factors to reduce the dimension of the factor space; examine the impact of the environmental factors on the software reliability assessment improvement; investigate whether people have different opinions on ranking the environmental factors depending on their years of experience; and test the association between software reliability assessment improvement and participant's background information. Statistical analysis methodologies including\u00a0\u2026", "num_citations": "38\n", "authors": ["449"]}
{"title": "Entropy based software reliability analysis of multi-version open source software\n", "abstract": " The number of issues fixed in the current release of the software is one of the factors which decides the next release of the software. The source code files get changed during fixing of these issues. The uncertainty arises due to these changes is quantified using entropy based measures. We developed a Non-Homogeneous Poisson Process model for Open Source Software to understand the fixing of issues across releases. Based on this model, optimal release-updating using entropy and maximizing the active user's satisfaction level subject to fixing of issues up to a desired level, is investigated as well. The proposed models have been validated on five products of the Apache open source project. The optimal release time estimated from the proposed model is close to the observed release time at different active user's satisfaction levels. The proposed decision model can assist management to appropriately\u00a0\u2026", "num_citations": "37\n", "authors": ["449"]}
{"title": "A comparison analysis of environmental factors affecting software reliability\n", "abstract": " Fifteen years ago, Zhang and Pham launched a survey to investigate the impact of software development environmental factors (EFs) on software reliability assessment. Software development has gone through substantial changes during the past fifteen years. How different the environmental factors have become? This paper aims to revisit the 32 environmental factors and analyze their impact on software development and reliability based on a current survey to software development practitioners. The participants of this study come from 20 various organizations and they hold different positions and work on different application areas. Statistical analysis method, such as principle component analysis, relative weighted method, Tukey method, backward elimination, and correlation analysis are applied to analyze these factors. We compare the findings in the two studies and list the most significant factors based on the\u00a0\u2026", "num_citations": "36\n", "authors": ["449"]}
{"title": "A testing-coverage software reliability model with the uncertainty of operating environments\n", "abstract": " In this paper, we develop a new testing-coverage software reliability model with the uncertainty of operating environments. Examples are included to illustrate the goodness-of-fit of proposed model and several existing non-homogenous Poisson process (NHPP) models based on four sets of failure data collected from software applications. The explicit mean value function solution for the proposed model is presented. The results show that the proposed model fits significantly better than other existing NHPP models based on three criteria such as mean squared error, predictive power and predictive ratio risk.", "num_citations": "36\n", "authors": ["449"]}
{"title": "Imperfect preventive maintenance policies for two-process cumulative damage model of degradation and random shocks\n", "abstract": " In numerous applications, the system may fail due to multiple competing risks, especially degradation processes and random shocks. In this paper, we develop a two-process combination model for degraded system subject to cumulative effect from random shocks and degradation with two kinds of path function, including additive and multiplicative. Two numerical examples with sensitivity analysis for additive and multiplicative degradation path are discussed separately to illustrate this combination model. Based on the definition of the cumulative damage system, the imperfect preventive maintenance policy (N*, T*) with a common improvement factor is obtained to minimize the expected maintenance cost rate. The uncertainty of the reliability estimation and maintenance optimization is considered in the problem modeling. Also some extensions for this combination model are suggested for further research.", "num_citations": "36\n", "authors": ["449"]}
{"title": "Optimal designs of (k, n-k+ 1)-out-of-n: F systems (subject to 2 failure modes)\n", "abstract": " The problem of achieving optimal system size (n) for (k,n-k+1)-out-of-n systems, assuming that failure may take either of two forms, is studied. It is assumed that components are independently identically distributed (i.i.d.) and that the two kinds of system failures can have different costs. The optimal k or n that maximizes mean system-profit is determined, and the effect of system parameters on the optimal k or n is studied. It is shown that there does not exist a pair (k,n) maximizing the mean system-profit.< >", "num_citations": "36\n", "authors": ["449"]}
{"title": "Modeling the reliability of threshold weighted voting systems\n", "abstract": " In many applications, ranging from target detection to safety monitoring systems, we are interested in determining whether or not to accept a hypothesis based on the information available. In this paper we model the reliability of threshold weighted voting systems (WVS) with multi-failure-modes, where a general recursive reliability function of the WVS is presented. We also develop approximation formulas for calculating the reliability of WVS based on a large number of units. We also develop reliability functions of time-dependent threshold weighted voting systems, where each unit is a function of time. Finally, the optimal stopping time that minimizes the total cost of the systems subject to a reliability constraint is discussed.", "num_citations": "35\n", "authors": ["449"]}
{"title": "Commentary: Steady-state series-system availability\n", "abstract": " Computation of steady-state series-system availability depends on specific assumptions made about the nonfailed components during the system failure. Sherwin recently made the case that the steady-state availability in a series-system is not calculated using the product rule. This commentary offers a clarification by illustrating 2 cases of steady-state series-system availability that frequently arise in reliability engineering. The product rule is valid for steady-state series-system availability under the circumstances: nonfailed components (viz, electrical components) continue to age \"normally\" during the repair of the failed component. Therefore, in computing steady-state availability of series-systems, it is important for practitioners to determine whether the nonfailed components continue to age \"normally\" (case-1) or do not age (case-2). It is also shown that for n/spl ges/2, case-1 steady-state availability of the series\u00a0\u2026", "num_citations": "35\n", "authors": ["449"]}
{"title": "A new generalized systemability model\n", "abstract": " In this paper, we present a new mathematical function, called systemability, by introducing the uncertainty of the operating environments as a random variable for predicting the reliability of systems in the field. Numerical calculations for several system configurations such as parallel, series, and k-out-of-n, are given to illustrate the results.", "num_citations": "34\n", "authors": ["449"]}
{"title": "A Bayesian predictive software reliability model with pseudo-failures\n", "abstract": " In our previous paper (2000), a Bayesian software reliability model with stochastically decreasing hazard rate was presented. Within any given failure time interval, the hazard rate is a function of both total testing time as well as number of encountered encountered failures. In this paper, to improve the predictive performance of our previously proposed model, a pseudo-failure is inserted whenever there is a period of failure-free execution equals (1-/spl alpha/)th percentile of the predictive distribution for time until the next failure has passed. We apply the enhanced model with pseudo-failures inserted to actual software failure data and show it gives better results under the sum of square errors criteria compared to previous Bayesian models and other existing times between failures models.", "num_citations": "34\n", "authors": ["449"]}
{"title": "A generalized software reliability model with stochastic fault-detection rate\n", "abstract": " We propose a theoretic model of software reliability where the fault detection rate is a stochastic process. This formulation provides the flexibility in modeling the random environment effects in testing software data. We examine two particular cases: additive and multiplicative noise and provide explicit representations for the expected number of software failures. Examples are included to demonstrate the formulas for specific choices of time dependent total number of faults and distribution of noise.", "num_citations": "33\n", "authors": ["449"]}
{"title": "Altered quasi-renewal concepts for modeling renewable warranty costs with imperfect repairs\n", "abstract": " We introduce the concepts of altered quasi-renewal based on the ordinary quasi-renewal process. The first is called the altered quasi-renewal process with random parameters and the other is called the mixed quasi-renewal process with considerations of replacement and repair strategies. Based on the proposed altered- and mixed- quasi-renewal processes, we develop the warranty cost models and also derive various reliability measures. This study can help the policy makers to make appropriate decisions related to the warranty polices with the objective of downsizing manufacturer\u2019s warranty costs. Also, the results of this study using mixed and altered quasi-renewal processes can be found helpful for practitioners to analyze the system\u2019s warranty cost. An application for practical and numerical examples is discussed to demonstrate the applicability of the proposed concepts.", "num_citations": "33\n", "authors": ["449"]}
{"title": "A vtub-shaped hazard rate function with applications to system safety\n", "abstract": " In reliability engineering, the bathtub-shaped hazard rates play an important role in survival analysis and many other applications as well. For the bathtub-shaped, initially the hazard rate decreases from a relatively high value due to manufacturing defects or infant mortality to a relatively stable middle useful life value and then slowly increases with the onset of old age or wear out. In this paper, we present a new two-parameter lifetime distribution function, called the Loglog distribution, with Vtub-shaped hazard rate function. We illustrate the usefulness of the new Vtub-shaped hazard rate function by evaluating the reliability of several helicopter parts based on the data obtained in the maintenance malfunction information reporting system database collected from October 1995 to September 1999. We develop the S-Plus add-in software tool, called Reliability and Safety Assessment (RSA), to calculate reliability measures include mean time to failure, mean residual function, and confidence Intervals of the two helicopter critical parts. We use the mean squared error to compare relative goodness of fit test of the distribution models include normal, lognormal, and Weibull within the two data sets. This research indicates that the result of the new Vtub-shaped hazard rate function is worth the extra function-complexity for a better relative fit. More application in broader validation of this conclusion is needed using other data sets for reliability modeling in a general industrial setting.", "num_citations": "33\n", "authors": ["449"]}
{"title": "Optimal design of systems with competing failure modes\n", "abstract": " Consider the problem of achieving optimal system size, n, and threshold value, k, for {k, n-k+1}-out-of-n and majority systems with competing failure modes. Components are s-independent and identically distributed, and the two system failure modes can have different costs. The authors determine optimal k, given n; optimal n, given k; and optimal k,n. Optimal implies minimum mean system cost for the {k, n-k+1}-out-of-n or majority system. The authors study the behavior of the optimal k as a function of failure probabilities, and of maximizing the reliability of majority systems. Numerical examples illustrate the results.< >", "num_citations": "33\n", "authors": ["449"]}
{"title": "A new criterion for model selection\n", "abstract": " Selecting the best model from a set of candidates for a given set of data is obviously not an easy task. In this paper, we propose a new criterion that takes into account a larger penalty when adding too many coefficients (or estimated parameters) in the model from too small a sample in the presence of too much noise, in addition to minimizing the sum of squares error. We discuss several real applications that illustrate the proposed criterion and compare its results to some existing criteria based on a simulated data set and some real datasets including advertising budget data, newly collected heart blood pressure health data sets and software failure data. View Full-Text", "num_citations": "32\n", "authors": ["449"]}
{"title": "A generalized fault-detection software reliability model subject to random operating environments\n", "abstract": " Many software reliability growth models (SRGMs) have been developed in the past three decades to estimate software reliability measures such as the number of remaining faults and software reliability. The underlying common assumption of many existing models is that the operating environment and the developing environment are the same. This is often not the case in practice because the operating environments are usually unknown due to the uncertainty of environments in the field. In this paper, we develop a generalized software reliability model incorporating the uncertainty of fault-detection rate per unit of time in the operating environments. A logistic fault-detection software reliability model is derived. Examples are included to illustrate the goodness of fit of the proposed model and existing nonhomogeneous Poisson process (NHPP) models based on a set of failure data. Three goodness-of-fit\u00a0\u2026", "num_citations": "31\n", "authors": ["449"]}
{"title": "A software reliability model with time-dependent fault detection and fault removal\n", "abstract": " The common assumption for most existingsoftware reliability growth models is that fault is independent and can be removed perfectly upon detection. However, it is often not true due to various factors including software complexity, programmer proficiency, organization hierarchy, etc. In this paper, we develop a software reliability model with considerations of fault-dependent detection, imperfect fault removal and the maximum number of faults software. The genetic algorithm (GA) method is applied to estimate the model parameters. Four goodness-of-fit criteria, such as mean-squared error, predictive-ratio risk, predictive power, and Akaike information criterion, are used to compare the proposed model and several existing software reliability models. Three datasets collected in industries are used to demonstrate the better fit of the proposed model than other existing software reliability models based on the\u00a0\u2026", "num_citations": "31\n", "authors": ["449"]}
{"title": "Loglog fault-detection rate and testing coverage software reliability models subject to random environments\n", "abstract": " Many software reliability growth models (SRGMs) have developed in the past three decades to quantify several reliability measures including the expected number of remaining faults and software reliability. The underlying common assumption of many existing models is that the operating environment and the developing environment are the same. In reality, this is often not the case because the operating environments are unknown due to the uncertainty of environments in the field. In this paper, we present two new software reliability models with considerations of the fault-detection rate based on a Loglog distribution and the testing coverage subject to the uncertainty of operating environments. Examples are included to illustrate the goodness-of-fit test of proposed models and several existing non-homogeneous Poisson process (NHPP) models based on a set of failure data collected from software\u00a0\u2026", "num_citations": "31\n", "authors": ["449"]}
{"title": "Recent studies in software reliability engineering\n", "abstract": " 16.1. 1 Software Reliability Concepts 16.1. 2 Software Life Cycle 16.2 Software Reliability Modeling 16.2. 1 A Generalized Non-homogeneous Poisson Process Model 16.2. 2 Application 1: The Real-time Control System 16.3 Generalized Models with Environmental Factors", "num_citations": "31\n", "authors": ["449"]}
{"title": "Optimal design of k-out-of-n redundant systems\n", "abstract": " A k-out-of-n system is one in which at least k out of n units are to function for the successful operation of the system. Examples of such systems are found in communication, multiprocessor and transportation systems. A k-out-of-n system is very general and includes the parallel, series, N-Modular Redundancy (NMR), and fail safe systems as special cases. In this paper, the optimization problems are formulated and solved for the minimum expected total cost of k-out-of-n systems. Some numerical examples are given to illustrate the results.", "num_citations": "31\n", "authors": ["449"]}
{"title": "A two-phase software reliability modeling involving with software fault dependency and imperfect fault removal\n", "abstract": " Most existing software reliability growth models (SGRMs) often assume software faults are mutually independent and the detected faults can be perfectly removed. However, those two assumptions are not realistic in practice since the dependent faults can also exist in the program. At the same time, it is unlikely to correct all the detected faults in the testing phase due to the limitation of testing resource, the skill and experience of the programmer, and multi-release consideration for software organization. This paper presents a new non-homogeneous Poisson process (NHPP) software reliability model with a pioneering idea by considering software fault dependency and imperfect fault removal. In order to clearly explain software fault dependency, some facts and examples are discussed in Section 1. Two types of software faults are defined, Type I (independent) fault and Type II (dependent) fault, according to fault\u00a0\u2026", "num_citations": "29\n", "authors": ["449"]}
{"title": "A testing-coverage software reliability model considering fault removal efficiency and error generation\n", "abstract": " In this paper, we propose a software reliability model that considers not only error generation but also fault removal efficiency combined with testing coverage information based on a nonhomogeneous Poisson process (NHPP). During the past four decades, many software reliability growth models (SRGMs) based on NHPP have been proposed to estimate the software reliability measures, most of which have the same following agreements: 1) it is a common phenomenon that during the testing phase, the fault detection rate always changes; 2) as a result of imperfect debugging, fault removal has been related to a fault re-introduction rate. But there are few SRGMs in the literature that differentiate between fault detection and fault removal, i.e. they seldom consider the imperfect fault removal efficiency. But in practical software developing process, fault removal efficiency cannot always be perfect, i.e. the failures detected might not be removed completely and the original faults might still exist and new faults might be introduced meanwhile, which is referred to as imperfect debugging phenomenon. In this study, a model aiming to incorporate fault introduction rate, fault removal efficiency and testing coverage into software reliability evaluation is developed, using testing coverage to express the fault detection rate and using fault removal efficiency to consider the fault repair. We compare the performance of the proposed model with several existing NHPP SRGMs using three sets of real failure data based on five criteria. The results exhibit that the model can give a better fitting and predictive performance.", "num_citations": "29\n", "authors": ["449"]}
{"title": "Systemability function to optimisation reliability in random environment\n", "abstract": " In this article, we present a new mathematical function, called systemability, by introducing the uncertainty of the operating environments as a random variable for predicting the reliability of systems in the field. Industrial manufacturers must deal with problems of how to predict reliability performance of systems during operating conditions in various applications from the customers' perspectives. The authors recently introduced a new model, called systemability, which appears very interesting and suitable for industry applications, because it separates the effects related to the machine components from the environmental factors. After a brief literature review, we discuss the use of systemability function to predict the reliability of the system in the field. In this article, the adequacy of systemability model that fit the observed real-world data of the two applications, in the automatic packaging machines for beer production\u00a0\u2026", "num_citations": "29\n", "authors": ["449"]}
{"title": "Fault-tolerant software systems: techniques and applications\n", "abstract": " Anthology of IEEE journal articles on the subject. Reprinting is tolerable except for the author photos. No index. Annotation copyright Book News, Inc. Portland, Or.", "num_citations": "29\n", "authors": ["449"]}
{"title": "On estimating the number of deaths related to Covid-19\n", "abstract": " In this paper, we discuss an explicit model function that can estimate the total number of deaths in the population, and particularly, estimate the cumulative number of deaths in the United States due to the current Covid-19 virus. We compare the modeling results to two related existing models based on a new criteria and several existing criteria for model selection. The results show the proposed model fits significantly better than the other two related models based on the U.S. Covid-19 death data. We observe that the errors of the fitted data and the predicted data points on the total number of deaths in the U.S. on the last available data point and the next coming day are less than 0.5% and 2.0%, respectively. The results show very encouraging predictability for the model. The new model predicts that the maximum total number of deaths will be approximately 62,100 across the United States due to the Covid-19 virus, and with a 95% confidence that the expected total death toll will be between 60,951 and 63,249 deaths based on the data until 22 April, 2020. If there is a significant change in the coming days due to various testing strategies, social-distancing policies, the reopening of community strategies, or a stay-home policy, the predicted death tolls will definitely change. Future work can be explored further to apply the proposed model to global Covid-19 death data and to other applications, including human population mortality, the spread of disease, and different topics such as movie reviews in recommender systems.", "num_citations": "28\n", "authors": ["449"]}
{"title": "A new warranty policy with failure times and warranty servicing times\n", "abstract": " We develop a new warranty policy with respect to the failure time and warranty servicing time, where those two variables are statistically correlated in bivariate distributions. Based on the developed approaches, we investigate the properties of the bivariate function, and obtain the number of warranty services in a warranty period using the field data. The warranty service includes the repair service, and replacement service. An approach of analyzing the warranty cost is proposed for a product in which the failure time and warranty servicing time are used simultaneously to determine the eligibility of a warranty claim. In the vast literature of the warranty modeling, the two-dimensional warranty is commonly consisting of the usage, and the age. In this study, we focus on the failure time and warranty servicing time as two factors under the warranty policy. Using the field data, the parameters are calculated, and the warranty\u00a0\u2026", "num_citations": "28\n", "authors": ["449"]}
{"title": "Safety and risk modeling and its applications\n", "abstract": " Today\u2019s products and safety critical-systems of most applications have become increasingly complex to build while the demand for safety and cost effective development continues. The interest in safety and risk modeling and assessment has been growing in many years to come. This book, consisting of 14 chapters, aims to present latest methods and techniques for quantifying the safety and risk with emphasis on safety, reliability and risk modeling and their applications in several areas including Aviation Systems and Security, Sensor Detection and Management Decision-Making, and Systems of Systems and Human Integration. The subjects covered include safety engineering, system maintenance, safety in design, failure mode analysis, risk concept and modeling, software safety, human safety, product safety in operating environments, human-hand safety modeling, sensor management, safety decisionmaking\u00a0\u2026", "num_citations": "28\n", "authors": ["449"]}
{"title": "A New Insight into k-out-of-n Warm Standby Model.\n", "abstract": " We provide a new insight into k-out-of-n warm standby model with identical components subject to exponential lifetime distributions. We show that all properties and computational procedures that are applicable for active redundancy are also applicable for the warm standby redundancy. In addition, we show that the reliability of both active and warm standby systems can be calculated using beta distribution. Similarly, the improvement in system reliability with an additional redundant component follows negative binomial (P\u00f3lya) distribution.", "num_citations": "28\n", "authors": ["449"]}
{"title": "A cost analysis of systems subject to random field environments and reliability\n", "abstract": " We present a generalized cost model subject to random field environments with considerations of cost to remove failures during testing and warranty periods, and penalty cost due to the system failures. We also determine the optimal release time policies that minimize the expected system cost. Many scientific contributions have been developed in software reliability modeling, while none has studied the manufacturing or industrial system reliability growth yet, considering also the differences between testing and operating environments. The application of the proposed model, in comparison to the nonhomogenous Poisson process Goel-Okumoto model, to an industrial application is discussed to illustrate how the proposed model can be used in practice.", "num_citations": "28\n", "authors": ["449"]}
{"title": "Warranty Cost Analysis for  Systems With 2-D Warranty\n", "abstract": " This paper presents new warranty cost models subject to two types of warranty periods (warranty and postwarranty periods), minimal repairs, and various types of warranty policies including free repair/replacement warranty and prorata warranty for  k -out-of- n  systems. Two-dimensional warranty is applied to investigate the warranty cost using repair times and failure times. We obtain the long-run expected cost functions per unit time with respect to both the warranty and postwarranty periods by considering the aspects of both the manufacturer and the customer. This paper considers a periodic preventive maintenance (PM) policy with both corrective maintenance and PM and also determines three decision variables including warranty period, repair time limit, and periodical maintenance cycles, which minimize the long-run expected cost. Numerical examples are discussed to demonstrate the applicability of the\u00a0\u2026", "num_citations": "27\n", "authors": ["449"]}
{"title": "A novel approach for optimal cost-effective design of complex repairable systems\n", "abstract": " Almost every engineering and manufacturing system consists of several subsystems, which are in general nonidentical and are subjected to stochastic failures and repairs. The system success logic can be represented using a combinatorial reliability model in terms of the states of subsystems, where as the success logic of each subsystem can be represented using a k-out-of- n structure. The long run cost associated with the downtime can be lowered by adding additional spares in each subsystem, which in turn can increase the operational and maintenance costs. Thus, it is desirable to find the optimal number of components in each subsystem that minimizes the overall cost associated with the system. The main contributions of this paper are the following: 1) formulation of an average cost function of complex repairable systems and 2) development of a new method to obtain tighter bounds for the optimal number of\u00a0\u2026", "num_citations": "27\n", "authors": ["449"]}
{"title": "A three-parameter fault-detection software reliability model with the uncertainty of operating environments\n", "abstract": " As requirements for system quality have increased, the need for high system reliability is also increasing. Software systems are extremely important, in terms of enhanced reliability and stability, for providing high quality services to customers. However, because of the complexity of software systems, software development can be time-consuming and expensive. Many statistical models have been developed in the past years to estimate software reliability. In this paper, we propose a new three-parameter fault-detection software reliability model with the uncertainty of operating environments. The explicit mean value function solution for the proposed model is presented. Examples are presented to illustrate the goodness-of-fit of the proposed model and several existing non-homogeneous Poisson process (NHPP) models based on three sets of failure data collected from software applications. The results show\u00a0\u2026", "num_citations": "26\n", "authors": ["449"]}
{"title": "Recent advances in reliability and quality in design\n", "abstract": " \" Recent Advances in Reliability and Quality in Design\" presents the latest theories and methods of reliability and quality, with emphasis on reliability and quality in design and modelling. Each chapter is written by active researchers and professionals with international reputations, providing material which bridges the gap between theory and practice to trigger new practices and research challenges. Postgraduates, researchers, and practitioners in reliability engineering, maintenance engineering, quality engineering, operations research, industrial and systems engineering, mechanical engineering, computer engineering, management, and statistics will find this book a state-of-the-art survey of reliability and quality in design and practices.", "num_citations": "26\n", "authors": ["449"]}
{"title": "Reliability analysis of tampered failure rate load-sharing k-out-of-n: G systems\n", "abstract": " Load-sharing systems have several practical applications. In load-sharing systems, the event of a component failure will result in a higher load, therefore inducing a higher failure rate, in each of the surviving components. This introduces failure dependency among the load-sharing components, which in turn increases the complexity in analyzing these systems. In this paper, we provide a closed-form analytical solution for the reliability of tampered failure rate (TFR) load-sharing k-out-of-n: G systems with identical components where all surviving components share the load equally. The closed-form analytical solution provided in this paper allows efficient computations of system reliability characteristics. The solution is applicable for a wide range of failure time distributions of the components. The efficiency of the proposed solution is demonstrated through several numerical examples.", "num_citations": "26\n", "authors": ["449"]}
{"title": "System reliability concepts\n", "abstract": " The analysis of the reliability of a system must be based on precisely defined concepts. Since it is readily accepted that a population of supposedly identical systems, operating under similar conditions, fall at different points in time, then a failure phenomenon can only be described in probabilistic terms. Thus, the fundamental definitions of reliability must depend on concepts from probability theory. This chapter describes the concepts of system reliability engineering. These concepts provide the basis for quantifying the reliability of a system. They allow precise comparisons between systems or provide a logical basis for improvement in a failure rate. Various examples reinforce the definitions as presented in Section 2.1. Section 2.2 examines common distribution functions useful in reliability engineering. Several distribution models are discussed and the resulting hazard functions are derived. Section 2.3 describes a\u00a0\u2026", "num_citations": "26\n", "authors": ["449"]}
{"title": "A generalized software reliability growth model with consideration of the uncertainty of operating environments\n", "abstract": " This paper proposes a generalized model to cover imperfect debugging and the uncertainty of the operating environment and its effect on fault detection rate into software reliability evaluation based on a non-homogeneous Poisson process (NHPP). Many NHPP software reliability growth models (SRGMs) have been developed to estimate the software reliability measures over the past 40 years, but most of these models assume that the operating environment is the same as the testing environment. However, in fact, due to the unpredictability of the uncertain factors in the operating environments for the software, they may considerably influence the software's reliability in an unpredictable way. So when a software system works in a field environment, its reliability is usually different from the original reliability prediction in the testing phase of the software development process, also from all its similar applications in\u00a0\u2026", "num_citations": "25\n", "authors": ["449"]}
{"title": "Biomedical and environmental applications of magnetic nanoparticles\n", "abstract": " This paper presents an overview of syntheses and applications of magnetic nanoparticles (MNPs) at the Institute of Materials Science, Vietnam Academy of Science and Technology. Three families of oxide MNPs, magnetite, manganite and spinel ferrite materials, were prepared in various ways: coprecipitation, sol\u2013gel and high energy mechanical milling. Basic properties of MNPs were characterized by Vibrating Sample Magnetometer (VSM) and Physical Properties Measurement Systems (PPMS). As for biomedical application, the aim was to design a novel multifunctional, nanosized magnetofluorescent water-dispersible Fe                3                O                4-curcumin conjugate, and its ability to label, target and treat tumor cells was described. The conjugate possesses a magnetic nano Fe                3                O                4 core, chitosan (CS) or Oleic acid (OL) as an outer shell and entrapped curcumin (Cur), serving the\u00a0\u2026", "num_citations": "24\n", "authors": ["449"]}
{"title": "An NHPP software reliability model with S-shaped growth curve subject to random operating environments and optimal release time\n", "abstract": " The failure of a computer system because of a software failure can lead to tremendous losses to society; therefore, software reliability is a critical issue in software development. As software has become more prevalent, software reliability has also become a major concern in software development. We need to predict the fluctuations in software reliability and reduce the cost of software testing: therefore, a software development process that considers the release time, cost, reliability, and risk is indispensable. We thus need to develop a model to accurately predict the defects in new software products. In this paper, we propose a new non-homogeneous Poisson process (NHPP) software reliability model, with S-shaped growth curve for use during the software development process, and relate it to a fault detection rate function when considering random operating environments. An explicit mean value function solution for the proposed model is presented. Examples are provided to illustrate the goodness-of-fit of the proposed model, along with several existing NHPP models that are based on two sets of failure data collected from software applications. The results show that the proposed model fits the data more closely than other existing NHPP models to a significant extent. Finally, we propose a model to determine optimal release policies, in which the total software system cost is minimized depending on the given environment. View Full-Text", "num_citations": "23\n", "authors": ["449"]}
{"title": "On the estimation of reliability of k-out-of-n systems\n", "abstract": " This paper presents a uniformly minimum variance unbiased estimator and the maximum likelihood estimates of reliability of k-out-of-n systems which are composed of n independent and identically distributed components with exponential lifetimes. The system is operational if and only if at least k of out the n components are operational. The reliability estimation results for the failure of uncensored cases (where there are m units put on test which is terminated when all the units have failed) and censored cases (when test termination is done upon the failure of r pre-assigned units) will be discussed. An application to illustrate the reliability estimation prediction for the power usages of computer system with quad-core, 8 GB of Ram, and a GeForce 9800GX-2 graphics card to perform various complex applications is discussed.", "num_citations": "23\n", "authors": ["449"]}
{"title": "Optimal system size for k-out-of-n systems with competing failure modes\n", "abstract": " We treat the problem of achieving optimal system size for k-out-of-n systems with competing failure modes. Assume that components are independent and identical distributed and subject to two types of failure: failure in open mode and failure in closed mode. The system works if and only if fewer than k components fail in closed mode and fewer than n \u2212 k + 1 components fail in open mode. In this paper we determine the optimal system size n which maximizes the system reliability. We also study the effect of the system parameters on the optimal system size n. A numerical example is given to illustrate the results.", "num_citations": "23\n", "authors": ["449"]}
{"title": "Emerging trends, techniques and open issues of containerization: a review\n", "abstract": " Containerization is revolutionizing the way that many industries operate, provisioning major impact to modern computing technologies because it is extra lightweight, highly portable, energy, resource and storage efficient, cost-effective, performance efficient, and extremely quick during boot up. These often facilitate efficient load balancing, low-level system maintenance, server consolidation (for efficient energy and resource utilization) and replication of instances over geographical locations for better fault tolerance to escalate application reliability. However, some recent literature have addressed various challenges (such as complex networking, persistent storage facilities, cross data centers and multicloud supports, security issues, and lack of available, capable container management APIs, etc.) regarding successful container adoption in industries, which might have resulted in a seemingly meager increase in\u00a0\u2026", "num_citations": "22\n", "authors": ["449"]}
{"title": "NHPP software reliability model with inflection factor of the fault detection rate considering the uncertainty of software operating environments and predictive analysis\n", "abstract": " The non-homogeneous Poisson process (NHPP) software has a crucial role in computer systems. Furthermore, the software is used in various environments. It was developed and tested in a controlled environment, while real-world operating environments may be different. Accordingly, the uncertainty of the operating environment must be considered. Moreover, predicting software failures is commonly an important part of study, not only for software developers, but also for companies and research institutes. Software reliability model can measure and predict the number of software failures, software failure intervals, software reliability, and failure rates. In this paper, we propose a new model with an inflection factor of the fault detection rate function, considering the uncertainty of operating environments and analyzing how the predicted value of the proposed new model is different than the other models. We compare the proposed model with several existing NHPP software reliability models using real software failure datasets based on ten criteria. The results show that the proposed new model has significantly better goodness-of-fit and predictability than the other models. View Full-Text", "num_citations": "22\n", "authors": ["449"]}
{"title": "Age replacement policy in a random environment using systemability\n", "abstract": " Preventive maintenance is a group of maintenance policies based on preventive actions in order to predate the failure of a component or a system. Usually, these policies are designed using a series of data related to the studied units. All policies do not consider the effect of the environment where the components or systems operate. In this article, one of the most used policies, the age replacement policy, is also discussed taking into consideration the environmental effects using an innovative concept, introduced by Pham, called systemability. Several numerical examples are carried out in order to illustrate the aim of this work. The importance of environmental factors is also demonstrated thanks to the application to a real case.", "num_citations": "22\n", "authors": ["449"]}
{"title": "Reliability modeling, analysis and optimization\n", "abstract": " As our modern information-age society grows in complexity both in terms of embedded systems and applications, the problems and challenges in reliability become ever more complex. Bringing together many of the leading experts in the field, this volume presents a broad picture of current research on system modeling and optimization in reliability and its applications. The book comprises twenty-three chapters organized into four parts: Reliability Modeling, Software Quality Engineering, Software Reliability, and Maintenance and Inspection Policies. These sections cover a wide range of important topics, including system reliability modeling, optimization, software reliability and quality, maintenance theory and inspection, reliability failure analysis, sampling plans and schemes, software development processes and improvement, stochastic process modeling, statistical distributions and analysis, fault-tolerant performance, software measurements and cost effectiveness, queueing theory and applications, system availability, reliability of repairable systems, testing sampling inspection, software capability maturity model, accelerated life modeling, statistical control, and HALT testing.", "num_citations": "22\n", "authors": ["449"]}
{"title": "Predicting operational software availability and its applications to telecommunication systems\n", "abstract": " It is essential to predict customer-perceived software availability during software development and determine when to release the software to maintain a balance among time-to-market, development cost and software quality. This paper presents methods and procedures to predict software failure rates from a user perspective in system test phases and to reverse-engineer in order to estimate software release time for given availability targets. Software reliability analysis is conducted based on non-homogenous Poisson process models. Software system test data of current release are used to estimate the number of residual faults by the end of system tests and data of previous releases or similar products (including system test data, post-system test data and field failure data) provide a means to predict a user-perceived average failure rate of a fault. Software system availability can be predicted from these estimates\u00a0\u2026", "num_citations": "22\n", "authors": ["449"]}
{"title": "Reliability analysis for dynamic configurations of systems with three failure modes\n", "abstract": " Analytical models for computing the reliability of dynamic configurations of systems, such as majority and k-out-of-n, assuming that units and systems are subject to three types of failures: stuck-at-0, stuck-at-1, and stuck-at-x are presented in this paper. Formulas for determining the optimal design policies that maximize the reliability of dynamic k-out-of-n configurations subject to three types of failures are defined. The comparisons of the reliability modeling functions are also obtained. The optimum system size and threshold value k that minimize the expected cost of dynamic k-out-of-n configurations are also determined.", "num_citations": "22\n", "authors": ["449"]}
{"title": "When to stop testing multi upgradations of software based on cost criteria\n", "abstract": " Software testing is an important phase of the software development life cycle to achieve highly reliable software. Due to the time and resource limitation during the testing phase, firms do not attempt to deliver a complete and perfect product in one development cycle. They plan multi upgradations of software by adding new functionalities. Many models have been developed in the past which discuss about when to stop testing and when to release the software to the users. But they have been limited to the study of single version only. In the present framework, we describe a unified approach to address an important issue of when to stop testing the multi-upgradation of software, which is a complex process. The total debugging cost for each upgradation includes the cost of debugging in the warranty period along with the testing cost. It is assumed that the software is supported till the warranty period is over. In the\u00a0\u2026", "num_citations": "20\n", "authors": ["449"]}
{"title": "Dynamic optimal control model for profit maximization of software product under the influence of promotional effort\n", "abstract": " In this paper, a decision model is presented for the sales of software product to determine the profit and marketing policy under the influence of promotional efforts. The paper focuses on dependence of the optimal profit on the promotional efforts when there is diffusion effect of demand on the sales function. An elaborate optimization policy considering the dynamic nature of production cost function is proposed and numerical example is illustrated. The paper also studies the behavior of the future profit and its impact on profit maximization model by considering the constant price during the entire planning horizon. The experimental results greatly help us to identify the contributions of each selected parameter and its weight. Some conclusions, limitations of this study and future direction are also discussed.", "num_citations": "20\n", "authors": ["449"]}
{"title": "Reliability of systems with multiple failure modes\n", "abstract": " A component is subject to failure in either open or closed modes. Networks of relays, fuse systems for warheads, diode circuits, fluid flow valves, etc. are a few examples of such components. Redundancy can be used to enhance the reliability of a system without any change in the reliability of the individual components that form the system. However, in a two-failure mode problem, redundancy may either increase or decrease the system\u2019s reliability. For example, a network consisting of n relays in series has the property that an open-circuit failure of any one of the relays would cause an open-mode failure of the system and a closed-mode failure of the system.(The designations \u201cclosed mode\u201d and \u201cshort mode\u201d both appear in this chapter, and we will use the two terms interchangeably.) On the other hand, if the n relays were arranged in parallel, a closed-mode failure of any one relay would cause a system closed\u00a0\u2026", "num_citations": "20\n", "authors": ["449"]}
{"title": "Reliability analysis of digital communication systems with imperfect voters\n", "abstract": " In this paper, we study the problem of achieving optimum redundancy for the digital communication systems, assuming that failure of units (channels and voter) may take either one of two forms. The reliability of the system is defined as the probability of making the correct decision at the output of the system when a digit enters the system. We derive a general expression for reliability of the system and also discuss the optimization issues that maximize system reliability.", "num_citations": "20\n", "authors": ["449"]}
{"title": "A software reliability model incorporating martingale process with gamma-distributed environmental factors\n", "abstract": " As the increasing application of software system in various industry, software reliability gains more attention from the researchers and practitioners in the past few decades. The goal of such an expanding application of software system is to continuously bring convenience and functionality in everyday life. Lots of environmental factors defined by many studies may have positive/negative impact on software reliability during the development process (Zhu et al. in J Syst Softw 109:150\u2013160, 2015; Clarke and O\u2019Connor in Inf Softw Technol 54(5):433\u2013447, 2012; Zhu and Pham in J Syst Softw 1\u201318, 2017b). However, most existing software reliability models have not incorporated these environmental factors in the model consideration. In this paper, we propose a theoretic software reliability model incorporating the fault detection process is a stochastic process due to the randomness caused by the environmental\u00a0\u2026", "num_citations": "19\n", "authors": ["449"]}
{"title": "Reliability analysis of k-out-of-n systems with partially repairable multi-state components\n", "abstract": " In some environments the components might not fail fully, but can lead to degradation and the efficiency of the system may decreases. However, the degraded components can be restored back through a proper repair mechanism. In this paper, we present a model to perform reliability analysis of k-out-of-n systems assuming that components are subjected to three states such as good, degraded, and catastrophic failure. We also present expressions for reliability and mean time to failure (MTTF) of k-out-of-n systems. Simple reliability and MTTF expressions for the triple-modular redundant (TMR) system, and numerical examples are also presented in this study.", "num_citations": "19\n", "authors": ["449"]}
{"title": "Reliability modeling of multi\u2010state degraded repairable systems and its applications to automotive systems\n", "abstract": " In this paper, we presented a continuous\u2010time Markov process\u2010based model for evaluating time\u2010dependent reliability indices of multi\u2010state degraded systems, particularly for some automotive subsystems and components subject to minimal repairs and negative repair effects. The minimal repair policy, which restores the system back to an \u201cas bad as old\u201d functioning state just before failure, is widely used for automotive systems repair because of its low cost of maintenance. The current study distinguishes with others that the negative repair effects, such as unpredictable human error during repair work and negative effects caused by propagated failures, are considered in the model. The negative repair effects may transfer the system to a degraded operational state that is worse than before due to an imperfect repair. Additionally, a special condition that a system under repair may be directly transferred to a complete\u00a0\u2026", "num_citations": "18\n", "authors": ["449"]}
{"title": "Microgrid topology for different applications in Vietnam\n", "abstract": " This paper proposes a common microgrid including distributed energy resources (DER) like diesel generation, photovoltaic cells (PV cells), wind turbine or other renewable energy sources (RES), an energy storage system and both ac and dc loads. This micro grid topology is applicable to various areas such as city buildings, a factory, a household, a small village or a rural farm. Case study for several areas will also be presented in this paper. For different cases, depending on the strength of the utility grid, the number of available DER and user convenience, ac, dc or hybrid microgrid can be applied. To improve the reliability of the microgrid, an energy storage system made of batteries connected in series is established to support the bus voltage immediately when the microgrid is disconnected from the main grid and when in stand-alone mode. This energy storage system can be charged from the main bus voltage\u00a0\u2026", "num_citations": "18\n", "authors": ["449"]}
{"title": "Reliability models for systems with internal and external redundancy\n", "abstract": " As the rapid development of computer-based systems and networks, more and more applications require non-stop, highly reliable services. To achieve high service availability and reliability, traditional redundancy inside the system may not be sufficient for these applications. External redundancy and recovery strategies are critical to meet the high availability requirements, reduce failure impact and improve survivability. New network infrastructure and protocols made these redundancy structures and recovery strategies feasible. This paper presents an integrated Markov reliability and availability model to estimate and compare the downtime of the different recovery strategies. The model provides quantitative evaluations of reliability for the different architecture options. This can be further combined with other performance and capacity analysis to support architecture design and other decision making.", "num_citations": "18\n", "authors": ["449"]}
{"title": "Reliability of decision making in human-organizations\n", "abstract": " In a human-organization system with n officers, a decision has to be made on whether to accept or to reject an innovation-oriented proposal that can either be good or bad. Each officer will review a given set of available information regarding the proposal in question and will thereafter return his or her decision. In the common formulation of this problem, the organization will accept the proposal if at least k officers decide for the project, where k is a prespecified threshold value. Hereby, each officer is subject to different errors and therefore, the final decision may be faulty as well. The reliability of such a system is the probability for the organization to make the right decision. In this paper, by imposing weights for each officer, generalized problem formulations are introduced and formulas for the corresponding reliability are derived.", "num_citations": "18\n", "authors": ["449"]}
{"title": "A Fuzzy Optimization Framework for COTS Products Selection of Modular Software Systems.\n", "abstract": " In this paper, we discuss a decision-making situation under uncertainty related to software creation through Commercial-Off-The-Shelf (COTS) based modules. We propose a bi-objective fuzzy optimization model of the COTS selection problem. The proposed optimization model simultaneously maximize the weighted quality and minimize the total cost of a modular software system subject to many limitations including maximum threshold on delivery time of the software and incompatibility among COTS products. The coefficients of both the objective functions and the delivery time constraints are characterized by fuzzy restrictions with triangular possibility distributions. Analytical hierarchy process technique is used to assign weights to the various modules according to their access frequencies and also using preferences of the software developer regarding technical specifications of the software system. Using\u00a0\u2026", "num_citations": "17\n", "authors": ["449"]}
{"title": "Analyzing the effects of air pollution and mortality by generalized additive models with robust principal components\n", "abstract": " In this paper, we propose generalized additive models (GAMs) based on the robust principal component analysis (PCA) methods, to quantify the association between daily mortality and air pollutant concentrations, especially PM10, CO, NO2, SO2 and O3, for confounding effects of long-term time trend, seasonality, weekday, and meteorological factors. The two PCA methods that will be applied into the GAM are: one is classic PCA (CPCA) and the other is robust PCA (RPCA) with minimum covariance determinant, called CPCA\u2013GAM and RPCA\u2013GAM, respectively. Comparing the analyses between GAM, CPCA\u2013GAM, and RPCA\u2013GAM, we can reach to the conclusions as follows: (1) results from CPCA\u2013GAM and RPCA\u2013GAM are consistent with each other; (2) RPCA is much more effective tool to detect outliners than CPCA; and (3) because PCA eliminates the collinearity between covariates, the coefficients\u00a0\u2026", "num_citations": "17\n", "authors": ["449"]}
{"title": "Master defect record retrieval using network-based feature association\n", "abstract": " As electronic records (e.g., medical records and technical defect records) accumulate, the retrieval of a record from a past instance with the same or similar circumstances, has become extremely valuable. This is because a past record may contain the correct diagnosis or correct solution to the current circumstance. We refer to the two records of the same or similar circumstances as  master  and  duplicate  records. Current record retrieval techniques are lacking when applied to this special master defect record retrieval problem. In this study, we propose a new paradigm for master defect record retrieval using network-based feature association (NBFA). We train the master record retrieval process by constructing feature associations to limit the search space. The retrieval paradigm was employed and tested on a real-world large-scale defect record database from a telecommunications company. The empirical results\u00a0\u2026", "num_citations": "17\n", "authors": ["449"]}
{"title": "Dependent competing-risk degradation systems\n", "abstract": " The failure of many units or systems, such as components, parts, machines, can be generally classified into two kinds of failure modes: one is catastrophic failure in which units break down by some sudden external shocks; the other is degradation failure in which units fail to function due to the physical deterioration. There are a great number of such cases for this kind of competing failure modes in our real life.", "num_citations": "16\n", "authors": ["449"]}
{"title": "Data mining methods and applications\n", "abstract": " In this chapter, we provide a review of the knowledge discovery process, including data handling, data mining methods and software, and current research activities. The introduction defines and provides a general background on data mining knowledge discovery in databases. In particular, the potential for data mining to improve manufacturing processes in industry is discussed. This is followed by an outline of the entire process of knowledge discovery in databases in the second part of the chapter.The third part presents data handling issues, including databases and preparation of the data for analysis. Although these issues are generally considered uninteresting to modelers, the largest portion of the knowledge discovery process is spent handling data. It is also of great importance since the resulting models can only be as good as the data on which they are based.", "num_citations": "16\n", "authors": ["449"]}
{"title": "Optimal release time and sensitivity analysis using a new NHPP software reliability model with probability of fault removal subject to operating environments\n", "abstract": " With the latest technological developments, the software industry is at the center of the fourth industrial revolution. In today\u2019s complex and rapidly changing environment, where software applications must be developed quickly and easily, software must be focused on rapidly changing information technology. The basic goal of software engineering is to produce high-quality software at low cost. However, because of the complexity of software systems, software development can be time consuming and expensive. Software reliability models (SRMs) are used to estimate and predict the reliability, number of remaining faults, failure intensity, total and development cost, etc., of software. Additionally, it is very important to decide when, how, and at what cost to release the software to users. In this study, we propose a new nonhomogeneous Poisson process (NHPP) SRM with a fault detection rate function affected by the probability of fault removal on failure subject to operating environments and discuss the optimal release time and software reliability with the new NHPP SRM. The example results show a good fit to the proposed model, and we propose an optimal release time for a given change in the proposed model. View Full-Text", "num_citations": "15\n", "authors": ["449"]}
{"title": "Reliability analysis of the CNC system based on field failure data in operating environments\n", "abstract": " Reliability is a measure of how well a product will perform under a certain set of conditions for a specified amount of time especially in the field environments. In this paper, a reliability study of a computer numerical control (CNC) system is described. For this analysis, field failure data from a shop manufacturing factory collected over the course of a year on approximately 20 CNC machine tools during their operating period were analyzed. Based on the field failure data, the two\u2010parameter exponential distribution was found to be applicable to describe the time between failures of the CNC system from among many distributions including Weibull, gamma, two\u2010parameter exponential, normal, and logistic using the chi\u2010squared test. In this paper, we discuss the reliability estimation of the CNC system based on the collected field failure data from a manufacturing factory using the maximum likelihood estimate (MLE) and\u00a0\u2026", "num_citations": "15\n", "authors": ["449"]}
{"title": "Modeling US mortality and risk-cost optimization on life expectancy\n", "abstract": " Human life expectancy has risen in most developed countries over several decades, causing observed demographic shifts. Many researchers have developed models to determine the expectancy of life at birth. Yet due to the complexity of the real-world mortality data, and recent global economy impacts, there is a great demand to search for new models to accurately predict the life expectancy, especially in the United States. In this paper, we focus on the analysis of mortality rate in the United States over a period of six decades (data from 1946 to 2005) with considerations of the six most common distribution functions in the mortality area. These functions are: Gompertz, Gompertz-Makeham, logistic, log logistic, loglog, and Weibull. Given complex mortality data, we develop models including algorithms to compute the mortality measures such as mortality rate, and then select the best function for predicting the life\u00a0\u2026", "num_citations": "15\n", "authors": ["449"]}
{"title": "A generalized logistic software reliability growth model\n", "abstract": " This paper presents a generalized logistic software reliability growth model that integrates time-dependent fault detection rate and imperfect removing rate per fault. We also derive a time-dependent logistic growth model and compare descriptive and predictive ability of a set of \u201cclassical\u201d NHPP reliability models with the one we developed based on a software failure data set. The results show that inclusion of both time-dependent imperfect removing and fault-detection rates into a logistic growth function may be worth the extra model complexity and the increased number of parameters required for a better relative fit based on several selection criteria.", "num_citations": "15\n", "authors": ["449"]}
{"title": "Optimal design of parallel-series systems with competing failure modes\n", "abstract": " The problem of achieving optimal subsystem size for parallel-series systems, assuming that components and systems are subject to two types of failure modes, open and short, is addressed. Components are statistically independent and identically distributed; the costs of the two modes of system failures need not be the same. The number of subsystems (m), for a given number of components in each subsystem (n), is optimized so that: the mean system profit is maximized showing how m depends on system parameters and showing that there does not exist a pair (m,n) maximizing the mean system-profit, and the mean system cost is minimized. The results are illustrated by several numerical examples.< >", "num_citations": "15\n", "authors": ["449"]}
{"title": "Environmental factors analysis and comparison affecting software reliability in development of multi-release software\n", "abstract": " As the application of the principles of agile and lean software development, software multiple release becomes very common in the modern society. Short iteration and short release cycle have driven the significant changes of the development process of multi-release software product, compared with single release software product. Thus, it is time to conduct a new study investigating the impact level of environmental factors on affecting software reliability in the development of multi-release software to provide a sound and concise guidance to software practitioners and researchers. Statistical learning methods, like principle component analysis, stepwise backward elimination, lasso regression, multiple linear regression, and Tukey method, are applied in this study. Comparisons regarding significant environmental factors during the whole development process, principle components, significant environmental factors\u00a0\u2026", "num_citations": "14\n", "authors": ["449"]}
{"title": "Reliability and optimal maintenance\n", "abstract": " This book aims to present a state-of-the-art survey of theories and methods of reliability, maintenance, and warranty with emphasis on multi-unit systems, and to reflect current hot topics: imperfect maintenance, economic dependence, opportunistic maintenance, quasi-renewal processes, warranty with maintenance and economic dependency, and software testing and maintenance. This book is distinct from others because it consists mainly of research work published on technical journals and conferences in recent years by us and our co-authors. Maintenance involves preventive and unplanned actions carried out to retain a system at or restore it to an acceptable operating condition. Optimal maintenance policies aim to provide optimum system reliability and safety performance at the lowest possible maintenance costs. Proper maintenance techniques have been emphasized in recent years due to increased safety\u00a0\u2026", "num_citations": "14\n", "authors": ["449"]}
{"title": "Software reliability modeling\n", "abstract": " For software qualification, it is highly desirable to have an estimate of the remaining errors in a software system. It is difficult to determine such an important finding without knowing what the initial errors are. Research activities in software reliability engineering have been studied over the past 30 years and many statistical models and various techniques have been developed for estimating and predicting reliability of software and numbers of residual errors in software. From historical data on programming errors, there are likely to be about 8 errors per 1000 program statements after the unit test. This, of course, is just an average and does not take into account any tests on the program.There are two main types of software reliability models: the deterministic and the probabilistic. The deterministic model is used to study the number of distinct operators and operands in a program as well as the number of errors and the\u00a0\u2026", "num_citations": "14\n", "authors": ["449"]}
{"title": "A generalized surveillance model with applications to systems safety\n", "abstract": " This paper presents a generalized surveillance model for predicting the performance of complex systems consisting of many subsystems (units). These subsystems are frequently inspected to keep the entire system operating satisfactorily. Systems of this type are encountered in many areas, including nuclear power plant, national defense system, transportation stations, medical monitoring control rooms, etc. The particular application that motivated a development of this model is an FAA project, where we were asked to develop a surveillance model to better understand both the inspection process and the repair station itself and to provide information that can be used to assist inspectors in scheduling and prioritizing their visits to the stations. A distinguishing feature of this surveillance model is that it combines two mutually dependent stochastic processes. One is a two-stage stochastic process for the occurrence of\u00a0\u2026", "num_citations": "14\n", "authors": ["449"]}
{"title": "On the optimal design of N-version software systems subject to constraints\n", "abstract": " Software size and complexity continue to grow and find new applications, such as safety assurance systems for nuclear power generators, electronic transfer systems for banking, and manned spacecraft. To achieve ultrareliability in computing, it is necessary to adopt the strategy of defensive programming based on redundancy. Redundant versions, however, require additional resources, e.g., additional costs in terms of programming effort, time needed for design and testing, and hardware requirements. This article addresses the optimization issue for the cost of N-version programming (NVP) and majority systems. The optimization problems are formulated and solved for the minimum expected cost of NVP and majority systems subject to the desired reliability level. The problems of maximizing the reliability of the NVP and majority systems subject to a constraint on expected system cost are also obtained. Numerical\u00a0\u2026", "num_citations": "14\n", "authors": ["449"]}
{"title": "A new mathematical logistic model and its applications\n", "abstract": " We present a new 4-parameter logistic growth model where the rate of change of quantity function is directly proportional to its remaining quantity for growth by a time-dependent logistic function per quantity per unit time. The model can be used to determine the expected number of quantities at time t. Several real world applications are discussed to illustrate the usefulness of the new model including the earthquake occurrence events, the student population growth, and the software modeling. Examples are included to illustrate the goodness-of-fit of the proposed model and existing logistic growth models based on real data sets collected from software applications, earthquake events in the US, and a high school senior class. Three goodness-of-fit test criteria and a recent normalized criteria distance method are used to illustrate the model comparisons. The results show that the proposed model fit significantly better\u00a0\u2026", "num_citations": "13\n", "authors": ["449"]}
{"title": "A software reliability model with vtub-shaped fault-detection rate subject to operating environments\n", "abstract": " In this paper, we develop a new software reliability model incorporating the uncertainty of system faultdetection rate per unit of time subject to the operating environments. Examples are included to illustrate the goodness-of-fit of proposed model and several existing NHPP models based on a set of failure data collected from software applications. The results show that the proposed model fit significantly better than other existing NHPP models based on MSE value. We also present a new method called, normalized criteria distance (NCD), for selecting the best model from among SRGMs based on a set of criteria taken all together. Example results show the proposed method offers a promising technique for selecting the best model based on a set of contributing criteria.", "num_citations": "12\n", "authors": ["449"]}
{"title": "Warranty system-cost analysis using quasi-renewal processes\n", "abstract": " In this paper, we present two alternative quasi renewal processes based on the quasi-renewal process recently developed by Wang and Pham [17]. The first alternative process is an altered quasi-renewal process with random parameter and the other is a mixed quasi-renewal process considering replacements and repairs. These mixed and altered quasi-renewal processes are used for developing the warranty cost models, reliability and other measures for k-out-of-n systems. A numerical example is discussed to demonstrate the applicability of the proposed methodology.", "num_citations": "12\n", "authors": ["449"]}
{"title": "Special issue on critical reliability challenges and practices [guest editorial]\n", "abstract": " The six papers in this special issue address various research challenges in the reliability-related areas including optimization, software reliability and measurements, network reliability, software quality, semisupervised learning, Bayesian approach, statistical anomaly detection, flooding attacks, genetic algorithms, multistate systems, system redundancy, service reliability, multiparadigm modeling, heuristic algorithms, and grid and distributed computing. The selected papers are briefly summarized.", "num_citations": "12\n", "authors": ["449"]}
{"title": "Optimal design for a class of noncoherent systems\n", "abstract": " The author addresses the optimization of (k-to-l)-out-of-n:G systems. A (k-to-l)-out-of-n:G system is a noncoherent system in which 'no fewer than k and no more than l' out of n units may function for the successful operation of the system. Examples of noncoherent systems are found in communication, multiprocessor and transportation systems. This class of noncoherent systems is very general and includes coherent systems, e.g. parallel, series, and N-modular redundancy, as special cases. There is an optimal system-size which maximizes the mean system profit. A numerical example illustrates the results.< >", "num_citations": "12\n", "authors": ["449"]}
{"title": "A software reliability model considering the syntax error in uncertainty environment, optimal release time, and sensitivity analysis\n", "abstract": " The goal set by software developers is to develop high quality and reliable software products. During the past decades, software has become complex, and thus, it is difficult to develop stable software products. Software failures often cause serious social or economic losses, and therefore, software reliability is considered important. Software reliability growth models (SRGMs) have been used to estimate software reliability. In this work, we introduce a new software reliability model and compare it with several non-homogeneous Poisson process (NHPP) models. In addition, we compare the goodness of fit for existing SRGMs using actual data sets based on eight criteria. The results allow us to determine which model is optimal. View Full-Text", "num_citations": "11\n", "authors": ["449"]}
{"title": "Maintenance modeling and policies\n", "abstract": " The systems used in production, transportation services, and communication services constitute the majority part of not only industrial activities but also our daily life. Most of them have many units or components with various structure that will degrade with time or usage, and even suffer from a sudden failure due to the random shocks. For some systems, such as military systems, aircrafts, and nuclear power plants, they are of great importance and cannot afford to any failure. A machine in industrial plant failing to work properly will interpret the whole production assembly line and cost a large amount of capital and labor, while the failure of the aircraft will endanger the life of all the passengers. Therefore, maintenance on these systems is necessary due to the two aspects: (1) prolong the service life of the products; (2) improve the system reliability to avoid unnecessary failure.", "num_citations": "11\n", "authors": ["449"]}
{"title": "Improving energy and power efficiency using NComputing and approaches for predicting reliability of complex computing systems\n", "abstract": " Opting to follow the computing-design philosophy that the best way to reduce power consumption and increase energy efficiency is to reduce waste, we propose an architecture with a very simple ready-implementation by using an NComputing device that can allow multi-users but only one computer is needed. This intuitively can save energy, space as well as cost. In this paper, we propose a simple and realistic NComputing architecture to study the energy and power-efficient consumption of desktop computer systems by using the NComputing device. We also propose new approaches to estimate the reliability of k-out-of-n systems based on the delta method. The k-out-of-n system consisting of n subsystems works if and only if at least k-of-the-n subsystems work. More specificly, we develop approaches to obtain the reliability estimation for the k-out-of-n systems which is composed of n independent and\u00a0\u2026", "num_citations": "11\n", "authors": ["449"]}
{"title": "Modelling the spare parts stock levels and its applications in industrial systems\n", "abstract": " This paper addresses a practical decision making by developing an innovative inventory and transportation cost model to determine the optimum number of the spare parts and its location in the warehouses subject to run-in, random and usage failure inside a maintenance network. The spare parts slow moving items inventory management is a complex problem. The demand is sporadic and difficult to forecast, the utilisation is specific, the part economic value is high, the failure causes system downtime costs and normally this complexity is compensated by overestimated the part inventories. The proposed procedure can be used to determine the spare parts stock level and allows to minimise a function cost sum of aggregate inventory costs and downtime production plant costs caused by the parts failure and transportation from the storage areas to the user or machines failed. Four real case studies from different\u00a0\u2026", "num_citations": "11\n", "authors": ["449"]}
{"title": "Mortality modeling perspectives\n", "abstract": " As the human lifespan increases, more and more people are becoming interested in mortality rates at higher ages. Since 1909, the birth rate in the United States has been decreasing except for a major significant increase after World War II, between the years 1946 and 1964, also known as the baby boom period. People born during the baby boom are now between the ages of 44 and 62. According to the National Center for Health Statistics, US Department Health and Human Services, in 1900\u20131902, one could expect to live for 49 years on average. Today, an infant can expect to live about 77 years. As of recent years and in prediction, the life expectancy for an infant born may be even higher. With the human lifespan increasing and a large part of the United States population aging, many researchers in various fields have recently become interested in studying quantitative models of mortality rates\u00a0\u2026", "num_citations": "11\n", "authors": ["449"]}
{"title": "Modeling software-reliability with multiple failure-types and imperfect debugging\n", "abstract": " This paper presents a software reliability model that is based on a nonhomogeneous Poisson process. The major contribution of this model is combining multiple failure types with imperfect debugging. In addition, the paper discusses cost models that can be used to determine the optimal time to be spent debugging. The software reliability model allows for three different types of errors: critical, major, and minor errors. Critical errors are the most difficult to detect and the most expensive to remove. Major errors are moderately difficult to detect and fairly expensive to remove. Minor errors are easy to detect and inexpensive to remove. The model also allows for the introduction of any of these types of errors during the removal of an error. Using the software reliability model developed, we determine the optimal debugging time necessary to minimize costs subject to reliability constraints. A numerical example is provided\u00a0\u2026", "num_citations": "11\n", "authors": ["449"]}
{"title": "Optimal design of life testing for ULSI circuit manufacturing\n", "abstract": " The life testing of ultra large scale integration (ULSI) circuits is perhaps the most complex manufacturing process found today. This complexity is, in part, the result of product diversity, uncertainty, and changing technologies, and has caused the cost of testing for ULSI circuits to grow exponentially. The author develops a testing cost model and determines the optimum sample size on test which minimizes the expected total system cost assuming that the cost of waiting per unit time and the cost of placing an ULSI circuit on test are given. Numerical examples are also provided to illustrate the methods.< >", "num_citations": "11\n", "authors": ["449"]}
{"title": "A fuzzy rule-based generation algorithm in interval type-2 fuzzy logic system for fault prediction in the early phase of software development\n", "abstract": " Reliability, a measure of software, deals in total number of faults count up to a certain period of time. The present study aims at estimating the total number of software faults during the early phase of software life cycle. Such estimation helps in producing more reliable software as there may be a scope to take necessary corrective actions for improving the reliability within optimum time and cost by the software developers. The proposed interval type-2 fuzzy logic-based model considers reliability-relevant software metric and earlier project data as model inputs. Type-2 fuzzy sets have been used to reduce uncertainties in the vague linguistic values of the software metrics. A rule formation algorithm has been developed to overcome inconsistency in the consequent parts of large number of rules. Twenty-six software project data help to validate the model, and a comparison has been provided to analyse the proposed\u00a0\u2026", "num_citations": "10\n", "authors": ["449"]}
{"title": "Fuzzy optimization approach to component selection of fault-tolerant software system\n", "abstract": " In developing software systems, a manager\u2019s goal is to design software using limited resources and meet the user requirements. One of the important user requirements concerns the reliability of the software. The decision to choose the right software modules (components) becomes extremely difficult because of the number of parameters to be considered while making the decision. If suitable components are not available, then the decision process is further complicated with build versus buy decisions. In this paper, we have formulated a fuzzy multi-objective approach to optimal decision \u201cbuild-or-buy\u201d for component selection for a fault-tolerant modular software system under the consensus recovery block scheme. A joint optimization model is formulated where the two objectives are maximization of system reliability and minimization of the system cost with a constraint on delivery time. An example of\u00a0\u2026", "num_citations": "10\n", "authors": ["449"]}
{"title": "A condition-based inspection-maintenance model based on geometric sequences for systems with a degradation process and random shocks\n", "abstract": " [en] In this paper we present a condition-based inspection-maintenance model for the systems subject to a degradation process and random shocks based on a geometric approach where the inter-inspection times are non-increasing. Upon the inspection of the system and its condition, inspectors need to decide whether to take an action such as preventive maintenance (PM) or corrective maintenance (CM), or no action is needed. We first derive an expected maintenance cost rate function subject to a degradation process and cumulative shock damage. Then we use the mean time to first failure as an initial solution approach to obtain the optimal maintenance policy consisting of two decision variables that minimizes the expected long-run maintenance cost per unit time. The two decision variables are the preventive maintenance threshold value and inspection times based on a geometric sequence. A numerical example is given to illustrate the optimal maintenance policy of the expected long-run cost rate model.(author)", "num_citations": "10\n", "authors": ["449"]}
{"title": "Basic statistical concepts\n", "abstract": " This brief chapter presents some fundamental elements of engineering probability and statistics with which some readers are probably already familiar, but others may not be. Statistics is the study of how best one can describe and analyze the data and then draw conclusions or inferences based on the data available. The first section of this chapter begins with some basic definitions, including probability axioms, basic statistics and reliability measures. The second section describes the most common distribution functions such as the binomial, Poisson, geometric, exponential, normal, log normal, Student\u02bcs t, gamma, Pareto, Beta, Rayleigh, Cauchy, Weibull and Vtub-shaped hazard rate distributions, their applications and their use in engineering and applied statistics. The third section describes statistical inference, including parameter estimation and confidence intervals. Statistical inference is the process by which information from sample data is used to draw conclusions about the population from which the sample was selected that hopefully represents the whole population. This discussion also introduces the maximum likelihood estimation (MLE) maximum likelihood estimation (MLE) method, the method of moments, MLE with censored data, the statistical change-point estimation method, nonparametic tolerance limits, sequential sampling and Bayesian methods. The fourth section briefly discusses stochastic processes, including Markov processes, Poisson processes, renewal processes, quasi-renewal processes, and nonhomogeneous Poisson processes. Finally, the last section provides a short list of books for readers who are interested in\u00a0\u2026", "num_citations": "10\n", "authors": ["449"]}
{"title": "Recent advances in reliability and quality engineering\n", "abstract": " This volume presents recent research in reliability and quality theory and its applications by many leading experts in the field. The subjects covered include reliability optimization, software reliability, maintenance, quality engineering, system reliability, Monte Carlo simulation, tolerance design optimization, manufacturing system estimation, neural networks, software quality assessment, optimization design of life tests, software quality, reliability-centered maintenance, multivariate control chart, methodology for measurement of test effectiveness, imperfect preventive maintenance, Markovian reliability modeling, accelerated life testing, and system availability assessment. The book will serve as a reference for postgraduate students and will also prove useful for practitioners and researchers in reliability and quality engineering. Sample Chapter (s). Chapter 1.1: Introduction (88 KB). Chapter 1.2: The Symmetrical Johnson Su Distributions (101 KB). Chapter 1.3: Application to Control Charts (79 KB). Chapter 1.4: An Example (84 KB). Chapter 1.5: How Kurtosis Affects Classical Charts (104 KB). Chapter 1.6: OC and ARL Curves (133 KB). Chapter 1.7: Conlusions (129 KB). Contents: Control Charts for Data Having a Symmetrical Distribution with a Positive Kurtosis (P Philippe); A Software Reliability Model with Testing Coverage and Imperfect Debugging (X Zhang & H Pham); Cost Allocation for Software Reliability (O Berman & M Cutler); General Reliability Test Plans for One-Shot Devices (W Zhang & WK Shiue); Multivariate Control Chart (MW Lu & RJ Rudy); Optimal Preparedness Maintenance of Multi-Unit Systems with Imperfect Maintenance and\u00a0\u2026", "num_citations": "10\n", "authors": ["449"]}
{"title": "Modeling and analysis of leftover issues and release time planning in multi-release open source software using entropy based measure\n", "abstract": " In Open Source Software (OSS), users report different issues on issues tracking systems. Due to time constraint, it is not possible for developers to resolve all the issues in the current release. The leftover issues which are not addressed in the current release are added in the next release issue content. Fixing of issues result in code changes that can be quantified with a measure known as complexity of code changes or entropy. We have developed a 2-dimensional entropy based mathematical model to determine the leftover issues of different releases of five Apache open source products. A model for release time prediction using entropy is also proposed. This model maximizes the satisfaction level of user's in terms of number of issues addressed.", "num_citations": "9\n", "authors": ["449"]}
{"title": "The role of Payment for Forest Environmental Services (PFES) in financing the forestry sector in Vietnam\n", "abstract": " Key messages Despite being a new funding source, PFES now contributes 22% of total forestry sector investment. The impact of PFES funding differs by location and actor group. To enhance the scale of PFES, and its impac", "num_citations": "9\n", "authors": ["449"]}
{"title": "A Logistic Fault-Dependent Detection Software Reliability Model.\n", "abstract": " In this paper, we present a logistic fault-dependent detection model where the dependent-rate of detected faults in the software can grow much faster from the beginning but grow slowly as the testing progresses until it reaches the maximum number of faults in the software. The explicit function of the expected number of software failures detected by time t, called mean value function, of the proposed model is derived. Model analysis is discussed based on normalized-rank Euclidean distance (RED) and other criteria to illustrate the goodness-of-fit criteria of proposed model and compare it to several existing NHPP models using a set of software failure data. The confidence interval for the parameter estimates of the proposed model is also presented. A numerical analysis based on a real data set of the 7 or higher magnitude earthquake in the United States to illustrate the goodness-of-fit of the proposed model and a\u00a0\u2026", "num_citations": "9\n", "authors": ["449"]}
{"title": "Maintenance and warranty concepts\n", "abstract": " In general, a warranty is an obligation attached to products that require the manufacturer to provide compensation for customer (buyer) according to the warranty terms when the warranted products fail to perform their intended functions. A warranty is important to the manufacturer as well as the customer of any commercial product since it provides protection to both parties. As for the customer, a warranty provides a resource for dealing with items that fail due to the uncertainty of the product\u2019s performance and unreliable products. For the manufacturer, it provides protection since the warranty terms explicitly limit the responsibility of a manufacturer in terms of both time and type of product failure. Because of the role of the warranty, manufacturers have developed various types of warranty policies to grab the interest of the customers. However, manufacturers cannot extend the warranty period without limit and\u00a0\u2026", "num_citations": "9\n", "authors": ["449"]}
{"title": "Mathematical systemability function approximations\n", "abstract": " In this paper, we present a mathematical systemability function approximation for predicting the reliability of complex systems in the field using the Taylor series expansion. Numerical reliability comparisons for systems with and without considering the field environment as well as systemability function approximation are given to illustrate the results. Often the exact systemability function for a complex system is a lot more difficult to obtain. The proposed systemability function approximation seems to be simple and accurate and can be easily used to obtain the reliability estimation of complex systems in the field. Thus, it is worth the extra effort to further validate the results with other complex applications.", "num_citations": "9\n", "authors": ["449"]}
{"title": "Software fault tolerance\n", "abstract": " 33.7 Conclusion             This chapter presents a non-homogeneous Poisson progress reliability model for N-version programming systems. We separate all faults within NVP systems into independent faults and common faults, and model each type of failure as NHPP. We further develop a reliability model for common failures in NVP systems and also present a model for concurrent independent failures in NVP systems. By combining the CF model and the CIF model together, we establish an NHPP reliability model for NVP systems. We also give an example to illustrate how to estimate all unknown parameters by using the maximum likelihood estimation method, and how to compute the variances for all parameter estimates in order to obtain the confidence intervals of NVP system reliability prediction.", "num_citations": "9\n", "authors": ["449"]}
{"title": "Reliability analysis of nuclear fail-safe redundancy\n", "abstract": " In many critical nuclear safety system applications, fault tolerance has been an essential design attribute for achieving high reliability. Generally, operating systems cannot achieve the intended reliability without employing redundancy. Redundant systems, however, require additional resources in terms of both labor and materials. Therefore, the redundancy level needed to achieve fault tolerance must be carefully determined by balancing the reliability and the cost of the systems. In this paper, the optimization problem is formulated and solved for the minimum average total cost of nuclear fail-safe systems. We also determine the optimal system size which minimizes the average total system cost subject to a restricted type I design error. Application and numerical examples are given to illustrate the results.", "num_citations": "9\n", "authors": ["449"]}
{"title": "Cost optimization of a class of noncoherent systems\n", "abstract": " A k-to-l-out-of-n system is a noncoherent system which no fewer than k and no more than l out of n units are to function for the successful operation of the system. Examples of noncoherent systems are found in communication, multiprocessor and transportation systems. Assume that the cost of units and the cost of system failure are given. In this paper, the optimization problems are formulated and solved for the minimum expected total cost of k-to-l-out-of-n systems. For fixed values of k and l, we determine the optimal number of units minimizing the expected total system cost. Several numerical examples are given to illustrate the results.", "num_citations": "9\n", "authors": ["449"]}
{"title": "Using systemability function for periodic replacement policy in real environments\n", "abstract": " In the last decades, various maintenance policies have been developed and widely used, including, but not limited to replacement policy and preventive maintenance policy, depending on an accurate estimation of the reliability and the failure intensity functions. Many studies, yet, haven't considered the environmental factors (EFs) and their effects on the survival distribution of operating units. This paper is a following up with our recent research about environmental impacts on preventive maintenance by investigating the periodic replacement policy using a systemability approach. The differences between the classical maintenance approach and the systemability approach have been investigated and applied to a real industrial setting to evaluate the importance and the relevance of taking into account EFs in the implementation of one maintenance policy versus another. Copyright \u00a9 2014 John Wiley & Sons, Ltd\u00a0\u2026", "num_citations": "8\n", "authors": ["449"]}
{"title": "Statistical Maintenance Modeling for Complex Systems\n", "abstract": " The first part of this chapter provides a brief introduction to statistical maintenance maintenance modeling subject to multiple failure processes. It includes a description of general probabilistic degradation processes. degradation process The second part discusses detailed reliability modeling reliabilitymodeling for degraded systems subject to competing failure processes without maintenance actions. A generalized multi-state degraded system multi-state degraded-system reliability model with multiple competing failure processes including degradation processes and random shocks is presented. The operating condition of the multi-state system is characterized by a finite number of states. A methodology to generate the system states when multi-failure processes exist is also discussed. The model can be used not only to determine the reliability of the degraded systems in the context of multi-state functions but also to obtain the probabilities of being in a given state of the system. The third part describes the inspection\u2013maintenance inspectionmaintenance issues and reliability modeling for degraded repairable systems with competing failure processes. A generalized condition-based maintenance model for inspected degraded systems is discussed. An average long-run maintenance cost rate function is derived based on an expression for degradation paths and cumulative shock damage, which are measurable. An inspection sequence is determined based on the minimal maintenance minimal maintenance cost rate. Upon inspection, a decision will be made on whether to perform preventive maintenance or not. The optimum preventive\u00a0\u2026", "num_citations": "8\n", "authors": ["449"]}
{"title": "Warranty cost models of renewable risk-free policy for multi-component systems\n", "abstract": " This paper presents several system warranty cost models of a renewable risk-free policy for multi-component products with system structure such as series, parallel, series-parallel, and parallel-series. Warranty cost distributions, expectations, variances, and prediction intervals are then derived to facilitate practical applications.", "num_citations": "8\n", "authors": ["449"]}
{"title": "Software reliability model with dependent failures and SPRT\n", "abstract": " Software reliability and quality are crucial in several fields. Related studies have focused on software reliability growth models (SRGMs). Herein, we propose a new SRGM that assumes interdependent software failures. We conduct experiments on real-world datasets to compare the goodness-of-fit of the proposed model with the results of previous nonhomogeneous Poisson process SRGMs using several evaluation criteria. In addition, we determine software reliability using Wald\u2019s sequential probability ratio test (SPRT), which is more efficient than the classical hypothesis test (the latter requires substantially more data and time because the test is performed only after data collection is completed). The experimental results demonstrate the superiority of the proposed model and the effectiveness of the SPRT. View Full-Text", "num_citations": "7\n", "authors": ["449"]}
{"title": "A generalized multiple environmental factors software reliability model with stochastic fault detection process\n", "abstract": " Software systems have been widely applied in numerous safety\u2013critical domains; however, large-scale software development is still considered as a complicated and expensive activity. As the latest trends in software industry accelerate the complexity and dependency of software development, such complicated and human-centered process needs to be addressed well. Meanwhile, recent survey investigations (Zhu et al. in J Syst Softw 109:150\u2013160, 2015; Zhu and Pham in J Syst Softw 132:72\u201384, 2017) revealed that environmental factors, defined from software development, have significant impacts on software reliability. Considering such significant impacts, we first propose a generalized multiple-environmental-factors software reliability growth model with multiple environmental factors and the associated randomness under the martingale framework. The randomness is reflected on the process of\u00a0\u2026", "num_citations": "7\n", "authors": ["449"]}
{"title": "Predictive modeling on the number of Covid-19 death toll in the united states considering the effects of coronavirus-related changes and Covid-19 recovered cases\n", "abstract": " Birdshot Uveitis (Birdshot) is a rare eye condition that affects HLA-A29-positive individuals and could be considered a prototypic member of the recently proposed \u201cMHC-I-opathy\u201d family. Genetic studies have pinpointed the ERAP1 and ERAP2 genes as shared associations across MHC-I-opathies, which suggests ERAP dysfunction may be a root cause for MHC-I-opathies. We mapped the ERAP1 and ERAP2 haplotypes in 84 Dutch cases and 890 controls. We identified association at variant rs10044354, which mediated a marked increase in ERAP2 expression. We also identified and cloned an independently associated ERAP1 haplotype (tagged by rs2287987) present in more than half of the cases; this ERAP1 haplotype is also the primary risk and protective haplotype for other MHC-I-opathies. We show that the risk ERAP1 haplotype conferred significantly altered expression of ERAP1 isoforms in transcriptomic data (n=360), resulting in lowered protein expression and distinct enzymatic activity. Both the association for rs10044354 (meta-analysis: OR[95% CI]=2.07[1.58-2.71], p=1.24 \u00d7 10(\u22127)) and rs2287987 (OR[95% CI]: =2.01 [1.51-2.67], p=1.41 \u00d7 10(\u22126)) replicated and showed consistent direction of effect in an independent Spanish cohort of 46 cases and 2,103 controls. In both cohorts, the combined rs2287987-rs10044354 haplotype associated with Birdshot more strongly than either SNP alone (meta-analysis: p=3.9 \u00d7 10(\u22129)). Finally, we observed that ERAP2 protein expression is dependent on the ERAP1 background across three European populations (n=3,353). In conclusion, a functionally distinct combination of ERAP1 and ERAP2\u00a0\u2026", "num_citations": "7\n", "authors": ["449"]}
{"title": "A testing coverage model based on NHPP software reliability considering the software operating environment and the sensitivity analysis\n", "abstract": " We have been attempting to evaluate software quality and improve its reliability. Therefore, research on a software reliability model was part of the effort. Currently, software is used in various fields and environments; hence, one must provide quantitative confidence standards when using software. Therefore, we consider the testing coverage and uncertainty or randomness of an operating environment. In this paper, we propose a new testing coverage model based on NHPP software reliability with the uncertainty of operating environments, and we provide a sensitivity analysis to study the impact of each parameter of the proposed model. We examine the goodness-of-fit of a new testing coverage model based on NHPP software reliability and other existing models based on two datasets. The comparative results for the goodness-of-fit show that the proposed model does significantly better than the existing models. In addition, the results for the sensitivity analysis show that the parameters of the proposed model affect the mean value function. View Full-Text", "num_citations": "7\n", "authors": ["449"]}
{"title": "A confidence-based approach to reliability design considering correlated failures\n", "abstract": " To maintain a competitive edge, technology manufacturers must produce systems that are reliable enough to satisfy customers yet cheap enough to engineer so that they are profitable. This paper presents an optimization model to maximize the statistical confidence in product profitability, permitting flexibility in the design and number of the units manufactured. This is unlike traditional approaches, which focus on the two cases that optimize the reliability of a single unit or the s-expected profit obtained from a very large number of units. These two extremes disregard a practical concern, namely the negative impact that a larger than s-expected number of failures will exert on product profitability. This paper formulates an optimization problem to mitigate this risk. Virtually all reliability optimization problems also assume that component failures are s-independent. The present paper does not impose this assumption. The\u00a0\u2026", "num_citations": "7\n", "authors": ["449"]}
{"title": "Weibull vs. normal distribution of demand to determine the safety stock level when using the continuous-review (S, s) model without backlogs\n", "abstract": " Determining safety stock is one of the most critical issues in inventory and logistics management. This paper presents a cost-based analytical model to determine the best safety stock and customer service level for uncertain demand of products during generic replenishment lead times and a continuous-review inventory system. The proposed model uses the Weibull distribution to model the generic stochastic demand. \u03b1 and \u03b2 are the scale parameter and the shape parameter, respectively. An experimental analysis based on the assumption of different distributions of demand of product demonstrates the effectiveness of the proposed model compared to the so-called normal-based traditional model. Significantly, the proposed Weibull-based model is a comprehensible and user-friendly tool that can be useful for production system managers and practitioners.", "num_citations": "7\n", "authors": ["449"]}
{"title": "Fuzzy multi-objective build-or-buy approach for component selection of fault tolerant software system under consensus recovery block scheme with mandatory redundancy in\u00a0\u2026\n", "abstract": " During the last two decades, there has been a growing interest in component-based software engineering (CBSE) both in academia and industry. In component-based system development, it is common to identify software modules first. Once they are identified, we need to select appropriate software components for each module. These components can either be bought as commercial off-the-shelf (COTS) components and probably adapted to work in the software system or can be developed in-house. This is a \u2018build-or-buy\u2019 decision. This paper discusses a framework that helps a developer to decide whether to buy or to build software components while designing a fault-tolerant modular software system. This paper proposes optimisation models for optimal component selection for a fault-tolerant modular software system under the consensus recovery block scheme. It is necessary to identify critical modules in the\u00a0\u2026", "num_citations": "7\n", "authors": ["449"]}
{"title": "Optimal design of systems subject to two kinds of failure\n", "abstract": " The problem of achieving optimal system size is treated for k-out-of-n systems, assuming that failure may take either one of two forms. Under the assumption that components are independent and identically-distributed and the two kinds of system failures can have different costs, the optimal system size n which maximizes the mean system profit is determined. The effect of the system parameters on the optimal system size n is studied. A numerical example is given to illustrate the results.< >", "num_citations": "7\n", "authors": ["449"]}
{"title": "Intelligent Information and Database Systems: 10th Asian Conference, ACIIDS 2018, Dong Hoi City, Vietnam, March 19-21, 2018, Proceedings, Part I\n", "abstract": " The two-volume set LNAI 10751 and 10752 constitutes the refereed proceedings of the 10th Asian Conference on Intelligent Information and Database Systems, ACIIDS 2018, held in Dong Hoi City, Vietnam, in March 2018. The total of 133 full papers accepted for publication in these proceedings was carefully reviewed and selected from 423 submissions. They were organized in topical sections named: Knowledge Engineering and Semantic Web; Social Networks and Recommender Systems; Text Processing and Information Retrieval; Machine Learning and Data Mining; Decision Support and Control Systems; Computer Vision Techniques; Advanced Data Mining Techniques and Applications; Multiple Model Approach to Machine Learning; Sensor Networks and Internet of Things; Intelligent Information Systems; Data Structures Modeling for Knowledge Representation; Modeling, Storing, and Querying of Graph Data; Data Science and Computational Intelligence; Design Thinking Based R&D, Development Technique, and Project Based Learning; Intelligent and Contextual Systems; Intelligent Systems and Algorithms in Information Sciences; Intelligent Applications of Internet of Thing and Data Analysis Technologies; Intelligent Systems and Methods in Biomedicine; Intelligent Biomarkers of Neurodegenerative Processes in Brain; Analysis of Image, Video and Motion Data in Life Sciences; Computational Imaging and Vision; Computer Vision and Robotics; Intelligent Computer Vision Systems and Applications; Intelligent Systems for Optimization of Logistics and Industrial Applications.", "num_citations": "6\n", "authors": ["449"]}
{"title": "Maintenance information system and failure rate prediction\n", "abstract": " A modern approach to the maintenance problem requires an efficient support operated by the information system. There are a lot of articulated data to be taken into consideration. A system that collects and organizes this information is a prerequisite for any further elaboration. Nowadays, information technology provides to maintenance engineers and practitioners an automatic software platform called a \u201ccomputerized maintenance management system,\u201d with some advantages but also some omissions. Often engineers and practitioners cannot wait for the implementation of the computerized maintenance management system; their policies require robust information since from the phase-in of the equipment or plant. They may wish to get reliability results more quickly than in the case of data coming from products operating under normal conditions. This situation is usually faced using the experience of the\u00a0\u2026", "num_citations": "6\n", "authors": ["449"]}
{"title": "Reliability analysis of dynamic fiber bundle models\n", "abstract": " Fiber bundle models are useful tools for explaining dynamic failure behavior in heterogeneous materials. Such models shed light on diverse phenomena such as fatigue in structural materials and earthquakes in geophysical settings. Building good theoretical models has proven straightforward, but analyzing them has required delving into statistical details of the interaction of various flaw features and failure configurations, which has proven to be deceptively difficult. In this paper, we present a new method for reliability analysis of dynamic fiber bundle models. As in previous works, we assume that a fiber has a failure rate following a power law in its load level. However, unlike the exponential distribution used in the previous works, we consider that the remaining lifetimes of the surviving fibers follow Weibull distributions according to either the cumulative exposure model or tampered failure rate model. We develop\u00a0\u2026", "num_citations": "6\n", "authors": ["449"]}
{"title": "A novel system reliability modeling of hardware, software, and interactions of hardware and software\n", "abstract": " In the past few decades, a great number of hardware and software reliability models have been proposed to address hardware failures in hardware subsystems and software failures in software subsystems, respectively. The interactions between hardware and software subsystems are often neglected in order to simplify reliability modeling, and hence, most existing reliability models assumed hardware subsystems and software subsystem are independent of each other. However, this may not be true in reality. In this study, system failures are classified into three categories, which are hardware failures, software failures, and hardware-software interaction failures. The main contribution of our research is that we further classify hardware-software interaction failures into two groups: software-induced hardware failures and hardware-induced software failures. A Markov-based unified system reliability modeling incorporating all three categories of system failures is developed in this research, which provides a novel and practical perspective to define system failures and further improve reliability prediction accuracy. Comparison of system reliability estimation between the reliability models with and without considering hardware-software interactions is elucidated in the numerical example. The impacts on system reliability prediction as the changes of transition parameters are also illustrated by the numerical examples. View Full-Text", "num_citations": "5\n", "authors": ["449"]}
{"title": "Modeling and analysis of software fault detectability and removability with time variant fault exposure ratio, fault removal efficiency, and change point\n", "abstract": " Software reliability growth models have been proposed to assess and predict the reliability growth of software, remaining number of faults, and failure rate. In previous studies, software faults have been mainly categorized into two categories based on its severity in removal process: simple faults and hard faults. In reality, fault detectability is one of the crucial factors which can influence the reliability growth of software. The detectability of a software fault depends on how frequently the instructions containing faults are executed. However, fault removability of a software fault depends on fault removal efficiency of debugging team. The main motive of this article is to incorporate the fault detectability in software reliability assessment. Fault exposure ratio is an essential factor for software reliability modeling that controls the per-fault hazard rate. It is strongly dependent on fault detectability. In this article, the effect of fault\u00a0\u2026", "num_citations": "5\n", "authors": ["449"]}
{"title": "Log canonical thresholds and Monge-Amp\u00e8re masses\n", "abstract": " In this paper, we prove an inequality for log canonical thresholds and Monge-Amp\u00e8re masses. The idea of proof is a combination of the Ohsawa-Takegoshi -extension theorem and inequalities in \u00c5hag et al. (Adv Math 222:2036\u20132058, 2009) and Demailly and Pham (Acta Math 212:1\u20139, 2014).", "num_citations": "5\n", "authors": ["449"]}
{"title": "Software reliability model considering time-delay fault removal\n", "abstract": " Software reliability has proven to be one of the most useful indices in evaluating software applications quantitatively. Among many different methodologies for constructing software reliability models, the software reliability growth models (SRGMs) based on the non-homogeneous Poisson process (NHPP) has been widely used in practical software reliability engineering and, has attracted many engineers and researchers who assess software systems.", "num_citations": "5\n", "authors": ["449"]}
{"title": "Optimal checkpointing interval for task duplication with spare processing\n", "abstract": " In computer systems, some errors often occur due to noises, human errors, hardware faults, etc. To attain the accuracy of computing, it is of great importance to detect such errors by fault tolerant computing techniques. 1Usually, an error detection of the process can be made by two independent modules where they compare two results at suitable checkpointing times. If their results do not match with each other, we go back to the newest checkpoint and make a retrial of the processes. In such situations, if we compare results frequently, then we could", "num_citations": "5\n", "authors": ["449"]}
{"title": "A novel approach for spares optimization of complex repairable systems\n", "abstract": " In this paper, several subsystems that are arranged in a complex network configuration is considered. The objective is to find the optimal number of spares in each subsystem that minimizes the overall cost associated with the system. The main contribution of this paper is in providing the bounds for the optimal spares quantity for each subsystem. The bounds are extremely useful to reduce the search space and hence improve the efficiency of the optimization algorithm. The examples demonstrate the efficiency and practical use of the proposed bounds.", "num_citations": "5\n", "authors": ["449"]}
{"title": "Estimating the COVID-19 death toll by considering the time-dependent effects of various pandemic restrictions\n", "abstract": " COVID-19, known as Coronavirus disease 2019, is caused by a coronavirus called SARS-CoV-2. As coronavirus restrictions ease and cause changes to social and business activities around the world, and in the United States in particular, including social distancing, reopening states, reopening schools, and the face mask mandates, COVID-19 outbreaks are on the rise in many states across the United States and several other countries around the world. The United States recorded more than 1.9 million new infections in July, which is nearly 36 percent of the more than 5.4 million cases reported nationwide since the pandemic began, including more than 170,000 deaths from the disease, according to data from Johns Hopkins University as of 16 August 2020. In April 2020, the author of this paper presented a model to estimate the number of deaths related to COVID-19, which assumed that there would be no significant change in the COVID-19 restrictions and guidelines in the coming days. This paper, which presents the evolved version of the previous model published in April, discusses a new explicit mathematical model that considers the time-dependent effects of various pandemic restrictions and changes related to COVID-19, such as reopening states, social distancing, reopening schools, and face mask mandates in communities, along with a set of selected indicators, including the COVID-19 recovered cases and daily new cases. We analyzed and compared the modeling results to two recent models based on several model selection criteria. The model could predict the death toll related to the COVID-19 virus in the United States and\u00a0\u2026", "num_citations": "4\n", "authors": ["449"]}
{"title": "Constituents of the Edible Leaves of Melicope pteleifolia with Potential Analgesic Activity\n", "abstract": " Melicope pteleifolia has long been consumed as a popular vegetable and tea in Southeast Asian countries, including Malaysia and southern mainland China, and is effective in the treatment of colds and inflammation. In the search for active metabolites that can explain its traditional use as an antipyretic, six new phloroacetophenone derivatives (3\u20138) along with seven known compounds (1, 2, and 9\u201313) were isolated from the leaves of M. pteleifolia. Their chemical structures were confirmed by extensive spectroscopic analysis including NMR, IR, ECD, and HRMS. All compounds isolated from the leaves of M. pteleifolia (1\u201313) have a phloroacetophenone skeleton. Notably, the new compound 8 contains an additional cyclobutane moiety in its structure. The bioactivities of the isolated compounds were evaluated, and compounds 1, 6, and 7 inhibited tumor necrosis factor-\u03b1-induced prostaglandin E2. Moreover, the\u00a0\u2026", "num_citations": "4\n", "authors": ["449"]}
{"title": "Managing electrical resiliency in a datacenter\n", "abstract": " Techniques are described for providing an automatic transfer switch (ATS), such as in a data center, to switch between a primary power system and a reserve power system based on a priority of the computing systems to which the ATS is supplying power. The ATS obtains the priority from a centralized database and obtains an existing load on the reserve power system. If the existing load is too high and the priority of the computing systems coupled to the ATS is too low, then the ATS may not switch to utilizing the reserve power system upon a failure in the primary power system.", "num_citations": "4\n", "authors": ["449"]}
{"title": "Reliability and cost-benefit analysis for two-stage intervened decision-making systems with interdependent decision units\n", "abstract": " This paper deals with a special type of voting systems, called two-stage intervened decision-making systems, in which the decision time of each decision unit will be a random variable and some supervising mechanism is included. A new decision rule is applied to such kind of systems, which makes the decision units become interdependent from each other. The reliability and cost-benefit models are developed. The optimization for the models is discussed and the optimal solution for a special case is also derived. A numerical example for model optimization is presented as well as some model comparison. Even though a specific application is used for model formulation and derivation throughout this paper, the modeling results can be easily modified and applied to many other systems.", "num_citations": "4\n", "authors": ["449"]}
{"title": "Relay architecture for transferring from redundant power sources\n", "abstract": " Automatic transfer switching apparatus and systems include a switching device including an input line, a sensor, and a parallel assembly of a solid-state relay and latching relay electrically connected with the input line. The solid-state relay is used to short the latching relay such that the latching relay can be opened and/or closed in an unloaded state while the input line is energized.", "num_citations": "4\n", "authors": ["449"]}
{"title": "The potential of REDD+ to finance forestry sector in Vietnam\n", "abstract": " Key messages Despite the great potential REDD+ shows for generating and contributing finance to support forestry in Vietnam, a reduction in both funds and funder commitment to REDD+, challenges in meeting funder requirements, and the", "num_citations": "4\n", "authors": ["449"]}
{"title": "High accuracy, compact on-chip temperature sensor\n", "abstract": " Embodiments of a temperature sensing apparatus are disclosed. The apparatus may include a voltage generator and circuitry. The voltage generator may generate a first voltage level and a second voltage level dependent on an operating temperature. In response to a given change in the operating temperature, the first and second voltage levels may change, with the second voltage level changing by a different amount than the first voltage level. The voltage generator may generate a third voltage level. The circuitry may measure the first voltage level, the second voltage level, and the third voltage level, and may calculate the operating temperature dependent on a ratio of a difference between the first voltage level and the second voltage level and the third voltage level.", "num_citations": "4\n", "authors": ["449"]}
{"title": "Automatic transfer switch with power quality module\n", "abstract": " An automatic transfer switch (ATS) includes power quality conditioners to condition electrical power fed to a power output of the automatic transfer switch. In some embodiments, the power quality conditioners include a surge protection circuit or a filter circuit. In some embodiments, the automatic transfer switch includes a battery module configured to supply electrical power to the power output of the ATS during a power disruption event.", "num_citations": "4\n", "authors": ["449"]}
{"title": "A cost model of an opportunistic maintenance policy okn-out-onf-surveillance systems considering two stochastic processes\n", "abstract": " Surveillance systems including security camera have been widely used to monitor some critical processes and enhance the safety-security level of high-risk large-scale security systems. With the earlier development of the reliability model considering the two stochastic processes, optimal maintenance policies can be obtained under related cost maintenance policy of ka-out-onf-surveillance system with assumptions. In this study, a cost model on the opportunistic consideration of the two stochastic processes is developed. The model includes maintenance cost dependent on the expected number of failed subsystems and penalty terms due to both soft and hard failures of the system. An algorithm to obtain the optimal maintenance policy is provided. Several numerical examples are given to demonstrate the validity of the modeling and the sensitivity of different parameters.", "num_citations": "4\n", "authors": ["449"]}
{"title": "A dependent competing risk model with multiple degradation processes and random shocks using constant copulas\n", "abstract": " In this paper, we develop a dependent competing risk model in which:(1) the dependent structure of random shock and degradation is modulated by a time-scaled covariate factor, and (2) the dependent structure among degradation processes is fitted by constant copulas, for a system subject to multiple-degradation processes and random shocks. A numerical example is discussed to demonstrate the proposed model.", "num_citations": "4\n", "authors": ["449"]}
{"title": "Optimal number of components for a parallel system with competing failure modes\n", "abstract": " The problem is treated of achieving optimal system size for a parallel redundant system assuming that failure may take either of two forms. We assume that components are statistically independently distributed and the two kinds of system failures can have different costs. The probabilities are given of component failure in open and short modes, q                0 and qs                , respectively. We determine the optimal system size n which minimizes the average system cost. A numerical example is given to illustrate the results.", "num_citations": "4\n", "authors": ["449"]}
{"title": "Optimal system-profit design of series-parallel systems with multiple failure modes\n", "abstract": " We treat the problem of achieving optimal subsystem size (m) for series-parallel systems assuming that failure may take either of two forms. We assume that components are independent and identically distributed (iid) and that the two kinds of system failures can have different costs. In this paper, we determine the optimal m that maximizes the average system-profit. We study how optimal subsystem size m depends on the system parameters. We also determine the optimal subsystem size, m, which maximizes the average system-profit subject to a restricted type I design error. Numerical examples are given to illustrate the results.", "num_citations": "4\n", "authors": ["449"]}
{"title": "Reliability analysis of dynamic redundant systems with imperfect coverage\n", "abstract": " This paper considers the problem of determining the optimal number of spare modules in a dynamic redundant system. Two versions of the problem are treated. First, it is shown how to minimize the average system cost. Second, it is shown how to maximize the reliability of the system with imperfect coverage. Some numerical examples are also obtained to illustrate the results.", "num_citations": "4\n", "authors": ["449"]}
{"title": "An empirical study of factor identification in smart health-monitoring wearable device\n", "abstract": " Smart Health-Monitoring Wearable Device (SHMWD) is one of the solutions to improve health-care quality and accessibility through early detection and prevention. A comprehensive framework of factor identification of SHMWD from wide-ranging considerations is needed. Indeed, quantitative support of identifying significant factors/features affecting product rating and review needs to be studied as well. This article aims to identify 123 environmental factors (EFs) and their associated categories of SHMWD from various perspectives, determine the important EFs based on customers' interest, and investigate the significant levels of EFs and each category on product rating, the significant EFs of each category, the correlation among EFs, and the principle components of the data set. Data analysis was conducted based on real data collected online (n = 769). Statistical learning methods, including relative weighted\u00a0\u2026", "num_citations": "3\n", "authors": ["449"]}
{"title": "Opportunities and challenges in mobilizing finance to implement Vietnam's Forestry Development Strategy for 2006-2020\n", "abstract": " This CIFOR Occasional Paper assessed opportunities and challenges in mobilizing finance to implement the Vietnam Forestry Development Strategy (VFDS) for 2006-2020. After 10 years of VFDS implementation, the forestry sector has witnessed many achievements", "num_citations": "3\n", "authors": ["449"]}
{"title": "Toward the development of a conventional time series based web error forecasting framework\n", "abstract": " Web reliability is gaining importance with time due to the exponential increase in the popularity of different social community networks, mailing systems and other online applications. Hence, to enhance the reliability of any existing web system, the web administrators must have the knowledge of various web errors present in the system, influences of various workload characteristics on the manifestation of several web errors and the relations among different workload characteristics. But in reality, often it may not be possible to institute a generalized correspondence among several workload characteristics. Moreover, the issues like the prediction and estimation of the cumulative occurrences of the source content failures and the corresponding time between failures of a web system become less highlighted by the reliability research community. Hence, in this work, the authors have presented a well-defined\u00a0\u2026", "num_citations": "3\n", "authors": ["449"]}
{"title": "Two-stage weighted intervened decision systems\n", "abstract": " This paper deals with the reliability and cost evaluation of a type of weighted voting systems under supervision. Both the input and output for each voting unit are binary, 0 or 1. In particular, the decision behavior of each unit is considered to be influenced and intervened by some control mechanism. In addition, the time for each unit to provide the result is a random variable and the probability that each unit will provide a correct result is a function of time. This system is called intervened decision system with weighted voting mechanism. This type of system can be encountered in many areas such as human decision systems, security systems and software development. The formulation and derivation of this paper is based on a human organization system and can be extended to other applications with some modification.", "num_citations": "3\n", "authors": ["449"]}
{"title": "Optimal design of life testing cost model for Type-II censoring Weibull distribution lifetime units with respect to unknown parameters\n", "abstract": " A life testing of manufactured units is performed in order to speed up testing through either reducing the time required for testing or establishing a predefined number of failures to stop the test. In this paper, we develop a testing cost model for Weibull distribution life time units embedded with its unknown parameter estimators with respect to Type-II censoring life test. In other words, the life test starts with n units and is terminated at a pre-assigned number of failures r. We then determine the optimum sample size on test which minimizes the expected total cost of performing the life testing subject to the unknown parameters of the Weibull distribution lifetime for a fixed number of failures equal to 2. Several numerical examples based on real failure data applications are presented to illustrate the proposed optimal cost design model.", "num_citations": "3\n", "authors": ["449"]}
{"title": "A dual-stochastic process model for surveillance systems with the uncertainty of operating environments subject to the incident arrival and system failure processes\n", "abstract": " Surveillance system is widely used today to enhance the security level of the protected area. Reliability of the entire surveillance system is a critical issue since the breakdown of such system would leave the monitoring area unobserved and encountered much higher risk under the attacks. This paper presents a dual stochastic-process model for predicting the reliability of surveillance systems consisting of many subsystems (units) with considerations of the environmental factors, skill of intruder to avoid detection, the intrusion/incident arrival process and subsystem failure process. Several numerical examples are presented to illustrate the proposed model.", "num_citations": "3\n", "authors": ["449"]}
{"title": "Toughening of Bisphenol-A Diglycidyl Ether-based Epoxy by Modification with Hydroxyl-terminated Liquid Natural Rubber\n", "abstract": " Hydroxyl-terminated liquid natural rubbers (HTNRs), prepared by the Photo-Fenton reaction, were used to modify bisphenol-A diglycidyl ether-based epoxy (DGEBA). A chemical link between HTNRs and the epoxy resin was promoted employing toluene diisocyanate. The reactions between elastomers and epoxy resin were followed by FTIR. The mechanical properties of the composites were evaluated and the microstructure was investigated using scanning electronic microscopy. The results showed that the impact resistance of HTNR-modified DGEBA was superior to that of the pure epoxy resin. For the composites with HTNR, the impact resistance increased with elastomer concentration up to 2.5 parts per hundred parts of resin. Higher concentration of HTNR resulted in larger particles which gave lower impact values.", "num_citations": "3\n", "authors": ["449"]}
{"title": "A condition-based maintenance model for periodically inspected systems subjected to competing failure processes\n", "abstract": " A condition-based maintenance model for periodically inspected systems subject to degradation and random shocks is addressed in this paper. Upon inspection, one will need to determine whether to perform a preventive maintenance, corrective maintenance, or to do nothing. In this paper, we present an expected maintenance cost rate model for systems subject to a degradation process and cumulative shock damage. An optimal maintenance policy consisting of two decision variables such as the preventive maintenance threshold and periodical inspection time, is determined that minimizes the expected long-run maintenance cost per unit time. To improve the search and better ways to obtain the optimum solution, we also propose a heuristic algorithm procedure based on the mean time to first failure concept in order to find the best optimum solution. A numerical example is given to illustrate the ideas of\u00a0\u2026", "num_citations": "3\n", "authors": ["449"]}
{"title": "Introduction to Maintenance in Production Systems\n", "abstract": " \u201cMaintenance is the combination of all technical, administrative and managerial actions during the life cycle of an item intended to retain it in, or restore it to, a state in which it can perform the required function\u201d (EN 13306:2001 Maintenance terminology). This chapter examines the fundamental definitions concerning maintenance, and discusses the maintenance question in product manufacturing companies or service suppliers. Emphasis is placed on integrating maintenance with the other activities of a company (e. g., production, R&D, quality assurance, purchasing). In conclusion, a survey on the status of maintenance in industrial companies and several observations about maintenance outsourcing are discussed.", "num_citations": "3\n", "authors": ["449"]}
{"title": "Reliability evaluation and reliability prediction models\n", "abstract": " Chapter 5 introduced the basic maintenance terminology and nomenclature related to a generic item as a part, component, device, subsystem, functional unit, piece of equipment, or system that can be individually considered. It is worth remembering the following definition of availability in accordance with the European standards and specifications: \u201cability of an item to be in a state to perform a required function under given conditions at a given instant of time or during a given time interval, assuming that the required external resources are provided.\u201d Availability, such as reliability and maintainability, refers to a production system as a combination of different functions, parts, and basic components whose failure and repair behaviors can be known or unknown. In particular, these behaviors can be eventually based on the availability of historical data of failures and repairs, whose statistical evaluation can\u00a0\u2026", "num_citations": "3\n", "authors": ["449"]}
{"title": "Renewable warranty models using quasi-renewal processes\n", "abstract": " In this paper we present two alternative quasi renewal processes based on a quasi-renewal process where the first alternative process is an altered quasi-renewal process with random parameter and the second is a mixed quasi-renewal processes considering replacements and repairs. We then present several warranty cost models for systems including k-out-of-n structure. A numerical example is discussed to demonstrate the applicability of the methodology.", "num_citations": "3\n", "authors": ["449"]}
{"title": "Cost allocation for software reliability\n", "abstract": " Managing a large software project to produce highly reliable code with limited resources, which is delivered on schedule, is a very difficult task. In this research the limited resource is the overall budget provided for the software system. Our goal is to maximize the reliability of developed and/or purchased software systems. We deal with the following aspects of resource allocation:", "num_citations": "3\n", "authors": ["449"]}
{"title": "A software reliability model with testing coverage and imperfect debugging\n", "abstract": " The following sections are included:  Introduction  Notation     Software Reliability Modeling  The Nonhomogeneous Poisson Process Model Formulation     Model Evaluation and Applications  Goodness-of-fit Test: Data from IBM Entry Software Package Evaluation for the Predictive Power: Data from a Real Time Control System     Conclusions   Acknowledgment   References", "num_citations": "3\n", "authors": ["449"]}
{"title": "Optimal cost design of replicated data in distributed database systems\n", "abstract": " Replicated data are a common strategy for achieving ultra-availability in fault-tolerant distributed database systems. Replication, however, requires additional resources, for example additional costs in terms of maintenance effort, software complexity, hardware requirements and time needed for testing consistency. Various replication control protocols have been developed to ensure data consistency. In this paper, we develop analytical methods for the quorum-consensus replication protocol that minimizes the total system cost by determining the optimal read quorum, the optimal number of system sites, or the optimal site availability assuming that (1) the cost of an individual site is an increasing function of the availability of the site and (2) the two system unavailable modes (i.e. read and write) can have different penalty costs. Several numerical examples and applications are provided to illustrate the results.", "num_citations": "3\n", "authors": ["449"]}
{"title": "Optimal system-profit design of k-to-l-out-of-n systems\n", "abstract": " A k-to-l-out-of-n system is a noncoherent system in which no fewer than k and no more than l out of n units are to function for the successful operation of the system. Examples of noncoherent systems are found in communication, multiprocessor and transportation systems. A noncoherent system is very general and includes the parallel, series and N-Modular Redundancy (NMR) systems as special cases. In this paper, for fixed k and l we determine the optimal system size which maximizes the average system profit. A numerical example is provided to illustrate the results.", "num_citations": "3\n", "authors": ["449"]}
{"title": "Optimal design of hybrid fault-tolerant computer systems\n", "abstract": " In many critical applications of digital systems, fault tolerance has been an essential architectural attribute for achieving high reliability. We address the problem of designing an optimal hybrid fault-tolerant computer system. The designer of a system is always confronted with the problem of trading off reliability vs. cost. On the one hand, it is essential to provide for each vital module of the system as many spare units as possible in order to ensure high reliability. On the other hand, it is essential not to have an excessively costly, heavy or bulky system. In this paper, we determine the optimal number of spare units which minimizes the average total system cost. A numerical example is provided to illustrate the methods.", "num_citations": "3\n", "authors": ["449"]}
{"title": "Reliability and Statistical Computing: Modeling, Methods and Applications\n", "abstract": " This book presents the latest developments in both qualitative and quantitative computational methods for reliability and statistics, as well as their applications. Consisting of contributions from active researchers and experienced practitioners in the field, it fills the gap between theory and practice and explores new research challenges in reliability and statistical computing. The book consists of 18 chapters. It covers (1) modeling in and methods for reliability computing, with chapters dedicated to predicted reliability modeling, optimal maintenance models, and mechanical reliability and safety analysis;(2) statistical computing methods, including machine learning techniques and deep learning approaches for sentiment analysis and recommendation systems; and (3) applications and case studies, such as modeling innovation paths of European firms, aircraft components, bus safety analysis, performance prediction in textile finishing processes, and movie recommendation systems. Given its scope, the book will appeal to postgraduates, researchers, professors, scientists, and practitioners in a range of fields, including reliability engineering and management, maintenance engineering, quality management, statistics, computer science and engineering, mechanical engineering, business analytics, and data science.", "num_citations": "2\n", "authors": ["449"]}
{"title": "Convergence of deep machine learning and parallel computing environment for bio\u2010engineering applications\n", "abstract": " Deep machine learning is an emergent area in the field of computational intelligence (CI) research concerned with the analysis and design of learning algorithms and representations of data at multiple levels of abstraction. Deep learning is a technique for implementing machine learning that provides an effective solution for parallel computing environment in bi-engineering problems that encompasses artificial intelligence (AI), artificial neural networks, reasoning, and natural language processing, helping the human intelligence and decision making process. The heterogeneous parallel computing architectures have been considered as \u2018\u2018keys\u2019\u2019for real-time bio-engineering applications, needing the design of a high-level operating system for matching the processing tasks to the appropriate machine learning paradigm in a mixed-machine parallel system. This effort finds to investigate the feasibility of a deep machine\u00a0\u2026", "num_citations": "2\n", "authors": ["449"]}
{"title": "Advances in reliability analysis and its applications\n", "abstract": " Nowadays, in system reliability engineering, advances in reliability analysis is perhaps one of the most multidimensional topics. This quick development has truly changed the environment of system engineering and this global design. Now with the help of simulations and virtual reality technologies, we can start more of the modeling task.The aspects dealt in chapter \u201cTime Varying Communication Networks: Modelling, Reliability Evaluation and Optimization\u201d are (i) TVCN models for representing features like mobility, links, and topology,(ii) description of the notion of Time-Stamped-Minimal Path Sets (TS-MPS) and Time-stamped-minimal Cut Sets (TS-MCS) for TVCNs as an extension of MPS and MCS, respectively, that are widely used in static networks,(iii) techniques for enumerating TS-MPS and TS-MCS, and evaluating reliability measure (s)\u2014particularly two-terminal reliability, expected hop, and slot counts\u00a0\u2026", "num_citations": "2\n", "authors": ["449"]}
{"title": "Reliability inference for VGA adapter from dual suppliers based on contaminated type\u2010I interval\u2010censored data\n", "abstract": " Type\u2010I interval\u2010censoring scheme only documents the number of failed units within two prespecified consecutive exam times at the larger time point after putting all units on test at the initial time schedule. It is challenging to use the collected information from type\u2010I interval\u2010censoring scheme to evaluate the reliability of unit when not all admitted units are operated or tested at the same initial time and a majority of units are randomly selected to replace the failed test units at unrecorded time points. Moreover, the lifetime distribution of all pooled units from dual resources usually follows a mixture distribution. To overcome these two problems, a two\u2010stage inference process that consists of a data\u2010cleaning step and a parameter estimation step via either Markov chain Monte Carlo (MCMC) algorithm or profile likelihood method is proposed based on the contaminated type\u2010I interval\u2010censored sample from a mixture\u00a0\u2026", "num_citations": "2\n", "authors": ["449"]}
{"title": "Statistically Significant Discriminative Patterns Searching\n", "abstract": " In this paper, we propose a novel algorithm, named SSDPS, to discover patterns in two-class datasets. The SSDPS algorithm owes its efficiency to an original enumeration strategy of the patterns, which allows to exploit some degrees of anti-monotonicity on the measures of discriminance and statistical significance. Experimental results demonstrate that the performance of the SSDPS algorithm is better than others. In addition, the number of generated patterns is much less than the number of the other algorithms. Experiment on real data also shows that SSDPS efficiently detects multiple SNPs combinations in genetic data.", "num_citations": "2\n", "authors": ["449"]}
{"title": "A two-stage intervened decision system with state-dependent random inspection mechanisms\n", "abstract": " This paper develops the performability and cost-benefit models for a multi-unit two-stage intervened decision-making system with majority voting rule. The decision process of the system can be divided into two stages, which is an inspection stage (stage 1) and a result submission stage (stage 2). In addition, the system is assumed to have binary inputs and outputs. During the inspection stage, there are two possible states for each decision unit in the system. Each decision unit will be visited at most twice during this stage by the supervisor, and the result for the visit depends on the state of the decision unit. The first visit will be conducted for certain, but the behavior of the second visit will be determined by the result of the first visit. The result provided by each decision unit can be submitted any time during the second stage and there is a checkpoint during this stage to make the process finish earlier. The performance\u00a0\u2026", "num_citations": "2\n", "authors": ["449"]}
{"title": "DETERMINANTS OF VIETNAM\u2019S POTENTIAL TRADE: A CASE STUDY OF AGRICULTURAL EXPORTS TO THE EUROPEAN UNION\n", "abstract": " This study aims at quantifying the determinants of Vietnam\u2019s potential exports to the EU, taking agricultural commodities as a case study. In order to achieve this, we employed a stochastic frontier analysis to estimate Vietnam\u2019s potential agricultural exports, and a system GMM approach to analyze the determinants of the estimated potential agricultural exports of Vietnam. The results show that Vietnam\u2019s potential agricultural exports to the EU have been high and on an upward trend. In addition, factors such as financial market development, trade freedom, technological readiness, and labor freedom have positive impacts on Vietnam\u2019s potential agricultural exports to the EU. Measures to improve the financial market development, remove trade barriers, increase technological capability, and promote labor freedom are strongly suggested in order to enable Vietnam\u2019s agricultural exports to attain its maximum level.", "num_citations": "2\n", "authors": ["449"]}
{"title": "Construction cost overruns in transmission grid projects\n", "abstract": " This paper analyses construction cost overruns of transmission grid projects in Vietnam. Firstly, attributes causing cost overruns were identified from previous studies through a comprehensive literature review, the reality during the construction phase, and discussions with experts in the power industry. Subsequently, a questionnaire was developed and the data were collected through the survey. Nextly, factor analysis was executed and seven key factors were also extracted, including: Management to human and construction resources; Competence of stakeholders; Policies of the Government; Construction policies; Relationships among main contractors, subcontractors, the workforce; Cost of materials and equipment; and Adverse objective attributes. In addition, from the result of factor analysis, the study ranked the attributes of seven key factors by mean value. The result revealed that attributes causing highest cost overruns are: Incompetence of project manager, Incompetence of construction supervision consultants and design consultants, Unstable interest rates, and Unstable construction policies. Finally, Spearman's rank correlation coefficient was also calculated to analyse the consensus in perspectives of stakeholders on the attributes causing cost overruns in projects. Findings of the study can help project manager, owner, contractor and consultant to propose appropriate solutions to reduce construction cost overruns of transmission grid projects.", "num_citations": "2\n", "authors": ["449"]}
{"title": "The prestige of stock exchanges and corporate cash holding in transition economies: a study on Vietnamese listed firms\n", "abstract": " The main purpose is to examine the relationship between corporate cash holding level and the prestige of the stock exchanges. And the other determinants in the listing requirements impact on cash holding level will be indicated. The paper uses a sample of 577 listed firms excluding the financial institutions on the Vietnamese stock exchange over the period 2007\u20132015. The results show that the listed firms on the stock exchange with higher prestige hold larger amount of cash reserve and vice versa. The study shows that there is a statistically significant connection between cash holding and the listing requirements such as profitability, dividend and information disclosure. The findings have implications on the cash management of listed firms in the stock exchanges with dissimilar prestige.", "num_citations": "2\n", "authors": ["449"]}
{"title": "Systemability: a new reliability function for different environments\n", "abstract": " Industrial applications often observe the difference between laboratory reliability test in standard conditions and component or system reliability when it is set in motion through different environments and real-world conditions. As a matter of fact reliability variable is considerably influenced by environmental factors. Environmental factors may change failure rate, reliability, and availability of systems.", "num_citations": "2\n", "authors": ["449"]}
{"title": "The Explicit Algebraic Reynolds Stress Models for Turbulent Flows\n", "abstract": " The explicit algebraic Reynolds stress models are obtained from second-order closure models that are valid for three-dimensional turbulent flows in non-inertial frames. The purpose of this present research is to simplify the development of the Reynolds stress anisotropy tensor. This anisotropy stress tensor has seven scalar coefficients and has seven tensor polynomial groups that are the integrity basis for the functions of both symmetric and antisymmetric tensors. This research will also explicitly determine the six independent invariants of the mean strain rate tensor and of the mean rotation rate tensor. The resulting algebraic equation for the anisotropy tensor depends on the choice of the model that is used to determine the dissipation rate and pressure-strain correlation. These equations also represent the slow pressure strain rate and an isotropic dissipation rate tensor of the Rotta model. The results of present research can be compared with the results of Gatski and Speziale that give the complete expression for a traceless symmetric second order tensor which depended on the symmetric and the antisymmetric tensor that involved ten tensor polynomial groups with five independent invariants. The present work reduces the ten tensor polynomial groups down to seven groups which drastically decreases computational time.", "num_citations": "2\n", "authors": ["449"]}
{"title": "Quality management systems and statistical quality control\n", "abstract": " Organizations depend on their customers and therefore should understand current and future customer needs, should meet customer requirements and strive to exceed customer expectations... Identifying, understanding and managing interrelated processes as a system contributes to the organization\u2019s effectiveness and efficiency in achieving its objectives (EN ISO 9000:2006 Quality management systems \u2013 fundamentals and vocabulary). Nowadays, user and consumer assume their own choices regarding very important competitive factors such as quality of product, production process, and production system. Users and consumers start making their choices when they feel they are able to value and compare firms with high quality standards by themselves. This chapter introduces the reader to the main problems concerning management and control of a quality system and also the main supporting decision\u00a0\u2026", "num_citations": "2\n", "authors": ["449"]}
{"title": "Basic models and methods for maintenance of production systems\n", "abstract": " The European standard EN 13306 (Maintenance terminology) distinguishes two main types of maintenance, called \u201cmaintenance strategies,\u201d as:             \u2022 \u201cPreventive maintenance ... carried out at predetermined intervals or according to prescribed criteria and intended to reduce the probability of failure or the degradation of the functioning of an item\u201d;             \u2022 \u201cCorrective maintenance ... carried out after fault recognition and intended to put an item into a state in which it can perform a required function\u201d.             One of the most critical decisions for the analyst, i. e., the maintenance manager, is the determination of the items subject to preventive maintenance, then the time schedule or the number of units of use suitable for performing the maintenance actions. A famous proverb, also used a lot in television spots of a well-known toothpaste, is \u201cprevention is better than cure,\u201d known as \u201cprevenire \u00e8 meglio che curare\u00a0\u2026", "num_citations": "2\n", "authors": ["449"]}
{"title": "On mortality modeling\n", "abstract": " Human life expectancy has risen in most developed countries over several decades, causing the observed demographic shifts. Many researchers have been focused on developing methods to evaluate and forecast the life expectancy based on various approaches including probabilistic, time series, Bayesian, stochastic processes, Markov processes, martingales, etc. In this paper we discuss a brief review of research and distributions commonly used in the area of mortality. We also discuss an empirical research finding that the logic distribution out performs the other distributions such as Weibull, Gompertz, Gompertz-Makeham, log logistic, and loglog based on the mortality data set for the year 2005 of the United States.", "num_citations": "2\n", "authors": ["449"]}
{"title": "A software testing-progress evaluation model based on a digestion process of test-cases\n", "abstract": " It is of great importance for software engineers and managers to evaluate software testing-progress in a large-scale software production process, since tremendous software development resources must be consumed to achieve high quality and reliability of a software product. By focusing on the behavior of the digested test-case data observed in the testing process, we construct a stochastic model and derive several quantitative measures for software testing-progress evaluation. Actual data observed in the testing process are analyzed by the proposed model, and we discuss the applicability of our models.", "num_citations": "2\n", "authors": ["449"]}
{"title": "Performability and cost analysis of degradable systems\n", "abstract": " Many models and methods have been proposed in the literature to evaluate the reliability of fault-tolerant systems. In this paper, we study systems with one degradation mode. We introduce a static model and present efficient algorithms for the evaluation of the system performance and cost. For the special case of a system consisting of identical units we derive an efficient procedure for the determination of an optimal system design with respect to a minimum system cost.", "num_citations": "2\n", "authors": ["449"]}
{"title": "Special Issue on Fault-Tolerant Software\n", "abstract": " CiNii \u8ad6\u6587 - Special Issue on Fault-Tolerant Software CiNii \u56fd\u7acb\u60c5\u5831\u5b66\u7814\u7a76\u6240 \u5b66\u8853\u60c5\u5831 \u30ca\u30d3\u30b2\u30fc\u30bf[\u30b5\u30a4\u30cb\u30a3] \u65e5\u672c\u306e\u8ad6\u6587\u3092\u3055\u304c\u3059 \u5927\u5b66\u56f3\u66f8\u9928\u306e\u672c\u3092\u3055\u304c\u3059 \u65e5\u672c\u306e\u535a\u58eb\u8ad6\u6587\u3092\u3055\u304c\u3059 \u65b0\u898f \u767b\u9332 \u30ed\u30b0\u30a4\u30f3 English \u691c\u7d22 \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u9589\u3058\u308b \u30bf\u30a4\u30c8\u30eb \u8457\u8005\u540d \u8457\u8005ID \u8457\u8005 \u6240\u5c5e \u520a\u884c\u7269\u540d ISSN \u5dfb\u53f7\u30da\u30fc\u30b8 \u51fa\u7248\u8005 \u53c2\u8003\u6587\u732e \u51fa\u7248\u5e74 \u5e74\u304b\u3089 \u5e74\u307e\u3067 \u691c\u7d22 \u691c\u7d22 \u691c\u7d22 CiNii\u7a93\u53e3 \u696d\u52d9\u306e\u518d\u958b\u306b\u3064\u3044\u3066 Special Issue on Fault-Tolerant Software MEYER JF \u88ab\u5f15\u7528\u6587\u732e: 1\u4ef6 \u8457\u8005 MEYER JF \u53ce\u9332\u520a\u884c\u7269 IEEE Transactions on Reliability IEEE Transactions on Reliability 42, 177-258, 1993 \u88ab\u5f15\u7528\u6587\u732e: 1\u4ef6\u4e2d 1-1\u4ef6\u3092 \u8868\u793a 1 \u30bd\u30d5\u30c8\u30a6\u30a7\u30a2\u4fe1\u983c\u6027\u30e2\u30c7\u30eb\u3068 \u305d\u306e\u5fdc\u7528 : \u30bd\u30d5\u30c8\u30a6\u30a7\u30a2\u4fe1\u983c\u6027\u306e\u5b9a\u91cf\u7684\u8a55\u4fa1\u6cd5\u3068\u9069\u7528\u6280\u8853 \u5c71\u7530 \u8302 \u5fdc\u7528\u6570\u7406 5(1), 15-34, 1995 \u53c2\u8003\u6587\u732e47\u4ef6 Tweet \u5404\u7a2e\u30b3\u30fc\u30c9 NII\u8ad6\u6587ID(NAID) 10003354129 NII\u66f8\u8a8cID(NCID) AA00668109 \u8cc7\u6599\u7a2e\u5225 \u96d1\u8a8c\u8ad6\u6587 \u30c7\u30fc\u30bf\u63d0\u4f9b\u5143 CJP\u5f15\u7528 \u66f8\u304d\u51fa\u3057 RefWorks\u306b\u66f8\u304d\u51fa\u3057 EndNote\u306b\u66f8\u304d\u51fa\u3057 Mendeley\u306b\u66f8\u304d\u51fa\u3057 Refer/\u3067\u3067(\u2026", "num_citations": "2\n", "authors": ["449"]}
{"title": "Mean life of k-to-l-out-of-n systems with nonidentical Weibull distribution components\n", "abstract": " A k-to-l-out-of-n system is a noncoherent system in which no fewer than k and no more than l out of n units are to function for the successful operation of the system. Examples of noncoherent systems are found in communication, multiprocessor and transportation systems. A noncoherent system is very general and includes the parallel, series and N-Modular Redundancy (NMR) systems as special cases. In this paper presents the formulas for calculating the reliability and the mean life of a k-to-l-out-of-n system with non-identical weibull distribution components.", "num_citations": "2\n", "authors": ["449"]}
{"title": "Optimal cost effective design of hybrid hardware redundant systems\n", "abstract": " In many critical applications of digital systems, fault tolerance has been an essential architectural attribute for achieving high reliability. We address the problem of designing an optimal hybrid hardware redundant system. The designer of a system is always confronted with the problem of trading off reliability against cost. On the one hand, it is essential to provide for each vital module of the system as many spare units as possible in order to ensure high reliability. On the other hand, it is essential not to have an excessively costly, heavy or bulky system. In this paper we determine the optimal number of spare units which minimizes the expected total system cost. A numerical example is provided to illustrate the methods.", "num_citations": "2\n", "authors": ["449"]}
{"title": "Optimal number of redundant channels for a digital communication system\n", "abstract": " In many critical applications of digital systems, fault tolerance has been an essential architectural attribute for achieving high reliability. This paper considers the problem of determining the optimal number of redundant channels in a digital communication system. The expected system cost is obtained. We determine the number of redundant channels minimizing the expected system cost. A numerical example illustrates the techniques.", "num_citations": "2\n", "authors": ["449"]}
{"title": "Modeling Reliability of Threshold Weighted Indecisive Voting Systems\n", "abstract": " In industry, the method of hypothesis acceptance based on available information is widely used in applications such as system modeling. Xie and Pham modeled the reliability of weighted threshold voting systems with a general recursive reliability function in human organization systems. As an extended study, we introduce a generalized weighted indecisive-voting n-unit system using a new decision rule consisting both a threshold parameter \u03c4 and a new indecisive parameter \u03b8. In general, indecision happens due to limited information, and an indecisive parameter is then applied if no decision is made. System reliability R is calculated to show the system performance. Main results imply a strong dependence of system performance on decision rule and large potential of model adjustment via parameter initialization. The contribution of this article is that we introduce the indecisive effect to make the model more\u00a0\u2026", "num_citations": "1\n", "authors": ["449"]}
{"title": "A two-stage intervened decision system with multi-state decision units and dynamic system configuration\n", "abstract": " This paper develops the performability and cost\u2013benefit models for a two-stage intervened decision system with majority voting rule and binary input and output. The decision process of the system contains two stages: an inspection stage (stage 1) and a result submission stage (stage 2). During the first stage, each decision unit in the system will have multiple states and a supervisor will come to visit each unit and check its state for at most twice. The supervisor will conduct the first visit to each unit for certain. However, the behavior of the second visit to each unit will be determined by its state during the first visit. In addition, each decision unit may be removed from the system given certain states during each visit. Therefore the structure of system may change during the decision process. The units which are not removed during the first stage can submit the result at any time during the second stage. However\u00a0\u2026", "num_citations": "1\n", "authors": ["449"]}
{"title": "Preface: reliability and quality management in stochastic systems\n", "abstract": " This special issue of the Annals of Operations Research is related to the Asia\u2013Pacific International Symposium on Advanced Reliability and Maintenance Modeling (APARM 2016), held during 24\u201326 August 2016, at the Hanyang University, Seoul, South Korea. It focuses on new international research in theoretical and related applications to solve quality management problems. Reliability and quality management are quite important in many practical stochastic systems, such as computer systems, logistic systems, production systems, and the like. Developing and applying operations research methods to measure the robustness and to solve the engineering problem of a stochastic system is indeed a crucial task, enabling the system manager to carry out improvements after measuring the stochastic system.", "num_citations": "1\n", "authors": ["449"]}
{"title": "Durability and reliability evaluation of chute on the floor of high-speed train\n", "abstract": " With the rapid development and the universal promotion of lightweight body design technology, floor chute fatigue performance gradually become the focus, and the key of evaluating the performance is obtaining real load and accurately evaluating the strength. Through the full test of the actual condition of the high-speed train line, the dynamic load generated by the random vibration of the vehicle body due to the track excitation is obtained, and the car chassis floor of the vehicle body is directly taken as the structural sample to test the fatigue limit during the test of floor slide fatigue. Based on the fatigue test data, the PSN curve of floor chute structure with a small sample size was made according to the principle of sample information aggregation. Then with fatigue cumulative damage theory, evaluating the fatigue durability of the floor chute suspension structure under the real dynamic load. At the same time, using the statistic model to establish the reliability analysis model of the floor chute with more hanging points and more equipment, and analyzing the system reliability of the car floor slide hanger equipment. The results show that the evaluation value of the system reliability is 0.86, and the fatigue life is significantly higher than design..", "num_citations": "1\n", "authors": ["449"]}
{"title": "Parametric simulation analysis and reliability of escalator truss\n", "abstract": " Escalators are now more and more widely used around the world. As escalator accidents occur from time to time, it is very important to analyze the structure and reliability of escalators. This paper studies the structure and reliability analysis methods of typical escalators. By applying parametric analysis methods, the efficiency and accuracy of escalator structure analysis are greatly improved. Because the failure of each structural member of the escalator truss system is dependent, this paper applies the dependent failure reliability modeling method and analyzes the reliability of the escalator truss system. The results show that the reliability analysis method of the truss system after considering the dependent failure is more reasonable. The above research work has certain reference significance for the design and analysis of escalators.", "num_citations": "1\n", "authors": ["449"]}
{"title": "Self-adaptive stress accelerated life testing scheme\n", "abstract": " The initial stress of step-up stress accelerated life tests is commonly too low which is not very efficient when used in accelerated life testing. Though the efficiency of step-down stress accelerated life tests has improved in general, researchers and engineers may have to deal with the failure mechanism changing due to the high starting stress. In this paper, we develop a new accelerated life test scheme, called self-adaptive stress accelerated life tests (SAS-ALT), with considerations of the relative stress loading criterion. We also develop algorithm steps for implementation of SAS-ALT. We formulate and discuss an optimization function of the proposed SAS-ALT that minimizes the variance of the system life time using the maximum-likelihood method, Fisher matrix, and accelerated model. Some factors of the SAS-ALT, such as stress and failure number, were chosen depending upon the optimum function. We\u00a0\u2026", "num_citations": "1\n", "authors": ["449"]}
{"title": "Reliability management and computing\n", "abstract": " Most of the products that affect our daily lives are becoming more complex. Reliability management\u2014which integrates processes, policies, and reliability predictions from the beginning of the product development life cycle to ensure high levels of product performance and safety\u2014helps companies address the challenges of increasingly complex systems and software and globally widespread processes in today\u2019s competitive marketplace. This special issue on reliability management and computing consists of 13 outstanding papers that address various research challenges in reliability management and computing related areas including optimization, maintenance, network reliability, replacement policies, software reliability and measurements, change-point modeling, renewal processes, statistical inference, fault-tolerant computing, etc.The first paper, by Chang, presents a new allocation approach that orders\u00a0\u2026", "num_citations": "1\n", "authors": ["449"]}
{"title": "A new NHPP reliability model with considerations of hardware and software failures\n", "abstract": " Today almost everyone in the world is directly or indirectly affected by computer systems. Computers indeed are used in diverse areas for various applications including air traffic control, aircraft, automotive mechanical and safety control, and hospital health care, affecting many millions of people. Many software reliability growth models have been proposed. Most of the research in these areas has been limited to consideration of either the hardware system alone, or the software system alone. In these systems, the hardware and the software subsystem are not independent of each other, and consequently the system reliability is affected by interactions between them. In this paper, we present a unified software reliability model with considerations of two types of failures: hardware and software. The explicit mean value function of the proposed model is presented. We show that the proposed model appears to be the\u00a0\u2026", "num_citations": "1\n", "authors": ["449"]}
{"title": "Reliability and Maintenance of the Surveillance Systems Considering Two Dependent Processes\n", "abstract": " The application of surveillance systems is a great enhancement of security level to the monitored area by providing important reference for the security teams to make prompt actions against threats or incidents.", "num_citations": "1\n", "authors": ["449"]}
{"title": "Computing the reliability of complex systems\n", "abstract": " Summary form only given. Reliability computing, which consists of modeling and predicting, has become of great interest in recent years due to spacious arrays of complex systems and applications in our everyday safety, security and economic welfare. Modeling is hard but predicting is much harder due to the uncertainties of the system's operating environments and human will when it come to the know-how of to use the analysis of complex data and modeling results in order to make effective decisions. This talk discusses some recent reliability computing and approaches that focus on the modeling of complex systems, both hardware and software, subject to the uncertainties of operating environments and big data security. Reliability computing results and model selections based on various criteria are also discussed. An application of camera security systems is also discussed.", "num_citations": "1\n", "authors": ["449"]}
{"title": "Optimal maintenance policies with preventive maintenance\n", "abstract": " In this paper, two maintenance policies, the block replacement policy and the age replacement policy, have been investigated, and compared under the warranty. Although there is much comparative research between the age replacement policy and block replacement policy, we focus on comparisons of both the maintenance policies, considering the warranty. We develop two maintenance cost models, with considerations of non-renewable and renewable warranty policies subject to minimal repair for the warranty period and the post warranty period, using failure times and repair times. Numerical examples are discussed, to demonstrate the applicability of the methodology derived in the paper.", "num_citations": "1\n", "authors": ["449"]}
{"title": "Systemability with loglog distributions\n", "abstract": " In this paper, we focus on the systemability aspects for systems subject to the uncertainty of operating environments. Often the exact systemability function for complex systems is difficult to obtain due to the random operating environments. This paper discusses a simple approximation of systemability with loglog distributions using the Taylor series expansion. The proposed systemability approximation function seems to be simple, yet accurate, and easy to use in practice. Thus, it is worth the extra effort to further validate the results with other distribution functions as well as complex applications.", "num_citations": "1\n", "authors": ["449"]}
{"title": "Parallel\u2010Series and Series\u2010Parallel Systems\n", "abstract": " This article will consider the reliability of two different systems, namely, a parallel\u2010series system and a series\u2010parallel system. The main focus is on finding the system reliability when the failure of each component depends on the length of time that the component is observed. In particular, the main result will be illustrated for exponentially distributed failure times.", "num_citations": "1\n", "authors": ["449"]}
{"title": "Spare Parts Forecasting and Management\n", "abstract": " Even without considering crashes and other damage during its life, a car needs parts such as its engine oil, tires, and brake pads to be replaced or changed. Similarly, production systems have the same need for spare parts. Although a very relevant topic, spare parts management has rarely been studied. How many spare parts are there in the local warehouse of the company? How can future demand be forecast? This chapter deals with these questions, and presents several methodologies to support decision making on this theme.", "num_citations": "1\n", "authors": ["449"]}
{"title": "Reliability analysis of degradable dual-systems subject to competing operating-modes\n", "abstract": " This paper presents a model to evaluate the reliability of degradable systems subject to competing operating modes: normal mode and overloaded mode. The principles of system functioning can be described as follows. The system functions:(1) in a good mode, called normal condition, if there are at least k out of n units must work; and (2) in an overloaded mode, called stressed condition, when at least m out of n units must work. Under normal conditions, the reliability of a component is p, whereas under a stressed condition, the reliability of a component is pz. Several applications are also discussed to illustrate such dual-systems in practices. We then derive a model for evaluating the reliability of the proposed degradable dual-systems of non-identical component reliabilities. We also discuss a cost model and present the optimal system design policy that minimizes the expected system cost of n-component systems.", "num_citations": "1\n", "authors": ["449"]}
{"title": "Mortality data analysis, modeling and prediction\n", "abstract": " Due to remarkable advances in medicine in the past decades and improvements in medical care, particularly in medications aimed at preventing heart disease and cancer, life expectancy in the United States has been rising in recent decades. With the human lifespan continues increasing and a large part of the United States population aging, many researchers [2-22, 26, 28-40] in various fields have interested in developing models to evaluate quantitatively the mortality measures such as the force of mortality rates and life expectancy for various populations. This paper analyzes the mortality rate data over a 10-period from 1994 to 2003 in the United States based on several distribution functions, such as Gompertz, Gompertz-Makeham, Logistic, log logistic, loglog and Weibull that commonly used in the area of mortality, to determine some important mortality measures including life expectancy. Here we find that, out of the six, the logistic distribution out performs the other five based on the ten years data from 1994 to 2003 in the United States. We also find that on average the expectation of life at birth in the United States in 2003 for overall, males and females were 83.6, 81.7 and 84.0 years, respectively. This new result shows that the life expectancy on average in the United States is significantly larger than in existing official reports. Life expectancy changes as one will get older. By the time of late adulthood, their chances of living longer increase. For example, although the life expectancy from birth for all people in the United States is 83.6 years, those who live to age 60 will have an average of almost 24.5 additional years left to live, making their\u00a0\u2026", "num_citations": "1\n", "authors": ["449"]}
{"title": "Reliability of redundant systems\n", "abstract": " This articles provides a brief concept of how to evaluate the reliability of redundant systems with various aspects including dynamic redundancy, imperfect coverage, fail\u2010safe redundancy, noncoherent systems, degradable redundancy, multiple failure modes, k\u2010out\u2010of\u2010n system redundancy, weighted systems, and load\u2010sharing redundancy.", "num_citations": "1\n", "authors": ["449"]}
{"title": "Software reliability and fault-tolerant systems: An overview and perspectives\n", "abstract": " This chapter outlines the basic concepts of software development process, reliability engineering, and data analysis. It also presents some of the existing NHPP software reliability models and their applications. Numerical examples are provided to illustrate the results. A generalized software reliability model considering environmental factors is presented and Sections 72.5 and 72.6 discuss briefly the software fault-tolerant concepts and software cost models, respectively.", "num_citations": "1\n", "authors": ["449"]}
{"title": "A systematic-testing methodology for software systems\n", "abstract": " In this paper, a software testing methodology called two-level testing is developed to improve testing effectiveness by reducing the testing efforts and at the same time, by ensuring the predetermined quality of software products. The testing procedure including the criteria for alternating 100% and sampling testing is proposed by incorporating the characteristics of testing behavior into the well-known sampling method. The metrics of the testing performance are derived based on the transition probability. Various combinations of controllable parameters ensuring equivalent quality are also provided for the purpose of effective application. A numerical example is provided to illustrate the testing effectiveness of the proposed testing method.", "num_citations": "1\n", "authors": ["449"]}
{"title": "Promotional Warranty Policies: Analysis and Perspectives\n", "abstract": " Warranty is a topic that has been studied extensively by different disciplines including engineering, economics, management science, accounting, and marketing researchers [7.1, p. 47]. This chapter aims to provide an overview on warranties, focusing on the cost and benefit perspective of warranty issuers. After a brief introduction of the current status of warranty research, the second part of this chapter classifies various existing and several new promotional warranty policies to extend the taxonomy initiated by Blischke and Murthy [7.2]. Focusing on the quantitative modeling perspective of both the cost and benefit analyses of warranties, we summarize five problems that are essential to warranty issuers. These problems are: i) what are the warranty cost factors; ii) how to compare different warranty policies; iii) how to analyze the warranty cost of multi-component systems; iv) how to evaluate the warranty benefits; v) how to determine the optimal warranty policy. A list of future warranty research topics are presented in the last part of this chapter. We hope that this will stimulate further interest among researchers and practitioners.", "num_citations": "1\n", "authors": ["449"]}
{"title": "NHPP software reliability and cost models with testing coverage\n", "abstract": " NHPP software reliability and cost models with testing coverage - Dialnet Ir al contenido Dialnet Buscar Revistas Tesis Congresos Ayuda NHPP software reliability and cost models with testing coverage Autores: Xuemei Zhang, Hoang Pham Localizaci\u00f3n: Quality control and applied statistics, ISSN 0033-5207, Vol. , N\u00ba. , 2004, p\u00e1gs. 107-108 Idioma: ingl\u00e9s Texto completo no disponible (Saber m\u00e1s ...) Fundaci\u00f3n Dialnet Acceso de usuarios registrados Imagen de identificaci\u00f3n Identificarse \u00bfOlvid\u00f3 su contrase\u00f1a? \u00bfEs nuevo? Reg\u00edstrese Ventajas de registrarse Dialnet Plus M\u00e1s informaci\u00f3n sobre Dialnet Plus Opciones de compartir Facebook Twitter Opciones de entorno Sugerencia / Errata \u00a9 2001-2021 Fundaci\u00f3n Dialnet \u00b7 Todos los derechos reservados Dialnet Plus Accesibilidad Aviso Legal Coordinado por: Fundaci\u00f3n Dialnet Inicio Buscar Revistas Tesis Congresos Ayuda Registrarse Universidad de La Rioja \u2026", "num_citations": "1\n", "authors": ["449"]}
{"title": "Highly reliable systems: Designing software for improved assessment\n", "abstract": " Very few computer users have escaped problems related to software failure, such as the mysterious loss of a file or a suddenly frozen operating system shell. Due to software faults, hours of work may be lost or, unknowingly, inappropriate business decisions may be made. Computer users, not having a better option, consider these events as tolerable and expected inconveniences. In terms of their consequences, these failures relate primarily to economics. The users\u2019 tolerance to inconvenience is inversely proportional to the price they are willing to pay for the given software product. Software price and software quality are the attributes which need to be in balance for the given application. However, failures in certain systems can lead to endangering the financial prosperity of a company, or even loss of human lives. These systems are called mission-critical (telecommunication, transaction-processing, etc.) and safety-critical (flight control, nuclear plant monitoring, rail-way signaling, etc.) systems, respectively. Generally, safety-critical systems require high levels of predictability. Security, availability, safety, reliability, performability, and other attributes characterizing their operation must be guaranteed. The term high-assurance systems was recently introduced to stress the significance of a multi-disciplinary approach to engineering critical systems and applications [Bhattacharya et al.(1997)].", "num_citations": "1\n", "authors": ["449"]}
{"title": "A Bayesian approach to the optimal policy under imperfect preventive maintenance models\n", "abstract": " Following the work of Barlow and Hunter (1960), many researchers have proposed various maintenance policies which include age replacement, block replacement, and periodic replacement with minimal repair at failure, etc.(Barlow et al.(1965); Valdez-Flores and Feldman (1989)]. Though age replacement and condition-based maintenance policies are also being studied by many researchers, time-based main-tenance policies where replacements are done at specified time intervals are most popularly used in heavy industries or steel companies. These industries generally have periodic preventive maintenance (PM) schedules three or four times a month and have a major overhaul one or two times a year.A simple maintenance policy assumes that the system becomes anew after each PM. In reality, however, the improvement after each PM depends on the age of the system as well as the cost and the cumulative number of PMs done since the last major overhaul or replacement. Hence, the system may not be renewed at each PM, but its failure rate generally increases with the number of PMs, while it can be assumed to be renewed at the major overhaul. This kind of PM is often referred to as being imperfect in the literature (Nakagawa (1981, 1986)]. The imperfect PM policy may be divided into a periodic case and a sequential case, where the periodic case is to make PM intervals constant while the sequential case is to make PM intervals shorter as the number of PMs increases. We reconsider Nakagawa's model (Nakagawa (1986)] for the imperfect (sequential) PM and replacement policy by adopting the Bayesian approach. Assumptions", "num_citations": "1\n", "authors": ["449"]}
{"title": "Hardware-software reliability perspectives\n", "abstract": " Nowadays the size and complexity of modern systems, such as nuclear power plants, medical monitoring control, real-time military and air traffic control, are extremely huge and it has been shown that there exist remarkable interactions between hardware and software. Ultra-reliability is a crucial need of complex critical systems that can inflict or prevent death. This chapter discusses (1) recent studies in software reliability that include nonhomogeneous Poisson process and Bayesian models; (2) the interactions between hardware and software failures; and (3) future research directions and challenge issues in reliability engineering.", "num_citations": "1\n", "authors": ["449"]}
{"title": "Brief Contributions\n", "abstract": " The IEEE Computer Society is an association of people with professional interest in the field of computers. All members of the IEEE are eligible for membership in the Computer Society, as are members of certain professional societies and other computer professionals. Computer Society members will receive this Transactions upon payment of the annual Society membership fee ($37 for IEEE members, $92 for all others) plus an annual subscription fee (paper only: $40; electronic only: $32; combination: $52). For additional membership and subscription information, visit our Web site at http://computer. org/subscribe, send email to help@ computer. org, or write to IEEE Computer Society, 10662 Los Vaqueros Circle, PO Box 3014, Los Alamitos, CA 90720-1314 USA. Individual subscription copies of Transactions are for personal use only.", "num_citations": "1\n", "authors": ["449"]}
{"title": "Reliability Design for Manufacturing ULSI Circuits\n", "abstract": " CiNii \u8ad6\u6587 - Reliability Design for Manufacturing ULSI Circuits CiNii \u56fd\u7acb\u60c5\u5831\u5b66\u7814\u7a76\u6240 \u5b66\u8853\u60c5\u5831 \u30ca\u30d3\u30b2\u30fc\u30bf[\u30b5\u30a4\u30cb\u30a3] \u65e5\u672c\u306e\u8ad6\u6587\u3092\u3055\u304c\u3059 \u5927\u5b66\u56f3\u66f8\u9928\u306e\u672c\u3092\u3055\u304c\u3059 \u65e5\u672c\u306e\u535a\u58eb\u8ad6\u6587\u3092\u3055\u304c\u3059 \u65b0\u898f\u767b\u9332 \u30ed\u30b0\u30a4\u30f3 English \u691c\u7d22 \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u9589\u3058\u308b \u30bf\u30a4\u30c8\u30eb \u8457\u8005\u540d \u8457\u8005ID \u8457\u8005\u6240\u5c5e \u520a\u884c \u7269\u540d ISSN \u5dfb\u53f7\u30da\u30fc\u30b8 \u51fa\u7248\u8005 \u53c2\u8003\u6587\u732e \u51fa\u7248\u5e74 \u5e74\u304b\u3089 \u5e74\u307e\u3067 \u691c\u7d22 \u691c\u7d22 \u691c\u7d22 Reliability Design for Manufacturing ULSI Circuits PHAM H. \u88ab\u5f15\u7528\u6587\u732e: 1\u4ef6 \u8457\u8005 PHAM H. \u53ce\u9332\u520a\u884c\u7269 Concurrent Engineering Concurrent Engineering, 349-362, 1993 Wiley \u88ab\u5f15\u7528\u6587\u732e: 1\u4ef6\u4e2d 1-1\u4ef6\u3092 \u8868\u793a 1 \u907a\u4f1d\u7684\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u306b\u3088\u308b\u30d5\u30a1\u30b8\u30a3\u4fe1\u983c\u6027\u306e\u6700\u9069\u8a2d\u8a08\u554f\u984c\u306e\u4e00\u89e3\u6cd5(\u30d5\u30a1\u30b8\u30a3\u3068\u907a\u4f1d\u7684\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0 ) \u4f50\u3005\u6728 \u6b63\u4ec1 , \u6a2a\u7530 \u5b5d\u96c4 , \u7384 \u5149\u7537 \u65e5\u672c\u30d5\u30a1\u30b8\u30a3\u5b66\u4f1a\u8a8c 7(5), 1062-1072, 1995 \u53c2\u8003\u6587\u732e22\u4ef6 \u88ab \u5f15\u7528\u6587\u732e1\u4ef6 \u56fd\u7acb\u60c5\u5831\u5b66\u7814\u7a76\u6240\uff08NII\uff09\u30a4\u30d9\u30f3\u30c8 Tweet \u5404\u7a2e\u30b3\u30fc\u30c9 NII\u8ad6\u6587ID(NAID) 10008807571 \u8cc7\u6599\u7a2e\u5225 \u56f3\u66f8\u306e\u4e00\u90e8 \u30c7\u30fc\u30bf\u63d0\u4f9b\u5143 CJP\u5f15\u7528 \u66f8\u304d\u51fa\u3057 RefWorks\u306b\u66f8\u304d\u51fa\u3057 EndNote/(\u2026", "num_citations": "1\n", "authors": ["449"]}
{"title": "A note on the Venn and Ben diagrams\n", "abstract": " Yellman introduced an alternative to the Venn diagram for mapping experiment state spaces, which he calls a Ben diagram. In this paper, Ben and Venn diagrams are generated and compared for two applications, a Hamming error correcting code and a set identity.", "num_citations": "1\n", "authors": ["449"]}
{"title": "Optimal design of majority redundant systems\n", "abstract": " In many critical applications of digital systems, fault tolerance has been an essential architectural attribute for achieving high reliability. This paper considers the problem of determining the optimal threshold level for majority systems so as to minimize the average system cost. Some numerical examples are given to illustrate the results.", "num_citations": "1\n", "authors": ["449"]}
{"title": "Optimal cost effective design of a class of noncoherent systems\n", "abstract": " A k-to-l-out-of-n system is a noncoherent system which no fewer than k and no more than l out of n units are to function for the successful operation of the system. Examples of noncoherent systems are found in communication, multiprocessor and transportation systems. Assume that the cost of units and the cost of system failure are given. In the paper, the optimization problems are formulated and solved for the minimum expected total cost of k-to-l-out-of-n systems. For fixed values of k and l, the author determines the optimal number of units minimizing the expected total system cost. Several numerical examples are given to illustrate the results.< >", "num_citations": "1\n", "authors": ["449"]}
{"title": "On the Study of Discounted Warranty Cost for Minimally Repaired Series Systems\n", "abstract": " Many factors should be considered in modeling discounted warranty cost (DWC) of repairable systems/products including system structure, component failure processes, methods of discounting as well as the warranty policy itself. In this paper, we present DWC models for repairable series systems. In particular, a free repair warranty (FRW) policy and a pro-rata warranty (PRW) policy are studied. The impact of repair actions on components\u2019 failure times is assumed to be minimal, hence non-homogeneous Poisson processes are used to describe the failure processes. Two types of discounting methods are considered in this paper: a continuous discounting function and a discrete discounting function. Expressions for both expected value and variance of DWC are derived. The applications of our findings can be seen in warranty design, warranty reserve determination and risk analysis. Our approach incorporates the\u00a0\u2026", "num_citations": "1\n", "authors": ["449"]}